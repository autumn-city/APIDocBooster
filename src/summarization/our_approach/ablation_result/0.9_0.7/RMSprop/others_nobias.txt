Vanilla adaptive gradients (RMSProp, Adagrad, Adam, etc) do not match well with L2 regularization. 
Usually batch sizes around 40 gives better results, as for my experience training with 40 batch size for 3 epocs using default RMsprop gives around 89% accuracy. 
Have you tried adjusting the learning rate of RMsprop optimizer ?  try with a very small value first ( default is 0.001, in keras implementation) and try to increment it with factors of 10 or 100.
In   particular, when combined with adaptive gradients, L2   regularization leads to weights with large gradients   being regularized less than they would be when using   weight decay.
There are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running average of first and second moment of the gradient. 
