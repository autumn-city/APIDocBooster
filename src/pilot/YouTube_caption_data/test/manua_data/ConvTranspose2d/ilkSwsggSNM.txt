 All right, let's now talk about convolutional auto encoders. And
 along with this topic, we will also talk about transposed
 convolutions and deconvolutions, which are also sometimes called
 strided convolutions. And I will also tell you why we need them.
 So here's an overview of how a convolutional auto encoder looks
 like. And if you just look at this figure here, it looks
 exactly like what you've seen for the fully connected auto
 encoder. The only difference is now that instead of using fully
 connected layers, we are using convolution layers for the
 encoder. And for the decoder, we are using these so called
 deconvolution layers or strided convolutions. So why do we need
 them? I mean, before, when we talked about CNNs in the
 convolutional neural network lecture, we used regular
 convolutions to down sample the images. So they become from the
 original size, they become smaller, as far as small
 convolutions we use in pooling layers. And how can we now
 reverse this? So how can we now go from a smaller representation
 to a larger representation? So for that, we need to kind of
 invert this convolution process. So for that, we are using these
 so called deconvolution layers. So there are many different
 words for that. So overall, what we are interested in when we
 talk about transposed convolution or deconvolution is
 that we want to increase the size of the output compared to
 the input. So what we want is we want to start with a smaller
 input and obtain a larger output, basically the opposite
 of the regular convolution. And like I said, there are different
 synonyms for this process. So sometimes it's called
 deconvolution. But yeah, mathematically, if you would want
 to be used a precise, let's say notation, mathematically, a
 deconvolution is defined as the inverse of a convolution, like
 the mathematical inverse, the inverse of a function. But we
 don't really need this or do this in the context of deep
 learning. So in deep learning, it's kind of enough to just have
 a convolution that up samples, it doesn't have to have exactly
 the inverse computation. So in that way, we call that also
 sometimes transposed convolution, I guess on the
 name comes from the fact that we go like the opposite from instead
 of going from large to small, we go from small to large. But even
 that term is a little bit, yeah, I would say weird. So sometimes
 people also use the term unconf or unconvolution. And sometimes
 people also use the term fractionally strided
 convolution, which might be a better term for that, if you
 consider how it is implemented. And I will show you the next
 couple of slides, how this transposed convolution also known
 as fractionally strided convolution is implemented. But
 again, if you read a deep learning paper, and people talk
 about a deconvolution or unconflare, or transposed
 convolution or fractionally strided conversion, chances are
 that all of these mean the same thing. So usually, people use
 all these words synonymously. And how it's implemented, I will
 show you in the next couple of slides. So at the top, to start
 with, let's take a look at the regular convolution first. So
 here at the bottom, we have the input in blue. And the output is
 at the top here in dark green. So this is a convolution with
 three by three kernel, right? So see that here, it's a three by
 three kernel. And if you look from here to here, it's going
 by two pixel dimensions. So it has a stride of two. So we're
 going here, and then the next here, and then we go down by two
 pixels. So actually, we start at the left corner again, so we are
 here now. And then we move again, by two positions. So
 it's just a regular convolution that you have already seen here
 with a three by three kernel, and a stride of two, and it
 outputs this two by two feature map. So in each, each iteration
 that generates one pixel, or one value. And then you get this
 two by two output. Now, the transposed convolution is shown
 at the bottom also with a stride of two, and three by three
 kernel, and it is doing the opposite. So that's maybe why
 it's called a transposed convolution, because it's doing
 it kind of backwards. So here, now you start with a two by two
 input. So the two by two input, what you get out is a five by
 five output. Right? So here, we have at the top, again, we have
 a five by five input, and a two by two output. So it's kind of
 doing the opposite. So the computation is that we go here
 from one pixel, and then we have our three by three kernel. And
 we obtain a three by three output. So in a regular
 convolution, if you have the kernel, you multiply each value
 here with a pixel value. So each weight is multiplied with a
 pixel value in the transposed one, it's essentially you take
 this value here, the one pixel value, and multiply it also by
 each value in the kernel. So you get three by three outputs. And
 then you move one position to the right and in the output,
 also, by a surface two, you move by two pixels. So in this way,
 you create this five by five output from the two by two
 input. So essentially going from a small image into a big image.
 Now, this is how it conceptually works. So if I go back one more
 time, so at the bottom here, this is how I like to think
 about a transpose convolution, this makes sense to me, this is
 intuitive to me. So again, here, this is again, what I showed you
 on the previous slide. That's how I think about it. However, in
 practice, this is usually emulated with a direct
 convolution. So there's this excellent paper on linked here,
 a guide for convolution arithmetic for deep learning
 that I mentioned earlier in this class, when we talked about
 convolution networks. And in this paper, they also describe
 the so called emulated approach using a direct convolution. So
 instead of implementing this process, there's a process that
 just uses a regular convolution to implement this transposed
 convolution. It's just both things here on the slide are
 achieving the same thing. It's just a different way of carrying
 it out. So in order to go from this two by two input to this
 five by five output, you can also implement it with this
 emulated approach, where you put these paddings onto the input.
 So here we have a stride of two and three by three kernel. So in
 this case, what would happen in this emulated approach, it would
 put here this padding. So what I'm highlighting here, this is
 like all padding, all these dotted lines are heading, it's
 adding the padding around the input image. And also, it, it
 adds the spacing between that's because of the stride. So and
 this is maybe why it's also called fractionally strided
 convolution. It's a weird word. But yeah, so if you look at this
 at the bottom, and you think a little bit about this, you can
 think about that. So if you walk through this mentally, step by
 step, you should be able to see that both the top and the bottom
 part here, they produce the same amount of these parts. But if
 you if you completed this whole process here, it should give you
 the same results as the completed feature map here.
 Because here you take also three by three kernel, but here you
 only consider this pixel, these other ones are essentially zero.
 And then you project it up to here. And this is essentially
 kind of the same as you are projecting this one up here in
 that way. And then you move this one to the right. And then you
 do the same thing now with this region here. Well, you have a
 strand of one thinker. So you're moving here, and then you do
 the same thing again. But now it's in a different position.
 It's now in the kernel. At this position here, the first
 iteration, let's say this was the first and the second one, it
 would be at this position. And this one will go here. And you
 continue. And at some point, when you go one row down, it
 will override and things like that. So in that way, it will
 give you essentially also this five by five output when you when
 you completed this, when you went also through the next
 rows. So long story short, I don't like to think of it like
 this, because this is kind of complicated, like doing the
 padding and putting these things apart. In my head, when I think
 of a transpose convolution, I think of it as like this, which
 I find more intuitive. But technically, yeah, the
 implementations, I think simpler in code, if you just reuse the
 convolution with this padding setup. Okay. Yes, another
 example. Here, it's a regular convolution with strike one,
 going from a four by four input to a two by two output. And here
 it's a transpose convolution, emulated with a direct
 convolution. So this is, again, an emulation here, it's a bit
 simpler to see because there's no stride. So here, you can just
 see there is a padding, two pixels on each side. And this
 will achieve the opposite. So going from a two by two input
 into a four by four output, whereas at the top, we go from
 four by four to two by two. Okay, so here's the equation, if
 you want to compute the output size, given the input, so as is
 is the stride here, and is either the height or the width.
 If it's quadratic, it could be the same K is the kernel size
 and P is the padding. So you can use this formula to compute the
 output size of a transposed convolution in pytorch. This is
 torch and n con transpose 2d. And here I just have some
 examples applying this equation to just make sure that this is
 indeed how pytorch behaves when you Yeah, when you look at the
 outputs. So you can plug in these numbers here for different
 kernel sizes, strides and padding. And sorry, different
 input, I didn't change those, you can play around with those.
 If you're interested, I only changed the input size here. But
 yeah, you can play around with this. And you should see this is
 how pytorch computes how you would know the output, given a
 certain input size, kernel size and padding. And notice also,
 one interesting aspect about that is also compared to a
 regular convolution, if you increase the padding, it
 decreases the output size in a regular convolution, if you
 increase the padding, it will also increase the output size.
 So it's the opposite here, padding decreases the size in
 regular convolution, it increases the size. So you have
 to kind of think backwards now. Okay. So yeah, this was the
 transposed convolution in a nutshell, one problem that might
 occur in practice, I honestly don't see it very often. But
 there's this problem of the checkerboard pattern. So if
 you're interested, there's an interesting article about that
 with nice visualizations. And because of overlaps in this
 transposed convolution, which is highlighted here, so these dark
 pixels, what might happen is that you might see certain
 checkerboard artifacts. So let me go, let me show you this
 slide again, this is the very first slide I already had
 earlier. So again, here, the regular conversion here, this is
 the transposed convolution. And you can see. So when I have,
 this is not the emulated one is the regular transposed
 conversion, how I like to think about it. So what you can see is
 when you go from this input here, into the output, and then
 do this for the next one, you have a stride of two, you can
 see, here, there's some overlap, right? So there's some overlap
 here. And then if you go down, then this one would be the area
 here of the highest overlap, right? So would overlap all four
 kernels at the end here. So in this can cause like certain
 checkerboard artifacts, so which are highlighted here, so you can
 see here, these, these kind of artifacts as some, some sort of
 pattern that appears here. And in this article, for instance,
 they done then instead recommend not using a transposed
 convolution, but you use a regular convolution, or
 essentially using nearest neighbor upsampling followed by
 a regular convolution, instead of using transposed convolution.
 Actually, I tried this out, it works well, too. So either way,
 I think it's fine. If you use transpose conversions, just you
 have to be careful that you don't encounters checkable
 artifacts. In this case, if you have a setup like this, and you
 may have a checkerboard artifact, how you would avoid
 this, or how you could avoid it is essentially by just using a
 two by two kernel, right? If you have a two by two kernel, and
 then you move it over by two positions, so the first one would
 be here, then you move it by two pixels, the second one would be
 here. And the last one will be cut off. But so you can
 technically try to avoid it by choosing your kernel size and
 stride such that there are no overlaps. But also, even if there
 are overlaps, usually, it's not always true that you find these
 checkable artifacts, we have to just look at the results and see
 if there's a problem. And you can optionally replace
 transposed convolutions by nearest neighbor up sampling and
 a regular convolution. If you're interested in that, I don't have
 a code example here, but I can share a code example with you on
 Piazza. So if you're interested, just ask me I can share code
 example, showing you how to implement that it's not a big
 deal. Alright, so that was convolutional auto encoders in
 a nutshell, like explaining how the transposed convolution works
 and so forth. And in the next video, I will show you how we
 can implement a convolutional auto encoder in pytorch.