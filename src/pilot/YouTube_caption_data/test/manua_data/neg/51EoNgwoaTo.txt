Neural Networks are found a lot in modern-day
technology.
They are for instance used in your favorite
voice-controlled home assistant, used in producing
and driving cars, or to diagnose diseases
in medical applications.
But what exactly is a neural net and how is
it able to remember information for a given
task?
That’s exactly what we’re going to find
out today.
I will start from the very basics and hope
to build up your understanding of the matter
from the ground up.
This video is only the first one of a couple
I have planned around this topic, so if you
don’t want to miss the next one, hit subscribe
now.
I also have outlets on Facebook and Instagram
for some behind-the-scenes info.
And if you like to discuss this topic with
other like-minded people and me, we have a
Discord server now.
All the links are in the description below.
What you will learn
First, we will talk about a neural net
from a birds-eye perspective.
After that, we will deep dive into the math
of one neuron to try to understand how it
makes a decision and is able to retain information
for a given task.
Beware there will be some math involved, but
if you are confident adding and multiplying
numbers, you are good to go.
Moving on, it is time to understand the limitations
of one neuron and how a net of neurons is
able to overcome these.
And while we’re doing this, we will also
put all of it into python code.
We really got a lot to do today.
If you are already familiar with this matter,
please don’t hesitate to use the timestamps
in the description below to skip to another
part of this video.
And now, without further ado, let’s get
started …
What is a neural network
The idea for a neural net is based on biology.
In your brain and throughout your nervous
system there are little cells called neurons.
Most neurons have a cell body, dendrites,
and an axon.
The dendrites of one neuron receive information
from other neurons.
This information is combined in the cell body
leading to a potential.
Each neuron has an activation threshold.
If the potential reaches the activation threshold,
pulses are generated by this neuron and sent
out via the axon to other connected neurons.
The connections between neurons can be stronger
and weaker, which means some incoming information
from other neurons have a bigger influence
on the activation of a neuron.
This is how neurons form nets, store information,
and make decisions.
Conceptually a common net looks like this.
We have an input layer in which information
from the outside system arrives.
This information is then processed by one
or multiple layers of neurons and arrives
at the output layer where the rest of the
system can receive the information from the
neural net.
As in biology, some connections are stronger
than others.
We then talk about some inputs having bigger
weights.
Here shown with thicker lines.
Not all neurons from one layer need to be
connected to all other neurons of the next
layer.
In fact, the connections of the different
neurons are pretty arbitrary and there are
a lot of different structures of neural nets
out there.
There are even some nets that connect back
from one layer to its previous layers.
In this video whatsoever we will just examine
the workings of a so-called feed-forward network
in which one layer feeds its information only
to the following layers.
But the math stays the same no matter what
your net is structured like.
On that note let’s move on and examine the
math of one neuron in a neural net.
A neuron, or you might find the name Perceptron
here and there as well, consists of five parts:
A couple of inputs with associated weights,
a bias, a sum, and an activation function.
Mathematically it looks like this: The weighted
sum of the inputs plus the bias is put through
an activation function and what comes out
at the other end is the output of the neuron.
In order to explain all the different parts
and why and how they work together, let’s
use an example.
Like a lot of others, I like a good whisky
from time to time.
In particular scotch.
But there are a lot of different whiskies
on the market and I would love to know if
I like a whisky before buying it or not.
This problem is called a classification-problem
and we can use a neural network to answer
this question for me.
Actually, just one neuron is enough in this
simplified example to make this decision.
Specifically, we want to have a neuron that
puts the whiskies into two categories: Whiskies
I probably like and whiskies I shouldn’t
try in the first place.
In order to build our neural network, we need
some data.
So I went out of my way and tried some whiskies
and jotted down if I liked them or not.
Now we need some properties we can use to
distinguish these whiskies.
These will be the input to our neuron.
So basically we need to decide now how we
can encode each whisky into numbers to feed
it into the inputs of our neuron.
And this is not as easy as it may seem.
There are countless ways to create a numerical
representation of a whisky like measuring
its color and handing it over as a floating-point
value, using the age of the whisky or the geo-coordinates
of its brewery.
If you don’t provide your net with the right
inputs it will come to very funky conclusions.
For instance, if I would hand in the name
of the whisky and its brightness.
The output later might be, that I like whisky
starting with a G and having a brightness
of less than 40% percent or something, something
like that.
That this would lead to a crappy output seems
obvious for this example, but in real-world
applications, it is often not that easy.
All of this is enough material for a video
on its own, and I will not cover it further
in this video.
But I wanted to mention it here because otherwise,
you won’t have a lot of fun with your neural
nets.
For our example, I went with two input parameters
per whisky: How full is the whisky in taste?
I call this Richness.
And how smoky is the whisky?
Smokiness
Using these I can put all the whiskies I have
tried on a 2d dimensional plane like this.
You might see now, that we could draw a line
between the whiskies I like and the ones I
don’t like to separate them into two groups
like this.
And this is basically what one neuron does.
Let me show you…
Our neuron has two inputs R and S, weights
for each of these, a bias, a sum of these
inputs, and an activation function.
Looking at the sum inside the activation function
f you might recognize this as the standard
form of a line in a 2-dimensional space.
If not, here is a little recap for you:
Most of us know that you can describe a straight
line in 2d-space using the slope-intercept
equation: y = mx + b
In which m is the slope and b is the y-axis
intercept.
Our 2d-plane doesn’t use x,y as axis but
richness r and smokiness s.
So: s = mr + b
Our drawn line intercepts the smokiness-axis
at around 0.25 and its slope is approximately
0.45 per richness.
Putting these values in we get: s = 0.45r
+ 0.25
But this doesn’t look like our sum inside
the activation function.
That’s because we have to go from slope-intercept
to standard form.
First, we subtract 0.45r and after that, we
subtract 0.25.
Giving us s - 0.45r - 0.25.
We now have the same straight line described
in standard form.
If we map the first half of this equation
to our sum inside of our neuron, we can see
that w1 has to be equal to 1 and that w2 is
equal to -0.45.
The bias, which takes the role of our smokiness
axis intercept, is -0.25.
If I were to put the richness and smokiness
of a whisky I like, Glenmorangie for example,
into this equation the result is negative,
but if I put in the values for Talisker that
I’m not really a fan of, into this equation,
the result is positive.
And this is actually true for all the whiskies.
So this function categorized them into two
categories, giving all the whisky above the
line a positive value and all the whisky below
the line a negative one.
The fact, that the values for whiskies I like
are negative numbers, and the ones I dislike
have positive values, doesn’t really matter.
The only thing that matters, is that the sign
of the values in these two categories differ.
And you might have realized by now, the bias
of a neuron is the axis-intercept of its straight
line.
Without it, we couldn’t move away from the
origin of the value space, which would really
limit the ability of our neurons to separate
the value space into categories.
We have now calculated the activation value
of our neuron for each of the whiskies.
The activation value of a neuron is the weighted
sum of its inputs and the bias.
And this is what's handed over to the activation
function.
The activation function is what models the
activation threshold in a biological neuron.
It basically tells the neuron when to when
and how to output information to the rest
of the net or the underlying system.
But there is more: The activation function
shapes the output of the neuron to be inside
a certain range.
Basically, with the choice of our activation
function, we decide how we want this neuron
to pass on the decision it has made to the
next neuron in line.
This is important because we sometimes don’t
want to pass on how strong the decision is
a neuron has made, but only the decision itself.
So there are a lot of different activation
functions used in neural networks.
Here are some examples for you to look at.
This is just a subset of commonly used activation
functions, there are a lot more.
The choice of your activation functions within
your neural net depends on multiple factors.
It depends on the problem you are trying to
solve, the structure of your net, and also
the training algorithm you want to use.
But I am getting ahead of myself.
In our simple example, we only have one neuron
and I will use the step function here.
The step function returns a one for all positive
inputs and a zero for all the rest.
Now we can put everything together in our
neuron, which results in the finished equation
like this.
As you can see the two incoming inputs have
the weights 1 and -0.45 and the bias is -0.25.
All of this now goes through the step function,
so we will get a 0 for all the whiskies I
like and a 1 for all the whiskies I better
not try.
Let’s put the Glenmorangie and the Talisker
through this neuron now and see if we get
the expected output values.
Perfect.
By now you should hopefully understand how
one neuron makes its decisions and how the
math behind it actually works.
Let me know in the comments below what your
favorite whisky is or if you have any questions
up until now whatsoever.
One word on the number of input parameters.
You might think that this works only for two
input parameters, but I can assure you all
of the math above applies for 3 or more input
parameters as well.
Instead of separating two classes of data
by a straight line, you can separate them
using a plane in a 3-dimensional-input value
space.
Or with higher dimensionality, it works using
a hyper-plane, but all of that is rather hard
to draw, so I go with the easy path of just
2 dimensions in this video.
Before we move on to actually build a net
of neurons, let’s put this neuron into some
python code first.
As always I start with defining some data
types.
Yes, there is a type system in Python.
If you want to know more about why you would
use a type system and how to use it, I have
a couple of videos about this topic on my
channel as well.
The first type I define is a vector.
A vector is nothing more than a list of floating-point
values.
We will program a general implementation of
a neuron here and we will use vectors to hand
in the input values and the weights to our
neuron.
Next, there is a type for an activation function.
It is defined as being a callable with one
floating-point input parameter and a return
value of type float.
That’s all the types we need right now.
Let’s move on to implement the neuron.
The neuron is implemented as a function.
It takes a vector of inputs and a vector of
weights.
The last parameter defines the activation
function we want to use with our neuron.
Before I go over the lines of code, here is
a short reminder of what the zip function
in python does.
Python’s zip function takes an indefinite
number of lists as input.
And joins all the values together in one list.
Each item in the resulting list is a tuple
of the values in the incoming lists at the
same index.
The resulting list has the same length as
the shortest incoming list.
Elements from longer lists are discarded.
Alright, back to our neuron.
The neuron function is implemented with a
couple of nested function calls.
Starting from the inside, the first thing
we need to do is zip the list of inputs and
the list of weights.
But wait, there is also a one attached at
the beginning of the inputs, why?
This is a little trick that is used to store
the bias of a neuron as the first element
of the weights.
So what we basically do is to think of the
bias as an extra input to the neuron with
a weight of one.
After we have zipped the two lists, we can
now iterate over each tuple inside of the
zipped list and multiply its values.
This results in a new list in which we store
the multiplication of the bias with one, the
multiplication of the first input value with
the first input weight, the second input with
the second weight, and so on
In order to get our sum, we sum up all the
values in the list of the multiplication results.
This results in our activation value of the
neuron which we then hand over to the activation
function.
That's it.
We can now return the output of our neuron.
Next, we need our activation function.
We settled for the step function and this
is how we implement it.
It simply returns 1 if the incoming value
x is greater than 0 otherwise we return 0.
And now we can put everything together.
First, we define our weights.
The first value in this list is the bias of
our neuron, followed by the weight for the
incoming smokiness, and followed by the weight
for the incoming richness of a whisky.
Now we define some input values for the Glenmorangie
and the Talisker.
If we call our neuron function now with the
weights and inputs for the two whiskies we
should get a 0 for the Glenmorangie and a
1 for the Talisker.
There you go…
So why and when do we need a network of neurons?
What we have done until now hardly counts
as a neural network because there was no network
of neurons.
And you might ask yourself, why would we even
need more neurons?
The whisky prediction works quite well, doesn’t
it?
Let’s talk about the limitations of one
neuron and how a neural net can overcome them.
I have prepared another example to give you
a feeling for why a neural network actually
exists in the first place.
If you are watching this channel, you are
most likely a little familiar with programming
and you most certainly have encountered the
OR function.
The OR function takes in two binary values
of 0 and 1 and outputs a 1 if one of the values
is a 1.
If we put this data on a 2d plane again we
can draw a line, and calculate our weights
and bias for our neuron.
If we now inject the values for A and B into
our neuron, we get the expected values back.
The same works for AND and Not-AND.
Let’s have a look at Exclusive-OR now.
The Exclusive-OR function only returns 1 if
both input values are not the same.
After putting this on a 2d-plane we have a
problem.
There is no way of separating the red from
the green dots by a straight line.
This means: A single neuron is not capable
of modeling this function.
Only if our dataset can be separated by a
straight line we can use a single neuron.
But we can use a network of neurons to model
the XOR-Function.
Specifically, we can combine an AND neuron,
an OR neuron together with a Not-AND neuron
to get the desired output.
In electronics, an XOR circuit can be constructed
by combining a NAND, OR, and AND gate.
Likewise, we can put our three neurons together
to create a neural net, that is able to calculate
the XOR function.
We can build a two-layer neural net, using
our NAND and OR neuron as the first layer,
feeding their output into the AND neuron that
will generate the output for the entire net.
Let’s try it out for some values.
For the input zero-zero the OR neuron returns
a zero which leads the AND neuron to output
a zero for the entire net.
For the input one-one, on the other hand,
the NAND neuron returns zero and for that,
the output of the net equals zero as well.
For the inputs zero-one and one-zero, both
OR and NAND neuron return a one which results
in a one for the result of the net after the
AND neuron.
This is how a neural net can learn more than
its single neurons and how it is able to model
very complex functions.
You also saw here how layers of neurons output
their decisions to the next layer.
Now it’s time to put all of this together
in python.
Starting with our neuron and step function
from the whisky neuron, the first thing we
need to do is to define the weights for our
NAND, OR and AND neurons.
After that, we can define our xor net.
This function only takes a vector of inputs.
Inside we have a nested function call, calling
first the NAND and OR neuron with the given
inputs and using the returned values as input
for our AND neuron.
The result of the last neuron call is the
result of the entire xor net which is returned.
In order to test our net, I define a list
of inputs I want to hand over to the net.
I then iterate over the inputs and call the
net with each of them.
In order to see something, I print the input
values followed by the result of the neural
net call.
Perfect, our neural net calculated the xor
function correctly.
Congratulations, you just programmed your
first neural net in Python.
But up until now, we have just put together
some neurons manually and we found the values
for our weights and biases using basic math.
But how can a neural net learn from arbitrary
data?
This is called training and it will be the
topic of another video in this series.
We will also look at how neural nets can make
decisions on other types of data.
How we can for instance hand in a picture
of a handwritten number and get the numerical
value of this number itself.
All of this and more will come up very soon.
So if you don’t want to miss it, please
consider subscribing to the channel and hit
the like button to help the youtube algorithm
learn that you like this kind of video.
I hope that you have a basic understanding
of the inner workings of a neural net by now.
If you have any questions whatsoever, please
don’t hesitate to comment below or join
the discord server to directly interact with
other coders and me.
Until we progress on our journey deeper down
the rabbit hole of neural net machine learning,
here are some videos for you to enjoy.
I see you next time.
Have a lot of fun CODERS!