okay hello everybody and welcome I'm Valentina 
it'so marketing content manager at warlift and  
today I'm with Max gerachi SEO expert we will talk 
about semantic publishing identities Gap analysis  
and how to use the application Max developed that 
is called the entities as with if and were lifted  
together if you have any question you can write in 
the chart and um so I will start the max welcome  
first of all and thank you and uh so please uh 
introduce yourself and start the presentation  
thank you so hello my name is Max gerachi I'm 
the co-founder of Makoto Studio A Little Agency  
hello my name is bologna and Northern Italy 
and specializing in content marketing and SEO  
I've been doing academic research for at least 
10 years in the sociology of Technology field  
so my usual tools of research were semiotics 
knowledge representation computational Linguistics  
and these tools came in very handy in the last 
few years in my daily job as an sco since the  
use of natural language processing have become 
probably easily applied to the online search  
as a result the so-called entity-based SEO has 
become increasingly relevant this patient gave  
me the opportunity during the pandemic thanks to 
the pandemic even it sound a bit strange to say  
but it gave me the opportunity to study python 
I started to use it just for scraping purposes  
but now I use for SEO tasks Automation and above 
all with machine learning and linguistic models  
so we'll see we'll see an application I've 
developed as Valentino just told so today I'm  
going to the whole about NTD SEO knowledge graphs 
and how to create one Knowledge Graph for your  
site through structured data let's start with the 
slides and to the SEO is an advanced approach to  
SEO concerning uh on-page optimization and even 
structuring the site so taxonomies and internal  
linking following the semantic evolution of search 
engines from lexical to semantic certain genes  
NTD SEO start to consider it's not the keywords 
which are strings of characters but entities that  
constitute the page topic the article introducing 
the knowledge graph things not strings which was  
published uh by Google in its official blog 
in 2012 is uh the The Watershed uh marking  
the bird of entity SEO the strings in the title 
are sequences of characters as I told and we call  
them keywords in SEO to understand and maybe also 
try to simplify what we call things is more or  
less a synonym for entity in general entities are 
conceptual objects that can be uniquely identified  
and they can be brands or places people 
things or more General and Abstract concept  
so for now let's say let's simply say that 
the knowledge graph is made of entities and  
their relationships but we'll come back to 
this definition to to a more formalized one  
it is easier to understand what an entity is by 
referring to topics in the term topics is what  
Google prefers to use in its communication to 
a broader audience uh whereas in the patents or  
other more technical documents they prefer to use 
the term entity to try to understand better what  
the knowledge graph is we have to introduce two 
concepts ontology and knowledge base because they  
are really uh interconnected they have a close 
relationship with the concept of Knowledge Graph  
and these are often overlapping and 
confusing Concepts and we have to  
say that they are confusing even in the 
academic research and the Enterprise the  
business related literature and consequently 
their dissemination so I think that tick can  
be really useful to try to agree upon some 
simple definitions so uh so we can better  
understand what the knowledge graph is and what 
it means to create a site-wide knowledge graph  
so uh one of the many definitions available uh 
between all the many definitions available I  
have chosen uh to present uh simplest one and two 
uh to offer two different definitions one is more  
formalized than the other one is more operative 
and business oriented I suggest some resources  
to to go deep in these subjects uh for sure the 
auto text and word lift logs are one of the best  
sources for this kind of information but also some 
academic uh some some academic text and researches
so let's start with the concept of the anontology 
ontologies are data models they are semantic data  
models that Define the types of things that 
exist in a knowledge domain and the properties  
that can be used to describe them so ontologies 
are generalized data models meaning that they  
only model General types of things that share 
certain properties but don't include information  
about specific individuals in our domain there are 
three main components of an ontology the classes  
so the distal types of thing that exist in the 
data on in our knowledge domain the relationships  
which are specific properties that connect classes 
and the attributes the properties that describes  
the individual class so as you can see this 
resembles what we have in structural data uh  
if we refer to schema.org vocabulary we 
can say that in fact it is an ontology  
what is the relationship between an ontology 
and the knowledge graph ontologies are the  
general data models that serve to create a formal 
representation of entities in a Knowledge Graph so  
they constitutes its backbone a knowledge graph is 
created when we apply an ontology the generalized  
data model to a set of individual data points 
solid Knowledge Graph is an instance or a sort  
of instantiation of an ontology in other words we 
can say that tontology Plus data data stored in a  
knowledge base plus a graph visualization 
a graph database equals Knowledge Graph  
let us come to the concept of knowledge base 
so what is a knowledge base uh the phrase  
knowledge base was was popularized in the 70s 
in the context of the rule-based expert systems  
in their seminal book about about artificial 
intelligence uh norving at Russells Define a  
knowledge base as a set of sentences or facts 
expressed in prepositional logic propositional  
logic is a is a part of logic that relies on the 
opportunity to to derive new Knowledge from the  
previous contained knowledge in our sentences so 
this is the very crucial point for the knowledge  
basis and the knowledge graph themselves as we 
will see the point is that we can use inference  
techniques uh called knowledge reasoning to derive 
new Knowledge from the knowledge base itself  
so we can view a Knowledge Graph as a graph 
database we call it even a rdf triple store  
I can will be back on this Zone which visualize 
a knowledge base so a knowledge graph is a graph  
structured knowledge base what's already have 
rdf is the resource description framework it's  
a standard proposed by Vu 3C and describing 
uh how to exchange reuse structured data to  
enable semantic interoperability among different 
applications that share information on the web  
each triple consists of three parts one part is 
the node which represents the subject The Edge or  
Arch is also called this way is the predicate 
that goes from the subject to the second node  
which is the object so we can refer to a triple 
represent it like subject to predicate to object  
but there are also other ways to look at this 
definition so we can tell uh entity attribute  
value is another way to to define a triple uh if 
we think to triples uh that we use in structured  
data the form is this one entity attribute 
and its value or even entity relation entity  
uh these last one emphasizes the interconnection 
between entities and how it can be part of the  
description of other entities as a its attribute 
thus forming a very dense semantic Network  
so here we have uh our new definition of 
Knowledge Graph uh that is I I find this  
straightforward uh and formalized at the 
same time and knowledge graph is a visual  
representation a graph database 
of an instantiation of an ontology  
so it's a graph representation made by nodes and 
edges and organize it into triples following the  
the rdf standard notes are entities and edges 
are the relationships between the entities
just just as with knowledge base even in the 
knowledge graphs we have the same problem and  
task of inference or knowledge reasoning to 
derive new knowledge and these task is called  
graph completion graph completion uh what is 
graph completion well knowledge graphs nowadays  
knowledge graphs are fueled by Machine learning 
and they use knowledge natural language processing  
and NLP to construct a comprehensive view of 
nodes and edges through semantic enrichment  
knowledge graphs completion is the inferring new 
age is the inference of new edges entities things  
facts based on the already existing relational 
data graph completion is divided in some other  
subtasks like entity prediction relationship 
prediction triple classification Google has  
filed several patents many of them commented by 
the late bislowski or now a knowledge graph a  
modern Knowledge Graph or the Google Knowledge 
Graph can update itself through the collection  
of new data on the internet and the use of what 
they call in a patent a reasoning engine this  
reasoning engine is exactly what we have called 
knowledge reasoning or or inference technique  
but on a Knowledge Graph like the Google one this 
inference technique are filled by Machine learning  
in terms of Enterprise knowledge graphs uh 
we have a very important reference which is  
which is uh sorry ah yes this one uh we have an 
important paper called industry scale knowledge  
graphs lessons and challenges and this paper 
is participated by big Enterprises which are  
leaders in the knowledge graph projects like 
eBay Amazon Facebook IBM Google itself Microsoft  
and so this definition in some way represent a 
sort of agreement that sort of consensus among  
a more operational definition there the 
knowledge graph is called is defined as  
um as a description of object description 
storage of objects of interest and their  
connections and Knowledge Graph provide 
like the layer of shared knowledge within an  
organization uh allowing documents products 
applications to use the same ontologer the  
same vocabulary and reuse definitions 
and descriptions that other creates
thank you yes knowledge graphs represented 
in standardized and interoperable rdf triples  
provide a great frame for framework for data 
integration unification linking and reuse  
so the purpose for creating a knowledge graph for 
our site is exactly these to have the opportunity  
to integrate data to unify them Interlink and 
also reuse a knowledge graph is in this sense  
a real asset uh for for our companies or sites 
through which the information conveyed by one of  
our sites is immediately accessible to search 
engines including the internal search engines  
and also intelligent agents the accent on 
the intelligence agents was put by certain  
Partners Lee in his first definition of 
this semantic web it was abandoned in  
in a more recent years but now that we have uh 
that we have machine learning based application  
like conversational agents or suggesting 
engines for related contents in a blog or  
for a product feed and an e-commerce these 
concept of uh being immediately readable not  
only for search engines but even for intelligent 
agents as really start to be to be a real thing  
so again the main benefits of creating a 
knowledge graph for for your site is the  
Improvement findability uh both externally and 
internally um also the opportunity to to have uh  
a new opportunity to Cluster or group and reuse 
the contents uh for example the word lift plugin  
um uh as some widgets that is possible to use to 
relate uh contents and to suggest other contents  
to user using uh relying on their semantic 
closeness so improve findability greater content  
grouping and improved SEO in general because 
we have a lot of tests uh some of them made by  
by word lift itself but not only a tasks that come 
validate the idea that that using structured data  
and structuring a site like Knowledge Graph 
improve SEO for sure it improves it improves  
the CTR because as you know we can win some 
special spaces on the syrup called Rich results  
now uh we we should be clear about 
what entities and knowledge graphs are  
and the normal benefits of generating one for 
one side we can describe and see in practice  
the activities that will allow us to to do so 
to create and publish a Knowledge Graph I'm  
referring to structured data implementation and 
the activity called semantic semantic publishing  
semantic publishing is the activity of publishing 
a page on the Internet by adding a semantic layer  
uh uh or we can tell that it's an activity of 
semantic enrichment semantic layer is a layer  
constituted by structured data that are 
immediately readable by machines so it's  
uh it helps search engines but also voice 
assistance and other intelligence agents  
as I told to understand the page meaning the 
context the structure the relation with other  
entities both internal or external to the site 
and making so information retrieval and data in  
progression much more efficient semantic 
publishing relies on adopting structured  
data and linking the entities covered in 
our document for present in our document  
to the same entities in various public databases 
we'll see practically practically how to do this  
um what are structural data I'm sure that you know 
but just to repeat in brief which are some of the  
advantage of adopting structural data structural 
data is metadata added to the HTML code and they  
make explicit and understandable to the machines 
the structure of the page its various components  
or we can say the discrete units of content 
present on the page for example video or List item  
a list of Articles a field of products or even 
an accordion will if I choose we can call each of  
these content present on the page a discrete unit 
of content and we can map these discrete units of  
content to our structured data this activity is 
called topic content modeling but structured data  
even allows to to make explicit the relationships 
among these various discrete unit of page and to  
assign them a different order or of importance 
and also just show to Google and other search  
engines the relations between these these pieces 
of content units of content on our page uh and uh  
and structural data also uh show the topic covert 
of the topic and its subtopics the entities that  
contribute to the Fine to unfold to unfold a topic 
structured data can be expressed with different  
vocabularies but the standard is the vocabulary 
the ontology created by schema.org a Consortium of  
made of search engines search engines the biggest 
one that created these widely used standards
yes as as I told the marking of the discrete 
units of contents let's take them as logs  
um it's called content 
modeling and can be uh usefully  
carried out during the defined phase nowadays uh 
we we tend in the in the design field there is a  
strong tendency to use to use blocks for design 
and these blocks can constitute this design  
blocks can constitute the the script content unit 
that we have so we can model our content on them  
uh the content model does the file can be 
uh related to a map of topics that we cover  
or that we will cover on our website and 
these activity is called topic modeling and  
it's useful to understand which topics are 
still covered or not on our on our site and  
the structured data help to make these topics 
subtopics entity explicit to the search engines
entity lynching as I've told is is the process 
of of linking identifying entities on a document  
and then connecting them linking them to entities 
present in a knowledge base in a public knowledge  
base this anti-linking activity on entity 
annotation is called wikification when the  
entities in the document are marked to entities 
present in the Wikimedia Foundation resources like  
Wikipedia or wikidata uh the schema vocabulary 
properties used for semantic publishing and that  
creates a bridge between structured data and 
density SEO are the about dimensions and the  
same as properties these properties are powerful 
but they are also underutilized by seos and it's  
partly because the structured data management 
plugins accept we really have some exception  
a word lift is best one in my in my personal 
opinion uh but generally plugins like yoast or  
ranked mat uh they automate the the structural 
data creation but they they don't allow so much  
customization and what we particularly need the 
about dimension in this Ms property to express  
the entities in our structured data uh we also 
have an important I'll show this an important  
property called the ID but we'll talk about it 
during the Practical presentation so that that's  
uh that's that's it for the introduction I I stop 
the presentation and I just go to my application  
and then do some let's start with the with with uh 
aside this is a web page from a client and I show  
these just to cite another important property 
because I see that many accounts on a typical  
mistake that people make is to publish only a few 
schemas especially when they author the structured  
data manually and they publish them as separated 
schemas so as as Highlands without interconnecting  
them if we analyze the structured data on these 
on these page just with the official validator  
we can see first of all that the schema let's 
do not consider the the body it's uh it's really  
articulated and long but if we look at its 
visual representation here I'm using a free  
tool called classyschema.org it's very useful 
to to see these these interconnections between  
between our different schemas so as you can see 
the schemas are not separated as Islands but they  
form the shape a dance semantic Network which 
tell a complex story to the search engine so  
for example we have a medical web page because 
the site is about kids health and so I use the  
medical web page property that I used to say the 
through two different two different type medical  
web page and even article because medical web 
page and this is called a double type schema or  
multiple type schema and it's useful because 
it allows to use properties from both of the  
user to skiva so our medical web page schema as a 
property called reviewed by which is not present  
for the article it is very important to establish 
the EIT of the site because every single article  
is both written by real physicians and reviewed 
by a very important physician so it's a signal  
that I want to be in immediately available to 
Google let's consider that this site even it  
was written by a physician was hit very hard by 
the very first knowledge update and it was after  
few months that I've integrated structured data 
through wordlift in this case but I'll show them  
the integration because it's an interesting one 
I'll show it later when after few months of the  
publication of this kind of structured data the 
traffic came back at the next update and now it's  
more or less at same levels as before the first 
the first Medical Update in this representation  
as you see we have the nodes which are the schema 
properties for example the fiq schema and the null  
the edges are the the properties so an fiq page 
this is node is part of this is the property of  
the medical web page the medical web page sorry 
this is another node so we have a table here  
um as you can see these these uh Pages uh these uh 
schema types are interconnected by by using their  
ID the ID is the unique machine identifier of uh 
the node and we can use it to create uh schemas  
as Snippets or breaks to uh to create such big 
and complex representation I've introduced these  
because my application relies on the IDS in order 
to merge your actual structure data to the ones  
generated by by the application so uh let's let's 
go with with this uh I I I just share it I try to  
reload it but type yes it's working but I have 
a local uh a local installation tool to show it  
just one second of patient this application 
uh is part of a project I'm realizing with  
Israel Goddard who is who is a smart Canadian 
marketer and it's a python application with a  
simple Streamlight interface extremely became very 
popular amongst the seos who use uh python to to  
create application for uh automatize for SEO tasks 
automation or for or even for machine learning  
and natural language processing applications and 
streamlit provides it's a library a python Library  
which provides a very simple user interface 
a very simple way to generate a user a user  
interface so uh let's let's do some live examples 
with day we take notice one we take this URL  
and as you can see we have two different 
options here the URL analysis or deserve  
analysis and the entity Gap analysis let's 
start with the URL we can check entities  
from a single URL or for a provided text if 
we have written an article but we haven't  
published it yet or URL versus URL let's start 
with the simplest one I I copy paste the URL  
and also select to extract all the IDS from the 
schema that is still present on your web page  
and I can also decide if I want to extract only 
the entities present on the tag title or in the  
meta descript and in The Meta description or 
even the entities present in all the ages H1 the  
headlines of the of the text but I'll go just 
with the with the ID uh maybe I prefer to run  
it it locally it's simplest and so let's use the 
same URL and I can extract as I stole the the ID  
and I go with the entities extraction it 
takes just few seconds because there are  
there are but maybe it finished here I can 
also extract categories and topics sorry for  
me going back and forth but this is a beta if 
you want to reach it the the application it's  
its URL is entitieschecker.com but as you can 
see I'm still fixing some some slightly issues  
so I submit I extract the ID using a python 
Library called extract and then I extract the  
topics and categories and categories too I I just 
see which is the fastest we have to just wait a  
few seconds here because in this version we are 
also scraping all the entities descriptions from  
from Wikipedia which can be very useful when 
we inject them in our in our structured data  
so the submit button is not working here you 
you have to have just few seconds of patience  
uh so uh we'll see the the best way to uh to take 
this portion of code created by the application  
and directly inject it in our current in our 
current structured data so it's quite finish it  
and then we can select the type of properties 
that we want to inject the entities into as  
I've told you the domain properties for entity 
injection and entity annotation are the about and  
the Machine property let's say something about 
how to correctly use these about to mentioned  
properties uh the about should refer to one or 
two entities at most because it represents the  
main topic of your uh page and it should be also 
present in the H1 title so if we optimize our  
article doing SEO entity-based SEO optimization 
we have to use this about property for just one  
of two entities Dimensions properties should be no 
more than three to five depending on the Articles  
length generally an entity or a subtopic because 
we've seen that an entity can be considered like  
all the the the subtopics that together contribute 
to create to unfold our main topic so Dimensions  
properties should be three to five and they should 
be this is an important Point explicitly mentioned  
in the markup if and only if a paragraph or a 
sufficiently significant portion of the document  
is about is devoted to that entity or just for 
this ambiguation purposes so uh we have as we  
can see many many entities on on a page real 
many entities of page on a page but we have  
to inject just three to five in our uh Mansion 
property and that they are the most prominent  
one or we can look at here the most Salient one 
the one which is the the the biggest silent score  
so we can check our entities they are ordered by 
salience so for sure the first one is our is the  
one to be injected in our about property and 
we also can choose some entities for the for  
the mentions properties uh when we choose them 
this button appears to download the entities  
foreign and if we open it we have this snippet 
of code the important part here is that we have  
extracted all the IDS present on your page 
and we have to choose the proper one the  
proper schema type where we want to inject 
our entities so in our case it's the article  
or it can be the medical web page but sure it's 
not the person or the parent audience as you can  
see I like to create very specific and little 
schemas and then connect them through the their  
IDs so I choose article and I download the 
The Entity and it's it's uh did is correctly  
one the one that we should use to inject it 
in the proper way for the entity injection  
we just use I'll show you a backhand the back 
end of my one of my personal sites uh let's see  
uh just dependency side because it has a simple 
plug-in to just copy paste to just copy paste your
your code from here let's say this one and just 
you can just inject these through this plugin and  
the plugin will be uh available for free from the 
application site and it's a simple code injector  
it injects code Snippets codes in the header 
and that's it so paste the Json generated by  
the entities with knife we put here the the the 
decode as you can see uh it creates structured  
data following this this format this is the about 
property this is injected in the schema with this  
ID which is the article schema and it has a name 
which is the name of of the The Entity and it's  
a type and we also ask the description and the 
linking and linking to to wikidata and Wikipedia  
uh I I use these uh this application to inject 
entities and the advantage is that it allows  
to do so uh even for very small sites uh when 
we cannot go to uh to bigger applications or  
more complex applications like word lift itself 
is I show you the back end of this article as  
I've told you I'm using word list on this site 
and I also want to show an important specific  
feature that that word lift as and that is not 
present in the application we go to the word lift  
plugin word lift essentially does the same thing 
of annotating entities and injecting them in the  
structured data the interface is immediate because 
you can it immediately extracts the entities from  
your text I I mean it does it live in real time 
and you can choose the one you want to to inject  
in structure data but sometimes it happens that 
the about the about property is not exactly what  
I want or maybe it extracts one property for for 
the about property and the quantity and I prefer  
to have two of them and so I use my application 
to to to to inject the other entity but the  
very cool thing here is that part of of word 
lift which has another same as property as we  
see in the same as is a connection property but 
this same as is not as same as the same same as  
that we use in in schema markup this is a an 
old property and this is to interconnect our  
entities our page to uh to publish our personal 
entities in the lld it's the link and open data  
cloud and it's an enormous network of public 
databases where the contents the entities the  
knowledge is published following the so-called 
five stars standard created by Partners Lee and  
one of the characteristics is the possibility to 
identify each entity through a unique identifier  
it's you it's the URI or the URL but URI is 
without the protocol The Exchange protocol and  
and word lift as these unique I really think 
that it's Unique on the market possibility to  
to create your own entities and to publish them 
on the linked open data data cloud so the last  
thing I want to to show you uh well as I told you 
extracting entities from text is quite the same  
that from URL it's when you don't have a published 
a text so you have to Simply copy paste it here  
or you can or you can do an URL versus URL so for 
example let's take these let's take these URL here
and let's also take a second urls
uh uh well uh again I I I it should be it is a 
long operation because it has to extract from  
two URLs so sorry because I'm running it locally 
and I don't want to waste your time for this but  
it simply compares the two URLs and show on 
a table which are the unique entities in your  
URL and the unique entities in for example your 
main competitor so you can see if you have to  
use some other URL some other entities on your 
pages but let's go to the serp analysis this  
is the other the other important feature that I 
want to show you uh I I recently seen that it's  
also possible to do this with uh the word lift 
add-on for for Google sheet but the add-on uses  
the word lift your personal word lift keywords 
so uh you can use these as an alternative and  
they also have different purposes and way of 
visualizing the results so let's just search  
for a query uh semantic SEO antique Co and we do 
our search in the United States and in English
sorry sorry uh yes I have to search these 
other yes uh because in the final version  
of the application we rely on different apis so 
we are just we're just checking uh which are the  
performances of of some of them this is mainly a 
test ambient for now so this is the the the serp  
in real time we scrap it with with scraping API 
so we have the first 20 results we can wordlift  
is here we can choose uh let's say Five Five 
results and then we have to change we have to  
choose this number and this is the number of the 
minimum pages that have to share an entity so for  
example I want to see all the entities shared 
by at least three of the pages present here
I extract them and I immediately can see only 
the shared entities by three pages and if you  
increase this number you only see the more 
important entities because we we consider  
that they are present on every single page or 
we can just go and see what is present on all  
the serp so we can select 10 results and say 
to show all the entities present on the serp  
well uh I I I I stopped now so maybe there is I I 
yes it's it's a bit long we can have space for for  
uh questions if you want okay thank you Max so we 
have a question Michaela um asks do you recommend  
using entities as anchor for internal links yes 
yes this is a very interesting uh question and uh  
me and other guys in some groups are doing tests 
uh on this subject on using uh entities as uh  
our uh internal link anchors uh uh from what I've 
seen till now I I absolutely recommend to do these  
but you can use even uh sorts of variation so you 
can include your entity in the anchor uh but also  
using its surrounding text as a part of the anchor 
to create a differentiation between your anchors  
profile but for sure the structuring of the site 
trying to follow the same kind of structure that  
we have in our knowledge base and in our knowledge 
representation so in our knowledge graph is for  
sure uh for for what I've seen till now a way to 
improve our uh internal links I'm doing a lot of  
tests of uh normal keyword based anchors against 
versus the entities-based anchors and I'm going  
with the entities right now so yes it can be it 
can be a proper way to do your internal linking  
okay and I have an author question so um 
I saw that in the application you show us  
before there was a property called called knows 
about yes yes the knows about yes so as we have  
seen we use the the I can put an URL here yes 
but we can use the about 10 mentions properties  
to define the topic and the subtopics covered 
on our document on our web page in our article  
I'm not referring to the article type the 
notes about property is another one that  
that we immediately can connect to external 
uh knowledge bases and it's used to define  
what a person or maybe we can use it even for 
organizations uh local businesses so what a person  
or an organization knows no this is uh for me for 
example on my on my web profile I I can inject  
the uh knows about uh I can use the notes about 
property to inject some specific entities like SEO  
because I effectively know about SEO or digital 
marketing or content marketing and you can do the  
same thing with an organization we can say that 
an organization knows about uh think it has uh is  
specialized on uh some some uh specific activities 
or Fields so they knows about properties another  
one it's less used obviously than mentions 
and about but it's very useful full on the  
writers or persons in general profiles or the 
the about page of an organization and so on yes
maybe I can stop sharing my screen so we have 
another question from Alessandro yes how do you  
select a focus how do you select anchor text 
based on entities what's the procedure or the  
logic you follow yes okay uh as I've told you I 
first of all extract all the entities from a page  
and uh when I do the topic modeling part of my of 
my site so when I analyze or create the topical  
map before starting to create the site itself it 
is a procedure that we can do even if the site  
is still live once I create a topical map and I 
see how how the different topics are clusterized
what is their semantic closeness I also can 
see if they are still interlinked or not the  
topical map allows me to to see if some articles 
are closed to each other on the on the semantic  
point of view if I don't have an internal link 
between these two pages then I I choose to enter  
them and in that case I I look at the entities 
of the page that has to link the other one so  
the source page and I choose an entity 
which is which has a a big salience for  
the page because I want to Interlink the main 
topics or subtopics so again the same entities  
that I've chosen for the about and mentions 
properties can be the one used to form the  
anchor as as a partial match or just creating uh 
a contextual sentence to to create a more a more  
surrounding meaning context that go Google 
or the other search engines can understand  
okay so they are essentially the main uh the main 
entities of the page that I use as anchors that's  
internally encounters okay so we have another 
question from Antonio and he said I see that  
in the latest version of the app there is an 
internal linking entry hi everybody recently  
could they ask you what is your approach yes I I 
will not show it here because for now it's it's  
embedded and it's uh relying on an EV machine 
learning model linguistic uh model and so it's  
a bit slow but we have many different approaches 
I won't tell what I am exactly using now but I  
can present some some of the approaches that we 
can use for qualifying pages to Interlink them  
first of all the first qualification is uh based 
on crawling the data so we can divide the pages  
into main buckets and they are the strong and the 
weak Pages or the pages that has uh uh an internal  
page rank which is high or low level and this is 
the first qualification but when we choose to uh  
Empower a weak page we need a suggestion of uh 
the best candidates from the strong Pages bucket  
to determine which are the uh more semantically 
uh close pages the more uh related Pages we can  
use language models for example uh bird bird 
itself just because it's a it's an open source  
model so it's free you can use it but you could 
use even paid models like gpt3 but you can use  
Bert to to do a task called semantic similarity 
check and it checks the similarity between the  
pages maybe just looking at their title and meta 
description and from these check you know which  
are the more uh semantically related Pages 
or you can also use the entities profile you  
can extract your entities and just choose 
to use the first five ten or whatever and  
and do a semantic check between the two profiles 
or we can use an entity profile from a page and  
match the entities against the ranked queries 
that we retrieve from search console or we can  
again import the rank and keyword from a paid tool 
like semrush or hrefs and then try to match that  
keyword to the text to the whole text the body 
of the article so we have different techniques  
the easiest one maybe not considering the string 
matching that it's quite not reliable at all even  
if most many or I'd say all the commercial 
tools that I've seen using a sample a simple  
um string matching we can at least use a fuzzy 
matching a more advanced than a kind of matching  
which considered the plural singular so that clean 
the text a bit and consider some synonyms and in  
this way we have a more precise suggestion we can 
use semantic and we can use fuzzy matching even in  
Excel or Google sheet so if you do this manually 
you can you can extract your ranked keywords from  
a source page and then from a Target page and 
try to fuzzy match the keywords against the other  
keywords or against the title or the headlines in 
your article so this is the different approaches  
that we have I'm choosing to try an approach 
based on machine learning and a big linguistic  
model uh even if it only relies on gpus so try it 
at home only if you have a fast GPU and are you uh  
thing else that you are developing to with the 
python or with the language language models  
to yeah something maybe there is something just 
another thing that I can show uh sharing my screen
uh is the screen shared right yep okay we 
have these these link relevance tool this is  
an interesting thing I'm taking into consideration 
recently thanks to a very smart SEO called Bradley  
Banner he specialized in local SEO and he is done 
he has a link building uh agency service and he  
has done and shared a lot of tests regarding 
the topicality of a backlink or of maybe uh  
uh site where you want to host your your guest 
post so we tend to consider that only the page  
where we post our content should be relevant 
to the content of our linked page or site  
but a lot of tests really demonstrate clearly 
demonstrate that the topic of so the category  
the topical category uh of the domain where you 
uh publish your guest post or the domain which  
is linking to or from which you are buying 
links because well everyone buy links and  
if the domain is not aligned if it's topical 
categories is not aligned to your topical category  
the link the backlink is uh less effective but in 
an order of magnitude so one thing that we can do  
using using the Google natural language processing 
API is to use a function called classy text to  
classify the text so to to obtain the man the main 
category of a domain and so we can put our domain  
here and the list of our backlinks or of the 
domains proposed by a seller or from the domains  
we collected for our guest posting campaign 
and we can check if that page is and also the  
domains are on topic versus our personal domain 
I I shared the screen to show an example but I  
should open nature of some other tool to extract 
a back linked list but this is exactly the way it  
works Google provides one two three categories per 
page and per domain and we can do a simple match  
between the categories of the target page and 
the categories of the Target or domain versus our  
category the category of our of our domain so 
I think that these uh uh Bradley I think that  
calls these topical trust flow and it means that 
your task flow trust flow is famous metric used  
by a third-part tool it's Majestic so I think you 
are all familiar with it all the seos are familiar  
especially the ones working on link building but 
Majestic uses its own classification algorithm  
so I think that and we are having great results 
in picking up the right the right sites where to  
publish Our Guest posts is to use uh Google API 
to immediately see how Google Sees the topical  
category of the page and to have them as closer 
as possible yes I think that this is quite an  
interesting feature and I hope that people will 
will be happy with this again I repeated the  
domain but go from tomorrow because I have to 
fix as you have seen a few many things uh but  
the domain is entitieschecker.com it's free uh 
for using the entitygap analysis because you can  
select the entities shared by search results and 
also confront them with your page to have the Gap  
so you can know which are the missing entities on 
your page on the entities uniquely present on your  
page this feature is free too but you just have 
to subscribe for it just subscription because some  
some application has some bought visiting them and 
it continues to crash so I I prefer to use a sort  
of identification just to avoid just to avoid bot 
traffic but it's free thank you thank you so much  
Max for the time for your time for this webinar 
I thank you everybody to stay with us um uh I say  
um have a good night a good day and see you next 
time so thank you Max thank you all the guys from  
worldly five uh I have to disclose myself I have 
a personal passion I felt in love with word lift  
years ago so I can consider myself an ambassador 
because but it's it's it was particularly  
inspiring the concept and even some of the codes 
that Andrea cyber ended CEO from work shared I  
studied them during the pandemic and started to 
think how to uh to start from them to implementing  
my personal thing so thank you to for inviting 
me this this to be a one of your guests this  
particular really happy for me so in order for 
me thank you thank you see the next time bye bye