 After this, we will update the alpha term unlike AdaGrad there is one more term in this equation known as 'rho' this is used to avoid infinitely increasing alpha value.  As we saw in AdaGrad, the alpha value was increasing with each iteration but in the case of AdaDelta, the alpha first increased till 50 iterations and then continuously decreased so now there is no problem of learning rate decay. 
 So, in today’s class we will  talk about two more algorithms,   one of them is RMSProp and the other  one is Adam and we will also see a   very closely related algorithm which  is very closely related to RMSProp. 
 And we said   that there is a very closely related algorithm  very closely related to this RMSProp; so,   that closely related algorithm  is what is known as AdaDelta.  So, we have a closely related algorithm known as  AdaDelta. 
Very few people use it today, you should instead stick to: 
This algorithm is very similar to Adadelta, but performs better in my opinion. 
