Applying the CrossEntropyLoss on LogSoftmax reduces the effective range of the output of the model and it can be argued adversely affects the rate at which the model learns.
Because if you add a nn.LogSoftmax (or F.log_softmax) as the final layer of your model's output, you can easily get the probabilities using torch.exp(output), and in order to get cross-entropy loss, you can directly use nn.NLLLoss.
And, there is only one log (it's in nn.LogSoftmax).
nn.CrossEntropyLoss() combines nn.LogSoftmax() (log(softmax(x))) and nn.NLLLoss() in one single class.
In order to stabilize Logsoftmax, most implementations such as [Tensorflow (hyper-link)] and [Thenao (hyper-link)], use a trick which takes out the largest component max(x_i).
For logsoftmax, we begin with:
At forward time we have (with x = input vector, y = output vector, f = logsoftmax, i = i-th component):
The output of LogSoftmax is simply the log of the output of Softmax.
That means you can just call torch.exp(output_from_logsoftmax) to get the same values as from Softmax.
So, if I'm reading your question correctly, you would calculate LogSoftmax, and then feed that into NLLLoss and also exponentiate that to use in your filtering.
Since LogSoftMax preserves order, the largest logit will always correspond to the highest confidence.
Your above example with the LogSoftmax in fact only produces a single output value, which is a critical case for this example.
This explains why most of the examples you find online are performing the LogSoftmax() over dim=1, since this is the "in-distribution axis", and not the batch axis (which would be dim=0).
LogSoftmax is done across features dimension, you are doing it across batch.
m = nn.LogSoftmax(dim=1) # apply over features
m = nn.LogSoftmax(dim=0) # apply over batch
You shouldn't use torch.nn.LogSoftmax at all for this task.
m = torch.nn.LogSoftmax(dim=1)) you can either use positive dimension indexing starting with 0 for the first dimension, 1 for the second etc.
