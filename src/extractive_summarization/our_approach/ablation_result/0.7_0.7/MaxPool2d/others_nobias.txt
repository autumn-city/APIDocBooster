Pooling is of MUCH MORE IMPORTANCE in convnets. 
 But you see, max pooling used much more in the neural network than average pooling. 
But if you look carefully at what's going on you may notice that the after first convolutional layer the dimension of your data might severely increase if you don't do the tricks like pooling. 
And, by doing so, achieves one of the most interesting things about convnets; robustness to displacement, rotation or distortion of the input.  Invariance, if learnt, is located even if it appears in another location or with distortions. It also implies learning through increasing scale, discovering -again, hopefully- hierarchical patterns on different scales. And of course, and also necessary in convnets, pooling makes computation possible as number of layers grows.
