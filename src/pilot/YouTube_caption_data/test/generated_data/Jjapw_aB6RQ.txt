hello everybody so welcome to this third
video of this playlist and today what we
are going to do is to check out
dropout so in
programming frameworks or let's say in
python you might have something like
this
touch dot n n dot drop
out
with the probability p let's see for
this example it is 0.5
so dropout is a technique that is that
is used
in deep learning to prevent overfitting
and the way it works is that
it's zeros element of um of some
activations in some layers with a
probability
p which is this uh using samples from a
bernoulli distribution
so you might have a network
like this a simple network and
you want to apply drop out at this layer
and if the activations of this layer are
something like this
of dimension 3x2 uh
they are going to become some of the
values should be converted to non-zero
while others zero
and um depending on this probability but
before we do that
we need to calculate something called
scale factor
f which is 1 divided by 1 minus p
because it for this example it is 1
minus 0
1 sorry 1 minus 0 0.5
which is uh two so
if this was one uh it means that
there's a hundred percent chance that
any of these values
will be made to zero and if this was
zero there's a zero percent chance that
any of these values will be zero
so and this scale factor you know some
of the values will be zero so you need
to make up for that
so um that's why we have a scale factor
that is multiplied by the non-zero
values but
let's see how it is so these activations
will be converted to something like this
and again uh the dimension um
will be will be the same three by two
and um let's say this has been selected
to the first one has been selected to be
non-zero
we're not just going to copy the value
over multiplied by this scale factor so
it's going to become
one point four
seven five three
let's see this has been selected to be
zero
which is zero this one also zero
[Music]
this one has been selected to be
non-zero so it's going to be multiplied
by this scale factor it's going to
become three point
eight nine one nine this one also
minus two point six zero
four six and um this one has become zero
so one more important thing and
yeah these are the new values of the
activations
and um one important thing is that
the dropout is only applied during
training and not testing and um you up
use these values both in
forward probability propagation and
backward propagation
in fact i'm going to put in the link the
link
the paper that first implemented dropout
ah yes thanks for watching don't forget
to like and subscribe