recurrent neural networks wow that
sounds cool what is that don't worry
about it we will cover that along with
some introduction to it with some
examples and then we're going to do a
project where we're actually going to
use weather data to try and predict
the
temperature in the future
so don't expect some perfect results
because as far as i know there's been no
weather prediction that was 100 accurate
so far at least over the time maybe once
in a while it is but not all the time
so if you're new to this series this is
the machine learning with python course
it's a 15 part
series with
tutorials on each concept and projects
doing a neat concept and we are here in
the recurrent neural network or rnn so
let's jump to the jupyter notebook if
you don't know where that is there's a
link down below in the description you
can download all the resources available
all the course material it's going to be
amazing so see you in jupyter notebook
recurrent neural network rnn okay the
goal of this lesson is to understand
what is this recurrent neural network or
rnn i talk about all the time right and
then build an rnn on a time series
wow
feed forward neural network
so what we've done so far like in our
deep neural networks cnn and all that
kind of stuff it's been feed forward
what does that mean it means that the
information
only travels in one direction right so
so
uh
that's what it does so in a recurrent
neural network you actually have a
neural network that generates output
that feeds back into its own input so
you kind of have recurrent iterations of
the same data
so it can do
so what is the advantage of this rnn
well it can do one-to-many relationships
or many-to-many relationships and these
are actually not possible with feed
forward neural networks right when you
have the output dimension right like say
in the classification we did right we
had if we had two classes there were
only two optional outputs and that was
all it cannot say anything else on that
it's like the output is fixed
so
just wait a minute if it doesn't make
sense it will make sense just in a few
seconds but this can generate sentences
right okay so this makes sense right so
imagine you have a neural network that
needs to reply to you right so it needs
to come with an answer so sometimes the
answer is yes or no so that would be one
neural network right but sometimes you
need to say yes this is a case this is a
house this is a building or it has to
analyze an image for instance and you
say on the image i see three people
three two buildings a car
and a doberman dog okay
so the output would depend on how many
objects you identify or the machine
learning algorithm identifies so that is
a one-to-many relationship
and it's especially also good for time
series and we're going to explore that
so what is the time series for instance
when you look at the stock market you
have uh
the stock price going up and down that's
the time series also with the weather
data that weather data that we're going
to explore in our project it's going to
be amazing it's not the same weather
data as from the first lessons we had so
don't worry about that so
just a few examples of recurrent neural
network or rnn google translate right
it's a management relationship you you
type something in and then it translate
it and it's meant to many
voice recognition is also you know siri
or what's called on android i don't know
uh
recognizing your voice
and the video copyright violations also
uses there are many many examples so
some of the things we've been doing here
in our machine learning journey here
it's quite simple and some concepts are
easy to understand really really easy
even though it's very complex what's
happening below and some of them for
instance like the
reinforcement learning it is so simple
that you just implement it from scratch
it's so simple right but
rnn i would say it is not that simple i
mean you understand the high level
concept effort that data can feed back
into the loop
that's what you need to understand so
let's just look at this well
it is complex
rnn is complex
but luckily
it's not needed to understand it in
depth it's not needed so this is a quote
by one of the authors of keras which we
are using for this rnn well
let's just see what he says you don't
need to understand everything about
specific architecture of an lstm cell
which is the one we're going to use
just that lstm cells is meant to allow
past information to be re-injected at a
later time
so that's again it you need to
understand that okay you have passed
information and it is re-injected again
but you don't need to understand all the
in-depth theory and all that
forget about it if this is your first
thing
the power of it is still available to
you remember in the beginning i was
talking about right well you don't need
to be a car mechanics in order to drive
a car you just need to know there's an
engine and maybe you don't even know
need to know that but you just need to
know how to use a dashboard in the car
and that's basically it it's the same
with machine learning you don't need to
be the mechanic the one developing the
lstm
cells you don't need to know all that no
just leave that to the experts and just
figure out how to use it all you need to
know is data gets re-injected
so what are we gonna look at today
what are the libraries pandas numpy
matplotlib tensorflow
awesome and then we have a csv and then
we have something
new here min max scalar so
what does that do so basically it
transforms features by scaling each
feature to a given range and this is
what we need we need to make these
normalization transformations all the
time so that's what it does
and then we have some of the things we
know sequential dense dropout and then
the lstm this is the short or long short
term memory layer
by
invented by this guy here okay
so if you want to read a bit more about
it you can read about it here i don't
know how much it actually says here
uh used in guides so you can actually
get some guides here around it and
play along with that but let's start
here
and uh what the first thing you need to
do is import all that kind of
things we're using here that's a lot
right and then we're gonna start by
reading our data
if i could
not miss type it
and today the data is in pde
read csv and let me think if i can
remember
what it is that we are reading so it was
apple stock apple stock today and
since it's a time series
well i'm not really sure
if
i want to parse the dates
parse
dates
true and set no
index
call
zero okay so here we have the data
ahead
here we have it right
so we have parsed these as states here
and why do i do that is because i want
to transfer some for training and some
for
for non-training and the data set here
starts in 2000 and you can get your own
data set your own
stock data set if you want that instead
and the data sets ends in 2021
uh we're only going to use it until 2020
because these recordings and recordings
are done in 2020 and i want a full year
of data and
so what we need to do now is to have the
training data and the test data here and
uh
let's do that data train it's gonna be
all the data here from
location and we're going to take it from
2000
from the beginning
until 2019 and we're going to take it on
adjusted close
i'm actually not sure
if we needed the numpy
but let's just keep it in number data it
doesn't matter if we have it or not test
data
log
and then we take 2020 and i know there's
data in 2000 how is it called 21
adjusted
close but
but we're going to ignore that today
perfect so now we have the data train
as we see here
now the reason why i actually want it as
a number is it's easier to do the
indexing that it is than in a data frame
so here we have the data here and uh so
one problem here is that uh
we need to scale it right
so
that's the first thing we're going to do
it's like
we're going to use the min max scalar
and
so
we take the scalar
mean max scalar good and what it does it
basically scales the data data
train
scalar fit
our fit
transforms we both fit and transform and
take data
train but we need to reshape it so what
this reshape does it is instead of
having it in one single
line it will be
one
entry per
per in one entry per row
good and then we have data tests we use
the scalar again now it's fitted already
so now we can actually
we can actually just transform
oh i've made a space here transform
scalar
now it works
transform
data
test reshape minus one comma one there
perfect here so now we have it scaled
right so if we just look at the data
here uh let's just take it all so now
you see also you see here is like one
item on each row and the highest price
actually the last price it is one here
and if we look at
data
test we will see actually the price goes
above here
and goes down again and and above again
and
the reason why you don't
you don't what's called fit the data
into an entire data set is
basically you would tell the algorithm
something about the test data that you
didn't know at the training stage right
so if you fit it here on all the data
it actually would be scaled accordingly
to the test data so that would
implicitly tell something to the model
that because the training data would be
maybe in the interval from 0 to 0.8 it
would simply implicitly maybe get an
idea
that's kind of the thought right that
there's some data above that and you
don't want that in this case it actually
would be there
perfect so what have we done we have
normalized the data with a min max
scalar
and
let's go and
make the data preparation and this is a
bit more involved actually so what is
this we want we want to
take a history of data
and then we want to predict the next
price so we take the price for in a size
of 40
and then we take the label to be the
next
value so we want to predict the next
value okay
what do i mean we'll get there def
data
preparation maybe
let me take data here
and what does it do so we have the x
here
and we have the y here
to be lists we're going to transform
them afterwards to numpy ring so for i
in
range
range
and what do we range for so let's do the
window size of 40 and length of data
and what do we do we take x
and append
data
from
i
minus 40
to
i
right and
that's basically it we need to take the
correct
frame
why do we need to take this one
and uh
why
append data for i right
i'm actually a bit confused about this
one this one shouldn't be there
so
this is basically it and then we take
and say x is equal np array
for x
and y
y
is equal to np array for y
and
then we need to
[Music]
x is
x
reshape we need to reshape it
because this is why we need this point
zero in order to keep the shape there
reshape
[Music]
shape zero
comma x shape
one and one here so
return
x and y okay so this is our data
preparation so let's try to see if
everything went well so
data
tray okay so this is not what i want to
write so it x
x train y
train
equals to data
preparation
of
data
train perfect so now we see here x
train
shape we have this shape here
and
y
train
shape we have this shape here right so
now we get it right so we have
apparently 4991
and we have window sizes of 40 and we
have this additional dimension here we
need and then we have the y
training data and this is just
predicting the next price
so let's also do this for test
x test y
test
data preparation
oh
i misspelled it and you saw that
oh that sucks data
test
you have to live with that
i apologies
this this annoys me
okay so we
we need to
so it's
so when you try it it's not with this
misspeller okay
perfect
so far so good
good good good
now we want to make our model on this
here and it's going to be again it's
going to be a simple model and
i highly advise you to do something more
advanced so model here we take the
sequential and then we
add a layer here and it's going to be
this lstm
and
units it's going to be 45
return
return sequence okay you cannot return
sequence
it's true that means you can have
feedback
input
shape
x
train
shape we take the first dimension this
is a 40 and then one here
perfect and model add here and take
another l
lstm
i cannot hit the keyboard here units
and just take 45 again there
i don't know if this is good
but let's try this
return
[Music]
sequence
true that means we can feedback in it
model
add
lstm
units
let me do 45 again here and no return
sequence
and then
let me take model
model add and we do a dense
and it has units
one here okay perfect so this is
basically it and then we take model
compile
and we do optimizer
[Music]
and we do
loss
mean we take the means
[Music]
where
error
metrics
acura resc
perfect so this is
apparently
not perfect and probably some
misspelling here oh that's a long
keyword understood return sequence it's
not called return sequence
return sequences
return sequences okay perfect there you
go uh and then we have model
fit now we're gonna fit our model
it's going to x train
y
train
and
let's do epochs
and i just put batch size as well to 32
which is a default so this is fitting
here and it didn't fit very well here
and probably something
on no loss function
again a misspelling
mean square
error
squared error
there you go okay so you need to type in
everything correct and when i type so
faster it's or i'm sitting here
communicating with you you distracted me
a bit so
you know that yeah you know that yeah
so i only do five here because i'm not
gonna wait here forever and it's
actually quite fast it's also quite
small one but the idea is the two first
layers have return sequences so it can
return the data inside here and let's
see how it works
uh so let's do some prediction
infrared and uh
it's a model
and predict
and then we take
x test
perfect and now we have predicted
so let's try
to okay let's just look at the dimension
here print shape so one thing oh
actually that's quite okay here so we
have 213
around
the number of trading days in a year so
that fits pretty well and let's try to
map it out and see how it goes fig
axis
plt sub
plots okay amazing and ax
plot and what you want to plot first why
pred here
and
ax
plot the real price here y test here
and color
red
so here you go
so
the red is the real price you see it's
way more
going up and down it's made more yeah
and the predicted is actually above here
you see it's kind of
it doesn't
we must say it doesn't do a well
prediction it is a
kind of slow but maybe the network is
also too naive so this is just to get an
idea how you can get started and you
might notice in one thing that the
intervals here are
not the prices actually
but
you can actually transform that back so
you can actually say y
on scaled
for instance call it like that
and uh scalar and you say inverse
transform
oops and why pred here so then you get
the real values and uh yeah let's just
try to do that axis
plt sub
plots
and
why
real values here it is a data
lock
and it was 2020 right oh
20
and it was
adjusted close so we just read the
unreal data first here into numpy
perfect oh we're doing a figure that was
not what i wanted to do so we have the
data here so x plot and we have uh y
real here and we
actually plot it
from position 40 why because we skipped
the first 40 values
plot and y
uh how is it called unskilled
should we have a color i don't know it
figures out itself right so here you
actually see the exact same chart here
but the difference is that here you have
the
prices in u.s dollars here and then the
day numbers here right so here you go so
you can see how to transform it
back with this inverse transform and
yeah that's basically it so now we're
going to do a project instead where we
work on weather data which is probably
even less predictable than the stock
market see you in the next one we're
going to introduce the project it's
going to be amazing see you in a moment
[Music]
are you ready for this project it's
going to be amazing we're going to use a
or create a recurrent neural network and
we can use it to predict weather
forecast or predict the temperature in
the future okay so are you ready for
that so let's dive into it
so here the project is recurring neural
network we're gonna approach a project
on weather prediction on time series
data so it's similar to our stock data
just a bit more predictable that's my
claim at least so the first one is
import the libraries we're going to use
perfect
and then we are going to
execute this cell to download the data
then we can read the data
awesome
and
then we're gonna limit the data set
because basically it has measurements
every 10 minutes but
as many people point out it's not like
the temperature is gonna or the data is
gonna change much within 10 minutes what
matters is maybe once an hour so you
actually do that by removing that and it
says here how to do that so
do that
investigate the data look at the
correlation inspect the columns that are
correlated or not
it's often you know you have a lot of
data here and a lot of it might not
matter
potentially some data could be
transformed like this here the
these wind
velocities but we're just gonna ignore
them in this one here if you want to you
should transform them because they are
not really good for the model in the way
they are
then we are going to add periodic time
intervals so this is quite interesting
this is new
uh
so the temperature is correlated to the
time of day right you might expect it's
warmer in the mid day than it is at
midnight at least in most places on
planet earth i think that's the case so
right now the model doesn't really
unders our model doesn't really
understand time measurement so you want
to make a curve that goes from maybe it
starts on one at midnight and then zero
and -1 at midday and then it goes back
up right
sinus or a cosine s curve it's actually
cosine squared that one right and the
same with seasons right it might be
warmer and it's summer than winter so
you also want to do one which is really
slow that goes one cycle per year
how do you do that well try to follow
along here along here you made a data
frame with the index which is time stamp
underscore as for instance then you do a
mapping and then you have the day period
and the year period
and then you follow these here follow
these formulas here
perfect
then we split the data and again here we
do a validation set and a testing set so
this is also kind of new why do we have
that well this is about it read about it
there if you want to build a solid
mallet your
model you want to follow specific
protocols for splitting your data into
three sets one training one validation
and one final evaluation which is a test
set so you already saw previously that
in some sense we had in the algorithm we
put a validation
set along with it and we're going to do
that as well and it's called validation
set here but we're also going to make a
test set here so what to do
use the length of the data and split it
into
70 training 20 validation and 10 percent
for testing
then we're gonna normalize it
yeah you need the data to be in the
interval from zero to one
and how do you do that you get the mean
and the standard from this and only do
it on the test or train data
and then you do this transformation here
assuming the naming fits and you have to
do that with train mean and uh
the transformation valid on test data is
done similarly
on validation and says data is done
similarly but you of course with train
mean and train stand out because you
need to make the same transformation and
not like that
then we create data sets and here i also
explain a bit better with the input
width the offset and the label
and
in our previous one we took input width
of 40
and the offset was actually only one
because we took the we had 40
data points and then the next data point
was actually the one we wanted to
predict
and here we want to have 24 so a data
for one hour and then we're going to
predict it in 24 hours so we're going to
predict
24 hours ahead in time
wow
so
giving all the measurements right now
what is the temperature
tomorrow at the same time
then we're going to make a function
input width with offset assume we always
use label with one right this is the
label width
i call the function create
dataset which takes argument
all this
piece here
and let it create two empty lists x and
y
convert it i mean just try to follow
along here if you get confused well i'm
there to help you in the next one
then we create a model and again here we
make a really really simple model so
don't expect fireworks if it doesn't get
specific data
perfect
and then we
add
yeah then we train the model
so actually compile and fit the model
down here we could have compiled it up
there we didn't do that do this in step
12.
and then we try to predict data
where x is assigned to the test data
right
and remember here oh that was the
thing to notice here in our fit we have
validation data to be the validation
data frames
perfect
or data sets not data frames
and then we plot the results we do that
one way to do it is like this
and then optionals
calculate the correlations
perfect are you excited i hope so
because this is actually going to be
amazing you will learn some new things
along the way here i often like that
when you do something you should learn
something new so here we go we are
getting more and more advanced so i hope
you enjoy it so try it out yourself if
you get stuck don't worry about it i'm
there to help you in the next one so see
you there in a moment bye-bye
[Music]
and how did it go did you manage all by
yourself let me know in the comments if
it was too difficult or not i'm also
here to learn how to teach better so
please let me know so let's get started
i hope you manage some of it on your own
and if you didn't try at all i highly
encourage you to try it before you see
how it can be done this is really how
you learn you don't learn by watching me
doing it you'll learn by doing it
yourself it's like yes you learn some
tricks along the way but it's like for
instance imagine you want to learn to
play soccer i know soccer is maybe not a
big thing in the us but in europe it's a
big thing
and if you watch the players yes you'll
learn some tricks but honestly you learn
way faster if you go out on the field
and play soccer yourself right
yeah the same is with programming
good so let's jump into it i introduced
it already so we import the libraries
here
we
download the datasets here
and
then we need to read the data here from
step two here
so so
there's a thing here right we have the
csv path here and we need to use that
path there so this is actually where we
downloaded the data so let's get that so
pd
read oh we need to say
data i guess
pd read csv but here we have to have c3
path there and then it says parse
dates
to be true and index
called to be zero and let's just
date data ahead here
it's a good idea and you see here it's a
big data set so it takes actually quite
some time to read the data here
quite a long time because it is a big
data set
and it continues really really long i
don't know how many rows they are
actually we should investigate that
afterwards it could be fun right
and if we ever get the data set online
on board in our
data frame
i'll be surprised if not but you never
know okay here we go and does the data
look okay
it looks actually pretty okay here so
perfect so let's just get the length of
data here
[Music]
and you see here oh yeah it's it's
uh 420 000 columns of data or rows of
data not columns rules of data
wow amazing but
as
as we see already here we're going to
limit the data set so
that's how we are going to do it
so we're gonna only take every fifth
column
so let's do that
[Music]
data data
five colon colon six
and then we have less data so let's just
see the length of data now and you see
we have
one sixth of the data so we only have
one data point per hour because if you
look at the data here it's like 10
minutes past 20 minutes past 30 minutes
past 40 minutes passed and so on and now
we can also look at that
data
hit
you see we start by one o'clock two
o'clock three o'clock instead
good so
let's
investigate the data so data core
so what does chord do remember it's
correlation right so we see that
in the diagonal
it's a big set of data here but in the
diagonal here you see you have ones all
the way and that's because it is the
same against the same but what we're
really looking for is the temperature
in
degrees here and we want to see other
some that are really low low correlated
and you will actually see some of them
like these down here are really low so
we are gonna remove them
the reason is that these data needed to
be transformed in order for the model to
make sense of it so we are just gonna
remove them okay
so
let's just put it in as a data frame and
call data drop
and
we're gonna drop
this list here
i think i put a space there i'm getting
used to that
perfect
i think we have to put axis one here
axis one
perfect
and still not happy
we need a comma here
perfect so here we go right
awesome i should actually just help you
so you don't make the same mistake as me
when you copy paste
and axis one is probably also one you if
you don't remember drop
yeah
it's a bit difficult okay perfect
so
let's continue so
what to do here right we need to
transform the temperature so let's do
that
time
stamps s we have
that to be
data frame index here so we have all the
timestamps in there and then we want to
transform
them
so let's do that times stamps
s
equals
time stamps s
map
and then we do pd time stamp
oh not stat stamp
time
stamp
okay so that's the transformation here
so now we have the data here as seconds
instead
we can look at that so here we have it
that seconds instead of a date right so
before here in this format here
just show it here
we had it as a
time index rule right so what we did
here was transforming it to seconds
instead and why do you do that it's
because then we can transform it easier
to
what we want
so assign do like this assign the data
frame index to a variable time oh this
is what we did and then we
make a variable day to be
24 times 60 times
60
and then year to be
oh that's difficult 365.2425
times day
wow who made this up
and then we make a column data
data frame
what is it called
day
sin
and np sin
timestamp
times
2 times n pi divided by day
and we need to do similar
for
cosinus
and then we need to do similar for a
year right
similar for year
here
and
here okay so now we have made these
curves of the data instead of having
seconds which will which would be linear
then we have these sinus curves and
cosine squares we don't really know
which one is going to be the best one
and if actually let's just try to do
this
correlation thing here and we find this
down area actually with the temperature
you see here actually this one
correlates the most with temperature and
then sinus second most on the daily
basis right
but of course it's not going to be
perfect but there will be some
correlation on it it's at least
something you can use which is not
how's it called which is uh
better than the date
which it doesn't understand okay so now
we need to split the data so let's just
have a length of the data frame we call
the data frame yeah
train data frame and what's that it's
going to be the data frame from
from
0
and then we need to take integer
and we take n times
0.7
and then we have
validation data frame data frame and we
take this from
this piece here
and
it should be nine instead here right
and then we do test data frame to be
data frame
and we take it from
this one here
uh actually we don't need the zero here
but it's okay we just leave it perfect
so now we have 70 here
uh 20 here and 10 percent here right
this is the 20 and this is the 10 right
so we added it up like that i hope you
understand that perfect
then we need to normalize the data and
what we need to do is we need to find
the train mean
we do it on the
train
data frame and we actually just take the
mean value on it and then we have the
standard deviation
no
train standard deviation standard
st std right and train
d3
std right so now we have these values
there and then we need to transform them
and for instance we do it let's just
copy here
and uh
copy here and copy here so here we have
validation
validation
and
test
and
test so again here we keep the train
mean train mean and train standard
deviation for all of them because you do
only the calculation on the training
data and not on the validation and test
data okay
now for this one this is this
i
know is a bit more complex and again
i explained what all this means but
basically we give this as input data
this frame of 24 and we want
a prediction down here
so let's see if we can make sense of it
so
yeah def create
data set as it says up there and uh
which takes argument all this
piece here
so it takes input with offset
i don't know why i take
yeah offset
uh
offset zero okay so actually we tried
start with offset zero by default let's
do that first and then you can play
around with it afterwards with different
sets and let's just follow the manual
there create two empty lists it's just
like we did in
in
in the lesson
and convert the data frame into numpy
and sign it to data x
so df
to
numpy
and then we have
data
y
which is
df
and what do we have here we have the
predict column
and to numpy
perfect
so far so good let's just add an extra
line here so we can read the data and
then for i in
range
and where do we range from
we range from
[Music]
input width
that's where we start right
oh input
width
and where do we end we end at the length
of the
we end of the length of theta
x
and then we minus
offset
okay perfect
and colon here and then we are so then
we take x append
and then we take data x here
and what do we take we take
we take from
i'm i minus i input width
until
i
and we take all the data here in this
case
and y
append
and then we take data
y
i
plus offset right so this is offsetted
because we are predicting
if we just took the first index it will
here and plus offset
and by default we're going to take
offset 0 and later you can play around
with it with longer offset okay perfect
awesome
and then we are going to transform this
into numpy arrays
so
x and p
array
x
y
y equals n p array
y
awesome
and uh
what's next then we are going
to
reshape return x comma y reshape
minus one
comma one so it's actually just
reshaping how y is
it's making from one long row to one
column per
thing
perfect
awesome
and uh
let's try to do this
train
data frame or data set we should call it
uh create data set and we take we take
the train
data frame and then we have a validation
data set and we create data set
and
validation
data frame
and then we have test
data set equals to create
data set test
data frame okay
perfect so
here we have it
and should we just
take a look at the shapes
train ds
shape
it doesn't
it's not called train it doesn't have
shape
we need probably to index into it oh
[Music]
so yeah okay so so
for each one train oh yeah of course i
returned two here i just forgot that
right so this is a test now this is uh
how it's called the
features and we have a 49 000 here and
it's 24 and 15 right so we have 15
features we have 24 hours of data and we
have this many data sets
perfect
create the model so we're continuing
model this is straightforward
models
if i could type it sequential
maybe i just add these things here
so the key thing to notice here is that
we have return sequences to be true
that means that we
feed the data again
perfect
then
train the model
[Music]
and
fit the model
and we're gonna run five times here not
too overdue if you want to play around
with it i can advise you to put it a bit
up here maybe that will help
and
again here we have the validation data
here so you will
validate so for each run it will train
it on the training data and then you
validate it and finally we will actually
test the data
predict the data on the testing data and
see how it performs accordingly to that
and uh yeah
and again
just remember this is a quite simple
model we make here it's only
this one layer here and it might
you should play around with that too
and here we have the last one to see how
it goes
perfect okay so
now let's get the test data
test data set here right so now we have
the test in there and then we have y
thread
to be the model
predict
predict
x right
so now we actually predicted all the
values and uh yeah i actually just
want to show you
why
pred
shape
uh wipe red shape
it's not yeah okay it's like this
obviously and you see here you actually
have 24 values for each one of the
prediction because it predicts
all the way through there so let's try
to
[Music]
know that
so i actually think this is
straightforward copying
to plot the data
so this is the y data
and this so that's uh y data is a real
data
and the prediction data is called wipe
red for me i call it wipe right up there
so you should recall that and then
notice here i take minus one index here
because that's the last value of this 24
here okay
and uh i is not identified so let's just
put it somewhere let's just make some
index here
here we go so you actually see here so
the red one is the predicted data and
the green one is the real data here
yeah so you see here's a temperature and
predicted from the previous temperatures
this is how it goes
i don't know if it's good or bad it's
not doesn't look perfect at least
but it's there but it's also very simple
model we created here and
yeah so optional calculate the
correlation uh
so
correlation between the two data frames
okay let's do that
uh
df
c for correlation so i don't overwrite
the other data it's a pde data frame
and
what do we do we get a
the real data here
and the real data was called y and
the the predicted data was called y
on the score pred there so here we have
it and then we just say day f underscore
c
core
so
uh what did i do wrong here
okay so so obviously i forgot that this
one here
is a not a single list here so we have
to take
the minus one comma
zero here okay perfect
it was not what i wanted it was a comma
here
and the same thing for this one
obviously
this is also index okay perfect so here
you actually see
that the real value is highly correlated
with the predicted value so it's not
that bad so it means that they actually
go pretty much hand in hand it might be
that it's like
like that but so it doesn't say it goes
like this but it say that it's basically
when this one goes up the other one goes
up and the other ones go down and then
when it goes down it goes down so that's
what correlation says so you can also
measure it with accuracy and so on and
get some measurements and try to improve
the model i will highly advise you for
instance to change add some more layers
and try to play around with it and
maybe increase this one here to get some
more i don't know
actually says
the loss was
it was not that high so
maybe there are many potential to create
it better
never mind i like to hear below there
what you did and
how well you did what were your scores
and uh i'm looking forward to hear that
if you like this one so please like
subscribe and all that kind of stuff in
the next one we're actually going to
change what we're going to work in with
we're going to work with text
so another big thing in machine learning
is how to extract information from text
and we're going to explore some
philosophies and not philosophy some
aspects of that and uh
then we're gonna turn yeah into later
into information retrieval and how to
structure data how does google actually
figure out what's relevant in a document
that's really amazing and
how can you represent words actually as
information and how they relate to each
other it's it's actually mind-breaking
if you haven't heard about it before so
i highly advise you to continue this
journey if you
even though it's not your highest
interest in text because there will be
some things that you probably never
thought about i promise you that so
unless anything let me know in the
comments and subscribe like and all that
kind of stuff see you in the future