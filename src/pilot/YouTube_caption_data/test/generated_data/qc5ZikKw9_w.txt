all right ready to go okay
what's the energy guys come on bring it
up bring it up okay yeah after party
well thanks a lot for everyone to
everyone for making it to the
presentation on Friday morning my name
is Dan Banga I do lead a team of
business development managers at amazon
web services focusing on machine
learning and deep learning platforms and
services specifically sage maker and the
ecosystem of products that come around
it it's my pleasure to present alongside
face book today my friend Jeff Smith
from face book AI research and the goal
of today is to talk to you in in death
and details about the pie tours the
experience of a developer as it as it
comes to PI torch on on the cloud on
sage maker just for a little bit of
housekeeping
there will be code we will be showing
some code but don't worry about all the
details because as a matter of fact the
code are available the code is available
on github so just what we want to focus
on today is the best practices some of
the insights that we can share right now
so that you you at least get that you
got this like that home and the details
are available online for you to read
okay
also this is a 400 level session we
won't be doing going through a lot of
definitions but again everything is
available to read online and the code is
heavily documented for for for your
benefit okay so I'd like to get this
started by walking you through some of
the ten top ten strategic trends in
technology for 2019 as you all know
we're walking towards the world of
autonomous things and immersive
experience in blockchain as you might
have seen there are tons tons of these
net new technologies that are coming in
impact in our world and I believe you're
all here today because you understand
the impact that AI has over the world
that we are all going
being today tomorrow but particularly
some of the things that caught my
attention when I was looking at these
changes are things like digital ethics
and privacy or smart spaces or AI driven
development and those are practical
aspects of using deep learning and AI to
make essentially the experience better
when it comes to AI driven development
for developers and data scientists but
as well as the experience better when it
comes to humanity in general so we're
seeing all these trends coming up and
we're seeing AI involved in all of these
and at AWS specifically we're seeing
tens of thousands of developers actively
building machine learning models on the
cloud today so much so that there's a
250% growth year over year and then this
is mostly enabled by the the speed at
which these developers and their
scientists can build trained and tuned
machine learning models on the cloud so
part of what we do with sage maker and
other products and services is to
accelerate the process that someone has
to go through from ideation to
realization as far as their project is
concerned when it comes to machine
learning and eight out of ten of
tensorflow workloads that were observing
nowadays is running on AWS including the
benchmark now we're working with tons
and tons of customers a lot of them you
might you might know if you're using
ZocDoc to go to the doctor for example
you're probably using tensorflow natal
you are using tensorflow on AWS in the
backend DigitalGlobe is using say
shoemaker to analyze large images and
and then provide those large images and
insights on those large images to their
own customers as a service so we're
seeing a heavy momentum going towards
leveraging machine learning on the cloud
today and in sports analytics as well
some of the fun examples are folks like
the NFL essentially interesting
terabytes of data into the cloud and
leveraging Amazon Sage maker to
transform the experience that some of
their customers have when it comes to
the game we're saying similar dynamics
in sports analytics with Formula One
essentially ingesting terabytes of data
through the cars during race and then
using machine learning on sage maker to
transform their experience of visual
experience that folks that are watching
the races are having so our approach
that it appears to enable all of these
capabilities and all these accelerations
is to essentially start by being
customer focused 90 to 95 percent of our
roadmap is driven by the feedback that
we get from customers we are aggressive
at the pace at which we innovate if you
observe the keynotes and all the
launches that we announced lately you
you this might not be a surprise to you
and we also focus on the breath and the
Deaf at the same time there's more AI
and machine learning services in
production at AWS and anywhere else and
because we're aggressively listening to
what our customers are asking for we're
essentially building anything that
they're asking for anything that is
popular we support every framework or
the most popular framework that exists
nowadays pythor stencil fluid makes
night cafe you name it and then we
package all of this with within a
secured environment and we embed
research and development into everything
that we do for example in sage maker
will provide algorithms that are coming
from the research that we do at Amazon
to come and we make them available for
our customers to consume so now let's
jump into the developers experience
today as a developer you're probably
working with I mean by developer I mean
they're scientists and AI engineer
developer you're probably partnering
with a DevOps engineer or someone that
owns the infrastructure and you're
working together to essentially put your
products in production right so you
build your AI models using things like
Jupiter notebook and then you have to
hand that over to the devil person to
put that in production but I want to
challenge all of us today to think about
is this concept of AI driven development
where you're working on an integrated
workflow an integrated AI application
development workflow that includes a few
things one of them is the AI developers
tool as a developer what kind of SDK
what kind of tool which you require in
order to build AI models the second
thing that comes in
in a picture is of course there are
algorithms because you as an AI
developer again you want to use the
tools that make your life easier but you
also want to build some algorithms what
if some of these algorithms were already
figured out by folks like us and made
available to you in order for you to
pick them up and then use that directly
or what if the same environment would be
able to provide you the ability to bring
your own algorithm in pi torch or
another framework language the third
thing is an end to end machine learning
platforms that is fully integrated with
the developer tools and then the
algorithm and that's where CH mail comes
in the picture and the fourth thing is
of course the model templates and
packages and SDK and everything that we
can't empathize to make sure that the
experience that you have is reproducible
and repeatable and so that's where a
sage maker and PI thoughts are really
nicely combined to essentially make sure
that the developers experiences is as
easy as possible and we gotta jump into
a details right now I believe who here
is not familiar with sage maker pretty
much no one so we're going to go really
rapidly through the details you've seen
the AWS machine learning stack before we
support the GPU that are provided by
Nvidia Volta v100 so the fastest GPU in
the world all the frameworks that are
available and then as of a couple of
days ago sage maker was up mented with
ground truth data labeling capabilities
elastic inference reinforcement learning
workflow and collaboration capabilities
so there's a lot that we make available
there all of these is packaged behind an
API that again enables AI driven
development and just to accelerate again
on the capabilities of sage maker as a
reminder you have a notebook instances
environment that enables you to build
models to train in tune models as well
as deploying models in production and
the key call-out here is that all of
these are available within again the
framework and integrated framework of
cloud data Lake solutions compliance and
other capabilities pay-as-you-go and all
the things that you know now PI torch to
approach the PI torch experience or the
experience of a pythons developer and
sage maker I want to walk you through
the concept that I call the anatomy of a
deep learning framework
seance ash maker and the experience of a
Python Chevelle / or a developer in
general usually starts with the
developer working on either their laptop
or the Jupiter notebook and for that I'm
actually going to switch to my laptop to
get to get the work load started okay
okay I'm gonna get back to the
presentation and move back to sharing my
screen when it's possible so as a
developer you get started with with your
laptop or Jupiter notebook environment
and when the experience is a shoemaker
starts like this well you you have
control over part of the infrastructure
some of the infrastructure you can
either kick off jupiter notebooks or
training environment or hosting
environment and that control is
augmented by capabilities like a stage
maker identity sorry it obvious identity
and access management that would enable
you to basically have access controls in
in their environment and then you would
be able to encrypt data and in the EBS
drives and everything data at rest and
in transit using the key management
service of AWS now all that work
together with Amazon s3 to provide data
in the model artifacts to to the model
the other piece of the equation is
Amazon ECR that makes it possible for us
to package algorithms in different
doctor containers for the workload that
you have to run through one of them is
PI tours the other ones are tend to flow
MX net trainer and it's also possible
for you to bring your own algorithm
within this ecosystem now the other
thing that is important is logs because
we don't want the developers to have to
run machines all the time or after the
workloads are done it's possible for us
to essentially externalize the logs back
to carwash logs and make sure that we
shut down their infrastructure after the
workload is done now I believe you're
familiar with the Jupiter notebooks
experience it's an easy to machine that
we spin up for you and then we put
sample notebooks in there to practice
and then for the training ecosystem we
spin up an ephemeral cluster that comes
attached with an EBS volume and we fetch
the data automatically from the cloud
storage in
to that training environment with the
possibility of streaming that data in
directly bypassing the the EBS volume
there and work taking the data directly
to to the instances for training now the
other thing that are that is important
is the hosting experience so with a few
clicks you can essentially host your
machine learning models behind the rest
endpoint that is managed by Sage maker
or behind the badge in France a batch
transformation endpoint that is also
managed by Sage maker now what happens
for when you want to consume your models
well it's possible again within a sage
maker environment to using AWS lambda
and API gateway communicate with that
rest endpoint that you've made available
for for serving your models now let's
dig a little bit into the docker
containers that we pull in in order to
train your machine learning models if
you want to look at it from bottom up as
far as the stack is concerned with in
that container we have a data agent
which has a responsibility to
essentially go fetch the data shard the
data distribute the data to multiple
machines and bring that to the tooter
cluster there is also a lock matrix
agent which also has a responsibility to
collect the logs from every machines
that are involved in the training
process and then push those to the cloud
wash logs metric and then when you go
slightly higher up the stack you get the
possibility to distribute your training
jobs depending on the frameworks that
that is that you're leveraging and also
you have possibility to have CUDA
libraries in case you're using GPUs
going slightly higher up the stack we
get PI towards libraries install whether
you if you're using a PI torch container
we get tensorflow installed if you're
using a tensor flow container and so on
and so forth and then going again
slightly higher up the stack we get the
Amazon sage maker SDK which is the most
powerful item on this entire slide the
sage Maker SDK is that thing that makes
it possible for us to do everything here
fetch the data on your behalf log the
data on your behalf you can summon sage
maker through the SDK and say I want
four or five six machines and we will
figure out how to do everything else on
your behalf and way higher up the stack
we have your algorithm and again the
algorithm could be an Amazon provided
algorithm or it could be you bring your
own algorithm
in tensorflow but for that for the for
the impetus for this specific case but
for that scenario you wouldn't have to
build all the distribution and
everything else that happens within the
container because we bring all of these
capabilities all you have to bring a
co-pilot or script and and we'll walk
through an example now what happens when
you do import sage maker when you
essentially import the sdk within your
working environment if you remember that
stack that we just walk through the
Python that the Python SDK does a few
things
well first you you import sage maker for
this specific case of pi torch
we provide sage maker the Python that
has a couple of classes one is PI 2 and
PI throws model who here is familiar
with scikit-learn ok quite a number of
people so you can think about the Python
estimator in sage maker as an analogy to
psychic learn where we packaged the
context class that has the possibility
to talk to the infrastructure it has a
possibility to fetch the Python
container it has the possibility to hand
over happy parameters to your training
job and so that's how the developer
communicates with sage maker and
communicates with the PI college
environment so that's essentially what
happens here as I create a Python
estimator and within that estimator I
pointed to my main function and then
I'll say I want the MLP 3 to extra-large
which is a GPU machine and as I tell the
version of pi thoughts that I want to
use and the hyper parameters that I want
to leverage now the second thing is
similar to scikit-learn when you want to
start training the model you say pythons
estimator that fit essentially asking
sage maker to go fetch the data using
the data agent from s3 and you point it
to these channels so the content of this
dictionary is actually in this case we
have training which is the pointer to
the training data set on s3 and then we
have test which is a pointer to the test
data set on s3 so in this case a shoe
maker would know how to go fetch the
data in these two buckets and it would
know that the trained one is for the
purpose of training and a test one is
for the purpose of testing so pretty
straightforward and once you're done
training you probably want to host your
model behind the endpoint and it's
extreme
easy with sage maker to essentially host
your model all you have to do is use the
Python model class point stage may
appoint the Python model class to the
location of the model itself on Amazon
s3 and then give it some rows and all
that for Sage maker to be able to go
fetch that data on your behalf and then
with six laters DEP LOI it's possible
for Sage maker to spin up an instance in
this case three instances and then host
your model on your behalf and expose
that behind the rest API now in that
training example we had three machines
involved in the training process and
what does that look like specifically
well when you have three machines that
station maker spins up on your behalf
all of them have the same stack again
that we walk through from the data
agents in the lock matrix and the first
thing that happens is that the data
agents in all these machines go to fetch
the data from the cloud storage again
stage maker knows how to shot the data
and distribute that to all the machines
so in this case one third of the data
would go to one machine if that's what
you decide or it's also possible for
Sage Maker to take the entire data set
and hand that over to all the machines
that are involved in the process
now once the data are fetched by the
data age and they're placed under these
folders opt ml input data train and test
because again we pointed it to two
training folders now what you have to do
as a BIOS developer is to know that from
your code you have to go fetch the
training data in these folders and so
our responsibility is to go get it from
s3 put it here your responsibility is to
get it from here and use it in your code
the next thing that happens is in this
scenario specifically it's a distributed
training exercise and because it's a
distributed training exercise our PI
torch containers provide the distributed
capabilities in pi torch distributed
backing capabilities in PI for which one
of them is n wcl another one is glue
with this TCP capability as well the
developers don't have to worry about
that so the sage makeup I Tosh
containers bring that and make it
possible for these machines to
communicate during a distributed
training process and once the model is
trained stage makeup puts the model in
ops ml model out sorry ops mmm
and any with the name of the model in
this case it's a table oh and after the
model is put there essentially what
happens is a shoemaker fetches the model
and then puts that in the model artifact
bucket now if you have some outputs like
images or anything that is generated by
your model as you train it there's also
an up ml output folder in which if you
put information so H maker is going to
fetch and push back to the cloud storage
so that's the responsibility of stage
maker in the process of training a
distributed algorithm on a cloud and
your responsibility is to know where
these things are and fetch them and use
them now the lock metrics agent also as
you do train your model it goes around
and then fetches all the logs from all
the machines aggregates them and pushes
them back to the cloud storage so that
you can to the cloud watch logs so that
you can visualize your logs after the
execution of a training jar now to go
higher up the stack and talk
specifically about pythons 1.0 what is
the strategy there and what are the
benefits that the developers have there
it's my pleasure to introduce you Jeff
Smith from Facebook a I research hello
everyone my name is Jeff I'm here from
Facebook
a I research I'm really happy that
Amazon gave us the chance to come and
tell a little bit of our story behind
why we built pi torch and what we do
with PI tour should Facebook so let me
dive into that so one of the most
important things we do with a I at
Facebook is to make our existing
products better and so these are things
like social recommendations asking your
friends what exactly you know searching
for a restaurant or something like that
I'm so you can see an example of
something like that so that to be able
to work with that sort of data set of
your friends there's other interesting
applications one of them is I really
like which I will point to later is
machine translation people all around
the world use Facebook this is one that
I use on a nearly daily basis we use
machine translation as a way of using
deep learning models to be able to allow
people to talk to each other who don't
speak the same language
accessibility is another application
which depending on your personal
background you may not have seen before
but it's a really powerful use of deep
learning to render all of this rich user
content that we have across things like
Facebook and Instagram accessible to
people who have different abilities and
we do this with with pi torch it's not
just the good parts of applications
though that AI helps us with it's also
the difficult stuff the protection of
the community the integrity of your
experience as a Facebook Instagram user
and so this is preventing things like
share baiting as you see there and even
suicide prevention detection so these
are really important applications that
we're investing a lot in internally
across the board to be able to make sure
that our platform is used in a way that
really benefits you the users the most
as as much as possible and deep learning
plays a really key role in that as
powered by PI torch ok so there we go we
do this as an extraordinary scale today
so pie charts today is our end and
research production platform performing
300 trillion
inference operations a day we also run
all of this code on mobile devices so
the technology you're going to see today
is actually deployed on over a billion
mobile devices and so there are a lot of
unique challenges that come from
operating of that scale on servers and
in mobile devices and so it takes a
fairly unique approach to technology to
develop the tool team that allows people
to be successful in doing things like
that so our choice for this is pi torch
I'm gonna be a little bit of background
on what is pi torch the first thing I
want to point out is that pi torches has
gotten really popular here and so we've
seen this big uptick and contributors on
github we have now become the second
fastest growing project and all of open
source on github which is really
exciting to see so it's been this really
great community success story in which
we've developed an open-source AI
framework in collaboration with everyone
in the community if you're not familiar
with it here are some of the places you
might start to touch it if you as a user
these are just AP is that allow you to
do various things like have like used
specific optimizers and this is just
kind of a sampling of some of the
utilities that come out of the box that
are really easy to use I'll show you
them kind of in action when it makes it
a little bit clearer okay so the the
first example here to be clear we have
now two AP eyes across Python and C++
we're showing them both here just to
show that they're they're very similar
in their structure and so your use of
them really just depends upon what suits
your application the best here we're
showing an example of some of the the
out-of-the-box functionality you get
module is our way of defining a neural
network layer and then you and then I'm
gonna show you a few more functions you
can get so you get things like drop out
nonlinear functions right out of the box
here's some example of a few more things
so we have we have like data loaders
from from torch vision which allow us to
get example datasets and things like
that and again the Python and C++ are
very similar and they're they're very
idiomatic to both of their languages
that we're trying to really work with
you as a developer in the way that
you're going to program to begin with I
want to talk about the journey of
research to production because this is
really what makes or breaks AI
technology today because AI is a very
old topic but it's a fairly new topic
within production and this is an area
that I personally spend a lot of time my
career on and I'm really excited at the
solutions we've been able to develop so
I'm gonna tell you the story of what is
research to production at Facebook and
so this is kind of a this is a snapshot
from maybe say the past year or so so we
have these three different unique
technologies that have their own
capabilities ok there we go okay alright
so high-torque historically was used for
prototyping it grew out of facebook AI
research and was really focused on the
experience of AI researchers trying to
develop papers but not necessarily to
deploy to production onyx is the open
neural network exchange format which we
developed in collaboration with a number
of large companies within the larger
tech industry to allow us to have inner
operation between deep learning
frameworks it's real focus is on being
able
to transfer neural network models from
this research mode into production and
then finally historically we developed a
different deep learning framework called
cafe 2 which is really optimized to be
able to execute on that trillions of
inference operations a day mission that
we have organizationally this is great
but we want to make it smoother we want
to put less work and then we want to
have this really smooth transition from
prototyping all the way out to
deployment and that is what PI torch 1.0
is all about so what is PI torch 1.0
it's a seamless path from research to
production we want to make sure that
whatever you can do in research you can
deploy with the least friction possible
and so we have a lot of tools for this
part of this is is allowing you to
really opt in and decide when you want
flexibility and when you want to be
concerned with having the sort of static
assumptions that we have in a production
mode but to be able to have a tool chain
that doesn't require you to make
substantial changes just because you've
decided ok this model is good I want to
ship it we talk about production ization
this is really the area I think that of
AI technology where people have really
only recently started to develop very
powerful and useful solutions as an
engineer and an engineering manager this
is one of the biggest rate limiters I
have seen on a large number of teams
I've been on so when we talk about
production ization there are things we
need to do to be able to operate at
Facebook scale you need to use your
hardware really efficiently because we
we need to do so much inference
operations who have training jobs
they're enormous which means that we
also need to be able to do that quite
scalable we need to be able to able to
address large clusters of GPUs and we
nee able need to be able to go across a
range of devices CPUs GPUs mobile
devices another thing that we want to be
able to do is to be able to optimize our
code the solution within pi torch 1.0 is
this really unique and powerful
technology which we call the JIT the JIT
allows us to be able to take this
flexible experimental mode and adapt it
for production use cases either
incrementally or as a whole program what
we're talking about is this difference
between eager and static mode within
deep learning frameworks in general
classic Pike torch has really focused on
this eager mode where depending on how
you're running a program maybe you
actually just want to see the result
right now or maybe you want to be able
to export a graph and using onyx or
something like that to be able to
optimize it later
static graph deep learning frameworks
like cafe to have this sort of
assumption that you're going to run your
program just to produce a graph and that
graph itself is going to be executed to
produce your result intrinsic conflict
between these two there's some pros and
cons obviously people like flexibility
and research it allows them to pursue
the development of new ideas but it's
really hard to ship because we can't do
things like optimization to run it
efficiently on the static side of this
chart of course this gives us the
ability to optimize to run efficiently
but this gets in the way of your
development workflow when you're really
in that early research stage so what
we've done is we've taken classic PI
torch that a lot of people know and love
that is simple and debuggable and is
straight Python as you know it and we
now call that the PI torch eager mode
again its main negative is that it we
can't optimize it for production and for
a lot of use cases some users can't
deploy with up with a Python runtime
environment so that's eager mode we now
have script mode what can you do a
script mode well script mode allows you
to extract out what is the graph from
your flexible PI torch program and be
able to optimize it for efficient
execution using the JIT compiler there's
two ways to do this one is that you can
do this in a tracing mode that allows
you to perform an execution of your
program to get its structure so if you
have no conditional logic within your
entire deep learning framework is there
your entire deep learning program you
can actually just run it through once
and then you'll get this the static
representation that you can execute then
in a way that can be highly optimized or
you can move incremental e taking things
function by function giving you that
sort of incremental adaptation of
research to production and all this is
is opt in at the level of of simple
annotations a way that you can dive into
precisely how we use technology like
this today is to use our fair seek
implementation fair seek is the is the
framework
that we developed to perform the six
billion neural machine translations we
perform on a daily basis it's really
leading research technology that is
actually deployed at extraordinary scale
today making people happy
all across Facebook and you can use it
on a specially optimized version of the
project which you can find on this
github project and you can get it and
you can understand how you can use it on
stage maker today and with that I'm
gonna pass back to Dan thanks thank you
Jeff okay thanks
so to recap stage maker provides the
end-to-end machine learning platform
capabilities it can dance at the same
rhythm as the developer to basically get
to a a driven development and pìkô which
provides all the api's and all the
capabilities from a deep learning
standpoint that can take the developer
from research to production in a
seamless manner and Facebook and Amazon
work together to implement phasic which
is basically Facebook AI research
sequence the sequence library on top of
AWS Amazon sage maker so it's possible
for you to play around with it right now
now we're going to switch to the demo
and essentially for the demo we will I
would walk you through a practical
aspect of what Jeff just described for
all of us so we will who here is
familiar with generative address here
networks a few people okay so ganz are
essentially the generative adversarial
networks were invented by younggu fellow
and it's a it's a mechanism through
which you can basically train two
networks to neural networks to work
against each other for good at end of
the day in this specific example of DC
GaN what we are going to implement is a
neural network that is striving to
generate fake images in a neural network
that is striving to basically
discriminate and correct those fake
images as the
in finding out whether these images are
fake or real so the idea there and the
intuition there is that you you want to
be able at the end of the day to
generate net new images from random
noise and as that generator generates
net new images you also have a trained
discriminator that is trained that is
basically classifying these images as
fake or real and you train them together
and a benefit there is that a point of
equilibrium is where the discriminator
is almost at the quaint point us as in
5050 where it's not certain whether an
image is is is real or not because it
starts very high in confidence because
it was trained on recognizing real
images and it ends up as I'm not sure
anymore and by that time you basically
train a generator to give you some
images that as a dad are as real as
possible now in the images domain it's a
pretty fun example and it's very visual
so that's why we selected that for the
demo but you can extrapolate that
concept and basically train threat
detection models with a threat detection
discriminator and a detection generator
you can train fraud detection models you
can train all sorts of different types
of models with that type of mindset so
that's what we selected that for for the
demo and now I'm going to switch to to
my laptop to walk you through some more
details so the first thing is the we
will walk through some code and the
first thing that I want to call out is
I'm using my own laptop to do stage
maker specific development right again
that's all thanks to the stage maker
Python SDK so the stage maker Python SDK
is pip installable you can have that on
your own laptop and what that makes
possible is for you to basically use
sage maker on your own laptop or usage
maker on the Jupiter notebook instance
on the cloud whichever one works for you
I guess we should start by the jupiter
notebook so the sage maker console looks
like this and then if you go to notebook
and as of a couple of days ago we have
all of these other capabilities that are
available but if you do create a jupiter
notebook instance it's very easy to just
open the notebook here and i've already
done that and then i would land into my
my own Jupiter notebook folder with
with massage maker DCA an example now
the structure of this example and which
again is available in github is I have a
notebook to basically walk through some
of in an exploratory manner to some of
the the things that I want to play with
right so I'm showing you the notebook
now because it helps me do some preview
of the data I hope it helps me visualize
my data on a subsample data set in order
for me to know that I'm working with the
proper data set that I need but it's
very straightforward the idea here is to
to load the date to get a subset of the
data locally and then use that to
explore your data set at a very at a
very low skill level now so I just want
to point out that the experience is
possible on Jupiter notebook instance or
sage maker the next call-out that I have
is when you do execute on Sage Maker you
have this concept called local mode now
sage maker local mode makes it possible
for developers to essentially kick off a
station maker job using the sage maker
SDK and the sage sage maker syntax but
leveraging docker containers on their
local machines before testing so what
that makes possible is you're using the
same syntax and semantics and you're
kicking off a job locally you can clean
up your code you can remove some errors
and all these things I can typically go
through but by the time you're done with
that it's very easy for you to
extrapolate and push the same piece of
code to Sage Maker and train to a larger
instance 10 instance 120 instances if
you want now I'll start by working you
through the what you need to do to
execute as in you assuming you already
have your neural network we'll get into
that but assuming you already have that
and you want to kick off a job with a
shoemaker this is what the code looks
like it's very straightforward so
essentially I do import sage maker and I
import some extra libraries so that can
basically help me visualize my data this
is the essence of sage make a local mode
this role is basically my sage make a
role that enables a shoemaker to do
things on it appears on my behalf I put
it in my environment variables because
I'm sharing the code and github and I
don't want you to run things in my
account so that's why it's there but you
you will have your own role there and in
this instance type variable I'm keeping
I'm keeping two versions of it whether I
want to submit the job to Sage maker or
I want to submit the job locally so in
the case I want to submit the job
locally I say I want to use the local
mode and and a point a shoe maker to my
input data said in this case I have the
the face data set in my in my s3 bucket
and this is what we we went through
slightly earlier now you create this
estimator object which again is like the
spark context in spark if you familiar
with it or is the cyclic learn estimator
and you point it to your entry point the
entry point is the main function is a
function that you that is actually
executing the training process of your
training job the other thing that the
estimator object is expecting from you
is the source directory now the sorcerer
actually is very important because
sometimes your code has helper libraries
has utility libraries has dependencies
so all of these dependencies you can put
them in the source directory and then CH
maker is going to fetch that and put
that on every container docket container
as it spins up the job for you that next
thing is a role and the framework
version now when you say I want the
framework version 1.0.0 point one dev
because you're using the Python
estimator stage maker knows to go get
pythons 1.0 and make it available to you
if you said you wanted vers firmware
version zero point four point zero as a
shoe maker would create a container with
Python zero point 4.0 for you and in
this case I say I want two instances and
then this is an instance type that I
want now here the instance type is local
what we're going to do is that we're
going to change that to MLP 3/16
extra-large which is a GPU based
instance running on on on on th maker
now the next thing that I need to
specify is the hyper parameters here I
say well because it's the demo I say I
want to run I already run it for a
longer period of time but I say I want
to run it for two epochs and this is a
distributed back-end that I want to use
and for selecting a distribution
back-end you can say you want glow you
want n CC L you want
depending on how you want to distribute
your workload but you don't have to
again install and configure it CH maker
does it by default here I can change my
back-end to glue an end of a CL and you
can you can compare performances there
now this display after it's basically a
hyper parameter that I use in my code to
display the pictures because again I'm
generating pictures as part of this
exercise so I want to be able to see
these pictures as I go through my my
development and this specifically and
one of the reasons I selected this
example is because again in some cases
you might be training a deep neural
network and at the end of the training
of a deep neural network you might want
to have a conversation with your team
which your management and all these
things so you my advice we use to think
about displaying some of your results
creating new metrics and I'll show you
some of the code to basically measure
the amount of time it takes for you to
fetch the data out the amount of time it
takes for an iteration the amount of
time it takes with a batch all of these
things you can point them out to Sage
Maker and then basically it will be able
to from where you stored it in an output
folder pick that up and put it back on
s3 for you so that you could review that
later on now the number of workers is
basically it's taken by the training
data loader that Jeff spoke about and
it's a way to hyper thread the mechanism
through which you can fetch the data
from the data loader and I specify my
batch size and the base job name now the
cool thing about running sage maker
kicking off a job with sage maker when I
do that estimator fit is that I have
this weight parameter now if I say
weight is equal to true what is going to
happen is that my laptop is going to ask
sage maker to start a job go to s3 and
fetch that data give it that
infrastructure and get the job started
but because I said weight is equal to
true the log matrix agent is going to go
and fetch logs from all of these
machines that are contributing in the
job and then present them back to me on
my client and my client could have been
it could have been my Drupal or notebook
as well so we're going to get this
started
well and then I'm going to get into
describing the actual PI torch code
now the job is starting can you see can
you see correctly yeah so CH maker is
starting a training job
the other important thing that I wanted
to call out some folks ask me so what
about my own library what if I wanted to
install tangible in this specific case
for example I'm using tensor board to
present the metrics the custom metrics
that have generated as well as some of
the images that I'm generating on the
fly because I want to visualize those in
fake images that were created through
time and so you can have these
requirements this text file basically
part of your source directory in this
case in my source directory I have these
requirements or text file and in your
requirement or text file whatever
library you put in there as long as it's
pip installable sage maker will pick
that up and install it for you so this
is very powerful in a sense that you can
start a job with PI torch and sage maker
but if you have dependencies and extra
libraries like an NLP library that you
want to use or like like tend to board
or like anything else that you want to
use that we don't provide by default
just by putting that in the requirements
the text file station maker will be able
to find that requirement the text file
take the libraries install the libraries
in every container before it starts
trading your job so again that's the
relationship between you sage maker and
and the framework so while it's starting
the instances I'm going to walk you
through the actual Gantt code for pi
torch now we worked through the launch
code which is basically the execution
code for for the main function the
structure of my the structure of my
source directory is like this so I have
my main function which basically runs
the the training loop but before I get
there I want to start by the actual
neural network function so I keep my
neural network definitions in one file
and then I keep my main function in
another file now the neural network
definitions file is very straightforward
that's exactly what Jeff showed you
earlier PI torch makes it very easy for
you to describe your neural network
using the simple API is that that that
Jeff introduced you to now
to create my generator I use a tenon
module class that not Jeff told told us
about which basically makes it possible
for you to declare your neural network
in it in a decorative way now it
provides simple api's to to create
linear layers to create convolutional
layers to create LCM layers and in a
parameterised way so that you don't have
to do a lot of writing code now when you
do create these layers with an end and a
module module it does other things in
the backend other things like
identifying what kind of parameters are
involved in those layers identifying
whether these parameters need their
gradients computer identifying whether
these weights are contributing to the
last function that you want to optimize
and then keeping track of these
gradients so that when you do your back
propagation step it will be able to
update these gradients on your behalf so
all of that is similar to the end user
and PI torch makes it a lot easier for
you to just decoratively
declare your neural network it also has
this sequential class that basically
makes it possible for you to stack your
decorations of a neural network in this
case for my generator because the
generator starts with a vector of
basically noise a hundred dimensional
vector of nothing random noise and from
there it tries to create an image
something in the shape of an image so it
does transpose convolution in two
dimension two basics to go from a
one-dimensional vector back to the shape
of an image and for that it uses the
number of channels that you provide
because we're using color images and and
and other parameters so that's what's
going on here the code is available on
github so you can you can read the
details and these part of the of the
initialization class of the generator is
concerned with instantiating the the
weight of that generator again there are
helpful libraries for you to either
randomly instantiate the weights of that
neural network of that part of a neural
network or instantiate the weights with
a normal distribution centered at zero
in standard deviation of 0.2 for 0.02
for example
right okay so the other important thing
when you're creating a newer network
when you're declaring a newer network
with with PI torch is the forward
function now the fourth function is
designed to pass your data through the
the through the neural network
architecture that you've created now
this is important okay my trading job is
actually starting already by the way so
this is going on I'll walk you through
the logs in a minute
so the fourth function is important in
the sense that it's what takes your your
actual data and passes it through the
neural network now you might observe
here that there is no backward function
that you have to create the backward
function is responsible for after the
data is passed through the neural
network and then it gets to the end and
it's it's basically evaluated by the
last function then you need to
essentially penalize all the weights in
the neural network for their
contribution on the on the last function
so for that you use back propagation
which applies the chain rule all the way
up from the last function back to every
single weight that contributed in that
execution and so that back propagation
is a lot of differential equations that
you typically have to do yourself you
have to track all the weights and sum
your networks can get very very very big
big all the way up until billions of
parameters so PI torch seamlessly keep
these parameters and this tract
relationship between all the weights
that are contributing to the outcome and
when you when you use the backward
function in your in in in pi torch it
does back propagation and try to wait
for you automatically so backward for
the backward function is provided by the
autograph package and pi towards you
don't have to implement it yourself so
we've done the same for the
discriminator which basically does the
opposite of the generator the generator
does transport convolution to create an
image the discriminator goat does normal
convolution to identify the objects in
the image and classify that at the end
with a sigmoid activation function which
is essentially a binary a binary logic
function that says well if it computes
the probability between 0 and 1 that
that says how much it believes that that
image is real or fake right so this if
you're familiar with object detection
and object classifications it's pretty
much the same thing that's what a
discriminator is doing and you could
also initialize that here and it has a
forward function that's pretty much it
so this is a structure of neural network
that does something as complex as as
generator generative atmosphere networks
now the other thing that you that you
have to do now is your main function so
you have declared your neural network
you need now to use it to fetch the data
to pass the data through your model to
train your mini-batches and to do
everything else that you need to do now
specifically the point I want to hit
here is the operational aspect of
training neural network or training
models I spoke to you earlier about
using tension board to log metrics now
besides importing all the other PI torch
libraries it's very important - here we
go okay so this stencil board X library
makes it possible for you to use 10 to
ward with my torch all right it was part
of my requirements or text file
installable
so you would be familiar with everything
pi thoughts related up there because
it's common already but I installed
tensor board X to be able to write logs
of any kind that I want to tend to board
during my execution so that's what I do
here I important about X it gives me
access to this summary writer and I have
this tangible log directory that I put
that I pass through summary writer to
create a writer that I will use to write
images logs and everything else now the
interesting thing that I want to call
out here as it relates to sage make a
remember when I told you that sage maker
knows where to put things for you you
know you need to know what to put things
for say shoe maker this is one of these
examples a sister SDK to the sage maker
SDK is called decision maker containers
if you don't have it installed install
it what is a shoe maker containers does
is it provides you with a lot of
environment variables that are already
available in Sage Maker one of these
environment variables is the output data
there
the output data there is essentially the
environment variable for where sage
maker is expecting you to put some of
your output that is going to fetch and
go prêt up put on on s3 on your behalf
so that's what I've done here I'm
creating my summary writer and I'm
pointing that to these output data
directory and I'm naming my folder runs
and that's where I'm going to have my
tensor board metrics now the rest of it
is well this is a helper function to
load the to load the generator and
instantiate instantiate that to load the
discriminator and instantiate that as
well and there starts my training
function now I commented the training
function so that you could read the code
and github and get to the details we
don't have all the time to go through
all of that but importing for the
important point to to call us are how do
how do I know because or how do I create
my scripts in such a way that I can use
it regardless of whether I'm using a CPU
or GPU regardless of whether I have
multiple GPUs or not so I want to have
my code to be agnostic to all of these
possible changes in infrastructure
whenever I want to kick off a stage
maker training job and that's basically
what is happening here so I'm checking
part of the arguments that sage maker
provides to me are the number of hosts
that I have again I don't need to figure
that out because if I kick off a job
with two or three or four hosts sage
maker is going to pass that to me as
part of the argument to my main function
training job so I can use that
information and say hey if the number of
the hosts that you've created for me is
more than one and there is this
parameter for distributed back-end and
it's not known well I believe I want to
dispute a training job and so if I have
a disability training job do all of
these things so this is if you're a
Python developer you probably familiar
with this - this will be the training
job in PI towards you you essentially
need to have a view of your world or
your world sighs machines have different
ranks within that world size and that's
how they can distribute the work among
themselves distribute the data the logs
and all this
so and that's what I'm doing here so I'm
getting I'm grabbing the environment
variables I'm grabbing the host rank of
my machine and I'm instantiating my my
distributed Python she submitted from
class to with that back-end that was
that was provided by me and with the
host Frank that is that that this
machine has now this specific machine
because you have a copy of the script on
multiple machines every single one of
them has a rank in the in the world of
machines that you have and what's H
Maker does is if it starts three or four
or five machines it would basically give
them different ranks and if you read the
rank from the environment variable then
you will be each machine even though the
script is the same in all the machine
each machine will be instantiated with
its own host rank I hope it makes sense
so far if it makes sense say yes
all right perfect I'm not talking to
myself
okay so so then the next thing you know
you've you've given stats you you you
loaded your model you've found out
agnostic Allah if you are code is if
your workload is distributed or not if
GPUs are supposed to be used or not and
the next thing to actually know this
this is how you find out whatever you
have GPUs are not so if pythons provide
this Porsche CUDA is available as as a
feature and this torch CUDA is available
essentially gives you an output of true
or false if you have GPUs available or
not and so I can leverage that and
create this device variable that to
which I'm going to push all my 10
servers and all my newer networks and
everything else if you use fighters
before before Pike just 1.0 to pass your
10 source to two GPUs was it was
challenging so pathos 1.0 makes it a lot
easier to point all the tensors all the
network structures and everything else
to a specific device and you only set
that up once and you can you can use
that for going on for it now the next
thing I need to do is to load my data
and I have a helper function to send
create my data loader you can read that
in the utilities file and I load my
model which returns a generator and a
discriminator which is what we are
returning here in the load model
function and my torch also has
distortion cuda device count so Turku
device count is essentially returning
the number of GPUs that exists on that
machine so again you can use this
function to find out if you have more
than one GPUs on your machine and if you
have more than one GPUs on the machine
then you can use all of these GPUs that
you have on the machine so yeah if you
have more than one GPUs on the machine
patos provide the nn module has this
data parallel class which is a wrapper
around your normal and in module class
but now that it knows that you have
multiple GPUs in the machine it's going
to paralyze the data to all these GPUs
again something you don't have to worry
about the only thing you have to do is
find out if you have more than one GPUs
and push that to all these GPUs using
data parallel and the way you push all
of these to the GPUs is by saying
generator to device discriminator to
device and your device is true its GPU
if you're using GPU CPU using CPU and
you find that agnostic lis using this
function again the beauty of this is
whether you're on CPU and GPU this will
work agnostic way the next the next
thing to do is to create your your loss
function and pythons provide again with
Eden module the multiple loss function
in the case of the of the generative
adversarial Network we're using the
brand new banner across the entropy loss
which is essentially computing a cross
entropy across multiple valve different
values or different variables in this
case we have two outputs that we care
about
fake are not fake so the BCE computes
the likelihood of something being fake
or something not being faking gives you
probability between 0 and 1 the next
thing is the optimizer and all of these
I hope you can see how sequential
programming with pi torch can be as
opposed to other frame
work where you have to create a graph in
something and then push that part or
makes it possible for you to in a
sequential manner describe your new
network your data loader and everything
step by step and the next thing you have
to do here is instantiate an optimizer
and optimizer is also provided by the
toss up team library and that mainly
optimizers rmsprop and adam and all
these other guys now here we're using
the atom optimizer I'm using an
optimizer for each new piece of the
neural network the generator or the
discriminator and the way it knows to
work with that piece of a neural network
is because I'm passing the discriminator
that parameters to the discriminator
optimizer and the generator that
parameters to the generator optimizer
now fast-forward there's some helper
libraries here to print the logs to
average load the the average logs the
average meter the average time it took
me to load the data to run through a
batch and all these other things now the
other important thing here is a training
loop the training loop is where you're
actually training your data where you're
actually training your model and so with
my training loop I'm essentially
tracking the time it takes me to to
update I'm sorry tracking the time it
takes me to load the data I can also
pass the real faces which is say a
tensor with the actual real images to
the GPU I have to do that for every data
set that I have because I'm using the
GPU I said the the batch size to the
data is loaded in batches so I want to
pick up the batch size and everything
else that follows is to essentially pass
the data through the neural network do
the forward propagation compute the loss
and then do the backward propagation and
iterate through that over and over so
you can read the the details there I
commented aim from the logic about the
last function here so that you can read
that later and scrolling down to the
next part that matters is writing the
log so because I have that writer with
that writer that I created I can
essentially write all these values that
I care about and I encourage you to be
creative about what you want to
right if you want to track anything you
can put it here so I'm tracking average
time to load a batch average time to do
this a number of other things now if my
display after if my if my display after
parameter was set to 200 so after two
hundred iterations I want to see a new
image that's essentially what I'm doing
here so after two hundred iterations I
want to print the logs to to the to
stand it out and when you print the
standardization make a pic setup and
sends it to your logs and I also want to
visualize some of these images and
that's what I use here with the vu
chilled safe image and safe image and
writer add image so save image will save
an image to a folder that I will that I
upload for myself and right image it's
going to write the images 210 to board
and that's pretty much it the rest of it
is is to do some plotting and saving the
model which is pretty straightforward
and I run the main function now let's
look at the logs of running the main
function that we started earlier a
Kotori earlier CH maker would install
the libraries on your behalf so that's
what happened here it starts this
collecting tend to board decision maker
finding tend to board in my requirements
a text file and installing installing it
for me on different machines now it's
kicking off the training job giving me
some of the parameters and I made it I
made it run for just two epochs so that
we won't stare at the logs training for
forever so and then it starts plotting I
mean it start reporting the logs and
everything else that you probably
familiar with if you train the new
network before so that's pretty much the
experience there
now going back to the console if I go
back to stage maker training jobs I can
see the training job that we just
executed this one and if I go to the
details there's a shoe maker will give
me the metadata and will give me
information about the output and if I go
to this output folder then it pushed my
model there and it also pushed the
information all the information that I
was that are stored in in those images
and everything else that is related the
other interests
thing that I wanted to show you is the
logs that the log matrix has pushed to
to clash logs so here I can visualize
GPU utilization CPU utilization I can
also the same logs in case I wanted to
wait for the logs that were presenting
to my presented to my laptop are also
sent to to Sage maker and these are the
logs for multiple jobs that I've
executed through time so forever you can
see your logs afterwards and tend to
board all the metrics that are pushed to
tensile board are visible afterwards
right so the average batch time that I
was tracking the the loss for the
discriminator the loss for the generator
all of these things are available for
for you to visualize afterwards and the
images as well available so because I
pushed the images after each iteration
it's possible for me to visualize how
the network through time to generator
through time has been trying to generate
through yeah has been trying to generate
fake images like initially it's really
really dirty and really random and as
you go forward it becomes clearer and
clearer and I didn't wait enough for it
to start generating images that look as
real as possible and then you can also
plot that against the real images so
that's that and the last thing before we
call it a day all the files that I was
saving in the output data directory
after each epoch stage maker would fetch
them up and then push them to the cloud
storage so I've already downloaded that
from a previous execution so this is
really good if you want to have a
conversation with your team after these
are the image at epoch number 25 or
whatever you can come and pick them up
from here and you also have the real
know these are still the fake images and
you also have the yeah
all of these fake images and you also
have the the lost function that you can
plot after every epoch right and you can
visualize that and have a conversation
around that afterwards
the other interesting thing that's
available there is a creator a Jif or
gif depending on how you pronounce that
for for the loss of a generator and
discriminator as part of the one of the
helper functions that's available there
so again you can you
and use that to do some plotting
internally create some animation and
then after you push everything it's
going to be available in this case you
see the generator starts with a high
error but over time it's the error of
generator starts going down and then if
I train for long enough you'll have seen
the discriminated meeting the generator
somewhere halfway through to a position
where to the equilibrium position that
would indicate that the generator has
gotten a lot better at generating images
and the discriminator as getting a lot
worse at finding out whether these
images are true or not and also created
an animation for for the results for you
to be able to animate the epochs of
everything through time now with that
I'm going to switch back to the slides
and back to the the example code
hopefully you took it took a picture of
that and thank you thanks again for your
time
[Applause]