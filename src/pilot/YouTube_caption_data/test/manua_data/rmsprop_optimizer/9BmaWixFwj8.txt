Hello everyone, welcome back to the tensorflow tutorial series
We are going to talk about the optimizer in Tensorflow
There are lots of different optimizer in tf
The most common one you can choose is called Gradient Descent
Gradient Descent is one of the most important method in machine learning (most basic one)
If you search "Tensorflow optimizer" in google, you will find this website
There are 7 types of optimizers for you to use
You see, the first one is called GradientDescentOptimizer
So this tutorial is for you if you are familiar with machine learning
This gradient descent is highly depended on how many data you pass to the training
For example, when only pass 1/10 or 1/100 data size into training, then this GD becomes the SGD
SGD only train a partial data for each training step
Therefore, one of its advantages is we can accelerate the learning time, it learns to go to the global or local minima fast then use pure GD
However the GD spend more time in training because it minimize all data samples every time
The second common used optimizers are called MomentumOptimizer and AdamOptimizer
If you know AlphaGo or computer plays atari, they used this RMSProp to train
All these optimizer only change the learning rate for each step
For example, the momentum optimizer consider not only the learning rate for this training step
But also consider the learning rate on the last step, or last few step
So use this optimizer will speed up the learning speed
The others will also consider the learning rate similar to the momentum method
So they can also speed up your training time and get to global or local minima faster
Let's have a visual
For example, the machine learning want to go to this global minima
Typically, the machine learning is to learn how to walk from the more error (red line) to less error (blue line)
The red area= large cost
The blue area = less cost
As we initialize at the start point
Then you can compare those optimizer and their speed
They have different learning path
Let's compare Momentum and SGD
SGD is slower then others but faster then pure GD
While momentum considers the last learning rate, it walk step increased
And the next step is the same as last step
So as you see, the first few of step for momentum went to wrong direction, but it correct this later on
So this is the one of the most important field in machine learning
Back to this website, I recommend use the GradientDescent if you are beginner
If you wanna know more about those optimizer, please find the link in my description
If you understand how those optimizer work, then you can consider to use momentum and adam
or RMSProp
We often use momentum and adam when we have basic understanding about them
This is all for today. If you have any question, please leave a comment
Please subscribe for more tutorials about ML, tf and python basic
Thank you for your support