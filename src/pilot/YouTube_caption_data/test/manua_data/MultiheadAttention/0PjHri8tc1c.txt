 So in the previous video, we talked about a basic form of
 self attention. Now we are going to get a little bit more
 sophisticated and talk about the self attention mechanism that is
 used in the attention is all you need paper.
 So yeah, just to recap, this is what we looked at, at the in the
 previous video where we had the self attention mechanism defined
 as follows, where we had this input sequence, which were
 embeddings, word embeddings, then we computed the dot
 products here between one particular input and all the
 other inputs. And then we normalize those with a softmax
 and computed the output as the attention weighted inputs. So
 for each input for each input AI, we will get a vector. And if
 we have, let's say T words, we will get T vector. So from one
 to T, essentially a matrix, tension matrix. Okay, but yeah,
 we are now kind of extending this concept of this basic
 attention, using the attention that is used in the paper
 attention is all you need. So how does that one look like? So,
 first of all, notice that there was one big problem with the
 basic version of self attention. And the problem was that it did
 not involve any learnable parameters. So the previous
 version of self attention, the basic form was actually not very
 useful for learning a language model, because how do we update
 this, right? So if we want to develop a language model that is
 for instance, supposed to translate text, how do we make
 it better if there are no learnable parameters, right? So
 so here we are now introducing three trainable weight matrices
 that are multiplied with the input sequence embeddings, the
 x is that we had before. So now, instead of let me go back, so
 instead of just computing the stop product here between the
 input, and the query, we now involve weight matrices. In
 fact, we will use three different types of weight
 matrices, we call the one w q, which corresponds to the query,
 k for the key, and V for the value. So when we now compute
 this matrix multiplication between word, the word embedding
 xi, which is a vector, and this matrix w q, we get the so called
 query. For this one, we get the so called key. So these are
 vectors. And the value is also vector. This is the value
 between a matrix w V, and the input x i. So what's new is now
 that we have modified versions of this word embedding. And
 these are weight matrices that can be updated with back
 propagation, for instance. So here's a drawing of how this
 self attention mechanism looks like for a particular input. So
 let's consider the one in the middle here, x two word, the
 second word in the sentence. So this current we can consider
 this as the current input, we call this the current query. So
 I mean, carries a little bit ambiguous, because we also have
 the queue here. But let's consider this as our current
 input here. And we compute these three things that I showed you
 on the previous slide by matrix multiplication. So if I go
 back, it's just a matrix modification to compute these
 three things. And we do this actually for all the inputs. So
 we also do this here, and here. So from word one, two, up to the
 last word, the teeth word. Now, we are also computing the
 attention values here as a dot product. However, here, we are
 now computing it instead of computing it between the
 sequences, x one and x, let's say, I, instead of computing it
 like this, we computed actually between the query and the key.
 So these are just modified versions of that. So here we are
 regarding q two as the current input. So we are using q two
 here everywhere. This is our query. And then we use the key.
 So maybe I should use different colors. So for the blue one
 here, use blue key for green one here for itself, we use green
 one. And for this one, we use this one. But this query is all
 the same, the keys are different, but the queries are
 the same. So why the terms query key and value, this I think
 comes from the field of databases. So here, it's not
 here, it's not really that relevant. It's just different,
 different names for different parts of this computation here.
 And you can think of this. And this is also known as the stock
 product is multiplicative attention. There's also the
 other types of attention, like additive attention. But here,
 it's like a form of form of multiplicative attention. And
 essentially, for each query for each year for each query, the
 model learns which key value input, it should attend to. So
 continuing with this model here, when we computed these, there's
 also a normalization via the softmax. And then these are
 added up to form this a two, which is the context aware
 embedding of the input. So if I go back, this is similar,
 essentially to what happens here, right. So when we have our
 context aware, embedding of the input x i, now we have the same
 thing, except the computation is a little bit fancier, because we
 involve these three matrices. I will also show you, I mean,
 there's also a scaled version of that, but one step at a time. So
 here is the not scaled version. So here, in the center, we have
 again, a softmax. So the softmax of these dot products. So that
 these attention weights, these A's sum up to one, just like
 before. And then what's new is we multiply this here by the
 value. So we do that for all the t values. So we are summing over
 them here. This will be a vector. So this will be the
 vector for the second word, a two corresponding to x two, we
 would do the same thing also for x one and xt, so up to xt. So we
 would repeat this process, but we would each time swap the
 query, then by let's say query, one, the first word, and then
 also the teeth word, and so forth. But notice also, what's
 cool about this is we could do this all in parallel, there's no
 sequential nature of that. So we could all compute these in
 parallel. Yeah, to explain a little bit better what was going
 on in the previous slide, I made a copy of the previous slide and
 edit some annotation about the different dimensions of the
 different parts in that figure. So let's walk through this from
 left to right. So here, x i x one, this is a word, a word
 embedding vector. So you can think of it as a one times the
 dimensional matrix or the dimensional vector, where the E
 is the embedding size in the original attention is all you
 need paper, they used 512 as the embedding size. But of course,
 this is a hyper parameter, it's something you can choose. It's
 arbitrary in that way, can have 256 or 1024, or some other
 number as the word embedding size, as we have seen, also,
 when we worked with RNNs. Then we have our matrices here, or
 w's, the query key and value. And they have to have, of
 course, the same dimension, I mean, the same number of rows as
 we have columns here for the matrix multiplication. So we
 have the everywhere and then the output size or the number of
 columns is dv, dq, and decay. And in the original paper, they
 had dq equaling decay, I mean, of course, that's also necessary
 for the stock product here, right, you have to match the
 dimensions. And in the original transform paper, they also had
 dq equal to dv. So this is it, we'll see later, why that is,
 it's because we have also certain stacking going on and
 things like that. So that's determining our output size.
 Okay, um, what else? Yeah, so these will be, of course,
 scalars, these stock products, right, because it's a
 modification between two vectors. So our softmax here,
 this will give us our essentially our scaled attention
 weight, that we then multiply by this value vector, which is
 the one times dv dimensional vector. And then we sum that
 up. So the output size of this one would be one times dv. So
 that's, yeah, just annotated what what these ports are, no,
 you know, what is the vector? And what's the matrix? That
 wasn't clear before. But yeah, this is, I think, maybe just a
 another summary of the previous slide. Here's an example of these
 attention visualizations. So again, this one here is just for
 one, one word, right? So this is just for the second word. So
 it's attention vector for the second word. So the previous
 slide, we had looked at the second word as the input. But of
 course, we have also other words in the input. So we would
 repeat this whole process for every input element. So here on
 the left hand side, consider the first word. And here on the
 right hand side, we consider the last word. Whereas here, we
 consider the second word. But this might be a misleading
 visualization. It looks like there's a sequential part of it.
 That's not true. All of these can be computed in parallel. So
 we don't have to wait for one to finish before we can compute
 the next. So these are all parallel computations, which is
 one nice aspect about these transformer models or the self
 attention mechanism in the transformer model. And then so
 we get an attention aware embedding for each of the words
 and then these essentially, you can think of it as a matrix now
 an attention matrix, where each row corresponds to to the
 embedding attention embedding for each word. So it should if
 we have a T here, this should actually be T. Okay. Um, yeah,
 here's just like, more like compact notation for that. So
 now consider for the inputs, we represent the inputs as t times
 the dimensional matrix, where these are embedding size again,
 and t is the input sequence size. So I'm just summarizing
 everything here in the center. So now, instead of doing these
 steps individually here, can just write this as one matrix
 multiplication. So we now have, let's say, the matrix Q. So we
 instead of having q1 here, q1, q1, and so forth. So these
 results, we can just summarize that as a matrix at t times dq
 dimensional matrix, k times, t times dk and t times dB. And
 then we can compute this attention matrix, which are t
 times t dimensional matrix. So this is one, maybe one
 disadvantage of the self attention approach is that this
 is kind of large, if you have a large input sequence, because
 yeah, it's like a pairwise in a way a pairwise similarity score
 here. So it's n squared, spot t squared, the bigger of that. So
 it's not the most I would say memory efficient approach. But
 well, at least we can compute things in parallel. So we have
 now kindly a kind of slightly modified version of the dot
 product that I showed you before. We call that the scale
 dot product, which is because of this one, I will explain that
 in the next slide. Just focusing again on this whole thing here.
 So now we have q times k as a matrix multiplication, then we
 take the softmax. So we have q times t, q times k as the matrix
 multiplication, and we have this scaling here, and we'll talk
 about it in the next slide and set. And then we have the soft
 max. And then we multiply by the matrix V. So this is just the
 more compact form of what I've shown you in the previous
 slides. And then we will get this t times dv dimensional
 attention matrix. Okay. So what is this scaling factor here in
 the denominator. So this is just to prevent these individual dot
 products. So if you do a matrix multiplication, you can also
 think of it as multiple dot products between these q and
 case. So to prevent these to become too large, we scale them
 because if you think of the softmax function, if you have a
 very negative input to the softmax function, it will be
 zero. If you have a very large input, a very positive large
 input, it will be close to one in the softmax. So in order to
 prevent a very sharp distribution of values in the
 softmax, we have this scaling factor. Because if you maybe
 just think back of the logistic sigmoid, which is essentially
 similar to the softmax, except that softmax incorporates all
 the other ones, but it's kind of like a sigmoidal thing. If you
 have values close to one, close to zero, there's like this
 saturation, right? So you want to prevent values from being
 too extreme. I mean, the same concept, this is, of course, not
 a softmax, but the same concept applies to the softmax. So by
 scaling, we prevent the softmax from having a distribution that
 is too sharp. Yeah, and here, just for reference is the
 visualization of the scaled dot product extension from the
 tension is all you need paper. So again, they're just
 summarizing, summarizing that visually, the matrix modification,
 and the scaling here.
 Optional mask, we will talk about the mask later when we
 talk about the transformer. Here in these steps, there is no
 masks. So a transformer also consists of an encoder and a
 decoder. And the decoder has a mask. So here can actually
 ignore it. And then we have the softmax. And then we have a
 matrix modification with V. So this is essentially summarizing
 this one. Okay, yeah, okay. This was self attention in the
 scaled dot product attention. In the next video, I will talk
 about the multi head attention. And then we will be one step
 closer to the transformer.