hi and welcome to one of arm's ai tech
talks bringing you the latest in ai
trends technologies and best practices
from arm and our ecosystem make sure you
subscribe and hit that notification bell
so you don't miss any of our upcoming
talks and don't forget to view the
playlist too in the link above
i hope you enjoy this tech talk
hello good morning everybody or good
afternoon wherever you are joining us
from um we're delighted to have you here
today
we've got imagi mob in the house and i
understand they have a giveaway which uh
johanna will uh tell you a little bit
more about that but i'm fresh off tiny
ml summit from last week i don't know if
any of you all attended but it was
pretty amazing um great to see all the
companies that were uh
delivering solutions and papers it was
exciting was my first face-to-face since
the pandemic so that was fun it was
great to see people in person
and imagi mob were there i got to meet
anders the ceo of imagemob and um it was
great to uh great to talk to him so and
he may pop in here i do see him here so
he may uh have a little cameo on this
tech talk so
um
but before we get started i want to tell
you how you can find us on twitter so
and uh johan if we go to the next slide
briefly
so if you want to tweet us at arms
software dev um that would be great we
might give you a little shout out and a
follow from our developer account um you
can find all these talks on our software
developer youtube channel um this this
this talk will be available uh shortly
after this presentation you can also
find a lot of our other tech talks in
case you missed some of the amazing
companies that that are doing these tiny
ml and solutions ml solutions on arm you
can we've got like 24 hours of them at
this point and you will learn a lot for
sure um and then for our upcoming tech
talks we have uh
some exciting
uh
other topics coming up so we've got kik
so coming um they have been in our
partner program for a couple years love
keeps so they've always got something
new to to show you and uh they'll be
talking about our new arm virtual
hardware and what they've done with it
so that's pretty fantastic um we have
emsa coming they have done some really
cool things on our new etho shoe 55 so
you don't want to miss that it's pretty
um impressive and then um we've got noda
who is coming to talk about uh
how to how to design neural models um
a hardware aware approach for doing that
so
anyway um
thank you for all the comments on chat
uh everybody needs a raspberry pi 4. i
hear they're hard to get your hands on
right now but you know you never know
like that hopefully hopefully they'll
the our supply chain issues will ease up
a little so we can all have a our pie um
so i will stop talking because you
didn't come here to listen to me i'd
like to um
introduce uh johann mom from imaginemob
hey johan how you doing hello mary nice
to be here
great to have you here um
be here again i would say
yes
this is our second tech talk it's it's
so great to have you back and i'm
looking forward to hearing what you've
gotta you gotta share with us and
everybody today so so johan is an ai
engineer and he's product owner at
imagimob he's got a lot of experience in
r d
um
projects uh phd in numerical fluid
mechanics which i'll tell you i've seen
the slides and there's some impressive
math equations on there so look out for
that be prepared by the math that you're
going to you're going to see in this
presentation and
msc in engineering uh physics from
uppsala university so johan i think it's
all you now and um i'll i'll hand it
over to you please ask questions in the
q a box we're gonna have lots of time
for questions um and uh i'll let johann
give you a little more information about
the giveaway and
starters pre-sale
thank you very much so
you hear me you hear me good and yep i
hear you're great yeah
great
yeah thanks very much for the
introduction and and yeah i'm very glad
to be here
here again to get this invitation and um
nice to see so many
um
listening um
so
yeah i will
uh
let's see here
just short about imaginable i will start
start here
um so we were founded in in 2013
with the idea that
hai and thai or tiny ml
is going to be become big and and the
field has certainly grown grown since
then and
by tinyml we mean machine learning
models that can run on embedded hardware
like arm microcontrollers that we
typically use in in our projects
and
we provide um
our tiny ammo platform as software as a
service it is a desktop application
currently supported on windows
we also provide our expertise in
thailand
bundle to
together with the with the
application as a service
we have experience from from uh many
customer projects and
it is that knowledge uh from from these
projects that we build in into our
product
we're active in international events and
we are in the committee for the tinyml
sweden group where we select speakers
for
for it for dynamic talks
and we are the sponsor of the tiny ml
foundation and with this i would say
that we are an important voice in the
dynamic community
where we help to to shape the future of
tainan
we're based in in stockholm sweden the
capital of sweden
and we are
15 employees at the moment
uh the agenda for today
after a short intro where i will present
what we are doing
uh and what what we're offering i will
dive into quantization
uh which this talk is about uh it's
it's about quantization of neural
networks that that is the core of our
platform
i will start out with some basics to
explain uh
what quantization is and why we need it
and then i will explain
uh what post-training compensation is in
full integer quantization how it can
be implemented and and
how we can get it to work in the end
at the end i will show how we can
quantize neural networks in imaginable
ai
but
first first um
just
introduction
about the magi mob ai our vision is is
simple we want to simplify the
development of of embedded ai
because developing small and efficient
machine learning models alone requires
some skills and expertise but if you're
on top of that as the development of
embedded
software the task becomes very complex
and time consuming
that's
that's our experience
but with this tool we want to
we want the application engineers out
there to focus on their skills
and on the application they want to
build not having to learn the details of
machine learning and low level
programming
so as you can see here
on this bar
the tool takes the user through the
entire process from data collection
data annotation model builder
thing and evaluation of generation of c
code that that you can integrate to your
firmware
and as i discussed in my previous
arm tech talk we have chosen the track
to generate generate plane c code
out of the train models
and this has many benefits for instance
flexibility portability and speed
but with that being said i should also
say that we have some interesting and
very promising partnerships where we can
actually choose
to output some something else we can
choose output binaries for for specific
neural network
hardware accelerator
so
so um yeah that's short about
the vision
say it is a general
development tool for dynamic
applications
we support
integration of many
types of sensors like accelerometers
gyrus thermometers etc
and
the primary project types we support are
classification and anomaly detection
projects
but we have also decided to go deep into
certain verticals like human activity
recognition
where an example is the bell pal
watch that you can see
see here it's um it's a
watch with with the built-in fault
detection
and we have also worked a lot with radar
technologies um
and
we have built the fully functional
prototype
of gesture-controlled in-ear headphones
together with osm group and and akonir
here in sweden
so akroner they manufacture this tiny
radar ship that that osm group helped to
to
build into the earphone
and we built
the the the software the
the machine learning algorithm inside
imagine mob ai
audio is another application we have uh
gone deep into where we have typical
preprocessor layers to build efficient
audio models
and recently we we created
starter projects that
makes it easy
for the user to to get started
so we have
built-in data and pre-trained models and
documentation that
that
yeah
that will help the user to get started
so
that's
short about imagine ai
and now i will go into quantization of
neural networks and that's
a work we started
around one year ago so i mentioned it in
the last talk but
but um
but since then we have we have done a
lot of progress
uh the the development started as a
customer request to support quantization
of lstm layers uh that was not supported
in any other framework at that point in
time so
we decided to dig into it and deliver
what what the customer wanted
and
i want to make this topic interesting
also for people without so much
knowledge in neural networks
and computer science so i will start out
with the basics
so let's start from the very beginning
how do digital computer store and
manipulate manipulate data because that
if we understand this we understand why
why we need integers in in
when we run on the smallest mcus but
this will come in
in a while
let's start here
computers represent data in in sets of
binary digits
and one bit is the smallest possible
piece of information
represented on a digital computer
and it represents one of two states so
it's basically a small switch
that is represented by an electrical
voltage or some
electrical state in the hardware
and this is the reason we use the base
two system to encode information since
we just need these two states
if we group eight bits together we get
one byte of information
and let's see how this byte can take on
different values
if all bits in in this bite are turned
off they are
yeah this all the switches are turned
off
uh we get zero since um
since with the base two system each
digit represents
a base of two and an exponent of zero up
to seven if we start from the right
and this also means that if we turn on
the last
bit here
we we get one because then we have two
to the power of zero which is
which is one um
but
but if we want to have uh
three then we get um
we turn on the last two bits
so
with an eight bit
integer
we can represent 256 values because it's
the combination of all these
um
and
within 16 bit integer we can represent
65 536 values
this is what happens under the hood of
these
integer representations
all right integers are great but but
decimal numbers are quite
handy for us when we want to calculate
something like how far does the ball get
after two seconds when i drop it
uh then it's quite handy to to have
decimal numbers or floating point
numbers as they are called in
in computer language
and they are represented by
um
by um
[Music]
in this way so we have a
certain number of bits for the sign
exponent and the mantissa
uh the
mantissa being the the decimal
points here the decimal
values
and
if we have a 32 bit float
we use eight bits for the exponent and
23 bits for the mantissa whereas if we
use a 64-bit double
we have 11 bits for the exponent and 52
bits for the mantissa
and but the important thing here is that
up here
adding and multiplying
can be done using bitwise operations and
shifting and that's really really fast
whereas down down here uh multiplying
and adding takes much more
time to do it there's there are more
cycles
to do all these operations
and that's kind of the background
so to do floating point operations uh
what computers usually have is an fpu a
floating point unit
and all
modern pcs smartphones and tablets and
powerful microcontrollers
have this included
so for instance the arm cortex m4f
m7f and m55 they have
an fpu option whereas the smallest mcus
that the m0 m1 and m3 they do not have
an fpu
here you can see an example of the of an
m4f
uh
it's a silicon labs one repeat board
whereas here the arduino board
uh
this has only an m3
core
and
what options do we have them for for the
smaller stems use well
we can
emulate floating point operations in
software so we do these operations in
software
it takes at least five to ten times more
time
so we waste a lot of cycles
to do these operations
what we another option is then to
transform the data to integers
and do
the calculations with with the integer
arithmetic
um
while we're doing that we can also
choose to to use
16 or 8-bit integers
and by doing that we can save some ram
and flash memory but
yeah
that's that's an option
but this transformation requires some
thinking and care otherwise we will get
the large quantization error and
that's that's the
um
yeah
what i will discuss more here
so
now we know why we need quantization for
the smaller stems to use
and to perform the actual quantization
we have two primary options and one is
to
define
the the model the neural network itself
using
integers already from the start so we do
the training and the inference
uh
using using integers this is called
quantization aware training
this has its own
complexities and and difficulties uh can
be difficult for instance to to
get the model to converge during
training what we also can do another
option is to use post training
compensation
and here we define the model using
floats we train uh the model with floats
but then afterwards when it's trained we
we quantize the model
and
um
there are many
ways to do this we can mix integers and
floats in various ways
but what we will focus on here is and
what we have done is fully integer
quantization which has the best
performance and accuracy if you
implement it
carefully and and in a good way but but
this removes all the floating point
numbers from the code
so
as as i show here to the right
um
when the data comes into the model
we quantize the we quantize it so we we
transform it to integers and then we run
all the operations using
integers and then at the end we can
de-quantize the the the output data if
we if we have the need for it
um
but but on these small
sensors usually everything is integers
anyway and then we don't need to to do
this
um
right so
how do you do this
the first step is to define a mapping
between integers and floating point
values so a transformation between the
two
um
here i use a
an example where all the floating point
values are between minus
2 and 5
shown here on the on the x axis of this
graph
if we use a simple
rounding
then we get
this is just an example that shows you
that if we're not careful we will get a
large quantization error so you see this
step here
the resolution is poor and all decimals
are
get wiped out
but we can do better better than this
because
think about it if we have an 8-bit
integer we have more buckets um
so
so if we think think of stretching out
the y-axis here
um
we
we can get something like this so much
smaller
quantization
error so you see that
now we use
all the all the available values for an
8-bit integer
between minus
128 and plus 127
and
we use
a scale
scale and the zero point value so that
we
we stay within these these limits
if we don't do that we get overflow and
then we'll use some information
um but one interesting key observation
here is that
we can choose these values
as we
as we like
as long as we respect these um
these mean and max values for that for
the integer
and this is what we do in in our back
end so we create functions
that can change a scale value and keep
the zero point value or vice versa
um
or keep one end point and change the
other etc
and this is important to note here
this is our our way forward in this
to solve the problem
uh okay so now we have the
[Music]
the mapping ready
so
what we now have to do is is um
the following steps so let's uh find the
scale and sharepoint values which we
call the quantization parameters and
then we use those to transform the input
data
uh and then we also have to define some
quantized operations
um
and we followed the ideas
the idea in jacobs
which is an excellent paper so we we
transform on our ad
to become a queue ad and
our mull to become a
cumul meaning a quantized operation
instead using integer values
uh so
let's um let's look at this by using an
example becomes easier
so we will multiply two numbers a and b
together and we get the the number c
um
all right so then we
[Music]
we use the context operation to solve
um
to solve for for c i the integer c here
so
first we just transform the the
left-hand side and the and the
right-hand side
um
and then we solve for the integer c
so if we
if we do that we get this
expression and as you see the the scale
value
i i just multiply scale a and scale b
together
[Music]
so remember now that all the values in
this expression are supposed to be
integers so
we can observe that if the remainder in
this division is not
zero we will have
an error and that that's exactly the
quantized error that we
that we will introduce them but if we
set the scale a b to be equal scale c
for instance we will have a zero zero
error after this operation
uh so
now you start to realize here here that
if we're
if we clever we can set these values
manipulate the values so that
we get
we get a reduced error
and
to see how this quickly can become a bit
complicated we we add a second operation
after our first multiply so let's add
another scalar to the result d
to the result c so so c plus d it
becomes a new
a new number e
again
we use
the
the mappings we have already defined
and now we solve for
for
the integer version e here
and what we can see is that this
expression
has dependencies
uh in the scale values from all previous
expressions
but
if we set them to be equal we can
simplify the expression
and
because then we see that they are
actually cancelling out
but then we yeah we have to set them
equal so so that's another requirement
and if we also
set this zero points to be zero
we get the very simplified expression
here
uh this is just an addition of three
integers and and no error is introduced
but
this uh
means that we have we have to fulfill
this requirement that that we have set
up
and
by noting down all these requirements we
get the system of equations
uh that has to be fulfilled and in this
small example it looks like this but in
a real model the system will be much
bigger
um but the thing with this
system is that it's a it's a hard
problem to solve it's over often over
determined which means that it's an
optimization problem
and often the round operator is present
which makes the problem uh non-linear
and the solution of the system
are integers which all together makes
makes it a really difficult
mathematical problem
uh so what we do is we use an heuristic
approach to this
it may not be the optimal solution but
we can
most often find a solution with a with a
small enough error
so what we do in an automated way is
is
we go through these steps
that i will show you here
first we generate normal floating point
c code
then we feed some training data or
validation data through the model
we record the min max on on all the
buffers and weight matrices etc
we also record a expected output
um
using these naive
um quantization parameters that span the
whole width so to speak
so we use those values but then we go
over the compute graph again and we
check all the operations and their
quantization contracts
and then we we sort according to if the
contracts are required or if it's just
accuracy
and and we figure out
which sets belong to each other for
instance if if a equals b and b equals c
then a equals c and
so on so we do a lot of these things
under under the hood
um we we set new scales uh
on the tensors so so that we fulfill the
contracts and then
we quantize the model using using these
parameters so we turn them into actual
integers
and finally we generate c code and
we
run
a test with the contest code and
calculate the quantization error so
compared to the
to the float code
so
that's
the theoretical background of what we're
doing
behind the scenes
and
what i will do now is to train and
quantize the model in imagine mob ai so
you will see how it how it looks like
and what i will do here is to take a
human activity
test case
so we will
we want to classify human motion based
on on an accelerometer mounted on a on a
person
so we start out by
creating a human activity starter
project
we select here some somewhere to store
the project and the data
so as i said before it has some example
data available
and there's also some documentation
uh
how to use the project and what it
contains
and here on the data tab we see that
data is already imported
and we can just double click on a file
here
to see it here is for instance a file
with some jumping data we can play
through and look at the labels
how they are
located and
then we have some
i guess
yeah this is some
running data
and here here we also have a video
attached time synced with the data so
that it's easier to
to label the data
right so in the classes
tab here we we see the output classes
and we can change the weight
of the classes to
to make it the model more sensitive to
certain classes if we have less data for
those for instance
in the preprocessor tab we can add
custom preprocessor layers
in this case the data is quite simple so
we we just
add a sliding window that collects
the data in
and feed it to the to the network
afterwards
now in the training tab we generate a
set of models
and
and
yes some some of these models have lstm
layers that we can see here for instance
and they have the input size
from the preprocessor and the output
size is the number of
classes
the output
neurons is six in this case
so here we press
start training job and then we will send
this
job to the to the training server
all right so that's the first step to
get
trained models
next
when the model has
the job has finished
we can see this green check mark
we can see
the accuracy f1 score on the models
and
we can sort them according to the best
model
etc
and then it's time to download
and
here what we
download is the trained
h5 model itself
but also the model predictions and the
input and output data for the for the
compare tests that we will perform later
on
right
so
here i have downloaded
this to my local computer
and
here we have the predictions
and if i double click on the model file
i will see the
architecture
um
and i can see the evaluation
and here if i go to the edge tab
then
this is where i convert the model to c
code
and i select here
on cc code and
somewhere to store the output
and here i have the option to to
quantize
only the network or the the network plus
the preprocessor and this is what i will
show choose here
here i i
select
some representative data
from which we will compute the min and
max values that i talked about before
and down here we set the tolerances for
the quantization test
okay and down here this is the
this is the
where where we select input and output
files for
the python comparator so this is to
compare that
that the c code that is generated
[Music]
has the same output as the trained
python model
and then finally we click build edge
and this is
right
so here we generate the
normal float
c code
from which we collect the minimax
statistics
and now we also generate the output for
the quantization compare test
this will take
a few
seconds here
and what what we're doing now is that we
perform the steps i explained earlier
where we set the min and max and the
buffers and go go through our contracts
and this will also take a few seconds
yeah
and then we run through all the test
files to compute the quantization error
and finally also we do the python
compare test
and we can see that our tests
passed
good
uh okay
so
now we have generated some c code
and it
only has floating point buffers for
input and output and it's easy to
integrate into into the firmware on any
arm cortex series mcu
even the smallest ones uh the only calls
that needs to be added or the
or
calls to one call to to initialize the
model one to enqueue data into the model
and one to dq data from the model
and
here uh here i have a
code snippet where we see that the queue
function where the lstm cell is called
16 times
and to the right
here we see all the operations that that
are needed to perform one of these lstm
cell calculations and as we can see
there are many many
calculations and and
as you can
[Music]
picture your yourself
um
we need to have the
the quantization
has to be below here and and yeah so
that's the balance in all this um
in this game
and
we have done a performance test uh using
a long chain of lstm cells and we we
have seen that the speed up is around
six times
uh running float code
versus quantized code on an mcu without
fpu
but when we have the fpu on the mcu the
performance is roughly equal and that's
that's expected i mean if we if we have
an fpu
then this is unnecessary this is
the takeaway here is that this is needed
for the smaller stems used without an
fpu and
so yeah that's um
that's my last slide
thank you so much johan that was very
very very interesting i don't think you
could get a better
university class describing in detail
how this stuff works so uh thank you for
sharing that we we do have some
questions that have come in so let's get
to a few questions and no we did not
forget about the giveaway so shortly in
a few minutes we'll be uh posting a link
where you can enter to win um
10 dialogue iot multi-sensor uh board
which is which is uh pretty amazing
sensor actually i was looking at it it
does have a cortex m0 in it with no
fpu's as we learned today the core
technology does not have fpus so in a
minute i will paste uh the landing page
where you can go in and enter for um one
of these boards that that imagine mob is
giving away to you guys for free so
let's get to the questions um
let's see uh
just general from farine what is your
opinion on the performance of
quantization aware training generally it
often results in low accuracy with some
models and data sets
um yeah quantization aware training i
think it's a great idea i
we don't have so much experience in this
we uh
we read a few papers
before we started out uh with some
strategy and and
um
yeah we we went for the option that that
seems to be
uh seems to be
yeah easier i am
so
i cannot say too much about the
quantization that we're training
but but
i i guess it's great if you get it
to work um
but um yeah
yeah and is um another quick question is
quantization of the model after training
possible
uh
after the training yeah that that is
post training uh quantization so that
that's what i described here
cool great um
and how robustness of quantization
strategy to new data that may not be
represented in the typical data set used
in the algorithm
um
[Music]
so yeah so
i would say try to get
representative data
in there
that that makes it much better uh
because
if we get some some
some strange data out here then
we we may not have
been able to adjust
the parameters to that data and then we
can get some overflow and we yeah
we lose
we lose information
yeah yeah
okay um
yeah these questions thanks so much
everybody for these questions so let's
see if we get through them all in the
next five or ten minutes here um
it's is uh
satya preet is asking is full integer
quantization equal to quantize aware
training
uh no
both approaches have only integers
but but in in in the quantization where
training you you
start out with integers you do the
training with integers everything like
with integers but in in our case here
we we do everything after the training
has completed
but but we still have everything
in in integers but there are other
variants where you mix
integers and floats and yeah you can do
all kinds of things but
but we yeah we have focused on on the
on the purest
way to do this because
we want to get rid of all the
all the floats all the noise um
uh manuel's got a couple questions so uh
clarification is is is the quantization
error is it the error of the inference
due to quantization or the information
loss on each tensor due to the
quantization
what did you what was the first thing
you said so so i think he's asking if it
is is a quantization error is it the
error of the inference due to the
quantization
or is it due to the information loss on
each tensor
due to the
quantization so he's trying to
understand where
where where a quantization error might
occur and why
yes so exactly it's a good question it
occurs
because
because when we have all these um
all these parameters that we set on on
on the tensors
the mappings on the tensors then
um
sometimes we're not able to to find
the optimal uh parameters and
we will have some some rounding
there because we haven't then we have
integer operations so we have
uh we have to do
yeah
some some decimals are are
disappearing
and
and that is the quantization error so
so
i don't know if that's um
and
an answer but
but yeah
yeah yeah i i think that's as close
manual pop in the pop in the q a box
again if that didn't quite answer what
you were uh
your question was
um
yes so maybe to clarify a bit more uh
when we compute the quantization error
that is
um
the difference between
if we would run run the model with with
the floating point operations and
floating point data
and then
we quantize the data run it through the
the quantized model and then we we check
the difference
uh
and and that's um yeah that's the
quantization error
great thank you i know it's it's dense
to try to explain this stuff so well
done on that um so just for everybody um
i just put a a
link in the chat if you click on that
that should take you to a forum for
a giveaway of the dialogue iot
multi-sensor so um have at it good luck
they're giving away 10.
um next question on slide 14 and slide
18
um
how
how will i out how do the outliers
affect the quantization process for
example
the men and the max of all tensors is
this addressed
any comments on robustness
uh let's see they said side 14
14 is that
maybe
is it this one
yeah i think so it's got the
just a question about how outliers
affect the quantization process yeah
that's like 14. yep
and then and it also referenced slide
18.
right okay i i see i see yeah
that
that is a problem um
right exactly because um
that that make uh so basically what what
the question is about is that okay most
of the data is here
so we would like to focus all our
buckets around this data here but then
we have one here
so we have to stretch out
to this one so we will get
a worse resolution
and
this is our solution to this problem is
to work with the pre-processing um
to
to um
yeah
to try to put the data together
it could it could be
um
simple multiplies and things like that
but but uh
more often we we work with ffts and so
on which
which usually puts the data together
okay
um let's see sorry it looks like the
link i posted is having some issues i'm
going to try to fix it
let's go to the next question
um
so uh do you support quantization lesser
than eight bit
um
well
um
no it's dance right now
easy enough but uh
it wouldn't be hard for us to
to put it in but but we haven't really
we didn't need that so far
okay
um
and then another question is uh
what benchmark apps have you tried on
this have you have you tried ml
0.5 for example
so what was the question what kind of
just have you tried any benchmark apps
have you have you tested the performance
on you know like a a public performance
like ml perf zero
five uh no no we we had our own tests on
this to okay yeah
yeah yeah
um
let's see and then really quickly um
where do we find function definitions uh
for example the q con
1d underscore s 16 s
16 s 16.
ah
like um like i showed in the end perhaps
yeah yeah exactly so the question was
where do we find it yeah the the
definitions for those functions
ah i say i see um
well
download the magimob ai
and and quantize the model and and
generate c code and then you will have
it
speaking of how how do how would people
get a hold of um the the imaginable
studio do you have a like an eval
license um or a trial
uh offering that that people can sign up
for and use
if you go to our
uh webpage
our homepage that you can
you can sign up there for a free trial
it's um
30 days
that you can use yeah and that's
imagimob.com
let me put the link in for everybody
here
um
to make it handy and easy
um okay and then
while this is getting loaded here we go
uh let me put this in the link the chat
for everybody
okay
uh i think we have time for just a
couple more questions so
let me grab let me
scroll through and find these here
let's see
regarding the slide on the full integer
quantization do you have any references
on the techniques to handle outliers
uh the slide or the phone
yeah so so i think back on that slide 14
with the with the outliers um is there
is there any kind of reference or
technique on how to handle these
outliers
uh no i don't have any
uh no
it's um
okay
um but but yeah like i said before we we
we work with
uh preprocessors and um
but um
but yeah it can it can be a problem
yeah yeah
all right couple couple last questions
um are you are you planning to generate
other types of binaries or formats as
output in the future
yeah that's a that's a good question we
we went for the c code
path uh already from the start that was
actually before
any any other types of
binaries
or frameworks existed uh
and
yeah we've heard from from embedded
programmers that
it's it's a nice
thing to have
access to the c code
but
uh we also have the these um
interesting partnerships where we
generate
other types of binaries that um
that
that fit in into the
um
some neural networks
accelerators
great
great
um and lastly
um since i think we're pretty much out
of time and we'll let let everybody get
on with their days is um can you convert
uh
uh oh my other way can you convert
models from other frameworks to c code
from other frameworks yes so currently
we support
the tensorflow formats but um
yeah that's what we have at the moment
pi torch
is is something we may support but uh
yeah
great okay
well johann this was
great and very very very informative
really really thorough explanation of of
of quantization this is
um
i probably will have to go back and
watch this again so that i can really
like understand some of the concepts
that you spoke about but um thank you
very much for yeah you have to look at
it again too
and you do this stuff it's hard machine
learning's hard i think you know it's
it's something that we all are trying to
make easier for people but really it's
it's hard so everybody doing it out
there you know just keep at it it's it's
um
you know with tools like uh the
imaginable studio and all these things
that that are available now you it it's
made your jobs a lot easier that's for
sure so
and it's so many different um aspects of
it it's not only the machine learning
itself i mean the quantization is is an
old topic let's say yeah for sure but we
we combine different things now and and
um yeah so it requires skills from
all over the place yeah yeah definitely
so please um everybody feel free to if
you have questions you can contact amaji
mo imagine mob directly i've put their
website in there if you you know want to
want to dig in and try out their tools
sign up for their eval
uh license and you know they're they're
they build this stuff they know what
they're talking about so they will help
you with with your projects so again
johan thank you so much it was so great
to have you here can't wait for the next
one um anders it was great to have you
here as well and everybody have a
wonderful day and evening check our
youtube channel for the replay of this
and we will see you back here again in
two weeks our next virtual tech talk
thanks everybody bye
[Music]
you