so since last time okay welcome back
thank you for being here
last time yang was using the tablet
right and how can you use the tablet I
don't use the tablet right so I should
be as cool as the Anna at least I think
one more thing to begin with there is a
spreadsheet where you can decide whether
you'd like to join the slack channel
where we collaborate over making like
some drawings for the website fixing
some mathematical notations having some
kind of you know fixing the error in
English in the English grammar or
whatever so if you if you're interested
in a help in improving the content of
this class feel free to fill out the
spreadsheet okay we are already a few of
us on the slug channel so I mean if you
want to join if we are you're welcome so
instead of writing on the whiteboard
because it's impossible to see I think
from the upper side we're gonna be going
to experiment with a new toy here all
right
first time so you know I'm a little bit
tense last time I screw up with a
notebook so okay alright so we're gonna
be talking starting with a small review
about linear algebra I hope I'm not
offending anyone I'm aware that you
already taken linear algebra and you're
very strong in it but nevertheless I'd
like to provide you my intuition my
perspective okay it's just you know one
slide not too much so maybe you'd like
to take out some paper and pen or you
can just you know whatever follow along
so so this one is going to be linear and
linear algebra review
okay am i waiting a little bit let me
wait for a sec ready yes no shake your
head okay fantastic alright so we were
talking last time we had a neural
network with the input on the bottom
then we had like an affine
transformation then we have a hidden
layer right so I'm gonna just write the
first equation we're gonna have that my
hidden layer and since I'm writing on
with a pen can you see anything yeah
so since I'm writing with a pen I'm
going to be putting a underscore
underneath the variable in order to
indicate it's a vector okay that's how I
write vectors so my H is gonna be a
nonlinear function f apply to my Z and
Zed is gonna be my linear input
okay the linear when the output of the
affine transformation so in this case
I'm gonna be writing here Z is going to
be equal to my matrix a times X okay we
can imagine there is no bias in this
case it's generic enough because we can
include it bias inside the matrix and
have the first item of the X equal to B
1 so if this X here belongs to RN and is
dead here belongs to RM first question
what is the size of this matrix okay
fantastic right so this matrix here is
gonna be our M times n you have as many
rows as the dimension where you should -
and you have as many columns the
dimension where you're shooting from ok
all right so let's expand this one right
so this matrix here is gonna be equal to
what do you have a 1 1 a 1 2 so on until
the last one which is gonna be shout
thank you 1
and yeah 1n then you have second one's
gonna have 1 a 2 1 a 2 2 so on until the
last one which is 2 n right and then you
keep going down until the last one which
is going to be what are the indexes M 1
right okay so you had a and 1 a M 2 and
so on until amen okay thank you all
right and then we have here our X right
so you have X 1 X 2 and so on until xn
right
you're more responsive than last year
good thank you alright so we can also
rewrite this one in different ways so
the first way I'm gonna write this one
is gonna be the following so I'm gonna
have here these a 1 then I'm gonna have
here my a 2 and then I have the last one
which is gonna be my a and ok and then
here I'm gonna be multiplying this by a
column vector right so my column vector
I'm gonna be write it like this alright
so what is the output of this operation
so these are metrics you have a vector
the outcome is going to be a vector so
what is gonna be the first item of my
vector I don't use dots because I'm not
a physicist actually I am that we are
doing linear algebra so what should I
write
alright so that's already transpose
because those are a row vector so I just
write a I'm gonna just mean just right
right so I have a 1 X ok so there is no
transposition here no dots around and so
on second element is gonna be a two okay
X and then until the last one which is
gonna be D okay there's no dot but sure
like someone is calling that dot product
instead of the cross product but that is
assumes like you use a different kind of
notation
all right so and this is gonna be my so
how many elements does this vector have
M okay so we have Z 1 Z 2 and so on
until Z M and this is my final set right
my vector is that okay fantastic so now
we are going to be focusing a little bit
about the meaning of this thing over
here okay other questions so far
all right this is very trivial so far I
hope I mean let me know if if not okay
okay so let's analyze one of these guys
here so I'd like to figure out what is
the meaning of writing a T times X right
so my a T is gonna be my generic a I so
let's assume in this case when n equal
to okay so what is ATX so ATX is going
to be equal to what so let me draw here
something so that is gonna be easier for
you to understand so this one is gonna
be my a are these gonna be my alpha and
then here you have like it's gonna be my
X and is this gonna be here my X I so
what is this what is the output of this
product here these are here
see you can sorry 8/8 transpose it let's
call it let's say that is a row vector
can you see know what is gonna be the
output of this operation here no why
this is this is like a generic one of
the many aids right there are M aids
this is one of those mas so I'm
multiplying one of those A's times my X
right let's assume there are only two
dimensions so what is gonna be the
output of this scalar product can
someone tell me no no no like normal
scalar product hold on
so you have egg hey here do you have
here this part here is gonna be a 1 it
is gonna be a two okay then you have
here x1 and now you have x2 right so how
do you express this scalar product here
okay so I'm just writing let me know if
you took clear so I'm gonna write here a
1 times X 1 plus a 2 times X 2 right
this is the definition clear right so
far yeah no okay yeah question yeah that
is meant to be a transpose Y a role
times column vector so let's assume a is
a column vector then I have role x
colouring so let's keep writing this
stuff here so what is a 1 how can i
compute AI a 1 second ok ok
so I'm gonna write here the a 1 is going
to be the length of vector a times
cosine alpha then what about X 1 someone
else the same right now wait what what
is X 1 same thing right different
letters so someone say something you're
following you're completely confused
you're not having any idea it's too easy
I have no idea what's going on here it's
good right so far ok what's gonna be the
second term the this one is gonna be X
here write x cos x I right and then you
had the second term which is going to be
what magnitude of a shout I can hear ok
sine of alpha and then
okay thank you okay
all right I'm gonna be just putting
together those two guys so you're gonna
get equal magnitude of a times magnitude
of x times cosine alpha cosine Phi plus
sana sine alpha and sine cosine a sine X
I sorry
what is be starting the parentheses all
right so there's the cosine of the
difference of the two angle right
everyone knows trigonometry here right
sorry high school stuff so this one is
gonna be equal to a x times the cosine
of cos x i minus alpha right oh the
other way around right from monoxides
so what does this mean you can think
about each element so far is clear I I
didn't do any magic yeah shake your head
like these four yes these four no this
for maybe no something's working okay so
you can think about whenever you
multiply a matrix times a vector that
basically each output of this operation
is gonna be measuring so okay hold on
what is this cosine how much is cosine
of zero one so it means if these two
angles if the two vectors are aligned
which means there is a zero angle
between the two vectors you can have the
maximum value of this element right
whenever when you have the minus the
most negative value when they are
opposite right so when they're in
opposition of phase you're gonna get the
most negative magnitude but then if you
apply just let's a redo you're gonna cut
all those negative things you're just
checking for the positive matches right
so neural net basically just perhaps
who's gonna be figuring out only the
positive matches right and so again when
you multiply a matrix times a column
vector you
be performing element-wise sorry scalar
product between each column each row of
the matrix which represents your kernel
right so whenever you have like a linear
layer your kernel is gonna be the whole
row of the matrix and now you see what
is the projection of that input on that
column I am on the input on that flow
right so each element of this product is
going to tell you the alignment with
which the input is what is the alignment
of the input with respect to the
specific row of the matrix okay
yes no this should shape some more like
intuition while we are using these
linear transformations they are like
allowing you to see the projection of
the input onto different kind of
orientations let's say this way okay you
can try to you know extrapolate this in
high dimensions I guess the intuition at
least I can give it to you works
definitely in two and three dimensions
in higher dimensions I kind of think it
works in a similar way next lesson we
are going to watch we are actually we
are going to see how what is the
distribution of the projections in a
higher dimensional space is this sort
gonna be so cool I think all right so
this was the first part of I think of
the class oh well there is one more more
part so actually here this said here we
can also write it in a different way so
maybe this is maybe it's known maybe
it's not known when I saw it first time
I didn't know so you know it's it's cool
that sometimes you see these things once
again maybe so let's go back here is the
things that there and so you can express
this Z as being equal the vector a 1 in
this case a 1 is gonna be the first
column of the a matrix ok and this one
is going to be multiplied by the scalar
x1 now you have the second column of the
matrix so I have a 2 this is multiplied
by the second element of the X right
until the last one which is gonna be
again I can't hear if it's M or n m like
this or end you know sign language which
one n right you know sign language no
you should learn it's good you know
inclusivity a and write the last column
times your X and of course because n X
has a size of n there are n items right
and so basically when you also when you
do a you know transformation a linear
you know apply a linear operator you
can't be basically waiting each column
of the matrix with the coefficient that
is in a you know you have first column
times the first coefficient of the
vector second column and by the second
item plus third column times the third
item and so you can see the output of
this DN transformation is a weighted sum
of the columns of the a matrix okay so
this is a different kind of intuition
sometimes you see these as like whenever
you want to express your signal your
data is a combination of different you
know the composition this is kind of a
linear composition of your input alright
so that was the first part it is the
recap about the linear algebra a second
part is going to be something even more
cool I think questions so far
no easy too easy you're getting bored
sorry okay all right then I'm gonna
speed up I guess
alright so let's see how we can extend
the one that the stuff we have seen
right now to convolutions right so maybe
convolution sometimes are a little bit
weird let's see how we can do an
extension to convolutions
all right so let's say I start with the
same matrix so I'm gonna have here four
rows and then three columns okay so my
data has to be if I had if I had this
matrix if I multiply this to a column my
column vector should be of size three
thank you all right so let me draw here
my column vector of size right and this
is gonna give you a output of size four
okay fantastic
but then is your data let's say you're
gonna listen to some nice audio audio
file is your data just three simple long
how long is gonna be your data let's say
you listening to a piece of music that
is like three minutes
how many sample does three minutes of
all your health yeah I guess what is
gonna be my sampling rate let's say
twenty two okay twenty two thousand kilo
Hertz right 22 kilohertz so how many
samples three minutes of music have
second you sure is monophonic
or stereophonic just kidding okay so
you're gonna multiply the number of
sample the number of seconds right
number of seconds times the frame rate
right the okay the frequency in this
case anyhow so this signal is gonna be
very very long right it's gonna be keep
going down so if I have a vector that is
very very long I have I have to use a
matrix which is gonna be very very fat
wide right okay fantastic so this top
keeps going this direction
alright so my question for you is gonna
be what should I put in this location
here
what should I put here so do we care
about things that are further away
no why not
because our data has the property of
locality fantastic so what am I gonna go
what am I gonna be putting here a big
zero right fantastic mom okay so we put
a zero here and then what is the other
property so let me start drawing this
stuff again right so can I have my
kernel of size three and then here I
have my data which is gonna be very long
right so hold on I can see all right so
here there are zero then let's say what
is the other property my my my date my
natural data have stationarity which
means the pattern that you expect to
find this can be kind of repeating over
and over again right and so if I have
here my three values here perhaps I'd
like to reuse them over and over again
right and so if these three values let
me change the color maybe so you can see
in there is the same stuff so I have
three values here and then I'm gonna be
using these three same values one step
for the rain and I keep going further I
don't down and I keep going this way all
right so what should I put here on the
bottom what should I put here a zero
right why that why is that because of
locality of the data right so putting
zeros around is called it's also called
padding but in this case it's called
sparsity right so this is like sparsity
and then the replication of this thing
that is over and over over again
it's called session I'd was the property
of the signal this is called weight
sharing yeah
okay fantastic all right so how many how
many values do we have now how many
parameters I have on the right hand side
well so we have three parameters over
left on the left hand side instead we
had product right so if the right side
is gonna be easy is the right side gonna
work at whole you have three parameters
one side on the other side you have 12
okay that is good using locality and
spiral and whatever not like sparsity
and parameter sharing but then now we
end up with just three parameters isn't
this too restrictive how can we have
multiple multiple parameter what's
missing here in the big picture there
are multiple channels right so this is
just one layer here and then you have
this stuff coming out from the from the
from the board here so you have the
first kernel down here now he has some
second kernel let's say listen and I
have last one here right and so you have
each plane of these metrics being
containing just one kernel which is
replicated multiple times who knows the
name of this matrix now so this is gonna
be called triplets top list matrix
okay so what is the main feature of
these topless metrics what is the big
big big thing that you won't notice it's
a sparse matrix okay okay what is gonna
be here this first item over here what
is the content of the first guy yeah so
this one here is gonna be the extension
of my linear transformation which was
you know I have a signal that is longer
than three samples therefore I have to
make this matrix fatter second part is
might be given that I don't care about
what things like things that are here
down I don't care about things that are
here if I look at the points that are up
here so gonna be putting a big 0 here
such that Oh everything that is down
here it gets cleaned up right and then
finally I'm gonna be using the same
kernel over and over again because I
assume that my data is stationary and
therefore I assume that similar patterns
are gonna be happening over and over
again therefore I'm gonna be using this
one which is written here parameter
sharing weight sharing
finally given that this one gives you
only three parameter two parameters to
work with I will use several layers in
order to have different you know
channels so this one is one kernel
before one kernel was the whole row of
the matrix okay so when you have a fully
connected layer the only difference
between a fully connected layer and a
convolution is that you have the full
dam row of the matrix so what is gonna
be in this first item over here anyone
so the green colonel let's call the
green corner just a one let me actually
make it glow green because it's green
karna so you have a one times what it's
gonna be from the number one to number
three right and then the second item is
gonna be same same dude here a 1 and
then you're gonna have the X shifted by
one and so on right make sense yeah and
then we're gonna have this one is going
to be the green output then you're gonna
have the blue output one layer coming
out and then you have the further one
the red one coming
we've been one layer out okay what's the
iPad experience cool
yes no I liked it okay other questions
so again the blue circle this one it's a
big zero
that's the sparsity one the same is here
yes no yeah yeah
yeah green so here I just put a lot of
zeros inside here so I killed all the
values that are away from the little
part and then I repeat the same three
values over and over again because I
expect to find the same pattern in
different regions of this is the big big
signal I have this one here so I said
that in this case I'm gonna have just
three values right and we started with
12 values and I ended up with 3 which is
really really little so if I'd like to
have let's say 6 values then if I want
to have six values and I can have my
second 3 on a different plane and I
perform the same operation whenever you
multiply this matrix times a vector you
perform a convolution so it just to tell
you that a convolution is just a matrix
multiplication with a lot of zeros
that's it yeah so they're gonna have
this one here then you have a second one
you have a third one so you have three
versions of the input alright so for a
second part of the class I'm gonna be
showing you some more interactive things
please do participate to the second part
as well alright so let's try so I have
rebranded I have rebranded the website
and now has slightly some like that the
environment is going to be called P PBL
so pi torch deep learning instead of
sleep learning mini course it was too
long so let me start by running this one
so
my torch deep learning so we can do this
Conda activate activate PI torch deep
learning and then let's open not the
Jupiter notebook all right so now you're
gonna be watching the going over the
listening to corners so I show you a
convolution on paper well on my tablet
now got me making listening to
convolution so can such that you can
really appreciate what this compositions
are here we said a the new kernel right
which is called P PBL Python sleep
learning so you're gonna notice the same
kind of you know procedure if you update
your system alright so in this case we
can read the top here so let me hide the
top here
alright so given the assumption of
locality stationarity and composition
compositionality we can reduce the
amount of computation for a matrix
vector multiplication by using a sparse
because local popplets matrix because
stationary scheme in this way we can
simply end up rediscovering the
convolution operator array moreover we
can also recall that a scalar product is
a simply a normalized cosine distance
which tells us the alignment of two
vectors
more specifically we compute the
magnitude of the orthogonal projection
of two vectors onto the other and vice
versa
so let's figure out now how all these
can make sense by using our ears okay so
i'm going to be importing a library that
professor here at NYU made and here I'm
gonna be just loading my audio data and
I'm gonna have that in my X and then my
sampling grade is going to be
in the other variable so here I'll just
show you I'm gonna have like 70,000
samples in this case because I have a
simple in rate of 22 kilo Hertz and then
my total time is gonna be three seconds
okay so three seconds times 22 you get
what so it's not 180 you were saying it
was a hundred and eighty was three right
oh it was three minutes oh you're right
it's three seconds so you actually are
correct my dad so this is three seconds
so times 22 kilo Hertz you get 17
roughly some 70,000 samples here I'm
gonna be important some libraries to
show you something and then I'm gonna be
showing you the first chart so this is
the audio signal I have imported right
now how does it look like wavy okay cool
can you tell me how it sounds that was a
good guess the guess was you can tell is
actually what's the content right from
this diagram because the amplitude of
like the the the y-axis here is gonna be
showing you just the amplitude can I
turn off the light is okay or you're you
sure okay thank you
I really dislike this like okay
goodnight Oh see how nice it is okay
cool alright so you can't tell anything
here right
you can't not tell what is the what is
the sound right so how can we figure out
what is the sound inside here so for
example I can show you a transcript over
the sound and actually let me let me
actually force these in your in your
your head right so you're gonna have
hold on your network
all right so now we actually heard it
okay so now you can actually see ting
ting ting boom you know you can kind of
imagine a little bit but okay so what
notes did we play there like how can I
figure out what put out the notes that
they are inside so I'm gonna be showing
this one so since this is a bit brighter
I can see your faces. How many of you can
not read this? Oh, ouch…
Okay, so let me see if I can ask for some help.
Maybe someone can help us out here.
Okay. Let's see.
Hey, hey Alf!
Oh, hi Alf!
How's going?
Yeah, yeah, I'm fine, thank you.
Nice class there!
Oh, thank you for the class.
Oh, nice sweater you too!
Nice sweater!
Oh, we're wearing the same sweater!
Can you help us out?
They don't know how to read the
Oh, the connection…
What the f- hell!
They cannot read the score!
Can you help us out, please?
Alright!
Let me try to help you out.
Thank you!
Let me switch the camera.
Alright.
Please, do.
So, here we can go like…
and hear first how it sounds everything.
So, it's going to be like that.
How cool is that?
Thank you. It took four
lessons for you to clap me. So now…
This is very nice of you. Let's keep
going.
A♭, then we have an E♭, and then a A♭.
The difference between the first A♭and the other one in frequencies
is that the first A♭ is going to be twice the frequency as the other one.
And instead, in the middle, we have the 5th.
We're going to figure out what is the frequency of that.
And then, we are gonna be going to a B♭, here.
On the left hand side, instead, we have the accompagnement,
and so we're gonna have a A♭ and B♭
And then B♭ and E♭.
So, if we put all together, we gonna get
this one.
Alright? Simple, no?
Yep! Thank you!
Bye-bye!
Byeeeeee!
See? This took a whole damn day to
prepare…
I was so nervous before come here…
I didn't know if it actually would have worked…
Both of them, tablet and this one.
I'm so happy!
Now I can actually go to sleep, later.
Anyhow, so this was like in the first
part you're gonna have the first note
there's A♭ that you have a B♭
A♭ and B so you have … and the and the
difference between the first pitch and
is one octave there for the first
frequency is gonna be twice the second
frequency okay so whenever we're gonna
be observing the waveform one sign will
have to a shorter like the half the
period of the other one right the
especially the the a the a flat on top
is gonna have a period which is half of
the period of the a on the bottom one
right so you have okay if you go half of
this one you get right okay okay so how
do we actually get these notes out from
that spectra from the from the waveform
who can tell me how can I extract these
pitches these frequencies from the other
signal any guess okay a free transform
that I think if are kind of a good guess
what does it happen if I perform now a
Fourier transform of that signal anyone
can actually answer me you cannot raise
your hand because I don't see just shout
so if you basically perform the Fourier
transform of the whole scene are you
gonna hear like the whole notes together
all together right but then you can't
figure out which pitch is playing where
or when actually in this case right
ha so we need kind of a Fourier
transform which is localized and so a
localized Fourier transform in time or
in space depending on whatever domain
you are using its called spectrograph
right and so on I'm gonna be now
printing for you the spectrogram oh
sorry
and I'll be printing here the
spectrogram of this one here and so here
you can compare the two alright on the
first part here on this side here you're
gonna have this peak here at 1600 which
is the the higher power there we go now
you have a second one which is this peak
here you can see this peak right and you
see this peak yeah okay so these Peaks
are gonna be the actual notes I play
with the right hand
so let's actually put those together and
I'm gonna have here the frequencies so I
have 1600 1200 and 800 can you see here
I have 1600 and 800 Y one is double of
the other because they are one octave
apart so if this is this gonna be okay
and this is a fifth which is like a also
has a nice interval so then we actually
generate these signals here and then I
gotta be concatenate the mouth so I'm
gonna be playing both these the first
one is actually the original audio but
let me try again
the second one the concatenation yeah so
it's a bit loud I can now cannot even
reduce the volume oh I can reduce it
here too much okay let me go again all
right
so this is the concatenation of these
four different pitches so guess what we
are going to be doing next so how can I
extract all the notes that I can hear in
a specific given piece so let's say I
you play a full score and I'd like to
know which pitch is play at what time of
what so the answer was convolution just
for the recording I'm asking convolution
of what no convolutional spectrogram so
you have convolution of your input
signal with what with some different
kind of pitches right which pitches will
you pick let's say you don't see the
spectrum because let's say I'm I just
play any kind of piece of music so I'd
like to know all possible notes they are
there what would you do
you don't know all pitches how would you
try that right so in which are all the
pitches you may want to use if you're if
I'm playing the piano all the keys of
the piano right so if I have if I play a
concerto with the piano then I want to
have like a piece of audio for each of
those keys and I'm gonna be running
convolutions of my whole piece with the
old kids right and therefore you're
gonna see hits which are the alignment
of the cosine similarity
whenever you get basic
the audio matching your specific kernel
so I'm gonna be doing this but with
these specific tones I actually extract
it here so here I'm going to be showing
you first how the two spectrograms look
like the left side is going to be the
spectrogram of my actual signal X of T
and on the right hand side I have just
the spectrogram of this concatenation of
my pitches so here you can clearly see
this but then here is first of all what
are these bars here these vertical bars
you follow me right I can see you had to
actually talk back what are these red
bars here vertical bars now it don't
tell van I already told you right
these are and the vertical what what is
it sampling issues right transitions so
whenever you have deep you actually have
one white waveform one waveform and then
the other one one away from his to stop
so it's no longer periodic and whenever
you do a Fourier transform of a non
periodic signal you get you know crop
that's why whenever you get the junk to
the junk junction between these the jump
here you're gonna have this spike
because it's gonna get basically you can
think about the jump is like having a
very high frequency right because it's a
it's like a delta right so you actually
get all the frequencies that's why you
get all the frequencies here boom okay
makes sense right so far kind of alright
so this is the green version that I
cannot even sign and what
left side here why is on the left-hand
side all red down there okay
yes you know so the left side left hand
the cold are the one I show you on the
bottom left side okay so let me finish
this class and then I'll let you go so
here I'm gonna be convoyed show you
first all the kernels you can tell now
the red one is going to be the first
chunk of my signal they want the real
one and then you can see the first pitch
is gonna be the has the same frequency
can you see
so the problem have the same same same
delta T the same interval the same
period can you see you cannot nod your
head because again I don't see you had
to answer me can you see or not okay
thank you fantastic and so this one is
that the third one you can see that it
starts here in the period-- and it
finishes here if you go up here you're
gonna see exactly there were two of
these guys right so that's why you can
see that's how you can see this is like
twice the frequency of the one below
finally I'm gonna be performing the
convolution of these four corners with
my input signal and this is how we look
like okay so the first kernel has a high
match in the first part of the of the
score so between zero and zero five
seconds the second one starts just after
after the first one then you have the
third one starting at zero three I guess
and then you have the last one
starting at the zero six okay and so
guess what I'm gonna make you listen to
convolutions now are you excited
oh okay you actually answering now good
alright and so these are the outcomes
let me lower a little the volumes
otherwise you complain yeah I can't
lower the
okay so the first one let's try again
how cool is that it's just listen to
convolution okay so basically this was
on shit I have one more slide because I
felt like there was some confusion last
time about what is the different
dimensionality of different type of
signals so I'm really recommending to go
and take the class of young Bruna which
is math for deep learning and I stole
one of his you know small things he was
teaching I just put a one slide here
together for you so this slide is the
following so okay so we have the input
layer or the samples we provide into
this network and so usually our last
time I define this I have this curly X
which is gonna be made of those x i's
which are all my data same post right
and so I usually have em there are
samples so my I goes from n 1 to n ok so
is this clear on is this notation clear
because it's a bit more formal I use AMA
less formal but then somehow someone was
feeling a little bit not comfortable I
think so this one is just my input
samples but we we can also see this one
is this curly X which is my input set as
the set of all these functions as x i's
which are mapping my Omega capital Omega
which is my domain to a RC which are
going to be basically my channels of
that specific example in so here I'm
gonna be mapping those lowercase Omega
to these x i's of omega so let's see how
these different this from the previous
notation so I'm gonna give you now three
examples and I you should be able to
tell now what is the dimensionality and
is all in this example so the first one
let's say
I have likely one and show you right now
in the time just fun piece of you know
audio signal so my Omega is going to be
just the samples like sample number one
sample number two like the index right
so you have index one index to index
until these 70,000 whatever we just saw
right now okay and the last value is
going to be the T capital T which is the
number of seconds divided by the delta T
which would be the you know 1 over the
frequency and this is going to be a
subset of n right so this is a discrete
number of samples because you have a
computer you always have discrete
samples so this is my input data and
then what about the image of this
function so when I ask what is the
dimensionality of this type of signal
you should answer this is a
one-dimensional signal because the power
of these n here is 1 ok so this is like
a 1 dimensional signal although you may
have the total total time and sorry the
1 there it was a sampling interval on
the right hand side you have the number
of channels can be 1 if you have a mono
signal you can have 2 if you have a
stereophonic so you have mono there you
have a 2/4 stereophonic or what is 5
plus 1 that should all be like 5.1 how
cool alright so this is still a one
dimensional signal which may have
multiple channels nevertheless is still
one dimensional signal because there is
only one running variable there ok is it
somehow better than last time yes no
better thank you
let's say thanks to John alright second
example I have here my Omega is going to
be the Cartesian product of these two
sets the first set is going to be going
from 1 to the height and also this one
in discrete and the other one is going
to be going from 1 to the width so these
are the actual pictures and so this one
is a 2 dimensional signal because I have
2 different 2 degrees of freedom in my
domain what are the possible channels we
have so here the possible channels that
are very very common are the following
so you can have a grayscale image and
therefore you only output one
scrape one scalar value or you get the
rainbow there the color and therefore
you get like my X which is a function of
the coordinates w1 sorry Omega 1 Omega 2
which is not be the each point is
represented by a vector of three
components which is gonna be the R
component of the point Omega 1 Omega 2
the G component at Omega 1 Omega 2 and
the blue component of Omega 1 Omega 2 so
again you can all think about this as a
big big data point or you can think
about this as function mapping a low
dimensional domain which is a two
dimensional domain to a three
dimensional domain right finally the
twenty who who knows the name of the
twenty channel image yeah this is a
hyper spectral image it's very common to
have a better than 20/20 bandwidth but
then sorry finally cooking gas this one
my if my my domain is r4 x r4 what can
it be
no no this discrete right
this is r4 so it's not even computer ha
who said something that I heard yes is
this correct so this is space-time what
is the second one
yeah which momentum has a special
special name it's called for momentum
because it has a temporal information as
well right
and so what's gonna be my possible image
of the X function let's say it's equal 1
what is it do you know
so this could be for example the
Hamiltonian of the system okay so this
was like a bit more mathematical
you know introduction or mathematical
procedure how do you say you'll make
make a more precise definition so that
was pretty much everything for today let
me turn on the light and let I see you
on next Monday
thank you for being with me