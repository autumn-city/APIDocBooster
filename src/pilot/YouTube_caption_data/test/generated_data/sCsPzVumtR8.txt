[Music]
hi everyone I'm Tong Joanne from Matt
Matthew Cecil so today we using some
great advances in pi torch about the
Jade the name cancer and makes precision
trainer so many other things and today I
want to talk about another aspect in
python that has been improved
significantly in the last two releases
that is the if this works okay alright
that is the path towards data loading
pipeline so the way I want to structure
my five minutes is I want to give a
brief introduction about how the
components within Python data loading as
well as the server updates we've made so
first let's take a look at the three
components in PI to our data loading one
of the core object in the pathogenicity
desert the desert is essentially a
Python object that represents a
collection of the other samples and the
way we do those in PI torch is that we
have this map like interface where you
give a single index and the opting or
dealers and part of it and if you look
at this code segment and ignore the
where the indentation issue and it takes
an index and resource file and basically
returns it and the next component is the
sampler so now we have a data set that
texting index and feeds out a data
sample but we also need to specify which
sample we want so the sampler is
essentially what we use to specify the
data loading order and it is can be
viewed as a string of data set indices
here is a simple example about how you
can implement the sampler that just
feeds out sequential index from 0 to n
minus 1 and finally probably the most
important one that you will use is the
data loader object the data loader
object is the entry point of the data
loading pipeline and it combines the
data set as well as December simpler and
also provides functionalities has such
as single and multi says multi processed
data loading as well as automatic
collision into batches so here if you
take this data loader and specify the
batch size equals four for example the
way it the thing it does under the hood
is that every time you ask for data from
this data loader it fetches the next
four in
this is from the sampler as and
individually one by one as the data's
had to get a little sample from it and
collate the list of four samples into a
single fashion
am i back all right nice okay great
cool yeah so back to the other loading
we have these three components in pipe
impact watched a loading the data set
the sampler and the data loader and if
you you probably have noticed that
there's a big constriction there's a big
constraint in this three object that is
the data set only lost one sample at a
time
and this could be quite efficient and
could potentially add very big overhead
if in cases where say you're like
reading data from a database where you
really want to do POC loading so the
first thing we did is that we reelect
relax the constraint and allow you to
relax the API so the data set is now not
only can handle individual samples but
again all those also handle loading from
the list of indices into batch samples
this enables essentially Bach loading so
this is a very simple code segment that
illustrates how this works and since
everything is matching the season batch
samples we also need to update our
sampler to return the best indices at
every time and this is a very simple
sampler that just keep returning the
list of activities and now the dataset
instead of in the guitar Tom method
instead of handing a single index it
handles a list of indices and you can do
things like work to a database and
return it as collated sample because the
dealer said not natively handles the
collation returns the batch sample and
the data loader doesn't need to do the
batching so we specify back sides equals
tune on here to signify that to tell all
order and everything should just work as
this and additionally we notice that
there's there is so much more types of
data cells than just like maps out that
assess so oh so in certain cases you
potentially want your dealer said to be
an infinite data stream or even some
locks generated in real time so in this
cases you might not have like really
clear definition between your mapping
from indices into data samples but
really you have a stream of data so this
is what we call it worried about it as
that you know in this case
if trajectory data set if you're working
with robotics or reinforcement learning
and you should realize there there's a
glut of algorithms that essentially
working infinite streams of trajectories
sampled from the environment and now
instead of like inheriting from the data
space cause you can heard from the
iterable dealers at base class and
overwrite the aether method which
essentially yells all the trajectories
you want and by using this interface it
essentially also gives you like finer
control of data loading in the sense
that your batch size and the data and
data order can become dealer infant data
dependent right so here I have a simple
example where your DV queries could
timeout for example if you're like a
particular art is bad or in the indexes
pet or something like that so you can
actually filter your data based on
though the content you get from the
query and you can even do fancier things
like mark which indices of that and so
you don't really get to example them in
the next iteration cool
so to recap before we have this like
single index to a single sample data set
and now we introduce two new types of
services one is the badge the badge
style which allows your back loading to
enable use cases like going from the
database and or you can do like data
streams the Creator bottle assess where
you can even generate the data set from
say logs from real time or even I don't
know like from infinity crawler and
stuff like that so here are the two
major improvements we've done to della
loading which hopefully can make the
already more effective and efficient in
padua and thank you
[Applause]