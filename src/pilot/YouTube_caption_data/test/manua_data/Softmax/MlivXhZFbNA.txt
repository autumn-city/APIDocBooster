- Hi everyone, and welcome
to the second lecture
of Deep Learning Systems,
Algorithms, and Implementation.
This lecture's going to be a refresher
on the basics of machine learning,
as well as a walkthrough
of softmax regression,
as an example illustration
of all the ingredients
of a machine learning algorithm.
Now this lecture's going to
be a fairly quick run through
of kind of basic machine learning.
And the expectation here is
that this should be a refresher for you.
You should have seen this
in some form or another beforehand.
Now, if you've haven't,
if you've seen it in a
slightly different form,
that's okay, but if this is
all brand new information to you,
then you probably want to look at kind of
a more basic machine
learning course before taking
this more systems-focused,
and a little bit more advanced course.
So let's jump right in talking
about two things today.
We're going to talk about the
basics of machine learning.
And then as I said, using this example
of softmax regression or
multi-class logistic regression
as kind of an illustrative
example of these ideas.
All right so first of all, the
basics of machine learning.
Let's suppose you have the following task,
which in fact you will
have in your homework.
Suppose you want take images of digits,
like the one you see here on the right,
and write a program
that will classify these
into one of nine, 10 different categories.
the digits zero through nine.
Now there are a few
ways you could do this.
But kind of the traditional, I would say
basic programming way to
deal with this problem,
is you sort of think
hard for a little while
about the nature of digits.
You think what makes a zero a zero,
at least as far as an image is concerned?
What makes a one a one, et cetera?
And you try to encapsulate this
logic in a computer program.
This is how we sort of
learn to code most things.
And when this is something
like writing a program
to sort a list.
This is actually a great
way to do things, right?
Because we can kind of
structure logically the steps
of what it takes to sort a list.
The preconditions, the
post-condition of each step,
and build the logic that way.
But for many tasks like this
image task, for example,
this ends up being a pretty hard problem.
So I consider myself a reasonable coder,
at least normalized to professor level.
And also, I guess,
excluding some outliers,
like the other instructor of this course,
but normalized across all professors.
I'm a pretty good coder.
And I would find this task very hard.
I don't think I could do it very well.
Because again, computers don't
see images the way we do.
We look at these digits, we
immediately see their form,
their structure, all this kind of stuff,
but computers don't see that.
And to build all the
logic about how you create
what makes a three a three,
and things like this?
What are the shapes involved?
What pixel values?
How do you turn a grid of pixel values
into an actual classification of a digit?
That would be quite hard
for me actually to program.
And so we need to look, or at
least we would like to look
at some other way of solving
these kinds of problems.
And this is exactly what
machine learning gives us.
Now, the machine learning
approach to this problem,
and I should be clear this is really
the supervised machine learning
approach to this problem.
Supervised learning is the majority
of what we'll cover in this class.
But we will cover a bit
of unsupervised learning
later on in the class.
But the supervised learning
paradigm for this problem
is to say, it's hard for
us to write a program
that can classify those digits.
But what's pretty easy for us
to do, relatively speaking,
is collect a bunch of
images of digits along
with their known labels,
or their known target
that we want to predict.
And then, so we take a
collection of these things,
which we call our training data.
Our training data essentially
consists of these images.
So this thing on the left here is supposed
to be an image of a four,
along with its true label four.
And precisely because we are good at this,
as humans, we can do a good job with this.
And so it's fairly
straightforward to do this
to collect a training set like this.
Now what we do then is we
take those training data
and we feed it into a
machine learning algorithm.
And what this algorithm produces that
is a Model h, produces some
model, which we call h.
Or hypothesis, or really you can think
of this as a mini program.
And the way this program works is that
if you feed in examples like these images,
it will produce the output four,
at least approximately speaking.
And so if I feed in examples,
like we see in the training
set, in the training data,
this will produce the desired output.
Now, of course, what's
important about this,
if you do this well, it will work not just
for examples that were
in our training data,
but it will also work for
pictures in general of digits.
And we actually won't talk
about the intricacies here
of generalization and what makes
a good and generalizable function.
Because what we're going to cover really
in the majority of this class,
and in this course is this box here.
And what I want to run through today
is basically what does this box contain?
What does it actually have inside of it?
And so what this box contains
is really three things.
And in fact, every machine
learning algorithm,
more or less consists of just
three different elements.
And the three different
ingredients are the following.
The first is the hypothesis class.
So this basically specifies
what is the structure
of that Model h that we actually are going
to create with our machine
learning algorithm?
Now, typically we say that this model
is parameterized by a set of parameters.
And what these parameters do
is these parameters specify
the exact nature of this mapping.
They specify sort of how
a particular instance
of a model will map from
its inputs to its outputs.
That's the first ingredient
of machine learning.
The second ingredient
is the loss function.
And the loss function specifies
what makes a good hypothesis?
How do we characterize when a hypothesis
is performing well on this problem?
And the last element of machine learning
is an optimization procedure.
And this essentially is the way
that we optimize over the parameters
to find a set of parameters
that approximately minimize
the sum of losses on our training set.
And what's really amazing is that
all machine learning algorithms,
whether they be deep learning algorithms,
or simple sort of linear
classifiers, or gradient boosting,
or really anything you can propose.
All of them, at least supervised learning,
though unsupervised
learning actually also fits
this mold to a certain degree as well.
All the algorithms that we have
in machine learning fit
this basic structure.
And so even though there seem
to be hundreds, thousands
of machine learning algorithms sometimes,
it's kind of hard to keep up
with just how many there are.
One thing that's very helpful
is understanding how each
of these algorithms fits
into this framework.
Because there aren't really thousands
of machine learning algorithms,
there's basically one.
There's one algorithm that sort of has
to just specify these
three different components.
And then you have your
actual sort of instantiation
of a machine learning algorithm.
All right, so what we're going to do
with the rest of this class
then is look in some detail
at softmax regression,
also known, as I mentioned,
as multi-class logistic regression.
And illustrate these basic components
in the context of softmax regression.
Now, as I said again, I'm going to repeat,
we're going to go through
this quite quickly.
And some of these topics if
you have not seen before,
this would likely be
too fast to go through.
But what I want to do is just highlight
how softmax regression exemplifies
these different elements.
And really you can do the same sort
of thing for any algorithm,
but we're going to use softmax
regression as an example
before next class, then moving
to basic neural networks.
All right, before I actually define
these three ingredients
though, for softmax regression,
I want to start off
with our basic setting.
So our setting here
that we're talking about
is multi-class classification.
In particular, a k-class
classification setting
where I say there's k different classes.
And what we have in the setting
is we have training data
where the inputs to our
system are vectors, x^(i).
The notation here x^(i) being in R^n,
means that x^(i) are n dimensional vectors
with real valued quantities.
So essentially each x^(i)
would look something like this.
It would look like a big vector specifying
the different elements
from x^(i)_1 to x^(i)_n
specifying the different
elements of x^(i).
And in the case of, for
example, these images,
these images of digits, you just saw,
this could be these elements here will be
the different pixel value
intensities in the image.
The next thing we have is the outputs,
or desired sort of the desired
outputs of our algorithm.
Also, called the targets or outputs,
or they're called
several things like this.
And these outputs, y^(i)
are scalar quantities,
which really they're,
because we're talking about
a classification challenge,
or classification problem,
these are discrete values
that range from one to k, where
k is our number of classes.
And finally, i here indexes over
the examples in our training set.
So i ranges from one to m and indexes over
the different samples or
examples in our training set.
So this particular notation,
I'm going to be consistent
with it throughout the course here.
And it's important to sort of remember
these different things,
because they will do things like effect
the dimensionality of the
vectors we care about.
But throughout this lecture
and for the majority
of this course, n, is going to represent
the dimensionality of the input data.
So this is how big of
a vector is our input?
k will be the number of classes,
how many outputs we're trying to predict.
Or how many different classes
do the outputs take on?
And m is going to be the number of points
in our training set,
the number of examples we
have in our training data.
And so, for example,
in the case of our MNIST
digit classification problem,
in that particular
example, n is equal to 784.
Because our digits in that
case are actually represented
as grids of 28x28 pixels.
And each position, each pixel position has
a grayscale scalar value.
It actually ranges from
zero to one, think of it as.
And this is how we represent our image.
So there are 784 dimensions of our image.
There are 10 classes in our example there,
because we have 10 different categories,
we're trying to categorize each digit as.
Namely the numbers, zero through nine.
And finally, in this case, there happen
to be 60,000 examples in our training set.
So n, k, and m, take on these values
for the example of the MNIST
data you will work with.
But of course for other problems
they'll take on different values.
All right, now let's talk
about the hypothesis function
that we think of when we talk
about k dimensional classification.
And in particular, we'll also talk about
the linear hypothesis class
that softmax regression uses.
But first more generally
speaking a hypothesis function.
Remember, this is sort
of that first ingredient
of our machine learning program,
is a function that maps
vectors in our end.
Those are the inputs that the
dimensionality of our input,
to real valued vectors in R^k.
Now this is a bit confusing,
or it might be a bit seem
counterintuitive at first,
because we think of hypothesis
as generating our outputs,
which are the numbers
one or one through k.
But in this case for various reasons,
which we'll cover in a second,
we really want hypothesis
classes that output
a bit more smooth indications
of how likely in some sense,
in some very informal sense
we think each output is.
And so in the classification setting,
we don't actually have outputs
that are just the discrete classes.
We have the output of
our hypothesis function
again, being this k dimensional vector,
where k here is the number of classes.
And in particular, in this setting h_i,
So that would be the ith
element of our output.
So our output remember is vector valued.
So we would write
something like h(x) equals
a vector where the first
component is h_1(x)
all the way down to h_k(x).
And here the ith component indicates
in some sense how likely
the class is to be class i.
So for example, this
component here would give
some indication of how likely
the class was to be class one.
Next one, class two, and down to class k.
And I'm using the term sort
of likely or belief here
in very big quotation marks,
because these are not yet probabilities
or anything like that.
We're going to actually talk about sort of
a probabilistic
interpretation in a moment.
But for now, these should just be thought
as some indication of the likelihood,
or I shouldn't even use that word.
The belief that the
output is in fact class i
for this element h_i.
Now, in the case of a
linear hypothesis class,
it is a particular
instantiation of this type,
of this general hypothesis function
where h takes the following form.
All right, we see the h_theta,
and we use this notation h_theta here
to denote the fact that
theta are our parameters.
And h is really a, even
though we think of h
as a function of inputs to
outputs, h is really also
a function of the parameters,
depends on our parameters.
So we write h_theta.
And in this case, we're going
to write that h_theta(x)
is equal to just theta transpose times x.
Where theta here
is a matrix, which has
n rows and k columns.
And just sort to make sure this
these sizes make sense, right?
Let's make sure that in fact, this all,
this is a legitimate operation,
theta would be an n by k matrix.
So theta transpose,
and we just use the
transpose actually kind of,
for reasons of convention
in the one dimensional case,
this makes a bit more sense.
So we often use a transpose
here. You don't have to use it.
In fact, we'll give it
up shortly when we talk
about the more standard
notation for deep learning.
But transpose just swaps the roses
and columns of a matrix.
And so the
theta transpose would be a
k by n dimensional matrix.
Times an, well, we think of
vectors as being column vectors.
This would be an n by
one dimensional matrix.
So their product as desired would be a, k
by one dimensional matrix, really i.e.,
a, k dimensional vector.
So we are in fact, correctly mapping
from n dimensional inputs
to k dimensional outputs.
And in fact, in some sense,
this is a linear transform
between those two dimensions.
It's in some sense, one of
the simplest possible mappings
or hypothesis functions
you could really have
that maps between these two quantities.
So before we move on to the,
this is actually going to
be the hypothesis function
for softmax regression.
But before we move on
to the second element,
which is the loss function,
we want to take a brief detour and talk
about matrix batch notation.
So it's often more convenient.
We sort of in a previous
slide, wrote out the hypothesis
as a function of a single example.
So applied to this single
example in our training set,
this is the form of the
hypothesis function.
But it turns out in many
settings to be more convenient,
to write things, not in
terms of the application
of that hypothesis to a single example,
but to many examples at once.
And we're going to do
this with matrix notation
for our operations.
Now it turns out this
is actually more than
just kind of a mathematical
nicety as well.
This is also going to be
really important when we come
to implementing these
operations efficiently.
Basically, because matrix
operations are much more efficient
than carrying out many vector operations.
And because of this, it
winds up not just on GPUs,
but also on CPUs being
incredibly important
to implement things in matrix batch form
for efficiency of execution.
And so this matrix batch notation,
I'm sort of harping on a little bit here
because I don't just want the emphasis,
it's not just a notational
nicety, though it is.
Once you get used to it's very nice.
What it really is, is it's a way also
of carrying out these operations
more efficiently in code.
And so you want to be familiar not just
with how to write these things
as a bunch of for loops,
but how to write them really
as efficient matrix operations.
So we're going to define the,
a batch matrix, big X,
which is going to be a
matrix, which is m by n.
Remember m is the number of examples,
and n is the dimensionality of the input.
And what this matrix is,
is this matrix is just
a stacking of all the
examples in our training set.
So in particular, the first element,
or the first row in this matrix
is equal to the first example x^(1),
but because x our examples,
we typically think of vectors
as column vectors.
To make it a row vector,
we have to transpose it.
So the first row is
this transposed vector.
Second row would be the
second example, transpose down
to the mth example transpose.
And that's going to be our
design matrix containing
all the examples in our training set.
We're going to do a similar thing
for the outputs, the targets.
We're going to form a m dimensional.
Now it's just a vector, in fact,
because each output is scalar valued.
They're discrete, but
they're still scalar valued
between one and k.
And so we think of a
stacking of all these things
as just a vector,
an m dimensional vector, which contains as
its first element y^(1) and y^(2) all the
way down to y^(m).
And the nice thing about this notation
is that not only can we apply it
to the basic matrices X
and y we can also think
of the hypothesis function as applying
to an entire batch at a time.
So for example, we can write h_theta(X)
overloading notation a little bit.
We can write h_theta(X),
as being equal to the stacking of
all the elements, h applied
to each different element
in our training set.
So the first row of this quantity here is
just equal to the hypothesis
applied to the first example.
The next row, the second example,
all the way down to the mth example.
But the really nice thing of course,
is that we can write this
in much more efficient,
much more compactly than even this.
If we look at what this
actually is, well, what is this?
This is just going to be, well,
the first row of this thing,
our hypothesis applied to x^(1) would be,
that would be theta transpose x^(1),
but that whole thing transposed.
So let's just write that
a little bit more briefly
as x^(1)
transpose times theta.
That's the first row of this matrix.
And similarly for all the other rows.
So the second row would be
x^(2) transpose times theta
all the way down to x^(m)
transpose times theta.
Extend these a bit.
Those lines there just mean
that sort of I'm emphasizing
the fact that it's a row, the whole row.
They're not negative signs or anything.
But now, if we look at
this also, we can see,
well, this is just kind of the definition
of a matrix multiplication.
So this thing here just equals,
the first element is x^(1) times theta,
the next x^(2) times theta, et cetera.
This is just of course equal
to our big matrix X times theta.
So the nicety of all this is
that we can find very efficient ways
to write these seemingly
complex expressions,
like our hypothesis
applied to every element
of our data set.
All right, so let's now
move to the second element
of the softmax regression algorithm,
which is a loss function.
How are we going to evaluate
the quality of our predictions?
Well, the most obvious thing
we could do in some sense
is just, this is classification,
let's just measure the prediction
by whether or not it's correct.
This is something called just
the error of the classifier.
It's a very sort of simple,
it's the most obvious loss function.
And so all it is, so we can measure
the error between our prediction here,
and the true label via a loss function
that we just call either
the zero one loss,
or the error, we're going to
call here called the error loss.
And this is sort of a very
intuitive loss function,
but we can write it formally too.
We can write it formally
as this, the prediction,
the loss that we suffer on the prediction
is going to be zero basically,
if our prediction is right.
But for a classification, remember what
it means to be right, is that
we should be most confident
in some sense about the correct class.
And again, with our
hypothesis function mapping
from R^n to R^k, what this means is
that this quantity h_i(x) we want,
for our prediction to be
correct we want that quantity
to be the largest at the true class y.
And the way we can read this formally
is we say this loss is zero if the argmax,
so if the maximum index.
The index corresponding to
the maximum value of h_i
is equal to y.
And this is very simple.
I mean, all this is saying
is that if the biggest entry
in our prediction is equal to, corresponds
to the correct class, we suffer loss zero,
otherwise we suffer loss one.
And really actually, this
is very often how we assess
the quality of classifiers.
It almost goes without
saying, it's almost so obvious
that we don't even bother
saying this in some sense.
But this is very frequently
how we actually assess
the quality of classifiers.
If you ever, whenever you report error
of your classifier,
accuracy of your classifier,
this is how we do it.
Of course, accuracy being
one minus the error.
What the average error of a classifier is,
is just the average loss.
This loss averaged over
the whole data set.
So this is a very intuitive loss function
to use for classification.
The problem is this loss function
is very bad for optimization.
It's not very easy to
find a set of parameters
that minimize this error loss.
It actually winds up being,
for even a linear classifier,
an NP-hard problem to find a classifier
that achieves the
minimum number of errors.
But maybe more fundamentally, another sort
of very annoying thing about this loss
is that it doesn't
provide any information.
It's not actually what
we call differentiable.
And what this means is that
if we vary our hypothesis a little bit,
this loss typically won't change.
It won't change locally
around our current set of parameters.
Because you're probably not right
on a switching point
where one thing switches
from one to the other.
And even if you are in switching point,
that's sort of a point of also
a non-differentiable point
because it's sort of switches
back and forth immediately
between zero or one loss.
And this is very bad for
actually optimizing parameters,
because as we'll see in a second
in really all, not all machine learning,
but in probably all of deep learning,
the most common way of
optimizing, finding parameters,
of optimizing parameters
of finding parameters
that are good are via
gradient-based methods
via derivative based methods.
And so this loss function is very bad
for actually optimizing over parameters.
Even though it might
be good for quantifying
the performance of classifiers.
And so what this leads us to is another,
a different loss function that we're going
to use for softmax regression,
called either the softmax
or the cross-entropy loss.
This term cross-entropy
loss has become kind of
the standard in most
machine learning these days.
And so we will define it in
fact, as the cross-entropy loss.
And in order to define this
loss we're going to convert
between our hypothesis output.
In some sense, these
predictions that correspond
to some kind of vague notion of belief.
We want to convert that into something
that looks more like a normal probability.
So we want to take our
hypothesis function,
these real valued quantities,
and we want to convert
that into a probability.
And the way we're going to do
this is we're going to define
the probability that
the label is equal to i.
And we're also going to call
this term sometimes just z_i
just for convenience here.
Now, what do you need from a probability?
Well, a probability has to be positive,
and it has to sum to one.
And our hypothesis output,
h_i that is not positive.
It can be negative too, and
it doesn't have to sum to one.
So how do we make it sum
to one and be positive?
Well, the first thing we can do
is we can just take our hypothesis, h_i,
take this term, h_i, and
just exponentiate it.
So now that will make it positive.
So we just exponentiate any
number positive or negative.
The result will always be positive.
But they won't always sum to one,
so that sort of solves the first problem,
the probability of positive,
but that doesn't solve the second problem,
where probabilities
have to the sum to one.
So the thing we can do next is
just normalize this quantity
by the sum of all the outputs.
So we're going to take our hypothesis
at index i, exponentiate it,
and then we're going to
normalize it by the sum
of all our exponentiated hypotheses.
And now this quantity is
in fact, at least obeys
the basic principles of a probability.
It is positive, and the sum
over all the probabilities
of all the labels will always equal one.
Another way of writing this is to say,
in kind of in vector forms is say that z,
the vector of all the probabilities
is the normalize function,
that just means we divide
by their sum, applied
to the exponential of
the hypothesis function.
One thing that will come up a
lot is that whenever we apply.
Whenever we take kind of
a scalar function here,
like the exponential and
apply it to a vector,
all this means all this term here means,
is it just means the exponential applied
to each element in that vector h(x).
And so we're going to
frequently apply kind of non,
or scalar functions to this.
And you kind of have to infer what we mean
just by the context here.
So normalize, actually is
a little bit more complex,
because it looks at the entire vector.
But all this normalized
function does is, for a vector,
it makes the vector
normalized to sum to one.
All right, so that's how we're going to,
And you may be familiar
with this operator.
This is also called a softmax operator.
It's sort of a standard way of mapping
between arbitrary real
value components and
the output of, and a
probability distribution.
Now you may, if looking
forward a little bit here,
if you are familiar with neural networks,
you may have seen things like
this where people often take
this operation, which they also call,
this is also called a softmax
operation, I should say.
This is also equivalent
to the softmax called
of just h(x).
So softmax is just the
combination of the exponential
and then normalizing it.
You may see some of
this in neural networks,
people define the output of a network
to be the softmax applied
to something before that.
But that actually for various reasons,
that's actually not the
right way of doing things,
in my opinion.
The output, and even
in a setting like this,
the output of a linear classifier is just
the linear function,
not the softmax applied
to linear function.
The softmax, and these sort of things,
at least at the final level,
final sort of loss function
layer becomes an operation
that sort of is embedded
in the loss itself,
not an operation you apply to the network.
I'll come back, to back to that in second.
All right, so now that we
have a probability here
that we sort of mapped from
our arbitrary hypothesis output
to a probability, we need
some way of quantifying
whether our probabilities are good or not.
Whether our vector of probabilities
is a good set of predictions or not.
And this is now fairly
obvious what we might do,
a good measure of how good
our prediction is just
is the probability of the true class high.
So what's the probability of
our, under this notation
what's the probability
of the label being the true target, y?
And so we want to make that
probability as large as possible,
but for sort of reasons of convention,
we typically think of loss functions
as being things that we minimize.
So we want to make something
small, some error small.
So we could minimize them, I guess,
the negative probability.
But minimizing probability is
actually for various reasons
is a bit not very well
numerically conditioned.
So what we typically do is we
take the log of this thing.
And this is a very common thing.
This is called the negative log loss,
or this sort of very common
way of deriving loss functions,
where we define our loss function.
And in fact define the
cross-entropy loss function
as just the negative log probability
of the label being the true class under
this distribution here.
And we can write that out
a bit more explicitly.
We can just of write this out
a little bit more concretely
here, as also equal to.
Well, let's just sort of
take the log of this thing.
So we have the negative
log of the numerator first,
which is the exponentials the log
and its exponential cancel.
And we just have negative h_y(x).
So it's a negative, the y component.
Remember y here is a discrete
value between one and k,
so we take the yth
component of our output.
Then minus the log of the denominator.
But then we also subtract off
because we're taking the log of
the denominator we subtract that off.
So we're fact adding the log of the sum
from j equals one to k, of
the exponential of h_j(x).
And that is called the cross-entropy loss,
or sometimes the softmax loss,
or just the multi-class logistic loss,
sometimes it's called.
But these are all really the same thing.
And the name cross-entropy loss
is the most common these
days in machine learning.
And that's how we define
the loss function we're going
to use in softmax regression.
Now, as I said before,
the right way to think about this is
as this loss function applied
to a linear hypothesis class,
not sort of this negative
log being the loss,
or this negative log here being the loss,
applied to some softmax of this thing.
And there are subtle reasons for that.
For example, if you look
at the convexity properties
of this thing that this
problem ends up being,
well, for softmax regression
will end up being a convex problem.
That's much easier to see here.
You also, typically,
unless you really have to
don't want to actually
form a softmax operation.
Because if the number
becomes numerically zero,
and you're taking the log of a zero,
then things can blow up.
So it's much better to think of the output
of hypothesis class
being a linear function.
In this case, these things are for those
that've heard the term
before these are also called
the output of hypothesis class
will also be called the logits, sometimes.
When they are then fed into
this cross-entropy loss.
But this is the right way of doing things,
both I think conceptually,
but also numerically.
It ends up being much nicer just sort of.
You don't want to actually compute
this softmax operation here explicitly.
You want to compute this loss function,
applied to a linear hypothesis class.
All right. So now with
that, let's talk about
the final ingredient
of softmax regression.
Which is actually going to
take the most time to cover,
because we have to of discuss a little bit
about how you optimize these things.
But before we do so
again, this is going to be
the third and final ingredient
of softmax regression,
which is going to be the
optimization problem.
How do we find good values
for those parameters theta?
But before I do that, I
sort of want to specify,
or just mention the fact that
what the third ingredient
of our machine learning
algorithm really is doing
is solving the following
optimization problem.
We are minimizing over theta.
So what this notation means
this notation here is notation
for an optimization problem,
that means we're searching
over all possible values,
or at least trying to search
over all possible values
to find some value of theta that minimizes
the quantity here on the right.
And this quantity is
just going to be the sum
from i equals one to m, I guess
it's really be the average.
So the average from i equals one to m,
of our loss function, so our loss here,
applied between our
prediction on the ith output,
and the actual ith output.
So I want to take a little bit of time
to emphasize this problem
here, because this problem,
kind of stated as such,
I kind of think of this as
the core machine learning
optimization problem
that describes all machine
learning algorithms.
Every machine learning
algorithm in one way or another
is trying to find a set of
parameters that minimize
the sum of, or the average loss,
between some predictions
and the true labels
on the training set.
And in fact, this formulation
here really includes
therefore, all the aspects of
a machine learning algorithm.
It includes the hypothesis function.
It includes the loss function,
and it includes the optimization problem.
So really this problem, maybe
with some slight changes
like adding regularization,
but we typically don't even do that
in deep learning that much.
But this problem really is
the core machine learning problem.
And every algorithm, every
machine learning there is
in one way or another,
solves a problem like this.
At least every supervised algorithm,
probably every unsupervised algorithm too.
And arguably every reinforcement
learning algorithm, or not.
That one's a little trickier.
Certainly every supervised
learning algorithm
is just a different take
on solving this problem.
And so we can even look
sort of concretely,
what does that look like then
for something like softmax regression?
Well, in sort of the
simplest way of writing it,
it's just pretty similar, right?
We're minimizing over
theta that part's the same.
We're trying to find
a good values of theta
that minimize all we're going to do here
is plug in the specifics
of the softmax regression.
So we're minimizing
the cross-entropy loss,
and our hypothesis class
happens to be a linear one.
So it's going to be theta
transposed times x^(i) and y^(i).
And this is the
optimization problem we want
to solve for softmax regression.
Now, of course, the question is, well,
how do we actually find that?
How do we find the value of
theta that minimizes this,
or at least the value of theta
that is this sort of matrix
of parameters, remember.
The way we define about this function,
how do we find the values that provide
a good mapping between inputs and outputs?
And so the way we're going to do this
is via a technique
called gradient descent.
But to cover gradient
descent, I first want to,
I first go in a little bit of a digression
about a quantity called the gradient.
And so let's suppose that we have
a matrix input, scalar output function.
All right so a mapping from,
in this case, I'll just use
our actual mapping theta,
which is n by k vector
to a real valued vector.
And actually going back to the
previous slide for a second.
The thing I want to emphasize
is that this whole quantity here, in fact,
the thing that we're trying to minimize
for the purposes of minimization,
we can think of this
as just a function of theta, right?
So the quantity that
we're going to be taking
the gradient of later on is exactly going
to be this optimization objective.
But for is such a function.
So for is some function f theta.
How do we actually start to minimize
that function over theta?
Well, one sort of very
nice thing you can do
is use this quantity called the gradient.
And what the gradient is,
the gradient is a vector,
I'm sorry, a matrix of partial
derivatives of this function.
And in particular, if the function,
if theta itself or the
input of the function
is n by k dimensional,
then importantly here,
the gradient, which we write as
this little upside down triangle.
The gradient with respect
to theta of f(theta),
that's how we sort of write this.
This is also going to be an n by k matrix.
And it's going to just be a matrix of
all partial derivatives of this function.
So the first element would
just be the partial derivative
of f(theta) with respect to theta_11.
Going to the last, or the
last element in the first row
would be the partial
derivative of f(theta),
with respect to theta_1k.
Down here, partial derivative
with respect to f(theta).
theta_n1, and finally all the way down
to the partial derivative of f(theta)
with respect to theta_nk.
And these partial derivative signs,
what the partial derivative just means
is that for the purposes
of that derivative,
for the purposes say,
of this derivative here,
you are treating every
other element of theta
besides theta_11, as
if it were a constant.
So every other element
that you differentiate,
you would just treat as
if it were a constant term
that were not a function of theta_11.
That's what the partial
derivative here means.
So this is the definition of the gradient.
And now one thing I
really should mention here
is that this notation here
is admittedly really bad.
But it's the thing people use,
and so we're going to
have to get used to it
to a certain extent.
And the reason why it's so bad is
that this first usage of theta here,
this is actually saying what parameter
we're differentiating with respect to.
So if f has one argument,
you actually don't even need
that subscript you're always taking
the gradient of its argument.
But as we'll see later,
if the functions have a lot of arguments,
you want some way of specifying,
which argument you're
differentiating with respect to.
And so this is of what this
subscript is saying here, right?
It's just saying which argument
we're differentiating with respect to.
This argument here is
an actual value of theta
that we're differentiating at.
It's the point that
we're differentiating at.
And so it causes quite a bit of confusion,
but the reality is you have
to just get used to this.
So, sorry, notation is bad.
We use a lot of bad notation
in machine learning.
But I'm intentionally
including bad notation
rather than using better notation.
Some people use an index here,
they'll use one, or zero
or things like that.
And that's objectively better
notation, but it's not common.
So I'm going to use the
common but bad notation,
and you'll have to just
unfortunately get a bit used to it.
Now, the nice thing about this gradient
is what those gradient does is it points.
Remember from sort of 1D Calculus, right?
The derivative of a function was equal
to the slope of that function.
Well, that same intuition
holds in higher dimensions too.
And so if we have some function,
so this function here is a function where
this would be the minimum,
and these would be points of
the function getting bigger and bigger.
What the gradient does,
remember the gradient
is sort of itself,
a matrix or for a vector case.
If the input function was a vector value
it would be a vector.
It points in the direction
of greatest increase of f,
at least locally, so if we're
right here, this is theta.
The theta value we're
evaluating the gradient at,
then the gradient would point in
the direction of maximal increase.
And that's a very, very powerful notion.
We don't need to somehow
search all possible values of f
for the one that actually
causes it to increase the most.
With a little bit of
calculus, you at least
in a local sense, have the direction,
already immediately have the direction
in this parameter space that increases
the function the most.
And that's sort of amazing.
You can get it basically,
and as we'll see,
there are typically exist ways
of computing gradients very efficiently.
This is what the whole lectures
of automatic differentiation will cover,
but we can compute it very efficiently.
And in doing so, we can
really easily figure out how
to change or how to optimize
our parameters theta.
So how do we do that?
Well, this leads us very immediately to
the core algorithm I would argue
of almost all machine learning.
So how do we then use this
fact to optimize our function?
Well, if the gradient
points in the direction
of maximal increase, then if we want
to minimize our function, which is again,
the convention for minimizing losses.
What we can do is we can set theta
to be equal to theta minus
some multiple of its gradient.
So here alpha is what's
called a step size,
or a learning rate.
It's just some small, positive quantity.
So we subtract off some
small positive multiple
of the gradient.
And it should make sense
that these are all the
correct size here, right?
Because theta here in our case is n by k.
The gradient's also n by k.
Multiplying by a scalar
just scales the whole thing.
And so this is sort of a
valid update we can do,
all the sizes check out.
And the key idea though, is
because the gradient points
in the direction of greatest increase,
if we want to minimize the function,
then subtracting off a
small amount of the gradient
causes that function to decrease.
Now, and this actually,
before I go into the details of this,
I should just take a moment to say that,
to sort of appreciate the
power of this approach.
So it is not an exaggeration
to say that this basic idea
of gradient descent
powers all deep learning.
Maybe not, there's probably a
few gradient free approaches
that people occasionally use.
But 99% of all deep learning,
and because deep learning
is so prevalent here,
99% of all, or 90% say
of all AI that you see.
So those images of dogs, dog
professors, you saw last,
the last lecture,
the large language models, AlphaFold,
they're all basically built
using this one line of math.
That for some reason,
actually is also still
not often taught in under basic,
their first undergraduate
course, which I think is crazy.
This is such a fundamental algorithm
that it is really hard to overstate,
just how impactful
gradient descent has been
on really the field of machine learning,
but arguably even the
world as a whole right now.
It's really an amazing algorithm
that is foundational at this
point to all deep learning.
And of course, when I
say gradient descent,
I mean gradient descent and its variants,
really any gradient-based
optimization technique.
But it is still amazing how prevalent
these things really are.
Now the one sort of big
question you have here,
you may have looking at this,
and you will continue to have it even
as you get very experienced
with machine learning probably,
is what value we should
pick for this alpha here.
How big of a step do we take?
And it turns out well, it actually happens
a lot in the course, but the choice
of alpha here really very greatly affects
the behavior of this method.
So if you take a small alpha,
so I'm showing here just
another simple function.
So this is these blue lines here,
correspond to level sets of the function.
And it's going to bold out like this.
And if you take small gradient steps,
you sort of make small slow progress.
You sort of go here, and then here,
and you kind of make
small, steady progress
toward the optimum.
If you take bigger steps, you sort
of make pretty good, fast, rapid progress,
much more rapid progress
toward the optimum.
But if you take two large steps,
you start kind of bouncing around,
and not being optimal anymore.
And in fact that we
take a much bigger step,
We can take such a big step
that we actually overshoot
and increase our function each time
and kind of diverge off to infinity.
Wouldn't quite follow that path.
It would bounce around a bit differently,
I guess it would go like
here and here, whatever.
We did that last one.
But here, and sort of shoot
around this thing somehow.
I have a hard time drawing their paths,
but basically you'll start to diverge
and not converge to anything.
So the choice of step size here
is really, really important
when it comes to choosing how
we optimize this function.
And in fact, as a quick preview
for a few lectures from now,
if you've heard about optimization methods
like Adam or,
gradient descent with
momentum, things like this,
what those essentially
are, are those ways to try
to accelerate this process without having
to worry quite as much about step size.
That is a huge simplification, I know.
But basically a lot of slightly
more advanced optimization
methods are just ways
of trying to optimize these things
with a little bit less
effort in picking step size.
All right, and now finally,
what I'll say is even
the last algorithm we
described is not quite
the thing that we really
do in deep learning.
Because if we look at our loss function,
what is the thing we're
trying to optimize?
Again, we're trying to
minimize over theta.
This is now again for sort
of general machine learning,
not just softmax regression.
The average of the loss applied
between the hypothesis,
I'm sorry, that's h(x^(i)), and y^(i).
And if we were to take the
gradient of this whole thing,
well, the gradient of
this whole thing would
just be equal to the sum
of all the gradients of it.
So the gradient of this
whole thing would be equal
to the sum from i equals
one to m of the gradient
with the theta of that thing.
And the problem there is
that if I get more data,
this just to be clear, this is a sum
from one m, very clearly.
If I get more and more data
to apply gradient descent
to my kind of core optimization procedure,
I would have to do more and more work
for each gradient step, which
is really kind of wasteful.
It's sort of bizarre to
think that if in the limit
of having infinite data, we
will actually make no progress,
because we will just forever
be computing our gradient,
which doesn't seem to be the right thing.
And there's also other issues too.
In deep learning in particular,
if we try to compute the gradient over
all your data, you will run out of memory.
You can't fit all your data in memory,
let alone the terms you need to compute
the gradients in memory.
And so you'll actually
not be able to do this.
So what deep learning really does
is not run classical gradient descent,
but it runs a variant called stochastic
gradient descent, or SGD.
Now SGD kind of in theory
is can be formulated
as kind of this stochastic
minimization procedure
where you have sort of
sampling random variables,
which are in expectation,
have the expectation
of the true gradient, but
have some variance to them.
That's sort of a mathematical
interpretation of SGD.
What SGD in practice means is that you
just split up your data set
into what we call minibatches.
So these are basically are what these are,
are these are subsets
of your data of size B.
So big B here is what
we call the batch size,
or the minibatch size.
And what we do in
stochastic gradient descent
is we first sample a minibatch of data.
So we sample B examples,
or maybe more than even sampling them.
What we really typically do is divide
the whole data set up
into however many we need
to have each one be size B.
And then just cycle over
all these subsets of size B.
So we take the subsets of
size B where we have both
the, I guess we should
emphasize, we have both
the input matrix of the
stacked formula is B examples,
and the corresponding outputs.
And we update the parameters
by taking a gradient step
as if our loss was just those B examples.
And so the way to think about this is
that each step we take here is sort of
a rough approximation
to our true gradient,
but one that we can
take much more quickly.
And actually there are even
sometimes an advantage of this
in the case of deep
learning in particular,
because we often having
a little bit of noise
actually can be helpful in some cases too.
So it's sort of a noisy approximation
of the thing we think we would
ideally like to optimize,
but it's one that we can
optimize much, much quicker.
So basically rather than summing over,
just to emphasize this
on our equation here,
rather than something over all m we could
just sum over all B and
that's much faster to compute,
because there are only B of them.
Which is you know, hundreds say,
instead of millions,
potentially of examples.
And now I can sort more formally say that,
in fact, this is the algorithm that drives
all those advances you've seen
in deep learning over recent years.
This, and very minor variants, are how
for the most part we train
every single deep learning algorithm.
That we will talk later
about how we actually,
some of the more kind of
common methods like momentum,
or like Adam and things like this.
All right, so that's it, this
is the optimization procedure.
It's the third ingredient
of our softmax regression algorithm.
But to finalize it, we have one last step,
which is how do we actually
compute the gradient
of our objective of the sum of losses,
with respect to our parameters, theta?
In other words, how do
we compute the gradient
which is but the theta,
of the thing we want to optimize,
which is the sum of a
bunch of these terms.
If we can compute the gradient
of one such term we can
just sum them all together
to get the gradient
of our entire objective.
So how do we do this?
Well, it turns out it's
actually not that hard,
but it's not that easy either.
And in fact, when I
learned machine learning,
this is what we did.
You devise a new objective
and you go through by hand
and derive all the gradients.
We live in better times now.
And the reality is most of the
time when you develop models,
you will not have to do this.
In fact, the whole point of
automatic differentiation,
which is what we're going
to cover in a few lectures,
is an algorithmic way
of defining just what
the hypothesis and loss
function look like.
And then basically letting
the program automatically
compute gradients for us.
Automatic differentiation is
just a programmatic way
to create gradients.
But for this lecture and for
the next lecture, actually,
I'm going to, kind of show you
what you've maybe seen before.
We'll pretend we don't know that yet.
I'm going to show you kind of
what you may have seen before,
if you've taken the
machine learning course.
Because to write an automatic
differentiation tool,
you will have to at least
take some gradients.
So you will have to compute
some gradients here.
And I'm going to take you
through kind of, I guess,
how we used to do machine learning,
which is manually deriving the gradients
for all these things.
And yes, we really did
do this back in the day.
The dark ages, even though
automatic differentiation goes
back to the '70s, it
really did not take hold
until deep learning really took off.
This was sort of the genesis
of these tools being
really widely popular.
All right, so how do we
compute this gradient?
Well, it turns out it's
actually not that easy,
but we can break it down step by step.
It's not that hard either.
So let's first do the following.
So it's computing this
whole thing, actually,
it is a bit tricky.
Doing it correctly is
actually very cumbersome
if I'm honest, but let's start off
and we're going to do it kind
of incorrectly in a moment.
Let's start off with
doing a simpler problem.
This term here is a matrix,
this here is the terms of vector.
It gets a bit, they're sort of,
you have to play the chain
rule with that kind of stuff.
Let's take a simpler problem.
And first just think about treating h,
not as the hypothesis class,
but just as an arbitrary vector.
It's actually going to be
one step of our derivation.
And let's see if we can differentiate just
the compute just the gradient
of our cross-entropy loss,
treating its argument h as a vector.
So I basically want to
compute this thing here,
but remember the elements of
the gradient are just equal to
the partial derivatives, this thing here.
So I'm going to just first
to compute my gradient.
I'm going to first compute what
the partial derivatives are.
So let's just do that.
So this is going to be equal
to the partial derivative
with respect to h_i of well what?
I'm just going to write
our loss function again.
So this will be negative h_y.
You can go back a few slides,
but just trust me I'm just
going to be rewriting now what
the cross-entropy loss actually is.
Plus the log
sum from i equals one,
or I'll actually use j,
because I used i to index over examples.
So j equals one to k,
of the exponential of h_j.
I'm just writing the definition
of the cross extropy
the loss using h as now,
as arbitrary vectors.
Okay so what is this?
Well, this first term, let's just do this.
What's the derivative
of this first term here?
Well, that will be zero unless i equals y.
Otherwise it'll be one,
or otherwise it'll be
negative one actually, right?
So if y equals i, this
derivative of this term
is negative one, otherwise it's zero.
The way we can write this is
as the negative indicator, one indicator
of i being equal to y.
All right, what about this next term?
What is the derivative with
respect to h_i of this term?
Well, now we have to just sort of do it.
So let's play the basic chain rule.
So the derivative of the log of something
is the derivative of the thing
inside divided by that thing.
So this is going to be equal to plus
the derivative of this thing,
you think inside the log.
So that's going to be the
derivative with respect to h_i
of the sum from j equals one
to k of the exponential of h_j.
All divided by the sum from j equals,
I'll write it like this.
j equals one to k of
exponential of h_j.
But now,
this numerator here, well,
if I'm taking the derivative
of the sum of all these exponentials,
again, I'm taking a partial derivative
to the h_i of this sum.
And because all the terms
but h_i are considered to be
considered to be constants.
In fact, here I'm actually,
I mean, the reason why I used j there,
by the way in this case was
because I'm differentiating with the h_i,
not actually cause of the example.
So this is just because I'm
differentiating with h_i here.
So I had to use the sum over j.
But taking the derivative
of this inside thing here.
Well, that's going to be zero
for all j, except j equals i.
And for that one j equals i term,
well, the exponential, the
derivative of the exponential
is just that same thing.
So this whole thing here
is going to be equal to
the exponential of h, well
parenthesis there, of h_i.
Divided by, let's see if I can write this
without getting in the way of it.
The sum from j equals one
to k of exponential of h_j.
h_j
But this thing here should
look very familiar to you.
It's just that same softmax
operator we had before, right?
It's the exponential of
the hypothesis divided
by the sum of the exponentials.
I.e. the softmax, i.e., normalization
of the exponential of h.
So we can write this
whole thing much nicer,
the whole derivative now
much nicer in vector form.
In particular, we can
write this whole thing as
the gradient of the cross-entropy
loss with respect to h,
which is equal to z.
Which is going to be here z equals
normalizing the exponential,
really, exponential of
h, or the softmax of h.
Minus a term that has
a, minus this term here,
which this term here is going
to have a zero everywhere,
except the one in the yth position.
And a very common way of
writing that is as, oops,
z minus e_y.
Where e is called the unit basis.
Okay. so just to highlight
everything in the end,
this term here,
is our final gradient of
the cross-entropy loss
with respect to
the input, its input,
treating its input as a vector.
Okay.
So are we there yet?
So close, but so far in a way.
Because how do we compute
what we really want?
We actually don't want
the gradient with respect
to the input, to the loss.
We want the gradient with
respect to theta of this thing.
How do we compute that?
I mean, the way you would do it, right,
is you use something like the chain rule.
because we want the gradient of
with respect to something inside,
you can use the chain rule.
But the dimensions here are nasty.
And this is like the simplest possible
thing you can imagine, right?
It's a linear hypothesis function,
but this term here is a matrix.
This term here is a vector.
So we want the derivative of a
vector with respect to a matrix.
It's like some odd tensor.
It turns out this gets really
cumbersome really quickly.
And so we need some sort of more
powerful or more kind
of almost a rigorous.
because what we're about
to see on the slide,
sort of throws that out the window.
But we need some sort of more general
and generic way to take derivatives
when we have multi-variate,
multidimensional matrix,
matrix quantities, et cetera.
Eventually tensor
quantities, stuff like that.
So how do we do this? Well,
there's two things we can do.
We can first do the right thing.
And to be clear, the right thing is,
you can define
generalizations of gradients
like Jacobians that
actually are the derivative
of a vector output with
respect to a vector.
Or the matrix of all
partial derivatives there.
But when you have then matrices,
you're differentiating with respect to,
you need to use things
like Kronecker products and vectorization.
but you can formalize all of this
using matrix differential calculus.
So there really are ways
to sort go through formally
what every single derivative
you want to compute here,
or every single sort of gradient,
and partial derivative
and Jacobian, here it is.
You can also use out term by
term, but that's even worse.
You can do this all very formally
with matrix differential calculus.
We're not going to do that though.
What we're going to do is the horrifying,
but thing that everyone actually does.
Which is the following.
And if you haven't seen this before,
this is going to be
like a revelation to you
If you've done this derivation before,
and never realized that
what people actually do
is this thing, so I'm like
pulling the cover back here
on the big secret.
But it's not even a useful secret,
because it's how we
used to do these things.
So this is how we used to do
them and now it's useless.
But anyway, pulling back
the big secret here.
The thing you do to compute multivariate,
Jacobians, gradients,
derivatives, whatever.
Is you just pretend everything's a scalar,
apply the normal chain
rule and then rearrange,
or transpose your outputs,
such that everything works
in terms of the sizes.
And I really want to highlight
this sort of horror emoji here.
It is suitably, you should be
suitably horrified by this.
So this is what actually
is done in practice,
and it is a bit horrifying,
But this is what people really do.
And it actually gets, you get
sort of used to it quickly.
The thing I will say though,
is what's very important
to do if you ever do this
is to check your answer, and
check your answer numerically.
So we'll show actually
in the next lectures,
how you go about checking
derivatives numerically.
You'll do it in the second homework.
This is because so much can go
wrong with this first approach.
You can use it as a hack,
but then check your
answers after the fact.
But this is literally
what we do when we want
to compute nasty gradients,
or nasty high dimensional
derivatives, whatever.
It doesn't always work,
but it works a surprisingly
large amount of time.
And so always at least try this first.
Okay. So how does this actually work?
So let's say, let's try
to compute this quantity.
Let's try to compute the derivative
of the cross-entropy loss
of our hypothesis function.
This is the thing we actually want
to compute the gradient of.
But let's do it assuming
everything is kind of a scalar.
And I'm still going to use the
partial derivative sign here,
even though I should
probably use just the normal,
not scripted d's at this point.
But I'm going to use the
partial derivative sign
to kind to remind me that
I'm horribly cheating here
and should never actually do this.
But let's just do it.
Let's pretend all of this
really is scalar valued,
and kind of go from there.
All right. So what is this?
Well now we can apply the normal.
So if we're computing this derivative,
now we can apply kind of
the normal chain rule.
So the thing would be the
derivative of the loss,
the cross-entropy loss
of theta transpose x.
transpose x and y with
respect to theta transpose x,
that argument to the cross-entropy loss.
Times, and doesn't matter
what times means here,
because they're all scalars.
It just means scalar multiplied by
the derivative of theta transpose x.
And obviously we don't need transposes,
they're all scalars now,
but we'll still just include it,
because you can always
transpose a scalar, that's fine.
With respect to theta.
Okay. So that's what we do.
So what are those things?
Well now, if they're all scalar,
we actually can write this
this out really easily.
So this first thing here, this first term,
this is actually just the derivative
of the cross-entropy loss
with respect to its argument.
It's exactly what we had last time.
So this is actually going to
be equal to exactly what's
on the two slides ago.
It's going to be z minus e_y.
Were here z, just like before,
is equal to normalizing
the exponential of in
this case, the hypothesis,
which is going to be theta transpose y.
Times, well, this one here
when they're all scalars
is really easy, the
derivative of theta times x,
with respect to theta is just x.
All right.
Now we're almost done.
All we have to do now is talk
about what size these things are.
All right, so this thing here, z and e_y,
those are both k dimensional vectors.
So this is a k dimensional vector,
really a k by one dimensional matrix.
And this term here is x, so that's
just going to be an n by one.
n by one dimensional vector,
or again, n dimensional vector,
an n by one dimensional matrix.
All right. So how do we
compute this gradient here?
The gradient we want is n by k.
So how do we arrange those two terms,
this term and this term
to make an n by k dimensional output?
Well, the way we would have to do it,
and the only option we have
is to take this and say,
this is going to be x times
z minus e_y transpose.
That makes an n by k dimensional matrix.
And that in fact is the correct
gradient that we are after.
So yes, this is very embarrassing
that we do it this way,
but we do it this way.
And in fact, even in
some of the deriving some
of the gradients for individual operations
in automatic differentiation,
you will also do it this way.
Okay, so the funny thing about this is
this even works for the batch case too.
So we can, again, abusing notation a bit,
we can write the loss
function we really want
for a whole minibatch,
which is sort of the loss function applied
to the hypothesis of a whole minibatch,
which is just X times theta.
We can write this all
in the exact same way.
So things actually are
exactly the same as before.
If I want the derivative of
this with respect to theta
of loss, cross-entropy
loss of X theta, and y.
Well, that just will equal
the partial derivative
of the cross-entropy loss of X theta.
And y, this is now matrices is
the only difference from before.
Partial derivative with
respect to X times theta.
And then derivative of X
theta with respect to theta.
This thing here would equal,
well, I'll write it like,
I'll write it like
big Z minus I_y.
What this is saying here
is this is big Z here
would be something like normalizing...
Taking the hypothesis applying it to all
the elements in our matrix x,
and then normalizing by rows.
So it would be like normalize,
and that'd be normalize here by row.
I'm going to run out out space,
I'm going to lose myself.
Of exp of X theta.
Okay, that's what big Z is.
And I_y that's like a one-hot encoding,
like the e_y in each row.
It's like one-hot encoding
of the output classes.
But then times X as before.
And now, we just do the same exact thing.
So this matrix here,
I'm running out of space right below.
But this matrix here would be m by k.
Because we would have each row
would be kind of these terms.
And we already know that
X here, X here is m by k [NOTE: X should be m x n].
[NOTE: X should be m x n].
Right, so how do we get our final product?
Which again, our final
product is still n by k.
Well, what we do is we take things,
to transpose them properly,
it would be X transpose.
So X is m by k, so we want k by m.
And then times this thing here.
Times Z minus I_y.
And that, whoops let me
just highlight it, this here
is the actual gradient we want.
So you know like five, how
many characters this is,
you know, 10 or so characters,
that's the actual gradient we want.
And then you might sort
of course, you know,
why is this transpose, like
why are these tranposes here
kind of reversed from this one?
Remember, our big matrix X here.
We had the rows are already kind of
the transpose versions of
the individual entries.
And so to compute this sort of properly
you have to transpose them depending
on how you set up these matrices.
But this thing here is the
final expression we're after.
It's actually somewhat confusing.
This is now a matrix, this
is another matrix here.
It's quite cumbersome to do this properly
with Kronecker products, and tensorization
or vectorization all that kind of stuff.
It's but again,
once you've derived it you
can in fact check numerically
this is the correct thing,
and it is in fact the correct gradient.
So I just want to call you out
to the title of this slide again.
It is rather embarrassing
that we do things this way.
You will eventually not do
this at all in this course,
because of course, this
course is going to be
about how you build automatic
differentiation tools
that lets you avoid this.
But this is how we used
to do machine learning.
This is used to be the
trick that everyone sort did
in machine learning, and maybe
no one really talked about.
But now you know the secret
too, if you want to do kind
of old style machine learning.
One thing I will mention
here before I go on,
and we're pretty much done now,
but the one thing I will
mention here is that
it's really easy to screw this up.
These sizes are really important.
So if you, for example,
choose k equal to n,
then it's really easy to
mess up the dimensions here.
So you have to choose the dimensions
as different as possible as you could be
to make this all work.
All right, but now we are done.
So despite what I admit is
a fairly complex derivation,
what we are left with
for softmax regression
is something incredibly simple.
And I really want to
emphasize this simplicity.
What does the final softmax
regression algorithm look like?
Well, you split up your data
your training set into batches.
You iterate over these batches of size B.
So you iterate over your
training, batches of size B.
And for each batch, you
just update the parameters,
according to this gradient rule.
I guess there's sort of one, I mean,
obviously you have to compute
what Z is to really do this.
But this is five, six
lines of Python code.
All those derivations you
saw ultimately translate
to an incredibly straightforward,
simple algorithms to implement,
which you will actually
implement on the first homework.
And that is the entirety of
the softmax regression algorithm.
So despite some non-trivial
math to get there.
You should be acknowledge the fact
that this is not completely
trivial, these derivations here.
Even though we kind of cheated our way
into the hard part of the derivation,
this is ultimately kind
of a one line description
of the actual resulting algorithm,
which you can implement again
in a few lines of Python code.
And if you do it and implement
this on the MNIST data,
you'll find you get about
little bit less than 8% error.
So that is probably better,
again 10 away classification,
one of 10 classes with
a linear on this class,
you're able to get 8% error.
That is probably better,
maybe I'm not quite sure,
but that's probably better
than what I could do by hand.
So that's really, really impressive.
And what we're going to do next lecture
is we're going to do the same thing
basically, in the exact same way.
The only difference is we're
going to use neural networks.
But what that means is just
a fancier hypothesis class.
We're going to use the
same basic loss function
a cross-entropy loss.
We're going to use the same
optimization procedure,
gradient descent, or
stochastic gradient descent.
Naturally, computing the
gradients is a bit trickier,
but the procedure is the same.
And the only difference for
deep learning in some sense,
or for neural networks is that we use
a fancier non-linear hypothesis class,
instead of the linear hypothesis class
of softmax regression.
That's going to be next time.
And for one more lecture next
time we are going to go through
this, the old tedious way.
And after that, we'll go
through it the nice way
where you can now after next lecture,
I guess after you've implemented
automation differentiation
tool, you can forget
about gradients and let the
automatic differentiation tool
that you write, actually
do all this math for you.
In the proper way I should mention too.
All right. See you next time.