 And  having batch norm before the activation, that's usually how  yeah, that was originally how it was proposed in the paper. 
 And this is  essentially it's in that way, step one of batch norm is  similar to the input standardization. 
For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.  However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].
BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice. 
 That's something you would also  usually do or could do when you compute the input standardization. 
