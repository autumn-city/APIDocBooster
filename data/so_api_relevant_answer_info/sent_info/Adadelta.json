["The thing you need to know about AdaDelta is the general context of online machine learning.\n", "What update rules like AdaGrad and AdaDelta do is to provide a \"nicer\" way to perform updates --- this can mean things like making sure the parameters converge to their optimal value faster, or in the case of AdaDelta, it means that the parameters x \"step closer\" to their optimal values in appropriately-sized steps, with the step size changing based on past performance.\n", "AdaDelta is purely an update rule.\n", "because the point of AdaDelta is to make the learning rate into a dynamic value rather than forcing us to choose an arbitrary value for it at the start.\n", "Adadelta optimizer seems to have no support on GPU nor TPU.\n", "When you call tf.train.AdadeltaOptimizer(...).minimize(), TensorFlow creates additional variables, which are not covered by the init op that you created earlier.\n", "...after the construction of the tf.train.AdadeltaOptimizer should make your program work.\n", "This is likely to be very inefficient, and the Adadelta algorithm will not adapt as expected because its state is recreated on each step.\n", "For example [AdaDelta optimizer (hyper-link)] has this in the docs:\n", "Defined in [tensorflow/python/training/adadelta.py (hyper-link)].\n", "[https://pytorch.org/docs/stable/_modules/torch/optim/adadelta.html (hyper-link)]\n", "In the original version of Adadelta you don't have to set an initial learning rate.\n", "Now, let's compare the [Adadelta implementation (hyper-link)] of Keras to the [original paper (hyper-link)]:\n", "comes directly from the underlying Keras [source code for Adadelta (hyper-link)].\n", "Adadelta decay factor, corresponding to fraction of gradient to keep at each time step.\n", "This algorithm is very similar to Adadelta, but performs better in my opinion.\n", "If you really want to use Adadelta, use the parameters from the paper: learning_rate=1., rho=0.95, epsilon=1e-6.\n", "Adadelta has a very slow start.\n", "I think Adadelta performs better with bigger networks than yours, and after some iterations it should equal the performance of RMSProp or Adam.\n", "Here is my code to play a bit with the Adadelta optimizer:\n", "Adadelta optimizer has a way to adaptively change learning rate but still, it needs an initial value of learning rate.\n", "Although as you can see in tensorflow [source code (hyper-link)] to achieve the exact results of Adadelta paper you should set it to 1.0:\n", "Adadelta is an adaptive learning rate method which uses exponentially decaying average of gradients.\n", "However, note that, by default, decay parameter for Adadelta is zero and is not part of the \u201cstandard\u201d arguments, so your learning rate would not be changing its value when using default arguments.\n", "I suspect that decay is not intended to be used with Adadelta.\n", "The first entry point is [python/training/adadelta.py (hyper-link)] from tensorflow main repo.\n", "For example, in [core/kernels/training_ops.cc (hyper-link)] you can find CPU impelmentation of ApplyAdadelta op.\n", "Use an adaptive gradient algorithm like Adam or Adadelta or RMSProp.\n", "Adadelta for example does not.\n", "AdaDelta is solid as well though.\n", "then follow up by accessing attributes of torch.optim.adadelta without ever having explicitly imported torch.optim.adadelta.\n", "By doing this, people can't accidentally take a dependency on being able to use torch.optim.adadelta after importing only torch.optim, so the developers are free to refactor it to move implementations of specific classes and other APIs around, without making special efforts to ensure import torch.optim also imports all those submodules just to preserve behaviors from code that performed improper/incomplete imports.\n", "But Adadelta doesn't: self.weights = accumulators + delta_accumulators\n", "So, although the weights should be saved, you're looking at the wrong variables, and Adadelta seems to have a buggy code.\n", "If you use decay with Adadelta, you should probably manually save and load iterations or create a custom copy of the optimizers code where you add iterations to the weights changing the line above with:\n", "The Adadelta for instance, has accumulators and delta_accumulators.\n", "I was able to use the default learning rate with the Adadelta optimizer.\n", "keras adadelta learning rate is 1.0, tf adadelta learning rate is 0.001.\n", "tf.keras adadelta uses a default learning rate of 0.001 while keras uses 1.0.\n", "Here's an answer why adadelta should be avoided : [How to set parameters of the Adadelta Algorithm in Tensorflow correctly?\n", "In fact, the problem is with the optimizer, not the model, to validate that, you can try training a keras model with tf AdaDelta, it will also show poor results.\n", "<tensorflow.python.keras.optimizer_v2.adadelta.Adadelta at 0x7f8c7fc3ce80>\n", "<keras.optimizers.Adadelta at 0x7f8c7fc3c908>\n", "The true culprit is the default learning rate used by [keras.Adadelta (hyper-link)] vs [tf.keras.Adadelta (hyper-link)]: 1 vs 1e-4 - see below.\n", "It looks like the default learning-rate of Adadelta optimizer in Keras is 1.0 while it is 0.001 in tf.keras.\n", "When you switch to tf.keras, the learning-rate of Adadelta is too small that the network does not learn anything.\n", "It also seems like you are using Adagrad for your Keras model, but you are using Adadelta for your Lasagne model.\n", "It seems that it is because of optimizer='adadelta'.\n", "the default learning rate for Adadelta optimizer in keras version is\n  1.0 and in tensorflow.keras is 0.001.\n", "So to fix the problem try to use optimizer = tensorflow.keras.optimizers.Adadelta(lr = 1.0) instead of optimizer='adadelta'.\n"]