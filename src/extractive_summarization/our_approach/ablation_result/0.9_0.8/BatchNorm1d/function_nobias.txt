It is simple: BatchNorm has two "modes of operation": one is for training where it estimates the current batch's mean and variance (this is why you must have batch_size>1 for training).  The other "mode" is for evaluation: it uses accumulated mean and variance to normalize new inputs without re-estimating the mean and variance. In this mode there is no problem processing samples one by one.
 Yeah, and also  note that now when we use batch norm, batch norm has learnable  parameters.  So if we use batch norm in a given layer, it has we  have an additional two vectors that have the same dimension as  the bias vector, right.  So if we have, we use batch norm here in  this layer, we will have two, four dimensional vectors, like  this bias vector here would also be four dimensional, right,  because there's one bias for each in layer activation. 
 So  yeah, here, that's the first step of batch norm, there are  two steps.  So the first step is to normalize the net inputs.  So the j is the  feature index again.  So you can actually use batch norm for any  type of input.  So we will also see there is a two dimensional  version for that for convolutional networks later on. 
Core ML does not have 1-dimensional batch norm. 
If you want to convert this model, you should fold the batch norm weights into those of the preceding layer and remove the batch norm layer. 
