image captioning given an image we want
to summarize the image in a phrase or in
a sentence what I usually do in my
videos on such concepts is we would take
some state of the art paper and try to
dissect their approach but I want to do
things a little differently this time
let's try to devise our own approach to
image captioning and compare it to the
state of the art the goal of doing this
kind of exercise is to get you thinking
like an AI researcher so that you can
come up with similar architectures for
any problem and in the process
understand that those researchers aren't
really super human so we have a problem
of image captioning and we want to
determine how to use neural networks to
solve it and we're doing this from
scratch using our understanding of
neural nets when you hear the term
neural network what do you think perhaps
an interconnection of neurons that takes
raw data as input performed some hocus
pocus in the middle and spits out some
probability and classification problems
or some real values in case of
regression problems this notion of
neural nets isn't incorrect but with
that understanding can you really say
what these layers represent what exactly
are these layers well to understand this
it's better to think of neural nets from
a more mathematical perspective there
are basically mathematical functions
that transform one kind of variable to a
variable of another kind it could be
vectors to vectors as we would see in
classification problems or vectors to
scalars as we would see in regression
networks we treat each of these
interconnections seen in every layer as
a transformation on the input so each
layer is simply the vector
representation of the same input this is
going to be important in some time so
keep this in mind let's define the
structure of our problem now identifying
the inputs and the outputs the input to
an image captioner is some kind of image
like a matrix or a tensor the output is
a sentence basically a sequence a
sequence is a set of variables that has
some defined ordering to it sentences or
sequences because one word has to come
after another and in that order to have
some meaning I said before that neural
nets are mathematical functions that map
one kind of variable to a variable of
another kind now if one of those
variables is a sequence then we get a
recurrent neural network or at least
that's the first thing we would think of
in such problems
so because of output sequences of the
image captioner you may think of
recurrent neural networks coming into
play like any other network it helps to
think of recurrent Nets as mathematical
functions that map sequences to vectors
vectors to sequences and sequences to
other sequences image captioning would
fall under a vector to sequence
representation sure the input image
isn't exactly a natural vector but the
output sentence is most certainly a
sequence cool so we have the last part
of our architecture a recurrent neural
network let's now take a look at our
input it's an image to feed this to our
neural network we need it in some form
of vector format first thing that comes
to mind is simply flattening the input
image so the matrix or tensor becomes a
one dimensional vector now this works
but this image representation is pretty
sparse a better way to represent an
image is through convolutional neural
networks consider the basic convolution
neural network architecture this is the
Linette five architecture with the basic
convolution activation and pooling
layers followed by fully connected
layers if you want intuition on each of
these layers you can check out my series
on the convolutional neural Nets but you
really don't need to know all that to
understand one
about to do here here instead of
thinking of these layers as some complex
transformation let's do the same thing
we did before with the recurrent neural
nets and the vanilla neural networks
and think of each of these
transformations as some mathematical
function at each layer we are just
chaining functions performing
transformations on the same input so
this layer is the condensed matrix or
tensor representation of the image and
what is this layer it's basically the
dense vector representation of the input
image this holds true for any network
where we have a sequential flow of
information that is where all
information passes through the layers so
we can pass an image into a CNN to get a
dense vector representation of the image
then we can pass this dense vector to an
RNN to generate a sequence the sentence
or phrase that describes the image so
nice our architecture is now taking some
form but there are some tweaks we can
still do for example we can take into
account the meanings or the semantics of
the captions instead of just treating
them as raw numbers so how do we do this
our recurrent neural Nets are typically
trained using a mechanism called teacher
forcing that is the correct labels are
used to train the recurrent Nets for the
next state this is done to ensure the
back propagation through time algorithm
doesn't become super expensive in case
you were wondering the truncated be PTT
algorithm is a method of training
recurrent neural networks the output for
our case is the words of the caption
each word can be represented by a one
hot encoded vector in teacher forcing we
would feed this one hot vector of the
previous word in the caption directly to
the next iteration but let's be smarter
about this instead of simply feeding the
word in the next iteration we can learn
a set of
embeddings w e now w e is a set of word
embedding vectors that incorporates the
meaning of a word and the closeness to
other words in terms of meaning and
semantics if you are using this on
specific types of data it is good to
learn these embeddings during the
training of your LS TM Network
simultaneously instead of feeding some
word s T in the teeth iteration we would
feed a vector W est that incorporates
the meaning of the word in this way the
image captioner has knowledge of
language while generating captions so
that's awesome we now have an
architecture that converts an image to a
caption using neural networks and to end
but you may be thinking this is great at
all but what is a state of the art image
captioner what is the forefront of AI
research and my answer to this is well
you're looking at it this architecture
that we talked about just now is the
state of the art and is the basis for
the paper show-and-tell for image
captioning we show an image to the CNN
part and then the RN part of the
architecture generates captions to tell
us what the image is about hence
show-and-tell
if you understand everything I just said
you've understood show-and-tell
so Congrats but can we go beyond this
let's throw attention in the mix just
because we can attention involves
focusing on certain parts of an image
while generating different words of the
caption this can help create more
detailed sentences of an image so how do
we do this let's come up with the
architecture for attention intuitively
in our previous architecture we took the
dense vector representation of the image
from the FC layer of for CNN but
attention involves looking at different
spatial regions of an image so it makes
sense to get a tensor representation of
an image to preserve spatial features
to do this we can use any of the
convolution output layers remember
convolution activation and pooling are
just mathematical functions applied to
an input so the output of any of these
operations is a tensor representation of
the input itself this tensor has L
regions where each region of an image is
represented by the vector AI there are
two types of attention that we can
perform soft attention and hard
attention soft attention involves
constructing a word considering multiple
parts of an image to a different degree
ZT is the context vector to generate the
teeth word for an image think of it as
the parts of the image to concentrate on
while generating a specific word alpha
TI is a strength or a probability value
that ranges in 0 to 1 its magnitude is
basically the amount the image captioner
should focus on the region I to generate
the teeth word the other type of
attention is hard attention instead of a
value that determines how much of the
image part to consider each part of the
image is either completely considered or
completely disregarded while generating
a specific word STI takes on a binary
value of 0 or 1 if 1 it means the i3
j'en is considered while constructing
the teeth word otherwise it isn't now
let's get back to our architecture we
changed the Show and Tell architecture
by taking a convolution output instead
of the FC output but we need a vector
input to our RNN we extract a context
vector Z using our attention mechanism
in this way we can generate words
considering different parts of an image
this architecture that we have here is
now the show attend Intel Architecture
and this is considered the forefront of
recent research on visual attention we
show the image to the CNN focus our
attention to specific regions of a name
and then tell the caption using the RNN
show attend intel it's as simple as that
let's take a look at some code this here
is the show part where we show our image
to the convolution neural network
consisting of a set of convolution
activation and pooling layers the attend
and tell part is a part of our caption
generator class so it is here that we
build the LS TM model this is done while
considering the word embedding as the
input and for every word getting the set
of alphas and context vector this method
bill sampler will allow us to generate
the caption itself the results are
pretty slick the image captioner is able
to generate meaningful captions for the
input images note that these results are
for soft attention but it can be
modified easily for heart attention if
you want to know more about soft and
heart attention I've made a video on
visual attention from a different
perspective so check that out I hope
this video gave you an intuition on how
to think about neural networks allowing
you to create them from scratch
AI researchers are humans too they just
happen to get ideas before anyone else
thinking about neural networks
mathematically really does help and I
hope you can see why thanks for watching
and if you liked the video hit that like
button hit that subscribe button share
the video with friends family
acquaintances your next-door neighbor
perhaps and I look forward to your
support