Hi everybody and welcome to Lesson 6, where
we're going to continue looking at training
convolutional neural networks for computer
vision, and so we last looked at this the
lesson before last, and specifically we were
looking at how to train an image classifier
to pick out breeds of pet, one of 37 breeds
of pet, and we've gotten as far as training
a model. We also had to look and figure out
what loss function was actually being used
in this model. And so we talked about Cross-entropy
loss, which is actually a really important
concept and some of the things we'll talk
about today depend a bit on you understanding
this concept. So if you were at all unsure
about where we got to with that, go back and
have another look. Have a look at the questionnaire
particular, and make sure that you're comfortable
with Cross-entropy loss, if you're not you
may want to go back to the 04_mnist_basics
notebook, and remind yourself about MNIST
loss, because it's very, very similar. That's
what we have built on, to build-up Cross-entropy
loss. So having trained our model, the next
thing we're going to do is look at model interpretation.
There's not much point having a model if you
don't see what it's doing and one thing we
can do is use a confusion matrix, which in
this case is not terribly helpful. There's
a kind of a few too many, and it's not too
bad, we can kind of see some colored areas,
and so this diagonal here all the ones that
are classified correctly. So for Persians
there were 31 classified as Persians, but
we can see there's some bigger numbers here,
like a Siamese, 6 were misclassified they
actually continued, considered a Birman, but
for when you've got a lot of classes like
this it might be better, instead, to use the
�most_confused� method and that tells
you the combinations which it got wrong the
most often. In other words, which numbers
are the biggest. So actually here's the biggest
one, 10, and that's confusing, an American
Pitbull Terrier or a Staffordshire Bull Terrier.
That's happened 10 times and a Ragdoll is
getting confused with a Birman 8 times. And
so I'm not a dog or cat expert, and so I don't
know what this stuff means, so I looked it
up on the internet, and I found that American
Pit Bull Terriers and Staffordshire Bull Terriers
are almost identical, that, I think, they
sometimes have a slightly different colored
nose, if I remember correctly, and Ragdolls
and Birman's are types of cat that are so
similar to each other that there's whole long
threads on cat lover forums about �Is this
a Ragdoll?� or �Is this Birman?�, and
experts disagreeing with each other. So, no
surprise that these things are getting confused.
So when you see your model making sensible
mistakes, the kind of mistakes that humans
make, that's a pretty good sign, that it's
picking up the right kind of stuff, and that
the kinds of errors you're getting also might
be pretty tricky to fix, but, you know, let's
see if we can make it better. And one way
to try and make it better is to improve our
learning rate. Why would we want to improve
the learning rate? Well, one thing we'd like
to do is to try to train it faster. Get more
done in less epochs, and so one way to do
that would be to call our fine-tuned method
with a higher learning rate. So last time
we used the default, which, I think, is 1e
neg 2 (0.002),, and so if we pump that up
to 0.1, it's going to jump further each time.
So remember the learning rate, if you've forgotten
this, have a look again at Notebook 4, that's
the thing we multiply the gradients by, to
decide how far to step, and unfortunately
when we use this higher learning rate, the
error rate goes from 0.08 in three epochs,
to 0.082, so we're getting the vast majority
of them wrong now. So that's not a good sign.
So, why did that happen? Well, what happened
it's rather than this gradual move towards
the minimum, we had this thing where we step
too far, and we get further further away.
So when do you see this happening. which locks
in practice like this, your error rate getting
worse right from the start, that's a sign
your learning rate is too high. So we need
to find something just right, not too small
that we take tiny jumps and it takes forever,
and not too big that we, you know, either
get worse and worse, or we just jump backwards
and forwards quite slowly.
So to find a good learning rate we can use
something that the researcher Leslie Smith
came up with called �The Learning Rate Finder�,
and �The Learning Rate Finder� is pretty
simple. All we do... Remember, when we do
Stochastic Gradient Descent, we look at one
mini batch at a time, there are a few images,
in this case, at a time, find the gradient
for that set of images for the mini-batch,
and jump, step our weights based on the learning
rate and the gradient. Well what Leslie Smith
said was �OK let's do the very first mini
batch at a really really low learning rate,
like 10 to the minus 7, and then let's increase
by a little bit there like maybe 25% higher
and do another step, and then 25 percent higher
and to another step�. So these are not epochs,
these are just a single, a similar mini batch,
and then we can plot on this chart here. �OK,
at 10 to the minus 7, what was the loss?�,
and, �At 25% higher than that what was the
loss?�, and �At 25% higher than that what
was the loss?�. And so, not surprisingly,
if you do that at the low learning rates,
the loss doesn't really come down, because
the learning rate is so small that these steps
are tiny, tiny, tiny, and then gradually we
get to the point where they're big enough
to make a difference and the loss starts coming
down, because we've plotted here the learning
rate against the loss. All right? So here
the loss is coming down as we continue to
increase the learning rate. The loss comes
down until we get to a point where, our learning
rates too high and so it flattens out and
then oop it's getting worse again. So here's
the point above like 0.1, where we're in this
territory. So what we really want is somewhere
around here, where it's kind of nice and steep.
So you can actually ask it, the learning rate
finder. So were used �lr_find� to get
this plot. We can we can get back from it,
the minimum and steep. And so steep is where
was it steepest, but the steepest point was
5e neg 3 (5e-03), and the minimum point divided
by ten, that's quite a good rule of thumb,
is 1e neg 2 (1e-02). Somewhere around this
range might be pretty good. So each time you
run it you'll get different values. A different
time we ran it, we thought that maybe 3e neg
3 (3e-3) would be good, so we picked that.
And you'll notice the learning rate finder
is a logarithmic scale. Be careful of interpreting
that. So we can now rerun the learning rate
finder, setting the learning rate to a number
we picked from the learning rate finder, which
in this case was 3e neg 3 (3e-3), and we can
see now that's looking good. Right? We've
got an 8.3% error rate after three epochs.
So this idea of the learning rate finder is
very straightforward. I can describe it to
you in a couple of sentences. It doesn't require
any complex math, and yet it was only invented
in 2015, which is super interesting. Right?
It, it just shows that there's so many interesting
things first to all, to learn and discover.
I think part of the reason perhaps for this
it took a while is that, you know, engineers
kind of love using lots and lots of computers.
So before the learning rate finder came along,
people would like run lots of experiments
on big clusters to find out which learning
rate was the best, rather than just doing
a batch at a time, and I think partly also
the idea of having a thing where a human is
in the loop where we look at something, and
make a decision is also kind of unfashionable.
A lot of folks in research and industry love
things which are fully automated, but anyway
it's great we now have this tool, because
it makes our life easier and fast AI certainly
the first library to have this, and I don't
know if it's still the only one to have it
built in, at least to the basic, the base
library. So, now we've got a good learning
rate. How do we fine-tune the weights? So,
so far we've just been running this fine-tuned
method without thinking much about what it's
actually doing, but we did mention in Chapter
1, Lesson 1 briefly basically what's happening
with a fine tune, what is transfer learning
doing, and before we look at that let's take
a question. �Is the learning rate plot in
�lr_find� plotted against one single mini
batch?�. No, it's not it's just... It's
actually just the standard, kind of, walking
through the, walking through the data loader,
so just getting the usual mini batches of
the shuffled data, and so it's kind of just
normal training, and the only thing that's
being different is that we're increasing the
learning rate a little bit after each mini
batch, and, and keeping track of it.
along with that is is the network reset to
the initial status after each trial. No certainly
not we actually want to see how it our learns.
We want to see it improving so we don't reset
it to its initial state state until we're
done. So at the end of it we go back to the
random weights we started with or whatever
the weights were at the time we ran this.
So what we're seeing here is, is something
that's actually the, the actual learning that's
happening as we at the same time increase
the learning rate. Why would an ideal learning
rate found with a single mini-batch at the
start of training keep being a good learning
rate even after several epochs and further
loss reductions? Okay question. It absolutely
wouldn't so let's look at that too shall we.
And, ya go on. Can i ask one more?. Of course
it is an important point so ask us. It is,
it is very important. For the learning rate
finder why use the steepest and not the minimum?
We certainly don't want the minimum because
the minimum is the point at which it's not
learning anymore. Right so so the first flat
section at the bottom here means in this mini
batch didn't get better. So we want the steepest
because that's the mini-batch where it got
the most improved. And that's what we want.
We want the weights to be moving as fast as
possible. As a rule of thumb though we do
find that the minimum divided by ten works
pretty well. That's Sylvian�s favorite approach.
And he's generally pretty spot-on with that.
so that's why we actually print out those
two things. LR mean is actually the minimum
divided by 10 and steepest point is suggest
the steepest point. Great,good questions all.
So remind ourselves what transfer learning
does. but with transfer learning remember
what our neural network is. it's a bunch of
linear models basically with activation functions
between them. And our activation functions
are generally ReLU�s - rectified linear
units. If any of this is fuzzy have a look
at the 04 notebook again to remind yourself.
And so each of those linear layers has a bunch
of parameters to the whole neural network
has a bunch of parameters. And so after we
train a neural network on something like imagenet
we have a whole bunch of parameters that aren't
random anymore. They're actually useful for
something and we've also seen that the early
layers seem to learn about fairly general
ideas like gradients and edges and the later
layers learn about more sophisticated ideas
like what our eyes look like or what does
fur look like or what does text look like.
So with transfer learning we take a model.
so in other words a set of parameters which
has already been trained on something like
imagenet. We throw away the very last layer
because the very last layer is the bit that
specifically says which one of those in the
case of imagenet 1000 categories is this an
image in. So we throw that away and we replace
it with random weights. Sometimes with more
than one layer of random weights and then
we train that. Now, yes. Oh I just wanted
to make a comment and that's that I think
the learning rate finder. And I think after
you learn about it the idea almost seems kind
of so simple or approximate that it's like
wait this shouldn't work like or you know
shouldn't you have to do something more more
complicated or more precise that it's like
I just want to highlight that this is a very
surprising result. That some kind of the such
as simple approximate method would be so helpful.
Yeah, I would particularly say it's surprising
to people who are not practitioners or who
have not been practitioners for long. I've
noticed that a lot of my students at USF have
a tendency to kind of jump in to try to doing
something very complex where they account
for every possible imperfection from the start.
And it's very rare that that's necessary.
So one of the cool things about this is a
good example of trying the easiest thing first
and seeing how well it works. And this was
a very big innovation when it came out that
I think it's kind of easy to take for granted
now but this was super super helpful when
it was. It was super helpful and it was also
nearly entirely ignored. None of the research
community cared about it and it wasn't until
fast.ai, I think in our first course talked
about it that people started noticing. And
we had quite a few years in fact is still
a bit the case where super fancy researchers
still don't know about the learning rate finder.
And you know, get, get beaten by you know
first lesson fast.ai students on practical
problems because they can pick learning rates
better. And they can do it without a cluster
of thousands of Buddhas
okay, so transfer loading. So we've got our
pre-trained Network and so it's really important
every time you hear the word pre-trained network
you're thinking a bunch of parameters which
have particular numeric values and go with
a particular architecture like ResNet 34.
We have thrown away the, the final layer and
replace them with random numbers. And so now
we want to train, to fine tune this set of
parameters for a new set of images in this
case pets. So fine-tune is the method we call
to do that and to see what it does we can
go learn.fine-tune?? and we can see the source
code and here is the signature of the function.
And so the first thing that happens is we
call freeze. So freeze is actually the method
which makes it so only the last layer�s
weights will get stepped by the optimizer.
So the gradients are calculated just for those
last layers of parameters and the step is
done just for those last layer of parameters.
So then we call fit and we fit for some number
of epochs which by default is one we don't
change that very often. And what that fit
is doing is it's just fitting those randomly
added weights. Which makes sense right. they're
the ones that are going to need the most work.
because at the time in which we add them they're
doing nothing at all they're just random.
So that's why we spend one epoch trying to
make them better. After you've done that you
now have a model which is much better than
we started with. it's not random anymore.
All the layers except the last are the same
as the pretrained network. The last layer
has been tuned for this new data set. The
closer you get to the right answer, as you
can kind of see in this picture, the smaller
the steps you want to create as this sorry,
the smaller steps you want to take, generally
speaking. So the next thing we do is we divide
our learning rate by 2 and then we unfreeze.
So that means we make it so that all the parameters
can now be stepped and all of them will have
gradients calculated. And then we fit for
some more epochs and this is something we
have to the pass to the method. And so that�s
now got to train the whole network. So if
we want to we can kind of do this by hand,
right. And actually cnn_learner, we have by
default freeze the model for us, freeze the
parameters for us. So we actually don't have
to call freeze. So if we just create a learner
and then fit for a while. This is three epochs
of training just the last layer. And so then
we can just manually do it ourselves unfreeze.
And so now at this point as the question earlier
suggested maybe this is not the right learning
rate anymore so we can run LR find again.
And this time you don't see the same shape.
You don't see this rapid drop because it's
much harder to train a model that's already
pretty good. So instead you just see a very
gentle little gradient. So generally here
what we do, is we kind of try to find the
bit where it starts to get worse again. And
go about which is about here and go about
ten let you know a multiple of ten less than
that. So about 1e-5 I would guess which yeah
that's what we picked. So then after unfreezing,
finding our new learning rate and then we
can do a bunch more. And so here we are, we
are getting down to 5.9 percent error. Which
is okay but there's, there's better we can
do. And the reason we can do better is that
at this point here we're training the whole
model at a 1e-5 so ten to the minus five learning
rate which doesn't really make sense. because
we know that the last layer is still not that
great it's only had three epochs of training
from random. so it probably needs more work.
We know that the second last layer was probably
pretty specialized to imagenet and less specialized
to pet breeds so that probably needs a lot
of work. Whereas the early layers but kind
of gradients and edges probably don't need
to be changed much at all. But what would
really like is to have a small learning rate
for the early layers and a bigger learning
rate for the later layers and this is something
that we developed at fast.ai and we call it
discriminative learning rates. And Jason Jasinski
actually is a guy who wrote a great paper
that some of these ideas are based on. He
actually showed that different layers of the
network really want to be trained at different
rates. Although he didn't kind of go as far
as trying that out and seeing how it goes,
it's more of a theoretical thing. So in fast.ai,
if we want to do that we can pass to our learning
rate, rather than just passing a single number,
we can pass a slice. Now a slice is a special
built-in feature of Python, it's just an object
which basically can have a few different numbers
in it. In this case, we�re passing it two
numbers. And the way we read those, basically
what this means in fast.ai is our learning
rate is the very first layer. We'll have this
learning rate, 10 to the -6, the very last
layer will be 10 to the -4, and the layers
between the two will be kind of equal multiples.
So they'll kind of be equally spaced learning
rates from the start to the end. So here we
can see, basically doing our kind of own version
of fine-tune. We create the learner, we fit
with that automatically frozen version, we
unfreeze when we fit some more. And so when
we do that you can see this works a lot better.
We're getting down to 5.3, 5.1, 5.4, and error.
Well, that's pretty great. One thing we'll
notice here is that we did kind of overshoot
a bit, and it seemed like more like an epoch
number 8 was better. So kind of back before,
you know, well actually, let me explain something
about fit_one_cycle. So fit_one_cycle is a
bit different to just fit. So what fiit_one_cycle
does is it actually starts at a low learning
rate, it increases it gradually for the first
one-third or so of the batches until it gets
to a high learning rate. The, the highest
were the, this is why they're called lr_max;
it's the highest learning rate we get to.
And then for the remaining two-thirds or so
of the batches it gradually decreases the
learning rate. And the reason for that is
just that, well largely it's kind of like
empirically, researchers have found that works
the best. In fact, this was developed again
by Leslie Smith, the same guy that did the
learning rate finder. Again it was a huge
step, you know, it really dramatically accelerated
the speed at which we can train your networks,
and also made them much more accurate. And
again the academic community basically ignored
it. In fact, the key publication that developed
this idea was not even, not even past peer
review. And so the reason I mentioned this
now is to say that we can't,we don't really
just want to go back and pick the model that
was trained back here because we could probably
do better, because we really want to pick
a model that's got a low learning rate. But
what I would generally do here is I�d change
this 12 to an 8, because this is, this is
looking good. And then I would retrain it
from scratch. Normally you�d find a better
result. You can plot the loss and you can
see how the training and validation loss moved
along. And you can see here that, you know
the, the error rate was starting to get worse
here. And what you'll often see is, often
the validation loss will get worse a bit before
the error rate gets worse. We're not really
seeing it so much in this case, but the error
rate and the validation losts don't always,
they're not always kind of in lockstep. So
what we're plotting here is the loss but you
actually kind of want to look to see mainly
what's happening with the error rate, because
that's actually the thing we care about. Remember
the loss is just like an approximation of
what we care about that just happens to have
a gradient that works out nicely. So how do
you make it better now? We're, we're already
down to just 5.4 or if we'd stopped bit earlier
maybe we could get down to 5.1 or less error.
On 37 categories that's pretty remarkable,
that's a very very good pet breed predictor.
If you want to do something even better, you
could try creating a deeper architecture.
So a deeper architecture is just literally
putting more pairs of nonlinear activation
function also known as a non-linearity, followed
by these little linear models. Put more pairs
on to the end
and they're basically... the number of these
sets of layers you have is the number that
you'll see at the end of an architecture so
there's resnet18, resnet34, resnet50, so forth.
Having said that, you can't really pick resnet19
or resnet38--I mean you could make one--but
nobody's created a pre-trained version of
that for you, so you won't be able to do any
fine-tuning. So like you can theoretically
create any number of layers you like, but
in practice (most of the time) you'll want
to pick a model that has a pre-trained version
so you kind of have to select from the sizes
people have pre-trained and there's nothing
special about these sizes: they're just ones
that people happen to have picked out. For
the bigger models, there's more parameters
and more gradients that are going to be stored
on your GPU and you will get used to the idea
of seeing this error, unfortunately. �Out
of memory.� So that's not out of memory
in your RAM, that's out of memory in your
GPU. CUDA is referring to the language and
the system used for your GPU so if that happens,
unfortunately you actually have to restart
your notebook, so that's a kernel > restart
and try again. That's a really annoying thing,
but such is life. One thing you can do if
you get an out of memory error is: after your
cnn_learner call, add this magic incantation
to_fp16(). What that does is it uses for most
of the operations, numbers that use half as
many bits as usual. So they're less accurate,
this half-precision floating point or FP16.
That will use less memory and on pretty much
any Nvidia card created in 2020 (or later),
and some more expensive cards even created
in 2019, that's often got a result in a 2
to 3 times speed up in terms of how long it
takes as well. So here, if I add in to_fp16()
then I will be seeing often much faster training.
And in this case what I actually did is I
switched to a resnet50 which would normally
take about twice as long, and my per epoch
time has gone from 25 seconds to 26 seconds.
So the fact that we used a much bigger network,
and it was no slower, is thanks to to_fp16().
But you'll see our error rate hasn't improved.
It's pretty similar to what it was, and so
it's important to realize that just because
we increase the number of layers, it doesn't
always get better! So it tends to require
a bit of experimentation to find what's going
to work for you. And of course, don't forget
the trick is: use small models for as long
as possible to do all of your cleaning up
and testing and so forth. And wait until you're
all done to try some bigger models and because
they're going to take a lot longer. Ok, questions.
How do you know or suspect when you can �do
better?� You have to always assume you can
do better... Because you never know! So you
just have to... I mean... Part of it though
is: do you need to do better? Or do you already
have a good enough result to handle the actual
task you're trying to do? Often people do
spend too much time fiddling around with their
models rather than actually trying to see
whether it's already going to be super helpful.
As soon as you can actually try to use your
model to do something practical, the better.
Yeah, how much can you improve it? Who knows!
You know, go through the techniques that we're
teaching in this course and try them, and
see which ones help. Unless it's a problem
that somebody has already tried before and
written down their results, in a paper or
a Kaggle competition or something, there's
no way to know how good it can get. So don't
forget after you do the questionnaire to check
out the further research section. One of the
things we've asked you to do here is to read
a paper. So find the learning rate finder
paper and read it, and see if you can kind
of connect what you read up to the things
that we've learned in this lesson. See if
you can maybe even implement your own learning
rate finder. You know, as manually as you
need to and see if you can get something--that
you know, based on reading the paper--to work
yourself. You can even look at the source
code of fastai�s learning rate finder, of
course. And then can you make this classifier
better? So this is further research, right?
So maybe you can start doing some reading
to see what else could you do? Have a look
on the forum and see what people are trying.
Have a look on the book website, course website,
to see what other people have achieved, and
what they did, and play around. So we've got
some tools in our toolbox now for you to experiment
with.
so that is that is PET Breeds that is as you
know a pretty tricky computer vision classification
problem and we kind of have seen most of the
pieces of what goes into the training of
it, we haven't seen how to build the actual
architecture but other than that we've kind
of worked our way up to understanding what's
going on. So let's build from there into another
kind of Dataset: one that involves multi-label
classification. So what's multi-label classification?
or maybe... so maybe let's look at an example.
Here is a multi label dataset where you can
see that it's not just one label on each image
but sometimes there's three bicycle, car perso,,
I don't actually see the car here maybe it
has been cropped out, so a multi-label dataset
is one where you still got one image per row
but you can have 0 1 2 or more labels per
row so we're going to have a think about and
look at how we handle that but first of all
let's take another question.
R: is dropping floating-point number precision
switching from FP 32 to FP 16 have an impact
on final loss.
J: yes it does, often it makes it better,
believe it or not, it seems like you know
they're kind o, it's doing a little bit of
rounding off, is one way to give it, drop
some of that precision and so that creates
a bit more bumpiness, a bit more uncertainty
it was you know of a stochastic nature and
you know when you introduce more lightly random
stuff into training it very often makes it
a bit better and so yeah FP 16 training often
gives us a slightly better result but I you
know I wouldn't say it's generally a big deal
either way and that me it's not always better.
R: would you say this is a bit of a pattern
in deep learning less us exact stochastic
way ��.?
J: for sure not just in deep learning but
machine learning more generally, you know
there's been some interesting research looking
at like matrix factorization techniques which
if you want them to go super fast you can
use a lots of machines, you can use randomization
and you often, when you then use the results
you often find you actually get better, better
outcomes.
R: just a brief blog for the fast day I computational
linear algebra course which talks a little
bit about �..
J: that's it really well that sounds like
a fascinating course, and look at that it's
number-one hit here on Google's, so easy to
find but by somebody called Rachel Thomas
hey that's persons got the same name as you,
Rachel Thomas.
All right so how we going to do multi-label
classification so let's look at a data set
called Pascal which is a pretty famous dataset,
we look at the version that goes back to 2007
been around for a long time and it comes with
a CSV file which we will read in CSV as comma
separated values and let's take a look each
row has a file name one or more labels and
something telling you whether it's in the
validation set or not so the list of categories
in each image is a space delimited string
but doesn't have a horse person it has a horse
and a person. pd here stands for Pandas. Pandas
is really important library for any kind of
data processing and you'll use it all the
time in machine learning and deep learning
so let's have a quick chat about it. not a
real panda, it's the name of a library and
it creates things called data frames that's
what the DF here stands for and a dataframe
is a table containing rows and columns. pandas
can also do some slightly more sophisticated
things than that but we'll treat it that way
for now so you can read in a data frame by
saying pd for pandas. Pandas read CSV give
it a file name you've now got a dataframe
you can call head to see the first few rows
of it for instance a data frame as a iloc
integer location property which you can index
into as if it was an array vector it looks
just like numpy so column means every row,
remembers row comma column and zero means
zeros column and so here is the first column
of the data frame. now you can do the exact
opposite so the zeroth row and every column
it's going to give us the first row and you
can see the row has column headers and values
so it's a little bit different to Numpy and
remember if there's a comma column or a bunch
of comma column at the end of indexing in
Numpy or PyTorch or Pandas whatever you can
get rid of it. And these two are exactly the
same , and you can do the same this here by
grabbing the column by name, the first column
as Fname, say df.fname you get that first
column you can create new columns so here's
a tiny little data frame I've created from
a dictionary and I could create a new column
by for example adding two columns and you
can see there it is. So it's like a lot like
NUmpy or Pytorch, except that you have this
idea of kind of rows and and column named
columns and so it's all about and tabular
data. I find this API pretty unintuitive a
lot of people do but it's fast and powerful
so it takes a while to get familiar with it
but it's worth taking a while and the creator
of pandas wrote a fantastic book called �Python
for data analysis� which I've read both
versions and I found it fantastic it doesn't
just cover pandas it covers other stuff as
well like iPython and Numpy and matplotlib
so highly recommend this book this is our
table so what we want to do now is construct
data loaders that we can train with and we've
talked about the data block API as being a
great way to create data loaders but let's
use this as an opportunity to create a data
data loaders or a data processor create a
data block and data loaders for this and let's
try to do it like right from square one so
let's see exactly what's going on with datablock
so first of all let's remind ourselves about
what a dataset and the dataloader is. A dataset
is an abstract idea of a class you can create
a data set data set is anything which you
can index into it like so or and you can take
the length of it like so so for example the
list of the lowercase letters along with a
number saying which lowercase letter it is,
I can index into it to get 0 comma �a�.
I can get the length of it, to get 26 and
so therefore this qualifies as a dataset and
in particular dataset normally you would expect
that when you index into it you would get
back at tuple because you've got the independent
and dependent variables, not necessarily always
just two things it could be more that could
be less but 2 is the most common so once we
have a dataset we can pass it to a dataloader
we can repress we can request a particular
batch size we can shuffle or not and so there's
our data loader from �a� we could grab
the first value from that iterator and here
is the shuffled, seven is H four is E twenty
is U and so forth and so I remember a mini-batch
has a bunch of a mini-batch of the independent
variable and a mini-batch of the dependent
variable if you want to see how the two correspond
to each other you can use zip so for zip passing
in this list and then this list so B0 and
B 1 you can see what zip does in Python is
it grabs one element from each of those in
turn and gives you back the tuples of the
corresponding elements since we're just passing
in all of the elements of B to this function,
Python has a convenient shortcut for that
which is just say *B and so * means insert
into this parameter lists each element of
B just like we did here, so these are the
same thing so this is a very handy idiom that
we use a lot in Python zip * something is
kind of a way of like transposing something
from one orientation to another. Allright
so we've got a dataset we've got a dataloader
and then what about datasets our datasets
is an object which has a training data set
and a validation set dataset so let's look
at one now normally you don't start with kind
of an enumeration like this like with with
an independent variable and a dependent variable
normally you start with like a file name for
example and then you, you kind of calculate
or compute or transform your file name into
an image by opening it and a label by for
example looking at the file name and grabbing
something out of it so for example we could
do something similar
here this is what data sets does so we could
start with just the lowercase letters so this
is still a dataset right because we can index
into it and we can get the length of it although
it's not giving us tuples yet so if we now
pass that list to the datasets plus and index
into it we get back the tuple and it's actually
a tuple with just one item this is how Python
shows tuple with one item as it puts it in
parenthesis and a comma and then nothing okay
so in practice what we'd really want to do
is to say like okay we'll take this and do
something to compute an independent variable
and do something to compute the dependent
variable so here's a function we could use
to compute an independent variable which is
to stick an A on the end and our dependent
variable might just be the same thing with
a B on the end but here's two functions so
for example now we can call datasets passing
an A and then we can pass in a list of transformations
to do and so in this case I've just got one
which is this function at an a on the end
so now for index into it I don't get A anymore
I get AA if you pass multiple functions then
it's going to do multiple things I've got
F 1 then F 2 a a B that's this one that's
this one and you'll see this is a list of
lists and the reason for that is that you
can also pass something.
like this: a list containing �f1�, a list
containing �f2�, and this will actually
take each element of �a�, pass it through
this list of functions, and there's just one
of them, to give you �aa�, and then start
again, and separately pass it through this
list of functions, there's just one, to get
�ab�. And so this is actually kind of
the main way we build up independent variables
and dependent variables in Fastai, is we start
with something like a filename, and we pass
it through two lists of functions: one of
them will generally kind of, open up the image
for example, and the other one will kind of
parse the filename, for example and give you
a independent variable and a dependent variable.
So you can then create a DataLoaders object
from datasets by passing in the datasets,
and a batch size, and so here you can see
I've got shuffled �oa�, �ia� etc.
�ob�, �ib� etc. So, this is worth
studying to make sure you understand what
Datasets and DataLoaders are. We don't often
have to create them from scratch, we can create
a DataBlock to do it for us. But now we can
see what the DataBlock has to do. So let's
see how it does it. We can start by creating
an empty DataBlock. So an empty DataBlock
is going to take our DataFrame, so we're going
to go back to looking at our DataFrame, which
remember, was this guy. And so if we pass
in our DataFrame, we can now, we will now
find that this Datablock has created Datasets,
a training and a validation Dataset for us,
and if we look at the training set, it'll
give his back an independent variable in the
dependent variable and we'll see that they
are, the same thing. So this is the first
row of the table what's actually shuffled,
so it's a random row of the table, repeated
twice. And the reason for that is by default
the DataBlock assumes that we have two things,
the independent variable and the dependent
or the input in the target, and by default
it just copies, it just keeps exactly whatever
you gave it. To create the training set in
the validation set by default it just randomly
splits the data with the 20% validation set.
So that's what's happened here. So this is
not much use. What we what we actually want
to do if we look at X for example is grab
the �fname�, the filename field, because
we want to open this image. That's going to
be our independent variable. And then for
the label, we're going to want this here �person
cat�. So we can actually pass these as parameters
�get_x� and �get_y�, are functions
that return the the bit of data that we want.
And so you can create an user function in
the same line of code in Python by saying
lambda. so lambda �r� means create a function,
doesn't have a name, that's going to take
a parameter called �r�. We don't even
have to say return it's got to return the
�fname� column, in this case. And �get_y�
is something, which is a function that takes
an �r� and returns the labels column.
So now we can do the same thing called �dblock
dot datasets�. We can grab a row from that,
from the training set, and you can see look
here it is there is, there is the image filename
and there is the space delimited list of labels.
So here's exactly the same thing again, but
done with functions. Okay so now the one line
of code above has become three lines of code,
but it does exactly the same thing. Okay we
don't get back the same result because the
training set... well wait why don't we get
the same result?
Oh, I know why. Because it randomly shuffles,
it's randomly picking a different validation
set. Because the random spit is done differently
each time, so that's why we don't get the
same result. One thing to note, be careful
of lambdas. If you want to save this DataBlock
for use later, you won't be able to. Python
doesn't like saving things that contain lambdas,
so most of the time in the book and the course
we normally use, avoid lambdas for that reason
because this is often very convenient to be
able to save things. We use the word here
serialization, that just means, basically
it means saving something. This is not enough
to open an image because we don't have the
path. So to turn this into, so rather than
just using this function to grab the �fname�
column, we should actually use pathlib to
go, a �path/train� and then column. And
then for the �y�, again the labels it�s
not quite enough, we actually have a split
on space. But this is Python, we can use any
function we like, and so then we won't use
the same three lines of code as here, and
now we've got a path, and a list of labels.
So that's looking good. So we want this path
to be opened as an image, so the DataBlock
API, lets you pass a �blocks� argument,
where you tell it, for each of the things
in your tuple, so there's two of them, what
kind of block do you need. So we need an ImageBlock
to open an image, and then, in the past we've
used a CategoryBlock, for categorical variables,
but this time we don't have a single category,
we've got multiple categories, where we have
to use a MultiCategoryBlock. So once we do
that, and have a look, we now have an 500
by 375 image as our independent variable,
and as a dependent variable we have a long
list of zeros and ones. The long list of zeros
and ones, is the labels as a one hot encoded
vector, a rank one tensor, and specifically
there will be a zero in every location where,
in the vocab, where there is not that kind
of object in this image, and a one in every
location where there is. Sso for this one
there's just a person so this must be the
location in the vocab where there's a person.
We have any questions? The one hot encoding
is a very important concept, and we didn't
have to use it before, right? We could just
have a single integer saying which one thing
is it, but when we've got lots of things,
lots of potential labels, it's convenient
to use this one hot encoding. And it's kind
of what, it's actually what's going to happen
with, with the, with the actual matrices anyway.
When we actually compare the activations of
our neural network to the target, it's actually
going to be comparing each one of these.
Okay so the categories, as I mentioned, is
based on the vocab, where we can grab the
vocab from our Datasets object, and then we
can say okay, let's look at the first row,
and let's look at the dependent variable,
and let's look for where the dependent variable
is 1. Okay and, and we can have a look, parse
those indexes, there's a vocab and get back
a list of what it actually was there. And
again each time I run this, I'm going to get
different results. So each time we run this
we're going to get different results because
I called �dot datasets� again here, so
it's going to give me a different train-test
split, and so this time it turns out that
this actually, a chair, and we have a question.
Shouldn't the tensor be of integers? Why is
it a tensor of floats? Yeah, conceptually
this is a tensor of integers, they can only
be 0 or 1, but we, we�re going to be
using a cross entropy style loss function,
so we're going to actually need to do floating-point
calculations on them. That's going to be faster
to just store them as float in the first place
rather than converting backwards and forwards,
even though they're conceptually an �int�
we're not going to be doing kind of �int
style calculations� with them. Good question.
I mentioned that by default, the DataBlock
uses a random split, you might�ve noticed,
in the DataFrame though, it said here's a
column saying what validation set to use,
and if the data set you're given, tells you
what validations had to use, you should generally
use it because that way you can compare your
validation set results to somebody else's.
So you can pass a splitter argument, which
again is a function, and so we're going to
pass it a function, that's also called splitter,
and the function is going to return the indexes
where it's not valid and that's going to be
the training set, and the indexes where it
is valid, that's going to be the validation
set. And so the splitter argument is expected
to return two lists of integers, and so if
we do that we get again the same thing, but
now we're using the correct train and validation
sets. Another question? Sure. Any particular
reason we don't use floating point eight?
Is it just that the precision is too low?
Yeah, trying to train with 8-bit precision
is super difficult it's, it's so flat and
bumpy, it's pretty difficult to get decent
gradients. But you know it's an area of research,
the main thing people do with 8-bit or even
one bit data types, is they take a model that's
already been trained with 16-bit or 32-bit
floating-point and then they kind of round
it off. It's called discretizing, to create
a kind of purely integer or even binary network
which can do inference much faster. Figuring
out how to train with such low precision data
is an area of active research. I suspect it's
possible, and I suspect, I mean people have
fiddled had some success I think you know
it could turn out to be super interesting
particularly for stuff that's been done on
low-powered devices that might not even have
a floating-point unit. Right, so the last
thing we need to do is to add our item transforms
RandomResizedCrop, we've talked about that
enough, so I won't go into it, but basically
that means we now are going to ensure that
everything has the same shape so that we can
collate it into a DataLoader so now rather
than going �datasets� we go DataLoaders,
and display our data. And remember, if something
goes wrong, as we saw last week you can call
�summary�, to find out exactly what's
happening in your DataBlock. So now you know
this is something really worth studying this
section because data blocks are super handy
and if you haven't used Fastai 2 before, they
won't be familiar to you because no other
library uses them, and so like this has really
shown you how to go right back to the start
and gradually build them up, so hopefully
that'll make a whole lot of sense. Now we're
going to need a loss function again, and to
do that let's start by just creating a learner.
Let's create a Resnet18 from the DataLoaders
object we just created, and let's grab one
batch of data and then let's put that into
our mini-batch of independent and dependent
variables and then �learn dot model� is
the thing that actually contains the, the
the model itself, in this case it�s CNN,
and you can treat it as a function, and so
therefore we can just pass something to it.
And so if we pass a mini-batch of the independent
variable to �learn dot model� it will
return the activations from the final layer.
And that is, shape 64 by 20. So anytime you
get a tensor back, look at its shape, and
in fact before you look, at its shape, predict
what the shape should be. And then make sure
that you're right. If you're not, either you
guessed wrong, so try to understand where
you made a mistake, or there's a problem with
your code, and this place, 64 by 20, makes
sense because we have a mini-batch size of
64, and for each of those, we're going to
make predictions about what probability is
each of these 20 possible categories, and
we have a question. Two questions. Two questions,
all right! Is the DataBlock API compatible
with out of core data sets like Dask. Yeah,
the DataBlock API can do anything you wanted
to do, so you're passing it, if we go back
to the start. So, you can create an empty
one, and then you can pass it anything that
is indexable, and yeah so that can be anything
you like, and pretty much anything can be
made indexable in Python and that's something
like Dask is certainly indexable. So that
works perfectly fine. If it's not indexable,
like it's a, it's a network stream or something
like that, then you can�t use the DataLoaders
Datasets API directly which we'll learn about
either in this course or the next one. But
yeah, anything that you can index into, which
certainly includes Dask, you can use with
DataBlocks. Next question, where do you put
images for multi-label with that CSV table?
Should they be in the same directory? They
can be any way you like, so in this case we
used a pathlib object like so. And in this
case, by default it's going to be using�
Let me think about this. What's happening
here is the path is.. Oh it's saying �dot�
okay the reason for that is that path.BASE_PATH
is currently set to path, and so that displays
things relative. Well let's get rid of that.
Okay so the path we set is here, right? And
so then when we said get_x, it's saying path
slash train whatever, right? So this is an
absolute path, and so here is the exact path.
So you can put them anywhere you like, you
just have to say what the �path� is. And
then if you want to not get confused by having
this big long prefix that we can, don't want
to see all the time, just set BASE_PATH to
the path you want everything to be relative
to, and then it'll just print things out in
this more convenient manner. All right, so
um this is really important that you can do
this, that you can create a learner, you can
grab a batch of data that, you can pass it
to the model, this is just plain PyTorch,
this line here right no Fastai. You can see
the shape, right? You can recognize why it
has this shape, and so now if you have a look,
here are the 20 activations. Now this is not
a trained model, it's a pre trained model
with a random set of final layer weights,
so these specific numbers don't mean anything,
but it's just worth remembering this is what
activations look like, and most importantly
they're not between 0 & 1.
And, if you remember from the MNIST Notebook,
we know how to scale things between zero and
one, we can pop them into the sigmoid function.
So the sigmoid function is something that
scales everything to be between zero and one.
So let's use that. You'll also hopefully remember
from the MNIST Notebook that the MNIST loss,
the MNIST loss function, first did sigmoid,
and then it did �torch.where�, so and
then it did �.mean�. We're going to use
exactly the same thing as the MNIST loss function,
and we're just going to do one thing, which
is going to add �.log�, for the same reason
that we talked about when we were looking
at softmax, we talked about why log is a good
idea as a transformation. We saw in the MNIST
Notebook, we didn't need it but we're gonna
train faster and more accurately, if we use
it, because it's just more, it's going to
be better behaved, as we've seen. So this
particular function, which is identical to
MNIST loss plus �.log� jhas a specific
name and it's called binary cross entropy,
and we used it for the threes vs. sevens problem,
to, to decide whether that column is it a
three or not, but because we can use broadcasting
in PyTorch and element-wise arithmetic, this
function when we pass it a whole matrix is
going to be applied to every column. So is
the first column, you know, what... So it'll
basically do a �torch.where� on, on every
column separately and every item separately.
So that's great, it basically means that this
binary cross-entropy function is going to
be just like MNIST loss, rather than just
being �Is this the number three?� it'll
be �Is this a dog?�, �Is this a cat?�,
�Is this a car?�, �Is this a person?�,
�Is this a bicycle?�, and so forth. So
this is where it's so cool in PyTorch, we
can kind of run, write, one thing and then
kind of have it expand to handle higher dimensional
tensors, without doing any extra work. We
don't have to write this ourselves, of course,
because PyTorch has one and it's called �F.binary_cross_entropy�.
We can just use PyTorch�s. As we've talked
about there's always a equivalent module version
so this is exactly the same thing as a module
�nn.BCELoss�, and these ones don't include
the initial sigmoid, actually. If you want
to include this initial sigmoid you need F.binary_cross_entropy_with_logits
or the equivalent nn.BCEWithLogitsLoss. So
BCE is binary cross-entropy. And so those
are two functions, plus two equivalent classes
for multi-label or binary problems, and then
the equivalent for single label like MNIST
and pets is �nll_loss� and �cross_entropy�.
That's the equivalent of �binary_cross_entropy�
and �binary_cross_entropy_with_logits�.
So these are pretty awful names, I think we
can all agree, but it is what it is. So in
our case we have a one hot encoded target,
and we want the one with the sigmoid in, so
the equivalent built-in is called �BCEWithLogitsLoss.
That we can make that our loss function, we
can compare the activations to our targets,
and we can get back a loss, and then that's
what we can use to train, and then finally,
before we take our break, we also need a metric.
Now previously we've been using as a metric
accuracy or actually error rate. Error rate
is 1 - accuracy. Accuracy only works for single
label datasets, like MNIST and pets, because
what it does is it takes the input, which
is the final layer of activations, and it
does our �argmax�, what �argmax� does
is it says: �What is the index of the largest
number in those activations?�.
So, for example, for MNIST, you know, maybe
the largest, the highest probability is seven,
so this �argmax� would return seven. And
then it says, �OK there those are my predictions.�,
and then it says, �OK is the prediction
equal to the target or not, and then take
the floating-point mean�. That's what accuracy
is. So �argmax� only makes sense when
there's a single maximum thing you're looking
for. In this case we've got multi-label. So
instead we have to compare each activation
to some threshold. Our default is 0.5, and
so we basically say, �If the sigmoid of
the activation is greater than 0.5, let's
assume that means that category is there and
if it's not let's assume it means it's not
there�, and so this is going to give us
a list of trues and falses, for the ones that
the based on the activations it thinks are
there, and we can compare that to the target,
and then again take the floating point mean.
So we can use the default threshold of 0.5,
but we don't necessarily want to use 0.5,
we might want to use a different threshold,
and remember we have to pass, when we create
our learner, we have to pass to the metric,
the metrics argument, a function. So what
if we want to use a threshold other than 0.5.
Well we'd like to create a special function,
which is �accuracy_multi�, with some different
threshold, and the way we do that is we use
a special, built-in in Python called partial.
Let me show you how partial works. Here�s
a function called �say_hello�, �say_hello�
somebody with something. So �say_hello�
Jeremy, where the default is �Hello�,
that says �Hello Jeremy�. �say_hello�
Jeremy, comma �Ahoy�, gonna be �Ahoy!
Jeremy�. Let's create a special version
of this function that will be more suitable
for Sylvain. It's going to use French. So
we can say partial, create a new function,
that's based on the �say_hello� function,
but it's always going to set say_what to �Bonjour�
and we'll call that �f�. So now f(�Jeremy�)
is �Bonjour Jeremy and f(�Sylvain�)
is �Bonjour Sylvain�. So, you see, we've
created a new function, from an existing function,
by fixing one of its parameters. So we can
do the same thing for �accuracy_multi�.
Hey, let's use a threshold of 0.2, and we
can pass that to metrics, and so let's create
a �cnn_learner� and you'll notice here
we don't actually pass a loss function, and
that's because fast AI is smart enough to
realize. �Hey, you're doing a classification
model, with a, a multi-label dependent variable.
So I know what loss function you probably
want�. So it does it for us. We can call
�fine_tune� and here we have an accuracy
of 94.5 after the first few, and eventually
95.1. That's pretty good. We've got an accuracy
of over 95%, was 0.2 a good threshold to pick.
Who knows? Let's try 0.1. Oh, l that's a worse
accuracy. So, I guess, in this case we could
try a higher threshold. 94. Hmm, also not
good. What's the best threshold? Well what
we could do is call, �get_preds�, to get
all of the predictions, and all of the targets,
and then we could calculate the accuracy,
at some threshold, then we could say, �OK,
let's grab lots of numbers between 0.05 and
0.95, and with a list comprehension calculate
the accuracy for all of those different thresholds,
and plot them. Ah, looks like we want a threshold
somewhere a bit about 0.5. So, cool, we can
just use that, and it's going to give us 96
in a bit, which is
going to give us a better accuracy. This is
a, you know, something that a lot of theoreticians
would be uncomfortable about - I've used the
validation set to pick a hyper parameter,
a threshold, right. And so people might be
like, �Oh, you're overfitting, using the
validation set to pick a hyper parameter.�
But if you think about it, this is a very
smooth curve, right. It's not some bumpy thing
where we've accidentally kind of randomly
grabbed some unexpectedly good value. When
you're picking a single number from a smooth
curve � you know, this is where the theory
of like don't use a validation set for for
how parameter tuning doesn't really apply.
So it's always good to be practical, right.
Don't treat these things as rules, but as
rules of thumb. Okay, so let's take a break
for five minutes and I'll see you back here
in five minutes time.
Hey,welcome back oh I want to show you something
really cool - image regression. So we are
not going to learn how to use a fast.ai image
regression application because we don't need
one. Now that we know how to build stuff up
with loss functions and the DataBlock API
ourselves, we can invent our own applications.
So there is no image regression application,
per se. But we can do image regression really
easily. What do we mean by image regression?
Well, remember back to a lesson, I think is
lesson 1, we talked about the two basic types
of machine learning, or supervised machine
learning - regression and classification.
Classification is when our dependent variable
is a discrete category or set of categories.
And regression is when our dependent variable
is a continuous number, like age or XY coordinate,
or something like that. So image regression
means our independent variable is an image
and our dependent variable is a continuum
or one or more continuous values. And so here's
what that can look like which is biwi head
pose dataset. It has a number of things in
it, but one of the things we can do is find
the midpoint of a person's face, see. So the
biwi head pose dataset, so the biwi head poste
dataset comes from this paper, �Random Forest
for Real Time 3D Face Analysis.� So thank
you to those authors. And we can grab it in
the usual way, untar_data. And we can have
a look at what's in there, and we can see
there's 24 directories numbered from 0, from
1 to 24. 1,2,3, and each one also has a .obj
file. We're not going to be using the .obj
file, I'm just the directories. So let's look
at one of the directories and as you can see
there's a thousand things in the first directory.
So each one of these 24 directories is one
different person that they photographed. And
you can see, for each person there's frame
3 pose, frame 3 RGB, frame 4 pose, frame 4
RGB, and so forth. So in each case we've got
the image, which is the RGB, and we've got
the pose, which is pose.txt. So as we've seen,
we can grab our use get_image_files to get
a list of all of the files image files recursively
in a path. So once we have an image filename,
like this one, sorry like this one, we can
turn it into a pose file name by removing
the last 1,2,3,4,5,6,7 letters and adding
back on pose.txt. And so here is a function
that does that. And so you can see, I can
pass in an image file to img2pose, and get
back a pose file, right. So PILImage.create
is the fastai way to create an image,
at least a PIL image. It has a shape (in computer
vision, they're normally backwards, they normally
do columns by rows but that's why it's this
way around) whereas PyTorch and Numpy tensors
and arrays are rows by columns. So that's
confusing, but that's just how things are
I'm afraid. So here's an example of an image.
When you look at the ReadMe from the dataset
website, they tell you how to get the center
point from, from one of the text files, and
it's just this function (the details don�t
matter, it is what it is). They call it get_ctr
and it will return the XY coordinate of the
center of the person's head, or face. So we
can pass this as get_y, because get_y, remember
is a thing that gives us back the label, okay.
So, so, here's the thing, right. We can create
a DataBlock and we can pass in as the independent
variables block, ImageBlock as usual. And
then the dependent variables block we can
say PointBlock, which is a tensor with two
values in. And now by combining these two
things, this says we want to do image regression
with the dependent variable with two continuous
values. To get the items, you call get_image_files,
to get the y, we'll call the get_ctr function
to split it. So this is important, we should
make sure that the validation set contains
one or more people that don't appear in the
training set. So I'm just going to grab person
number 13, just grabbed it randomly, and I'll
use all of those images as the validation
set. Because I think they did this with a
Xbox Kinect, you know, video thing, so there's
a lot of images that look almost identical.
So if you randomly assigned them then you
would be massively overestimating how effective
you are. You want to make sure that you're
actually doing a good job with a random, with
a new set of people, not just a new set of
frames. That's why we use this and so funcSplitter
is a splitter that takes a function. And in
this case we're using lambda to create the
function. We will use data augmentation and
we will also normalize (so this is actually
done automatically), now this case we're doing
it manually. So this is going to subtract
the mean and divide by the standard deviation
of the original data set that the pre-trained
model used, which is ImageNet. So that's our
data block, and so we can call dataloaders
to get our data loaders, passing in the path.
And show_batch, and we can see that looks
good, right. Here's our faces and the points.
And so let's like, particularly for as a student
,don't just look at the pictures, look at
the actual data. So grab a batch, put it into
an xb and a yb (x batch and y batch), and
have a look at the shapes. And make sure they
make sense. So why is this 64 by 1 by 2? So
it's 64 in the mini batch (64 rows), and then
the coordinates is a 1 by 2 tensor. So there's
a single point with two things in it. It's
like you could have like hands, face, and
armpits, or whatever � or nose and ears
and mouth. So in this case we're just using
one point and the point is represented by
two values, the x and the y. And then why
is this 64 by 3 by 240 by 320? Well there's
240 rows by 320 columns (that�s the pixels
it's the size of the images that we're using),
mini-batches 64 items now what's the 3? The
3 is the number of channels, which in this
case means the number of colors. If we open
up some random grizzly bear image and then
we go through each of the elements of the
first axis, and do a show_image you can see
that it's got the red the green and the blue
as the three channels, so
that's how we store a three channel image.
It is stored as a three by number of rows
by number of columns rank three tensor and
so a mini batch of those is a rank four tensor.
Tat's why this is that shape. So here's a
row from the dependent variable okay there's
that XY location we talked about. So we can
now go ahead and create a learner passing
in our dataloaders as usual, passing in our
pretrained architecture as usual, and if you
think back, you may just remember in Lesson
one we learn about y_range. Y_range is where
we tell Fastai what range of data we expect
to see in the dependent variable. So we want
to use this generally when we're doing regression
though the range of our coordinates is between
minus 1 and 1, that's how Fastai and Pytorch
treats coordinates the left hand side is minus
1 or the top is minus 1 and the bottom in
the right 1. No point predicting something
that's more than minus 1 or bigger than 1
as that is not in the area that we use for
our coordinates. [Rachel] I have a question.
[Jeremy] sure just a moment. So how does Y_range
work? Well it actually uses this function
called sigmoid_range which takes the sigmoid
of X multiplies by hi minus lo and adds lo
and here is what sigmoid_range looks like
or minus 1 to 1. It's just a sigmoid where
the bottom is the lo and the top is the hi
and so that way all of our activations are
going to be mapped to the range from minus
1 or 1. Yes Rachel. [Rachel] can you provide
images with an arbitrary number of channels
as inputs specifically more than three channels?
[Jeremy] yeah you can have as many channels
as you like. We�ve certainly seen images
with less than three because we've been grayscale.
More than three is common as well you could
have like an infrared band or like satellite
images often have multispectral. There's some
kinds of medical images where there are bands
that are kind of outside the visible range.
Your pre-trained model will generally have
three channels. Fastai does some tricks to
use three channel pre-trained models for non
three channel data but that's the only tricky
bit other than that it's just just a you know
it's just an axis that happens to have four
things or two things or one thing instead
of three things. There's nothing special about
it. Okay we didn't specify a loss function
here so we get whatever you gave us which
is a MSE loss. So MSE loss is mean squared
error and that makes perfect sense right you
would expect mean squared error to be a reasonable
thing to use for regression we're just testing
how close we are to the target and then taking
the square picking the mean. We didn't specify
any metrics and that's because mean squared
error is already a good metric like it's not
� it has nice gradient so behaves well but
it's also the thing that we care about so
we don't need a separate metric to track.
Let's go ahead and use lr_find() and we can
pick her learning rate so maybe about n to
the minus 2 we can call fine_tune and we get
a valid loss of 0.0001. So that's the mean
squared error so we should take the square
root but on average we're about 0.01 off in
a coordinate space that goes between minus
1 and 1. Well that sounds super accurate,
took about 3 and a bit minutes to run. So
we can always call in Fastai, and we always
should, our results and see what our results
look like
and as you can see Fastai has automatically
figured out how to display the combination
of an image independent variable and a point
dependent variable on the left is the target
and on the right is the prediction and as
you can see it is pretty close to perfect.
One of the really interesting things here
is that we used fine_tune even though think
about it the thing we're fine-tuning imagenet
isn't even an image regression model. So we're
actually fine-tuning an image classification
model that becomes something totally different;
an image regression model. Why does that work
so well? Well because an imagenet classification
model must have learnt a lot about kind of
how images look like, what things look like
and where the pieces of them are they kind
of know how to figure out what breed of animals
something is even if it's partly obscured
by borscht shorts, in the shade, or it's turned
in different angles. You know these are pre-trained
image models are incredibly powerful computers,
you know computing algorithms. So built into
every imagenet pre-trained model is all this
capability that it had to learn for itself.
So asking it to use that capability to figure
out where something is, it's just actually
not that hard for it and so that's why we
can actually fine tune an imagenet classification
model to create something completely different
which is a point image regression model. So
I find that incredibly cool I gotta say. So
again look at the further research after you've
done the questionnaire and particularly if
you haven't used dataframes before please
play with them because we're going to be using
them more and more. [Rachel] I�ve a question.
[Jeremy] I'll just do the last one and also
go back and look at the bear classifier from
notebook 2 or whatever; hopefully you created
some other classifier for your own data. Because
remember we talked about how it would be better
if the bear classifier could also recognize
that there's no bear at all or maybe there's
both a grizzly bear and a black bear or a
grizzly bear and a teddy bear so if you retrain
using multilabel classification see what happens
see how well it works when there's no bears
and see whether it changes the accuracy of
the single label model when you turn it into
a multilabel problem. So have a fiddle around
and tell us on the forum what you find. You've
got a question rachel [Rachel] is there a
tutorial showing how to use pretrained models
on 4-channel images; also how can you add
a channel to a normal image; [Jeremy] well
the last one is how do you add a channel to
an image? I don�t know what that means.
Okay I don't know; Like an image is an image.
You can�t add a channel to an image. it
is what it is. I don't know if there's a tutorial
but we can certainly make sure somebody on
the forum has learned how to do it. it's super
straightforward; pretty much automatic. Okay
we're going to talk about collaborative filtering.
What is Collaborative filtering? Think about
on Netflix or whatever, you might have watched
a lot of movies that are sci-fi and have a
lot of action and were made in the 70s. Netflix
might not know anything about the properties
of movies you watched. They might just know
that they are movies with titles and IDs.
But what it could absolutely see without any
manual work is find other people that watched
the
same movies that you watched. And it could
see what other movies those people watched
that you haven't, and it would probably find
they were also, we would probably find, they're
also science fiction, and full of action,
and made in the 70s. So we can use an approach
where we recommend things, even if we don't
know anything about what those things, are
as long as we know who else has used or recommended
things that are similar, you know, the same
kind, you know, many of the same things that
that you've liked or, or used. This doesn�t
necessarily mean users and products. In fact
in collaborative filtering instead of saying
products we normally say items, and items
could be links you click on, diagnosis for
a patient, and so forth. So there's a key
idea here, which is that in the underlying
items and we going to be using movies in this
example, there are some, there are some features,
they may not be labeled, but there's some
underlying concept of features, of, of those
movies, like the fact that there's a action
concept, and a sci-fi concept, and a 1970s
concept. Now you never actually told Netflix
you like these kinds of movies, and maybe
Netflix never actually added columns to their
movies saying what movies are those types,
but as long as like, you know, in the real
world there's this concept of sci-fi, and
action, and movie age, and that those concepts
are relevant for at least some people's movie
watching decisions, as long as this is true,
then we can actually uncover these they're
called latent factors, these things that,
kind of, decide what kind of movies you want
to watch, and they�re latent, because nobody
necessarily ever wrote them down or labeled
them or or communicated them in any way. So
let me show you what this looks like. So there's
a great dataset we can use, called MovieLens,
which contains tens of millions of movie rankings,
and so a movie ranking looks like this. It
has a user number, a movie number, a rating
and a timestamp. So we don't know anything
about who user number 196 is. I don't know
if that is Rachel, or Sylvain, or somebody
else. I don't know what movie number 242 is.
I don't know if that's Casablanca or Lord
of the Rings or The Mask. And then rating
is a number between, I think, it was 1 and
5. A question. Sure. �In traditional machine
learning we perform cross validations and
k-fold training to check for variance and
bias trade-off. Is this common in training
deep learning models as well?� So cross-validation
is a technique where you don't just let your
data set into one training set and one validation
set, but you basically do it five or so times,
like five training sets, and like five validation
sets representing different overlapping subsets,
and basically this was this used to be done
a lot because people often used to have not
enough data to get a good result, and so this
way, rather than, kind of, having 20% that
you would leave out each time you could just
leave out like, 10% each time. Nowadays it's
less common that we have so little data that
we need to worry about the complexity and
extra time of lots of models. It's done on
Kaggle a lot. As on Kegel every little fraction
of a percent matters, but it's not, yeah,
it's not a deep learning thing, or a machine
learning thing, or whatever. It's just a,
you know, lots of data or not very much data
thing,
and do you care about the last decimal place
of� Or not. It's not something we're going
to talk about, certainly, in this part of
the course, if ever, because it's not something
that comes up in practice that often as being
that important. There are two more questions.
�What would be some good applications of
collaborative filtering outside of recommender
systems?� Well I mean depends how you define
recommender system. If you're trying to figure
out what kind of other diagnoses might be
applicable to a patient, I guess, it's kind
of a recommender system, or you're trying
to figure out where somebody is going to click
next, or whatever, it's kind of a recommender
system, but, you know, really conceptually
it's anything where you're trying to learn
from, from past behavior, where that behavior
is, kind of, like a thing happened to an entity.
�What is an approach to training using video
streams, i.e., from drone footage instead
of images? Would you need to break up the
footage into image frames?� In practice
quite often you would, because images just
tend to be pretty big, sorry, videos tend
to be pretty big. There's a lot of... So,
I mean, theoretically the time could be the
the fourth channel. Yeah, or, or a fifth channel.
Fifth channel. So if it's a full color movie
you can absolutely have, well I guess fourth,
because you, you can have, it would be a five,
rank five tensor being batched by time by
color by row by column, but often that's too
computationally and too memory-intensive.
Sometimes people just look at one frame at
a time, sometimes people use us a few frames
around, kind of, the key frame like three
or five frames at a time, and sometimes people
use something called a Recurrent Neural Network,
which we'll be seeing in the next week or
two, you can treat it as a sequence data.
Yeah, there's all kinds of tricks you can
do to try and go and work with that. Conceptually,
though, there's no reason you can't just add
an additional axis to your tensors and everything
just work. It's just a practical issue around
time and memory. And someone else noted that
it's pretty fitting that you mentioned the
movie The Mask. Yes, was another accident
I guess I got masks on the brain. I'm not
sure if we're allowed to like that movie anymore,
though and I've liked it when it came out.
I don't know of, what I think nowadays, it's
a while. Okay. So, let's take a look. So we
can untar data ML_100k. So ML_100k, is a small
subset of the full set. There's another one
that we can grab, which is about the whole
lot 25 million, but 100k is good enough for
messing around. So if you look at the README
you'll find the main table, the main table
is in a file called �u.data�. So let's
open it up with �read_csv� again. This
one is actually not comma separated values,
it's tab separated rather confusingly we still
use csv, and to say delimiter is a tab, �\t�
is a tab, and there's no row at the top saying
what the columns are called, so we say header
is None, and then pass in a list of what the
columns are called. �.head� will give
us the first five rows, and we mentioned just
before what it looks like. It's not a particularly
friendly way to look at it, so what I'm going
to do is I'm going to, cross tab it, and so
what I've done here is I've grabbed the top,
I can�t remember how many it was. Well,
who cares? One, two, three, four...
fifteen, or twenty movies, based on the most
popular movies and the top bunch of users
who watch the most movies. And so I basically
kind of reoriented this, so for each user
I have all the movies they've watched and
the rating they gave them. So empty spots
represent users that have not seen that movie.
So this is just another way of looking at
this same data. So basically what we want
to do is guess what movies we should tell
people they might want to watch. And so it's
basically filling in these gaps to tell user
212, do you think we would, they might like
movie 49 or 79 or 99 best to watch next. So
let's assume that we actually had columns
for every movie that represented, say, how
much sci-fi they are, how much action they
are, and how old they are. And maybe they're
between -1 and 1. And so like the last_skywalker
is very sci-fi, barely action, and definitely
not old. And then we could do the same thing
for users. So we could say user1 really likes
sci-fi, quite likes action, and really doesn't
like old. And so now if you multiply those
together, and remember in PyTorch and Numpy,
you have element-wise calculation, so this
is going to multiply each corresponding item.
It's not matrix multiplication. If you're
a mathematician, don't go there - this is
element-wise multiplication. If we want matrix
multiplication, it would be an @ sign. So
if we multiply each element together, next
with your equivalent element in the other
one, and then sum them up, that's going to
give us a number which will basically tell
us how much do these two correspond. Because
remember, two negatives multiplied together
to get a positive. So user1 likes exactly
the kind of stuff that the last_skywalker
has in it, and so we get 2.1. Multiplying
things together element-wise and adding them
up is called the dot product and we use it
a lot, and it's the basis of matrix (shouldn�t
say modification). matrix multiplication.
And so make sure you know what a dot product
is, it's this. So Casablanca is not at all
sci-fi, not much action, and is certainly
old. So if we do user1 times Casablanca, we
get a negative number. So we might think,
�OK, user1 won�t like, won't like this
movie. Problem is we don't know what the latent
factors are, and even if we did we don't know
how to label a particular user or a particular
movie with them. So we have to learn them.
How do we learn them? Well, we can actually
look at a spreadsheet, so I wrote a spreadsheet
version. So we have a spreadsheet version
which is basically, what I did was I popped
this table into Excel and then I randomly
created a (let�s count this now, 1, 2, 3,
4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15),
I randomly created a 15x5 table here. These
are just random numbers and I randomly created
a 5x15 table here. And I basically said, Okay,
well let's just pretend, let's just assume
that every movie and every user has five latent
factors. I don't know what they are. And let's
then do a matrix multiply of this set of factors
by this set of factors and a matrix multiply
of a row by a column is identical to a dot
product of two vectors. So that's why I can
just use matrix multiply now. This is just
what this first cell content so they then
copied it to the whole thing.
So all these numbers there are being calculated
from the row latent factors dot product with
or matrix model play with the column latent
factors. So in other words I'm doing exactly
this calculation, but I'm doing them with
random numbers. And so that gives us a whole
bunch of values, right. And then what I could
do, is I could calculate a loss by comparing
every one of these numbers here to every one
of these numbers here. And then I could do
mean squared error. And then I could choose
stochastic gradient descent to find the best
set of numbers in each of these two locations.
And that is what collaborative filtering is.
So that's actually all we need. So, rather
than doing it in Excel. And we�ll do the
Excel version later, if you're interested
(because we can actually do this whole thing
and it works in Excel), let's jump and do
it into PyTorch. Now one thing that might
just make this more fun is actually to know
what the movies are and MovieLens tells us
in u.item what the movies are called and that
uses the delimiter of the pipe sign weirdly
enough. So here are, here are the names of
each movie. And so one of the nice things
about Pandas, is it can do joins just like
SQL. And so you can use the merge method to
combine the ratings table and the movies table
and since they both have a column called movie,
by default it will join on those. And so now
here we have the ratings table with actual
movie names, that's going to be a bit more
fun. we don't need it for modeling, but it's
just going to be better for looking at stuff.
So we could use data blocks API at this point
or we can just use the built in application
factory method. Since it's there we may as
well use it, so we can create a collaborative
filtering data loaders object from a data
frame by passing in the ratings table. By
default the user column is called �user�,
and ours is, so fine. By default the item
column is called �item� and ours is not,
it's called �title�, so let's pick �title�
and choose a batch size. And so if we now
say show_batch, here is some of that data.
And the rating is called �rating� by default,
so that worked fine, too. So here's some data.
So we need to now create our (assume we're
going to use that five numbers of factors)
so the number of users is however many classes
there are for user and the number of movies
is however many classes there are for title.
And so these are, so we don't just have a
vocab now, right, we've actually got a list
of classes for each categorical variable,
for each set of discrete choices. So we've
got a whole bunch of users (944) and a whole
bunch of titles (1635). So for our randomized
latent factor parameters, we're going to need
to create those matrices. so we can just create
them with random numbers. This is normally
distributed random numbers, that's what randn
is. And that will be n_users, okay, so 944
by n_factors, which is 5. That's exactly the
same as this except this is just 15. So let�s
do exactly the same thing for movies. Random
numbers and movies by 5, okay. And so to calculate
the result for some movie and some user we
have to look up the index of the movie in
our movie latent factors, the index of the
user in our user latent factors and then do
a cross product. So in other words we would
say, Like oh okay, for this particular combination
we would have to look up that numbered user
over here and that numbered movie over here
to get the two appropriate sets of latent
factors. But this is a problem because look
up in an index is not a linear model. Like
remember, our deep learning models really
only know how to just multiply matrices together
and do simple element-wise nonlinearities
like ReLu. There isn't a thing called lookup
in an index. Okay, I�ll just finish this
bit. Here's a cool thing, though - the lookup
in an index can, is actually can be represented
as a matrix product, believe it or not. So
if you replace our indices with one-hot encoded
vectors, then one-hot encoded vector times
something is identical to looking up in an
index. And let me show you. So if we grab,
if we call the one-hot function, that creates
a, as it says here, one-hot encoding. And
we're going to one-hot encode the value 3
with n_users classes. And so n_users, as we
discussed, is 944, all right, then. So if
we go one-hot one-hot encoding the number
3 in to n_users one_hot_3, you get this big
array big tensor and as you can see in index
3, here 1 2 3, we have a 1 and the size of
that is 924. So if we then multiply that by
user_factors, our user_factors, remember,
is that random matrix of this size. Now what's
going to happen? so we're going to go 0 by
the 
first row and so that's going to be your 0s
and then we're going to 0 again we're gonna
0 again and then we're going to finally go
1, right, on the index 3-row and so it's going
to return each of them and then we'll go back
to 0 again. So if we do that (remember @ is
matrix multiply) and compare that to user_factors_3,
same thing. Isn't that crazy? So. it�s some
kind of wierd, inefficient way to do it, right
,but matrix multiplication is a way to index
into an array. And this is the thing that
we know how to do SGD with, and we know how
to build models with. So it turns out that
anything that we can do with indexing to array,
we now have a way to optimize. And we have
a question. There are two questions. One,
�How different in practice is collaborative
filtering with sparse data compared to dense
data?� We are not doing sparse data in this
course, but there's an excellent course out
here called Computational Linear Algebra for
Coders. It has a lot of information about
sparse. The fast.ai course. A second question,
�In practice, do we tune the number of latent
factors?� Absolutely, we do, yes. It's just
a number of filters like we have in at any
kind of a big planning model. All right, so
now that we know that the procedure of finding
out which latent set of latent factors is
the right thing, looking something up at an
index is the same as matrix multiplication
with a one-hot vector (I already had it over
here), we can go ahead and build a model with
that. So basically if we do this for a whole
for a few more indices at once, then we have
a matrix of one-hot encoded vectors. So the
whole thing is just one big matrix multiplication.
Now the thing is, as I said this is a pretty
inefficient way to do an index lookup, so
there is a computational shortcut, which is
called an embedding. An embedding is a layer
that has the computational speed of an array
lookup and the same gradients as a matrix
multiplication. How does it do that? Well,
just internally it uses an index lookup to
actually grab the values, and it also knows
what the gradient of a matrix multiplication
by our one-hot encoded vector is, or matrix
is, without having to go to all this trouble.
And so an embedding is a matrix multiplication
with a one-hot encoded vector where you never
actually have to create the one-hot encoded
vector. You just need the indexes. This is
important to remember because a lot of people
have heard about embeddings and they think
they�re something special and magical, and,
and they're absolutely not. You can do exactly
the same thing by creating a one-hot encoded
matrix and doing a matrix multiply. It is
just a computational shortcut, nothing else.
I often find when I talk to people about this
in person I have to tell them this six or
seven times before they believe me because
they think embeddings are something more clever,
and they're not. It's just a computational
shortcut to do a matrix multiplication more
quickly with a one-hot encoded matrix by instead
doing an array lookup. Okay, so let's try
and create a collaborative filtering model.
In PyTorch, a model, or an architecture, or
really an nn.module is a class. So to use
PyTorch to its fullest, you need to understand
object-oriented programming, because we have
to create classes. There's a lot of tutorials
about this, so I won't go into detail about
it, but I'll give you a quick overview. A
class could be something like Dog, or ResNet,
or Circle - and it's something that has some
data attached to it and it has some functionality
attached to it. Here�s a class called �Example�
- the data it has attached to it is �a�
and the functionality attached to it is �say�.
And so we can, for example, create an instance
of this class, an object of this type Example.
We pass in �Sylvain�, so �Sylvan�
will now be in ex.a and we can then say �ex.say�
and it will call �say� and it will say
passing it �nice to meet you� so that
will be X. And so it'll say, �Hello� self.a,
so that�s �Sylvian, nice to meet you�.
There, it is. Okay, so in Python the way you
create a class is to say class and its name,
then to say what is passed to it when you
create that object, it's a special method
called __init__. As we've briefly mentioned
before, in Python there are all kinds of special
method names of that special behavior - they
start with two underscores they end with two
underscores and we pronounce that �dunder�,
so �dunder init�, __init__. All methods
in, all regular methods instance methods in
Python always get passed the actual object
itself first (we normally call that self)
and then optionally anything else. And so
you can then change the contents of the current
object by just setting self.whatever to whatever
you like. So after this self.a is now equal
to �Sylvian�. So we call a method, same
thing - get passed self, optionally anything
you pass to it. And then you can access the
contents of self which you stashed away back
here when we initialized it. So that's basically
how object, or you know the basics of how
object-oriented programming works in Python.
There's something else you can do
when you create a new class, which is you
can pop something in parentheses after its
name, and that means we're going to use something
called inheritance. And what inheritance means
is, I want to have all the functionality of
this class, plus I want to add some additional
functionality. So, Module is a PyTorch class,
which fast.ai has customized, so it's kind
of a fast.ai version of a PyTorch class. And,
probably in the next course we'll see exactly
how it works. And, but it looks a lot like
a, it acts almost exactly like just a regular
Python class - we have an __init__, and we
can set attributes to whatever we like. And
one of the things we can use is an Embedding.
And so an Embedding is just this class that
does what I just described - it's the same
as an, as a linear layer with a one-hot encoded
matrix, but it does it with this computational
shortcut. You can say how many, in this case
users are there, and how many factors will
they have. Now, there is one very special
thing about things that inherit from Module,
which is that when you call them, it will
actually call a method called �forward�.
So �forward� is a special PyTorch method
name. It's the most important PyTorch method
name. This is where you put the actual computation.
So to grab the factors from an embedding,
we just call it like a function, right. So
this is going to get passed here the user
IDs and the movie IDs as two columns. So let's
grab the zero index column and grab the embeddings
by passing them to user_factors. And then
we'll do the same thing for the index one
column, that's the movie IDs, pass them to
the movie_factors. And then here there's our
element-wise multiplication and then sum.
And now remember we've got another dimension
this time. The first axis is the mini-batch
dimension, where we want to sum over the other
dimension, the index one dimension. So that's
going to give us a dot product for each user,
so for each rating for each user movie combination.
So this is the DotProduct class. So you can
see if we look at one batch of our data, its
of size, shape 64x2, because there are 64
items in the mini batch and each one has,
this is the independent variables, so it's
got the user ID and the movie ID. And, oh,
�Do deep neural network based models for
collaborative filtering work better than more
traditional approaches like SVG or other matrix?�
Let's wait until we get there. So, here is
X, right, so here is one user ID movie ID
combination, okay. And then for each one of
those 64, here are the ratings. So now we
created a DotProduct Module from scratch,
so we can instantiate it, passing in the number
of users, the number of movies, and let's
use 50 factors, and now we can create a learner.
Now this time we're not creating a cnn_learner
or a specific application learner, It's just
a totally generic learner. So this is a learner
that doesn't really know how to do anything
clever, it just draws away the data you give
it and the model you give it. And since we're
not using an application-specific learner,
it doesn't know what loss function to use,
so we'll tell it to use MSE and fit. And that's
it, right. So we've just fitted our own collaborative
filtering model where we literally created
the entire architecture, it's a pretty simple
one, from scratch.
So that's pretty amazing. Now the results
aren't great, if you look at the movie lens
data set benchmarks online you'll see this
is not actually a great result. So one of
the things we should do is take advantage
of the tip we just mentioned earlier in this
lesson, which is when you're doing regression,
which we are here, right. The number between
1 and 5 is like a continuous value we're trying
to get as close to it as possible. We should
tell fastai what the range is, we can use
the y_range as before, so here's exactly the
same thing we've got a y_range, we've stored
it away and then at the end, we use as we
discussed, sigmoid_range, passing in, and
look here, we pass in *self.y_range, that's
going to pass in by default 0,5.5. We can
see, yeah not really any better, um it's worth
a try, normally this is a little bit better
but it always depends on when you run it.
I just run it a second time while it's weird
looking. Um, now there is something else we
can do though, which is that if we look back
at our little Excel version. The thing is
here, when we multiply these latent factors
by these latent factors and add them up, it's
not really taking account of the fact that,
this user, may just rate movies really badly
in general, regardless of what kind of movie
they are. And this movie, might be just a
great movie in general because everybody likes
it, regardless of what kind of stuff they
like. And so it'd be nice to be able to represent
this directly, and we can do that using something
we�ve already learned about, which is bias,
we could have another single number for each
movie which we just add, and add another single
number for each user which we just add, right,
now we've already seen this for linear models.
You know this idea that it's nice to be able
to add a bias value. So let's do that. So
that means that we're going to need another
embedding for each user, which is of size
one it's just a single number we're going
to add, so in other words it's just an array
lookup, but remember to do an array lookup
that we can kind of, take a gradient of, we
have to say embedding. We will do the same
thing for movie bias and so then, all of this
is identical as before and we just add this
one extra line, which is to add the user and
movie bias values. And so let's train that
and see how it goes. Well, that was a shame
it got worse, so we used to have, not finished
here, 0.87 0.88, 0.89, so it's a little bit
worse. Why is that, well if you look earlier
on it was quite better it was point 0.86,
so it's overfitting very quickly and so what
we need to do is, we need to find a way that
we can train more epochs without overfitting.
Now we've already learned about data augmentation
right like rotating images and changing their
brightness and color and stuff, but it's not
obvious how we would do data augmentation
for collaborative filtering right. And so
how are we going to make it so that we can
train lots of epochs without overfitting.
And to do that we're going to have to use
something called regularization, and regularization
is a set of techniques which basically allow
us to use models with lots of parameters and
train them for a long period of time, but
penalyze them effectively for overfitting.
Or, in some way cause them to try to stop
overfitting and so that is what we will look
at next week. Okay well thanks everybody,
so there's a lot to take in here so please
remember to practice to experiment but listen
to the lessons again because you know for
the next couple of lessons things are going
to really quickly build on top of all the
stuff that we've learnt. So please be as comfortable
with it as you can, feel free to go back and
re-listen to it and go through and follow
through the notebooks, and then try to recreate
as much of them yourself. Thanks everybody
and I will see you next week or see you in
the next lesson whenever you watch it