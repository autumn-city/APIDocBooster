torch.nn.functional.cross_entropy function combines log_softmax(softmax followed by a logarithm) and nll_loss(negative log likelihood loss) in a single function, i.e.  it is equivalent to F.nll_loss(F.log_softmax(x, 1), y).
The cross here refers to calculating the entropy between two or more features / true labels (like 0, 1). 
 So yeah, the  negative log likelihood and the binary cross entropy are  equivalent.  And in practice in deep learning, people just say  cross entropy, multi category cross entropy, which would be a  multi class version of the negative log likelihood, which  we will cover later in this lecture when we talk about the  softmax function.  So just to keep it brief, the negative log  likelihood that we just covered a few videos ago, is the same as  what people call the binary cross entropy, they were just  formulated in different contexts.  So negative log  likelihood comes more like from, I think it's like, it's probably  from a statistics context, I don't know the first paper, or  reference that mentioned that.  But this is something usually I  see in statistics papers, and the binary cross entropy thing  has originated from the field of information theory, or computer  science.  So we have actually seen that, or not, the cross  entropy, where we have seen the self entropy, or just entropy,  and statistics 451. 
 And  there's also a multi category version is the multi category  cross entropy, which is just a generalization of the binary  cross entropy to multiple classes.  So in order to make  that negative log likelihood or binary cross entropy work for  multiple classes, we assume a so called one hot encoding, where  the class labels are either zero or one for some reason, it was  cut off here. 
 We have   to use something called cross entropy loss,  and this is actually the loss function that   fastai picked for us before without us  knowing. 
