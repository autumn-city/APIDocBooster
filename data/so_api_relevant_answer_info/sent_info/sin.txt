Yes, there are software algorithms for calculating sin too.
Basically, calculating these kind of stuff with a digital computer is usually done using [numerical methods (hyper-link)] like approximating the [Taylor series (hyper-link)] representing the function.
Numerical methods can approximate functions to an arbitrary amount of accuracy and since the amount of accuracy you have in a floating number is finite, they suit these tasks pretty well.
For sin specifically, using Taylor expansion would give you:
sin(x) := x - x^3/3!
Note: (1) works because of the aproximation sin(x)=x for small angles.
Functions like sine and cosine are implemented in microcode inside microprocessors.
First of all they use [CORDIC (hyper-link)], but they may also use a short Taylor series to polish up the result of CORDIC or for special cases such as computing sine with high relative accuracy for very small angles.
Intel-like CPU of the x86 family have a hardware implementation of the sin() function, but it is part of the x87 FPU and not used anymore in 64-bit mode (where SSE2 registers are used instead).
Software implementations of transcendental functions such as sin() typically use approximations by polynomials, often obtained from Taylor series.
Here is an example for cosinus:
using this we can get the new term of the sum using the already used one (we avoid the factorial and x2p)
If there is no hardware support then the compiler probably uses the latter method, emitting only assembler code (with no debug symbols), rather than using a c library --- making it tricky for you to track the actual code down in your debugger.
In GNU libm, the implementation of sin is system-dependent.
Since October 2011, this is the code that actually runs when you call sin() on a typical x86-64 Linux system.
It is apparently faster than the fsin assembly instruction.
Source code: [sysdeps/ieee754/dbl-64/s_sin.c (hyper-link)], look for __sin (double x).
When x is very very close to 0, sin(x) == x is the right answer.
A bit further out, sin(x) uses the familiar Taylor series.
When the angle is more than about 7°, a different algorithm is used, computing Taylor-series approximations for both sin(x) and cos(x), then using values from a precomputed table to refine the approximation.
When |x| > 2, none of the above algorithms would work, so the code starts by computing some value closer to 0 that can be fed to sin or cos instead.
Older 32-bit versions of GCC/glibc used the fsin instruction, which is surprisingly inaccurate for some inputs.
fdlibm's implementation of sin in pure C is much simpler than glibc's and is nicely commented.
Source code: [fdlibm/s_sin.c (hyper-link)] and [fdlibm/k_sin.c (hyper-link)]
I'll try to answer for the case of sin() in a C program, compiled with GCC's C compiler on a current x86 processor (let's say a Intel Core 2 Duo).
pow, sin and cos for power, sine, and cosine respectively).
But the GCC compiler wants you to link to the [math library (hyper-link)] (libm.so) using the -lm compiler flag to enable usage of these math functions.
Now the compiler may optimize the standard C library function sin() (provided by libm.so) to be replaced with an call to a native instruction to your CPU/FPU's built-in sin() function, which exists as an FPU instruction (FSIN for x86/x87) on newer processors like the Core 2 series (this is correct pretty much as far back as the i486DX).
Now if the program executed the software version of the sin() function, it would do so based on a [CORDIC (hyper-link)] (COordinate Rotation DIgital Computer) or [BKM algorithm (hyper-link)], or more likely a table or power-series calculation which is commonly used now to calculate such transcendental functions.
Any recent (since 2.9x approx.)
version of gcc also offers a built-in version of sin, __builtin_sin() that it will used to replace the standard call to the C library version, as an optimization.
Look at file dosincos.c located in unpacked glibc root\sysdeps\ieee754\dbl-64 folder
BTW, they do use Tailor series to calculate sine and cosine.
They come in calculating transcendental functions from scratch (using Taylor's series) as if nobody had ever done these calculations before in their lives.
If you had a really fast divider, the fastest way to calculate sine might be P1(x)/P2(x) where P1, P2 are Chebyshev polynomials.
Then you choose the appropriate combination of Chebyshev polynomials (is usually of the form cos(ax) = aP(x) for cosine for example, again where P is a Chebyshev polynomial).
Then you implement your algorithm in software in the form:
(((p4.x + p3).x + p2).x + p1).x + p0
....and this is how you'd calculate cosine to 7 decimal places on that hardware.
Square root is faster to use a double iteration of Newton raphson method using a lookup table first.
Now, that being said, there are techniques whereby the Chebyshev polynomials can be used to get a single precision result with a low degree polynomial (like the example for cosine above).
Computing sine/cosine/tangent is actually very easy to do through code using the Taylor series.
if you want sin then
For example for the sine function, the error near x = 0 should be much smaller than for larger values; you want a small relative error.
So you would calculate the Chebyshev polynomial for sin x / x, and multiply that polynomial by x.
And with functions like the sine function, if you are careless then it may be possible that the result that you calculate for sin x is greater than the result for sin y even when x < y.
For example, sin x = x - x^3/6 + x^5 / 120 - x^7 / 5040...
If you calculate naively sin x = x * (1 - x^2/6 + x^4/120 - x^6/5040...), then that function in parentheses is decreasing, and it will happen that if y is the next larger number to x, then sometimes sin y will be smaller than sin x.
Instead, calculate sin x = x - x^3 * (1/6 - x^2 / 120 + x^4/5040...) where this cannot happen.
For example for sin (x), where you need coefficients for x, x^3, x^5, x^7 etc.
you do the following: Calculate the best approximation of sin x with a polynomial (ax + bx^3 + cx^5 + dx^7) with higher than double precision, then round a to double precision, giving A.
Now calculate the best approximation of (sin x - Ax) with a polynomial (b x^3 + cx^5 + dx^7).
Then approximate (sin x - Ax - Bx^3) with a polynomial cx^5 + dx^7 and so on.
Here is an implementation (originally from the ZX Spectrum ROM): [https://albertveli.wordpress.com/2015/01/10/zx-sine/ (hyper-link)]
Here's the sin function:
[http://git.uclibc.org/uClibc/tree/libm/s_sin.c (hyper-link)]
[http://git.uclibc.org/uClibc/tree/libm/k_sin.c (hyper-link)]
If there is no tail, an approximate answer is generated using a polynomial of degree 13.
If there is a tail, you get a small corrective addition based on the principle that sin(x+y) = sin(x) + sin'(x')y
Concerning trigonometric function like sin(), cos(),tan() there has been no mention, after 5 years, of an important aspect of high quality trig functions: Range reduction.
When a large value was requested like sin(pow(2,30)), the results were meaningless or 0.0 and maybe with an [error flag (hyper-link)] set to something like TLOSS total loss of precision or PLOSS partial loss of precision.
Good range reduction of large values to an interval like -π to π is a challenging problem that rivals the challenges of the basic trig function, like sin(), itself.
Sample: [sind() (hyper-link)]
The W3C doc doesn't use concepts like wrong and sin, but it does use those like provide the means, may be appropriate and discouraged.
Why for sin() in <math.h>, do we need -lm option explicitly; but,
  not for printf() in <stdio.h>?
Because both these functions are implemented as part of the "Single UNIX Specification".
The [UNIX wars (hyper-link)] led to increasing divergence from the original AT&T UNIX offering.
For truly size constrained applications, it permits reimplementations of the math library in a non-standard way (like pulling out just sin() and putting it in a custom built library.
The maximum derivative of sin is 1, so if if x1 and x2 are within epsilon of one another, then sin(x1) and sin(x2) are also within epsilon.
This can be faster than what the native code does, since the native code (probably) uses a lookup table for polynomial coefficients, and then interpolates, and since this table can be small enough to stay in cache easily.
If you wanted, you could also create a lookup table over (1/sin(x)), since that's your actual function of interest.
Either way, you'll want to be careful around sin(x) = 0, since a small error in sin(x) can cause a big error in 1/sin(x).
For sin (but not atan) you can actually get simpler than a table--just create
First, you're taking atan(value) to get the angle, and then using value again with sin to compute h.  So we have the scenario where one side of the triangle is 1:
I've always used lookup tables for sin and cos, and found them to be fast.
The answer now is -sin(x), what I wanted.
As for the sin(pi) thing, the issue is that process_sympy() cannot distinguish the pi as a symbol from pi as a number.
This expression cannot be evaluated just by calling sympify, since it has an attribute telling not to evaluate it.
You solved the problem by converting that expression to a string and then sympifying it, but a more direct way of doing it is just by using the doit() method in the expression:
The funm(A,@sin) call performs a matrix sin operation, which is a different operation from the sin(A) call, which performs the sin of each individual entry in the matrix.
The matrix sin is instead performed by computing a power series as defined here:
[http://www.johndcook.com/blog/2008/03/14/what-is-the-cosine-of-a-matrix/ (hyper-link)]
This page also provides a good discussion about what a matrix sin is used for, and how it differs from the sin of an individual element.
But if the sin is at the beginning then pos will be 0, not 1, and so your for loop will never execute at all.
Also, note that you can't test for std::sin being defined: You'll need to set up a suitable macro name.
If 'sin' exists in namespace std, the it will be directly called with arguments of double.
If it doesn't then the value will be implicitly converted to 'MYLIB_double' and the overload will be called which will call sin in either the std or (since std::sin(double) doesn't exist), the global namespace.
This function controls the movement (height specifically) of the character such that the amount of movement is controlled via a [sine wave (hyper-link)] over time.
And a sample run using your test data, showing it in action:
Good start, but if we're recursing, what we're really looking for is something that only computes one of those terms, and then calls itself with updated arguments to compute the next term.
You will probably need to be a government agency to see a list of SINs.
Also, as Paul noted above, any algorithm which verifies whether a SIN is a possible candidate can also be used to manufacture SINs that aren't legitimate through brute force.
If there is an algorithm to generate SIN's and a database of assigned SIN's you could combine the two to search through and find a real SIN in fairly short order allowing you to gather a list of all active SIN's.
It also contains information on valid uses for SINs.
Having worked with SINs before I do know that there's some correlation to the beginning 3 numbers of the SIN and the person's age.
With that said Why are you using SIN numbers?
It is an extremely vital piece of information - If you have someone's SIN number you effectively are them - and should under no circumstances be used by anyone except for government related purposes!
Secondly if you have a legitimate usage of SIN numbers, are you familiar with [PIPEDA (hyper-link)]?
Enter sin on request and it will tell you (1) if valid, (2) if you have do not have right to information on the sin--indirectly verifying it,or (3) access if your represent a client is now on file for the sin in question.
There's a bunch of potential implementations of sin and friends in [this SO question (hyper-link)], but typically it boils down to a few usual methods:
Built-in processor code (fsin)
For example, 1.57079637 is not exactly pi/2 so its sin() may not be exactly 1.
I would advise you to consider using a [switch-case (hyper-link)] statement instead of if-else for letting the user decide which function to apply.
Use the existing function Math.sin() and Math.cos()
if you want to initial convert of 1 to 1*Pi do this:
), use Math.sin(Math.PI*2*a)
echo sin(2 * pi()); //returns -2.44929359829E-16 (should be 1)
echo sin(1 * pi()); //returns 1.22464679915E-16 (should be 0)
One option would be to use the Taylor series for sine to get an approximation:
sin(x) = x - x3 / 3!
If you evaluate the first five or six terms of this polynomial and x is small, you'll probably get a very good approximation for sin x.
You could look into numerical approximation using [Taylor Series (hyper-link)].
sin(x) = x − x^3/3!
You can see the points used by Plot using the following command
You can see that inserting a single evaluation point in the center of the hole will produce almost identically looking plot.
The way to increase the angle used to decide when to subdivide by using Refinement option (I got it from the book, but it doesn't seem to be documented in product)
Here you can see that increasing it by 1 degree from default 5 fixes the hole.
You are looking at something called "aliasing".
sin is a periodic function with a period of 2*pi (because it's in radian, not in degrees).
In the second plot, however, linear interpolation between the few points of int1 paints a sine-wave that is not really there.
What you're describing is a signal processing problem called [aliasing (hyper-link)].
Basically, if you don't sample a sine wave often enough, the discretized sine wave can appear to have a lower frequency than the actual continuous wave did:
sin(x) has a frequency of 1 rad/s so you must sample at least as often as 2 rad/s = 0.318 Hz, or about 1 sample for every 3 units.
shouldn't you be using fabs and not abs?
When dealing with doubles, you should be using fabs instead.
You may wish to double-check which version you are using.
"for angle 1.51423 it states that sin = 29"
That's most probably an error of observation, not an error of the sin function.
Math.sin expects a value in radians
I think sine squared can be done with either:
r = (sin(theta)).^2 + (cos(theta)).^2
They discuss how TI calculators perform sine regression.
[Algorithm Used for Calculating the Sine Regression (hyper-link)]
You've mixed up your imports a bit by importing the individual functions and also referring to them using np.
You're not going to need numpy's sin, cos, and pi methods because we'll use the sympy ones.
This long expression strings get really confusing really fast.
Sympy has it's own version of sin and pi which you are interchanging with Numpy's versions.
np.sin(arr) on such an array is performed by calling [x.sin() for x in arr], i.e.
it delegates the task to the sin method of each element.
The only indication of your using sympy is the sym.Matrix call.
At best it will try to evaluate the code with by passing symbols to the numpy code, resulting in this error.
Matrix makes a sympy object by passing the indices to the function.
If we try to use np.sin in this function:
As I explained above, trying to use nonumeric values as np.sin arguments results in the error in delegation to a sin method.
Your code further complicates things by passing the values through the scipy.quad, which in turn passes the integration variable x plus n and r to Phi_V.
Using the function that @Ian suggests, using the sympy versions of sin, etc:
You're using two different units.
The reason you're getting different results is because the calculator is giving you the sin of 27.5 degrees, whereas Google is giving you the sin of 27.5 radians (which is equivalent to 1576 degrees).
As for your Java program, which is what we actually care about on this site, Java's built-in [Math.sin (hyper-link)] and [Math.cos (hyper-link)] work in radians.
According to the [docs (hyper-link)] the parameter that sin takes is
I moved x and y into the local scope and used radians in my sin() and cos() functions.
For the last two, I called the asin and acos before converting.
From the help page on sin which you can read by typing in ?sin in the R console:
To get the sin(52.517) you need to convert those degrees to a radian measure first:
The taylor series expansion for sine is a series of terms with odd taylor's coefficients that alternate  in sign.
Below is your sin func with minimal changes to make it work.
sin was not being initialized before being modified
the extra subtraction of rads at the end from sin was not required.
Seemed that the ACOS/COS/SIN extensions I found for Doctrine were not really that great.
[https://github.com/wiredmedia/doctrine-extensions/blob/master/lib/DoctrineExtensions/Query/Mysql/Sin.php (hyper-link)]
Some implementations of sinf are just simple calls to sin with pre and post conversion to and from float.
For example see : [http://code.metager.de/source/xref/gnu/octave/gnulib-hg/lib/sinf.c (hyper-link)]
sin & cos are not functions that are built into vba.
They are, however, built into excel and can be accessed through the $WorksheetFunction$ object of vba as $WorksheetFunction.Sin$ and $WorksheetFunction.Cos$.
I had to add Imports System.Math at the top to make the compiler recognise Sin, Cos etc.
[http://en.wikipedia.org/wiki/Sine?section=9#Properties_relating_to_the_quadrants (hyper-link)]
According to the [Action Script reference (hyper-link)] the sin function expects the parameter as radian.
Flash is using radians units, the calculator is using degree units.
Using float requires conversions.'
In C, cos is double precision and cosf is single precision.
In C++, std::cos has overloads for both double and single.
Here are my results, using the equation "y = amplitude * sin(pi * (x - center) / width) * sin(pi * (x - center) / width" with fitted C# code.
I suggest making a test using this equation with these parameter values as initial parameter estimates.
f(x) = a + c*sin(x+b)*sin(x+b) ):
As @paxdiablo mentioned, there's just too many lines of information, so a way to fix that is by removing endl and adding \t for degree, sin and cos as follows, the following code displays all the required information as I checked.
The code is almost same but I removed using namespace std and replace your for loop body with the following code:
@Michel JS this one uses vectors as u asked, took some time since I didn't know much about it but here u go, I'm hoping this is what you wanted.
One thing to keep in mind is that the range of the sin function is -1.0 to 1.0.
To make a sine wave that does not look like a straight line you can multiple by a factor to make it have a bigger amplitude.
Here is an example that is similar to your code that draws a sine wave that you can see.
Now that we can plot the sine of x we can move on to plotting the sine of 1/x.
Next we note that the sine of 1/x where x is positive will be approaching zero as 1/x will approach zero as x gets large.
Since we know that the sine of zero is zero we expect our plot to approach zero.
Instead of using x for our angle measurement we can use a variable named theta for the angle.
Take a look at [sin(1/x) and x sin(1/x) Limit Examples (hyper-link)] for a deeper look at what is going on with this function.
When code uses <math.h>, the way to call the sin function is sin(theta).
When code uses <cmath>, the way to call the sin function is std::sin(theta) (unless the code uses the abomination using namespace std).
From math:
sin(A + B) == sin(A) * cos(B) + cos(A) * sin(B)
cos(A + B) == cos(A) * cos(B) - sin(A) * sin(B)
Store sin(A) and cos(A) in two variables.
Then for updating them use temporary copy of one of them, otherwise you will update the second using the new instead of the old value of the first.
persistent objectX  stores current and is initialised with initial sin(timeFactor)
("persistent" as in "keeps value across executions of update code",
 in contrast to "temporary" as in "only keeps value during update code";
 this is to avoid using the "global" attribute, which implies poor data design)
Credits to John Coleman for spotting the problem in initial idea to use
1 == sin(A)*sin(A)+cos(A)*cos(A)
That would have been actually
sin(time+delta)== f(sin(time), delta)
But it fails for 50% of a full period.
So I hope this
sin(time+delta)==f(sin(time), cos(time), delta)
also helps.
PS: I always check that [Sin]s are valid on import or entry (code)
you probabely have sin,cos etc imported from org.apache.spark.sql.functions (they take Column).
If you want scalas math functions, write import math._ inside haversine, or use m̀ath.sin, math.cos
Here's a working, slightly-simplified form of your program that calculates sin for a range of numbers from 0 to 3, showing the stdlib calculation to compare.
Square root is relatively simple to calculate, but I'm not convinced it's a huge difference between sqrt(1-sin*sin) and calculating cos again.
What processor may also be a factor, and what other calculations are done "around" the sin/cos calculations.
The GNU C library has [a sincos() function (hyper-link)], which will take advantage of the "FSINCOS" instruction which most modern instruction sets have.
If you don't do that, I'd go with the "sqrt(1-sin(x)^2)" route.
In every processor architecture document I've looked at so far, the FSQRT instruction is significantly faster than the FSIN function.
If precision is not critical the fastest way to get sin or cos is to use tables.
Hold some global const array with sin and cos values for all agles with a step you need.
So your sin/cos function just need to cast angle to index and you get the result.
FMath::Sin means your using a sin function from the FMath class.
:: means inheriting from or "coming from" so think of it as "FMath has a function im calling called Sin"
uses the FMath::Sin Sin method of the FMath class to do a basic sin calculation.
You can do an easy [tutorial on sin and cosine here (hyper-link)]
If your code was System.out.println("| sin(" + currentPoint + ") = " + Math.sin(currentPoint)); you would expect this:
In other words, the sine of 360 radians is 0.9589, but the sine of 360 degrees is 0.
System.out.printf("| sin(%d) = %.7f\n", currentPoint, Math.sin(Math.toRadians(currentPoint)));
Sin(360 degrees) = 0
Sin(180 degrees) = 0
Sin(150 degrees) = .5
Sin(120 degrees) = .866
For some reason it displays the sin of 360 to be -0.00.
Below is the API description of the [Math.sin (hyper-link)] method.
sin
public static double sin(double a)
It's expressing values in bits that's biting you.
In contrast, given any fixed number of bits, most calculations with real numbers will produce quantities that cannot be exactly represented using that many bits.
Also note that Math.PI, which is a double value, is not PI, but just an approximation of PI, and Math.sin(Math.PI) gives you the double value which is the closest to actual mathematical value of sin(Math.PI).
To get the expected answer, as sin() & cos() are between -1, 0 , +1, try to add 1 round it to the accurancy needed and substract 1.
Math.sin(Math.toRadians(90)) and the result must be 1
the below code gives the answer for the sine of an angle using Maclaurin series
Sub testDBLSIN()
Debug.Print "VBA SIN", sin(0.25 * Pi)
Debug.Print "TAYLOR", DBLSIN(0.25 * Pi), "DELTA", DBLSIN(0.25 * Pi) - sin(0.25 * Pi)
End Sub
Function DBLCOS(x, Optional ITER As Long = 10) As Double
    Dim P, k, x2, d, f, c As Double
    f = 2
    c = 2
    k = 1
    x2 = x * x
    SUM = 1
    P = 1
    For I = 1 To ITER
        k = -k
        P = P * x2
        SUM = SUM + k * P / f
        c = c + 1
        f = f * c
        c = c + 1
        f = f * c
    Next
    DBLCOS = SUM
End Function
 Function DBLSIN(x, Optional ITER As Long = 10) As Double
    Dim P, k, x2, d, f, c As Double
    k = 1
    c = 3
    f = 6
    P = x
    x2 = x * x
    SUM = x
    For I = 1 To ITER
        k = -k
        P = P * x2
        SUM = SUM + k * P / f
        c = c + 1
        f = f * c
        c = c + 1
        f = f * c
    Next
    DBLSIN = SUM
End Function
sin() takes radians, and Windows calculator takes degrees by default.
You can calculate the vector z which gives you (cos theta, sin theta) when you add to (1,0), where theta = 2*pi/step.
Then you add this vector to (1,0) and get the next  sin value as the y-coordinate of the sum.
Then you rotate z by angle theta (by multiplying by the rotation matrix through angle theta) and add this to your previous vector (cos theta, sin theta), to get the next sin value as the y-coordinate of the resultant sum vector.
This requires computing cos theta and sin theta just once, and then each update is given by a matrix multiplication of a 2x2 matrix with a 2-d vector, and then a simple addition, which is faster than computing sin() using the power series expansion.
Provisionally, If you think of another name of sin  the !.
execute :
sin(2+3):
sin(3)+3
So logic is good the same, but you will need to change to handle the input of string from single character.
You could consider using the built in Fourier transform that MATLAB provides instead of writing your own.
The key point in here is using window function.
Try reducing the timestep, by increasing the 1003 value.
All you need is to add multiple sinus graphs to a customPlot object, and add an offset to each sinus.
If you want to work in degrees you should make the conversion (or define your own sin_degrees function) like:
I am guessing google uses degrees as angle measure in sin, while c++ uses radians and that is the reason for the difference.
You cannot take the arcsine of a number outside [-1, 1].
There is no angle that can produce a sine outside of that range.
The output range of sine is [-1, 1], so the inverse sine of any number larger than 1 is meaningless on the real line.
Because the arcus sinus returns result in radians and radian is defined as
Using a lookup table is probably the best way to control the tradeoff between speed and accuracy.
You can make a look-up table with sin values for some values and use linear interpolation between that values.
The sine can be approximated by straight lines of slopes 2/pi and -2/pi on intervals [0, pi/2], [pi/2, 3*pi/2], [3*pi/2, 2*pi].
A rational algebraic function approximation to sin(x), valid from zero to π/2 is:
(Using subroutine DHFTI, by Lawson & Hanson).
The results should be within about 5% of the real sine.
For advanced expression parsing and evaluating, use 3rd party solutions
such as [DDMathParser (hyper-link)].
First of all - Math.pow(x, 2.0) is equal to sin * sin, and the second approach should be much more efficient.
When it comes to answer to your question - does the variable "sin" you want to use in second Math.pow() exists?
The problem with your current code is that you are trying to access a variable named sin but that variable doesn't exist.
Since you're starting with Java, let's start over.
You want to calculate h = v² sin²( θ ) / ( 2 g ).
First, the numerator is v² sin²(θ).
Then we need to calculate sin(θ) and square the result.
If we simplify this into a single expression, we come up with:
What you are describing is known as the "arcsine" function.
It's available in Java as [Math.asin() (hyper-link)].
Use the [arcsin (hyper-link)] function
You can use Math.asin(0.324); to get the value of x.
Please read the [Math.asin() (hyper-link)] javadoc to be aware of what is returned by the function.
You have to use the inverse function or arcsine.
It has the same relationship with sine as dividing has with multiplication.
To calculate sine inverse in Java you can use
It returns the arc sine of a value; the returned angle is in the range -pi/2 through pi/2..
you need to use the these Methods, Math.toDegrees and Math.asin, in the next order:
Remember that the asin() function return a double RADIANS result.
Static methods are invoked using this operator as well.
If you go with option 1, you can register the sin() function, and the expression will work unchanged.
For your fastSin(), you should check its documentation to see what range it's valid on.
The units you're using for your game could be too big or too small and scaling them to fit within that function's expected range could make it work better.
The result is that sin is about 1.8x slower (if fastSin takes 5 seconds, sin takes 9).
You could Also use two lookup tables on for degrees and one for tenths of degrees and utilize sin(A + B) = sin(a)cos(b) + cos(A)sin(b)
the fastsin function) is approximating the sine function using a parabola.
(Edited to specify what is approximating the sine function using a parabola.)
You can compute a table S of 256 values, from sin(0) to sin(2 * pi).
Then, to pick sin(x), bring back x in [0, 2 * pi], you can pick 2 values S[a], S[b] from the table, such as a < x < b.
memory saving trick : you actually need to store only from [0, pi / 2], and use symmetries of sin(x)
If you rephrase the return in fastSin as
you can see that y is a parabolic first-order approximation to sin(x) chosen so that it passes through (0,0), (pi/2,1) and (pi,0), and is symmetrical about x=pi/2.
If we want values outside that range we can use the 2-pi periodicity of sin(x) and that sin(x+pi) = -sin(x).
(I'm not sure why y*abs(y) is used rather than just y*y since y is positive in the 0-pi range).
We might expect y to be a decent approximation to sin(x), but the hope is that by picking a good value for P we get a better approximation.
I would find a and b by in the same way as above, this time giving a system of two linear equations in a and b to solve, rather than a single equation in p. I'm not going to do the derivation as it is tedious and the conversion to latex images is painful... ;)
NOTE: When answering another question I thought of another valid choice for P.
The problem is that using reflection to extend the curve into (-pi,0) leaves a kink in the curve at x=0.
In stead of calling the cos() or sin() every cycle of the loop, create variable before the loop which contains the value of cos(angle) or sin(angle) already.
Math.sin(..) make an interpolation to get a good value, therefore you cannot expect to get exact values as result.
In addition while interpolating the sin value the problems of float/double arithmetic is an other issue.
Because you're passing an int value that may equally become a float or double, the compiler complains.
For your specific case, it may be exactly what I've shown, in that you're calling sin() with an integral type.
If that is the case (using integral types), the first thing you should realise is that sin() takes its argument as the number of radians rather than degrees, so you will almost certainly want to use floating point (there are 360° in a circle but that's only 2π radians).
using namespace std;
Negating an angle will negate its sine, but leave its cosine unchanged.
You are calling sin() with an argument that it doesn't support.
math.sin() must take a numerical value as its only argument.
Or else, you need to use sym.sin instead of math.sin:
I'm not sure this is going to work in all possible cases (nlsat is a bit special), but you can use get-value to get arbitrary expressions evaluated under the model, e.g., after (check-sat-using ...) you can add
First of all, a cosine of 180 degrees should be equal to -1, so the result you got is right.
Secondly, you sometimes can't get exact values when using sin/cos/tan etc functions as you always get results that are the closest to the correct ones.
In your case, the value you got from sin is the closest to zero.
The value of sin(PI) that you got differs from zero only in the 9th (!)
C/C++ provides sin(a), cos(a), tan(a), etc.
OP's code with 180 passed to  DegreesToRadians(d) and then to sin()/cos() gives results that differ than expected due to rounding, finite precision of double() and possible a weak value for PI.
The below reduces the angle first to a -45° to 45° range and then calls sin().
This will insure that large values of N in sind(90.0*N) --> -1.0, 0.0, 1.0.  .
Note: sind(360.0*N +/- 30.0) may not exactly equal +/-0.5.
My solution is to use the new math.h functions __cospi() and __sinpi().
Performance is similar (even 1% faster) than cos() and sin().
Trying to optimize sin() and cos() (that are already optimized to the hilt) won't gain you much.
the other operations are going to take up time that is comparable to sin and cos. Did you run the profiler by splitting up the expression into its constituents?
That should give you an idea of how much the relative cost of sin() and cos() is.
Are you trying to set up a surface that when looked on edge it looks like a sine wave?
i.e y = A * sine (w * x + p) where A is amplitude, w is angular frequency, and p is phase.
Sine is a continuous function but you are take only 150 samples.
Issue no 1 is that the formulas you are using
Going once round a circle is 360 degrees or 2*pi, so you can convert from degrees to radians by multipling by pi/180, as shown below in python code to incorrectly and then correctly get the sin of 90 degrees.
As this is an assignment, I won't do it for you, but a related example is the series for sinh(x).
You can produce the terms in "one shot", using a Python [list comprehension (hyper-link)].
You can produce sin and cos in a similar way.
The one missing ingredient you need is the signs at each point in the series.
I got this result:
    Enter an angle (in degrees): 180 Enter the number of terms to use: 5
    Cos:-0.976022212624, Sinus:0.00692527070751
Note that because the Maclaurin series is centered on x=0, values of theta closer to 0 will converge much faster: calc_sin(radians(-90), 5) is -1.00000354258 but calc_sin(radians(270), 5) is -0.444365928238 (about 157,000 times further from the correct value of -1.0).
One can completely avoid all power and factorial computations by using and combining the recursive definitions of factorial and integer powers.
Further optimization is obtained by computing both cos and sin values at once, so that the powers are only computed once.
Your linspace starts from 0, thus when you evaluate the function to integrate, at the beginning of the trapezoidal integral, you have: sin(0)/0 = nan.
When you get the first nan, nan + 1.0 = nan: this means that in your code, when you are summing up the integral over the intervals, the first nan is messing up all of your results.
This is your code fixed (I'm running it in python2, this is what is installed in the pc I'm using now):
Discalimer The limit of sin(x)/x -> 1 for x -> 0, but due to floating rounding for sin(1e-12)/1e-13 = 1!
You could make the function return 1 (the limit of sin(x)/x in 0) instead of NaN for x == 0.
Also, Taylor approximation of sin function works only in the interval (-pi, pi).
After your comment, it got clarified that you do not need a single point but a horizontal line.
To do so, you first define an array containing values 2.7 by using np.ones(100) * 2.7 and then just pass it to the function.
For plotting a single point at x=2.7, there are two ways (among possible others).
First option is to just specify the two x-y numbers and plot using a  marker as
The first condition ensures that the two points are exactly non-overlapping, and the second ensures that b is on the curve you want (in this case, I think this function is Math.sin).
According to [this answer (hyper-link)] on math.stackexchange.com, the solution for calculating the arc length along a sine wave is an elliptic integral which is difficult to calculate analytically.
Fortunately you have a computer at your disposal that can apply the same formula using numerical integration to achieve an approximation of the desired curve.
Given the derivative of sin(x) is cos(x), if you increment your x variable very slowly (with a per-step increment of dx) then at each step your ds will be dx * sqrt(1 + cos(x)^2).
When the variable demoivre is true, complex exponentials are converted into equivalent expressions in terms of circular functions: exp (a + b*%i) simplifies to %e^a * (cos(b) + %i*sin(b)) if b is free of %i.
Here is an example using the [Maxima Online Calculator (hyper-link)]
This is because your sin call is a constant value and gcc optimizes it out (even when compiling with -O0 and without -lm).
There is no call for sin here.
When you need both, always compute the sin and cos of a value at the same time.
Make sure you're using all the processor has to offer - x64 architecture, plus any vector instructions that would help.
Have you tried benchmarking it against Math.Cos(), or other methods of approximating trig functions (you can get very good approximations with a few simple multiplications using [Taylor Series (hyper-link)])
Of course, as BlueRaja pointed out, using C++ will almost certainly help as well.
Using assembly language probably won't do much good though -- for a table lookup like this, a C++ compiler can usually produce pretty good code.
At one time, there was no question that using tables to avoid complex calculations was a solid strategy.
One thing you could try would be to use the fact that cos(x) = sin(x + pi/2).
And make the sine table one quarter larger, so you can reuse it as the cosine table starting one quarter in.
But even if not, the decreased cache usage might be worth more than the added time for the offset into the sine table.
cos(x) = sin(pi/2-x).
sin(pi + x) = -sin(x)
Compare with Math.Sin anyway.
But even a unsafe, optimized version does not seem to come anywhere near [Math.Sin (hyper-link)].
The [sin (hyper-link)] of large numbers like 6.2831853071795856E+45; makes no sense.
Consider using static analysis tools like [Fluctuat (hyper-link)] on your C or C++ code.
As a rule of thumb, trigonometric functions like sin, cos, tan  should not be used with large numbers (e.g.
It will slow down calculations by a factor of thousands, but it could give you meaningful results for sin (pi * 1.e+45)
Try also to use [GNUplot (hyper-link)] to "visualize" the sin function (for reasonable numbers).
Mathematically the sin of every finite real number is between -1 and 1.
So Math.Sin(6.2831853071795856E+45) = 6.2831853071795856E+45 is wrong.
As a sidenote there was a change in how sin/cos/tan are calculated .NET.
Sin, Cos, and Tan on AMD64 Windows were previously implemented in vm\amd64\JitHelpers_Fast.asm
by calling x87 floating point code (fsin, fcos, fptan) because the CRT helpers were too slow.
They are using the same CRT other C++ programs are using.
.NET Core >= 2.0 (haven't tested previous versions): Math.Sin(6.2831853071795856E+45) == 0.824816390616968
.NET Framework 4.8 (32 and 64 bits): Math.Sin(6.2831853071795856E+45) == 6.28318530717959E+45
Made some checks, and Math.Sin(6.2831853071795856E+45) == 6.28318530717959E+45 is the "official" answer of the fsin opcode of [x87 (hyper-link)] assembly, so it is the "official" answer of Intel (about 1980) about how much is the sin of 6.2831853071795856E+45, so if you use an Intel, you must trust that Math.Sin(6.2831853071795856E+45) == 6.28318530717959E+45, otherwise you are a traitor!
The true value of sin(6.2831853071795856 × 10⁴⁵) is approximately 0.09683996046341126; this approximation differs from the true value by less than 10⁻¹⁶ parts of the true value.
(I computed this approximation using [Sollya (hyper-link)] with 165 bits of intermediate precision.)
However, you won't get this answer by asking a C# function with the signature public static double Sin (double a), or a C++ function with the signature double sin(double).
6.2831853071795856 × 10⁴⁵ is not an IEEE 754 binary64, or ‘double’, floating-point number, so at best you will learn what the sin of a nearby floating-point number is.
The IEEE 754 binary64 floating-point number 6283185307179585571582855233194226059181031424 is a good approximation to 6.2831853071795856 × 10⁴⁵ in relative error (it differs by less than 10⁻¹⁷ parts of the true value), but the absolute error ~2.84 × 10²⁸ is far beyond the period 2 of sin (and nowhere near an integral multiple of ).
So the answer you get by asking a double function will bear no resemblance to the question your source code seems to ask: where you write sin(6.2831853071795856E+45), instead of sin(6283185307179585600000000000000000000000000000) at best you will get sin(6283185307179585571582855233194226059181031424), which is about 0.8248163906169679 (again, plus or minus 10⁻¹⁶ parts of the true value).
The error in your input to sin could just as well have come from a small measurement error, if your ruler doesn't have many more than 10⁴⁵ gradations.
The error could have come from some kind of approximation error, say by using a truncated series to evaluate whatever function gave you sin's input, no matter what kind of arithmetic you used to compute that input.
The problem is that you asked to evaluate the periodic function (sin) at a point where a small relative error corresponds to an absolute error far beyond the period of the function.
So if you find yourself trying to answer the question of what the sin of 6.2831853071795856 × 10⁴⁵ is, you're probably doing something wrong—and naively using double floating-point math library routines is not going to help you to answer your question.
But compounding this problem, your C# and C++ implementations both fail to return anything near the true value of sin(6283185307179585571582855233194226059181031424):
The [C# documentation for Math.Sin (hyper-link)] advertises that there may be machine-dependent restrictions on the domain.
Most likely, you are on an Intel CPU, and your C# implementation simply executes the [Intel x87 (hyper-link)] fsin instruction, which according to the [Intel manual (hyper-link)] is restricted to inputs in the domain [−2⁶³, 2⁶³], whereas yours is beyond 2¹⁵².
Inputs outside this domain are, as you observed, returned verbatim, even though they are totally nonsensical values for the sine function.
A quick and dirty way to get the input into a range that will definitely work is to write:
[code snippet]
That way, you aren't misusing the Math.Sin library routine, so the answer will at least lie in [−1,1] as a sine should.
And you can arrange to get the same or nearby result in C/C++ with sin(fmod(6.2831853071795856E+45, 2*M_PI)).
The C++ implementation of sin that you are using, however, is simply broken; there is no such restriction on the domain in the C++ standard, and with widely available high-quality software to compute argument reduction modulo  and to compute sin on the reduced domain, there's no excuse for screwing this up (even if this is not a good question to ask!).
I don't know what the bug is just from eyeballing the output, but most likely it is in the argument reduction step: since sin( + 2) = sin() and sin(−) = −sin(), if you want to compute sin() for an arbitrary real number it suffices to compute sin() where  =  + 2 lies in [−,], for some integer .
Typical x87-based implementations of sin use the fldpi instruction to load an approximation to  in binary80 format with 64 bits of precision, and then use fprem1 to reduce modulo that approximation to .
In contrast, typical math libraries, such as the venerable fdlibm, usually use an approximation with well over 100 bits of precision for argument reduction modulo , which is why fdlibm derivatives are able to compute sin(6283185307179585571582855233194226059181031424) quite accurately.
However, the obvious x87 computation with fldpi/fprem1/fsin gives about −0.8053589558881794, and the same with the x87 unit set to binary64 arithmetic (53-bit precision) instead of binary80 arithmetic (64-bit precision), or just using a binary64 approximation to  in the first place, gives about 0.35680453559729486.
Judging by the number you fed in, I would guess you may have been trying to see what happens when you try to evaluate the sin of a large multiple of 2: 6.2831853071795856 × 10⁴⁵ has a small relative error from 2 × 10⁴⁵.
Of course, such a sin is always zero, but perhaps you more generally want to compute the function sin(2) where  is a floating-point number.
The standard for floating-point arithmetic, IEEE 754-2019, recommends (but does not mandate) operations sinPi, cosPi, tanPi, etc., with sinPi() = sin(⋅).
If your math library supported them, you could use sinPi(2*t) to get an approximation to sin(2).
In this case, since  is an integer (and indeed for any binary64 floating-point numbers at least 2⁵² in magnitude, which are all integers), you would get exactly 0.
Unfortunately, [.NET does not yet have these functions (hyper-link)] (as of 2021-02-04), nor does the C or C++ standard math library include them, although you can easily find [example code for sinPi and cosPi floating around (hyper-link)].
Of course, your question still has the problem that evaluating even the sinPi function at very inputsis asking for trouble, because even a small relative error (say 10⁻¹⁷) still means an absolute error far beyond the function's period, 2.
f(x) = sin(x) oscillates between plus and minus 1 with period 2 * pi, i.e.
sin(x) = sin(N * 2 * pi + x).
Forget about programming language and forget about sin() for a moment and think about what the value in scientific notation 1.23E+45 means.
Back to sin(), C# and C++: since the pure mathematical function output repeats every 2 * pi (~6.28) and double precision floats (IEEE 754 64 bit doubles [https://en.wikipedia.org/wiki/IEEE_754 (hyper-link)]) in both C# and C++ have only 15-17 significant decimal digits, no output from the sin() function can possibly be meaningful because the input values are "missing" (imprecise by) at least 30 significant/relevant decimal digits.
Map radii at regular intervals to their sin values, that's your table.
When you want to use it, round the radius you want a value for to the nearest key in your table, that's your sin value.
Can't see why you'd do this on a normal / modern machine - sin() is a hardware instruction.
An old but good recurrence relation for sin/cos interpolation described [here (hyper-link)].
If you can't make your table much finer, approximating the values using the derivative should get you a better result.
for small h. (You can get better approximations using higher derivatives or by using both values in the table between which your (computed) angle lies, but that would take longer, and I gather speed is the reason for the LUT in the first place.)
You don't have to use Angular for it, in javascript functions are [already there (hyper-link)] Math.sin(x)
I got both the function in javascript Math.sin(x) and Math.cos(x).
Just take the absolute value of a sin wave.
Don't forget that the sine function requires radians, so the value you pass in as a parameter must be in the right units.
However should you persist with the incorrect sinusoidal model: The "top half" (or positive) part of a sine wave runs from 0 to pi radians.
The sine represents only the y term (height), you should not have an x term there, that simply determines the horizontal step for each point.
Here is the fresh-written, parametric code for both a sinus wave and a parabolic wave.
Which means you have undefined behavior since you use the mismatching format specifier %lf.
printf("sin(%lf) = %lf \t", interval, abs(sin(interval))); change this line with
printf("sin(%lf) = %lf \t", interval, fabs(sin(interval)));
Using == only evaluates to true if you have the same object (i.e.
two identical references) but "sin"(or "cos",etc) and amalgar are always two different objects.
You should be able to fix it by using
The quotient between term k-1 and k of the sine series has the nice simple form
This allows for a very simple implementation using a minimum of floating point operations:
Since neither the power nor the factorial are actually computed, the largest value in this computation is the term where n-0.5 <= x <= n+1.5 with size about exp(x).
And since the series converges more slowly when x is large, take x modulo 2π before you start.
Polynomial approximations to sine etc.
Using more terms, effectively a higher degree polynomial, can improve accuracy up to a point, but you soon get into increased rounding errors.
You need to pick a narrow domain to calculate using the series, and then reduce inputs outside that range to a value in the range with the same sine.
Check in your regular calc for sin(3.1416) keep the value in radians.
Try y= Math.sin(x)*100;
And take a scale of
1 =100 divisions
That way you can take on the decimals.
Like immibis pointed out, Math.sin takes an angle in radians.
Just convert your degrees to radians while passing it to Math.sin, that should solve your problem.
I believe it should be dividing by m instead of n, since you are only working with computing half the points.
Also, the computation of imag[w] is missing a negative sign.
amp has only half the points it should, so using the length of amp isn't right here.
It should use cosine instead (sine introduces a phase shift):
SIN and COS functions take the arguments in radians.
Because with "-O3" the compiler precomputes sin(2*pi) at compile time, with one algorithm.
Also I tried various options, proposed here: neither -ffloat-store, nor -fno-builtin do not make any difference, as well as long double and sinl.
In theory the sinus function maps every (real) value to a value between -1 and 1.
Since you can not generate every real value with your computer, and since the sin function happens to be monotonic, repeting itself every 2*Pi, plotting the values between 0 and 2*Pi with a small increase each step will normally do.
y is sin(x)
You're effectively using a 2d rotation matrix 
(I'm pretty sure you have a typo where you swapped source_x for source_y in the definition of display_y)
Instead of using units could you just use numbers?
Short answer: sin sin 0.5 parses as (sin sin) 0.5
Long answer: The expression sin sin 0.5 parses as (sin sin) 0.5.
Now, the sine function works on any Floating type.
So Haskell, trying to deduce the type of your expression, determines that sin :: Floating a => a -> a must be floating, so Floating a => a -> a is a floating point type.
In your case that is a lot of radians, and so you are seeing [aliasing (hyper-link)] because your x resolution is not enough to properly resolve the wave.
The condition x - sin(0.75) < 10**(-6) is obviously not true when x very different from sin(0.75), so the do while loop is never entered.
If you change it to x - sin(0.75) > 1e-6 the loop will proceed, but it will run forever, because your iteration is wrong.
sin is a function so it should be called as sin(value) which in this case is sin(theta) It may help to consider writing everything in intermediate steps:
However for calculating y you need element-wise operations since theta and t are vectors (and in this case you are not looking to do matrix multiplication - I think ...)
It is best not to write actual code like this since, in this case, the last line is a lot cleaner.
EDIT: Brief note, if you fix the sin(theta) error but not the .^ or .
* errors, you would get some error like "Error using  * Inner matrix dimensions must agree."
Any of the two can be used to achieve the same goal of replacing sin/cos with other functions.
In the code below we're replacing the function s (which is an alias to sin) with the undefined function s1 (whose symbolic name is s).
There is not difference sin denotes the [sine function (hyper-link)].
Make sure you are using the [radians (hyper-link)].
With loss='mse', the accuracy is going to be weirded out since it is not a classification task.
I suggest to not worry about it, as your loss is decreasing.
Your output isn't wrong, it's just the side effect of using computers for calculations.
The problem with the first code snippet is that the sampling period is exactly half the period of the sinusoid.
In the second code snippet, since [linspace (hyper-link)] is inclusive at its end points, the sampling period is slightly different.
So you don't have the same problem as above, and you do get a sinusoid.
However, you have a different problem which is now made evident, namely [aliasing (hyper-link)] due to insufficient sampling.
Observe how the frequency of the plotted sinusoid is very different (much smaller) from what it should be.
Also, avoid choosing the sample rate as an integer multiple of the sinusoid frequency, to prevent problems caused by the sampling process being "coupled" to the signal variations as in your first snippet.
When you need performance, you could use a precalculated sin/cos table (one table will do, stored as a Dictionary).
You'd still have to "calculate" sin(x) and cos(x), but it'd be decidedly faster, if you don't need a high degree of accuracy.
Modern Intel/AMD processors have instruction FSINCOS for calculating sine and cosine functions simultaneously.
Here is a small example: [http://home.broadpark.no/~alein/fsincos.html (hyper-link)]
Edit:
[Wikipedia (hyper-link)] suggests that FSINCOS was added at 387 processors, so you can hardly find a processor which doesn't support it.
Edit:
[Intel's documentation (hyper-link)] states that FSINCOS is just about 5 times slower than FDIV (i.e., floating point division).
Edit:
Please note that not all modern compilers optimize calculation of sine and cosine into a call to FSINCOS.
Technically, you’d achieve this by using complex numbers and [Euler’s Formula (hyper-link)].
should give you sine and cosine in one step.
It could (and might) well take longer to do it this way (just because Euler’s Formula is mostly used to compute the complex exp using sin and cos – and not the other way round) but there might be some theoretical optimisation possible.
The headers in <complex> for GNU C++ 4.2 are using explicit calculations of sin and cos inside polar, so it doesn’t look too good for optimisations there unless the compiler does some magic (see the -ffast-math and -mfpmath switches as written in [Chi’s answer (hyper-link)]).
But I'd look to the fast implementations of SinCos that you find in libraries such as AMD's ACML and Intel's MKL.
Update 22 Feb 2018: Wayback Machine is the only way to visit the original page now: [https://web.archive.org/web/20130927121234/http://devmaster.net/posts/9648/fast-and-accurate-sine-cosine (hyper-link)]
Since they have similar terms, you could do something like the following pseudo:
This means you do something like this: starting at x and 1 for sin and cosine, follow the pattern - subtract x^2 / 2!
from cosine, subtract x^3 / 3!
from sine, add x^4 / 4!
to cosine, add x^5 / 5!
to sine...
If you need less precision than the built in sin() and cos() give you, it may be an option.
Modern x86 processors have a fsincos instruction which will do exactly what you're asking - calculate sin and cos at the same time.
A good optimizing compiler should detect code which calculates sin and cos for the same value and use the fsincos command to execute this.
Tada, it uses the fsincos instruction!
If you are willing to use a commercial product, and are calculating a number of sin/cos calculations at the same time (so you can use vectored functions), you should check out [Intel's Math Kernel Library.
It has a [sincos function (hyper-link)]
According to that documentation, it averages 13.08 clocks/element on core 2 duo in high accuracy mode, which i think will be even faster than fsincos.
and you will get declarations of the sincos(), sincosf() and sincosl() functions that calculate both values together - presumably in the fastest way for your target architecture.
Many C math libraries, as caf indicates, already have sincos().
Sun has had sincos() since at least 1987 (twenty-three years; I have a hard-copy man page)
Became a built-in in gcc 3.4 (2004), __builtin_sincos().
"Another example is precomputing small tables--for example, a table of
  sin(x) by degree for optimizing rotations in a 3D graphics engine will
  take 365 × 4 bytes on a modern machine.
I have posted a solution involving inline ARM assembly capable of computing both the sine and cosine of two angles at a time here: [Fast sine/cosine for ARMv7+NEON (hyper-link)]
This article shows how to construct a parabolic algorithm that generates both the sine and the cosine:
DSP Trick: Simultaneous Parabolic Approximation of Sin and Cos
[http://www.dspguru.com/dsp/tricks/parabolic-approximation-of-sin-and-cos (hyper-link)]
Remember that cos(x) and sin(x) are the real and imaginary parts of exp(ix).
And since |x-y| is small (at most half the distance between the y-values), the Taylor series will converge nicely in just a few terms, so we use that for exp(i(x-y)).
Another nice property of this is that you can vectorize it using SSE.
An accurate yet fast approximation of sin and cos function simultaneously, in javascript, can be found here: [http://danisraelmalta.github.io/Fmath/ (hyper-link)]  (easily imported to c/c++)
It has good accuracy (maximum deviation from sin/cos on the order of 5e-8) and speed (slightly outperforms fsincos on a single call basis, and a clear winner over multiple values).
The names of these functions seem to imply that they do not compute separate sin and cos, but both "in one step".
Assembly (for x86) without /fp:fast but with /fp:precise instead (which is the default) calls separate sin and cos:
So /fp:fast is mandatory for the sincos optimization.
due to the missing "precise" at the end of its name.
On my "slightly" older system (Intel Core 2 Duo E6750) with the latest MSVC 2019 compiler and appropriate optimizations, my benchmark shows that the sincos call is about 2.4 times faster than separate sin and cos calls.
If you don't specify easing, it will default to 'swing', which is why it looks progressively more "violent."
You can specify 'linear' for easing to get more of the effect you desire.
You are using pi before initializing it.
I suggest using a more accurate value of pi such as 3.14159265359.
You are losing precision by storing these values in an integer type.
Since sinx is a double you should be using printf("%f", sinx);
Using a fixed finite number of taylor series terms to compute sine is going to lose precision quickly as the argument gets farther away from the point at which you did the series expansion, i.e.
To avoid this problem, you want to use the periodicity of the sine function to reduce your argument to a bounded interval.
If your input is in radians, this is actually a difficult problem in itself, since pi is not representable in floating point.
You are not incrementing the sin, or the n. Also hardcoding the entry value for the factorial is not good.
Added the sin initialization:
Into a for which includes the increment that was missing as well:
Also turned the sin calculation:
This is better since it avoids calls to pow and the need to make any explicit factorial calculations (fakultaet).
in error = np.sin(x) - Lagrange: You haven't defined x anywhere, so this will probably result in an error.
Also, you're subtracting a function (Lagrange) from a number np.sin(x), which isn't going to end well.
Since sin(x) is always within [-1..1] range the best you can do is
restore (when you need actual sin value)
This is because sin(3.14) is smaller than 1.
The best way to proceed is to analyze exactly what sin options you use.
Making a fully functioning sin replacement is a lot of work -- it can be done, but it may turn out that you only need a small percentage of the commands.
The simplest way to proceed is to create a front-end for sin that correctly interprets the options that are actually used in your system.
I don't know fast_sin, but it's possible it's mainly memory-bandwidth limited.
If your generator is the mt19337, it's a lot more complex than your sine, so parallelizing your sine doesn't do much, because most of the time is spent generating random numbers.
The generator loop is slow, but not that slow that it completely overshadows the sine loop.
Timing was done using the portable omp_get_wtime() timer routine.
As you can see, the overall speed-up with 6 threads ranges from 1,65x to 1,85x, while the speed-up for the sine loop alone ranges from 5,60x to 5,86x.
On my systems that brings no noticeable advantage (the savings are on the same order as the measurement error), most likely since CentOS's kernel has transparent huge pages turned on by default.
The method p in your program calculates just a single term in this polynomial, so you just need to sum the sequence from 0 to N, for example:
Looks like your dtype is object meaning numpy tries to call an attribute sin on each of the elements of the array.
sin^2(x) is the same (mathematically) as (sin(x))^2, so you can write
In addition to the above, if you're using Java 7 or earlier, you can import Guava, and use the Optional<> class there.
In C++, sin has an overload float sin(float f).
To force the use of double sin(double d) you need to cast the argument: sin(static_cast<double>(x)).
So the value of s need not be exactly the same as the intermediate result for sin(f) in (3).
Values of sin(a) and cos(a) are rational numbers only for particular angles (see [http://planetmath.org/encyclopedia/RationalSineAndCosine.html (hyper-link)] ) so you can't store values of sin(a) or cos(a) as Decimals without losing precision.
This defeats the purpose of converting sin and cos values to Decimal and having built-in functions that return sin and cos values as Decimals.
Moreover, you can't store all rational numbers as Decimal without losing precision because python's decimals are (-1)**_sign * _int * 10**_exp so the divisor of a rational number must be 10 for Decimal to store it with full precision.
Talking specifically about sin: the generic implementations of the math functions are in the math directory: however, it appears that there is no generic definition of sin.
And here you will find [sysdeps/ieee754/dbl-64/s_sin.c (hyper-link)], which is the code you are looking for.
If there were an implementation of sin in assembly language for a particular processor, it would be in a file named sin.S (or possibly s_sin.S) somewhere else in sysdeps.
When you use sin() or cos() in your C code, it is almost certainly the compiler that provides the implementation, rather than your C library.
To evaluate "float(sin(radians(39))*41/28)" statement python converts 39 degrees angle into radians (0.680678) then computing sin(x) which returns the sine of x radians, so we will get sin(39 degree)*41/28.
to evaluate "sin(degrees(39))*41/28" google understands it as "sin(39 degree)*41/28", so it is not converting units between radians and degrees.
It is just calculation sine of 39 degree.
Note that this is unlikely to improve performance, since a table of this size probably won't fit in your L2 cache.
After that tsin and tcos can be implemented inline as
To get the expected answer, as sin() & cos() are between -1, 0 , +1, try to add 1 round it to the accurancy needed and substract 1.
On the left the time-behaviour of the sine and the sine with squared argument (effectively you have a linear frequency change, an upchirp.
You could think of one x as the time, the other incorporated into your frequency, which is now rising linearly with time.
In the spectrum (showing positive and neg frequency as FFT is defined as complex FT) you can see that the frequency of the sine wave is stable , whereas the sin(x**2) sweeps a range of frequencies.
With the sin(x**2) you could easily violate the Nyquist theorem if the argument rises too fast , leading to undersampling in the time -domain (left) and aliasing the frequency domain (right).
I believe Sin(X) is the sum of x to the power of n, divided by n, alternating between negative and positive on each iteration.
Have you tried asking for sin (0.5155) instead?
You are also missing a parenthesis in
Since you know g and G as well as a, so you can get
Given the mathematical formula provided by the questioner, “x_1 = x_0*cos(Alpha) - y_0*sin(Alpha), y_1 = x_0*sin(Alpha) + y_0*cos(Alpha)”, I tested code in the question with several values.
Example:  cos_sinus(30, .3, .6) produces .669616.
I specifically check for -0 using std::signbit and closeness to zero, and consider sin(-0) = -0.
Don't compute each term using factorials and powers!
My recommendation is to use the actual Taylor series about the closest value of sin(x) for which the exact value is known (i.e., the nearest multiple of pi/2, not just about zero.
You could consider using scipy.optimize for this problem:
np.sin will in general be as precise as possible, given the precision of the double (ie 64-bit float) variables in which the input, output, and intermediate values are stored.
You can get a reasonable measure of the precision of np.sin by comparing it to the arbitrary precision version of sin from mpmath:
Thus, it is reasonable to say that both the relative and absolute errors of np.sin have an upper bound of 2e-16.
The standard equation solving approaches won't work for you, since you don't have a standard function.
A simple way to estimate the accuracy of sin() AND cos() for a given argument x would be:
The basic idea behind this approach is as follows... Let's say our sin(x) and cos(x) deviates from exact values by a single "error value" eps.
That is, exact_sin(x) = sin(x) + eps (same for cos(x)).
If we choose x such that sin(x)+cos(x) hovers around 1 (or, somewhere in the range 0.5-2), we can roughly estimate that eps = |1 - sin(x)**2 - cos(x)**2|/2.
First, from the bounds of the sine you know that any solution must be in the interval [-abs(a),abs(a)].
Apart from the interval containing zero, you also know that there is exactly one root in any of the intervals between the roots of cos(x)=1/a which are the extrema of a*sin(x)-x.
For the positive root one knows x/a=sin(x)>x-x^3/6 so that x^2>6-6/a.
The accuracy of the sine function is not so relevant here, you'd better perform the study of the equation.
If you write it in the form sin x / x = sinc x = 1 / a, you immediately see that the number of solutions is the number of intersections of the cardinal sine with an horizontal.
The extrema are found where x cos x - sin x = 0 or x = tan x, and the corresponding values are cos x.
Also note that for increasing values of x, the solutions get closer and closer to (k+1/2)π.
You're trying to fit points to the model: y = sin(ax + b).
Since you're using linear regression, you need a linear model.
So one way to do that is compute arcsin for each point and now compute the linear regression.
The model is now: arcsin(y) = ax + b.
When you stop, you will end up with an approximation of the result of sin.
And more importantly, it should be of type double, since int is for integers.
So instead of calculating each term from scratch (using the power and factorial functions), we can calculate each term iteratively as we go.
(Note it is assumed that the library method sin is perfectly accurate upto the precision of double)
The argument to sin() and cos() is in radians, not degrees.
For instance using CORDIC...One of my first hits on Google:
[A survey of CORDIC algorithms for FPGA based computer (hyper-link)]
Yes, it's possible to generate a lookup table for arcsin.
2) Find the results of arcsin functions.
5) Implement this lookup table in VHDL like the ones you've found for sin or cos.
Similarly, the [variables (hyper-link)] inside sockaddr_in6 all start with the prefix sin6_, and the variables inside sockaddr start with the prefix sa_, and so on.
Use y = Math.sin(x*Math.PI*2/getWidth()); which will scale your x values before plugging them into your sin function.
Well here's the code,not only it can draw the graph of sin(x) but also it can draw any graph using an external parser.
C++ does not have implicit multiplication like you might be accustomed to in math -- (a)(b) tries to call a as a function, not multiply a and b, and in this case, a is sin(x).
The return value of sin(x) is a double, and a double is not a callable function.
Considering that (sin(x)) * (2*pi*B))/(2*pi*B*(x)) is just sin(x)/x;, I strongly suspect that you wanted to write either sin(2*pi*B) / (2*pi*B*(x)); or sin(2*pi*B*x) / (2*pi*B*(x));.
sinc(2*pi*B*x), but that's not C++) .
We're guessing a bit, and so is the compiler.
The version for cosine is the following (together with a simple test harness):
To understand the trick, let's focus on the sine example and recall the McLaurin expansion of the sine function:
what appears as the last step in the above schema is computed first in the sine function (the example is relative to a call with j==4, so that the loop variable k starts with a value of 3 and goes down to 0) .
for arguments near zero, you should compute sinc(x) as 1-(x^2)/6+(x^4)/120
If the value is outside of [-1,+1] and passed to asin(), the result will be nan
That takes away the predefined handler function (which treats sin and cos as prefix operators).
In case your context precludes using : lisp an alternative might be to define a function like:
I'm not sure if this kind of aliasing is problematic for programmatic reasons; however, in terms of flexibility for tex output, I find it very useful, as one can access the preferred form simply by switching between sin(x) and psin(x).
Therefore, since I use a 16-bit bus, I need about 5461 clock cycles for one sin/cos.
We can then easily compute the output frequency using the formula given in the datasheet.
To see the sin/cos wave with Vivado, right-click on the sin/cos signal and select 'waveform style' and then 'Analog'.
sin(PI) should equal 0, for an exact value of PI.
If you need different behavior you should write your own sine function.
Even so, using floating point in this way will always introduce a certain amount of error.
Still, you can create your own sin and cos versions that check against your known Pi value and return exactly zero in those cases.
You know, just for the mathematical correctness out there: sin(3.14159265) ins't zero.
this will zero values below threshold, use it like this cut(sin(Pi))
You're walking into the wonderful world of default argument promotions in C.  Remember, lldb doesn't know what the argument types or return type of sin() is.
The correct prototype is double sin (double).
So sin is getting garbage input.
Second, sin() returns a double, or 8-byte on these architectures, value but you're telling lldb to grab 4 bytes of it and do something meaningful.
If you'd called p (float)sin((double)70) (so only the return type was incorrect) lldb would print a nonsensical value like 9.40965e+21 instead of 0.773891.
If you were calling sinf(), you'd have problems because the function expected only a float.
If you want to provide lldb with a proper prototype for sin() and not worry about these issues, it is easy.
Math.sin and Math.cos methods will accept double values, and return a double.
This will work passing x and y as int as it will be casted implicitly to double
You can do this using [Groovy (hyper-link)] scripts.
Multiplying by sinOf^2
You are confusing int with double.
Also, you have not allowed for sines to be negative (the y and z calculations both yield a negative sine).
But I don't understand why you are not using signed short
Note the 65535 and not 65536, which would be out of range for sine of 90°.
You're using the abs function, which expects an int and returns an int.
I'm using python and using numpy so can do:
Too many documents claim that x87 instructions like fsin or fsincos are the fastest way to do trigonometric functions.
As CPUs become faster, old hardware trig instructions like fsin have not kept pace.
With some CPUs, a software function, using a polynomial approximation for sine or another trig function, is now faster than a hardware instruction.
In short, fsincos is too slow.
Yet, SSE has no equivalents for x87 instructions like fsin.
For amd64, libm in both [FreeBSD (hyper-link)] and [glibc (hyper-link)] implement sin() and such functions in software, not with x87 trig.
glibc has [optimized x86-64 assembly for sinf() (hyper-link)] (the single-precision sine) with a polynomial approximation, not with x87 fsin.
Steel Bank Common Lisp uses fsin in its [x86 backend (hyper-link)] but not in its x86-64 backend.
For x86-64, SBCL compiles code that [calls sin() in libm (hyper-link)].
I timed hardware and software sine on an AMD Phenom II X2 560 (3.3 GHz) from 2010.
I compiled this program twice, with two different implementations of sin().
The hard sin() uses x87 fsin.
The soft sin() uses a polynomial approximation.
My C compiler, gcc -O2, did not replace my sin() call with an inline fsin.
Here are results for sin(0.5):
Here soft sin(0.5) is so fast, this CPU would do soft sin(0.5) and soft cos(0.5) faster than one x87 fsin.
And for sin(123):
Soft sin(123) is slower than soft sin(0.5) because 123 is too large for the polynomial, so the function must subtract some multiple of 2π.
If I also want cos(123), there is a chance that x87 fsincos would be faster than soft sin(123) and soft cos(123), for this CPU from 2010.
The arcsine is the inverse sine function that you are looking for.
In Java, this would be asin().
You're looking for Math.asin, which calculates the arcsine of a value.
arcsine(sine(375°)) ≠ 375°
The answers above will give you the VALUE of the parameter that went into your Math.sin() function call.
However if you actually need access to the parameter itself, (so you can modify before/after execution etc etc), you should trace the execution call to where the Math.sin function was invoked and see what was passed into it.
You might be interested in just using Math.Sin(x)
We calculate the value of the sin function by using the series expansion of it which is basically a the sum of terms.
functions like cos, sin and tan are in the standard library and can be used by including math.h.
More specific info can be read by typing 'man sin' for example.
You will then see that sin() takes a radians arguments, not degrees.
If Seed7 expects radians, you may need to convert degrees to radians before applying sin or cos:
Assuming no special requirements applicable to your use case, the "standard" approach is to convert the fixed-point result to double (this is an error-free transformation for most common fixed-point formats), and compute the absolute error of that versus the standard math library's sin(double) result:
For a periodic function like sin, you may observe that the phase error grows with the magnitude of the arguments due to an insufficiently precise approximation to π used in the argument reduction.
Below is a worked example, using a particular fixed-point implementation of log2 as the function under test:
The sine function takes a real and returns a real.
Therefore, you should use a floating-point type for x and sin(x).
Let's also write a function that emulates sin from <math.h>:
Since we are going to use these factorials in a double calculation, we can just as well use double here.
If we write a driver program to print out some test values and compare them with the standard math library's implementation of sin:
C Runtime libraries are missing.
Either include them or try to compile using VS command prompt.
You probably should be using std::fabs:
If the values in the array are irrelevant, then the output should be sin( currentPositionOfArray*PI / (lengthOfArray-1) ).
This means that for your array [0, 1, 2, 3, 4, 5, 6], you get the sin of [0, PI/6, PI/3, PI/2, 2PI/3, 5PI/6, PI], which is [0, 0.3, 0.7, 1, 0.7, 0.3, 0]
Just an example, supposing you use a panel in your form on which to draw the polar flower:
For the sin, the error is 1 ulp.
Since float has limited precision, your round off error is larger than your final value.
You can "reduce" the problem by using double-precision.
Standard implementations of sin/cos involve taking the modulo of the argument by 2 pi to make it small.
Any of those choices qualifies as a sinusoid.
is a sinusoid for any choice of phi (as would be cos(2*pi*fs*t + phi)) .
phi is the initial phase (or simply phase) of the sinusoid.
sin2theta = sin(theta)*sin(theta)
You can try using that.
Best option  and easier approach would be using [universal-tween-engine (hyper-link)]
I think it looks better than sine for a color fade.
The function f(x) = sin(pi + 0.2*x) is periodic.
The sequence sin(pi + 0.2*n) where n=0, 1, ... is not periodic.
sin(0.2*x + pi), x in R (i.e.
x - real) is periodic, as we could find such x0 that sin(0.2*x + pi) = sin(0.2*(x + x0) + pi)
sin(0.2*x + pi), x in N_0 (i.e.
Your for loop syntax is wrong and you're missing some braces - try:
This is just a consequence of the way that the sine function works.
Instead, you might try using a method called [Blum Blum Shub (hyper-link)] (named after the authors of the original paper, wonderfully).
here is an example of use of Math.asin():
You got this wrong meaning of "sin^2θ".
It's actual meaning is sin2θ.
squaring the sin and cos and adding:
I would recommend multiplying a number by itself for squaring over using Math.pow().
The problem is that you're using the comma operator, which evaluates the left-hand side, discards the result, and returns the value of the right-hand side.
That is, the value of (cos(phi2), 0.0 , sin(phi2)) is sin(phi2) (the parentheses don't change this).
One easy method to deal with large x without using the value of pi is to employ the trigonometric theorems where
and first reduce x by halving, simultaneously evaluating the Maclaurin series for sin(x/2^n) and cos(x/2^n) and then employ trigonometric squaring (literal squaring as complex numbers cos(x)+i*sin(x)) to recover the values for the original argument.
See [https://stackoverflow.com/a/22791396/3088138 (hyper-link)] for the simultaneous computation of sin and cos values, then encapsulate it with
The major issue is using the series outside its range where it well converges.
The first step in finding my_sin(x) is range reduction.
Alternative, if using C11, use remquo().
You're using a different overload:
it should perform the same with or without using namespace std;
I guess the difference is that there are overloads for std::sin() for float and for double, while sin() only takes double.
Inside std::sin() for floats, there may be a conversion to double, then a call to std::sin() for doubles, and then a conversion of the result back to float, making it slower.
Maybe using namespace std; gives a lot of unused stuff in executable file.
I did some measurements using clang with -O3 optimization, running on an Intel Core i7.
std::sin on float has the same cost as sinf
std::sin on double has the same cost as sin
The sin functions on double are 2.5x slower than on float (again, running on an Intel Core i7).
Your equation is for a calculation of the sin of an angle in degrees.
Most, maybe all, computer language sin() functions expect the angle to be in radians.
Unfortunately, you need to have a value for pi to convert from one to another, so unless you can find a sin-in-degrees implementation you’re somewhat stuck.
Here is the example I made (basically your code with above fixes implemented), which only rotates the rectangle which follows the graph and not the whole graphics object using [AffineTransform#createTransformedShape() (hyper-link)]:
Alternative: using dictionary rather than try/catch
This has to be done since the function is expected the output to be initialized and returned.
Using MATLAB's vectorized operations can greatly reduce the necessity of writing intricate loops.
Below are two examples of evaluating and plotting a truncated sinc signal.
This method uses a logical array named Range to truncate the sinc and constrain the signal within the range [0,π].
By element-wise multiplying the sin(x)./x the sinc by Range the signal is truncated.
This process can be visualized as multiplying a rectangular pulse that's only positive and exists within the range by the sinc.
Furthermore, you don't pass an "angle" to asin, you pass it a number from -1 to +1 and it returns an angle.
In your case, you'd want to want to take the height of the ladder and divide it by its length to give you your sin (between -1 and +1), then take the asin of that value.
Odds are you also want to take the angle returned by asin in radians and convert to degrees.
I think your best choice is to copy the function code into your sources and fill in all the missing functionality - functions that sin calls, all (if any) constants etc.
When i is 0, then "0.25 * math.pi * i" is precisely 0 and when you calculate the sine, you get exactly 0.0.
If you calculate the sine, you get a number which is very, very close to zero, but because of limited accuracy, not exactly zero.
But using NSAffineTransform will make it easier.
Not that I would actually do this (I would look at Core Plot), but you could plot such a graph using a Core Image generator filter, like this:
You can't solve the performance problems with lookup tables; as the lookup tables would cost more than having a lookup table for "sin" instead.
As mentioned in the comments above, the problem is that you're using a way too low sampling frequency to plot a detailed graph.
You can see this using the function below:
Increasing to Fs = 100, the situation improves:
You are computing the sin of 100.7 radians, and the answer given is the correct one.
[http://www.google.com/search?hl=en&q=sin+of+100.7 (hyper-link)]
The sin function is expecting radian.
And then use sin(DEGTORAD(107))
For example, splot u,0,sin(u), u,sin(u),0 will plot the two (!)
parametric curves u,0,sin(u) and u,sin(u),0.
The variable u is the parametric dummy variable, for a simple sine function we only need one of them, even in 3d mode.
Your user can put in basically any code to be executed using this input.
It is difficult for you to write a program to understand and evaluate expressions, especially ones that include functions like sin or sqrt.
However, if you are sure that the user of your program is safe (and that is usually a bad assumption), you can get Python to do the evaluation using the built-in eval function.
Then, if you run this program and the user types math.sin(2), the program prints
which is the correct value of the sine of two radians.
Excel's [SIN function (hyper-link)] expects 'the angle in radians for which you want the sine' (see [RADIANS function (hyper-link)]).
If you are looking for the sine of 0.6248° then the correct formula is,
All that said, I don't see any point in doing this using assembly.
I believe the answer to this question, without overly complicating the code, is no, since numpy already optimizes the vectorization (assigning of the cos and sin values to the array)
In your function "y=a * sin(b*x+c)" b changes the period of the sine wave, the time it takes to go one full cycle.
sin = 1 and this declaration will then have precedence over the original function.
to see if you have a variable called sin?
The Wolfram article has the expansion formula stated using q = e**(ix) so there is a complex term.
If you are using gfortran try: -O2  -fimplicit-none  -Wall  -Wline-truncation  -Wcharacter-truncation  -Wsurprising  -Waliasing  -Wimplicit-interface  -Wunused-parameter  -fwhole-file  -fcheck=all  -std=f2008  -pedantic  -fbacktrace
Since you can do this problem by hand, try outputting each step with a write statement and comparing to your hand calculation.
Tried adding a runnable snippet to your question, however as others point out, screenValue was missing initialization and that prevented the code to run here on StackOverflow.
After adding that line, sin starts working, with the glitch I described earlier.
Now you can type numbers, press sin, and it works.
You can even type 1, +, 1, =, sin, and it will show the result.
However if you type 1, +, 1, sin (note the missing =), it will show NaN, that's what the suggestion with eval() can fix:
Now you can type 1, +, 1, sin too, and it will work.
I assume you need to recalculate the input from degrees to radians, since you're asking for degrees.
Structs are an aggregate of a number (1 or more) of types that can be dealt with as a single unit in certain circumstances (assignment, parameter passing).
While the Taylor series for sin(x) converges (because eventually the factorial in the denominator grows larger than the exponential in the numerator), for modest values of x, the values involved become very large before the quotients become very small.
Firstly, the derivative of sin(x) is cos(x) or, to put it more formally:
I guess you could solve sin(x) using the [Taylor series for cos(x) (hyper-link)]:
derivative of sin(3)
derivative of sin(2)
derivative of sin(1)
One can only assume that you want to evaluate the derivative of sin(x) [period].
A possible interpretation is that you are looking for a numerical approximation of the sine derivative, and want to recursively narrow the interval over which you are calculating the slope.
You could take advantage by the fact that tan(x) contains both sin(x) and cos(x) function.
So you could use the tan(x) and retrieve cos(x) ans sin(x) using the common transformation function.
I've just timed this and it is about 25% faster than using sin and cos.
You can use complex numbers and the fact that e i · φ = cos(φ) + i · sin(φ).
I'm using here the trick from [https://stackoverflow.com/a/27788291/674064 (hyper-link)] to make a version of [cmath.rect() (hyper-link)] that'll accept and return NumPy arrays.
A pure numpy version via complex numbers, e iφ = cosφ + i  sinφ,
inspired by the answer from [das-g (hyper-link)].
This is faster than the nprect, but still slower than sin and cos calls:
I compared the suggested solution with [perfplot (hyper-link)] and found that nothing beats calling sin and cos explicitly.
To decide when to stop, you can look at how much x is changing from one iteration to the next, or look at the residual magnitude |sin(x) + x - y|.
Or, since the iteration converges fast, just perform a fixed number of iterations.
I dont know the exact way that Matlab calculates sin(x) - but you can investigate this by calculating it using the power series, i.e.
Having said all that the sin(pi) is within eps so you should use that fact for you purposes.
The reason is, that sin(pi)=0.0, so every small error no matter how small is is huge compared to 0 and thus is visible.
Differently, for sin(pi/2)=1: if the algorithm produces an error smaller than eps (around 2.220446e-16), you would not see this error because 1+eps=1.
You'd just have to generate the data that will represent a sin/cos wave and pass it in to the chart engine.
sin and cos don't return angles, so you can't speak about their return values as of degrees or radians.
Not only you need to apply the radians<->degrees conversion to the angle before applying sin or cos to it, you also need to convert degrees to radians, not the other way around.
Saxon 9 (including HE, tested with 9.6, see also [http://saxonica.com/html/documentation9.6/functions/math/sin.html (hyper-link)] saying "From Saxon 9.6, available in all editions.")
I have also tested that XmlPrime 4 runs the above stylesheet correctly when using the command line option --xt30 so it also has built-in support for those functions.
As I know there's no out of box XSLT function for advanced
  calculation, anyone have any idea how I can do math such as
  Cosine/Sin() within XSLT?
Using the FXSL library, one can have among other things trigonometric functions, written in pure XSLT 1.0 or XSLT 2.0:
The wellknown trigonometric functions: sin(), cos(), tan(), cot(), sec(), csc()
Hyperbolic trigonometric functions: hsin(), hcos(), htan(), hcot(), hsec(), hcsc()
Inverse functions: arcsin(), arccos(), arctan(), arccot(), arcsec(), arccsc(), archsin(), archcos(), archtan(), archcot(), archsec(), archcsc(),
There is no need in calculation of the distance and arcsin of the angle (instead of yours sin) - you can just use Math.atan2(dy, dx);.
From the documentation of sind:
For integers n, sind(n*180) is exactly zero, whereas sin(n*pi)
     reflects the accuracy of the floating point value of pi.
So, if you are extremely troubled with the fact that sin( pi ) is not precisly zero, go ahead and use sind, but in practice it is just a wrap-around sin so you actually add a tini-tiny bit of overhead.
Personally, I prefer the elegance of radians and use sin.
The values of sine move in the interval [-1,1].
There is really no need to have the cast when calling sin.
would automatically use sinf under the hood.
sin (angle * PI/180);
The sin function takes input as radian:
Let's look at your calculation of sin.
Your problem is the numerator of the Maclauran series, which, in your expression, should be the cosine of zero (which equals 1), alternatively with sign + and -.
because sin(x)**2 + cos(x)**2 = 1, you need only calculate sin(x) or cos(x) with the Maclauren series.
Below I've assumed we're estimating sin(x) (in which case cos(x) = 1-sin(x)**2)**0.5);
because the derivative of sin(x) is cos(x) and the derivative of cos(x) is -sin(x) and sin(0) is zero, we need only calculate the odd terms of the series, whose numerator is alternatively cos(0) and -cos(x) (1 and -1);
your computation of factorial is inefficient, since it is only necessary to update the factorial value for each incremented value of n;
In view my comments above, the expression used to approximate the value of sin reduces to the n/2 terms:
Let's look at the calculation of sin_and_cos(x,n) when:
You should think of using reduce whenever you want to compute a sum or product of values from an array, hash or other type of collection, possibly after performing a transformation of each value (e..g, [1,2,3].reduce(0) { |t,i| t + i*i } #=> 14.
0.50000095 is passed back to reduce as the new value of t, but as all elements of the enumerator have now been processed, this value is returned as the approximate value of sin(0.523598).
Lastly we calculate the approximate value of the cosine:
Note the cosine is positive in the first and fourth quadrants, so
For degrees close to zero, use of just the first and second derivatives of sin provide a good approximation.
Consider, for example, the sin of 30, 150, 210 and 330 degrees.
First, let's create a method that returns the approximation for sin given the number of degrees (not radians) and n, rounded to seven decimal digits.
We can improve accuracy for a given value of n by using the series expansion to estimate the sine of angles between 0 and 90, and deriving the sine for degrees greater than 90:
To avoid this, a common way to compute sine and cosine is to first fold to between -45 and +45 degrees.
Will it make your sin() closer to the real sin?
The 'real' sin is probably computed in hardware with 64-bit fixed-point arithmetic, and will be "correctly rounded" to 53 or 24 bits well over 99% of the time.
This adds a bit of overhead since you have to pop the value out of the SSE register onto the stack, load it into the x87 unit, peform the calculation, pop the value back onto the stack, and then load it back into XMM0 for the function result.
The sin calculation is pretty heavy, though, so this is a relatively small overhead.
I would only really do this if you needed to preserve [whatever idiosyncracies (hyper-link)] of the x87's sin implementation.
Other libraries exist that compute sin more efficiently in x64 code than Delphi's purepascal routines.
Also, as David said, using trig functions with ridiculously large arguments is not really a sensible thing to do anyway.
So this is what I end up using now:
But even without Inline this is already hundreds of times faster than System.Sin(), so I'm going for this.
Change the call to std::sin(radian).
As written it's trying to call your sin function.
Note that functions like sin, cos, and so on do not return angles, they take angles as input.
The sine of the double constant you wrote is about 0x1.e89c4e59427b173a8753edbcb95p-2, whose nearest double is 0x1.e89c4e59427b1p-2.
I should also point out that Mathematica is probably trying to solve a different problem---compute the sine of the decimal number 0.497418836818383950 to 20 decimal places.
Firstly, if your numerical method depends on the accuracy of sin to the last bit, then you probably need to use an arbitrary precision library, such as MPFR.
The exact sin of the decimal .497418836818383950, which is
The exact sin of the 64-bit float nearest .497418836818383950:
I was multiplying ifft result with 32767 and that was causing the issue in the result.
Then you can just use that matrix and try to do the cos and sin calculations all at once:
You could take even strides as a mask and use fill A with the cos.  Then take odd strides as mask and fill with sin.
Not sure if that helps (or if correct), just another way to think without using the loop.
Note your sincos_lut doesn't analyze either.
Also note there were associated errors in the sincos_lut, both in port declarations and signal declarations.
If you look at your mem component and the positional association list in the instance port maps you'll find you're missing an integer argument for both of these representing Address.
(And you could have produced integers from the sincos_lut entity/architecture pair, but without a range constraint that would have likely been a 32 bit value).
although, since R does vectorized computations, t <- 1:200; v <- sin(t*pi/50) would be better.
To do a proper phase shift, you should add (or subtract) from tm, not from the sin value.
For frequency stability, you will want to move the sine generator to a timer interrupt, after debugging.
The sin function computes the sine of x (measured in radians).
The sin functions compute the sine of x (measured in radians).
According to [Standard for the sine of very large numbers (hyper-link)], it recommends (but does not require) correct results even for large magnitude arguments.
You can also assign x*sin(3./x) to a variable, e.g.
My original answer using scipy to compute the integral:
It seems there is a bug in SymPy that gives an answer in terms of zoo at 0, because it isn't using limit correctly.
Then you could use the fact that the derivative of sine is cosine.
Last it's sin(a) vs sin(b).
Having a, calculate the next point with sin(x)==1.
A google search will pop up many such efforts, I found this one quickly: [http://goelhardik.github.io/2016/05/25/lstm-sine-wave/ (hyper-link)]
An RNN does not work on the same principals because it maintains a state and can make use of the state being passed forward in the sequence to learn the pattern of a single period of the sine wave and then repeat that pattern based on the state information.
Math.sin(), Math.cos(),     etc
Essentially store sin(x) and cos(x) values for a bunch values.
To calculate sin(y), find a and b for which precomputed values exist such that  a<=y<=b.
Now using sin(a), sin(b), cos(a), cos(b), y-a and y-b approximately calculate sin(y).
It's not possible to generate one-off sin calls with just two multiplies (well, not a useful approximation, at any rate).
For instance, consider that the following difference equation will give you a sinusoid:
Essentially the sinusoidal oscilator is one (or more) variables that change with each DSP step, rather than getting recalculated from scratch.
The simplest are based on the following trig identities: (where d is constant, and thus so is cos(d) and sin(d) )
However this requires two variables (one for sin and one for cos) and 4 multiplications to update.
However this will still be far faster than calculating a full sine at each step.
The general idea of getting periodically sampled results from the sine or cosine function is to use a trig recursion or an initialized (barely) stable IIR filter (which can end up being pretty much the same computations).
As a matter of fact, I was porting the VST BLT oscillators to C#, so I was googling for good sin oscillators.
In this particular implementation, the magnitude of the sin wave decays over time.
But in retrospect, the need to keep the sin(phase), and sin(m*phase) oscillators tightly in synch probably means that they have to be resynched anyway.
Probably missing from the STK code.
[http://devmaster.net/forums/topic/4648-fast-and-accurate-sinecosine/ (hyper-link)]
There's some sample code that calculates a very good appoximation of sin/cos using only multiplies, additions and the abs() function.
everytime before using a specific function, you may just import the functions you wish to use directly.
The best way of Googling this is to figure out how it's done by some popular language, such as by searching for std::sin implementation, which would give you the C++ implementation.
Other methods include using assembly instructions so that the calculation is done in hardware (though this has occasionally led to [problems (hyper-link)]).
Since in the end you add up all the digits, having a single 7 vs. 16 does not make a difference on the final result.
To generate lots of valid SINs you could generate random nine-digit numbers, compute their "check sum", find the remainder of that check sum when divided by ten, and adjust one or more of the digits in positions 1, 3, 5, 7, or 9 to get the correct check sum.
This sounds an awful lot like the purpose of your real code is to validate user-entered SINs, and you want to randomly generate SINs in your test suite for testing your validation code.
This doesn't mean you can't use data at all; it's perfectly fine to generate a set of 10,000 good SINs and 10,000 bad SINs once, save that data, and then write a test against your validation method that you get the right result for all of them.
Later on, you may still find a bug where your validator fails for a specfic SIN.
The correct response at this point is permanently adding that SIN to your test data.
Here's a quick Google result:
[http://www.testingmentor.com/tools/tool_pages/ssnsin.htm (hyper-link)]
The projection of this arc to x-axis is delta_x = s * sin(fi) and to y-axis it is delta_y = s * cos(fi)
The coordinates of a circle can indeed be completely defined using the trigonometric functions sine and cosine:
y = sin(angle)
y = sin(angle) * radius
taylor_sin(x) = sum[0->n] ( [(-1 ^ n) / (2n + 1)!]
I use an integer to compute -1**n since using a real may lose precision over time.
The [java.util.Math (hyper-link)] utility class has the arc sin method, asin(...), that returns the radian result which can then be converted via Math's toDegrees(...) method to degrees.
What you're going to find is that the 500 Hz signal is just barely sampled fast enough to get the high, low and zero crossing.
you want to check out how arrays work in numpy rather than the sin function.
Be careful, sin and cos functions take a float or a double as input parameter.
It seems you want your program input to be in degrees, but this is a problem because sin and cos use radians.
1) You need to use the sin and cos from NumPy as the math versions do not provide vectorized operations.
It would take a longer training time to see it as x waves passing by, to count as a pattern.
But skip forward on the sin line, or far easier, use your good working model, and then test it with divided inputs.
---edit -- (too long for in comment reply)--
Yes, it's hard to debug neural nets, though I think you have to go back to basic principles, is a raising signal a pattern, it can only be detected if it raises (enough) up and down in the training.
Perhaps you can improve here by altering training sample order, so take a random position in on the sinewave, as the internal "narrowing/nearing" error correction might get over convinced into a certain direction, cause its last 70 samples went up why would 71 go down.
Since sin(x) does not seem to be working for small values of x, my suspicion is that the Nvidia driver is replacing it with native_sin(x), which evaluates to a function directly implemented in the hardware, but which may not be that accurate or support the full range of numbers.
Then use double for the value of PI to avoid losing too many decimals
using long long for your factorial method or you get overflow.
my recusive implementation of pow using successive multiplications is probably not needed, since we're dealing with floating point.
fact could be using floating point to allow bigger numbers and better precision.
For instance, for values of x between [pi/2, pi), sin(x) can be calculated by sin(pi - x).
The sine of 120 degrees, which is 40 past 90 degrees, is the same as 50 degrees: 40 degrees before 90.
Sine starts at 0, then rises toward 1 at 90 degrees, and then falls again in a mirror image to zero at 180.
The negative sine values from pi to 2pi are just -sin(x - pi).
A similar approach for cos, using identity cases appropriate for it.
Because, for some reason I cannot understand, you've imported sin and exp from math and not from numpy, the sin(x) that you use in the definition of f(x) expects a scalar.
until I try to use the sin function in some way:
Here, however, you are using math.sin which is unknown to numpy - it's a "regular" function to it and no vectorization can happen there*.
Since [math.sin (hyper-link)] is expecting a single Python scalar, you get the error.
To fix these, I'd suggest you use numpy's sine function i.e.
np.sin (and also np.exp in lieu of math.e).
There you are passing Python scalars so everyone is happy :)
math.sin here) with it to vectorize it but it's essentially a for loop so it is not at all preferable when an already-vectorized version exists i.e.
np.sin.
When you input 90, and convert it to radian using the method Math.toRadians, the result is not exactly pi/2, and hence when you pass this non-exact value to cos it gives a non-exact value.
Use it like easing(linear(t))
2) You have a needless nextSine variable.
3) You have a syntax error in your code as you meant **q**Sin, not aSin.
You need to draw a curve... X,Y,Z components of your curve is coming from the equation you have possibly with sin and cos and then you need to draw a curve by sampling the curve with small increments and connect the samples with lines...
If you want to know how asin or sin works in C++, see [asin (hyper-link)] and [sin (hyper-link)].
In particular, you can use SineModel (see [here (hyper-link)] for details).
SineModel in lmfit is not for "shifted" sine waves, but you can easily deal with the shift doing
Now you can fit to sine wave
Note that in SineModel all parameters "are constrained to be non-negative", so your defined negative amplitude (-100) will be positive (+100) in the parameters fit results.
note the argument of sin/1 it's in radians.
If you want not to forget the genesis of the value of sin(45),
you can create another term with additional parameters, for example
and obtain also the function name (F=sin) and the argument (X=45) along with the result itself (Y)
Also note,  sin() usually accepts radians, not grades, so sin(45) perhaps yields something you don't expect
I studied and understand that values can't be assigned to a term using =, == or =:= like procedural languages.
Even though we don't define the facts or rules for obtaining sin,cos,tan, prolog answers for sin, cos, tan automatically.
Check the form, using a regular expression is the easiest way : group od 3 digits separated by a dash
[code snippet]
By the way if(validSin = true) is not correct, it'll assign the value true to validSin and then evaluate the result : true always (primitive equals is done with ==)
Since function y(x) = a*sin(m*x + t) is continuous, maximum will be either at one of the interval's ends or at the maximum inside interval, in this case dy/dx will be equal to zero.
This code has singularities at s = 0 and s = 1, both of which occur, and will be numerically unstable due to the singularities.
You probably triggered the unstabilities with the different computations of sin_pi_over_n_[0] and sin_pi_over_n_[1].
While this doesn't explain the different behaviour for different sin_pi_over_n_[5] calculations, similar unstabilities are likely.
sin(x+k*π/2) does not necessarily equal sin(x)
Outside that range you'll need to be a bit cleverer about the reduction you're using: simply reducing modulo pi / 2 won't give correct results.
(Hint: it's safe to reduce modulo 2 * pi, since the sin function is periodic with period 2 * pi.
Now use symmetries of the sin function to reduce to the range 0 to pi / 2.)
I'd be surprised if you were really intended to compute the answer using explicit factorial and power calls, though - I'm almost sure that you're expected to compute each term from the preceding one as described above.
Also, you can use the acos and asin functions directly on s_x and s_y respectively.
In math is reverse operation for sin and cos.
This is arcsin and arccos.
But usually if it have cos and sin function then it can have reverse function.
asin(s_x), acos(s_y), perhaps, if you are using c.
I use the acos function to get back the angle from the given s_x cosinus.
But because several angles may result to the same cosinus (for example cos(+60°) = cos(-60°) = 0.5), it's not possible to get back the angle directly from s_x.
My error was reusing the neural_network_model() function and thus creating a new set of variables.
Note that usually sine and cosine are computed together, I always wondered why the standard C library doesn't provide a sincos function.
atan2 is computed with a call to sincos and a little logic.
Other than +, which consumes two numbers, Sin consumes only one number.
In C library math.h, there was a sincos function
Just use sin and cos separately and turn on optimizations.
C compilers are pretty good at optimizing, and they will probably realize that you are computing both the sine and cosine of the same variable.
The compiler will probably optimize away any calls to sin or cos in favour of simply using SSE intructions to calculate it.
I'm not sure SSE has a sincos opcode but even calculating them separatly is faster than calling any sincos function that the compiler won't optimize out.
You can then get the result on a single line (with C++ 17) with:
When compiled with optimizations, it should produce the exact same code as calling sin and cos separately.
You can confirm this is the case with clang++ -std=c++17 -S -o - -c -O3 sincos.cpp on the following test code:
On MacOS with clang, both test functions compile to the exact same assembly (minus the name changes) that call ___sincos_stret to perform the combined computation (see [https://stackoverflow.com/a/19017286/973580 (hyper-link)]).
OP's loop code takes 2.4412 s, and the vectorized solution takes 0.7224 s. Using OP's Horner method and the rewritten sin expression it takes 0.1437 s.
Do not ask why it works ;) actually your [x] must be equal to [angle] in this code, and formula is [y] = ([pie] * [angle]) * sin(1/[pie]/[angle]), [x] = [angle]:
sin and cos take radians as input, not degrees.
You'll need to convert your angles to radians before passing them into sin and cos:
Well, you've already created arrays for sin and exp stored in y and z respectively.
Further, note that sin(2x) performs a compression by a factor of 2 (tip of the hat goes to knedlsepp for noticing my blunder).
Due to the periodic nature of sin(x), we know that elements will repeat after 2*pi, and so if you want to go to 4*pi, simply subsample y by a factor of 2 and then append these same elements to a new vector.
It isn't quite right with respect to the sin(x) part in your code.
Further, note OP's code has undefined behavior when radToIndex == UCHAR_MAX as that is an invalid index to int sini[UCHAR_MAX];.
Using above fix and 3 below fixes: table size 256, round index, round the value of sine, use double for table creation results in:
Table creation sini[i]=sinf(i*2*M_PI/UCHAR_MAX)*MAGNIFICATION; also truncates toward 0.
So the sini[] is made smaller, not closest int.
As a general note, since float is typically 24 bit precision and int likely 31+sign, use double for table creation for additional improvements.
Further, recommend using UCHAR_MAX + 1 See [BAM (hyper-link)]:
By using
Here's a program, once again adapted from your posted code, demonstrates that using the above logic works.
One could add a companion to Math.sin that works on degrees this way:
(Some people don't like extending built-in objects, but since one doesn't instantiate Math instances — at least, I don't — this doesn't seem terribly offensive.)
Math.sin accepts values in radians, while your calculator is set to degrees.
Math.sin works in [radians (hyper-link)], I guess your calculator is in degrees.
As was said above, Math.sin() requires the use of radians as input.
itself if it is one of "cos", "sin", "tan" or "ctg"
No it won't, you have to call sin with a number like this:
If you aren't, you can, using [static imports (hyper-link)]; you can write import static java.lang.Math.
*; and then sin(whatever); and this is OK. Be aware of the warning from Sun, though:
If you happen to be running something below Java 1.5, you can still write sin(whatever), but this method must be present...
For example if your arg is "I wandered lonely as a cloud", what is the meaning of sin() of it?
There are a number of ways to solve your problem, not only by using sine function.
Cosine is used here instead of sine just for convenience.
After this we need to subordinate orthogonal vector by sine function, and then perform addition of these two vectors, so the sum will move object following exact sine curve.
It looks like following what NumPy itself does keeps things clean: "extended" mathematical operations (sin…) that work on new objects can be put in a separate name space.
Thus, NumPy has numpy.sin, etc.
The C preprocessor can't provide sin() or cos().
The closest solution I can think of to reduce your magic numbers is creating a header with the most used sins or co-sins values:
And let the compiler optimization reduce it to a single constant.
As long as your arguments are all within the range [-π/4,+π/4], you can use the same formula standard implementations of libm use to compute sin.
Source: [http://www.netlib.org/fdlibm/k_sin.c (hyper-link)]
photonAngle is in degrees but Math.sin/cos takes radians.
Furthermore, I doubt the C code gives the same result as the Python code, because it uses cos() instead of sin().
The calcualtion for the image translation is mImage.setTranslationY((float) (amplitude * Math.sin((incrementalValue) * Math.PI)));.
If you insert the values and incrementalValue ( = 0) doesn't change, this calculation will always return 0 because sin(0) = 0 and one 0 in a multiplication makes the product 0.
Besides, i suggest you change the calculation to sin(animator.getAnimatedValue()*PI*2) this makes the wavelength of the sine  1[image] and the image will move up and down once per repetition of the animation and it won't jump when the aimation restarts.
The problem is that it's taking the sine of 90 radians, not degrees.
Try sin(M_PI_2) for 90°.
I guess the \; output for asinh etc.
I am using Maxima in connection with the STACK system for computer-aided assessment ([home page (hyper-link)], [Github (hyper-link)]).
Having done that, it is still useful to remove the right binding power annotation as in Robert Dodier's answer, otherwise you will get (\sin(x))^2 instead of \sin(x)^2.
To be safe when using these types of objects (Pen, Brush, Font, and more need to be disposed) wrap them in a using statement:
Your wording is a bit confusing... but what I understand is that you have a point in the 2D space and you want to advance it a particular distance (hypotenuse) aiming a specified angle above the horizon.
GCC notices that you are taking the sine of a constant and replaces it with the actual value of sin(0.5) immediately, as you can see from the disassembly of main below:
Now you actually need to call the sin() function which is defined in the   math library.
Note the call to sin@plt.
From your screenshot it looks like you are using some kind of a visual tool around gcc.
The answer is only 0 for sin(pi) - did you include all the digits of Pi ?
I'm
not sure what the ULP technique you're using is, but I suspect you're
applying it wrong.
The sin() function and the approximated value of M_PI.
Even if the sin() function were 'perfect', it would not return zero unless the value of M_PI were also perfect - which it is not.
Sine of π is 0.0.
Sine of M_PI is about 1.224647e-16.
The following does not print the sine of [π (hyper-link)].
It prints the sine of a number close to π.
Note: With the math function sine(x), the slope of the curve is -1.0 at x = π.
The difference of π and M_PI is about the sin(M_PI) - as expected.
The rounding problem occurs when using  M_PI to represent π.   M_PI is the double closest to π, yet since π is irrational and all finite double are rational, they must differ - even by a small amount.
So not a direct rounding issue with sin(), cos(), tan().
sin(M_PI) simple exposed the issue started with using an inexact π.
This problem, with different non-zero results of sin(M_PI), occurs if code used a different FP type like float, long double or double with something other than 53 binary bits of precision.
I'm pretty sure your coprocessor has sin and cos operations, and you can call them using Assembler, something like:
If you really need to implement the sin and cos functions yourself you should use the taylor series sin x = x - x^3/3!
The issue is:
In the first example, you are updating the global variable angle, but in the second you are updating this.angle,
In the first case, you are recreating the object viewFinder each frame, and since the value of angle is changed, the values of both dx & dy are re-calculated for the new object.
The changes made to the sine wave graph:
While generating your sine wave, you need to incremently change the value of the y axis since, you havn't done that you obtained a straight line.
I used this expression y =  180 - Math.sin(counter) * 120; where counter's value is increasing (to obtain the value of increase you can literally take any PI/value but we've made use of PI/18 small increments for each coordinate as you can see initialized in the increase variable )exponentially not linearly and then decreasing again and we have made the for loop run for 2*PI instead of PI to get 2 complete wave instances, one before the origin and one after.
BTW: Placeholder %f can be use only in string formating (or string parsing).
sin takes a double and returns a double - sinf takes a float and returns a float.
In other words sin is double precision and sinf is single precision.
If you're using an old compiler that doesn't have sinf you can implement it as:
#define sinf(x) (float)sin((double)(x))
sin takes a double and returns a double and is defined by ANSI C.  sinf isn't.
sinf() was added to C in C99, which Microsoft Visual C++ does not fully support (even so, Visual C++ 6 was released before C99 was standardized).
You can use the sin() function, which takes a double (sinf() takes a float).
since   cos ϴ = X / H  (basic definition of cosine),  we can say (via simple algebra)
if you're using degrees, you have to multiply by Math.PI / 180 to get the correct value.
Now our classic definition-of-cosine formula (when translated to our terms) looks like
"why am I using cosine to calculate the vertical velocity then?
Finally, why is there a negative in the cosine?
since the y-axis is facing the other direction, you multiply all y-values by a negative 1 to get the correct orientation
And since you want the new value to depend on the old value, you just use the variable there too:
But GLSL is picky with its numeric types, so since u_time is a float, you have to multiply (or divide, or add, or whatever you do) with a float:
#2 is just changing the 2 for a division, or using a number between 0 and 1.
For #3, you use a non-constant value for the other fields of the vec4 constructor, for example you could use more sine waves with some offset added to them, to make multiple colors.
Regarding optimization, the GCC compiler is probably throwing out the loop, since you don't do anything with the result.
compiled with GCC 4.9.2 using
And some processors have machine instructions for sin, cos and tan but they might not be used (because the compiler or libm know that they are slower than a routine).
the trig functions are not as accurate aa the other math functions on the x86, so sin / cos will not give the same result as tan, which is something you should bear in mind if IEEE compliance is your reason for asking this.
As for the speed up, sin and cos can be obtained from the same instruction, so long as the compiler is not brain dead.
The compiler can not therefore substitute sin/cos without breaking the standard.
mySin should return double.
sin values can be calculated with the Fourier series.
Your mySin function is wrong in at least 5 separate ways.
Using ** to represent exponentiation (to avoid confusion with xor), the correct formula is
The return value of mySin is completely ignored.
Instead, sinx is the first argument to main, which is the number of command-line arguments the program received (including the name it was run as).
To fix mySin, have i only go over odd values, and have every iteration of the loop compute x**i/i!
and add it into the current value of sinx.
To fix main, declare a local sinx variable in main and assign sinx = mySin(x, n) instead of declaring sinx as an argument.
But since i is not a pointer, it returns the error you saw.
You seem to want full floating point precision (64 bit) and perform very complex operations, as division and trigonometric functions (sin/arcsin).
1 * sin()
1 * arcsin()
When that isn't feasible, for example if you wish to take a sin() of an object that stores x and y displacement rather than an angle, you will need to provide either your own equivalents of such functions (usually as a method of the class) or a function such as to_angle() to convert the object's internal representation to the one needed by Python.
Finally, it is possible to provide your own math module that replaces the built-in math functions with your own varieties, so if you want to allow math on your classes without any syntax changes to the expressions, it can be done in that fashion, although it is tricky and can reduce performance, since you'll be doing (e.g.)
a fair bit of preprocessing in Python before calling the native implementations.
you implement __float__() in your new type and then sin() will work with your class.
in other words, you don't adapt sine to work on other types; you adapt those types so that they work with sine.
if there is no obvious mapping from your object to a float then there probably isn't a reasonable interpretation of sin() for that type.
anyway, for convincing proof that what you want isn't possible, python has the cmath library to add sin() etc for complex numbers...]
If you want the return type of math.sin() to be your user-defined type, you appear to be out of luck.
If you want to be internally consistent and duck-typed, you can at least put the extensibility shim that python is missing into your own code.
Now you can import this sin function and use it indiscriminately wherever you used math.sin previously.
It's not quite as pretty as having math.sin pick up your duck-typing automatically but at least it can be consistent within your codebase.
It is not an aliasing issue.
It is easy to understand when you think of one period of sine wave.
But this kind of alignment with a complete period does not happen when there is only one period of the signal with a lag since the edges are always padded with 0.
sin(pi) does not return 0 because pi is not π.
Even with extended precision and the best acos()/sin() routines, the value of π can only be approximated.
The sine of machine-pi is indeed about 1.22...e-16
From your next line it appears that you wanted to do it with append, but since you wanted to work with numpy arrays, it's better to start off with an array (like in my example) and assigning to it as you go along.
Fixing those errors, this would be a working version of your question, with the solution using pandas (which I consider to be simpler and more readable, but works no better or worse than the other answer)
Usually it's a single cycle.
It's certainly slower to use texture lookups for sin/cos as it is to execute the instructions.
You'd have to test this out yourself, but I'm pretty sure that branching in a shader is far more expensive than a sin or cos calculation.
[Google'd links (hyper-link)] say cos and sin are single-cycle on mainstream cards since 2005 or so.
see how many sin's you can get in one shader in a row, compared to math.abs,frac, ect... i think a gtx 470 can handle 200 sin functions per fragment no probs, the frame will be 10 percent slower than an empty shader.
If you use both sin and cos in your shader, you can calculate only sin(a) and cos(a) = sqrt(1.0 - sin(a)) since sin(x)*sin(x) + cos(x)*cos(x) is always 1.0
Math.sin(): 652ms
Array(): 41ms
Float64Array(): 37ms
[Jsbench (hyper-link)] shows the precomputed array to be 13% faster than using Math.sin()
Math.sin(): 100% (1718 ms)
Run them in a benchmark and the results are surprising.
Testing array and sin.
To be fair to sin I do not do a deg to radians conversion.
sin() calculation is not obliged to be accurate to the last binary bit, but to within 1 bit.
Good sin() implementation are correct to the last bit**.
if sine has the following form:
Which is the sine form you are looking for I believe.
In your example, the e-16 is very small and is basically zero as you said, but somewhere in the calculation there was probably a rounding error for the cosine values but it just so happened not to be the case for the sine values.
You can try a different range for the integral of sin and probably find the same thing happening.
[http://www.coranac.com/2009/07/sines/ (hyper-link)]
It has a couple of algorithms for computing approximate sin(x) values, with both C and assembly versions.
There is already a FCOS and FSIN opcode since the 80387 (circa 1987) processor
If you are using gcc you need to pass -lm at the end.
I can speculate that's happening because you are calling sin directly on a double literal and gcc can do its own magic without requiring libm - it can simply compute it directly and replace the call entirely with the result.
-lm needs to be at the end of the command, most likely in the first case with the literal the compiler is optimizing out the call to sin and therefore does not need to link against the library.
Here is a [live demo (hyper-link)] which we can see from the assembly in the literal case gcc optimizes out the call to sin and this [demo (hyper-link)] shows the non-literal case and we see in the assembly:
You assume that sin takes degrees (it takes radians)
You use integer division (if sin indeed took degrees, which it does not, you should have used 180.0 in place of 180)

pi = n*(sin(180/n));
    
int *n; double *pi;
The inaccuracy can be traced to the fsin processor instruction used by the sinl library code.
The instructions fsin, fcos, and fptan are not accurate to 1.0 ulp as Intel claims:
[http://notabs.org/fpuaccuracy/ (hyper-link)]
It doesn't document anything for sinl.
If you really want a fast and accurate sinl, I'm not too sure where to look.
[CRlibm (hyper-link)] does sin (the variant for doubles).
[MPFR (hyper-link)] will handle it, but it'll be many times slower than fsin.
In the reduction stage a perfect implementation would have to somehow generate the missing 53 or 64 bits of accuracy after subtracting (x - M_PI), as an naive implementation would calculate this interim value as zero.
Since you can derive a set weights to approximate a sine or cosine function, that must inform your idea of what inputs the neural net will need in order to have some chance of succeeding.
Further down the wikipedia page on Taylor series, there are expansions for sin and cos, which are given in terms of odd powers of x and even powers of x respectively (think about it, sin is odd, cos is even, and yes it is that straightforward), so if you supply all the powers of x I would guess that the sin and cos versions will look pretty similar with alternating zero weights.
(sin: 0, 1, 0, -1/6..., cos: 1, 0, -1/2...)
I think you can always compute sine and then compute cosine externally.
I think your concern here is why the neural net is not learning the cosine function when it can learn the sine function.
Since cosine can be computed by sine 90 minus angle, you could find the weights and then recompute the weights in 1 step for cosine.
Since the number of significant digits of a float is about 7, and the slope (first derivative) of sin() and cos() at that points is +/- 1, I would say that the results are as good as you can expect.
One alternative is to use grads or degrees instead of radians, so that the multiples of full circles, as well as multiples of each quadrant are integers and also the sines and cosines of those arguments can be represented exactly.
Consequently in a well written library sin((pi/2)+(2*pi)*n) increases with n; with poorly written library, the argument is evaluated modulo approximation of 2*pi, giving exactly the same offset for every n.
That's because the C/C++ runtime function sin() expects the argument in radians, not degrees.
sin takes its arguments in radians, not degrees.
Bonus: Since 5*PI/180 is constant, you can precalculate that.
Maybe it's offtopic, but using simple trigonometric identities, you can calculate answer without any loops for arbitrary sum value with a simple formula:
where t is your step angle (5 degrees), result_sum - needed acumulated sum of consecutive sines (=3 in your case)
For sin/1 the summand is an = (-1)n * x(2*n+1) / (2n+1)!.
The predicate init_sin/4 and mp_sin/4 then compute sine backwards from the smallest summand to the biggest summand a0, using Horner schema to get the sum as well at the same time:
The internal predicate mp_math/3 is here used for BigDecimal arithmetic, so that we can also compute sin/1 with a precision of p = 100, which is not possible with usual floats.
Since all [computations (hyper-link)] are done in the given precision, when you compute backwards with Horner schema, you need very few bits.
To get a p = 4 result I was using p = 5, and to get a p = 100 result I was using p = 102.
When you put sc.next() in each if check, and your input doesn't match 'sin', then it'll stop there and wait for input because the else if block that's coming up also has an sc.next() call.
Depending on this comment, I assume you can draw (x,y) coordinates, so this is not your problem, but rather to find y for a given x so the resulting graph is a sine.
Well the simplest case is y = sin(x).
Since pi is an irrational number and goes on forever, the Python version is not actually exact pi.
As a result, the sin of almost-pi is almost-0.
[why am I getting an error using math.sin(math.pi) in python?
You have the wrong equation for sin(x), and you also have a messed up loop invariant.
The formula for sin(x) is x/1!
The angles(thetas) are passed through the sin() and cos() function so that the observations are in the range [-1,1].
This fixed range of [-1,1] helps in stabilising the training in the neural networks which has been explained well [here (hyper-link)].
You could even use one of the sin() or cos() as your observation.
The reason(which I can think of) for using both sin() and cos() is probably to give more information about the state.
Maybe using both sin() and cos() leads to a faster convergence.
I ran DDPG with just sin() and theta_dot in one experiment and with sin(), cos() and theta_dot in another experiment.
The usage of both sin() and cos() is experimental I guess.
Since the actual sine function goes between -1 and 1, it means that your yEnd will go between scale - scale and scale + scale.
Since your scale is half the height of the window, 2×scale means the full height of the window.
It shouldn't be adding the sine to scale.
Think: when the sine is -1, you want it to be at the greatest distance from the middle of the window.
Since width/50 is more than width/64, you will be drawing more than your width.
That is assuming your X data is in degrees, and that you really intended to convert that to radians (as numpy's sin() function requires).
The fit result is terrible, because these data are not sinusoidal.
[sin (hyper-link)] is not in your current package.
Then search for the maxima in the spectrum using a detection threshold.
If another app imported your package and then found out that sin() wasn’t what it expected, things could explode.
The input of 'sin' is in radians ([http://en.cppreference.com/w/cpp/numeric/math/sin (hyper-link)])
fsin, fcos, and fsincos instructions were not available until the 80387.
Your compiler is unaware of these instructions in the version of Turbo-C++ you are using.
If you are running your program in an environment that emulates an 80387 or has a physical 80387 present then you can consider encoding the fsin instruction with a db directive.
According to the Instruction Set Architecture reference The [fsin (hyper-link)] instruction is encoded as D9 FE.
Using LD preload will work.
Now I define my own sin
mysin.hpp
mysin.cpp
As a consequence of this, numpy.sin tries to call sin as a member function of the elements in the result of A.dot(w) which is not defined.
Produces error: np.sin(np.array([np.array(1)], dtype=object))
No error: np.sin(np.array([np.array(1)]))
As @Adelin has mentioned above, simply call np.sin(A.dot(w)[0].
Integral of sin(x) equals -cos(x)
Definite integral of sin(x) from x = pi to x = 0:
-cos(pi) - (-cos(0)) = 2
Since quad compute a definite integral, I can't see any problem.
What you're trying to do can be achieved using the following:
If you don't need all that precision, create a lookup for the sin() values you need, so if 1 degree is enough, use double sin_lookup[360], etc..  And possibly float sin_lookup[360] if float precision is sufficient.
Also, as noted in comments, at a certain point as per Keith, "You might also consider using linear interpolation between lookup values, which should give you substantially better accuracy (a reasonably continuous function rather than a step function) at a fairly small cost in performance"
It seems to me that sine1, sine2, sine3 and sine4 arrays are completely independent from eachother.
So you are basically running a single for loop for 4 different arrays which have no dependency.
Actually combining the use of threads (consider this with OpenMP) and the use of a table for the sin is a good idea.
In most cases, you're computing something of the form sin(k * a + b), where a and b are constants, and k is a loop variable.
Thus, you only need to to do the full trig calculation for k == 0, but the rest can be calculated via this recurrence (once you have calculated cos(a) and sin(a), which are constants).
And by the way, there are really infinitely many solutions, because sin is 2*pi periodic.
SymPy currently doesn't support giving all of them directly, but it's easy enough to get them by using sin(z + 2*pi*n) instead of sin(z):
When np.sin is called on an object array, it tries to call the sin method of each element.
(Which I was using)
Let's look at why the result from sin gives 1.
Also, the sin function has errors in the evaluation of sin.
Let's assume that your sin calculation has a similar magnitude of error.
This calculates (possibly) the sin of your PI/2 but at a higher precision (using the same approximation for pi).
The sine of p/2 is about 1 − 1.8747•10−33.
The closer of those is 1, so the closest representable value to sine of p/2 is exactly 1.
The cosine of p/2 is about 6.123233995736765886•10−17.
If you use a better value, you'll get a better answer from sin.
Note, that I've used more accurate input values to sin here, their average is closer to pi/2 than your example.
Since react-native interpolation doesn't support function callback, I made a helper function:
PC/SC driver for ACS USB CCID smart card readers
Middleware to access a mart card using PC/SC (development files)
If x is expressed in radians: Math.sin(x)
If x is expressed in degrees: Math.sin(Math.toRadians(x))
If x is expressed in grads: Math.sin(Math.PI * x / 200)
The gradient function for sin x is cos x
The first thing you should note is that you'll have to be careful to pick the correct root for sqrt(1 - sin(x)**2), otherwise you run the risk of evaluating |cos(x)|, which is not the same.
Even when I was using FORTRAN, the clever FORTRAN compilers would use the trig functions available on a chipset.
sin()
Try 
=SIN(MOD($B5,23)/23*2*PI())
My longer version using bc:
In your case, I suggest to create a bashj file with a java methods loading data into double[][] from the file, and a java function returning the Math.sin() and Math.cos() of the loaded data, using row col index as parameters...
Using .
operation since the term x is used in an anonymous function.
Using MATLAB version: R2019b
I would recommend trying to round the value down a decimal place or two, using something like this:
At first order, sin(pi-epsilon)=epsilon+O(epsilon^3)
So, if we have an approximated value of pi instead of pi (with an error up to 1/2 unit of least precision), then we expect this error to be transferred directly to the sine result.
Since 2 ≤ π < 4;, this means that the gap between pi (the numerical approximation) and π (the exact irrational number) is bounded by eps.
Now using the result from @aka.nice answer, and that sin(π) = 0, we have that
sin(pi) = | sin(pi) - sin(π) | ≈ |pi - π| < eps
Note: there is also some slight rounding between sin(pi) (the numeric result) and sin(pi) (the exact result), but this is of order eps2, so can be ignored in this case.
It suffers from numerical instability and underflow, since after a while (~100 loops, depending on x) a becomes 0.
correct answer of sine taylor series
When using nested forms, you need to specify which nested attributes should be whitelisted:
As for the style of the graph you can switch thhe fill off and the points off by passing in null to the apripriate parrameter on the LineAndPointFormatter.
As you are aware, math.sin and math.cos expect their arguments to be in radians, not degrees, and, as this line shows:
You have to declare "nested result maps", either nested in a single result map or referenced: use association and collection tags.
Therefore, the result of Math.Asin(3/4) = Math.Asin(0) = 0.
If you want to gain the result of Math.Asin(0.75f), you should use Math.Asin((double)3/4) or Math.Asin(3.0f/4.0f), or whatever formatting that has floating point.
You might use approximation of sin(x)≈x for small values of x to get much simpler equation, which is pretty trivial to solve.
You can combine chars and variables using [num2str (hyper-link)] like this:
length = h / math.sin(radians)
To make crickt_007's right answer absolutely clear: radians which you did not use after you calculate it should be the argument of the sine:
You don't have all the independent variables defined for a sin() function.
An arbitrary sin() function is of the form Asin(Bx + C), where C is the phase.
I don't think you'll get a fit using any method without all the independent variables defined.
The comma is Python syntax that denotes either a single-element tuple.
In this case, it is used for argument unpacking: plot returns a single-element list, which is unpacked into line:
The result is that you're importing [a module (hyper-link)] which does not have a sin function.
Since you already imported pygame on its own, you can qualify its members (which you seem to be doing anyway) and remove the from ... import * lines.
You can determine what the limiting factor is for your GPU and application by using the CUDA Occupancy Calculator, which is included with CUDA.
