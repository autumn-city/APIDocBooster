YUFENG GUO:
Scikit-learn has long
been a popular
library for getting
started with machine learning.
But if you haven't had a
chance to try it out yet,
that's all right.
Let's go check it out together.
Welcome to Cloud AI
Adventures, where
we explore the art, science,
and tools of machine learning.
My name is Yufeng Guo,
and on this episode,
I'll help you get started using
scikit-learn in a Kaggle kernel
and point you to some
resources to guide
your learning moving forward.
Let's start with a little
dash of history for context.
Scikit-learn was originally
called scikits.learn,
and started life as a Google
Summer of Code project
by David Cournapeau.
The scikit part of the name came
from it being a sciPy Toolkit.
And from there,
scikit-learn has steadily
gained adoption and
popularity to where
it is today, a well
documented and well loved
Python machine learning library.
If you take a look
at scikit-learn
at scikit-learn.org--
and you definitely should--
you'll quickly notice that the
version number is quite low,
0.19 as of this recording.
Don't be scared off by that.
The library has been
around for quite a while,
and is very well-maintained
and quite reliable.
What's really neat
about scikit-learn
is the rich suite of
tools for doing things
that I like to call
"around" machine learning.
It includes everything
from dataset loading
and manipulation
to preprocessing
pipelines and metrics.
In my opinion, the
really incredible part
about scikit-learn is
the vast collection
of machine learning algorithms
that are all included.
And you can just try
them out, most of them
with minimal code adjustments.
It's truly an amazing way
to get a handle on what
different types of models do,
as well as gain some intuition
around how the various
parameters for a given model
perform.
Let's take a look now at a
simple example of scikit-learn
in action on a Kaggle kernel.
We have a dataset
here of zoo animals,
just 101 different animals.
The classification
task is to determine
which of the seven different
classes these animals are in.
We'll load it up using our
typical approach with pandas.
Note that the class type
field is in the final column,
and that's the column we'll be
interested in predicting on.
In the past, we shuffled
and split the data by hand,
using pandas, as
we can see here.
Now, we won't run that because
scikit-learn has consolidated
all these tasks
into one function,
since they're just
so commonly used,
and it's called
train_test_split.
It takes care of creating
training and test data
for your features and labels.
We can see the shapes of these
are exactly as we would expect.
The default is to
use 25% of the data
for the test and the
remaining 75% for training.
We'll use a Support
Vector Classifier, or SVC,
in this example.
But you can easily swap out this
line for a different machine
learning algorithm.
Next, we'll call
fit on it, which
is really the same as train.
And then we'll call .score to
evaluate the model performance.
Finally, we call predict
to try out a few examples.
As you can see, scikit-learn
has an API that maps really,
really similarly to the
conceptual workflow,
making it easy to use.
So that was just a
very simple, quick look
at how you can
integrate scikit-learn
into some of the existing
work that we've been doing.
But I know that you
can do much more.
Dig in to the tutorials
and the documentation,
and make some awesome models.
Next time, we'll talk about the
other side of machine learning
with scikit-learn,
making predictions
and how to scale that up.
Thanks for watching this
episode of Cloud AI Adventures.
And if you enjoyed
it, please like it
and subscribe to get all
the latest episodes right
when they come out.
And don't forget to explore the
vast world of machine learning
models that scikit-learn
has to offer,
all with a simple, clean API.