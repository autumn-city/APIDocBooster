For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.  However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].
BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice. 
 So we had if we  had three features, we had three gammas and three betas.  Now, we  extend this concept here to the two dimensional case where we  compute these four inputs that are four dimensional, right,  because we have now the batch size, we have the channels, we  have the height, and we have the width.  So we compute the batch  norm now, over the number of inputs height and width.  So in  that sense, we, we combine these. 
Add the model.eval() before you fill in your data.  This can solve the problem.
(I don't think PyTorch has a way to automatically do this for you.) 
