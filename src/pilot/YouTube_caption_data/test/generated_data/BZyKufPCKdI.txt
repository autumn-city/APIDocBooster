[Music]
all right so hi my name is Adam part of
the core developer team of Pi Church
which is sort of a framework used for
training machine learning models so
let's start with why so there are all
these other frameworks in the sort of
deep learning and machine learning
landscape and so that's a that's a good
question so the most important division
between these frameworks it's in here
it's like a left column in the right
column so they're sometimes
characterized by defined by defined end
run frameworks or static graph
frameworks which are on the Left which
are on the left in here and then they're
also the the define the defined by run
that's the terminology introduced by
chain ER and also they're called dynamic
graph frameworks so PI Church belongs to
the second category I won't really be
going into the differences between
frameworks in the in the other category
because I simply don't have time for
this I will only like sort of explain
why this is the sort of more classical
approach to to doing machine learning
frameworks I'll explain the difference
between these columns mx net supports
both but recently they pushed a lot they
pushed a lot of work into gluon which is
a pie chart like interface to a mix net
they also had an imperative back-end
before so that's why I decided to
include it in here but it could be
somewhere in the middle so that's so for
the left column i'll be using tensorflow
as an example so that's how you would
write your code in tensorflow so you of
course inferred the packages then you
define variables like all tensorflow
implements its own sort of virtual
machine that executes all of your
computation so in here you define the
variables which are the like data blobs
that will live along with your
computational graphs and I'll show you
to find placeholders which are
like empty places in your graph where
you will be fitting in input data and
targets and so the programs usually have
two parts in the first part you just
declare the computation so you do it
only once usually and when these lines
are executed actually nothing is run yet
it's only that in the background the
framework instructs the the graph so in
some internal representation and then
also you derive the gradients and you
also put the update rule for your model
inside inside the computational graph
and then there is the actual part that
that does the computation and this is a
for loop and for this you simply use the
graph that you constructed before you
take the data from somewhere you feed it
into the graph and this is like a
entirely completely separate virtual
machine from Python it's chewing
complete you can declare loops
symbolically and have they and have them
executed there so the characteristic of
these frameworks are that they have this
declarative syntax or symbol or they
just construct the computation graph
symbolically they have this custom
virtual machine that interprets the
graphs you construct they need you to
sort of give them the whole computation
you'll be doing up front they need
complete information which is two
benefits one of them is that it's really
easy to serialize such graphs because
like the we have complete information
contained in this so we can just dump it
in some format to disk and then load it
in another machine and it's exactly the
same thing and also it's easy to
optimize just because you see everything
from the beginning even before you start
running so there are three things the
first things we didn't really like them
so pie chart is like a lot of people in
patrasche community are researchers and
we really believe that these aren't very
suitable for this kind of usage so the
problem with declarative and symbolic
syntax is that um the actual line where
you make the error is very far from the
place where the air is actually raised
so
you spent a lot of time sort of
correlating the errors you get from the
virtual machine with your actual code
and yeah and also custom virtual machine
this also is linked to these errors but
also once you make the error like if
your code is just in Python you would
like use PDB or just print statements to
see what happens around your code if you
have a custom virtual machine there's
which is implemented like in C C++ you
probably just need custom debugging
tools which is additional overhead to
debug and these are extra tools to learn
so it hurts productivity and the last
thing is we don't really want the end to
know everything upfront so there are
some models that have such dynamic
structure that it's really hard to
represent them in this framework in this
framework so we really wanted to to sort
of integrate with Python natively so
that you can just use regular control
flow statements that you have in Python
without like symbolic like if you want
to iterate over trees and tensorflow and
like collapse trees with some network
that's really hard
so in pi touch the same snippet would
look like this I will be going over the
API later so you just declared that your
weights and then you just have your
regular Python for loop where you create
the data these lines actually run the
computation so that anything happens the
stack trace will point you to the route
through the line where something breaks
this is very similar to numpy that you
that I surely know then you also we have
an automatic differentiate a
differentiation package that I'll be
talking about later and then you have
the update rule for your network and
some gradient management so the things
we really focus about is minimizing
mental overhead that you have to put
into work so we really don't like high
level abstractions and we really want to
keep things simple so that are you like
if you use one of our library functions
you Pro you should be able to imagine
how it actually works internally we
don't really want to have very magical
api's also so PI dirge is like the front
end itself is new but the whole back end
is
is coming from Lua torch all the math
functions so yeah I was previously using
Lua and Lua echo system is extremely
small and so like if you want to load an
audio file you better know some C and
and and like write your own package for
this in Python we don't want to do that
we want to integrate natively also we
want to allow these freeform models and
we were extremely focused on performance
we really want to keep it very fast and
match our exceed performance of static
graph frameworks so pie charts first and
foremost is in the array library very
similar to non pie so to live up to the
hype we of course had to call them
tensors but yeah basically these are nd
like numpy in the arrays you have
regular Python operators you have
support for numpy basic and advanced and
most of advanced indexing you have
broadcasting a lot of math functions are
supported as as methods and also in a in
this functional form we also paid users
love modifying things in place so you
also get these methods if something
modifies the thing you call it on it
hasn't underscore at the end so it's
really easy to see that also these
methods can be changed so you can like
do uniform which samples normal data
then you sort of add and multiply do
something else apply sigmoid and you
sort of have a chain of in place
modifications that you can declare like
that of course you have also typecasting
for other tense verbs by default these
are floats not double because I'll get
to the reasons later and and of course
pretty printing but yeah that wouldn't
be really different from numpy if it
wasn't that we really used a lot of GPUs
to Train machine learning models so pi
touch natively supports GPUs so - so -
youth so to do a computation GPU you
just call the CUDA method of the tensor
and this will actually transfer the data
to the GPU and then the API of that
object is exactly the same you just do
operations as if you did on the pea
razor and the CPU tensors and then if
you print this will transfer the data
back or like if you you can use the CPU
method which
transfer it back and so that they're
like the GPU and CPU synchronize because
they were in a synchronously and you can
actually see the data if you print the
CUDA tensor it will actually tell you
that it's not torch flow tensor it
starts at CUDA that flowed tensor and
it's in GPU zero and of course we
support multi GPU so for CUDA you can
give it the device ID you want to put it
on GPUs are numbered from zero in your
system also the code always executes in
the context of one of the devices so you
can change that with a context manager
but even if you're in the context of
different device and like D is a CES in
here on GPU zero on GPU one here we're
in a context of GPU zero this will
automatically switch the GPU for the
time of when you run the when you run
the code so you don't have to like worry
about context management too much but
yeah getting back to the Python
integration so we really wanted to
finally use some packages written by
other people so this is why we have a
numpy bridge so you can consider so in
here we construct a tensor with once and
then if you call the numpad method you
will get the ND array with the matching
type and and exactly the same contents
and this is like extremely efficient and
it's completely independent on the array
size so this cost like 0.5 microseconds
per call and this is because they
actually share the same data so like
converting to numpy and back this is
just a matter of like reallocating a few
bytes and see that describe how to
manage the memory but the data but the
actual data is shared between the two
arrays so if you modify X was a tensor
it's a torch sensor so if you modify it
you will see that the numpy array will
change as well if you modified you know
they re in place you'll see that the
tensor has changed as well and yeah and
they got in the other direction you can
either do torch from numpy which will
take an arbitrary number array and give
you a torch tensor of appropriate type
or you can just give it to the tensor
constructor and if the types match then
they will also like
we use the same storage yeah of course
numpy doesn't have a GPU back-end so
charge from dump I always give you a CVO
thing you can convert it to CUDA tensors
and if you convert CUDA tensors to numpy
then obviously the data has to be
transferred back to CPU so they don't no
longer a sure storage so usually using
numpy and say a GPU enabled model is
sort of slower and yeah but of course a
lot of machine learning is based on as
using gradient based optimization so we
have we need an automatic
differentiation package because
computing gradients by yourself is too
painful so to do that we have a variable
class which is like a box for a tensor
that kind of enables automatic
differentiation on it it starts the
tensor as its data attribute and also
has a grad phone attribute which is like
a closure that lets us compute the
gradient later so in here you can see
that there is like a simple polynomial
function we just take this and we ask
for a gradient of the output of this
function the output will be variable and
the output variable and it's grad fun
will sort of have the whole history so
it will know that you know firstly power
happened on X then independently a
multiplication happened and we added
them together and then we added four and
this always encoded in history of the of
the variable that will be output in here
and yeah so this returns you a new
variable this is because like the
gradient function themselves are also
usually differentiable so you can also
take higher-order derivatives or like
Hessian vector products or whatever you
want so this is how most training
scripts look I mean not necessary this
is the most scripts written in a
functional form look like some people
really like writing them in this way so
you have a function that defines your
model so you take an input and you take
Waits
so this will train a simple linear
regression model so you get your data
and target somehow you declare the
weights and also this requires grad flag
is important so we really designed
autograph to be extremely efficient both
in terms of memory and runtime
so there is this flight this flag will
actually propagate so by default when
you construct a variable it doesn't
require Grad because there's a lot of
like input wrapping yeah so like if you
have your input you probably will wrap
it in a variable and you almost never
differentiate with respect to the input
because you're not optimizing the input
or the targets so the default is false
so if you're declaring parameters you
have to remember to say that yes these
are the things that you will be actually
differentiating and yeah then you just
do a regular Python for loop again you
compute the output of your model you
compute mean square error loss you ask
for the gradients you get them and you
do optimization and here we take the
data attribute and modify it in place we
don't really want the optimization to
happen inside autograph like you don't
really want to differentiate through
optimization steps although you could
have if you wanted someone else actually
do that and yeah but I like this is sort
of a nicer way to write out this simple
example if you have more parameters this
will be a longer tuple sometimes the
management becomes more annoying I'll
talk about ways of dealing with it later
in the wild you'll probably see most
scripts looking like this so this is one
way to to differentiate things to give
output and you give the inputs you want
to sorry the inputs you want to
derivatives with respect to or you do
loss back or you take the output and go
backward on this and this will
automatically compute the derivatives
with respect to everything in your graph
that requires grad and so that's sort of
an easier version the derivatives when
the after they're computed they will be
stored in grad attributes of variables
and they will be accumulated so between
so if you do multiple backward coasts
they will be summed up so that's why at
the beginning of iteration you probably
need to zero all the all the gradients
then you differentiate and then you do
the optimization and that grad is also a
variable for the reasons I talked about
you can differentiate that too but yeah
this might seem kind of annoying but
there is a good reason for that so we
have another package which is called
optin and this actually contains
most of the gradient based optimization
algorithms people care about so you just
give it the list of parameters you
wanted to optimize and you also give it
parameters like learning rate or some
decay parameters for for some moment
estimates yeah and this sort of managers
of this state so Adam has also like a
few statistics it has to maintain this
is all contained in here so and also
this provides a simpler API so this will
0 the grad of all the parameters the
optimizer manages and after you call the
backward you just close step and that's
a single optimization step that will it
will read the gradients from the grad
attributes of the variables who gave it
and so autograph is really powerful
that's sort of how you you can you can
do arbitrary things like you can have
data dependent control flow that's
entirely fine you can even do in place
modifications that's something the most
automatic differentiation systems don't
support or most other frameworks also
don't this will be completely fine like
you can feel likes in this way you can
give it to the model you can compute a
loss function if you differentiate it
bro model will also get the gradients
because it kind of produced something
inside X yeah and also now let's move
into higher-level helpers so yeah there
are a lot of these neural network layers
and common operations that you might be
missing so torch hasn't an end package
and there are two interfaces to it
there's one that's based on n n modules
which is which which are the objects
that we introduced and in the functional
interface so also the network's you will
define yourselves who will subclass
modules so if you an N come to D and n
drop out to D and linear which are just
you know standard convolutional layers
fully connected layers if you assign
them they will automatically get
registered as sub modules so you can
also like so modules can be either
simple modules like these or they can be
containers like net and you can also
nest containers to sort of build up a
more complex tree structure of modules
like for for vision networks they often
have building blocks so you kind of
define the building blocks and then you
then you
connect them in a in a in income in a
full network and to in it you can also
of course pass arguments you can
parameterize like number of layers you
have and then just assign the you know
the layers in a loop in here and that's
that will work just fine and then these
modules are of course accessible as self
attributes the functional interface is
like it just holds no states so the
reason why you want to probably create
drop out as modules is to sort of avoid
passing this training flag to drop out
because you don't want to drop out at
evaluation time and yeah so if you
connect up modules in this tree
structure the like all the state
management and device manager in this
tree is handled automatically otherwise
you sort of have to write this code to
handle it but some functions are simple
like rally or max pool they don't depend
on that so like a rule of thumb that we
recommend is to like use modules for
stateful things because it's much easier
to manage state this way and for
functional things like activation
functions use the functional interface
and yeah and then you can like user
there are helpers for iterating over
parameters so you can do like you can
sum the number of parameters to get it
to get the total in your model we have
utilities for serializing and
deserializing the models but you can
switch the avowal and train all this
recursively traverses the tree you can
change the data type to half to like or
to double if you want more precision
during your training because you have
unstable model you can also zero the
gradients this way you can send it to
CUDA we have a data parallel and
distributed data parallel helpers so
data parallel only works within a single
machine over multiple GPUs distributed
data pair works across I possibly across
multiple processes so when you give a
batch to one of these modules it will
automatically split it into like as many
chances you have GPUs it will clone the
parameters of your model it will execute
them in parallel and I'll GPUs and yeah
and then concatenate the outputs and
return that too so it's really easy to
to use multiple GPUs all multiple
processes if your model
this sort of training scheme very well
and it's very common and yet another
thing we're really focused on is data
loading so in PI Church we have a
standard way of writing data sets and we
really sort of give that to community
and we're really amazed because now we
have vision package and text package and
these are all the datasets that we
support natively like you just download
the packet and you have all the data
loaders and everything there so you
don't really have to spend time if
you're doing research and using these
most common data sets for benchmarks you
just do it use a few lines and that's
enough so there is really a lot and
people are still sending pull requests
with new data sets we're merging them
actively and so yeah so if you have your
training script this is actually taken
from one of our examples so yeah you
just like branch and one of the flags
you give to your input and you just pick
different data sets and so if they're
more standard data sets like see far you
can even like pass a download flag so we
have also utilities that like you have
to say where the data is if the data is
not there it will automatically the
first time you run it it will download
the data set for you on packets and
prepares so that it's you know can be
loaded really fast next time so you
don't even have to play with that if
it's image and edits of course closed so
you have to get it yourself and tell us
where the data is and also you have
transforms so different data sets have
also also have like different
characteristics they need to be pre
processed in different ways we also have
a standard way of writing out transforms
for data sets so in here you can see the
like faux Elson we also do the center
crop which we don't do for for see far
and yeah and this is like the the reason
why we have so many data sets it's
because it's really easy to write one so
you can think of data set as a kind of
lazy list so this is a slightly
simplified minused class taken from our
actual code so so yeah so in the
initializer we like download and open
some files and this actually fits in
memory so we like pre-loaded data but if
it was like image net we
only preload like a mapping from sort of
number of sample to path some disk to
images and then and then you only have
to implement two things which is getitem
so that's a regular indexing operator in
Python and there is also the length
operator so we can check how many
samples are there in your data set and
yeah so then once your data set is asked
for a sample with some index if it's
minused we just slice it out and return
it and transform it and return it if
it's if it's like image net this is the
place where we would actually load the
image from disk and return the image so
this doesn't have to be particularly
fast because we also have like a star of
our data loading API this also was
ported to glue one lately
so there is a data loader class this
just accepts a data set object is the
first argument and this does a lot of
things for you this so this will
automatically paralyze the loading
across the number of workers you have so
this was apone multiple processes to
deal with that for you it will do the
data batching for you it will do the
shuffling for you
it can also page lock the memory so the
so that transfers to CUDA are faster and
can be potentially I synchronous as well
for that sort of more in advanced
feature and yeah and if you have like
this is for the sort of simple sampling
scheme if you want like hard- mining or
whatever you can just write your own
sampler as well and here which sort
which tells like which which is expected
to be like a stream of indices for the
for today loader to load the to load the
data so there is the so this is where
you kind of do the processing to find
the hard samples for your model and the
data loader will automatically load them
in correct order and then you just loop
over it this will every time you start
in your iterator this will spawn more
processes so and reshuffle your data set
so yeah that's how data loading works
and also more of Python integrations so
a few days after a release Brandon
who's a who's a PhD student at Carnegie
Mellon University he tweeted this like
white pied roads Larry creation is
powerful here's my layer that solves an
optimization problem with a primal dual
interior print method so he used this
classical optimization algorithm really
dense in linear algebra he embedded that
inside the model and sort of could run
optimization on some variables inside
his model as well they released that as
a package so you can use that too and
yeah and also sometimes there are
functions that that are available in
other packages and you would really like
them to work with auto grab because we
really care about so say that you know
you this is a simple example for like
normal probability density function but
you can imagine something more
complicated I don't know you have your
favorite FFT library and you want to
bind it so to do that you just write
this function class which has two static
methods that define how the operation is
performed and then how to compute its
derivative and so I won't be getting
into details of that because they simply
don't have time it's a really short
session then you just take the apply
method of this and you can treat that as
a regular Python function that can
accept X is a variable argument you can
also give it a non variable arguments in
here so here we take like a equally
spaced points between -5 and 5 we take
the probability at the probability
density function we differentiate at
once and then again to get the second
order derivatives and we can plot that
and you know we get the first the
function the first order thing and the
second order thing and yeah so that
works natively of course if the package
doesn't support GPU you will have to
take care of like moving the data back
to CPU in here and then back to GPU 2 to
maintain the data type and yeah another
important thing for some people who are
in researchers some of you might care
about deploying models so Facebook who's
actively I mean really actively
maintaining PI toge partner with
Microsoft they released a format for
neural networks called
on
it's a package again in style there is a
one-liner to export your model to onyx
from PI torch it only supports model
with static control flow now
unfortunately both the limitations of
onyx and our trait and our current
tracer and then you can run this in cafe
2 cm TK and a few hardware vendors also
pledged their support for the format so
hopefully soon on some custom hardware
as well there's also distributed I don't
have time to talk about this
unfortunately and the next steps of the
library so as I said we're really crazy
about performance we spent a lot of time
working on making autographs blazingly
fast so we like started from the levels
where autograph was like 10 times more
expensive than doing computation on
tensors right now it's less than two
times so it's like 1.8 microseconds for
a tensor method called compared to 3
microseconds a photograph tensor code we
really we're really spending a lot of
time and profilers now voting it down
and the future will be merging variable
and tensor because the operations are
simply so inexpensive that we can do
this and it will like make a lot of make
life a lot easier because you don't you
won't have to wrap everything in
variable all the time and yeah and
another big thing is you might have seen
glue on we're also working on a sort of
Justin time tracing compiler for models
so right now it's so that's the actual
name of the function just don't use it
right now if you download the master
version it's really experimental we're
only evaluating in a subset of a model
of models it can really it can only
handle like static models right now this
is also what we use for Onix export
we're working on adding support for
control flow so and some of the later
releases this is also going to be
supported but if you have like you know
an RNN which is a dynamic loop like over
dynamic length you really have this like
inside the sub graph of the RNN so
itself it's static so you can just
decorate the function of the iron n so
and like if you like do some small
modifications to Alice TM so you no
longer can use Korean n really fast
kernels so the perf drops sharply but if
you
just add compile we already see like two
to three times improvements in runtime
and yeah so we release hydrogen January
this year we already have like two
hundred two hundred thousand downloads
there are fifteen hundred community
repos some of really excellent quality
we have a lot of we have a lot of users
on our forums so if you have any
questions just go there we can't really
answer to everything but there are some
users who helped us a lot and we have
nearly 300 contributors and also this of
course is a group effort so there is a
large team car team from Facebook there
is their Twitter is also actively
maintaining parts and Vidya is helping
us a lot they're also using this
Salesforce has a lot of their internal
things based on pie chart there it's
also used at a lot of universities to do
research on enrollment and network
models and yeah alright so that's
everything I have so thanks to all the
people who write it thanks to all of you
for listening I was so have a bunch of
pirates tickers so if you want that just
catch me after after after the talk and
I'll be giving them out all right thank
you
[Applause]
Thank You Adam
just some questions I'll open up for a
couple of questions so there's a cool
trick I just saw where you can get like
Syfy stuff and you can put that in and
it'll just work if you do that but you
also want the coolest support and where
are the limitations there any other
questions
how much of Python is based on torch
hi if someone wants to contribute to
your project where are the places but
what would you recommend for the
beginning writing some kind of data
loaders or yes contribute to the library
but where to start further for the
people who
I don't think we have time so maybe you
can catch offline thank you thank you
very much
you