I am Mohammad Reza Mohebbian. 
Here is another tutorial about implemeting advanced deep learning and AI and concept
In another video we talked about implementing attention mechanisim.
and we looked into "attention is all you need" manuscript.
we implemented multihead attention so far.
so if we go up and see figure 1, there is a figure with transformer caption.
The main reason that we refer to the multihead attention is because it is a main component in transformer.
Introducing transformers evolved the seq2seq modeling and time-series predictions.
One the example of using transformer is assume we have a text and
we have a question. The problem is finding an answer, which is called question/answering problem.
The transformer calculate cross attention between tokenized question and text.
and finally can give the start, end location of the answer in the text
There are other applications for transformers
I will explain how last layers can be changed according to the problem statement
but please pay attention to details of the block diagram
We have a positional encoding (we ignore it here in the code, and we will explain about it a little)
It also said that for example the left block is repeated N times
And look at the multihead attention here, there is no cross connection and if you see the below image
V, K and Q are in order
So it means Q ,V and K are same and it is the self-attention
But look at other block, that has different Q than V and K, which means there is cross attention here.
so, we have implemented this block and we can implement transformers
I have created an empty project, called transformers, and I copied the MultiHeadAttention class from another tutorial.
Let's start from this block in paper:
In literature, the left block is usually called encoder and the right one is called decoder.
I can call the encoder as self transformer
and the right one can be called cross-transformer
So, I'll create a file, name it transformer
create a class called it selfTransformer
It should import nn.Module from pytorch
then, we will create init function and forward function because this class is inheriting from nn.Module
This block get an input
To avoid confusing during implementation, always think you are writing this for Question/ Answering problem
therefore, I suppose input is question and ouput is the text
the text that we want to search the answer on it
so, the input of the function is x which is question and I just write it in comment to not forget
MultiHeadAttention should be applied on x
So I define a MultiHeadAttention
the attention class needs query dimension. 
for simplicity, I set the name dim for dimension of the question (query)
since I am doing self attention, I do not need context dimension.
because if you see this figure again, you can see there is only self attention
number of heads is 8 as default, and if you want to define that variable outside, you can pass it to the init
but I want to have more simple code and ignore it for now
so, we defined a multihead attention
There are some operators and blocks which are repeated. For example see residual connection
therefore it is making sense if I create a Module just for residual connection
so I create a new file , called it Residual
I am assuming you know what is residual connection and what is its history
Like before, we define class and inherit it from nn.Module
and I import this one
Residual , to shed light on it, takes an input and function, and apply function on input and add input to result
the function can be anything like multihead attention
this is exactly as same as concept of residual
that function but also can other parameters that we can pass them all by *args and **kwargs
for instance, if you have f function with *args and call functions with different args
all args will pass into the function
but if you set name to input arguments like a=***, b=***
**kwargs can read all inputs in order and you see pycharm is not giving error in editor
so, by passing *arg and **kwargs, all inputs of the function will go into it.
So we have created a residual block so simply!