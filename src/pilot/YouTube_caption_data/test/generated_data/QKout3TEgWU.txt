all right i think we can get started
uh by the way i've seen some questions
about the about project one on classroom
we'll we'll try to answer that as soon
as possible uh
we uh tomorrow i mean next week is
midterm week so i
hope that you guys have some time anyway
so
yeah we'll try to address the questions
as soon as possible
all right before we begin the practice
session today as i said will you
i will try to cover the batch storm and
some popular
popular cnn architectures so yeah we all
again we have a lot of ground to cover
today so we'll just get right into it
so the last tuesday last tuesday i was
about to
talk about how to how to remove internal
distribution shifts but someone asked me
if i could
explain the internal distribution shift
again so um
well i mean we'll start with
distribution shifts so distribution
shift is when
when uh your model is trained your
your cnn or any your machine learning
model is trained on one
one type of training data so the
training data which has a specific
distribution a data distribution
and you try to uh deploy it or you try
to evaluate it on the test set
which has a bit of which has a different
distribution than the training set
so the example here is that for example
if both mnist and cipher 10 has the same
resolution 24 by 24
if they if they this is an assumption if
they have the same resolution
but you've trained your model on mnist
but you tried to
train it on sci-fi and then they are
vastly different images and this is just
digits but sci-fi has
like you know airplanes and animals and
humans
so your model won't work well
obviously because what it has learned
and what is being tested on are vastly
different
so that's what distribution shift is and
machine learning models are usually
vulnerable to this distribution but
there's another thing called
internal distribution shift which is
what
the first bachelor paper tried to
try to address so they they uh
i'm not sure if they were the first one
to actually you know recognize the
problem
the internal distribution but the at
least
they tried to handle it with batch
normalization so what happens is
uh if you have a multi-layer multi-layer
perceptron or
any multi-layer deep learning model then
in the pre in this in this guys in this
layer's perspective
this layer the activation of this layer
is the input
to this layer right so because
this the output of this layer is being
propagated to the next layer
and the next layer and the next layer so
this guy
this layer's input is this layer so the
for example the case layers input
is the k minus ones layers activation or
output
right and in the same sense
if this layer has learned to adapt to
certain distribution of this layer's
output
and it changes then it's going to have a
hard time adapting to the new
distribution
but that's what happens when you update
your model parameters
it's because the mod because the values
of this
layer's output changes or it depends on
the
weight parameter of of this this
intermediate layer here there's a weight
parameter so there's like
a uh some non-elective so this is k
minus one's layer this is k and
for example there's a f w
k uh
i'm just going to say x uh let's just
say h
h k minus 1 plus b b
k
so this is what happens so this is what
happens between
k minus one's layer k minus one layer
and the cake layer
and if and so that means case layers
this k k layers input is this guy
is is this guy and after a parameter
update
at each iteration this guy so
this and this changes and that means
case input changes after each
parameter update after each gradient
descent update
so that means the distribution changes
in the perspective of k
after each iteration so that's not good
so because
once you because each layer has been
trained to adapt to a certain
distribution from the
layer beneath from from the input which
is determined by the layer beneath
and if that changes then everything is
just thrown off so
each layer has to adapt again to the new
distribution
so that's not good so uh this exactly
what it says here so
each parameter update changes layer came
as well as activation
and it forces the layer k to adapt to
new distribution so
so for example this could be like you
know uh
so this is k minus one this is k minus
one layers
uh output for example let's say that
it's an output and it changes after like
iteration
maybe this is iteration one this is
after iteration two
this is iteration three
like each iteration the distribution
changes and in layer k's shoes that is
not a good thing so that's what internal
distribution shift is
so in order to remedy that uh it's just
i mean the the cost of that
normalization is not that difficult it's
just
let's just you know normalize everything
so just like you can normalize the input
to to uh input to the to the model i
mean the entire model so you can
you can normalize the entire data as the
entire training data
using this uh assuming that your
training data set is normally
distributed then you can
just z normalize your input like this
and in the same manner
you can normalize the output or the
activation of
all your layers every single layer's
output or activation
it can be normalized so that's what
that's what batch normalization is
so yeah
the values over um so you
okay so there's a question
um this before before i answer the
question so this
algorithm here is assuming that you are
applying batch normalization in just one
place like even though there's like
a lot of layers this is just an example
and we're talking about
the concept of bachelorization so let's
just say let's just say that
you are applying bachelor normalization
just
just between here just just just like
you know
here we we apply that just here so in
that case what we do is
we take the mean of the output
we need because what we eventually want
to do is
x minus mu divided by sigma but
this is not actually x this should be an
h
because we're dealing with internal
distribution shifts so we need to
work with the hidden layers value
but in this but here it's just they just
denoted that with with an x here so this
is actually this should be like an h
it's just a generic notation so you just
what you need to do in order to do this
is what you need to do
in order to do this you need to know
this and this so that is how you
calculate
that is what you calculate here
calculate the mean
calculate the the variance or maybe or
standard deviation whichever
and then then you do the normalization
remember that this is happening between
layers intermediate like
between two layers not not on the not on
the training set
and there's a cameo so there's one more
bit a bit of a technique
applied here which is scale and shift
this is uh to relax the uh
to relax the the con
how do i say relax the objective
let's just say that yeah yeah so uh just
trying to force
your activation values in all layers
to a normalized value to like xenon like
unit
to to to follow the unit gaussian
distribution like this
like this is bit of a it's a bit of a
harsh policy
like trying to force every single thing
to follow a normal distribution
is a bit is a bit you know a strict
guideline so you know instead what they
what batch norm does
is it it'll it allows some room to you
know to room to
uh to grow or to or to shift around so
that is what what this
is here so once you've normalized it you
scale it and you shift it you scale it
and you shift it so this is a scale
factor and this is the
shift factor so if that means you you uh
even though the spirit of batch
normalization is to normalize everything
to follow a unit gaussian distribution
uh but still it allows some room to
wiggle
so that the er so your your uh
at least that that guarantees some uh
performance
so that's what basically batch norm is
and in order to in order to
do scale and shift here in order to do
this you need to actually
learn learn gamma
and data so this is a learnable
parameter so you just have
two more learnable parameters here in
order to do batch normalization
this these these are like deterministic
operations you just
take a bunch of batch norm but you just
take a a
batch about a batch of data and then you
calculate the mu and calculate the sigma
but what you need to do is
in addition to that you need two more
learnable parameters
which are gamma and beta
this is just like either learning a bias
term we're learning a weight matrix you
just have two more
scalar values
all right bunch of questions
foreign
effect i i think you can have a bad
effect if you don't do the scaling shift
but uh i'm not sure if i i think it's
generally
a helpful thing like the the the
magnitude of helpfulness that depends
but
i don't think it hurts at least
all right i don't have a lot of time so
i will just address the question maybe
in class
okay so yeah so what the the whole idea
is
is this so you just want to just
you know you you you want to let
yeah you want the activation values of
each layer to follow
roughly the same distribution i say
roughly because there's a scale and
shift
uh factor here but roughly like you know
hope we hope that it'll just follow like
like maybe around here here uh
here or or or something something like
this rather than really
vastly moving like this if if your if
the activation value of
each layer has is fi it follows like a
similar distribution
then it's gonna make the the job of
layer k easier
okay so uh technically it's not that
difficult
so what you need to do in each uh
yeah what you need to do in uh in each
layer
and especially in confidence so because
batch normal is again
uh proposed in the context of combat
because people were using
com like multi-layer deep confidence for
classification
back in the day like 2015 so it was
proposed
to remedy the internal distribution
shifts in the context of content so
we're talking about common so that's why
there is a channel here
so so per channel mean
so shape is uh so we're going to assume
that this is
uh you have
a batch of n samples and
in each sample will have a certain
dimensionality
so for example if you are talking about
multi-layer perceptron then
you can set your input so your input
could be 784
but your internal in the dimensionality
could be 128 120 something like this so
that
that means d equals 128 here and
it says channel but you can think of
this as just you know hidden
uh hidden feature dimensionality or or
something like that
yeah so per channel per
per feature or per column in your
internal hidden layer this hidden layer
dimensionality
per your per column or per feature
you take the mean in this direction
in this direction so there there should
be like one two three four like one
twelve one
this is uh this is h uh zero this is h
no not a good notation
yeah this is just the yeah this is like
0
1 2 all the way to 127. so you take
each dimension and each dimension is
mu or or mean here so that's what what
happens here
and you also do this the same for the uh
for the various or the sigma
and then you apply batch norm per
dimension
per dimension so it it's a call it's
called batch normalization because
the normalization or calculating
calculating the mean and the sigma
happens across across the batch so for
if if your batch consists of maybe 64
samples then it cros
it happens over you you collect the
value over 64
different samples and then you calculate
the mu and sigma and then apply it
on each dimension like this each
dimension
right so uh yeah so you you take the
normalization
and then after then this you will have
128 muse so there will be 128 muse
and 128 sigmas and then you apply it
to your n by d matrix and then your
output would be of course
it's a normalized normalized matrix of
shape n by d again
right yeah the problem is
uh yeah again this is the same
same motivation if if this is a too hard
of a constraint or too harsh of a
constraint then
you want a bit of a wiggle room so
there's a wiggle room here
you learn a gamma and beta
and as a as i said this happens on each
dimension of your hidden layer so there
will be
e there will be gamma and beta for each
d like there will be 128 gamma 122 betas
right
and yep the problem i mean
so the whole idea is not that difficult
it's just you you have
you just at each mini batch
update you uh collect
the mean the mean and the variance for
each hidden layer dimension
and then you normalize it and then you
cut
you scale and shift it and that's how
you get
the same n by d n by d matrix in each
hidden layer
and you just set the gamma and beta as a
learnable parameter
and so it technically it's not that
difficult when it's training you just
that's all you need to do but the
problem is
in the test time so not the training
training time but on the test
test time uh you
you when you test when you test the test
data you
you don't necessarily have a mini batch
of test samples
for example you know maybe you on when
you deploy your model maybe you'll just
test
your model or you you just do the
inference or you use the model
one sample at a time just one one single
one test sample at a time
in that case you don't have a mini batch
and if you don't have a mini batch you
can't calculate this value
because that's normalization depends on
the mini batch right
so in that case there's a problem so the
problem
between the discrepancy of the mu
and the sigma between the training time
and the test time so
you need to do a bit of a trick uh a
bit of a trick when you want to when you
want to use batch normalization when you
train your model using batch
normalization and you want to test it
on test time
so you just uh calculate you just save
the
the mu value
as you train your model so when you
train your model you have you
do a bunch of mini batch updates and at
each mini batch update
you will calculate mu and mu's and
sigmas and you just don't throw away
after your mini batch update
you keep them around and then you do a
running average
over each mini batch update so for
example if you do mini batch
update for like you know maybe maybe 10
000 times or maybe
100 000 times to train your model then
you just save those values and just
do the running average you add them
divided by two then add them divided by
two and add them divided by something
like that like you just do keep a
running average and then you keep
the muse and sigmas around so that you
can reuse them
when you test your model uh at test time
right i mean these will be these will be
around like
gamma and beta are learnable parameters
so when you've trained your
when you've trained your model with
battery organization then the gammas and
betas will be there it's just that the
muse and sig bars need to be there
around
so that you can reuse them at test time
so at test time you still do the same
thing
right so the original bachelor
organization was
proposed to be slid between a fully
connected layer
and nonlinear activation which
is kind of weird to me actually because
i to in my
in my head a layer consists of fully
connected network
with uh with the
with the nonlinear activation function
so if you want to do bachelorization
then you probably want to
slide it here instead so there's like fc
and there's like uh there's nonlinear
activation function
then you slide in the batch norm so that
is how you you know slide the bachelor
between
each layer because there will be another
fc
after this linear layer
and then you maybe you can do another
bachelor
uh yeah so the original originally the
paper suggests
this architecture but uh but uh
i mean when i searched on google they i
read a read like a github article
uh where it says that the uh the
original authors
sergey and christian sigity they
actually
implemented in this way not not this but
not this but
in this way so it's still a topic of
debate like which is like
you know more appropriate approach for
which is or more effective
but i i would say this to my mind that
to me
this is this seems more natural this
this one seems more natural
yeah okay so yeah that's uh this is a
summary so yeah it makes the deep
learning model
much easier to train much easier to
train and that's because it
just you know stabilizes the whole thing
and
that means you can have a higher
learning rate which means you can have
faster convergence and
yeah there's a lot of benefit coming
from it uh
also i mean i guess it could act as a
regularization but i'm not sure if this
is kind of
too big of a factor uh
yeah the only caveat is it it behaves
differently during training and test
time
so this is a this is the only downside
of using bachelor in my head
all right so there are different stuff
there were some variations of bachelor
elimination there was vet bachelor
first when bachelor was was proposed
of obviously was proposed in the context
of confidence so
as i said batch norm on fully connected
network is quite easy you just have an n
by d matrix and then you
normal you you calculate the mu and
sigma over the
different samples in the mini batch so
this becomes one
and the dimensionality is is kept around
it is preserved
if you do it for confidence then how it
is applied is
you calculate the mu and sigma over the
entire
samples and height and width
so the spatial information is also kind
of collapsed here
the only thing that survives is the is
the channel
so you just have the muse and sigmas per
channel and then you'll play bachelor
and there's a another follow-up work
called layer normalization
which is uh which can't which doesn't
have this
discrepancy between the uh between the
training time and test time so
if batch norm calculates the mu and
sigma over the mini batch
layer norm layer norm calculates the
sigma
sigma mu and sigma over the
dimensionality
the hidden layer dimensionality that's
the difference so it doesn't depend
on the mini batch the size of the mini
batch whether it's 32 mini batch size or
64 millibars
it doesn't really matter as long as you
just have the some hidden dimensionality
then it will work for you so layer
normalization is another type of
normalization
which behaves the same so same behavior
in train and test time and it can be
used in recurrent neural networks as
well
so uh you will learn run into layer
normalization when we learn
transformer actually transformer uses
layer normalization inside
yeah there's also something called
instance normalization which is
strictly for uh converse
in my opinion so you just
you you don't when you calculate the mu
and sigma you don't
you don't collect information over the n
like here so batch norm
calculates the uh mu and sigma bet based
on the samples
and the height and width so there's all
ones here
and the only thing that survives is the
c instance normalization base is
what is this base it's just a single
instance based normalization so it
doesn't collapse over the end and
survives
c survives it's all is that the only the
spatial information collapses
so this is another thing so instance
normalization was used in
like a a style game basically
style again is with well before it was i
think it was used for
maybe as style transfer or
some yeah style trend i think it was
used for style transfer
uh but then it's kind of it became kind
of like an essential core
component for actually blowing up this
resolution of your image so if you want
to
create something like 1024 1024 image
you use something called adaptive
instance normalization
right yeah so that this just a different
figure like what what happens with uh
with different kinds of normalization
so uh yeah this i mean
when you're dealing with confidence then
you are actually dealing with
4d tensors because there's a batch of
different samples
there's different channels then there's
height and width
so it should be a 4d tensor but you know
we want we need to
we can't draw 40 tensor on on on 2d
planes so what we do is we
then what what happens here is you just
you know merged or concatenated
the height hand with the spatial
information together so that we have the
3d tensor so there's mini batch
there's channel and there's spatial
information and if if you
do batch norm what happens is you
collapse
on the two on two axes which are
batch lower axis and the spatial
information axis and the only thing that
survives
is the channel so channel survives
uh if you do layer norm if you do layer
norm then
the only thing that survives is you
collapse everything in the hidden
dimensionality
all the feature the hidden
dimensionality being the channel and the
height and width
so those hidden in from the hidden
representation related information
they are all collapsed and the only
thing that survives is your
ins is your batch norm i mean is your is
your
per is your per instance information
basically
and if you do instance normalization
then
you don't call that you only collapse
over you only collapse over uh
spatial information only so you only
collapse over
spatial information here because this is
the spatial information axis
so you collapse over here and then the
thing that survives is
uh here everything instances survive and
the channels survive
so yeah it's a different depiction
oh yeah we talked about dropout okay so
moving on
okay all right i've my
my plan was to talk about the batch norm
and the cnn architectures in 30 minutes
i guess that won't happen
so i'll only briefly go through resnet
only and uh
if i have time more time after the
practice session then i'll just talk
about the vgg and inception then but in
order to do the practice session we need
to know
we need to learn about uh resonance so
before in order to do that
i need to talk about resnet first yeah
maybe i'll just think might be five or
ten minutes
so the resonant is the very first paper
that allowed a very like super deep
super super deep learning deep learning
that deep learning
no super super deep confidence so we're
talking about like 100 layers or even
like thousand layers
before that before resonant uh the
maximum
number of layers was probably maybe like
15 or 20 or 22
25 even with the uh even with
all the techniques like using values and
all that yeah we couldn't go
beyond maybe several dozen or a couple
dozens but
resonant was the first one that enabled
using
hundreds of layers so this is a like a
like a revolutionary paper a very
seminal paper
and it just swept uh the challenge the
image in a challenge that year
just you know uh it won like i think it
won every single category of
img challenge uh il svrc not sure what
what
what is short for yeah the basic idea is
you just have a residual connection
between
uh some layers so for like
usually you have a lot of con uh conv
con plus pull
uh sorry not not before pull there
should be
so as i said the core layers are
conv plus non-linear activation
non-linear activation
and maybe pull from time to time
and then you do this a lot of the time
you repeat this
con plus none plus pull again
you know again and again again again so
that that is the essence of
convolutional neural network
and the the core idea here in resonant
is
the resonant is short for residual
network
and the reason that it's called residual
network is
you uh
let's so let's say that
there's an input x here and what you do
is you put
you make a highway between here and here
like this or you could do the same you
could put a highway between here and
here as well because as i said
in this guy's perspective this is the
input right
so like there is always a input between
intermediate layers like like one one
layer's output is another layer's input
so usually what you do is you put a
residual connection or skip connection
between
the input and the output of a single or
maybe
some non-linear active some non-linear
operations
so uh let's see
so it could be either you can have so
what what have
what have if you if you put a uh skip
connection like this what happens is
is uh your new
h uh hk
equals f
w h k minus 1
plus b plus h k minus 1.
so that becomes your new h k right so
what
this this this is the effect of having a
residual
connection and the benefit of this
is if you if the extra layers or if you
like having too many layers the problem
with having too many layers is that
every single layer needed to learn
something but
maybe you maybe having too many layers
is just unnecessary like
like for example if you set your model
to 100 layer but
in in reality what you needed to do the
task that you required
the test you need you wanted to do maybe
only required 50 layers so you just have
extra 50 layers lying around and they
they don't need to do anything but you
just
force the entire thing to learn
something so that is the problem so
if you have some extra layers of laying
around and you don't need that layers to
do anything
then what necessarily happens is this
w can can be just a identity matrix
right if if for example without the skip
connection
if if without the skip connection if w
h k minus one plus b if without the skip
connection if you
don't if you don't want to learn
anything here then w
must be identity matrix so that this hap
and of course b needs to be a zero b is
to be zero so this needs to be identity
matrix and b
is b needs to be zero and i mean f is
given there so i mean there's nothing
much we could do we can do with f but
what
we can do is actually at least we can
learn w to be identity matrix and b
to be zero which is not a which is kind
of a uh
not an easy task because you would
initialize your w
using just uniform distribution or
normal distribution so it has some
it's just like a like a black and white
noise and trying to
force that to become a identity matrix
is not an easy job
but if you have a skip connection then
and you don't want your layer to do
anything then w can be just
zero and b can be zero which is a way
more
way easier job than learning an identity
matrix
learning zero matrix is way easier than
learning identity matrix
so that is the essence of residual
connection
by having a residual connection here
this becomes
a residual term so the only thing that
the
network must learn is the difference
between your previous output and your
next output
so you just learn the residuals you
don't learn the the whole new
representation
your layer needs to learn the residual
and that is why it's called residual
network
and the highway or the skip connection
can be
a bit more uh more adventurous i would
say
so it doesn't necessarily must be like
this it could be you just you can't have
a highway
like here or maybe after three layers or
maybe after four layers doesn't matter
it's up to you like how many how many
non-linear operations you want to pack
in between a skip connection that's a
designer choice
but you it can be as as as short as just
a single
nonlinear operation
uh in the original paper residual
network
it was so yeah there was a highway or
there was a skip connection between two
convolutional neural two two
convolutional operations so there's a
skip connection
here and then interestingly uh they
put the non-linear activation after the
skip connection i guess that makes sense
because
uh you you you want to
you you don't want to be thrown off by
by the non-linear nonlinear
activation value here so maybe it's a
good idea to keep them as a separate
thing
but that again that's also also a hype
like a
a tunable factor it's up to you so the
original architecture
is like this like there's like a lot of
layers
and yeah so uh this that yeah this is
the motivation
i'll just skip that i just talked about
the motivation
why it's a good idea to have the extra
term lying around it's just to
it just makes the model or it just makes
the extra layers to do
learn nothing like it it just makes the
job easy
easy for lay extra layers to do nothing
uh yeah these are all very uh elaborate
uh explanation of the why we need to
skip connection but i guess i just
talked about that as well again
so yeah the architecture looks like this
so
you start with an input there's an image
and then you do a initial
seven by seven convolutional neural
network uh
seven by seven convolutional operation
and then you do some pulling
and then you start a very repetitive
process
so in the first there are like a lot of
blocks here so there's a block here
i i guess it could be like phase one
phase one uh phase two
and maybe phase n
and in each phase the channel
grows the channel size goes so you can
see that this
used to be this used to be 64 channels
and it becomes 128 channels
and after some phase some some number of
another like phases
this becomes at five finally this
becomes 512 channels
and then after that you pull it and then
do a fully connected a fully connected
layer of 1000 output because you are
training your resident to do a image in
a classification so you need 1 000
outputs so that's it
and the pull here is it's called global
average pulling so global average
pulling is
global average pooling is you just uh
collapse all the
all the channels into a single scalar
value you have one 512 channels right
so you have 512 channels
like you have 512 of them and
they will be maybe they will be some
some heightened with
uh you start with a 256 maybe 120 by 128
then this will collapse into
some maybe to 120 120 if you just
preserve the dimensionality all the way
then but what you do with global average
pulling is you just
you just take the mean value of your
entire channel so this
your entire channel becomes a single
scalar
value so this becomes dot dot dot dot
so global average pulling is dot dot dot
just you just have 512
dots and that becomes your so this
becomes your
512 dimensional vector and then you and
then you
apply a 1 000 dimensional linear
activation to be
to to do the softmax classification
yeah so yeah and the skip connections
are
made in such such a way that you do it
every after every two convolutional
operations so you you skip here and then
you skip here and then you skip here and
then you
skip here right
okay so that's i a bit more detail will
be given to you in the practice session
um
yeah uh yeah well i mean the
i hope that everybody has a like an idea
how to implement this if
if there were to implement this from
scratch but of course there will be a
there will be some backbone uh code
codes given to you in the practice
session but and you just need to fill in
the gap but still
if you were to do it from scratch i hope
you have like a vaguely rough idea how
you would do it how you would
uh do how you would up basically you
need to
do you need to repeat this process you
need to repeat this process like several
times and several times
several times right so you need to build
you need to code this or implement this
in pytorch
having a convnet rail you confident and
adding a skip connection then doing a
rail you and then you do a for loop of
this for several times and
after several times you need to jump to
phase two
and when you jump to phase two you need
to
double your channel here so doubling the
channel here
doubling the channel but here uh instead
of doubling the channel you do a stride
number two this is stride
value two so instead of doubling the
channel you
maybe downsize your image doing the
stride two
and then after that you repeat this
process again do it do some
repeat repeat this block several times
and then when you need to jump again
you ops and you you increa you double
your channel
and you set the stride to number two
again so that you know your
your dimensionality decreases and then
you know you do this again and again
until the very last layer
all right i'm sorry that i rushed the
things a little bit
yeah there's just a lot of ground to
cover and uh these are all technical
details
and i'm yeah i'm sure you'll run into
this anyway when you do the practice
session today so
yeah uh yeah there are a lot of
questions in the chat box i will address
them
in the class some more maybe after after
the class if there are people
still like hanging around uh yeah
right yeah there's a lot of good stuff
but it's just that
we don't have a lot we don't have a lot
of time so we'll just jump right into
the practice session today and then
if we have time we'll talk about a
little more material all right thank you
guys and
uh tia hunter please take over we can
start the we can start the
practice session now okay can i start
now um yeah let me
let me stop sharing my screen
okay let me uh
turn on my screen share my screen please
wait
okay can you see my screen
okay thank you then i will
let me start a practice session
uh this week we will cover the
convolution neural networks especially
the rest net implementation in this
practice session
and especially uh since we have a time
constraint
and we don't uh we maybe
we can train the resonant from scratch
on the image net
then so we will train
our resonance the small rats net on the
sleeper 10 data set which
is given by the uh perseveration data
set
in in default so now let me start
first of all uh as you can see the
preliminary
covers the how we import some
required libraries and the data sets
and it is very conventional to
conventional
so you can check easily what
libraries are imported and then
we first of all we will
we will use the pytorch
convolutional layer batch normalization
layer and the drawbar layer
and we will construct a single a simple
convolutional neural network to train
the ms dataset
so we also import the mnista
set and and construct the data loaders
in this way if you
if you attended the practice session of
the previous lectures
then it is very easy and
very comfortable to implementation right
and today we will uh
today since we will implement implement
the resnet
uh especially the 18 to train the cpr
10.
uh we also imported the c parts and data
sets
and uh construct the data loaders for
cpr 10
uh of the train and tesla splits
and especially uh different from the uh
amnesty data set
the the uh
data transformation of the c part 10
is a bit uh come uh
complex but it is very conventional so
it it is worth it
it is worth seeing how uh we
will transform our c part 10 data sets
to train
uh of training and the test space
so now let me see uh the
the transformation operation of the
training data set
will be composed of the random went up
like this
uh the the overall
uh the overall message of the
transformation of the train
is uh how we how can we
uh how can we enhance the
generalizability of
our uh network to train on the aesthetic
dataset
it means that uh if our datasets can
be transformed uh without losing the
semantically
without losing the semantic meanings of
the visual as
a visual things and then the model
also can predict the transformed
images in the same way so the random
crop
or random horizontal flip means that
we will uh crop the images from the
center
or the pixel size and the flip uh
slip in the uh horizontal uh
direction then uh in
then in case of human uh we can
uh we we will classify the uh
transform the images and original images
in the same
class then so we also expected a
trained model classify
the transformed images and original
images in the same class
so we will construct the we will define
the transformation
of the training data set as as like this
way
and the test uh transformation
is very simple
and it is very similar to the amnesia
data set
except it it has
additional uh transformation as a
normalization time
uh and then the training and
at the train and test the data assess
the transformation
uh trans the transformation uh
can be injected to the c part and data
set loader
as different data set uh
as in this way and we will use the
to aid a best site and use the poor or
workers to
define the data loader
over here is there any questions
okay so let's move on to the next part
uh from the uh from
this part we will cover the
convolutional batch normalization and
the drawbar layer of the
in the pytorch library so as you can see
the torsion in a package uh provides the
com
com 2d package to define
the convolutional layer uh in demand so
we will independent the signatu
[Music]
the signature to use the com computing
class is
r as like this so uh
let me let me explain briefly
uh the in channel and our channels means
that
uh the number of channels of the input
images and
the output uh the number of channels
that the
output of the accommodation layer uh
in the uh with this
with this convolutional layer and the
corner
and stride and padding which is covered
by the
lecture lecture so you can easily see
this what the each meaning of that
which meaning of this
so let me try uh
let me see how to use the convolutional
layer in pytorch and
with some example code so uh
the let me let me define
the uh the some input images
which has a which has
a 32 by 32 special spatial
size and three channels and
three channels and also uh in many cases
we use images in the
in a batch manner so we we will
we define the input image x with a
four-dimensional input which assumes
that we
have only a single images in the batch
and then uh as you can see the
this images are from the lecture note we
will
define some convolutional layer
that has that has a 5x5
size kernel and
the output channel will be the one
so the output of the convolutional layer
will be
size as 1 by 28
by 28 so in this case uh
please uh fill this uh
blank of the convolution to the layer of
the portion and package
and uh and please check
uh the output side of the convolution
layer
and it should be matched with a 1
1 1 20 28
and also check other examples in the
lecture note as below
so the in this lecture note
you can also see the other examples in
here
so let me see yes at the
lecture note a page 47 then
you can also check the
results of the of these examples with a
pi torch convolution layer in practice
when you use the input dimension like
uh okay uh
since we assume that there is only a
single image
in a batch which which is corresponding
to the
uh we we defined the data loader
with the batch size as one but in many
cases
we use that we use battery size as 2
128 or 256
so in these cases the input dimension
the first argument of the input
dimension will be larger than one
[Music]
okay the next question from the on one
home you
um in many cases
we don't use the random crop in the test
of time because
because we we
show we we usually want to show our
network performance in the same
benchmark and the same settings so uh
so the random so we don't use any
randomness in the test of time and
the next question is from jaiwan uh
okay they the value in the
transforms are normalized
[Music]
is usually is usually
not completed by ourselves but it is
it is shared from other uh
other code based but uh i
don't know in detail but i will
i will let you know where it comes from
after this session
uh no i i
as as long as like as long as i know
the center crop is not the same with the
random crop
the center crop uh i i i know
when i as long as i know the
center crop uh
actually different with the vendor crop
uh the random presented sleep
is not always required but in many cases
the human
classified the images the original image
and the horizontal flip the images
in the same class so the
we also expect the neural network
classify
in a human way so it is the effort of
the generalizability
in the trained model
can i move on to the next
[Music]
transform you know important quarter
[Music]
i think i cannot answer these questions
in in precise
and in a proper manner so uh i'll let
i'll let you know and after this
practice session
of the question from to you yeah
okay
okay so
the let me see so you can see
the other examples using the uh python
convolution to the layer uh with a while
you fill this blank and you can easily
see
the uh this size of the output
output uh corresponding to the expected
size of the
uh of
the expected size which is covered by
the lecture notes
okay uh let's move on to the
bachelor organization and the drawable
uh
the bachelor normalization and the drama
layer is very easy to use
and i think i
don't have much talking this
in this part so it is very easy to use
the
best normalization the best
normalization
can be imported from the torsion and
that bastion norm 2d
because we will use this uh we will use
the convolutional
layer and the base normalization
in sequential manual so the bachelor 2d
will be used and the number of features
is corresponding to the
output channel of the convolutional
layer
in usual and also the
drawbar 2d also can be used
with the probability which is uh by
default
0.5 value which is the probability of an
element to be zeroed so it is also
helpful to
uh train the model which is a more
generalizable
which has more generalizability power
in practice
and
then after you understand and how to use
the
battery normalization and the drawbar
layer
with this practice then you can build a
simple convolutional neural network
in this practice so i
i already i specify the
the input and output channels
size and the kernel and the stride size
in the comments of the code and so you
can easily
fill this blank with a builder's blank
and then since we will uh
use this a simple convolutional neural
network
to train the uh amnesia data set you can
easily
know the what is the input size
and input dimension of the linear layer
and output
dimension of the linear layer to
classify the amnesia data success in
practice
so i will not cover
this simple structure but you can
easily check the
implemented simple convolutional network
can be trained on the mnist with only 20
app folks
and you can easily see the
[Music]
performance on the tesla data set of the
mnist
so i will move on to the next uh
next part of the more
popular uh and the most popular
standard architectural resonance
uh in this section we will implement the
resonance
and the effectiveness of the visual
connections in terms of the
test performance but uh because
we don't have much time to implement and
also trained on the super person data
set
uh we will we will cover we will not
cover
the whole implementation in the session
and
uh please uh train your
train your own uh implemented the red
drone
resnet uh on cpr 10 after this
practice session and since i already
trained
and since i already trained the rest net
and
the other cnn architectures without the
registered connection
so i will show the
performance comparison in this session
in this intersection
so the overall structure of the resnet
is like below but but as you can see
the uh the some details
in the uh convolution layer
or the input and output channel size
size are a bit different from the
residual network
covered by the lecture notes because
because we implement the restroom at
work
on the c person data set which is a much
more
a smaller data set a small data assessed
than
the imageness so uh imagenet
so we will uh we will define and
implement more smaller
smaller size residual network in this
practice and so
to construct the registral network we
[Music]
virtual network we should implement
the component of the retro network which
called the residual blocks
so the original block
the residual bar will be
will be implemented to follow this
figure so in a simple manner
let me explain the input image the input
x
will be a feed forward to the conv
to convolutional layer and the
intermediate
activation function uh
active estimation function and the 3.3
composition layer
uh uh three
three by three convolution layer will be
used
and in some cases when in some cases
we will use a stride size two to
downsample the
input input spatial 5
and and the increased
output channels by a factor of 2
in that case so
in the case and also since the uh red
fuel network use the regular connection
which uh simply adding the input uh
inputs on the uh output of the
a convolutional a small convolutional
network box
so we will cover the
this residual block implementation
in this part
so i
i made some blanks in this code
so you can easily fill the blanks in
this code
with the uh they
understand with the understanding of
their uh
residual connection and the resonant uh
structure so
uh you can easily implement
it by yourself after this session
and the
after you implement after you fill the
blank
of this part and run
part then i want to note that
the simply adding the shortcut which
is which means the register connection
visual connection in this figure
then it won't work
it won't work by default uh
okay the question from tom june
uh do you mean by
do you mean by the residual connect
aware is the rational connection means
that
in the code or the figure
can you type code uh okay
the registered connections in the code
is implemented in this part and in this
part
uh okay sorry so
the original connection is implemented
with a
shortcut variable which which restores
the original input x
and the the forward origin input x will
be added
to the out uh the
the result of the
convolutional blocks and
and also
as you can see we will use the residual
blocks to construct the resnet but
in the second and the third residual
connect
residual block the number of output
channels
will be doubled and the
end and also the special size of the
input images will be down sampled uh
downsampled with a factor of
two so in this case the residual
connection
uh also downsample the
special size and also a double the
channel outputs to uh to
connect the visual connections without
the
dimensional mismatch so you should
implement
uh the down sampling method you should
implement
this part to use the down sampling code
using the down sample
boolean condition
building conditions in this register
block
i i describe
and i describe and the some
notes on this text so you can easily
see what should be
implemented in this blank and this
comment
let me see the questions from jejune
ah okay uh
no uh in short answer
uh please not so
the reason why we use the residual
is right to write
to not to train the uh
that's a train the convolutional network
the
unnecessary convolution network but uh
by simply adding the residual
connections to the output
of the convolutional network then the
network can simply use
what parameters to be updated
in while training the whole
resnet architecture so we
simply adding the shortcuts in the
output of the convolutional network
and yep
i think uh the answer
uh will not i think the answer
is not enough to the questions so i will
cover after this a practice session
sorry
okay any other questions with the
residual block
implementation
uh okay let me explain again about
down sampling uh since
the signature and the architecture of
the
uh resnet is uh using these three
the residual blocks to uh
reject blocks to downsample the special
size
and the doubling the output channel
alpha channels right so in this
second and the third residual blocks
then
we will doubling the channels and
also we will uh decreasing the sparse
spatial size of the inputs uh
with a vector of two in this case the
convolutional the first convolutional
layer will use this right
side with the two to decrease the
spatial size
and and also uh doubling the
output channels right the in this case
the outputs of the convolutional path
the dimension of the output of the
[Music]
convolutional path will not match with
the original impulse
right so in this case we shoot
down sampling the special dimension
of the original input and also
increasing
the doubling the channels
to match the to match with a
convolutional
uh layer path
so then i can write this
um
you me you mean the question from
sorrow do you mean
by this code in the
the resonance class right
or as i explained it in this
no no since
the residual blocks will be
placed in this
okay let me explain again
the
okay i look i will cover the first
question of the sorrow first
we will use the n residual blocks right
the the end residual the
output of the n residual blocks will be
16
and output of the end residual box will
be 32.
so in this means that
uh the only first residual block
of the of the among the
n will will decrease the spatial size
and doubling the channels and
the other n n minus one residual blocks
will
uh will uh have the same
input and output channels and there is
no need to use the down sampling
with the m minus one residual blocks in
this component
and
first conversation why it has big
kernel size as seven by seven
we do not use the big kernel size
and seven by seven in this current
session because we
will try we will we will train the
resnet on the zip r10 which is
much more much much smaller than the
image in a
dataset size
okay
the overall i i think it
is better to see the overall structure
of the residual network which
in which we which is used the block
as a residual residue block
so now let me explain the detail of the
resonance class in the resonant class
we will firstly use the convolutional
the small convolutional neural networks
to
make the input
with a more increase the channel
increase channel size so we will use the
images which has rgb channel three three
rgb channels
and use a three by three convolutional
layer
and the and we expect the
output uh
output of this convolutional
block will be 16 so
the so we will now use the seven by
seven
size of the kernel with this uh c
percent data set
so okay and then as i mentioned
as i mentioned above with the the
structure over structure of the
uh resonant in our case the
uh there are three uh
layers of red layers which
uh which of which uh have
i have have uh and residual blocks
so so you can
uh so as i mentioned the first
layers with it with the n residual
blocks
should output the same size
of the same size of the output of this
first convolutional block so
you can easily fill this blank
and and then the
other layers with the n
residual block should output
with the downsized special dimension and
the double
size of the output channels so you can
easily
fill this blank and destroy the option
so i think the
questions from the photo
will be corresponding to the answer of
the get layers
method in the bestnet class so
the game layers should uh should use
the predefined blocks which a
which we in our case is
a residual blocks so we can
pass the original blocks to use the down
sample
or not down sample or not
with the stride option and also
in the second and the third uh
layer blocks cases uh we should
uh downsample the input dimensions and
double the
uh output channels so uh
we only care about the
first block of the layer list in this
case
and the other and -1
blocks will be used in
a very simple manner so you can
implement it here
especially uh to append and
add the uh blocks with the uh
in input channels output channels and
the stride
options to define the issue and
blocks and add it to the layer list to
make the sequential module
the questions from uh
registered right now is down something
used for
each uh residual block uh no
uh if uh
if the uh
the layers with the and residual blocks
should uh down sample the image
downsample the input
outputs and the double the
output channels then the only first
residual block
in this layer so should use the down
sample
option
this is the uh the adjuster of the
implementing the residual network
so uh i recommend
you to read the references i attached
the bottom of this uh
at the bottom of this practice notes so
you can
see the references
and it will be very helpful
let's try the question from yongsan
[Music]
yes uh the stride okay uh let me
show some examples let me show the
examples in here
okay
okay as we already covered in the
lecture note
they uh the three by three
size the filter with a strider one
option
uh then and the padding with the one
then we will
uh we can keep the input demand
input special dimension uh
input dimensions so in this case uh
in this case we will use the stride so
it means that if we use the strider one
with the padding one with the
uh three by three corner size then
we will keep the input size without down
sampling
and if we use the stride the size with
two then it means that we will use the
uh it will use the down sampling options
with the same corner and the padding
option
you can easily check with some practice
with your own
jupyter notebook and a simple
simple examples with a convolutional
layer
definition okay so
if you uh implement the
whole original network in this manner
then
you can you can easily
and you can easily define the rest net
with various capacity
and in our practice notes so we will
use the resnet 18 but resonance 32
is also okay to train the c part 10 data
set
and this is the other code that
this is the other code that
there is to compare the residual network
and other cnn architectures with without
the
visual connection options so we will
simply replace the residual
block with this black class
then we can simply
define the cnn a simple
cnn without the residual connection
so you can easily check the performance
comparison with the deeper
network size and the effectiveness of
the visual connections in this case
so the train code is
fully implemented so you can if you
uh implement the ratio the resnet
class and the residual block class in a
proper manner then
your code will work with this train code
so
you can easily check the lost class and
the performance of the simple
residual network visual network and the
convolutional convolution in your
network without the residual
connection so
since the training
takes uh much time uh
depends on your uh gpus
or other uh machine
options so i already trained with it
i already trained with this zipper 10
data sets
and the best resonance and the loss
uh plus will be like this and
the uh and if you see the tesla
performance of
a with above the 91
points then your residual network
well implemented and also
as i mentioned you can compare
the simple plain convolution neural
networks with our dual connections
uh on simplifying datasets and as you
can see
the orange lost line
the orange lost line is above uh
to the red the loss of the residual
network
which means that uh the original network
uh original that's uh perform
better than the other cnn architectures
without visual connections and
the questions from sojourn
i i mentioned
that uh details in
this task but as i
let me explain we usually
uh train the large dataset or a large
capacity model uh with the learning
rate of decay in many cases so the
losses the last plus
of the fall in this in this
time step uh uh is
we this is because we uh decay the
learning rate by a factor of uh
0.1 so the with the train with
the training with the smaller learning
rate
will might help to convert
more well more better
so the overall practice
uh is
uh this is all about the practice
session
so uh if there are is any uh
other questions about the implementation
detail or
some questions of the use in thai tours
then please feel free to
[Music]
please feel free to a question in the
classroom
or mail to me thank you for
listening
explanation and the practice session
oh yeah unfortunately we've we had to
rush
our material today because we had a lot
to cover
to begin with um
i hope that everybody can go through uh
the codes and actually get familiarized
with how the how the resonant is
implemented i mean
in reality when you want to use
resonance you will just call resident as
an api
because they're good because they were
they're already there like popular
uh popular confident networks called
content models like vgnet
yeah sorry so popular networks such as
vgn
and inception and res they're already
implemented in piper so you can just
call them
because they're all the architecture is
already there for you but it's
important i just want i think it's
important to know at least how they are
implemented so they if you want to do
more
more in-depth uh serious serious uh
deep learning research or ai research
sometimes you need to actually build
things from scratch
and without understanding how these
popular models are implemented
you are there are certain limitations to
what you can do
so if you have time i mean hopefully you
will find time to
go through the codes today provided by
hunter and actually try to understand
what
each line and each like segments are
doing so that you can at least
have a rough idea how you would
implement resonance from scratch if you
were
if you were to do so
no i'm not i'm not sharing any screen
yet right now um
actually give me a second so
but you can't hear my voice right
yes uh yeah i will upload the the
solution version of the codes on the
classroom
all right um so i might i'd like to
answer some questions
uh that was asked before in the
beginning of the class
so uh youngji yoon asked
does does bachelor batch norm occur
between input layer and the first hidden
layer also
uh probably not because input layer
doesn't change
it batch norm is there because when you
update your model you're in there's an
internal distribution shift but
your input which are the training
samples they don't they don't depend on
any learnable parameters they are
they are fixed values so there's no
reason to use batch norm between the
input layer and the first hidden layer
uh okay jegan kim asked if i don't
misremember
last class professor said in these days
we don't use bn why
uh that must that is yeah i guess that
was quite a bit of a strong claim
i'm not sure if people use bien
explicitly or this
i yeah and when when i read like papers
these days i don't
ex i don't see explicit mention of batch
normalization like
whether the authors use bachelor or not
maybe it's because it's so
uh it's
uh such a given thing like you know if
everybody uses vegetables then there's
no reason to mention that they use
bachelor or
yeah i'm not really sure how often
they're used
uh it's not a bad idea to throw them in
here and there
but i'm not but at least uh the models
that i've seen
i don't think they use that children
that much
uh yeah i'm not sure what why the what
the reason is though
i mean layer like there are different
ways to normalize it like use if you use
layer norm then you don't need to use
batch normal
they are kind of a like two different
flavors of normalization
uh so for example in transformer or bert
or gpt like all those you know
transformer variants they don't use
bachelor they use layer norm and
i mean there's a limit there a bit a bit
of a like a
downside of using batch norm is the
discrepancy between
the training time and test time as i
said so
it's probably better to not have the
discrepancy by using
layer norm instead of batch form and you
can still
achieve similar like you know
normalization effect
i'm not yeah they won't be the same like
layer norm and bachelor
are not the same thing and sometimes or
maybe most times match norms are more
efficient than the layer norms but
it's just that it's easier to use like
there's one last thing
to care about if you use batch storm you
need to care about the
running average and then saving them and
then reusing it in test time if you use
layer norm you don't have to think about
that this just makes your life easier
uh so that's that
thank you me
why do you keep the certain
keep certain dimensions keep certain
axes around like how why do you make
them survive like such as cnn
a c or the channel or n being the
the batch axis yeah it's as you said
uh there's a specific reason why they do
so so batch norm
is as the uh the name says you normalize
across the batch so you collapse
or you you you gather the statistics
across the
batch axis the n x's
uh so the c survives uh if you do layer
norm
then what you want to do is you want to
you don't want to be
you don't want to be affected by the
size of your batch
mini bet so you don't want to collect
your statistics across the batch axis
you want to you you want to
collect the statistics the muse and news
and standard instead the muse and sigma
you want to collect their their values
across the hidden dimensions
that's why the uh the n axis survives in
that case
and if you do instance normalization
then like
another combination of axis survives so
yeah they serve different purposes
all right so i think i answered all the
questions so far
either using text in chat box or just
you know verbally
how many people do we have our own we
have 70-ish people
i so let's see
are you is there 70 people here are you
guys here to
so that i could explain like the popular
architectures the vg
and deception that i just skipped over
or should just or
is there any reason that you are like
sticking around
or do you no not actually any or do you
have any questions or
let me just take a poll like would you
like me to go through the
popular architectures the vg and see uh
inception that
okay at least there's one one person all
right all right okay we can do that
yeah before i move before i start with
the vg then maybe i can just
give a little more detailed explanation
on resnet like why they're such a like
like an awesome architecture
okay so all right no question all right
moving on so
um the reason that uh
yeah it's a very it's a very a keen
observation by the authors of resident
that
what used to happen before resident is
that
like for example if you increase the
number of layers or that means
your your you're increasing the power of
your
model because more layers means more
parameters more nonlinear
operations and more power to the model
but it
didn't work it didn't even overfit so
the problem with
the over so there's training error
and then there's test error and if
the model was over fitting that means
your the more powerful model would have
lower training error
but higher test error but that's not the
case here
it is it is the case that the the
smaller
layer the the model with the weak weaker
expressibility or or uh the
less powerful model achieves le smaller
trainier
means like better print predict better
accuracy right so
weaker weaker model is doing better job
than stronger model and both trainier
and tester
which means that the stronger a stronger
model
or the deeper model is not overfitting
there's something wrong with the
optimization
so it's a yeah not
not caused by overfitting it's not not
the overfitting it's the optimization
problem
something is wrong some we're not just
optimizing the model
model up appropriately so we haven't
even got to the point where
powerful model overfits we haven't even
seen overfitting yet it's just
we can't we can't just handle how the
bigger model to get
lower training error so the problem lies
with the
with the optimization not not the
overfitting so that's why uh
the authors of resident came up with the
residual network you know to to make the
optimization process
easier for deeper models
right yeah it's just
exactly like this so if you if there's a
if you have one more complement layer or
if you just start adding more layers and
more layers more layers but
if they happen to be identity layers
then it might as well you might as well
just
don't have that layers right so but it's
the same
the same principle if you just have a
lot of layers that can learn to do
nothing nothing means just identity
mapping you just
you just you just carry the what what is
given to you to the next layer
so it's a carryover layer and if your
multiple layers can learn to
carry over simply then there's no
like there's no downside of having a lot
of layers
because if if you have too many layers
then they'll just learn to do nothing
right so that is the whole idea of
residual connection
so yeah identity mapping
is allowed by having a skip connection
and then
your your network can learn to do
nothing way easier than before because
these just needs to be zero instead of
instead of your instead of your uh
yeah without without the skip connection
here without the skip connection
if you want to carry over if your layer
needs to learn to carry over then the
whole thing must be
the same as x right but if there's a
skip connection
and you want to carry over then your
internal
your your learnable parameters or your
or your operation
needs to learn to be zero which is way
easier than just learning to be
x your input so that's the whole like
you know theory behind
why residual connection is such a great
great model
right yeah so it uh so theoretically yes
uh you
your your residual layer your fx which
is intermediate the
the up non-operate non-linear operation
between the skip connection
needs to learn the residual residual
being this
the difference between this and this
here the old the
it is just learning being like like like
the error
so to speak
yeah yeah i hope this
makes the the the theory or the
motivation behind resonant more clear
all right some questions i still have a
question about resin i do not understand
why we skip the layers before you not
using them why do we keep
in case we need them so we don't know
like we don't know
we don't know the ideal number of layers
to use
right because what is the ideal number
of layers to use for image in that
classification is it 250 or is it 125 or
is it 64 or is it maybe it's 5 000 we
don't know
so but we would like to use a lot of
layers
just in case you know just to be on the
safe side
and as long as they can learn to do
nothing
then there's we're not losing anything
it's just learning the difference
between input and output
yeah right so yes i think i
answered the andre's question and cheson
to maintain the signal strength of
arabic to maintain the signal strength
of
error break propagation
uh you i guess you could say that but
it's
maintain the signal strength uh signal
strength of error is i guess that
more pertains to batch normalization
rather than resonant resonant is just
making the job of each layers
easier because you're just learning the
difference between input and output
okay yeah i hope that's clear
yeah they're just bunch of details so as
i said
uh there's there's a repeatable repeat
repeated repeated component or
like residual blocks that you use in
each
like every like here and here and here
and as i said uh you double
i've eaten after each phase you double
the channel with stride two
especially with stride two
and uh next one is yeah so the full
architecture is stack of residual blocks
stack of residual blocks instead the
residual block being being this
this guy you stack them like one two
three
four five six like a lot of them and
then there's like
there's like a phase here like phase one
phase two phase n
and uh periodically means different
phases
periodically when you when you jump with
one jump from one phase to another you
double the number of
double number of channels or double
number filters and down sample by using
stride two
that's it
right and uh it finally there's yeah as
i
well i mean i thought i briefly talked
about the this sequence of
sequence of uh fully connected layers
after the convolutional layers
that used to be there in the alex net or
vgg net and
that's that's not there anymore there's
no there's no fully connected
layers like sequence of fc layers
there's just global average pulling as i
said there's
global average pulling here and then you
just have one
linear not one a fully connected layer
to
predict the class of your image images
so this image in the classification so
you need 1 000 classes
so that's all there is just one fc layer
that's it
yep and yeah depending on how many
blocks you use how many phases there are
there are different layers like you can
have resin 18 resin 3450 100
152. i think people use resin 50 a lot
as like a
you know like a backbone structure if
you if you need to do anything with an
image
like if you want to do image captioning
or style transfer or object detection or
segment or pixel segmentation whatever
if you want to do whatever
you usually people use uh resident 50.
and yeah this is the training details
like you use you
you still use batch norm here inside the
resnet and there's something called
xavier initialization it's like
initializing
technique like how you want to
initialize the the weight parameters the
major weight matrices
uh you you still get the gradient with
momentum this is also another
like optimal optimization trick there's
a lot of trick going on basically
interestingly no dropout is used so it's
just uh
you just have the batch memorization and
the resonant and no no dropout
yeah and it just swept the first play
all the challenges
in 2015 uh ils drc and coco competitions
first place first solution of like like
all first place like all first books and
the gap between the first place and the
second place was
was pretty pretty big so it's uh it was
a very
uh yeah it's a very phenomenal paper
yeah before we move on to that let's
just go back all the way to the vgg net
and
inception
some questions okay
then
it's up to you how like how many layers
you want to skip
by making a residual connection if you
want to skip two layers or three layers
it's all up to you
yeah it's a designer's choice it's hyper
parameter
it just so happens that the the original
author has just skipped two layers
but there's no reason you can skip three
layers or just one layer it's up to you
during backpropagation transcends
through the skip layer as well of course
now transcends i'm not sure what you
mean by transcendence but it's just
you know the error signal is
propagated through the skip connection
as well because you're adding
your input to the output so the the the
add
addition the plus sign that is making a
connection
so you the error signals go through that
connection as well
[Music]
from danielle uh would you please
explain why some people do flattening
before dropout and then they
dance the link
i'm not sure if i understand your
question what do you mean by flattening
before dropbox
like you mean the spatial information
the height and width
in your in your hidden convolute in your
hidden layer
yeah i'm not really sure what you mean
like i haven't
yeah at least i haven't covered that in
in the lecture
assuming that people this is all
assumption this is all just you know out
of my
out of my head assuming that people when
people want to use dropout
in confidence they want to do it in such
a way
they just you know destroy the spatial
information
flatten your hidden layer so
because in confidence i'm sorry in
conflict with
like there's multiple layers of
convolutional networks
the the feature the hidden feature
between the layers they look like a 4d
tensor so
i'm just going to draw it in a 3d manner
so this is your
this is your batch norm and this is your
batch axis
so different samples this is your
channel
and this is your no i'm sorry so what do
we have yeah
this is your height and width and maybe
some people want to collapse this into
this looking like this like just b here
and
no actually no b times i'm sorry not not
b i'm sorry not this
not this it should look like right like
like this so b here the b survives and
then
there's just one long horizontal axis c
times
h times w and then you do drop out on
top of this and maybe maybe that's what
some people do
i'm not sure if that's a common
technique though
all right moving on to yeah so it's
already what time is it now it's
12 20 all right let's just try to wrap
it up in
at 20 minutes maybe
all right so yeah so this is the history
of imaging a challenge
and prior to 2012 which is the uh the
birth of deep learning this this was all
like shallow shallow days i guess none
deep learning era
and then so two things
uh maybe three thing yeah
four four notable things i would say so
there's alex in it and there's vg
ginette
and then there's inception net and then
there's resident and after that it's
just more or less just you know
fine tuning and more more engineering uh
by the way human performance is 95
95 accuracy so resnet is already
super human uh zfnet
zeiler and ferguson this is also
mentioned from time to time but most
notable
uh notable uh popular popular networks
are
lxnet vgnet inceptionnet and resident
and lxnet is
it's a pretty complicated architecture
with a lot of learnable parameters this
is like the largest
largest model number of parameter wise
but it just performs
not performs worse than the more more
effective
efficient ones uh the the ones that come
later like 2014 like
vgnet uh inception at resnet they use
they use less number of parameters
compared to alex net but they achieve
way better performance so it's just
better architecture basically
so yeah nothing too much to talk about
alex and it's just complicated there's
just like two two streams here there's
you have an input input image and then
there's one stream that's going this way
and
and another stream going this way and
then you just you know mix and match
with a fully connected net fully
connected layers at the uh at the top
uh yeah first use of rail value and
there was a dropout already used
before the drop of paper was published
so it's a pretty
uh pioneering paper we'll move on to
vg's net
directly so vgg net is
deeper than alex alex and had used eight
layers region that used vgd 16
this is like the popular one vg16 used
16 layers
and instead of using like
big big convolutional
up cut big filters like 11 by 11
you know
alice's uses 11 by 11 filter which is
like gigantic by
today's standards and then there's five
by five but if you look at vgg it's all
three by three so three by three three
by three three by three three by c
and as i said if you stack up multiple
layers of 3x3 it's the it's the
equivalent effect of having a single
large filter and you just and
you still use less number of parameters
you use less number of parameters and
you uh do multiple you you just use one
more number of nonlinear operations so
it's just better to use
you know sequence of small filters
rather than just one gigantic filter
so that's what vgg net actually did so
it's a pretty
pretty standard practice you have two
components one pull two component one
pull two confident one pull
three command one pull three comma one
pull and then just you know sequence of
fully connected nets so it's a pretty
standard standard operation
and there's another variant variation
called vg19 which uses
a bit more layers so there's instead of
three and instead of three and three you
use four and four
that's it
uh yeah
it's yeah there's less number of tricky
engineering things that was used in lx
net
and just yeah it's a simpler
architecture less number of parameters
deeper
layers and better better performance
so all in all just a good architecture
and then after that comes google
networks inception net
so it's called inception ma it's called
inception net because
this is the first time i guess it could
be said this is the first time
they use a re repetition of a block
like in resnet you use you repeatedly
use
residual block like multiple times here
you have something called
inception module and you repeatedly use
this in multiple places of course
of course i mean just like in resident
even if you use multiple blocks of
residual residual blocks it doesn't mean
their weights are
their waistline their weight or
learnable parameters are tied
they're different they're different uh
they they have different like let me
just jump to resin
yeah just like right here
even if you use the same block all the
time like here here here it doesn't mean
you're using you're recycling the block
it doesn't mean that you use the same
weight parameters over and over and over
again it's just that the
architecture is the same the weights are
different
so this whole thing like here one using
here and here and here they are
different
differently initialized with different
learnable weight parameters
they're not the same thing okay it's
just that the architecture is the same
the shape the structure is the same
their weights are different
and in the same sense even before that
inception that came up with that idea
before that
so they have a something called
inception block or inception module and
you reuse this
if you look at the entire inception
network you reuse this
here here here here here here like you
just
stack these up all the way till the end
and they just have the same architecture
they just but they still have different
weight parameters
so it's a pretty deep network compared
to vgg net it uses 22 layers and only
5 million parameters which is 12 times
less than lx net 17
27 times less than vg16 so it's a very
efficient it's a efficient model
and of course there's no fc layer so if
you look at this at the top
there's uh let me see yeah i think there
there's only one average global average
pulling here
one global average pulling then you need
you need the
final fc layer for predicting your image
class so that's
all there is to it yeah so yeah
okay uh yo so that is why it's called
inception inception that you have the
inception module that is used
all over all over the place and it's the
same principle applies to
inception version one inception version
two inception version three and section
order four is all the same
it's just that the architecture is like
slightly different from one to another
so yeah yeah
yeah they are they're inside the entire
entire network so it's inception logic
just like you know uh
the movie inception i guess
yeah just similar to the resnet you
start with some
initial initial convolutional operations
in this case they used
uh yeah the resolution is not good
enough in resnet they started with seven
by seven continents with stride two
i think they use some some preliminary
some some
you know seed operations or st
they call it stem network some seed
operations or initial operations that
you massage your input
to to make it into a smaller smaller
feature size and then you start adding
start you know repeating the inception
modules over and over again
so yeah they are the stack inception
modules
and then there's the final classifier
output and you can see that the same
classifier output is
you know added like slid in like thrown
in from time to time between the layers
uh it's just to you know just to uh
uh give the error message more
efficiently like if
if if if you think about if there was no
such thing as this plug in this block
here then
the entire network must depend on the
error signals coming from this final
classification error or cross-entropy
error but
if you slide in the same architecture
here and here then they
the same the same kind of signal flows
from here as well
and here as well and here as well so you
get more signals basically it's just for
so it's a trick kind of a trick to add
in like uh like
intermediate intermediate output output
classifier classification
error signal from time to time
yeah uh
yeah i already talked about global
average polling
this is just you know you yeah you take
the entire you collapse the entire
spatial information into a single scalar
value so h and w
becomes one and one you just collapse
your entire channel into a scalar value
and if you have like yeah
if you have like 100 channels then you
just are you're left with 100 scalar
which is just a vector like 100
dimensional vector
yeah and this is auxiliary
classification outputs to inject
additional gradient at lower layers
yep uh
yeah this this is just saying like how
how you calculate the total number of
layers there's like the one
two three four five six seven eight
nine ten eleven yeah something like this
you calculate the
number of layers the important thing is
how they constructed the inception
module actually
they wanted to make it into an efficient
network it uses only 5 million primary
which is like 27 times
less than vg's net as i said so the
important thing is how they constructed
the
inception module to make it into to to
repeatedly use this
to make an efficient entire model right
so the way they did it is they used
heavily depended on one by one
convolution actually
so one by one convolution is used
everywhere there's one by one
convolution here
and uh i hope you guys remember one by
one convolution is basically contracting
the channel size for example um
if this is your
i'm gonna i'm gonna throw away the batch
batch axis so we're only talking about a
single image so there's
uh h and there's w and then there's and
then there's
channel right and the one by one d one
convolution
is applied so the three by three
convolution
is applied like this like like three
like three by three
if you do one by one by one convolution
that is only applied to a single pixel
in like a like a fiber like manner like
here
and then if you use one by one
convolution filter with maybe
let's say this was 64 and you use one by
one convolutional filter with
with channel size 32 then what happens
is
you retain the dimensional you retain
the
uh the spatial information it's just
that your channel
is halved like this your c becomes 32
with one by one conv with 32 channels
then you're you're left with 32 channels
only
while retaining h and w spatial
information so basically one by one
convolution is
like a channel contraction or channel
compression algorithm
so by compressing your channel a lot of
the time
you are left with way smaller feature
size
and then that's how you make a efficient
inception module
so there's basically four different
channels oh
actually yeah this is okay this is a
less less light okay that's good
let me just explain it in a little more
a little more detail
so given a previous layer's output so
previous layers out previous layers
activation which would be a 4d tensor
so there's like 40 tensor being this is
going to be your batch
axis this is going to be your channel
axis this is going to be your
h and w axis so you're given
you're modulated given this and a single
inception module there are four streams
basically
there's a straight one by one
convolution there's
and then so which is retaining the uh
which is retaining
uh actually let me erase erase the batch
access here
it's better to explain with uh uh
c h and w h
w and c so you're you're given this and
then
if the first string or first uh first
channel not channel is not a channel it
could be an nvidia so first
first stream retains h and w retains h
and w
like this so it is still h and w
it's just that your channel probably has
decreased into some smaller channel c
prime because you're you're doing one by
one convolution
another stream is using three by three
convolutions so there will be
some different it'll be a little bit
contracted because you're
actually i don't think they use padding
here did they use padding i don't think
they use padding here so
uh
another c prime and h prime and w prime
because you're using three by three
con filter with one by one convolution
so there's cc prime h
prime w prime five by five this is gonna
be um even more
even smaller even more smaller than the
three by three convolution
with another c prime so c prime h double
prime w double prime
and another another stream is what is a
three by three max pulling so
you do max pulling like this so three by
three
uh three by three max pulling so you you
uh
your contracting your
spatial feature into a a third like 33
is survives so this becomes even more
like
way smaller than before as well so this
is like h triple prime w triple prime
and c prime
so you have a lot of a lot of streams
you have like a stream that
retains the spatial information stream
that does three by three comp
stream that does five by five con three
a stream that does max pulling
and then after you obtain obtain the
output from
different streams you cut you
concatenate them
so you just put everything together into
a very like
one gigantic you know like some some you
know some
hidden hidden feature hidden feature or
hidden tensor
that is like gigantically long like this
because you want to concatenate them and
then you do you do some massage to even
contract them again into like even
smaller smaller feature size so that's
how you do a efficient
operation on a huge uh huge tensors
basically you
you heavily use the one by one
convolution a lot of times so that the
channel
axis is massively compressed and then
you do a three by three five by five
convolution on top of that
and then you return it to the original
original uh
feature feature size or channel size so
you do this again
again and again like repeat it here here
here here
a lot of times and then that's how you
uh retain
the feature size while doing a lot of
like convolutional operations
so that's the that's where the
efficiency of inception it comes from
okay so yeah uh yeah we just have one
final
one final interesting figure here
so right so yeah this is uh
the best okay so this is the best
accuracy so far uh so alex net
accuracy is pretty pork uh uh top one
accuracy so
like you're doing imaging classification
so a single image
for a single given image you can have
like multiple multiple predictions
like it could be a dog or a cat or
whatever and you just type one egg
i'm sorry top one accuracy means you
just take top one
strongest class from your softnext
output and if that's correct
then it is accurate so it's that's top
one accuracy
and else top one accuracy is below 55
percent
uh resonant 18 is about 70
resident 152 is about 70
i would say 7 ish 77
inception v4 the most recent inception
or that
is 80 and then there there's like an
interesting diagram between the the
comparison between accuracy
and the number of operations this or
base number of our operations or maybe
the size of the
neural network which is correlated so
uh vg net is this is the biggest one
of the g19 um oh i guess i may have
misinterpreted i said
alex net is the biggest one but that is
not true so it's the vgg net is the
largest one
it has the largest number of parameters
and the largest number of operations
required
compared to that the accuracy is not not
that impressive so it's
it's somewhere in the 70s right compared
to that inception v4 here
has a way less number of operations and
way less number of parameters
it's only 30 35 million and it has is
showing the
highest top one accuracy so there's
still like a lot of rooms to
to you know meddle with the structure of
your network
just because you know there's there's
resident and then just because there's
like a resident at 152
it doesn't mean all the problems are
solved you can still do a lot of
interesting stuff you can
try to compress your model like downsize
your model and still retain the accuracy
and or maybe you can go deeper you can
go make it more efficient or maybe
just you know try to achieve higher
accuracy there's a lot of different ways
you can research
and yeah i think this is a very
interesting interesting uh
figure
all right so i think yeah yeah exactly
20 minutes
exactly 20 more minutes i took all right
okay so i still think we've rushed a bit
but i hope that at least everybody
understands roughly what i wanted to uh
tell
what i wanted to convey so thank you
very much uh
are there any questions so far
foreign
uh yeah any more
okay so the next week is midterm week so
we will just
yeah we'll just see we'll meet meet
again
uh the week after that so good luck with
your midterms and have
good luck with your projects i'm not
honestly they're not not that difficult
so yeah just
take your time and just try to train it
as as
you know sufficiently as possible and
you'll you'll you'll be fine
the time just also doesn't so no class
next week so you can just focus on your
midterms for other classes so no
no class next week it'll just it is just
midterm week
we'll resume a week after that
all right thank you guys if you have any
questions please post them in class
see you the week after next week bye