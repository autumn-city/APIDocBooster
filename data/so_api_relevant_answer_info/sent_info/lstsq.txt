np.linalg.lstsq(m1, m2) finds x  such that m1(x) = m2, similar to solving Ax = b.
lstsq is going to have a tough time fitting to that column of zeros: any value of the corresponding parameter (presumably intercept) will do.
To fix the intercept to 0, if that's what you need to do, just send the x array, but make sure that it's the right shape for lstsq:
If you want to access the coefficients naturally, scipy's [lstsq (hyper-link)] might be more appropriate, which is an equivalent formulation.
The normal equation and lstsq give the same result (according to [numpy.allclose (hyper-link)] when using that function's default arguments):
Now the normal equation gives a different result than lstsq:
Taking a look at the source of numpy, in the file [linalg.py (hyper-link)], lstsq relies on LAPACK's zgelsd() for complex and dgelsd() for real.
Then you can have a look at [https://docs.cupy.dev/en/stable/reference/generated/cupy.linalg.lstsq.html (hyper-link)]
The docs for lstsq don't look very helpful.
numpy.linalg.lstsq() only solves a system of linear equations, and this problem is pretty definitely nonlinear (although there are techniques to linearize systems of equations, I think this is not what you want in this case).
numpy.linalg.lstsq on the other hand is for line fitting (linear least squares).
The equivalent of lstsq in Apache commons is [SimpleRegression (hyper-link)].
With lstsq you have to transform the problem to y = Ap, where A = [[x 1]] and p = [[m], [c]].
Here is the same example mentioned in [lstsq docs (hyper-link)] written for SimpleRegression:
To obtain the least-squares solution to the equation Mb = x as given by numpy.linalg.lstsq, you can also use [numpy.linalg.svd (hyper-link)], which calculates the singular-value decomposition M= U S V*.
But the problem is that your vector is shape M = 48002, and  [np.linalg.lstsq (hyper-link)] takes dimensions (a.shape=(M, N), b.shape=(M,).
If I read the source code right (Numpy 1.8.2, Scipy 0.14.1
),  numpy.linalg.lstsq() uses the LAPACK routine xGELSD and scipy.linalg.lstsq() usesxGELSS.
Scipy now uses xGELSD by default [https://docs.scipy.org/doc/scipy/reference/generated/scipy.linalg.lstsq.html (hyper-link)]
As of Numpy 1.13 and Scipy 0.19, both [scipy.linalg.lstsq() (hyper-link)] and [numpy.linalg.lstsq() (hyper-link)] call by default the same LAPACK code DSGELD (see [LAPACK documentation (hyper-link)]).
I reported the incorrect default of rcond (see above Section) in [numpy.linalg.lstsq() (hyper-link)] and the function now raises a FutureWarning in Numpy 1.14 (see [Future Changes (hyper-link)]).
The future behaviour will be identical both in [scipy.linalg.lstsq() (hyper-link)] and in [numpy.linalg.lstsq() (hyper-link)].
future) default in Numpy 1.14, one should call [numpy.linalg.lstsq() (hyper-link)] with an explicit rcond=None.
Give a try to [scipy.linalg.lstsq() (hyper-link)] using lapack_driver='gelsy'!
[numpy.linalg.lstsq() (hyper-link)] wraps LAPACK's [xGELSD() (hyper-link)], as shown in [umath_linalg.c.src (hyper-link)] on line 2841+.
scipy's [scipy.linalg.lstsq() (hyper-link)] wraps LAPACK's xGELSD(), [xGELSY() (hyper-link)] and [xGELSS() (hyper-link)]: the argument lapack_driver can be modifed to switch from one to another.
See details and further references in [The difference between C++ (LAPACK, sgels) and Python (Numpy, lstsq) results (hyper-link)] .
Your question is unclear, but I am guessing you mean to compute the equation Mx=b through scipy.linalg.lstsq(M,b) for different arrays (b0, b1, b2..).
but if it is under or over-determined, you need to use [lstsq (hyper-link)] as you did:
or simply store the pseudo-inverse M_pinv with [pinv (hyper-link)] (built on lstsq) or [pinv2 (hyper-link)] (built on SVD):
a mostly empirical analysis of looped-lstsq vs. one-step-embedded-lstsq
There is no way, given the assumptions / standard-form of [lstsq (hyper-link)] to embed this independence-assumption without introducing a lot of zeros!
lstsq is:
When you use numpy.linalg.lstsq, the error function being minimized is
On a separate note, I cannot test it right now, but when using numpy.linalg.lstsq, I you don't need to vstack a row of zeros, the following works as well:
The 0th dimension of arrayB must be the same as the 0th dimension of arrayA (ref: the official documentation of  [np.linalg.lstsq (hyper-link)]).
You are doing a very poor use of np.lstsq, since you are feeding it a precomputed 3x3 matrix, instead of letting it do the job.
In scipy/linalg/basic.py, there is line 1031 lstsq function.
the argument lapack_driver in lstsq is set to None.
torch.lstq(a, b) solves minX L2∥bX−a∥
while  np.linalg.lstsq(a, b) solves minX L2∥aX−b∥
And also the returned rank from numpy.lianalg.lstsq is rank of 1st parameters .
For the function linalg.lstsq  you can look at the examples in:
[https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.linalg.lstsq.html (hyper-link)]
~\pycharmprojects\jupyternotebooks\venv\lib\site-packages\sklearn\linear_model_base.py in fit(self, X, y, sample_weight)
545         else:
546             self.coef_, self.residues, self.rank, self.singular_ = 
--> 547                 linalg.lstsq(X, y)
548             self.coef_ = self.coef_.T
549
~\pycharmprojects\jupyternotebooks\venv\lib\site-packages\scipy\linalg\basic.py in lstsq(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)
1219                                                cond, False, False)
1220         if info > 0:
-> 1221             raise LinAlgError("SVD did not converge in Linear Least Squares")
1222         if info < 0:
1223             raise ValueError('illegal value in %d-th argument of internal %s'
