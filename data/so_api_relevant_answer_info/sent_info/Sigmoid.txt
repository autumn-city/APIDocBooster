expit is still slower than the python sigmoid function when called with a single value because it is a universal function written in C ( [http://docs.scipy.org/doc/numpy/reference/ufuncs.html (hyper-link)] ) and thus has a call overhead.
But when you really need performance, a common practice is to have a precomputed table of the the sigmoid function that hold in RAM, and trade some precision and memory for some speed (for example: [http://radimrehurek.com/2013/09/word2vec-in-python-part-two-optimizing/ (hyper-link)] )
Here's how you would implement the logistic sigmoid in a numerically stable way (as described [here (hyper-link)]):
In general, the multinomial logistic sigmoid is:
I feel many might be interested in free parameters to alter the shape of the sigmoid function.
Second for many applications you want to use a mirrored sigmoid function.
Tensorflow includes also a sigmoid function:
[https://www.tensorflow.org/versions/r1.2/api_docs/python/tf/sigmoid (hyper-link)]
A numerically stable version of the logistic sigmoid function.
Use the numpy package to allow your sigmoid function to parse vectors.
The above code is the logistic sigmoid function in python.
If I know that x = 0.467 ,
The sigmoid function, F(x) = 0.385.
you don't have to use the actual, exact sigmoid function in a neural network algorithm but can replace it with an approximated version that has similar properties but is faster the compute.
For example, you can use the "fast sigmoid" function
Using first terms of the series expansion for exp(x) won't help too much if the arguments to f(x) are not near zero, and you have the same problem with a series expansion of the sigmoid function if the arguments are "large".
That is, you precalculate the values of the sigmoid function for a given number of data points, and then do fast (linear) interpolation between them if you want.
The sigmoid function looks like:
Using Eureqa to search for approximations to sigmoid I found 1/(1 + 0.3678749025^x) approximates it.
And you could just get rid of the constants if you don't care about getting it between the range [0,1] like sigmoid.
compared to sigmoid function or similar activation functions, allow
  for faster and effective training of deep neural architectures on
  large and complex datasets.
Also you might use rough version of sigmoid (it differences not greater than 0.2% from original):
Optimization of RoughSigmoid function with using SSE:
Optimization of RoughSigmoid function with using AVX:
[Two graphs for a sigmoid {Blue: (0.5/(1+(x^2))), Yellow: (-0.5/(1+(x^2)))+1} (hyper-link)]
acts like a sigmoid now because y(1-y)=y' is more let say round than 1/(2 (1 + abs(x))^2)
acts more like to fast sigmoid;
You can use the output of your sigmoid function and pass it to your SigmoidDerivative function to be used as the f(x) in the following:
But, the sigmoid seems to only produce a 0.5 (i.e., a zero classification) if it is Sigmoid(0).
classifyVector will return 1 iff sigmoid produces more than 0.5.
Since squashing through sigmoid makes positive values become bigger than 0.5 and negatives less than 0.5 (while, as you stated - zero is transformed to 0.5).
You can check the derivative here : [Sigmoid (hyper-link)]
you need to use for loops to apply sigmoid function to each element of vector or matrix.
So as defined g and z are of the same size, the code below should return the Sigmoid function.
Sigmoid function g(z)=1/(1+e^(-z))
Please add following code in file name sigmoid.m
Turn 1 into a array with the dimension of z/g, then compute the sigmoid.
From generic bipolar sigmoid function:
You could redefine the sigmoid function like so
and the resulting sigmoid function 1/(1+D(37).exp()) for -37 gives
Another solution is to use another sigmoid function, different from the one you use, that approaches 1 more slowly than yours does.
Remember that such hard sigmoid has zero derivatives everywhere so you won't be able to train it with any sort of gradient based technique.
You are right, both Transforms.sigmoidDerivative(x) and Transforms.sigmoidDerivative(x, true) should give the same results, it's a bug in dl4j.
Sigmoid always gives a value in [0,1] you need to round the value that means fix a threshold if it is higher than threshold then 1 else 0.
What you see is an artifact of the range over which you are plotting the sigmoid.
Moreover, you are plotting only the sigmoid when you do plt.plot(s).
For Theano backend Keras uses T.nnet.hard_sigmoid, which is in turn [linearly approximated standard sigmoid (hyper-link)]:
For reference, the hard sigmoid function may be defined differently in different places.
σ is the “hard sigmoid” function: σ(x) = clip((x + 1)/2, 0, 1) =
  max(0, min(1, (x + 1)/2))
You use the probability p = σ(x) returned from the hard sigmoid function to set the parameter x to +1 with p probability, or -1 with probability 1-p.
The hard sigmoid is normally a piecewise linear approximation of the logistic sigmoid function.
Depending on what properties of the original sigmoid you want to keep, you can use a different approximation.
shouldn't be Sigmoid function.
Sigmoid function often used in the middle layers of a Neural net.
how is Keras distinguishing between the use of sigmoid in a binary classification problem, or a regression problem?
it should be outclass=Sigmoid as stated in the documentation:
[http://www.pybrain.org/docs/quickstart/network.html (hyper-link)]
Sigmoid produces output between 0 and 1.
If you are using the same loss function for both softmax and sigmoid then it won't work.
And if you have more than 2 classes I don't think sigmoid is what you are looking for.
I agree with comment by @blue-phoenox that you shouldn't use sigmoid with cross-entropy because the sum of probabilities of classes does not equal one.
But if you have reasons for using sigmoid, you can normalize your output by the sum of the vector elements to make it equal to 1:
To rescale the output of the sigmoid (which is in the range [0,1]) to [0,10] you could of course generically use sig.InputScale(u,0f,1f,0f,10f) but of course in this case the scaling is as easy as u*10f.
However, I do not understand why you speak about the inverse sigmoid.
On the other hand, if you were instead trying to invert the sigmoid inclusive the input scaling, then you need to scale back the output of the logit from [-4,4] to [0,10]: I.e.
it may be the case where you want the output of your network to have
  the sigmoid transformation and use
  tf.nn.sigmoid_cross_entropy_with_logits without having the loss apply
  the sigmoid again.
If I am not wrong you are trying to give the logits a value processed by a sigmoid activation.
It seems not reasonable put a value between 0 and 1 to the logits since the sigmoid of that value is always between 0.5 and sigmoid(1), which means for your negative labels the model tries to train them as 0 but the minimun value from the model is 0.5.
A [sigmoid function (hyper-link)] is any function which has certain properties which give it the characteristic s-shape.
A [Sigmoid Function (hyper-link)] doesn't have bounds, that means it accept from infinitely small to infinitely large values.
Your problem is that you are mixing up the definitions of softmax and sigmoid functions.
The [sigmoid (hyper-link)] function is another special case of logistic functions.
However, a sigmoid function is not ensuring that an input vector sums up to 1.0.
In neural networks, sigmoid functions are used frequently as an activation function for single neurons, while a sigmoid/softmax normalization function is rather used at the output layer, to ensure the whole layer adds up to 1.
You just mixed up the sigmoid function (for single neurons) versus the sigmoid/softmax normalization functions (for a whole layer).
Let's implement a sigmoid function:
Now we take a look at the results that s (sigmoid) and sn (normalized sigmoid) give:
In this case you have to differentiate between three things : a sigmoid function, a sigmoid function with softmax normalization and softmax function.
A sigmoid function is a real-valued function which is simpy given by an equation f(x) = 1 / (1 + exp(-x)).
A sigmoid with softmax normalization is used to deal with two important problems which may occur during using of sigmoid function.
Without normalizing data - both variables would be squashed to almost one by sigmoid transformation.
If you have a list of values that range between 15000 and 25000 then sigmoid is going to give you near 1.0 for all of those.
sigmoid squashes everything to fit between 0 and 1 so very large values like that are going to asymptotically approach 1.
The problem is that for numbers with very large magnitude (like yours), the derivative of sigmoid is effectively zero.
Thus, even after running gradient descent, you will stay in the region where the sigmoid function evaluates to ~1 on every example.
Normalizing your data to mean 1 and variance 0 before the sigmoid layer will put input into a range where the derivative of sigmoid is non-zero, and your gradient descent algorithm will actually be able to optimize.
In this example the input to the "SigmoidCrossEntropyLoss" layer is the output of a fully-connect layer.
However, if you examine carefully the "SigmoidCrossEntropyLoss" you'll notice that it includes a ["Sigmoid" layer inside (hyper-link)] -- to ensure stable gradient estimation.
Therefore, at test time, you should replace the "SigmoidCrossEntropyLoss" with a simple "Sigmoid" layer to output per-class predictions.
I suspect there might be an error in the labels, and I also suggest you use a softmax activation fuction instead of the sigmoid function in the final layer, although it will still work through your approach, binary classification problems must output one single node and loss must be binary cross entropy.
In this setup, it's proper to use softmax instead of sigmoid.
Right now, with the multi-label setup and sigmoid activation, you are independently predicting the probability of a sample being class1 and class2 simultaneously (aka, multi-label multi-class classification).
Sigmoids can be useful when building more biologically realistic networks by introducing noise or uncertainty.
Another but compeletely different use of sigmoids is for numerical continuation, i.e.
The [sigmoid or logistic function (hyper-link)] does not have this shortcoming and this explains its usefulness as an activation function within the field of neural networks.
The sigmoid activation function will result in outputs that are between 0 and 1.
However, if you are using sigmoid activation functions throughout your neural network (i.e.
Historically, the sigmoid activation function was used throughout neural networks as a way to introduce non-linearity so that a neural network could do more than approximate linear functions.
However, it was found that sigmoid activations suffer heavily from the vanishing gradients problem because the function is so flat far from 0.
However, it's much simpler to add the sigmoid operation to the Core ML model itself and let Core ML worry about it.
You can do this by adding the sigmoid operation to the original model before you convert it, or afterwards (see my book Core ML Survival Guide for instructions on how to do this).
p_hat = sigmoid(np.dot(x_range, b[1]), b[0])
For example, if x is in [-100, 100], the original sigmoid won't overflow while your customized sigmoid will.
If we use tanh we must scale them to [-1,1], in case of sigmoid [0,1].
The logistic function (which is the generalized form of the sigmoid) already serves as a threshold.
tf.cast("...", tf.float32) tf.keras.activations.sigmoid("...")
I overcame this issue by wrapping the sigmoid function with np.minimum & np.maximum:
The issue seems to be that when the input to your sigmoid implementation is negative, the argument to torch.exp becomes very large, causing an overflow.
Creating the sigmoid shape:
Creating the sigmoid curve shape with SVG itself is pretty simple and just needs one path element:
The [feGaussianBlur (hyper-link)] element blurs the source graphic (our sigmoid) by the specified standard deviation value and the [feOffset (hyper-link)] offsets the resulting image by the dx, dy values.
If it is not then you'd have to make sure that the container box of the text doesn't overlap onto the sigmoid shape's area.
See [this answer (hyper-link)] that outlines the difference between softmax and sigmoid functions in tensorflow.
This explains the use of sigmoid function before the cross-entropy: its goal is to squash the logit to [0, 1] interval.
The formula above still holds for multiple independent features, and that's exactly what [tf.nn.sigmoid_cross_entropy_with_logits (hyper-link)] computes:
you can understand differences between softmax and sigmoid cross entropy in following way:
for sigmoid cross entropy, it actually has multi independently binary probability distributions, each binary probability distribution can treated as two class probability distribution
but for sigmoid, it looks a little different for it has multi binary probability distribution
for each binary probability distribution, it is
So consider scaling your numbers from 0 to 1 or 0 to 3 so that the result of the sigmoid function is useful.
For example sigmoid(a/[100, 100_000]) gives:
The first one is that a sigmoid is always between 0 and 1, so it will have a hard time fitting with those very high values (consider adding an extra argument to your sigmoid function to multiply the result with);
And finally, with those x-values, you get extremely small exponents in your sigmoid function (smaller than exp(-1000)) which will likely cause problems at some point.
Note that defining an array in numpy is a bit different than in Octave, but the sigmoid expression is almost exactly the same.
Alternatively, you can use the [vectorized Sigmoid function (hyper-link)] expit that is available in scipy:
I am really not sure why would you do that but you can declare a custom layer as below to apply sigmoid to weights.
In the tutorial, the sigmoid is applied to the last layer:
Shouldn't we use this sigmoid_derivative(np.dot(self.layer1, self.weights2)) instead of this sigmoid_derivative(self.output)?
because here you are trying to take the derivative of the sigmoid when you have not yet applied it.
Some notation before I continue: loss is L, A is activation (aka sigmoid), Z means the net input, in other words, the result of W .
Before continuing, remember that the second layer's activation is A2 = sigmoid(W2 .
For clarity, we'll write A2 = sigmoid(Z2).
So if you compute dA2/dZ2, you get sigmoid_derivative(Z2), which is sigmoid_derivative(W2 .
A1) or sigmoid_derivative(np.dot(self.layer1, self.weights2)).
So it shouldn't be sigmoid_derivative(self.output) because output was activated by sigmoid.
It turned out that &( z(L) ) or output was used, just to accommodate to the way sigmoid_derivative was implemented.
Here is the code of the sigmoid_derivative:
The mathematical formula of the sigmoid_derivative can be written as: &' (x) = &(x) * (1-&(x))
So to get to the formula above, &(z) and not z was passed to sigmoid_derivative in order to return: &(z) * (1.0 - &(z))
It is wrong to apply Sigmoid on top of ReLU function.
So theoretically sigmoid gets input between -inf to inf.
As the range is -∞ to ∞, passing it to the sigmoid function will give a value from 0 to 1
In conclusion, wrapping a sigmoid on ReLU will return a value in range 0.5 to ∞.
And wrapping a sigmoid on top of a weighted ReLU will return a value in range -∞ to ∞
The output falls between 0 and 1 because of the nature of the [sigmoid function (hyper-link)], there is nothing that stops you from having non binary feature set.
Your sigmoid implementation looks fine.
A sigmoid function is not a probability density function (PDF), as it integrates to infinity.
Regarding your interpretation of the results, even though the sigmoid is not a PDF, given that its values lie in the interval [0,1], you can still interpret them as a confidence index.
The motivation for using the sigmoid function was historically physically motivated.
However, given the long history of the step activation and its plausible physical motivation, people were hesitant to abandon it fully, and hence approximated it by the sigmoid function, which shares many of its characteristics, but is differentiable around 0.
In these, using the unbounded actiation functions such as the ReLU would quickly lead to an explosion in results, and people still use the sigmoid and/or tanh in these cases.
You've defined sigmoid as a class method, hence you need to use the self keyword like so
This is the function of an element-wise sigmoid operation on your array x:
If k is constant (i.e., a hyperparameter), F.sigmoid(k * x) should just work.
But sigmoids are continuous approximations of binary threshold units and it should be similar.
Now replace all the perceptrons in the network by sigmoid neurons, and multiply the weights and biases by a positive constant c>0.
Show that in the limit as c→∞ the behaviour of this network of sigmoid neurons is exactly the same as the network of perceptrons.
As you multiply all of the weights by large values, the tiny difference between sigmoid units and threshold units gets smaller and smaller.
Very large inputs into a sigmoid always produce 0 or 1.
The formula formula for the derivative of the sigmoid function is given by s(x) * (1 - s(x)), where s is the sigmoid function.
The advantage of the sigmoid function is that its derivative is very easy to compute - it is in terms of the original function.
Assumes that x is already the output of the sigmoid function, and so it is not to be re-computed the second time.
popt has the fitted values of your defined sigmoid function (L ,x0, k and b).
Pass them back to sigmoid:
As the warning states, the exponential in your implementation of the sigmoid function is overflowing.
Instead of writing your sigmoid function in terms of exp, you can use [scipy.special.expit (hyper-link)].
Check that it returns the same as your sigmoid function in cases where it doesn't overflow:
See [Numpy Pure Functions for performance, caching (hyper-link)] for my answer to another question about the sigmoid function.
You are missing the Access Level Modifier of your "sigmoid" method,
just type public, private or protected in the first position of the declaration.
Although it seems like your model is overfitting when using sigmoid, so try techniques to overcome it like creating train/dev/test sets, reducing complexity of the model, dropouts, etc.
sigmoid(g(f(x))) cannot be written as a linear function.
The sigmoid might work.
The problem is, your output layer's activation is sigmoid but it should be softmax(because you are using sparse_categorical_crossentropy loss).
Sigmoid logistic function outputs values in range (0,1).
It is like that because of the fact that Output(1-Output) is a derivative of sigmoid function (simplified).
In general, this part is based on derivatives, you can try with different functions (from sigmoid) and then you have to use their derivatives too to get a proper learning rate.
As Kelu stated, that part of the equation is based on derivatives of your transfer function (in this case sigmoid).
F.bernoulli_nll's input argument should not be sigmoided, because the function internally contains sigmoid function.
Therefore, when feeding the hidden variable to F.bernoulli_nll, sigmoid=False is specified.
You should be using the derivative of the sigmoid function somewhere in your backpropagation code.
Sigmoid is one of the possible activation functions.
Sigmoid is a non-linear activation function widely used in Logistic Regression and Artificial Neural Networks.
In machine learning, if we tend to learn a relationship between some features and a binary feature then we use a sigmoid function at the output layer ( which produces the outputs ).
Sigmoid produces an activation based on its inputs ( from the previous layer ) which is then multiplied by the weights of the succeeding layer to produce further activations.
If a greater positive value is intercepted by Sigmoid, it gives a fully saturated firing of 1.
Some Particular problems with Sigmoid ( And its replacement with ReLU ):
Sigmoid suffers from the problem of Vanishing Gradient.
This problem was tackled by the use of ReLU which does not squash the inputs ( like the sigmoid ) and hence the Vanishing Gradient problem was solved.
But when we use Sigmoid function we can see that a data far from others won't effect the separator too much.
Does this apply only if we use sigmoid function as squashing function?
No, activation distribution obviously depends on the activation function, that's why, in particular, the initialization techniques are different for sigmoid and relu based neural networks.
But tanh is a scaled and shifted sigmoid:
So if the activations are normally distributed for the sigmoid activation, they will still be normally distributed for the tanh.
Normalization means putting values in [0, 1] range, therefore you should not be getting 1's as outputs from sigmoid anymore.
b) CNN with Sigmoid activation function -> accuracy ~ 0.98, loss ~ 0.06
Whereas with Sigmoid: 1 output neuron is sufficient for binary classification.
When doing binary classification, a sigmoid function is more suitable as it is simply computationally more effective compared to the more generalized softmax function (which is normally being used for multi-class prediction when you have K>2 classes).
Usually a non-linear and differentiable function as for instance the sigmoid function.
Many applications & research has been applied with the sigmoid function (see Bengio & Courville, 2016, p.67 ff.).
Historically it was common to use the sigmoid function, as it was a good function to depict a saturated neuron.
Today, especially in CNNs other activation functions, also only partially linear activation functions (like relu) is being preferred over sigmoid function.
There are many different functions, just to name some: sigmoid, tanh, relu, prelu, elu ,maxout, max, argmax, softmax etc.
Now let's only compare sigmoid, relu/maxout and softmax:
sigmoid:
can bee seen as a generalization of sigmoid function
Sigmoid doesn't have this problem and can output anything, that's why it trains.
I was not able to tell conclusively from the docs, but it seems to me that binary cross entropy expects 1 output neuron that's either 0 or 1 (where Sigmoid is the correct activation to use) whereas the categorical cross entropy expects one output neuron for each class, where Softmax makes sense.
You could use Sigmoid even for the multioutput case, but it's not common.
When you use deep network you can see as u go near to input layers, in the calculation of gradient, number of derivative of sigmoid increases.
The maximum value of derivative of sigmoid is 0.25, and that is not always the case, value of derivative of sigmoid can be near to like 0.001 or something, in that case when these small terms increases, Gradient is decreased drastically.
So, USE ReLU in hidden Layer instead of Sigmoid
As far as I know, when the class become 2, the softmax function will be the same as sigmoid, so yes they are related.
Addressing your question about the Sigmoids, it is possible to use it for multiclass predictions, but not recommended.
Sigmoids are activation functions of the form 1/(1+exp(-z)) where z is the scalar multiplication of the previous hidden layer (or inputs) and a row of the weights matrix, in addition to a bias (reminder: z=w_i .
To recap, we want an output layer with number of neurons equals to number of categories, and sigmoids are independent of each other, given the previous layer values.
But Sigmoids are not guaranteed to sum to 1, while softmax activation does.
Shortly, the derivative of the loss is (sigmoid(z)-y) .
sigmoid'(z) (error times the derivative), that makes this quantity small, even more when the sigmoid is closed to saturation.
To do this, and I'm assuming that you're doing this implementation for educational purposes, apply sigmoid to the outputs of the all convs, these way your conv values doesn't go up.
Normally, RELU activation gives better results with convolutional layers, however you can get good results with sigmoid too.
Sigmoid is just 1 / (1 + e**-x).
This is the inverse function of sigmoid, implementation is straightforward.
over there you can choose your x and y data and the function that you want to fit over them (you can enter custom equations such as sigmoid).
Did you also change the function in the training, or you just used the same training method and then changed the sigmoid to tanh?
Have a look at the graphs of sigmoid and tanh:
sigmoid: [http://www.wolframalpha.com/input/?i=plot+sigmoid%28x%29+for+x%3D%28-1%2C+1%29 (hyper-link)]
tanh: [http://www.wolframalpha.com/input/?i=plot+tanh%28x%29+for+x%3D%28-1%2C+1%29 (hyper-link)]
In the sigmoid, the x = 0.5 gets us roughly y = 0.62.
Try printing the sigmoid values for your data and see if there is any between 0.5 and 0.62.
The reason behind using the sigmoid function is that it is derived from probability and maximum likelihood.
If for example your sigmoid function is like:
Conversely if you want to use sigmoid as a class method than you'll need to add a @staticmethod decorator to it eg:
That's exactly where I would expect sigmoid to perform well.
You don't meet sigmoids in hidden layers in practice due to the vanishing gradient problem and some other issues with large networks, but it's hardly an issue for you.
Sigmoid + crossentropy can be used for multilabel classification (assume a picture with a dog and a cat, you want the model to return "dog and cat").
So, you can't really expect to have that behavior from sigmoid.
The sigmoid function introduces non-linearity in the network.
The reason why you would use a sigmoid as opposed to something else is that it is continuous and differentiable, its derivative is very fast to compute (as opposed to the derivative of tanh, which has similar properties) and has a limited range (from 0 to 1, exclusive)
Ref: [https://medium.com/analytics-vidhya/activation-functions-why-tanh-outperforms-logistic-sigmoid-3f26469ac0d1 (hyper-link)]
In the interval of (0, 1] if gradient is diminishing over time t, Then sigmoid gives better result.
The sigmoid allows you to have high probability for all of your classes, some of them, or none of them.
So in your case if the model is good the prediction will not differ alot when either using sigmoid or softmax, softmax forces the sum of prediction to be 1 sigmoid doesn't do that.
Sigmoid:
Things are different for the sigmoid function.
The feature of the sigmoid is to emphasize multiple values (yes, can be more than one, hence called "multi-label"), based on the threshold, and we use it for the multi-label classification problems.
In general cases, if you are dealing with multi-class clasification problems, you should use a Softmax because you are guaranted that the sum of probabilities of all clases will sum 1, by weighting them individually and computing the join distribution, whereas with a Sigmoid, you'd be predicting the probability of each class individually, but not necesarilly weighted.
In this case, a multivariate sigmoid can be simulated as follows:
This can be seen to appear like a sigmoid in three dimensions:
If you're converting from a softmax to a sigmoid function, it usually means you are converting from a multi classification problem into a binary classification problem.
You can simply use for a sigmoid output:
When you changed it to a sigmoid, you get a scalar of either 0 or 1, saying the action is yes or no.
As discussed in comments section, the real problem turned out to be using sigmoid itself, which is not suited for such cases.
In neural network, the sigmoid activation function at output layer somehow borrows logistic regression (in which standard sigmoid function is often used) in the classification problem.
Also note that sigmoid usually refers to the shape (and limits), [tanh is a also sigmoid function (hyper-link)].
If the output of your network is sigmoid(w.x), this forces the output into the interval [0-1].
Incidentally, a no-hidden-layers neural net with sigmoid output trained to minimise cross entropy loss is logistic regression.
Sigmoid function is another logistic function like tanh.
If the sigmoid function inputs are restricted to real and positive values, the output will be in the range of (0,1).
This makes sigmoid a great function for predicting a probability for something.
Therefore both the Sigmoid and the Tangh can be used here.
a single neuron, without any activation or with sigmoid on it is a linear model.
Instead Sigmoid function is a differentiable function and you can use back-propagation algorithm on them.
Instead when you want to use Sigmoid function, you have to use gradient-based algorithms.
In our case, because we used Sigmoid function, f'(Z) is Y(1-Y) for binary Sigmoid and 0.5(1-Y)(1+Y) for bipolar Sigmoid.
The following is differentiation of Sigmoid function.
The transfer function, or sigmoid function, converts values in to probabilities
from 0 to 1.
Sigmoid prime has a nice curve and converts values in range of 0 to 0.5.
If you want to have continuous output try not to use sigmoid activation when computing target value.
Your question is not entirely clear, but I assume you are asking: "Why don't we just use the Sigmoid function without having to calculate its derivative?
Activation function: as the name suggests, we are wanting to know if a given node is "on" or "off", for which the sigmoid function provides an easy way to turn continuous variables (X) into a range of {0,1}.
Read more here: [https://en.wikipedia.org/wiki/Sigmoid_function (hyper-link)]
You need the derivative of the activation function (in this case sigmoid) because your final output is only implicitly dependent of the weights.
That's why you need to apply the chain rule where the derivative of the sigmoid will appear.
The output of a sigmoid is a single float between 0. and 1.
Thanks to Canberk Baci's comment I managed to overcome sigmoid output discrepancy.
When they both are given as 1, sigmoid function works as it was stated in the docs and neural network can predict something but with errors of course.
If you want the difference between 1 and your sigmoid function, you could define a function with the simplified mathematical expression:
softmax() helps when you want a probability distribution, which sums up to 1. sigmoid is used when you want the output to be ranging from 0 to 1, but need not sum to 1.
This means you can have sigmoid as output to predict if this pixel belongs to this specific class, because sigmoid values are between 0 and 1 for each output class.
Sigmoid or softmax both can be used for binary (n=2) classification.
Sigmoid:
            S(x) = 1/  ( 1+ (  e^(-x) ))
Softmax is kind of Multi Class Sigmoid, but if you see the function of Softmax, the sum of all softmax units are supposed to be 1.
In sigmoid it’s not really necessary.
Digging deep, you can also use sigmoid for multi-class classification.
In case you use sigmoid for multi class classification, it’d be like a marginal distribution and a Bernoulli likelihood, p(y0/x) , p(y1/x) etc
The sigmoid and the softmax function have different purposes.
For a detailed explanation of when to use sigmoid vs. softmax in neural network design, you can look at this article: ["Classification: Sigmoid vs.
If you have a multi-label classification problem where there is more than one "right answer" (the outputs are NOT mutually exclusive) then you can use a sigmoid function on each raw output independently.
The sigmoid will allow you to have high probability for all of your classes, some of them, or none of them.
It's because your sigmoid is defined only on the output of the Deep model and the way the WideDeepModel combines the two model's outputs is by adding them (and your Wide linear model can have arbitrary output).
If you want to squeeze the output between 0 and 1, set it to sigmoid.
The sigmoid is used in the loss
Naive application of sigmoid/softmax and cross-entropy is numerically unstable.
This is due to the exp in the sigmoid and the log in the softmax.
If you need the actual probabilities elsewhere, you can still just apply sigmoid/softmax at those points.
The very simple explanation is it's usage in output: sigmoid is used basically for binary classification by treating values from 0 to 1 as probability of primary class, and linear is used for regression problems.
(Keep in mind, the data is not well-separated in this case, so the curve doen't have a traditional look with classes separated at the 0.50 point on the sigmoid curve.)
Usually in classification problems (normally you use a sigmoid neuron for binary classification) the squared error is not really a good cost function.
The probability is given by the sigmoid function,
EDIT: It turns out the Sigmoid Function has its own [Maclaurin series approximation (hyper-link)]:
If that's the case then the answer is in theReLu function itself, what ReLu does is it doesn't scale the network output between [0,1] but instead returns max(0, x) which you don't want as your output/ground truth is scaled between [0, 1] and sigmoid does scale the network output between [0, 1] which is as per your ground-truth.
To understand better, consider you what the final layer of your network to return probability between [0, 1] which is achieved by using sigmoid but cant be achieved by ReLu due to the function definition.
So to compute loss your ground-truth and your network output should be in the same range which is achieved by sigmoid and hence model converges for sigmoid in your case.
Sigmoid activation worked but relu did not, everything else being the same.
If you plug this x into your sigmoid function you get:
The derivative of the sigmoid, s'(x) is:
Sigmoid function's value is in the range [0;1], 0.5 is taken as a threshold, if h(theta) < 0.5 we assume that it's value is 0, if h(theta) >= 0.5 then it's 1.
Sigmoid curve itself partially can act as a threshold.
The threshold could be considered to be the point where the sigmoid function is 0.5.
Some sigmoid functions will have this at 0, while some will have it set to a different 'threshold'.
The step function may be thought of as a version of the sigmoid function that has the steepness set to infinity.
There is an obvious threshold in this case, and for less steep sigmoid functions, the threshold could be considered to be where the function's value is 0.5, or the point of maximum steepness.
Idea of stable sigmoid comes from the fact that:
Use numpy's masking we can transform only the part of array which is positive or negative with specific sigmoid implementations.
In general, there's no point in additional sigmoid activation just before the softmax output layer.
Since the sigmoid function is [a partial case of softmax (hyper-link)], it will just squash the values into [0, 1] interval two times in a row, which would give be a nearly uniform output distribution.
By the way, if you chose not to use ReLu, tanh is by all means [a better activation function (hyper-link)] than sigmoid.
You can see the definition of the [sigmoid function (hyper-link)].
You'll need to set some threshold for the sigmoidal output.
If unluckily labels can't be divided, then you may need to check and change the loss function: for softmax it's usually cross entropy, which does not work well for sigmoid.
It corresponds to wX+b, not sigmoid(WX+b).
Why the sigmoid is not included?
well, in that case it'd be weird to call the resultant module Linear, since the purpose of the sigmoid is to "break" the linearity: the sigmoid is a non-linear function;
having a separate Linear module makes it possible to combine Linear with many activation functions other than the sigmoid, like the ReLU.
If the course says that a sigmoid is included in a "linear layer", that's a mistake (and I'd suggest you to change course).
Print out and see whether X and theta in your sigmoidFunction have values.
If you see the setActivationFunction() and calc_activ_func() in [ann_mlp.cpp (hyper-link)] the sigmoid returns value within [-1.7159, 1.7159] output when you set fparam1, fparam2 to 0, 0.
The functions is called symmetric sigmoid,but it actually compute tanh.
If you want the real sigmoid function, I think you need to implement it.
Sigmoid function is squashing its input to interval (0, 1).
I would advise you not to use sigmoid function as an activation function in your hidden layers.
The detailed explaination (as well as some useful tips if you want to keep sigmoid as your activation) might be found [here (hyper-link)].
There is no such thing as tanh is better when labels are {-1,1} and sigmoid is better when they are {0,1}.
tanh(x) maps the input to the interval [-1, 1] and sigmoid(x) maps the input to the interval [0, 1].
(in case of sigmoid)
Actually you can replace sigmoid function with any mathematical function turns a number into the range of 0 to 1.
If you change the sigmoid to tanh, that regression would no longer be a "logistic regression" because you are not modelling a probability.
Indeed, in the second case the backpropagation doesn't go through the sigmoid.
Explaining myself with a simple case:
you have labels in a binary form say a tensor [0, 0, 1, 0]
If your sigmoid is inside your custom loss function, you might have outputs that look like this [-100, 0, 20, 100], the sigmoid in your loss will transform this into something looking approximately like tihs :[0, 0.5, 1, 1]
The error that will be backpropagated will then be [0, -0.5, 0, -1].
The backpropagation will not take into account the sigmoid and you will apply this error directly to the output.
To summarize, the sigmoid must be in the network so that the backpropagation takes it into account when backpropagating the error.
In that case, you are using a step function; you could replace the whole conditional with your sigmoid function.
Maybe start with a single-layer network with one or 2 units to test out (sigmoid, SGD) first and then gradually increase the network complexity.
You can try using Functional API to create a model with n outputs where each output is activated with sigmoid.
You can just use 'sigmoid' activation for the last layer:
The sigmoid is applied to every class separately.
Please notice in case of softmax only one class should be the answer (as pointed out in another answer) and this approach (and mention of sigmoid) may indicate you are after multilabel classification.
If you want to train your network so it can simultaneously predict classes you should use sigmoid and change your loss to torch.nn.BCEWithLogitsLoss.
Value of l is the cost calculated using tf.sigmoid and the value of l2 is the cost (cost2) calculated using your custom sigmoid function and the values of l and l2 are almost the same for me.
Pretty [simple sigmoid (hyper-link)] with a clamp using the logistic function.
As mentioned in the answer by Jim J, sigmoid forces the output to the range [0, 1].
If you remove the sigmoid, the NN will have to learn that all the outputs should be in the range [0, 1].
The sigmoid might help making the learning process more stable.
You would use a sigmoid loss function, if the problem you are trying to solve involves the possibility of multiple values being "true", e.g.
The [sigmoid function (hyper-link)] is good for representing a probability.
For network layers that are not output layers, you could also use the sigmoid.
However, there are practical reasons not to use the sigmoid.
Sigmoid requires a fair amount of computation.
The slope of the sigmoid function is very shallow when the input is
far from zero, which slows gradient descent learning down.
Modern neural networks have many layers, and if you have several
layers in a neural network with sigmoid functions between them, it's
quite possible to end up with a zero learning rate.
The [ReLU function (hyper-link)] solves many of sigmoid's problems.
The derivative of the sigmoid function should only return near 0 for very large values, like x>5 or x<-5.
My calculator shows the derivative of the sigmoid function being ~0.1966 for an input of 1.
The sigmoid function may not be properly implemented
Where phi1 is the upper bound, phi2 is the midpoint of the sigmoid curve, and phi3 is the rate.
Here sigfunc is just an example for a sigmoid function, and A is the vector of the fitting coefficients.
I would suggest you use MATLAB's [Global Optimization Toolbox (hyper-link)], and in particular the [Genetic Algorithm Solver (hyper-link)], which you can use for your problem by optimizing (= finding the best fit for your data) the sigmoid function's parameters through genetic algorithm.
A sigmoid is not a specific function.
But all manner of curves can have [sigmoidal shapes (hyper-link)].
For example, the [error function (hyper-link)] (erf) has a sigmoidal shape and shows up in the [CDF (hyper-link)] of the [normal distribution (hyper-link)].
If you end up needing to write a custom fitting function - say, for performance reasons - I'd investigate MLE techniques for the particular form of sigmoid that you'd like to fit.
When we are using Sigmoid Function the sum of the results will not sum to 1.There are chances that sum of results of the classes will be less than 1 or in some cases it will be greater than 1.
Your question is too broad and there are lots of concept behind ReLU vs sigmoid.
But in short: 
Sigmoid Saturate and kill gradients (look at [Gradient descent (hyper-link)]) sigmoid are not zero centered because output of sigmoid is 0<output<1.
I can see for sigmoid you are using 
scipy but for ReLU its easy.
How sigmoid kills gradients and why they slow converge.
Q1 = How do I convert Sigmoid to ReLU and Q2 = how to add Bias to my code?
Q3 = Also, If I change Sigmoid to ReLU, do I have to make my dataset 0.0~1.0 range?
This is because Sigmoid function accepts 0.0~1.0 range of data, but I don't know what range ReLU allows.
So no such range like in sigmoid.
ReLU vs sigmoid vs TanH [Video (hyper-link)]
Here, for the binary_crossentropy and the sigmoid, you need 1 instead of 2 neurons.
model.add(Dense(1, activation='sigmoid'))
Of course, you need to make sure you provide the data in the right format (sigmoid and BCE [0,1,1,1,...] instead of softmax + CCE [[0,1],[1,0],[1,0],[1,0],...].
For possible Sigmoid functions, check here (tanh is not the only possibility):
[http://en.wikipedia.org/wiki/Sigmoid_function (hyper-link)]
typically people don't want [-inf...+inf] as the range of the input values, and don't want [-1...+1] as the range of output values -- therefore you might need a different sigmoid function!
you need to take the expected range of input values, and the expected range of output values, and use those to shift the actual sigmoid function, the weight-ranges and the value of the threshhold.
btw, I hand-derived a python function for the plain fast sigmoid:
A sigmoid gate outputs a value between 0 and 1.
The issue is that your sigmoid function is implemented in such a way that the automatically determined gradient is not stable for large negative values of x:
One heavy-weight option would be to use a [custom derivative rule (hyper-link)] to tell autodiff how to handle the sigmoid function in a more stable way.
In this case, though, an easier option is to re-express the sigmoid function in terms of something that is better behaved under autodiff transformations.
For your function, this means that the denominator of the sigmoid will very close to zero, so the function values blow up.
So I guess you have already read in the other question: sigmoid/tanh functions have a fixed output range.
For sigmoid, this is (0,1), while for tanh, it's (-1,1).
Sometimes people use the softmax and sigmoid interchangeably.
However, in this case it is indeed a sigmoid function because of binary class problem.
We want the predicted value sigmoid(weights * features) to be close to the true response (0 or 1) for all of the data points, but there may not be a way to set the parameters of the model to achieve this.
In logistic regression, you model probabilities using a logistic function (also known as a sigmoid function):
your function sigmoid is not dependent on the input z since it is used only in the line g = zeros(size(z)); and g is re-assigned again at the end of the function.
Sigmoid activation allows for a smooth curve of real values numbers from [0,1].
But the sigmoid activated neurons give you that spectrum of [0,1]
Strictly speaking, you don't need a sigmoid activation function.
As an alternative to the sigmoid, you could instead use a [hyperbolic (hyper-link)] tangent function.
The backpropagation learning rule relies on the fact that the sigmoid function is differentiable, which makes it possible to characterize the rate of change in the output layer error with respect to a change in a particular weight (even if the weight is multiple layers away from the output).
Note that as the k parameter of the sigmoid tends toward infinity, the sigmoid approaches the step function, which is the activation function used in the basic perceptron.
The sigmoid saturates at 1 and 0, when x->+/- inf, sigmoid -> 1/0 and d(sigmoid)/dx -> 0 and therefore depending on your data this might cause slower or "worse" learning.
The choice of the sigmoid function is by no means arbitrary.
When you take the derivative via the chain rule, you need to multiply by the derivative of the neuron's activation function (which happens to be a sigmoid)
The total error from the next layer l1_error is multiplied by the derivative of the current layer (here I consider a sigmoid a separate layer to simplify backpropagation flow).
What they mean here is [probabilistic interpretation of the sigmoid function (hyper-link)].
Sigmoid (or in general [softmax (hyper-link)]) is very often the last layer in classification problems: sigmoid outputs a value between [0, 1], which can be seen as a probability or confidence of class 0 or class 1.
In this interpretation, sigmoid=0.001 is high confidence of class 0, which corresponds to small gradient and small update to the network, sigmoid=0.999 is high confidence of class 1 and sigmoid=0.499 is low confidence of any class.
Note that in your example, sigmoid is the last layer, so you can look at this network as doing binary classification, hence the interpretation above makes sense.
If you consider a sigmoid activation in the hidden layers, confidence interpretation is more questionable (though one can ask, how confident a particular neuron is).
The big success of neural networks over the last several years is, at least partially, due to use of [ReLu instead of sigmoid (hyper-link)] in the hidden layers, exactly because it's better not to saturate the gradient.
If the sigmoid functions gives you a HIGH or LOW value(Pretty good confidence), the derivative of that value is LOW.
First of all, you got the sigmoid function wrong.
Here's a link that would help you understand better: [Derivative of the Sigmoid function
 (hyper-link)]
This is the equivalent of y = c + e^(-x*p), which is not a sigmoid, it's an exponential plus constant.
If you then still use a sigmoid, you'll be unable to make correct predictions (which should lie outside [0, 1] if scaled according to bounds determined by training data only).
"Why is using tanh definition of logistic sigmoid faster than scipy's expit?"
You want to fit a sigmoid, or actually a [logistic function (hyper-link)].
Here's the code that defines that sigmoid function and utilizes the scipy.optimize.curve_fit function to minimize the error by tuning the parameters.
The problem in the line that calculates the loss using sigmoid_cross_entropy_with_logits.
Unlike sparse_softmax_cross_entropy_with_logits, sigmoid_cross_entropy_with_logits expects the logits tensor and the labels tensor to have the same shape and type.
I think you should encode the labels into a BatchSize x 600 shape using the one hot encoding if your label values are 1-600 (or 0 -599) before using sigmoid.
However, I beleive using softmax will give you better results than sigmoid for multiclass classification.
BCEWithLogitsLoss combines sigmoid with BCE loss, thus if there is sigmoid applied on the last layer, you can directly use BCE.
The soft-max cross-entropy cost and the square loss cost of a sigmoid are completely different cost functions.
The softmax layer with cross-entropy cost is usually preferred over sigmoids with l2-loss.
Softmax with cross-entropy has its own pros, such as a stronger gradient of the output layer and normalization to probability vector, whereas the derivatives of the sigmoids with l2-loss are weaker.
While z evaluates to zero, sigmoid(z), and it's derivative, are non-zero (0.5, 0.25 respectively) at z=0.
"For a machine learning algorithm using numpy and has a sigmoid function.." Some aspects of your question are not clear.
You're never using the parameter p0 you're passing to your sigmoid function.
You should first rewrite your sigmoid function like this:
This means your model (the sigmoid) has only two degrees of freedom.
To get the slope of this function at any point, to be honest, I would just calculate the derivative symbolically as the sigmoid is not such a hard function.
But, because you ask specifically for the midpoint, so when x==x0 and y==.5, you'll see (from the sigmoid_derivative) that the derivative there is just -k, which can be observed immediately from the curve_fit output you've already obtained.
Getting from there to a logistic function requires a bit more work: you need to normalize target_vector so that the values lie in [0, 1], then apply scipy.special.logit (which turns a sigmoid on [0, 1] into a straight line), and then you can find the line of best fit to that.
As you can see it is a fair adjustment, but I guess if I change the definition of Y in sigmoid function, by adding a C multipliying the first 1, probably I would get a better adjustment.
Is any of those 6 values the parameters that characterize the sigmoid curve?
Both tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(...)) and tf.losses.sigmoid_cross_entropy(...) (with default arguments) are computing the same thing.
If you have a 2 class problem, output only 1 channel, use a sigmoid function (outputs values between 0 and 1).
The sigmoid function is simply
Sigmoid gives you values in the range [0, 1], which are the probabilities.
torch.round(torch.sigmoid(pred))).
Isn't it better to use the sigmoid once after the last layer within the network rather using a softmax and a sigmoid at 2 different places given it's a binary classification?
BCEWithLogitsLoss applies Sigmoid not Softmax, there is no Softmax involved at all.
This loss combines a Sigmoid layer and the BCELoss in one single class.
This version is more numerically stable than using a plain Sigmoid followed by a BCELoss as, by combining the operations into one layer, we take advantage of the log-sum-exp trick for numerical stability.
By not applying Sigmoid in the model you get a more numerically stable version of the binary cross-entropy, but that means you have to apply the Sigmoid manually if you want to make an actual prediction outside of training.
The only difference between tanh and sigmoid is scaling and offset.
Here is a simple sequential model using tanh and sigmoid activation layers
You are missing a self argument for the sigmoid function.
def sigmoid(z): -> def sigmoid(self, z):.
This self.sigmoid(self.z3) is effectively calling sigmoid with self as the first parameter and self.z3 as the second.
Yes, all sigmoid layers will suffer from slowing down learning except last one.
I guess your derivation is correct, actually Quadratic Error, Sigmoid + BinaryCrossEntropyLoss and Softmax + SoftmaxCrossEntropyLoss share same form of backpropagation formula y_i - y.
your regression target is of range (0,100), but you use sigmoid as the activation function of the last dense layer).
A2: First, using the sigmoid loss function means to train 10 binary classifiers, one for each class (i.e.
The only difference between sigmoid and softmax is that the sum of the class-wise predicted probability is always 1 for the softmax network, while may not necessarily to be 1 for the sigmoid network.
In other words, you might have confusions to decide a label during testing for the sigmoid network.
Regarding to why sigmoid is better than softmax, it is related to many aspects and difficult to analyze without careful studies.
One possible explanation is that sigmoid treats rows in the weight matrix of the last dense layer independently, while softmax treats them dependently.
Therefore, sigmoid may better handle those samples with contradicting gradient directions.
Finally, if you believe sigmoid version gives you better performance but you still want a softmax network, you may reuse all the layers until the last dense layer in the sigmoid network and finetune a new softmax layer, or use both losses just as in a multi-task problem.
I had already implemented stochastic gradient descent, so the only difference i had to make, to implement Logistic Regression, is to update the hypothesis function through a sigmoid function in for loops and change the order as long as the sign in the update thetas rule.
If you use sigmoid function, then you can only do binary classification.
The reason for this is because sigmoid function always returns a value in the range between 0 and 1.
Why do tensorflow recompute the Sigmoid's output for its gradient?
(hyper-link)] So the sigmoid node in the gradient will automagically reuse the sigmoid computation.
Derivative of sigmoid can be calculated in terms of sigmoid:
Because 0 is the center of the sigmoid curve.
Scaling a by a constant factor will adjust the slope of the sigmoid function.
The [difference between 1 and the exact value of sigmoid(21.2955703735) (hyper-link)] is on the order of 5e-10, which is significantly less than [machine epsilon (hyper-link)] for float32 (which is about 1.19e-7).
Keep in mind that even with 64-bit floating point computation this is only accurate to about 6 digits past the last 9 (and will be even less precise for larger sigmoid inputs).
In this case 1 - sigmoid(x) which is equivalent to 1 / (1 + exp(x)) or sigmoid(-x).
The sigmoid function is just one of many useful transformation functions.
x and sigmoid are overwritten every iteration and only the last ones are returned
The problem was that the tf.nn.sigmoid_cross_entropy_with_logits runs the logits through a sigmoid which is of course not used at validation time since the loss operation is only called during train time.
make sure to run the network outputs through a tf.nn.sigmoid at validation/test time like this:
The error is likely to be thrown after the loss function, because the only significant difference between tf.losses.sigmoid_cross_entropy and tf.nn.weighted_cross_entropy_with_logits is the shape of the returned tensor.
But it's important that weighted_loss and sigmoid_loss are different.
This is because tf.losses.sigmoid_cross_entropy performs reduction (the sum by default).
This is using SymPy's exp in the sigmoid function.
If you literally want sigmoid(1) and so on inside the function, then sigmoid needs to be a SymPy function that does not evaluate anything.
Sigmoid function will do an element wise operation, so each value in array will be 1/1+e^-(value)
You can't use sigmoid for multi-label output.
To import expit with the name sigmoid, use
YOLO uses a sigmoid activation function for x,y
However, if you are using a softmax or sigmoid in the final layer in the network, you do not need from_logits=True.
Softmax and sigmoid output normalized values between [0, 1], which are considered probabilities in this context.
Just expanding on my comment, here is a comparison between your sigmoid through vectorize and using numpy directly:
As you can see, not only does vectorize make it much slower, the fact is that you can calculate 10000 sigmoids in 250 microseconds (that is, 25 nanoseconds for each).
The only way to optimize this that I can think of is writing a sigmoid [ufunc (hyper-link)] for numpy, which basically will implement the operation in C. That way, you won't have to do each operation in the sigmoid to the entire array, even though numpy does this really fast.
The sigmoid function is available as [scipy.special.expit (hyper-link)].
Compare expit to the vectorized sigmoid function:
The CDF of the logistic distribution is the sigmoid function.
You can use the pdf method to compute the derivative of the sigmoid function, or the _pdf method which has less overhead, but "rolling your own" is faster:
logistic._cdf is a bit more robust than my quick implementation of sigmoid_grad:
An implementation using sech**2 (1/cosh**2) is a bit slower than the above sigmoid_grad:
But for derivative of sigmoid the following can be used:
The sigmoid function is useful mainly because its derivative is easily computable in terms of its output; the derivative is f(x)*(1-f(x)).
Therefore, finding the derivative using a library based on the sigmoid function is not necessary as the mathematical derivative (above) is already known.
the outputs of sigmoid function) are clipped due to numerical stability when computing the loss function.
Let's compare keras binary_crossentropy, tensorflow tf.nn.sigmoid_cross_entropy_with_logits and custom loss function(eleminate vale clipping)
There is a built-in sigmoid activation, which is tf.nn.sigmoid.
Otherwise, with your custom sigmoid your predictions will be either 0 or 1 and there is no gradient available for this.
I think you are getting above error because of Y = tf.cast(Y_train, tf.float64) line inside sigmoid_cost function.
You're doing binary classification with your layer Dense(1, activation = "sigmoid").
With a sigmoid activation, your output is a single number between 0 and 1 which you can interpret as the probability of your first class.
However, you don't make predictions with argmax with a sigmoid activated output.
Your model has only one output however you have four classes, so you need to change the last Dense layer to model.add(Dense(4, activation="softmax")), sigmoid is usually for binary classification.
tf.sigmoid(logits) gives you the probabilities.
You can see in the documentation of [tf.nn.sigmoid_cross_entropy_with_logits (hyper-link)] that tf.sigmoid is the function that normalizes the logits to probabilities.
Please replace g=1/(m+exp(-z)); with g=1./(m+exp(-z)); in your method sigmoid
Bipolar sigmoid: never heard of it.
Sigmoid usually refers to the shape (and limits), so yes, tanh is a sigmoid function.
And yes, you could use any sigmoid function and probably do just fine.
Generally the most important differences are
a. smooth continuously differentiable like tanh and logistic vs step or truncated
b. competitive vs transfer
c. sigmoid vs radial
d. symmetric (-1,+1) vs asymmetric (0,1)
Radial (basis) functions are about distance from a typical prototype and good for convex circular regions about a neuron, while the sigmoid functions are about separating linearly and good for half spaces - and it will require many for good approximation to a convex region, with circular/spherical regions being worst for sigmoids and best for radials.
Bipolar sigmoid = (1-e^(-x))/(1 + e^(-x))
This gives you a numerically stable implementation of sigmoid which guarantees you never even call math.exp with a positive value:
then you can do your sigmoid evaluation and at the end you could restore the defaults
Consider that the array of sigmoid values may now contain inf entries, your code has to deal with this eventuality.
In a mathematical context you are correct that sigmoid should never reach 1.0.
In order to use the sigmoid approach you should make the sum of the incoming weights at each node approximately one.
This will make the output of the sigmoid reasonable and allow your weights to converge quicker.
and a sorted Python list of (sigmoid_value, at_argument), this function creates the if-else tree:
The benchmarks are based on the 127 * n / sqrt(n*n + 4194304) sigmoid function of my case, and they are over the input range [-8000000, 8000000].
Because the loss function tf.nn.sigmoid_cross_entropy_with_logits is applying the sigmoid [itself (hyper-link)].
Apply sigmoid function on the product.
sigmoid : applies sigmoid activation function to the argument.
The example illustrated in the figure you posted (rendered below) will be limited to model linear problems where the output value is between 0 and 1 (the range of the sigmoidal function).
However, the model would support non-linear datasets if the sigmoidal was applied to the two nodes in the middle.
Using a sigmoid activation on two outputs doesn't give you a probability distribution:
Instead of tf.nn.softmax, you could also use tf.sigmoid on a single logit, then set the other output to one minus that.
We can find sensible starting values if we take a closer look at the function sigmoid
Since start_val_sigmoid returns a list we can use its output directly as the start argument in nls
And in theano, one has to compute sigmoid/softmax manually and then apply cross-entropy loss function.
Tensorflow does everything in one fused op, but the API with sigmoid/softmax layer was already adopted by the community.
If you want to avoid unnecessary logit <-> probability conversions, call binary_crossentropy loss withfrom_logits=True and don't add the sigmoid layer.
In Keras by default we use activation sigmoid on the output layer and then use the keras binary_crossentropy loss function, independent of the backend implementation (Theano, Tensorflow or CNTK).
If you look more in depth for the pure Tensorflow case you find that the tensorflow backend binary_crossentropy function (which you pasted in your question) uses tf.nn.sigmoid_cross_entropy_with_logits.
The later function also add the sigmoid activation.
To avoid double sigmoid, the tensorflow backend binary_crossentropy, will by default (with from_logits=False) calculate the inverse sigmoid (logit(x)=log(x/1-x)) to get the output back into the raw state from the network with no activation.
The extra activation sigmoid, and inverse sigmoid calculation can be avoided by using no sigmoid activation function in your last layer, and then call the tensorflow backend binary_crossentropy with parameter from_logits=True (Or directly use  tf.nn.sigmoid_cross_entropy_with_logits)
Note that @TheInnerLight points out in the comments that for your specific sigmoid function you can also write:
You could change dsigmoid to:
Have tried re-defining the sigmoid function wherever you use it?
As others have mentioned when you defined the function dsigmoid, you have it calling itself again.
In return sigmoid(x) * (1 - dsigmoid(x)) if you want dsigmoid(first argument) to be subtracted from 1, use return sigmoid(x) * (1 - x).
However, to the best of my knowledge, this does NOT create any serious issues for most of practical applications (although, there are some cases where applying the softmax/sigmoid function inside the loss function, i.e.
In other words, if you are not concerned with precision of generated probability values with sensitivity of less than 1e-7, or a related convergence issue observed in your experiments, then you should not worry too much; just use the sigmoid and binary cross-entropy as before, i.e.
the sigmoid function should not be used on the last layer if from_logits=True).
(this isn't your actual question, but did you mean to use the sigmoid activation function?
Sigmoid is used in output layer for binary classification but MNIST is multi class classification problem.
As Dan stated, your costFunction calls sigmoid twice.
First, it performs the sigmoid function on X*theta; then it performs the sigmoid function again on the result of sigmoid(X*theta).
Thus, sh = sigmoid(sigmoid(X*theta)).
Your cost function should only call the sigmoid function once.
This causes the sigmoid function to only be called once.
Since 1 + exp(-40) is indistinguishable from 1, sigmoid(40) should return 1.
Since you are doing multilabel classification you should use sigmoid, as you mentioned yourself.
This means that if you want to use sigmoid, you cannot use from_logits because it would apply softmax after sigmoid which is generally not what you want.
I used GAM which overfits here but you could replace this with whatever model you want, including sigmoid.
This loss (very slightly) pushes the network to produce even higher logit for this case to get its sigmoid even closer to 1.
You want to use corresponding elements of the weight_tensor to weight individual log(sigmoid(...)) terms, not the final output of cross_entropy).
However, note that this is already happening to a degree because of the shape of log(sigmoid(...)).
It's worth spending some time thinking why you believe(d) that some weight in f(x) = sigmoid(x * [trainable weight]) would result in a function that distinguishes even numbers from odd numbers.
This will run the sigmoid function on each element of the Eigen matrix my_matrix, and then return another matrix as the result.
A perceptron-style neuron's output is sigmoid(sum(xi * wi)) where the bias input x0 is 1, but the weight is not necessarily 1.
You definitely don't sum the 1 outside the sigmoid, but you also don't sum it inside.
You don't need sigmoid in this case.
Use sigmoid activation and a regression loss such as mean_squared_error or mean_absolute_error
SUMMARY:  if unsure, go with binary_crossentropy + sigmoid.
If most your labels are 0's or 1's, or very close to, try mae + hard_sigmoid.
As far as activations go - hard sigmoid may work better, especially if many of your values are equal to or very close to 0 or 1, as it can equal 0 or 1 (or approach them) a lot quicker than sigmoid can, which should serve as a form of regularization, since it is a form of linearization (--> weight decay).
Binary crossentropy: should generally work the best (w/ sigmoid)
Just be sure not to use hard sigmoid, for self-evident reasons.
Sigmoid vs. Hard Sigmoid
Here's the source code of hard_sigmoid:
EDIT BY OP:
ultra_hard_sigmoid fails, if you look at the source code implementation, because it is hard-coded in python and not handled by tensor expressions.
activateFunc – Parameter specifying the activation function for each neuron: one of CvANN_MLP::IDENTITY, CvANN_MLP::SIGMOID_SYM, and CvANN_MLP::GAUSSIAN.
Is this what you mean by sigmoid: [How to calculate a logistic sigmoid function in Python?
Keep in mind that the sigmoid function squashes its input onto an output range of 0..1, so multiplying it with a weight re-scales it to your required output range.
If you are scaling your input image in the range [0, 1] and passed it to the discriminator model, then you should apply sigmoid function at the output of the generator model.
So from here we can conclude that if you are taking grayscale images as input use sigmoid function and if you are using colored images use tanh function.
Note that J(theta) is the same as your formula above and h(x) represents the sigmoid function.
With sigmoid(X * theta) you evaluate the sigmoid of each of those dot products.
With sigmoid(X * theta)-y)' * X you are transposing the vector of sigmoid evaluations and computing its dot product with each of the columns  of your data set (i.e.
ReLU replaced sigmoid in the hidden layers since it yields better results for general purpose applications, but it really depends in your case and other activation function might work better.
If your classes are heavily skewed, and you want to balance it at the calculation of loss, then you have to specify a tensor as weight, as described in the manual for [tf.losses.sigmoid_cross_entropy() (hyper-link)]:
Sigmoid and softmax are not equal, even for the 2 element case.
The sigmoid (i.e.
Hence, if you wish to use PyTorch's scalar sigmoid as a 2d Softmax function you must manually scale the input ([ (hyper-link)]), and take the complement for the second output:
simply change to prediction_fn=tf.sigmoid
sigmoid(1 * 1 + 1 * 0) = 0.73105857863, tanh(1 * 1 + 1 * 0) = 0.761594155956
Rt = Sigmoid(1 * 1 + 1 * 0.20482421) = 0.769381871687
Zt = Sigmoid(1 * 1 + 1 * 0.20482421) = 0.769381871687
The blue curve is sigmoid like, starting near 1, making a turn roughly for T between 400 and 500 and then stays close to 0.
If you set ip_layer = mlp.Layer('Sigmoid', units=343) it should work.
In the intermediate layers the sigmoid function works well, just like a ReLU or any other.
The sigmoid function is still required, as the backpropagation works on computing the derivative of the sigmoid function, and not whether or not the neuron fired.
For neural networks, you don't need the exact value of the sigmoid function.
This is the sigmoid function:[ (hyper-link)]
I can't recall the slope at the moment, but here's what I'm talking about using a bipolar sigmoid as an example.
However, you're trying to force T to be Sigmoid when activation is None.
the use of (32bit) floats would appear to be hard coded in the [compute_weighted_loss() (hyper-link)] function used by [sigmoid_cross_entropy (hyper-link)] in Tensorflow
If you switch the activation to sigmoid you should modify your loss function with from_logits=False accordingly, so that the loss function expects values in the range of [0,1]
It is possible to compute (less-accurate) versions of sigmoid functions faster than the (exact) hardware implementations.
Then use that value to scale the output of the Sigmoid function.
f(x) = Sigmoid(x) * MAX_ENERGY
and all the output values are around 0.5, it seems to follow from the definition of the sigmoid function that all its input values are very close to 0.
The last layer should always use sigmoid (in the binary case) regardless of what you are trying to do.
The sigmoid function is used to estimate the probabilities that an example is in a given class, the prediction of an example is the class which the example has the highest probability to be in.
-> Don't use softmax
Single elements of your targets are negative -> Don't use Softmax, ReLU, Sigmoid.
Because sigmoid is defined as
Given the data you provided, I'd say that the warning you get with the resulting covariance matrix is an indication that the sigmoid function is very bad at doing the job of fitting such data.
You can also plot the resulting sigmoid function on top of your data to see if the resulting fit is a good one.
As for your case you need to use relu as your activation function in the last layer (output layer) instead of sigmoid
The range of relu is [0,inf).Then in that case you need to use 'MSE' as your loss metric.
Sigmoid is a function, Matplotlib expects numerical values, i.e., the results of a function evaluation, e.g.
The sigmoid function that is used as an activation function gives you the final output (Actual Output).
So it seems you're experiencing underflow, meaning that the weights of your neurons scale your input vector x to values that will lead to zero values in the sigmoid function.
Also: Are you using sigmoid functions in your output-layer?
You can keep shared layers and 2 different outputs one with sigmoid another with linear activation.
but if step is lower than min_step it calls .SigmoidPredict which does not return A and B. I do not think that the solution is to decrease min_step, but not to call .SigmoidPredict, so I commented it out.
here's a repository based on the latest source from cran with the call to SigmoidPredict commented out.
Inputting the sigmoid function 1/(1+e^(-t)), we are given an explicit formula for the derivative, which matches yours.
In your case, I will assume you already have a function Sigmoid(value).
Taking 
Dapprox = (Sigmoid(value+epsilon) - Sigmoid(value)) / epsilon
for some small epsilon and comparing it to the output of your function DSigmoid(value) should catch all but the tiniest errors.
In case numerical stability is an issue, there is another possibility: provided that you have a good implementation of the sigmoid available (such as in [scipy (hyper-link)]) you can implement it as:
There are other ways of writing the sigmoid function that are more [numerically stable (hyper-link)]
You can transform your input into the log space and run sigmoid after, this would shrunken down large values significantly.
Next, use sigmoid to generate new y-values:  
[code snippet]
EDIT: As pointed out by @AGP, Tensorflow already offers an implementation: [tf.log_sigmoid() (hyper-link)].
Given the definition of the logit function (as inverse of the sigmoidal logistic function), it is rather straightforward to implement it yourself (c.f.
As sigmoid(x) = 1 / (1 + exp(-x)),
logit(y) = sigmoid(x)^-1 = log(y / (1 - p)) = -log( 1 / p - 1)
small values of n for x in [-n, n]) as sigmoid(x) converges quickly towards its asymptote limits:
tf.log_sigmoid() is not a logit function.
[Xavier Initialization (hyper-link)]: this makes your neurons avoiding not only harsh values as an input to your first layer sigmoids but also from having the same problem with a consequent layers.
Convolutional neural networks (like standard sigmoid neural networks) do suffer from the vanishing gradient problem.
However, it has been shown (like [this paper (hyper-link)]) for several tasks that using Rectified linear units alleviates the problem of vanishing gradients (as oppose to conventional sigmoid functions).
we do not use Sigmoid and Tanh as Activation functions which causes vanishing Gradient Problems.
Sigmoid derivative has a maximum slope of .25, which means that during the backward pass, you are multiplying gradients with values less than 1, and if you have more and more layers, you are multiplying it with values less than 1, making gradients smaller and smaller.
sigmoid is a method of the Neuralnetwork class, so you need to create an instance of the Neuralnetwork class first, before you can utilize the sigmoid function, if you're calling it after the class definition:
I notice that your sigmoid method does not use self at all, that is, it does not depend on the instance.
But if it is closely related to the class, you may prefer to enclose it as a static method, removing completely the self from the sigmoid def:
Or sigmoid or tanh?
For example, a sigmoid works well for a classifier ( see the graph of
  sigmoid, doesn’t it show the properties of an ideal classifier? )
because approximating a classifier function as combinations of sigmoid
  is easier than maybe ReLu, for example.
Anyway I would pick Sigmoid function in hidden layer and Linear in output layer
Yes, this will work in the way that you want it if you use a standard sigmoid activation function.
The maths that proves this is a little complicated, but it effectively boils down to the fact that you are training the sigmoid function to generate the average value of the output training set (this is a consequence of using the squared error function in normal backprop).
for the discussion of a simple linear model with a L2 loss function
(then enriched with a sigmoid activation function).
The sigmoidal function S(c*z) for large c is is equivalent to the step function except at z=0 where H(z)=1 and S(c*z) = 0.5.
As you can see in the above graph it is clear that for values < -5 and values > 5 the sigmoid function outputs a value close to 0 or 1 respectively.
Here 5 is just used for representation but in reality this means that for extreme values of z the sigmoid function either outputs 0 or 1.
But when w.x + b = 0, z = 0 and sigmoid function outputs 0.5 or 1/2 which is not characteristic of a perceptron, which is described as "failing to mimic the behaviour of perceptron" in your question.
We will define the sigmoid and cost function first.
You have implemented a classification system that using the sigmoid will yield values between 0 and 1.
The first is the inexplicable use of 2 return statements in your sigmoid function, which should simply be:
Your (wrong) sigmoid:
See the [Derivative of sigmoid function (hyper-link)] thread at Math.SE, as well as the discussion [here (hyper-link)].
It seems like exactly because the derivative is either 0 or 1, but never just "approaching" zero (as in the sigmoid case) that leads to a sparse representation which turns out to help training.
This doesn't matter for the Sigmoid which does produce outputs in the range 0, +1.
However, you can easily write your own code that applies the sigmoid function, or any other single-argument function for that matter, element-wise to a vector.
See the CUDA program below for a worked example, in particular sigmoid_kernel().
A sigmoid function will 'favor' values closer to 0 and 1, so it would be harder for your model to output intermediate values.
In fact you don't have to normalize the input to be in [0, 1] for sigmoid.
The range of sigmoid [0, 1] is the range of its output.
What's more is that your input does not go directly into the sigmoid function so the range of your image input is not the same as the range of input that sigmoid would get.
You don't encounter this problem with the sigmoid function because it is a function that is differentiable everywhere, so for a sufficiently small epsilon, you should be able to achieve approximately the same value as the actual derivative of the function.
Just in case this helps, scipy has a [sigmoid (hyper-link)] function you can directly call on a matrix.
However the predictions will come from a sigmoid activation.
The sigmoid activation will change your outputs.
It is common to use a sigmoid activation with crossentropy as it expects a probability.
Now in the first case the sigmoid function provide a 5*5 matrix and the inverse of X is also 5*5.
Now in the second case, for the first row of grad, the sigmoid function also provide a 5*5 matrix but it's different because now X is a 1*5 matrix.
( X * theta) - y = m*1 matrix, hence the sigmoid is m*1 matrix.
X' * sigmoid is the main part here, because the other two terms are scalar, X' * sigmoid = m*1 matrix and finally your grad is m*1 matrix.
If you look closely the grad(1,1) is dependent upon X'(1,:) and sigmoid, and you have calculated the sigmoid using all the theta values.
Without a BIAS, the sigmoid cannot deviate from 0.
Here's a sigmoid with bias of 2.
Now sigmoid(0) = closer to 0.1
Although it seems that the default learning rate may be inappropriate at the first glance, the real problem here is that sigmoid activation is inappropriate.
Because your desired output should NOT be bounded, but using sigmoid implies a bounded output.
while x_i here is the output of the second last layer, which is activated by sigmoid, indicating that x_i \in [0,1].
