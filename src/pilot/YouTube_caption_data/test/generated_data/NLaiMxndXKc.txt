well good evening everyone
tonight's webinar for the deep
reinforcement learning nanodegree
program is going to be on pi torch it's
important lesson of course because
you'll be using PI tours throughout all
projects in this course so it's really
important to understand how PI torch is
used and tonight I'll give you a quick
introduction to some of the key ways
that that is done I'm using as an
example on the other Lander project and
that's like a preliminary project that
you'll do before the first real
navigation project that councilor
partier grade in passing this course but
you use that baseline to implement the
navigation so I thought we'd take a look
at that as a good example of Pi torchin
and how it's used
so with that let's pull up the slides
for today and we'll get started okay so
as I mentioned the ejector today is to
talk about PI torch and how that
framework is used in deep reinforcement
learning I'll use DQ n as an example of
the implementation architecture and just
you'll be seeing several different
agents throughout this course but the
first one that you really get into
pretty deep is DQ n so we'll use that as
our is our first example and then once
you define the model on the agent itself
we'll we'll use that model to do its
activities and non obtain actions
through the model to obtain an IQ values
and we'll show how that all works in the
process of going through this all right
so ki network is an example model and
again this is from the lunar lander
project and this this code segment here
is entirely the model dot PI file from
that project and this is the this model
dot PI pretty standard through all deep
reinforcement learning you you'll have
to kind of major helper files that
you'll use one is a model dot PI and one
is an agent pie and the model dot pie
diff
the neural network models that you use
and agent up high then has all the
actions in it for taking an action
taking a step learning all those
different activities so here the first
thing defining a model and I'll go
through this in three sections so I
wasn't quite sure how well this would be
visible at the resolution to show
everything on one page so this first
page that you see here is just full of
production and I broke it up into three
sections you see there the first one
just the imports and then in the
Declaration of the class that defines
this model and then the second step will
be to define the initialization code for
when you instantiate this this model and
then the last section is how you what
happens when you perform a forward pass
through the network
so let's touch hug each of those in turn
here so to start with I should mention
that pipe torch is a quite large
collection of a code baseline this is
quite a few different containers each
one of these containers has quite a few
different packages and each of the
packages had different containers so the
full set of all the packages is in a
documentation link and given the link
there you can go to piped org into the
docs folder and you'll see all the
different packages that the PI torch has
available one that you'll use in this
particular application a lot is the NN
package and inside that package there's
three different containers that use
again there's several others in here
that you're not going to be using and
it's worth your while to pull up that
documentation page and kind of peruse
through what's available but I'll just
touch on the the main sections out of
this that actually get used in the lunar
lander project as an example so the
first of these imports is the torch and
n dot module container and that
container basically should be used as
the the base class for any models that
you develop any neural network models
that you development and we'll see how
we declare this new class you're going
to build for your particular model for
your deep learning project as a subclass
of tour start and end up module
um I should mention torch should be not
capitalized there the should just be a
lowercase T so the second sort of major
container that you work a lot with and
there's quite a few things in this one
also is the N n dot functional and this
basically has all of these different
activation functions that are available
to you all the common ones that you
could think of and if you look at the
documentation there's several that you
may not have probably have never heard
of before but the ones that you'll see a
lot in deep reinforcement learning
include rail ooh 10 H and softmax the
nails are definitely in there along with
many others and the last one that we'll
use in this model is Torsten and linear
and this basically is the function that
you used to define a fully connected
layer in a neural network and in the
models that were using here they're all
affected so we'll be using this
extensively in this particular example
when you call Torsten and linear you had
to pass him the size of the input
features and the size of the output
features and whether you want to use a
bias or not and essentially it just
produces an output Y by multiplying the
input X times a weight matrix here
termed a transpose as the weight matrix
and adds a bias you can choose to learn
what the bias or without just set it to
true or false what do you want to use
that or not okay again here's the the
first half a dozen lines basically
import those three containers we just
talked about and then instantiate your
new class which we call here cute
network as a so class up and then dot
module pretty straightforward okay the
next section is the init method and
basically you'll be initializing your
class with the size of your state the
size of the output actions a random seed
and then some hyper parameters for the
size of the different layers
and we'll look at that here this is
basically that section and a little bit
larger font size so you see the the
inputs there at the top now this is your
you know model so you can add whatever
other different parameters you think
might be needed for example if you
wanted to use a 3 layer Network you
could add an FC underscore FC 3 units
parameter here to define the number of
hidden layer units and the in a third
fully connected layer of the network
so again just you know do whatever makes
sense for your application and and if
you're working through your navigation
project and later projects you may end
up finally you need to do quite a bit of
manipulation of these different sets to
come up with the set of hyper parameters
in a architecture that works for your
for your application so again there's
only five lines of code in this
initialization is pretty simple
straightforward you first just call the
super class initialization to perform
all the initialization that's defined in
the PI torch and in dot module and then
define the seed that you're going to use
which was passed in that's one of the
parameters so if you want to do we
create the same you know exact
implementation starting from the same
place you can recreate a set of results
that you previously seen or if you want
to try different random seeds to see how
things vary based on the seed you can do
that as well and the next things are the
definitions of the different layers so
here we're just defining three different
layers FC 1 FC 2 and FC 3 they're all of
the in-and-out linear container class so
they're all fully connected layers the
first one you know starts with an input
of state size it outputs a number of
units of size FC 1 units the second
layer is the takes it and our hidden
layer takes all the FC 1 units and as
input and then fully connected layer to
the professi 2 units that you defined
and then the last layer is the final
layer of this network takes that output
at FC 2 and outputs it to a set of
neurons on the out
sighs action sighs net that we're all
the actions are presented as outputs
okay now last thing in your model um is
to define what to do how this network
works to do a forward propagation step
so that that method is called forward
and forward takes a state a set of
states as input and then here again this
is fairly simple but you're basically
taking the FC one it passing in the
state information you got as input to
this forward pass passing it through FC
one and then performing a rail ooh
activation function on that result and
then you take that x value in that first
line and pass it into FC - so now we're
feeding it through the next layer of the
network and again on the output of that
layer another Allu activation function
and then again the last thing that's
done is to take that output of FC to
pass it through the third layer which
produces the state outputs and return
that to the caller so pretty
straightforward that basically is always
required to to implement a fully working
neural network for doing deep
reinforcement learning
now I mentioned let me just go back a
couple slides one thing that you may
find you need to do is to use some -
normalization so if you decided guys
needed take a look at and then that
batch norm 1d and you can see how to
implement a one-dimensional best
normalization on the inputs to
particular layers and when you define
the the init function for your T Network
class you need to define those batch
norms this was the same way as you
define your F C 1 and C 2 C 3 here they
just become a new layer definition
within the network and then once you've
done that you also have to include that
step in in the forward pass just to show
how you would for example take a batch
norm of the state before you
basudev Siwan and then we take a batch
norm of X before you pass a test these
two etc so if you decide the destiny did
take a look at that there's some
examples in the in the PI torus
documentation or you can take a look at
many other implementations of of cute
networks that you might find on the web
and it'll often include a batch norms
depth as a way to optimize the network
performance okay the second half of this
problem is going to be the agent side to
things so I mentioned at the outset here
that the two major files that you
typically see is utility functions used
in your implementation of a deep
reinforcement learning program has both
a model dot PI and an agent PI in in
this case our agents a DQ n agent so
it's appropriately named DQ and agent PI
and again this is from the lunar lander
project so here's the import section and
now I you know agent up I actually need
to take a look at it when you use this
this class in your implementation
there's quite a bit of code here I'm not
going to go through it totally line by
line I'm gonna concentrate on the
sections of it that really have two pi
torch sections that are important to see
how PI torch is used there's lots of
other things that are done in this
module that are not pie charts specific
related but more related to the to the
agents particulars of the agent itself
so we'll touch on mostly I here on on
the PI torch part so on the imports of
DQ an agent you can see we're importing
the model from our Q network this is the
class we just talked about the class
that we just defined and then you'll see
the first two imports they are the ones
that we've seen before same ones
basically as we use the model and here's
a new one at the end torch optimum and
this is where you would import a
optimizer to be used
I think lunar lander uses the atom
optimizer which we'll talk about here in
a second
but there's many optimizers to choose
from Adam is just one of them
traditional on stochastic gradient
descent of course is another one and
several others up pretty much you know
anything that's commonly used and
something that you're familiar with from
any of your working with a neural
networks there's probably an optimizer
in PI torch that implements that
optimization algorithm so there's the
documentation page if you'd like to take
a look at that in more detail so here's
the init method and just before I knit I
also included a device call here this is
something you want to do to make your
your code be kind of agnostic as to
whether or not you have a GPU available
or whether all that all the code has to
be executed in the CPU so typically this
is what's done to make that agnostic
choice you can have a simple statement a
device is uh is a torch device and use
CUDA column zero for channel 0 of the
first graphics card basically in your
system if the Nvidia CUDA
libraries are available and it detects
that card is being present otherwise it
says device to be CPU and then from then
on everything you do these references
device and your code will run equally
well on a CPU or on a GPU so that's a
nice nice thing they have so when you
initialize your your agent
you basically have to pass in the size
of the state space that you're you're
working on discussion in the environment
and then you also provide the size of
the action this is the output of the the
network instead of the sizes the number
of actions that are possible to be taken
and I recall DQ n as a it's a discrete
action space so these are basically the
number of discrete actions that are
possible from from your particular
environment and then here below we see
where as densa gating actually two
different copies of that q network model
DQ n uses the concept of a local and a
target network where learning takes
place initially on a local network and
then slowly it gets propagated to the
target network and that helps us
stabilize learning in the environment
stabilizes the activate
it's activities so here's where you
stand she ate this two networks to
basically the same in terms of state
size and action size it's just that here
you get to complete network so from that
same class that you had defined and then
now I hear we talked about the atom
optimizer so here's where we define the
optimizer to use and we are using atom
from the optimum packing that we
imported earlier just uh just a real
quick atom is a little more complicated
than stochastic gradient descent it does
similar things there's lots of different
optimizations it makes the major one
really is that instead of having a
common learning rate all across the
network for all time it keeps track of
gradients on a weight by weight basis in
the network and then optimizes the
learning rate based on how each one of
those weights are changing so if there's
a lot of change in a particular weight
it will automatically lower the earning
rate it says not much change it will
automatically increase the learning rate
so it really learns much better and more
consistently then stochastic gradient
descent and so it's a very commonly used
at optimizer so as I mentioned there's
the several different methods inside
this dqn agent and one of them is the
act method this basically takes a
forward pass through the network and
given a state it the network selects the
best action to take and this one this
example just takes the greedy action you
could modify this to add some extra code
and you probably should do that to do
epsilon gree where you would actually
have a random choice to taking a random
action but this particular one just
shows an example look taking the greedy
action the best action so starts with
getting the state by getting a state
tensor by taking the the numpy state
vector and essentially converting it to
one dimensional turning into a torch
tensor and then sending it to the device
you selected before either your CPU for
processing
or to a excuse me graph on your GPU
Johanna's available the next line is
important though that the second line of
code and the last line kind of go
together since we're doing a forward
pass here we first have to do is put the
network into evaluation mode so that it
won't be calculating gradients then we
do the forward pass and then we set it
back in to train mode at the end so that
it can learn after this active function
is propagated through so with the torch
no gradients that's basically we're
doing a forward pass there's no need to
store the gradient information and
you're gonna choose the action values by
doing that forward pass by passing in
the state vector and essentially doing a
forward pass through the network and
producing the action values will get
returned from this method okay a second
really pi torch related method is the
learned method
so while action is a forward pass
function to get a set of actions the
learned method is really concentrating
on the learning so a lot of back
propagation and defining what your loss
function is and then doing that back
propagation step using the loss function
so the learning method is where you
define all those activities you pass in
to learn a set of experiences and they
gave a discount factor from the bellman
equation and experiences you know you'll
find is really a collection of a set of
states actions rewards next states and
done flags so here we the first line we
pull out from the experiences all those
individual parts and then we're going to
predict a Q value so we first on the
target network we pass in the the next
day which we got from the experience and
use that to obtain a Q value for that
next date and then we use the bellman
equation to for the target estimate what
we think that the the Q value should be
and as you recall from the dolphin
equation that's basically the reward
that the
the environment gave us plus the
discounted value of that Q target next
state that we just obtained that is
basically the best actually we could
take that state so I mentioned that two
targets next when we pulled that out we
we saw that that was a loss to share
here just since I can only share this
screen
there we go okay so notice the the max
function there we when we calculate that
next value we want to get the the
maximum action the maximum Q value for
the best action we could take him
the greedy action in that state that's
that's part of the the bellman equation
for that she targets definition then the
second thing we're gonna do is on the
local network we're going to actually
take what it the local network thinks
the the Q value is and then our loss
function is actually what we expect from
our local network to have for Q value
and what the bellman equation would give
you for the Q value and we basically use
a mean squared error loss between those
two as our loss equation and then that's
that's used by the optimizer to do the
backward pass as you can see in the last
three steps here and that basically is
how the network learns so it's really a
little maybe confusing the first time
you see it but PI torch is it's really
per straightforward and how you can
define things to happen and it's very
compact and easy to express what it is
you want to have done and all the work
really gets done heavy lifting is done
by those all those different methods
that PI torch makes available to you so
that really is what I wanted to talk
about today and in summary we talked
about how we can use PI tours to build a
model of a neural network and we use DQ
n as an example
model we saw that was really pretty
straightforward you just define the
layers that you have and then hook them
up in the forward pass to to make the
network understand how to process a
forward pass through that network and
then we looked at a common agent in deep
reinforcement learning for discrete
spaces and that's the DQ n agent and we
looked at the hop high torch is used in
in that agent first by defining an
optimizer again in our case we used atom
is that optimizer
and in the act method we used forward
passes to obtain the actions based on a
state says data inputs and then in a
learn method we looked at how we tell PI
torch how to do back prop bond to do the
learning so that basically is that what
we want to talk about this evening if
you have any questions about this topic
or anything else in the deep
reinforcement learning program that
we're working with we're gonna have a
ask me anything session on slack
immediately after the live webinar so
feel free for the students that are
enrolled in this class to take a look
there and and I'll be happy to answer
your questions with that I'll talk to
you next week bye