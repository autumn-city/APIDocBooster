 In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.  This is numerically more stable. 
 Yeah, conceptually this is a tensor of integers, they can only be 0 or 1, but we, we�re going to be using a cross entropy style loss function, so we're going to actually need to do floating-point calculations on them.  That's going to be faster to just store them as float in the first place rather than converting backwards and forwards, even though they're conceptually an �int� we're not going to be doing kind of �int style calculations� with them. 
You can interpret the above true distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C. 
 So this particular function, which is identical to MNIST loss plus �.log� jhas a specific name and it's called binary cross entropy, and we used it for the threes vs. sevens problem, to, to decide whether that column is it a three or not, but because we can use broadcasting in PyTorch and element-wise arithmetic, this function when we pass it a whole matrix is going to be applied to every column. 
The cross-entropy loss that you give in your question corresponds to the particular case of cross-entropy where your labels are either 1 or 0, which I assume is the case if you're doing basic classification. 
