- Coming in from Seattle,
we got a couple of people
from India, Sundeep.
(Jules clearing throat)
- Okay. I think we're all...
We're all set up. We're live
on LinkedIn and YouTube.
Hi, everyone.
- Can you send me the links for those?
- Yep. Yep.
Let me just grab these
links to Jules, my co-host,
and then we'll go ahead and get started.
Hi, everyone.
Let me mute that.
One second.
Okay. There's the LinkedIn URL, Jules.
And then let me just get
the YouTube link for you.
All right. I think we're good.
I think we ever-
We're all set up.
Both livestreams look good.
So let's go ahead and get started here.
I'm gonna share my screen.
Good morning, Steve.
All right.
Share screen.
All right, let's get started here.
Hi, everyone. Thanks for joining us.
We're excited. We have two
really awesome speakers here.
We're gonna be speaking
about "MLflow Integrations
with PyCaret and PyTorch".
So we have Moez, who's the
founder and author of PyCaret,
and Geeta, who's Partner
Engineering Head at Facebook AI.
And a quick call-out to our
Data + AI online meetup group.
So this is part one of our tech talks
that we host on our online meetup group.
We run them live and stream
to YouTube and LinkedIn,
so welcome everyone from
joining us in those spaces.
So I invite you to join our meetup group,
so you can be notified
of our upcoming sessions
and all the awesome
sessions we have coming up.
And I also invite you to subscribe
to our YouTube channel,
turn on notifications.
We host all of our video recordings there.
So each of our sessions are recorded
and they're hosted there,
so we have a "Data + AI
Online Meetup Group" playlist,
and we also do kind of playlists
for different series that we run.
And then follow us on LinkedIn, too.
We're live there as well.
So our next meetup, we
actually have another one,
it's the fourth in a four-part series
with Salesforce Engineering,
and that's this Thursday.
So I invite you to check that session out.
I'd love for you to join us.
And we also have a playlist.
So we have the three video recordings
of the first three sessions
available on YouTube.
So I'll send that link
in all the chats so you can check it out.
And then we're super excited.
We have our summit coming up.
I know a lot of, I think we've
done a post here and there,
and I know a lot of the
folks joining us are new.
So, I always like to plug our summit.
We're super excited about it coming up.
That's gonna be May 24th through the 28th.
We have some really awesome
speakers, keynote speakers,
Malala, Ali, Reynold, Matei.
Super excited.
Tons of content, over 200 sessions.
You know, we have also
training the first two days,
so lots of good stuff in there,
sessions for everyone beginner
through advanced levels,
and a variety of topics.
So I hope you'll register
and hopefully we'll see
you there, end of May.
And just a quick moment
before I pass it to my co-host, Jules.
Just a reminder, this is recorded,
and I'll send the link
in all the chat spaces
so you can review it at a
later date if you'd like.
And then also please ask questions.
So, YouTube and LinkedIn, put
your questions in the chat.
We'll gather those,
and we'll have time after
each session for questions.
And then if you're on Zoom, it'd be great
if you could post those in the Q&A.
So without further ado,
let's pass it over to Jules.
- Well, thank you, Karen.
And let me see if I can
share my screen over here.
Lovely.
Okay. Can you guys see my presentation?
- Yep.
- Yep. Looks good.
- Okay.
Well, welcome, all of
you to our second part
in the series of "MLflow
Integration + Community",
and one of the reasons
we really wanna do this,
is to really acknowledge and recognize
and amplify some of the integrations
and some of the very valuable
community contributions
that we actually have
from the MLflow community.
And so today, if you are
not familiar with it,
we actually are using PyTorch.
I mean, if you're using MLflow,
you know, straight out of the box,
these are some of the integrations
and some of the built-in support
that we actually have
in MLflow right away.
And today you're going to hear about
the valuable contributions from PyCaret,
how you can actually
use PyCaret with MLflow,
as well as the ongoing contributions
that Facebook's AI team
and PyTorch's Engineering
team has actually made,
so we are actually quite
excited to hear about them.
And the community is
actually growing rapidly,
almost on daily basis.
We have organizations who
are actually using MLflow
in production.
These are the organizations,
and you can actually see
some notable names over here
who are contributing to the MLflow,
which is actually extending
its adoption quite a bit.
And we have close to about 90
organizations who are using it
and a number of them are
contributing significantly.
So if you are using MLflow in production,
if you're using it at scale,
we wanna hear about it.
And let me know,
so I can actually include
your logo in there.
Now, the contributions over
the period of three years
have sort of ranged in
number of areas in MLflow.
And the most important ones
are some of the deployment plugins
that are very common.
Last week, we actually heard
from the Azure ML people
who actually contributed the thin client
as well as how they can do
experimentation using ML Azure.
We also heard from
"algorithmia", whereby the plugin
allows you to log all
your models in MLflow
and then deploy them into
the target and run them.
RedisAI is another one that actually
has contributed a plugin that allows you
to do your MLflow logging
and tracking experiment
on your particular host,
and then deploy the models
for inference on the Redis API.
And then "mlflow-ray-serve"
is the newest plugin
contribution from the community,
whereby you can actually
run your deployed models on Ray Serve.
And then PyTorch,
"torch-serve" is another one.
There are other plugins areas where
you can actually have your MLflow project,
for example, run on a remote
machine with "mlflow-yarn".
And if you want to use a
different tracking store
other than the one that
we actually provide,
"elasticsearchstore" is another plugin
that actually has been contributed.
So you can see these
actually extend how MLflow
can actually be used with
other ML ecosystems as well.
So this is actually quite important
for us to actually have the
community contribution as well.
And then, you know, there
are more and more plugins
which are being contributed
from the community.
The SQL server, if you actually
want to use your backend store, other than
what we are using by default,
you can use the "db-store".
If you want to use an artifact store
for Alibaba to store your
artifacts and models,
you can actually use that.
And there are miscellaneous contributions
all from the community.
The Windows Microsoft actually provided
huge, huge Windows support for them.
The "mlflow-thin-client",
which you actually saw a
demonstration last week,
gives you the ability to actually work in-
Just the client itself that
reduces the boundary of MLflow,
and works very well with Azure ML.
R Studio for example,
is the one that actually
provided all the R
integration with MLflow.
That this is actually
really, really been a huge...
And then needless to say,
PyTorch integrations
are continuing to extend
the functionality that you
can actually use with MLflow.
Bug fixes, documentation, code snippets,
all these are coming from the community.
And so if you're interested,
put your integration,
and we'll be more than
happy to look at your PR.
And so I'm really, really looking forward
for your contribution as well
to extend the MLflow
to a larger community.
And if you look at all
of the board, I mean,
these are all the contributions
which are actually happening
on a monthly basis.
There are some spikes now and then,
because of the releases
every six or eight weeks.
So we actually are quite
excited to see that.
And in the last, you know, four months
we've actually seen over
4 million downloads,
and these are the downloads
which are actually happening.
So there is ample of
interest in the community.
And people are really
excited to use MLflow
in whatever capacity they have.
And then obviously, no open source project
or no open source community
is without some vanity stats.
So I'm gonna share some of
the vanity stats with you.
And since the release of MLflow,
we have had over, you know,
2K forks, about 8.9K stars,
86 releases yesterday, we released 1.6,
and we are at about 295 contributors.
So I want one of you guys
to be the 300th contributor
in the coming days or weeks.
And just last night, we released 1.6
that has some big rocks that
were part of the release.
One is for you the ability
to use PySpark ML estimators
with autolog, so you don't have to worry
about explicitly logging
any of the MLflow entities,
it does it all automatically for you.
We added CatBoost as
another MLflow flavor.
And some bug fixes here and
there, and enhancements.
So if you wanna get involved,
and I'm looking forward to hear from you
go to mlflow.org, join the
community, join the user group,
join the Slack channel,
and file your issues and file your PR.
And I'm the one who actually handling
all the triages and issues.
So I'm looking forward to that.
With that, I'm going to now
introduce our first speaker.
It's really a pleasure for
me to actually have Moez,
who is the original creator of PyCaret,
and he's gonna talk about how you actually
use PyCaret with MLflow today.
He is a seasoned data scientist
and original creator of PyCaret.
So Moez take it away. It's all yours.
- Thank you. Can you hear me okay?
- Yes, we can.
- All right.
Really great introduction, Jules.
I'm very excited to see the progress
MLflow has made last few years.
It's amazing.
Okay. You see the
presentation on my screen?
Okay, great.
- Yup. Yes, we do.
- Hello. Thank you everyone
for joining me here today.
I'll quickly introduce myself.
I'm a data scientist
with background in finance and economics.
I have 10 years of
experience, mostly working
with data and solving problems.
I have worked for healthcare,
education, consulting,
and Fintech institutions.
I'm a chartered accountant by profession,
so I'm a member of CPA, CMA, ACMA, CGMA.
And I have in last six, seven years,
I have changed four different
continents and I've worked
in four different countries
in four different continents.
And these days I'm based
in Toronto, Canada.
And I have a couple of
links here for my profiles.
I have a LinkedIn, very
active LinkedIn user,
Twitter and Medium.
I'm a very active blog writer.
I write four to five blogs each week.
And my blogs and tutorials
are mostly focused
around beginners who are just
getting into data science
without computer science background.
Bunch of important links here.
The most important link is this one,
www.github.com/pycaret/pycaret-demo-mlflow.
If somebody can please
write this link on chat.
So this is the repo
where this presentation,
as well as all the demos
that we are gonna see today,
they are uploaded there.
And there are a bunch of
links in this presentation
that are useful so you can get them later.
We have a Slack community.
We have a very active community,
that basically handles questions
and triaging on GitHub.
But if you are doing
something, stuck in a problem,
Slack channel is the fastest
way to get the answer
to your queries.
I personally am active on Slack channel
at least for an hour day to
respond to the questions.
You would find a bunch
of resources for PyCaret.
And this presentation has a lot of links.
So these image are links.
And there are a lot of
integrations of PyCaret with
tools like Ray Tune, fasi.ai,
AWS, Azure, MLflow, GCP.
I'm gonna leave this, skip the slides,
they're there just to...
For you to watch them later.
Okay, let's get started.
So this, what you see
here is a very high-level
machine learning life cycle.
It's a workflow.
It starts with a business problem,
goes to data sourcing and ETL.
So this would include things
like sourcing the data from
from your SQL server
or any other database,
doing an ETL process to
basically prepare the data
for modeling, not in the
sense of machine learning,
but also just in sense
of making data available.
This also includes things like...
Not every time you would have data
readily available in a SQL
server, so you'd have to...
For problems like let's
say customer churn,
you'd have to create the
dataset with the lag.
So anything that has to
do with creating dataset
at a database level,
comes under this stage.
Then you go to exploratory data analysis.
You haven't started modeling yet,
you are just exploring your data,
checking distributions,
checking multicollinearities,
to kind of create hypothesis
that you would use in the
data preparation stage.
The next, which is a big
one, data preparation.
This includes things like,
if you have categorical
features in your dataset,
you would do encoding simply.
One-hot encoding is the simplest version.
If you have missing values,
you would train missing value imputers,
to kind of impute the missing values.
Then you have bunch of things
like feature engineering,
feature selection,
polynomial features,
removing multicollinearity.
All that comes under
that data preparation.
And once you have your dataset ready,
you have split it into train and test set,
you get into modern training
and selection process,
which is where you would
train multiple estimators,
you would lock their metrics,
you would lock their hyperparameters,
you would do hyperparameter tuning,
depending on the use case,
you would sometime do
ensembling, maybe bagging, boosting
or maybe even complex
ensembling, like stacking.
And then once at the end of the day,
you have one model out of
all the extra models here,
one model which you
wanna use in production,
the final stage is
deployment and monitoring.
And as you pass through
this entire workflow,
you realize you generate tons of metrics.
And there are two form of metrics and-
Which is, one would be performance metrics
of the estimators themself,
which would be things like
accuracy, AUC, recall, MAE, Mean Squared.
And then you also have a lot of metadata
in terms of hyperparameters of models.
So think about decision tree,
where you would have
parameters like Max Depth,
you would have parameters like filters.
So the point is, when you pass through
the entire machine learning life cycle,
there's lot of metadata that
you have to take care of
if you wanna keep a log of it.
And that's where MLflow integration
with PyCaret comes handy
because when you use
PyCaret behind the scene,
we are integrated with MLflow,
and you will see that in demo.
And we record all that metric
and all the parameters,
and all the artifacts automatically
without you doing anything explicitly.
Okay.
So PyCaret is an open source, low-code
machine learning library,
and end-to-end model management tool.
And we call it "end-to-end"
because we have this integration
with MLflow that basically not only allow
to log the metrics, but
thanks to MLflow, we have...
You can now even serve the
models using MLflow ecosystem.
So this is basically, x-axis on this graph
is the modeling stages.
So you can see data preparation,
training, selection,
model evaluation.
y-axis is cumulative lines of code.
And we have performed a simple experiment
where we have written down a list of tasks
that we need to do, read
the data, perform some EDA,
train multiple models,
tune hyperparameters,
and evaluate it to some metrics.
And we have performed this
experiment simultaneously
using scikit-learn and PyCaret.
This orange line here is scikit-learn,
and the blue line here is PyCaret.
And what you would see is by the time
you finish your experiment,
you have approximately
written 170 lines of code
to achieve the same thing that
you would achieve in PyCaret
using 20 lines of code.
So it's a big impact for teams who are...
Who wanna keep it lightweight,
so that it's easy on maintenance.
And also, it's very powerful
for citizen data scientists,
which is not basically
centralized data science team,
but also people who are
working in different functions,
like marketing, finance, HR.
It comes really handy for them.
So far the use case supported in PyCaret
is a for supervised machine learning.
We have classification support,
both binary and multi-class.
We have regression for unsupervised.
We have clustering, anomaly detection,
association rule mining, and NLP.
And we have a brand new
module coming up soon,
Time Series module, which is,
which is just something
I'm really excited about.
Okay. So now we would see, I
have three demos for today.
And in all the demos,
you would see the flow
like we would train models,
we would tune models,
we would analyze the
performance, and we'll deploy it.
And one of the demo is
basically, I'm gonna show you
how you can even serve
the models using MLflow.
All right, let's get to the demo part.
Let me just bring my Notebook here.
First demo.
Do you guys see the Notebook?
Can somebody give me confirmation?
- [Jules] Can you zoom in
a little bit more, Moez?
- Sure.
Okay. Is that fine?
- [Jules] Yeah, that's good.
- Okay, great.
So this is the first demo, "Regression".
Again, the Notebook is already
uploaded on the Git link.
So to install PyCaret,
you can just do "pip install pycaret".
By default, the slim version of PyCaret
doesn't include all the functionalities,
but you have base
functionality when you do that.
And if you wanna do a full
version with this notation,
you can install full version of PyCaret.
For this demo, I'm using
a regression dataset.
This "datasets" module is
part of PyCaret, so when-
You can, basically, there
are a couple of toy datasets
uploaded on our GitHub, you can use it to,
just for trying out.
So this dataset here is, we
have the six patient attributes,
"age", "sex", "BMI",
"children", "smoker", "region",
and this "charges" column,
which is basically a continuous value.
So it's a regression problem,
where all these six
values are x variables.
This is my y.
And I wanna predict the charges
based on these six variables.
The first step in PyCaret, which kind of
handles all the data
preparation, is "setup()" command
and the "setup()" function,
or the way you write it is common
across all the modules of PyCaret.
So whether you are doing
classification, regression,
clustering, anomaly detection, NLB,
this format of writing code stays same.
So you don't actually have to learn
differently for different modules.
When you initiate this "setup()" function
from "pycaret.regression", it would
by default, infer the data
types of your features.
And if you find them okay, in this case,
"age" is Numeric, "sex" is Categorical,
"smoker", Categorical,
"region", Categorical.
If you find it okay, press Enter.
There's actually a way to override this.
And if you read the documentation
instead of function,
you will figure out, that if you want
to override the data types in PyCaret,
there is a parameter inside of function,
that you can pass.
Okay.
All the preprocessing, and
all the data preparation,
is handled at this stage.
So by default, when you execute "setup()",
PyCaret will split the dataset
into "Train" and "Test Set".
So you can see "Train Set".
So we had 1,300 records,
which were split into
"Train" and "Test Set".
The other thing that
PyCaret does by default,
is it would impute the missing values
if there are any missing
values in the dataset,
even if you don't have missing values,
it would create imputers in
the transformation pipeline,
so that when you have N/As in
your production data coming in
and you don't have control in front-end,
PyCaret would use those imputers
to impute the missing value,
and then pass it to prediction,
so that if you have null values coming in,
in your production dataset,
your function should not fail.
And then one-hot encoding.
So in this case, we had couple
of Categorical features.
You cannot use them directly for modeling.
So PyCaret, whether you ask it or not,
it would do one-hot encoding default.
Other than that, all the
other preprocessing features
such as normalization,
transformation, scaling,
feature extraction,
feature engineering, PCA,
combining rare levels,
outliers removal, all that.
You can just simply pass it with a flag
like "True" and "False".
So it's not really hard to do that.
It's a matter of just passing through,
but that's up to you.
If you wanna do it, you can pass it here.
All right.
This is how you can access
your transformed "Train
Set" and "Test Set".
So in this case, you can see
"age" and "BMI" remained same,
but because "sex" was
a Categorical feature,
it was kind of dumbified, or
I would say one-hot encoded.
Same with the other features here.
It's a table of ones and zeros.
All right.
So the first step...
So now that we are done with
the preparation of data,
the first step will come in
any workflow for PyCaret,
is this function called
"compare_models()".
And what "compare_models()"
does, is it would basically
train all the estimators
available in the model library.
So basically all the
estimators means all the
scikit-learn estimators,
XGBoost CatBoost, LightGBM,
NGBoost, genetic programming,
anything that is consistent
with fit/predict API,
would work in PyCaret.
And I think 99% are compatible
with that scikit-learn fit/predict style.
So when you run "compare_models()",
what it's doing is
behind the scene, it's screening
all the estimators available,
and doing a tenfold cross-validation,
and returning you these metrics.
So this is regression
so that means you get
Mean Absolute Error,
Mean Square Error, RMSE,
if it was a classification
problem, you would get things
like accuracy, AUC,
recall, precision, kappa.
And you would see this nice little table.
And it would basically,
based on the metric
whatever metric you define,
by default, it started to R2,
it would return you this "best" model,
and the metrics based on
tenfold cross-validation, okay?
So if I check the "best",
it's a GradientBoostingRegressor,
with all these hyperparameters.
And these hyperparameters are just
the default values of scikit-learn.
And if I just check the type of "best",
it's basically
"sklearn_ensemble" class, right?
So you don't have to, even
if you're using PyCaret,
to replace your boilerplate code,
you don't have to rewrite
your downstream processes,
because what you get from here
is basically scikit-learn model, right?
You can call ".predict()" out of it,
you can do whatever you can
do with scikit-learn class.
While "compare_models()"
is really good function
to kind of have a baseline model ready,
based on default hyperparameters,
the more granular model function we have
is called "create_model()".
So when you do "create_model()",
it basically trains the specific model.
And this "dt" is the ID of the model.
And if you check the documentation,
it would basically give you
all the IDs and the models.
So in case, if you wanna, let's
say, train Bayesian Ridge,
instead of "dt", you would say "br",
and now it would train
Bayesian Ridge, right?
I'll show you the Bayesian Ridge model.
Okay.
Behind the scene, when you
call "compare_models()",
what we also do is use
"create_model()" to create a loop,
and give you this output, right?
But "create_model()" kind of
give you more granular control.
All right.
With "tune_model()" function,
you can tune the hyperparameters.
So in this case, I have a decision tree,
tenfold cross-validated,
Mean Score is let's say
$3,148 in Mean Absolute Error.
Now I would tune the hyperparameter
using "tune_model()" function.
And let me just remove it for a second.
Let me just run this.
So when you do this,
this is gonna tune the hyperparameters.
And how does it get the hyperparameters?
Behind the scene, we have
defined dynamic grids.
So based on the estimator you are using,
it would use the dynamic
grids and iterate over
the random parameters to
kind of tune the models.
So in this case, just tuning randomly,
using scikit-learn random grid search,
we have gone from $3,148 to
$2,051 in Mean Absolute Error.
This would not always improve.
Sometimes tuning may
have a negative effect,
because at the end of the day,
it's a random grid search.
But this function also
provides couple of options.
So instead of random grid
search, if you wanna use
any Bayesian technique or TOPT
from libraries like Optuna
or tune-sklearn or even Ray,
we have a parameter here
called "search_library".
And if I just do
"search_library = 'optuna'",
and now behind the scene,
PyCaret would use Optuna Bayesian search
to tune the model.
So you can see 2,006, even better.
The problem is, if you were
to use these techniques
outside of your baseline code,
all these APIs are really different.
Optuna, hyperop, TOPT -
they are different, right?
And what PyCaret is
doing behind the scene,
it's like a wrapper, it would
use the same search space,
but based on this parameter,
it would just switch
to a different library for
doing Hyperparameter Tuning.
Now, if I just see
"tuned_dt", in this case,
I mean, previously, when we
use the random grid search,
we can have a look at
one of the parameter.
"min_sample_split" was 2.
Based on Optuna Bayesian grid
search, it happens to be 6.
And you can see other parameters are now
a little bit different, too.
We have a function called
"ensemble_model()",
which basically, would
take your "tune_model()",
which is this one, and
it would wrap it around
bagging ensembler.
So in this case, if you see, we started
with a decision tree, giving us $3,148
in Mean Absolute Error,
tenfold cross-validation,
we tuned it using
Optuna, we get to $2,006.
And now what I'm doing, is
I'm taking my "tune_model()",
passing it through an ensembler,
which basically by default
is bagging ensembler.
And now I'm a little bit better, okay?
And if I just show you what this is,
this is the same decision
tree that we tuned, right?
But now it's wrapped
around bagging classifier,
which basically means that it would build
the same decision tree.
By default, I think we
are using 100 estimators,
but you can change that
parameter if you want.
In Ensemble Model if you
check the documentation,
we are using...
Sorry, 10 estimators.
If you wanna change it, you
can change it to 100, 200,
whatever you want.
We also have Voting Ensemble,
which basically means
I can now create three
separate estimators.
So you can see "create_model('dt')"
is decision tree,
this is lasso regression,
(coughs) Excuse me. This is "knn".
I'm just doing "verbose=False"
because I don't want
it to flood my screen.
And then I'm passing these
three trained estimators
into a "blend_model()" function as a list.
And if I just run it, what it would do
is it would train the
three models separately,
and would pass it in a blender.
Obviously, performance is very bad
because I have randomly
chosen these models.
This is not how you would do it.
You would choose your best
models to be combined together.
And if I just show you what is "blender",
this is basically all these three models,
wrapped around a VotingRegressor.
And again, if you check
the type of "blender",
this is scikit-learn, right?
Similarly, there's another
technique called "stacking".
It's different than voting.
But again, if you check "stacker",
this is doing the same thing.
It's taking your individual
models and wrapping it
around a StackingRegressor
with a metamodel.
By default metamodel
is a linear regression,
but you can change metamodel
by passing the parameter "meta_model".
Okay. You can evaluate
you models in PyCaret.
So now that we have prepared
the data using "setup()",
we have trained bunch of
models, we have tuned,
now we have our best model.
Just pass your best model
in "evaluate_model()".
And you would see bunch of options here.
This is Hyperparameters of the best model.
You can see the Residuals
plot for this model.
You can check Feature Importance.
You can check Interactive Residuals.
So people coming in from R background,
kind of misses the typical Q-Q
plot and the Residuals plot
that they are used to seeing in R.
And we have kind of-
Somebody from community
has actually implemented that in Python.
So now people transitioning in from R
would feel like home in Python.
Okay.
We have also integrated with Shapely,
"sharp" package in Python,
which is basically another way to look at
the Feature Importance, not
exactly Feature Importance,
but this basically tells you
what the model is learning.
It's a little bit too
complicated to talk about this
in this 30 minute, but do check it out,
it's very interesting.
Interesting way to interpret your models.
"predict_model()" is a
function where you can
use your trained estimator now
on new data to generate inference.
And in this case, I can see if
you don't pass any data here,
it would just do the
holdout set inference.
And so you can get metrics to us.
So you remember our best model here
was doing 2,700.
So this best model here is
$2,702 in Mean Absolute Error,
but remember this is
cross-validated metric
that is calculated on this "Train Set".
What you wanna do is look at
the "Test Set" metric for this
before you actually trust on this model
and put in production for inference.
So when you just do "predict_model(best)",
you don't pass any data,
that has by default set to "None".
If you don't pass any
data, PyCaret would assume
that you want to check the
inference on holdout set,
so you can see the metrics.
So in this case, $2,386. Not
bad, it's a little bit off.
200, $250.
Normally the difference
between this metric and
cross-validated, if it's huge,
it would indicate overfitting.
But if you just check
the head of this dataset,
you would see, we have the "Label" column,
which is our prediction
attached into a data frame.
What I would do, is I would
create the new dataset,
I would copy the same data,
I would create "data2",
and I would drop the "charges" column.
I don't have a production
dataset for insurance,
so I'm kind of mimicking
by using the same dataset.
You finalize your model.
What finalize means that,
remember when you
initialize the "setup()",
your dataset is split into two parts,
"Train" and "Test", right?
And all the cross-validation
is happening on the "Train" path,
And then once you see the holdout scores
and you are you're okay with it,
only then you call the "finalize_model()".
Why? And what does finalized model do?
It will basically take your model
and fit it on the entire
dataset, which is 1,338.
So far, it's only fitted on 936, right?
But now, just before you
are pushing your model
in deployment, you don't wanna
leave your data on the table.
So you would basically take all the data
and just refit the same model.
Same hyperparameters, same everything.
You're just refitting when you finalize.
Now that we have a final
model, we can simply pass
that final model with
the "data" parameter,
and now it would return your prediction.
So in this case, you can see
it's a human-readable output,
one-hot encoding, and
all the transformations
is happening behind the scene.
But when we return you the output,
this is what you care about, right?
You can save the model by "save_model()".
And when you save model,
it's not just estimator,
it's all the steps in the pipeline.
So in this case, we haven't
done any normalization,
scaling, transformation,
feature engineering,
that's why there are few transformers
that are there by default.
So, for example, Simple_Imputer
is there by default.
The last parameter here would be "model".
So this, this is "model", right?
But if you were to do
normalization, transformation,
or anything in the setup stage,
you would basically have a long list
of transformers in the pipeline.
And they are automatically organized,
which basically means all you care about
is just calling the
"predict_model()" function
from the Pipeline.
All the orchestration
is happening internally.
And again, you can load
it from "load_model()",
and if you load it, this
is that loaded Pipeline.
All you need to do
is call the "predict_model()"
of this Pipeline
to generate inference.
You can deploy your
model on AWS, Azure, GCP,
with this one word
"deploy_model()" function.
So if I run this,
it would basically take
the entire pipeline,
and push it to this bucket.
And now you have a separate
Python process sitting,
or R process sitting somewhere,
but you would just load this
pipeline from this bucket
here in AWS, and call your
inference off this pipeline.
Very quickly, I would show another demo,
which is pretty similar, but
this time I'll show you...
So, I have this dataset here,
"AirPassengers" dataset.
This is a Time Series dataset.
Unlike the last example, the
order of rows in this dataset
is important, although there
is a new Time Series module
coming in PyCaret.
But today, if you wanna
handle Time Series data,
you can do it using
PyCaret's Regression module.
And that's what I'm gonna show now.
Let's do some visualization.
Let's just plot data on line graph.
I've created a moving
average for 12 months just
to see the incline, just
to see the trend of data.
So you can see, this is how it looks like.
We have time period from 1949 to 60 here.
We have this seasonality
that we see in the blue line.
This is actual numbers.
And this red line is a moving
average of actual numbers.
So this time what's
different is, you cannot
directly consume dates in
your machine learning models.
What you have to do is extract
the features out from "Date".
So things like month, year,
a continuous sequence of day,
of number, to capture the trend.
So that's exactly what I
have done in this part.
So now, after running this code, you see
this is how my dataset
looks like I have "Series",
which has just a
continuous sequence number,
"Year", "Month", and "Passengers".
And I've just dropped the "Date" column.
I'm now gonna split my dataset
into "train" and "test"
based on the "Year".
I wanna keep all the rows
before 1960 to "train",
and last year, 12 points
of 1960 as my "test" set.
Here you go, 12.
What's different this time
is when you initiate this...
This is exactly same as we
have seen in previous example.
But when you execute this,
you have to basically tell PyCaret
that "fold_strategy = 'timeseries'",
because the way you
would do cross-validation
for your Time Series data,
versus non-Time Series
is little bit different.
You cannot randomly shuffle that.
So this indicator would tell PyCaret,
"Okay it's Time Series data, and I cannot
do random shuffling."
And I would just add one more thing here,
is "log_experiment = true"
Oops.
And let's say "experiment_name
= 'airpassengers'".
And this time it didn't
ask for that confirmation,
because I have passed "silent = True".
Imagine if you didn't have "silent" here,
it would ask for this, right?
And when you are running this code
on Terminal, command
line, or in production
you don't wanna have
this confirmation box.
So what you would do,
you would explicitly define
your data types here.
Like this one, "numeric_features",
I have basically explicitly told PyCaret
that this is Numeric feature,
and you can say "silent = True"
because you don't wanna
press Enter in production.
(Moez clears throat)
Same thing.
I'll do the same, same function.
"compare_models(sort = 'MAE')".
So now this is fitting tenfold-
Fitting threefold Time
Series cross-validation.
Why three?
Because in our "setup()" code,
we have asked it to do "fold = 3".
10 would take a little
bit more time than this.
Okay. So now behind the scene,
this is running same thing.
This is running bunch of
estimators and fitting three folds,
and giving you cross-validated metrics.
And by the way, while that's complete,
you can use the same, exact
same thing on GPU as well.
There is a flag called "GPU",
which is set to "False".
You can pass "use_gpu = True",
and all the workload would fall on GPU
if you have a compatible NVIDIA GPU.
Okay, let's see what's the best model.
So this time you can see, we
have this model, which is-
Least Angle Regression is
our best model, but this
is wrapped around a
PowerTransformedTargetRegressor.
And the reason for that is,
I have passed "transform_target = True".
And the reason I did that is
because the mean of this data
is not stationary.
So I wanted to transform my target values.
By default, PyCaret use
Box Cox transformation
to transform the target values,
and then when you call the
inference of this model,
it would inverse the transform values
and return you the actual original scale,
because that's what you care about.
But behind the scene, it would
handle that. (clears throat)
Okay.
Again, I'm going to do a "pred_holdout"
25, not bad. I think the
cross-validation is 22.3.
Again, you can do "evaluate_model()"
to kind of learn the Residuals
or Feature Importance if it's available.
You can see it has big
bunch of seasonality here.
Let me just quickly plot.
So this is-
You see, red line is the prediction
that the model has learned from data.
The blue line is "Passengers".
And if I just do a visual
test, it looks okay.
Towards the end, this section
here is the "Test Set",
I've just create a backdrop
for you to notice that.
So it looks like a good fit to me,
to generate the prediction also.
For Time Series, instead of-
You don't have x variables
that would come in
and you would "predict_model()", right?
So what I do is I created
a future data frame.
My data...
Training data ends in 1960.
I've created a data frame
for next five years,
which is nothing but just
the same thing, right?
1961, one and continuous series.
I would finalize my model.
And I would just call the
"predict_model()" function of it.
Now I have future values for this dataset.
And if I just quickly
show you on the plot,
this is how it looks like.
Again, not bad, I would say.
But, one thing to note
here is we have done this
"log_experiment = True"
in our "setup()" function,
if you remember.
Where is the "setup()" function?
There. "log_experiment = True",
"experiment_name = 'airpassengers'"
So what happened behind the scene is-
If I initiate an MLflow
server, on my local host
you would see there would
be an experiment here,
called "airpassengers".
And in this "airpassengers", we have...
Let me zoom out.
We have all the models that we trained
using the "compare_models()".
So you have all the models here.
And if you just open one model,
let's say Least Angle Regression,
you would see date, timestamps,
bunch some of important information,
but also all the
hyperparameters of this model,
all the metrics of this model.
You'd have artifacts-
The grid that you see on the screen
is logged as an HTML file,
so if you wanna connect
your downstream systems with this.
But also you would have this model here
that you can serve right from here, right?
Now, one more-
I would skip Demo 3 in
the interest of time.
What I would do is-
You can use actually-
At this point, you cannot
use the MLOps capability
because I haven't-
My backend URI is a file system.
But I have an example here, despite here,
which is also uploaded on GitHub is,
I have created this Python script,
where I'm importing "regression",
I'm loading same "insurance" dataset.
Now I'm setting my tracking URI,
which is for setting it to "mlruns".
So by default, it would create
a database, SQLite database.
It would use that to record
the metrics and artifacts.
And I have this simple
script, where I've created
"create_model()" in a loop.
And what I wanna show you is,
if I now open my command line.
Let's navigate to this,
and run this script, "demo4.py",
what it would do is it would
now set the tracking URI
and run this script for PyCaret.
It would initialize the "setup()",
it would create models,
and notice that I have
"silent = True" here,
because this is a command line script.
Okay. So it has a started working.
Let's give it a minute.
Okay.
And one thing different is,
notice that how I open
my MLflow UI in Notebook,
with this command here
"!mlflow ui", right?
What is different when
you set tracking URI,
is you have to initiate
your MLflow UI server
with this command here,
which kind of points
to a backend store URI.
All right.
Okay. It seems like it's done almost.
Okay. And now I'm gonna
just copy this code here.
And I would initiate my
URI with the backend store.
And when I do that, if
I open my local host...
Oops.
I have to kill my previous MLflow server.
"activate pycaret-dev".
"cd pycaret/pycaret".
Initiate the server.
Okay.
Now that I have it running on my local.
Okay.
So "Insurance Demo 4", for some reason,
it's not showing up here,
I think have conflicting MLflow servers
running on my computer.
But essentially, it would
initiate another experiment.
And this time around, you would be able
to use the register model
functionality of MLflow,
which basically pushes your
model as an API on your local.
Let me just try a last time,
and if it doesn't work,
we'll start taking questions.
- Sometimes the demo gods go against us.
- [Moez] Yeah. Yeah, totally.
This is the time for that.
- I wouldn't worry about it.
- I think, I think what
happened just right now,
is because I have my other screen,
and I have a "localhost:5000"
MLflow running there,
which I didn't end,
and because I'm running
another local host with
with the backend, I
think it's conflicting.
The only problem is I don't
see the new experiment there.
But in a normal world
this would work for you.
- Or you can start the MLflow
server with a different port,
and then you wouldn't
have a conflicting port.
But I think you showed
the gist of the idea.
- Mm-hm. Yeah.
All right.
Okay. Do we have questions?
- Okay. Thanks a lot, Moez.
I think that was a flawless demo,
except for the conflicting ports.
But I think you pretty much
demonstrated how easy it is
for PyCaret to be able to
actually integrate in MLflow.
And all you actually had to do
for all of those experiments,
is just provide that experiment name.
And when you do that, the code behind
creates MLflow experiments,
and then starts and runs,
and tracks everything, correct?
- Right. That's correct.
- Okay. So the integration
is just like taking,
using the API to give
the log experiment name,
and that just automatically seamlessly
creates the tracking experiments for you.
- That's it.
- Brilliant. Love it.
Love it.
Okay. Question for you, Moez.
Does PyCaret support embeddings
for categorical features?
If I have a categorical
feature, how do I embed that?
- At the moment, the
only type of importing
you can do in Pycaret,
is one-hot encoding,
but we have plan to integrate things
like Weight of Evidence
and target encodings.
- And are you still limited
to use Python 3.6 for PyCaret?
Or are you gonna be
updating to 3.7 or 3.8?
- I think it works fine with...
I'm using 3.8. It works
fine with 3.6, 3.7, 3.8.
- And then earlier on, you actually showed
the comparison of the
number of lines of code
you actually need with
scikit-learn, and then with PyCaret.
Do you have some similar
comparisons say, with AutoGluon?
- No.
- And is AutoGluon
supported in your PyCaret?
- I'm not sure.
- Okay.
I believe AutoGluon is
AutoML library, right?
- Yeah.
It's a version of Gluon with
a little bit of AutoGluon.
So what is the difference
about doing Time Series in the
future version with PyCaret?
Does it actually run through more models?
- Yeah. So what I've shown you
today using Regression module
is all the regression models, right?
In future, for a separate
Time Series model
that we are going after,
it would also include
classical statistical methods
like (indistinct), vector autoregression,
and it would also then
include the regression models.
Our Model Zoo would be more comprehensive,
and we would have some
Time Series-specific
plotting functionalities,
like ACF, PACF plots,
decomp plots, and stuff like that.
- Yeah. Fantastic show, Moez.
A lot of people actually
giving a lot of compliments.
I think I don't see any
more questions over here.
I think there's definitely
a lot of interest
that people will be downloading your links
and having a go at it.
And the fact that it's-
Okay. Here's...
How does PyCaret handle unbalanced data?
- Good question. There is a
parameter in the "setup()".
It's called "fix_imbalance".
When you set it to "True", by default,
PyCaret will use this mode.
But there's another parameter in "setup()"
which is "imbalance_type",
where you can pass your own class
if you wanna do
undersampling, oversampling,
or even a hybrid.
As long as your classes are
compatible with "imblearn",
which is the library
for handling imbalance,
we would use those transformers.
Just like you would
use outside of PyCaret.
- Okay.
Does PyCaret currently
support or is planning
to support distributed training
with SPARK or PySpark ML?
- Yeah. Yeah.
I just put a poll yesterday on PyCaret.
- Yes, I saw that.
I saw that on PyCaret. I was
the first one to say yes.
Go ahead.
- And so we are figuring out...
So we have a very solid
integration with Ray
for our distributor need.
And in fact, one of our
core contributor is now
working in Ray as well.
So we have very solid-
So initially we were thinking
we would use either DAS or Ray
or any pythonic infrastructure,
but now we see a greater need
of having a library for SPARK exclusively.
So we are talking about that.
- What's the future for
PyCaret functions for NLP?
- At the moment our NLP is
very limited to topic modeling.
So you have classical things
like LDA, NNMF, factorization.
So only topic modeling, but...
And we don't have a very, to
be honest, we don't have a very
strong roadmap for NLP because I think NLP
has taken its own journey
in the last one, two years
with these Transformers.
I think there's lot of awesome
community work being done
exclusively on NLP, so it's not kind of
our focus at the moment for PyCaret.
- Okay.
And then we'll take the last
question before we head out
to the next speaker.
Thanks, Moez, you showed
a great demo, can you-
If I was using an SQL backend server
installed on the server,
how would I actually use that with MLflow?
I think I can answer that question,
but you can confirm that, Moez.
I think all you have to do
when you're actually doing that
is to provide the tracking URI
to that particular backend server.
And I think the experiment, set experiment
would actually take care of that, right?
- Exactly. So I've just posted this.
So, if you notice on my Visual Studio,
Demo 4, just before we set up
function, I have passed this
"mlflow set tracking uri" and
I have passed this for SQLite.
You would replace this code
with your tracking URI,
and PyCaret is smart enough
to kind of detect that.
- Okay. Well, thanks a lot, Moez.
We appreciate it. I think we've
answered all the questions.
In interest of time, we will
move on to our next speaker.
- Thanks, everyone.
- Thanks a lot. And continue
with your great integrations.
And send me the slides if you can
so I can upload it on LinkedIn.
So, send me the most latest one
that you actually presented today.
Thanks a lot, Moez.
- All right.
- Okay. Moving on.
Our next speaker, is my pleasure
to really introduce you to.
Geeta Chauhan.
Geeta Chauhan is the lead
PyTorch contributor for MLflow.
She leads the Partner
Engineering Team at Facebook AI,
and she has been quite
instrumental and lead
in contributing a lot of
integrations with MLflow.
She works very closely with
our MLflow Engineering team
and without any further ado,
Geeta, please tell us all the integrations
with PyTorch and MLflow.
What happened in the
past, what's currently,
and what we are thinking
of doing in the future.
Take it away Geeta.
- Thanks a lot, Jules, for
that lovely introduction
and Moez, thank you for
your wonderful presentation.
So let me share my screen.
So as Jules mentioned,
I lead the PyTorch Partner
Engineering at Facebook.
And today I'm going to talk
to you about "Reproducible
AI using PyTorch and MLflow".
So this is work that started last year.
We did the phase one launch of it
at the PyTorch Developer
Day back in November.
And now we're working on the next batch
of new feature integrations.
So we'll look at what are the challenges
for reproducible AI, then we will look
at how MLflow and PyTorch
as a combination solve this.
And then we'll go into a few demos.
So, unlike traditional
software, machine learning
involves a very continuous
iterative process,
where one is optimizing
for metric-like accuracy.
The quality of the results
depends on the data
and the tuning of the parameters.
Experiment tracking, if you
don't have a tool like MLflow,
is quite difficult to do.
You run into challenges, like
over time your data changes,
you face problems like model drift,
and the performance of
your model degrades,
so you have to continuously
monitor and upgrade your models.
Additionally, if you don't
bundle all the model artifacts
along with the model itself,
like the vocabulary files
in the case of NLP
models, they can get lost,
especially if your teams
are growing or, you know,
people are leaving the
company, maybe all the code
doesn't reside on GitHub or
your internal source code repos,
so it's very difficult to
track and trace all of that.
When you're deploying
your models in production,
you have to combine and
compare many libraries together
to get the optimal results.
And it is can be quite
challenging, especially due to
the very large number of
diverse deployment environments.
Same thing when you're training,
you have to mix and
match so many libraries.
So, you know, this continues
to be a moving target
and a big challenge that
people have to solve.
So luckily with the integration
of MLflow and PyTorch,
many of these challenges can be addressed.
So as you guys already know,
MLflow has these great features
for the model tracking,
all your experiment tracking,
you have the MLflow project,
where you can package all your
code into a reproducible run.
You have the model registry,
where you can store all
your published models and any
artifacts associated with it.
And the MLflow models,
which now provides options
to deploy your models
into the diverse serving environments.
And we did integrations
into all of these areas
with PyTorch.
This was part of the first launch
that we did back in November.
So we added the PyTorch
autologging support
with the MLflow tracking.
So here we use the PyTorch
Lightning as the training loop,
and we integrated autologging with it.
All these samples that we
have provided for PyTorch
have the MLflow project with it.
So, you know, all you have to
do is just call "mlflow run"
and, you know, you can get everything.
Everything is parametrized and very easy
to reproduce for your teams.
In the model registry, we added support
for saving the artifacts,
including the ability to save
and load TorchScripted models.
So Python supports both eager
and TorchScripted models,
and you can save those in
the MLflow model registry.
And lastly, we integrated the TorchServe,
as an MLflow deployment plugin,
so you can train your
models and then deploy them
using this plugin seamlessly
from the MLflow itself.
So let's look at some of these features.
So on the autologging side,
it is really very simple.
All you have to do is
"import mlflow.pytorch"
and call this one line to
start your autologging,
"mlflow.pytorch.autolog()".
And then you will have your
training loop like before.
So in this example, what you're seeing is
PyTorch Lightning training
for an MNIST classifier.
So you set up the model, you
set up the data module for it,
and you define the things
like "early_stopping",
"checkpoint_callbacks".
And you can then, you know,
do your model training
and get the results.
So autologging will automatically log
the relevant parameters for
each of these scenarios.
So if you're doing early
stopping, et cetera.
So all the parameters get logged.
So the learning rate,
things like model summary,
optimizer name, min delta, best score,
all these get logged.
You also have the ability to control
how often you want to do the logging.
You can do it every n iterations
instead of doing it at every iteration.
And you can define your own
custom user-defined metrics,
like F1 score or test accuracy.
This is what the comparison
of the experiment looks like
in the MLflow UI.
So this is one of the real great features
that I love about MLflow.
You can quickly, you know,
as you run your multiple experiments,
you can select them and do
the comparison and review
what is the changes across the parameters,
the metrics, you have all these plots,
the scatterplot, contour
plot, and parallel coordinates
to give you the full detailed analysis.
So what you see here is an
example of iterative pruning,
and you're seeing the contour
plot corresponding to it.
For the save artifacts, we added support
for saving the additional
artifacts, like, you know,
the vocabulary file.
If you are deploying your
models for production,
you may have additional
configuration files
like your class mapping file.
And in the case of TorchServe,
you will also need,
if your model requires
any additional packages,
then you can also bundle
your requirements,
store text, and provide
those additional extra files
to go along with it.
And for TorchScripting models,
again it is very easy to log,
you just call the "torch.jit.script()",
get the scripted version
of the model, and then
you call "mlflow.pytorch.log_model()"
and pass the scripted model,
and it will save it so, you know,
you don't have to do anything special,
both eager mode and
TorchScripted mode are supported.
And for those of you who
are new to TorchScripting,
it is basically a serialized
and optimized version
of a PyTorch model that runs
in a Python-free process.
So you can run it with a C++ backend.
That is how a lot of the
production deployments
happen inside Facebook.
You can also run it with a Python backend,
as in the case of TorchServe,
where it will give you a
lot more optimized speed.
We have seen speed-ups of
up to 4x for BERT models
when using TorchScript
versus the eager mode.
So this is a big picture
overview of TorchServe,
which is the model serving solution
that we jointly launched
with AWS, last April.
It is for serving PyTorch models.
It provides you all the default
handlers for common ML tasks
like image segmentation,
text classification.
You can write your own custom handlers.
We give a very nice
out-of-the box Model Zoo
to quickly get started.
It has a robust management
API, the inference API,
and the metrics API supports
dynamic auto-batching,
versioning, rollback support.
You can log all the common
metrics for performance tracking.
It has integrations with many
of the deployment solutions
like SageMaker, Kubernetes, MLflow,
and you can track all your metrics
using Prometheus and Grafana.
So this is the deployment
plugin with MLflow.
We provided both a command
line interface, as well as
you can run it, you know,
through the Python API.
You can have a local
instance of TorchServe
or a remote instance of
TorchServe, and it runs for both.
So you just call this
"mlflow deployments create"
to deploy the model and
to do the predictions.
You will call "mlflow
deployments predict",
and run your predictions on that.
So this is when you're calling
through the command line.
And below that is the code for the actual,
if you're calling it through the API.
So in the case of API, you will call...
You will get the deployment client,
and then call the "create_deployment()"
and the "plugin.predict()"
to run your prediction.
So some of the new features
that we are currently working on.
First one is support for Captum
for model interpretability.
So Captum is the model
interpretability library
for PyTorch, and it supports
attribution algorithms
for interpreting, you know,
you can do the interpretation
of your predicted outputs
with respects to the input.
You can do the prediction of
the outputs with respect to any
of the layers in your model,
and the neurons with
respects to the input.
And out of the box, we
provide many algorithms
for gradient- and
motivation-based approaches.
And the picture that you
see on the right-hand side,
it is a example of a
multi-model situation,
where you have both an image and a text.
So the stripes of the zebra
are what characterize that,
the image as a zebra and
in the word, the sentence,
what is on the picture,
the "is" and the "picture"
are more important,
and they get highlighted in green.
So these are the algorithms
that are supported for Captum.
You have the attribute
model output with respect
to the input features
on the left-hand side,
and with respect to the
layers on the right-hand side.
You may be familiar with
many of these already,
like the SHAP method,
DeepLift, Shapely, Saliency,
all these are supported.
And the good news is that
it is very easy to use.
All these algorithms
follow the same pattern,
so all you have to do is
import the, you know, like
call this "from captum.attr
import (the relevant algorithm)"
and then pass your model
to it and give it a input
and a baseline, and then call
the "attr_algo.attribute()"
and get the output results.
So in the new features integration,
we are adding support for Captum,
for model interpretability
during your experiment,
when you're training your model.
So this is already,
examples for this are already available
in the core MLflow repo.
We have added support for Ax/Botorch
for hyperparameter optimization.
And we are in the works for
the Model Signature support
in the MLflow TorchServe plugin
and for the model interpretability
on the TorchServe side
that is also in the works.
So the code example that you
see on the right-hand side
at the top is the code for the Captum.
As I mentioned, you just call
your "IntegratedGradient()"
and pass the model to it.
And then you pass the
inputs and get the results,
and then you can do the
visualizations for it.
And you can log all of this at present
as artifacts in MLflow.
And in future versions,
we will add support
for visualizations inside
the MLflow UI itself.
And at the bottom, you are
seeing some example code
for the model signature.
So with this, we will use the
MLflow Model Signature feature
and we are integrating that
in the TorchServe side.
So if you have saved the
signature of your models,
all the inputs and outputs, you know,
then we can do a validation of the inputs
with respect to the model signature,
and throw errors in case
the inputs do not match
the signature that was
specified for the model.
And the bottom left side
is the visualization that,
for the Captum results.
So for the model experimentation
and the Ax/Botorch,
I just wanted to give you
a view of how we use this
adaptive experimentation using
Ax/Botorch inside Facebook.
So what you see here is an
example of how we use it
for our newsfeed and
ranking of the content
across a variety of signals.
So we combine it using
a collection of rules,
like from a configuration
policy, and it is dynamic,
so we mix the results from
both the offline simulations
as well as the A/B testing.
And then we get the actual
results with the multitask model,
and we can optimize and
using the Ax/Botorch
for the best parameters, and use those
and get the next best
experiment candidates,
and iterate on that.
So now you can use the same
for hyperparameter
optimization using Ax/Botorch
with the MLflow.
So you set up the Ax client,
then you create the experiment,
you set up all the parameters
that you want to optimize on.
So here we are looking
at the "weight_decay"
and the "momentum".
And then we run the actual trials
and we log the best parameters in the end.
So let me dive into a few demos now.
Are you able to see my screen?
So this here is my server
running in the cloud,
and I just ran a iteration
for the Ax experimentation.
And while it is running,
maybe I can just show you
what the output looks like.
So at the end, you will
see all the trials,
so these are the Ax trials.
You can do the comparison
across the trials.
So the "momentum", "weight_decay",
these are the ones
that we are finding the
best parameters for.
And what you will notice is we are using
this "parent and child"
scoping feature of MLflow.
So the main process that
launched it as the parent ID,
and all the trials get
logged as the child.
And if you go and see the
results on the parent,
you will see that we have
logged the "optimum_lr",
the "optimum_momentum" and
the "optimum_weight_decay"
out of all the trials that were run.
And you can get all the details
about your model summary,
your data, et cetera.
The other demo I wanted to show you is,
I recorded it just so that
things work fine in the demo.
So this is for the Captum integration.
So for this one, we are using
a MLflow server hosted on AWS.
So we are setting the MLflow tracking URI
and the MLflow experiment
environment variables.
And then we call "mlflow run"
with the relevant parameters,
in this case, the "max_epoch",
and we just launch that.
And you will be able
to see the actual runs.
Why don't I show this to you here?
So this is the Captum run,
and it is saving all the artifacts.
So, in this particular case,
all these Feature Importance
with respect to Neuron,
the Feature Importance,
Neuron Importance, all
these are saved as images.
And in the future version, we are working,
discussing with the MLflow
team on how we can have this
as a better feature in the
MLflow experiment UI itself.
So if you wanna find
out more about Captum,
you should go to the captum.ai tutorials
and we have a great set of tutorials
for a variety of examples,
including Visual Question and Answering,
Recommender Models, BERT models.
So please take a look at that.
For the Ax/Botorch, go to ax.dev.
This is where all the
Hyperparameter Optimization details
are available.
And let me walk you through
the upcoming feature
that we are going to integrate soon.
This is for the Captum
Insights for model serving.
So this will come in the
MLflow TorchServe repo.
So what you see here is
again, a BERT example,
and we are calling using Captum
for the Integrated Gradients.
We are doing the computation.
So this here is the actual model for it.
We've set up the wrapper for it.
The tokenizer is specified.
And this is the scoring
function that we are using.
And this is where the Captum
attribution is getting called,
and then you can visualize it.
So we have given a
Jupyter Notebook for you
to generate these nice visualizations.
You can see that in the Jupyter Notebook.
And then when you integrate
it with TorchServe,
this is where we are creating
the TorchServe deployment.
And then we are going to have a new CLI
for the MLflow deployments,
called "explain".
So similar to "predict",
"predict" will be for
calling the inference
and "explain" is for getting
the interpretability results.
And when you call that,
then you will get the output
of the model interpretability
for your prediction.
And again, you can visualize the output
of that particular JSON
using a Jupyter Notebook.
So all these demos for
the main PyTorch demos
are available under
"mlflow/mlflow/examples/pytorch".
So you will see the Ax
Hyperparameter Optimization example,
the Captum one that we
just walked through,
and the iterative pruning,
which is also using
the Hyperparameter Optimization.
And let's look at some
of the code in detail.
So this is the Ax Hyperparameter
Optimization example.
We have set up the Ax client,
then we pass all the parameters
that we want to optimize
for the "lr", "weight_decay"
and "momentum".
And we are calling the trials in the loop,
and then we log the "best_parameters".
So these are the best
parameters that you saw.
In the iterative pruning, again,
we are using Ax/Botorch for this,
and we are doing optimization
for reducing the overall size
of the model, pruning
it to have a smaller,
compressed footprint of the model
when you deploy your model for production.
So here again, we have some parameters
that we are optimizing on,
and we are calling the
PyTorch pruning library,
and doing the iterative pruning for that.
So this is the "iterative_prune()",
we are pruning the model and saving it.
And the Captum.
This is for the Titanic database example.
It's a simple neural network.
We have all the training
and the test features.
So it is using the PyTorch
Lightning training loop.
And then we have the
visualization piece for it.
This is the actual training.
And this is where we are doing
the Captum feature conductance,
and the Layer Conductance
to get the interpret,
and the Neuron Conductance,
the three things that we had saved.
Okay.
So now, this is the complete workflow
that you will have for MLOps
using MLflow + PyTorch.
So as a data scientist,
you will build your PyTorch
models for autologging and all.
You will use the PyTorch
Lightning training loop.
You can train the model either
on a single device, on a single
machine, your CPU or GPU,
or you can do distributed training
which is just a matter of
passing the extra parameters
needed for your MLflow
Lightning training run.
And then when you're ready
with your experiments,
you will optimize the
model and TorchScript it,
and you will log the
TorchScripted model along with
your other previous experiment
runs into the Model Registry.
And from there, you will use
the MLflow TorchServe plugin
to deploy the models.
And then you can use the full features
on the TorchServe side
for your model management,
or the inference prediction-
The explainability, all the metrics
and everything from the
TorchServe main interfaces itself.
And these here are the
next set of features
that we are currently working on.
So the Jupyter Notebook that I showed you
for the Captum
Interpretability for Inference
in mlflow-torchserve.
So that PR is under review and
should get integrated soon.
We are discussing with the MLflow team
on how to do the Captum
Insights visualization
in the MLflow UI itself.
And we are also working on
the new PyTorch Profiler,
which was launched with the 1.8.1 release.
It has a new Tensorboard,
PyTorch Tensorboard Profiler plugin,
that you can visualize from Tensorboard.
And we are working with Jules and team
to figure out how we can
integrate this into the MLflow UI.
And we will continue to add
more examples for you guys
to quickly get started
on a variety of use cases
that would be relevant for
you in MLflow and PyTorch.
So, you know, please
reach out to us on GitHub,
engage with us on Discourse
and Slack channels.
If there are any specific features
that you would like to see,
please open up GitHub issues around that,
and give us feedback.
So here are some references for you.
Please check out the the
PyTorch MLflow Autolog blog.
We will be publishing
a new series of blogs
in this coming, this month and next month.
So please look out for those
on the PyTorch Medium channel.
You have the MLflow
TorchServe deployment plugin,
all the examples, the links for that.
And I quickly walked you through
the captum.ai, Ax, and Botorch sites.
And one more thing I wanted to mention,
is that we are, you know, we
have been working with arXiv
to see how we can make model
research reproducibility
an important item.
So what we have done
is, we worked with arXiv
and integrated all the code
in the arXiv site itself.
So if your paper is published with code,
you can now see the
corresponding code for it
on the arXiv site itself.
So we are investing
heavily on how to promote
reproducibility in the research community.
So this is one of the key
efforts that we did last year,
and we are continuing to work
with the major research conferences,
like NeurIPS, et cetera,
to have a reproducibility checklist.
So I have added some links for those,
for the reproducibility checklist to,
for you guys to think about
when you are building new models,
how to make it easier for other people
to adopt and reproduce the results.
So please use this in
conjunction with MLflow
to get great results across your teams.
This is my contact information,
in case you guys would
like to connect with me.
So I'll open it up for questions now.
- All right. Thank you, Geeta.
Thanks for giving us both
the past, the present,
and the future integration with PyTorch.
We are actually come a long way,
and has just been an incredible extension
of how PyTorch actually,
with MLflow extends the
ability for machine learning
and data scientists to actually
use those in conjunction.
And I was quite happy to see
that you could actually
reproduce all the examples
that were uploaded in
Mlflow, as a MLflow project.
So you can just run MLflow, run project,
and you actually get all the experiments.
So thanks a lot for that wonderful tour
of the MLflow integrations with PyTorch,
and what are some of the exciting
ones which are coming up.
We're really looking
forward to have the profiler
and the Tensorboard as
part of the integration
with MLflow UI, and the ones
that we are currently working on
to get the PR merged in.
Couple of questions.
I think this is for both of,
I don't know if Moez is there,
I think he's gone, but a question is,
"Is it possible to train
PyTorch models in PyCaret?"
The PyCaret guy is not here,
so I can't really answer for that,
but I'll try to get an answer for that
and put it on the Slack channel.
Let me see, do I actually
have any other questions
over here on Q&A?
Seems like I don't have
anything on LinkedIn.
Let me see on the YouTube.
If we have any questions over there.
A lot of compliments.
Great demo. Lot of work.
Yeah. We'll be posting out.
We will including the link
presentation and all that
in the YouTube description.
So once we actually get
the PDFs and all that,
we will update that.
No, I don't see any more questions.
So with that, I think we can,
we can probably conclude
this presentation.
Thanks a lot, Geeta.
Thanks a lot, Moez, in his absence.
You guys actually showed
a great, great way
to show that having MLflow allow you
to extend with the larger ML ecosystem,
and this is really proof
that MLflow is very adaptable
in terms of integrating
with a large ecosystem.
So thanks a lot, Geeta.
And thanks a lot, Moez, who has left.
I think he had another
meeting to attend to.
And if you want to see,
as Geeta pointed out,
if you actually want
to see more integration
from the PyTorch side, log
those issues on the MLflow.
And I meet with Geeta on
weekly basis to make sure
that we hear to the community,
what they actually want.
So if you want to see more of integrations
with other aspects of
PyTorch, let us know.
- Thanks so much, Jules.
And I just, I'll say another
thanks to Geeta and Moez
for these awesome presentations, and-
So the video recording is
available up on YouTube now,
and I posted that link in the
LinkedIn and the Zoom chat.
So I'll also update the description there
with all the resource links and the slides
when I get them.
So thanks, everyone for joining us
and I hope you have a
great rest of your day.
Take care.
- And thanks a lot.
And wherever you are, stay healthy.
- [Geeta] Thanks. Bye-bye.