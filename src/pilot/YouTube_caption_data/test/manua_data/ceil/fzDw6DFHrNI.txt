[MUSIC PLAYING]
SACHIN GUPTA: Hello.
I'm Sachin Gupta, VP GM for
Infrastructure at Google Cloud.
The roles of enterprise
architects and developers
are evolving.
Not only do you have
to keep the lights on,
you're expected to stay on top
of the ever-changing trends
and technologies that
create business value.
And you have to do all
of this while lowering
costs and improving performance
so IT infrastructure runs
quickly and smoothly.
Your role is critical to a
successful transformation.
For example,
adopting technologies
like AI/ML and containers is
becoming vital to businesses,
with more than 76% of
surveyed enterprises saying
that AI projects are
their top priority.
And meanwhile, security breaches
are so common these days
it doesn't even
make the top news.
They are disruptive
and costly and can
be avoided with the right
preventative methods.
After all, as Gartner
says, 99% of cloud breaches
are due to human error.
With Google Cloud, you can
innovate faster and more
easily while optimizing costs.
We know organizations
like yours still
have a lot of
infrastructure to migrate,
and we are committed to helping
you migrate more securely
and efficiently.
Global customers
and local partners
like Palo Alto Networks, H&M,
and Major League Baseball
rely on us to deliver scalable,
high-performing, highly
reliable cloud
infrastructure and services.
A big area we're investing
in is the expansion
of our global footprint to
meet the unprecedented global
customer demand.
Today I'll be
discussing how we are
partnering with you
to help drive business
value in three key ways.
First, we are driving
business transformation
and achieving new
outcomes with industry
leading AI/ML
unparalleled security
and modern infrastructure
services and solutions that
are designed for your industry.
Second, we are helping
you optimize your workload
performance while
reducing costs.
And finally, from
migration to management,
our mission is to help you
unlock this value simply
and easily.
Customers come to Google Cloud
to transform and innovate.
Let me share a
little more about how
we are driving this
change through AI
leadership, invisible security,
and cutting-edge industry
solutions
AI is in our DNA,
from AI-powered search
to YouTube recommendation
engines and Google Assistant.
We have decades of experience
running scaled, diverse ML
workloads and industry-leading
AI infrastructure
products and solutions.
Wayfair is using Vertex AI
to forecast global customer
demand, ensuring customers can
quickly access what they need
and to automate and personalize
AI-powered customer support.
Salesforce is using
performance-optimized Cloud TPU
v4 for conversational
scale out AI.
These outcomes are made possible
because of the innovation
across our AI stack.
And it starts with hardware
choices and performance
that help you keep pushing the
limits of AI in large models.
Cloud TPU v4 delivers
industry leading ML
training performance and scale.
With six terabits per
second interconnect,
you can run large-scale
training workloads
up to 80% faster and
up to 50% cheaper
compared to alternatives.
And that's how
companies like Cohere
deliver cutting-edge
natural language processing
faster and with a
lower carbon footprint.
We're also announcing
new A2-ultra GPUs,
built on NVIDIA'S A100, 80 gig
GPUs with high-speed memory.
AI Singapore has reduced the
loading time of large-scale
language models by 40% and
increase throughput by over 50%
with A2+, resulting in
increased productivity.
And customers are also
using Google Batch
to orchestrate and schedule
AI jobs of any scale.
With Batch, our
customer Locomation
was able to unlock AI insights
from their autonomous trucks
80% faster.
Google is committed to making
AI and machine learning
more open and accessible.
To further this, in
partnership with Meta,
we recently co-founded
the PyTorch foundation.
And for over a decade,
we've contributed
to critical AI projects
like TensorFlow and JAX.
Today, we are announcing
a new industry consortium,
the OpenXLA project, that will
unite an ecosystem of leading
machine learning
compiler technologies
and accelerate and simplify
machine learning innovation.
These open-source
API contributions
enable you to take your
AI idea and turn it
into reality, easily
and at low cost.
Next, I want to share how
we're transforming security.
At Google Cloud,
we are championing
a future of invisible security,
where security is engineered in
and operations are simplified.
We package the
expertise that we use
to protect our own business
and our billions of users
and make it available to you.
You can easily deploy
a wide range of tools
depending on your
own risk profile,
from prevention to
detection to remediation.
Today, I want to
highlight the next step
in our cybersecurity
journey as we welcome
Mandiant to Google Cloud.
By taking advantage of Google
Cloud's existing security
portfolio, our Google
cybersecurity action
team, and Mandiant's leading
cyberthreat intelligence,
you can stay protected at
every stage of the security
lifecycle.
Cloud Armor is another
security innovation
that provides advanced
ML-powered DDoS and web
protection for web apps,
services, and APIs.
It has prevented some of
the largest DDoS attacks
on the planet with zero
impact to customers.
Recently, the
largest HTTPS attack
was staged against a
Cloud Armor customer.
It was 76% larger than
anything previously
reported, the equivalent of
Wikipedia's daily requests
in 10 seconds.
And the customer
experienced no impact.
And for regulated
industries with
stringent and
country-specific requirements,
we offer controls to meet your
digital sovereignty objectives.
Sovereign controls
allows you to define
the location of your core
data, set access permissions,
and control your
cryptographic keys.
Supervised Cloud, which is
coming soon, is a fully partner
managed and operated
solution that supports
data, operational sovereignty
needs, and country or region
specific regulatory
requirements.
For highly sensitive
workloads that
require the most stringent
security requirements,
Hosted Cloud offers air-gapped
hardware and software,
including managed
infrastructure, AI/ML,
and database services.
Since transformation
takes different forms
for different
industries, we partner
with customers to build
industry-leading innovative
solutions.
Together with the
CME Group, we plan
to transform the derivatives
market through technology,
expanding access, and
creating efficiencies
for market participants.
In telecom, communication
service providers
like Bell Canada rely
on Google's network
to expand globally and deploy
5G networks with Google
Distributed Cloud Edge.
Google Distributed
Cloud Edge GPU-optimized
configurations bring the power
of GPU acceleration and machine
learning to enable
the future of retail.
Customers and partners, such
as 66degrees, AWM Smart Shelf,
and Ipsotek, are
using GPU optimization
to deliver innovative
retail solutions
at the edge, including
AR in the store,
shelf stock out notifications
for quicker restocking,
and cashier-less
checkout to reduce lines.
In media and entertainment,
we provide solutions
to customers like
U-Next, with streaming
built on the same Google
infrastructure we've
tested and tuned to
serve YouTube's 2
billion users globally.
To get a better picture of how
our media and entertainment
industry customers
innovate with Google Cloud,
I'm proud to introduce
Senior Vice President -
Technical Infrastructure of
Major League Baseball, Truman
Boyes.
[MUSIC PLAYING]
TRUMAN BOYES: Major League
Baseball's technology mission
is to connect with our fans.
Our infrastructure
team has historically
maintained applications
on-prem, and now we
have unlimited compute
from the public cloud.
And this allowed us to shut
down four data centers,
modernize all of
our infrastructure,
and to spin things up rapidly.
And in the off-season,
we scale it back.
Google's Cloud helps us to
understand the entire fan
journey, and
artificial intelligence
allows us to derive a
better connection to them.
We're able to have personalized
content with that fan.
It gets richer over time as
we learn more about them.
Working with Google,
we're preserving
the history of baseball,
going back into the 1940s.
And we're able to
make highlights
available to our fans.
Google Cloud hosts all of
these video clips for us.
And now we have an opportunity
to enrich this format
beyond what it is today.
We're looking to modernize the
entire platform that we have
and move into delivery
through Media CDN.
Major League Baseball
and Google Cloud
are connecting with our fans.
That experience is happening
in venue as well as
a digital experience.
And we're knocking
it out of the park.
[MUSIC PLAYING]
SACHIN GUPTA: Thank
you so much, Truman.
It is amazing to see how MLB
is innovating for the fan
experience, leveraging Google
technologies like AI, Media
CDN, and the reliability
and elasticity
of our global infrastructure.
We've shared a number
of ways in which we've
built our infrastructure
to enable transformation.
But we also continue to
build solutions and products
tuned to support your
top workloads and data
applications.
And we've optimized these for
both performance and cost.
One example of this is
Google Cloud VMware Engine.
VMware Engine is a fully
managed, native Google service
that helps you lift and shift
your VMware applications
to Google Cloud
faster and easier.
We are the first
external provider
to support VMware's
Cloud Universal
program, which makes
it easier for you
to migrate to the cloud.
And with built-in point
and click migration tools
and our instant
provisioning feature,
you can get workloads
running in your private cloud
in less than 1 hour.
Customers like LIQ have saved
60% in infrastructure costs
with VMware Engine by
migrating 80% of their business
applications and half
of their databases.
We are continuing to
scale VMware Engine
by increasing support from 64
to 96 nodes per private cloud,
all with four nines of
availability in a single site
and a dedicated 100
gigabit per second network.
This provides high
performance and reliability
for your most demanding
production workloads.
It is no wonder that
customers like Mitel
could overhaul their Unified
Communications platform
and data infrastructure
with VMware
Engine in less than 90 days.
Another example of
workload optimization
is a recently announced
Google Cloud High Performance
Computing Toolkit.
Several decades ago, quantum
computing was just a concept.
But now, with HPC
Toolkit, quantum AI
is easy and accessible.
HPC Toolkit is an
open-source tool from Google
that helps you quickly
and easily create
repeatable, turnkey
HPC clusters.
The Toolkit comes with several
blueprints and broad support
for third-party components such
as the Slurm Scheduler, Intel
DAOS, and DDN Lustre storage.
Next, I'm really excited
to announce C3 VMs.
It is the first VM on the
market to feature the latest
generation of Intel
Sapphire Rapids processors
and is built on
new Intel, Google
co-designed infrastructure
processing units, or IPUs.
All of this together means
differentiated performance,
security, isolation,
and flexibility.
C3 is the first VM in our
fleet with 200 gigabits
per second, low
latency networking,
to support a variety
of workloads,
such as data
processing, web serving,
and high throughput
HPC workloads.
Because clusters can be scaled
and paralyzed more densely,
we're seeing
customers and partners
like Ansys and Snapchat
completing jobs faster.
And Parallel Works is seeing
10x faster performance with C3
compared to the
prior generation.
Contact your sales rep to
join our private preview.
Moving on to another product
built to leverage the IPU,
Google Cloud Hyperdisk
is the next generation
of block storage, which will
be available on both Compute
Engine and GKE.
We are decoupling block storage
performance from the VM,
allowing you to tune
your storage performance
to your workload needs.
We estimate you'll see
around 50% better total cost
of ownership than
Persistent Disk
and 80% higher IOPS
per CPU compared
to any other hyperscaler.
We have built cost optimization
into many of our core products,
and we have exciting new
capabilities to announce.
Our new Flexible Committed
Use Discounts, or Flex CUDs,
can make it easier to save
and manage costs across teams
by giving you region and
VM family flexibility.
With Autoclass,
customers like Redivis
are reducing storage costs
and achieving better price
predictability in a simple way.
It automatically
transitions objects
to cooler storage, based on the
last time they were accessed,
and transitions to standard
storage upon access.
That brings us to the third way
we drive business value, ease
of use.
As cloud platforms have
become more versatile,
they often have also become more
complex to adopt and operate.
That's why Google Cloud
strives for radical simplicity,
from migration
through management.
Speaking of migration,
our new Migration Center
can reduce complexity,
time, and cost,
by providing key capabilities
in migrating and modernizing,
to virtual machines, containers,
or serverless computing.
With Migration Center,
Viant, a large media company,
in partnership with
Slalom, successfully
migrated an entire data
center to Google Cloud
in less than six months.
We also have a new offering
in our mainframe modernization
solution called Dual Run.
Dual Run lets you
replicate your mainframe
workload in Google
Cloud and run the two
environments in parallel.
This allows you to confirm
successful operations in Google
Cloud before your cutover,
which can massively reduce risk.
And that's why customers and
partners across industries,
like financial services
company Santander,
are seeing success
with Dual Run.
We also want to simplify the
way you manage and scale.
Managed instance groups,
or MIGs, with autoscaling
use application metrics to
radically simplify and improve
operational efficiency,
allowing you to scale in and out
without manual intervention.
And with the power
of Google's ML,
MIGs can predictably
scale in and out
based on historical data.
These three defining pillars
for Google Cloud infrastructure,
transformative,
optimized, and easy,
are the key tenets behind
our intentional engineering
efforts.
This is why so many
customers trust Google Cloud
and what helps to power such
innovation across the industry.
I invite you to try
Google Cloud and
our innovative new releases.
We look forward
to delighting you.
Thank you.
[MUSIC PLAYING]
SHAQUILLE O'NEAL: Hi.
I'm Shaquille O'Neal, and I'm
the founder of Big Chicken.
JOSH HALPERN: When you're trying
to build a national chain,
communication is so critical.
To do that, you need
a great partner.
And we're really lucky that
partner is Google Workspace.
SHAQUILLE O'NEAL: Google
Calendar is my girlfriend.
I don't know anything I'm doing
unless I talk to my woman.
Google Workspace.
Productivity and collaboration
tools for all the ways
you work.
[MUSIC PLAYING]
PATHIK SHARMA: Hello, everyone.
Thank you all for
joining us today.
I hope you are enjoying
Google Cloud Next.
Welcome to this
session on how to lower
your cost on Google Cloud.
We have a panel of
experts today to talk
about top-leading practices
for cloud cost optimization.
My name is Pathik, and
I lead cost optimization
in cloud FinOp digital
transformation practice.
Joining me here
is Yasmin, who is
Product Lead for Cost
Optimization on Google Cloud
Compute Services, and
Courtney, who leads
cloud FinOps at General Mills.
We have a packed agenda today
to talk about real world stories
on how General Mills approach
the discipline of cost savings
and accelerated their
adoption of cloud FinOps.
Next up, we will touch upon
the broader landscape and top
ways other cloud customers
are optimizing their span
and finally wrap up the session
with the newest features
and announcements
coming out at this Next.
Before we dive
right in, Courtney,
can you tell us a bit
more about General Mills?
COURTNEY BORMANN:
Absolutely, Pathik.
General Mills has
been making world--
food the world loves
for more than 150 years.
The brand you see
down in the corner
are just a handful
that feed families
across the globe
each and every day.
And our team of 36,000+
employees are passionate about
making food the world loves.
This mission comes to
life in everything we do,
including our Cloud
transformation journey.
We view Cloud as a key
business accelerator.
It's a way that we're able to
enhance operational insights
by connecting data
both internally
as well as externally to build
more personalized products,
make faster supply and
demand planning decisions,
and provide better service and
reliabilities to our customers
and partners.
It's also driving
efficiencies for us,
creating a scalable IT
platform at a competitive cost
and unlocking new
services and offerings,
allowing us to utilize
cloud-native capabilities
to build new digital
services and offerings
in an agile manner.
The graphs below help to
really highlight the journey
General Mills has been on.
As you can see, we started
in 2020 with just 5%
of our footprint on Cloud.
We're now around
35% and anticipating
by 2024 that will be at 65%.
So we're really in that
sharp growth curve right now.
And it's been
critical as our cloud
spending is growing to grow our
governance right along with it.
And FinOps is a critical
part of that growth.
The FinOps team is
focused on ensuring
we help to drive
waste out of our cloud
usage at General Mills.
YASMIN MOWAFY: It's great
to see, Courtney, that you
have a team focused on this.
So speaking of
efficiencies, what
has been your biggest
driver of cost optimization
and efficiency?
COURTNEY BORMANN: Yeah.
For us, that's
really been committed
use discounts, Yasmin.
And that's CUDs for short.
So we're migrating our
ERP systems to GCP,
and these CUDs have allowed
us to optimize our data center
bill by more than 50%.
Resource-based CUDs
are a really good fit
for optimization
on our machines,
because we know they
need to run 24/7 in order
to serve our global business.
Once we were confident
in the machine sizes,
putting the long-term CUDs
on these stable workloads
has been a big cost savings win.
Committing to resources
for a long-time horizon,
though, isn't a decision
that we make lightly.
We have a robust
governance process
around these commitments, with
cross-functional stakeholders
weighing in and a
formal purchase approval
process before we buy.
In addition to CUDs,
we're also starting
to dip our toes into another
rate optimization tool, which
is BigQuery Slots Reservations.
And these are really good
on our analytic workloads.
We're taking advantage
of this opportunity
for our non-prod projects
today and excited to expand
into our prod projects
in the near future.
While CUDs and Slots
are a great optimization
fit for several of our projects,
they aren't right for all.
And we are striving
for flexibility.
I was excited to hear
that the Google team has
a new type of CUD that's
going to be available
soon that's even more flexible.
YASMIN MOWAFY: That
is right, Courtney.
We are so happy to announce
the general availability
of a new type of a
spend-based commitment
for Compute Engine called
Flexible Committed Use
Discounts, or Flex
CUDs for short.
Flex CUDs will help
customers save up to 46%
off the on-demand GCE VM pricing
in exchange for one or three
year commitments.
Flex CUDs are ideal
for predictable
spend across GCE,
JKE, and Dataproc.
Those discounts will
just automatically
apply to most general purpose
and compute optimized VMs
in any region, any machine
family, any operating system.
So there's a great deal
of flexibility here.
And it also works across all
projects within the billing
account.
So whether your workload
requirements change,
you expand
geographically, or you
want to upgrade to the latest
and greatest VM we have,
Flex CUDs will just provide
you a simple and easy way
to save money and manage spend.
So Courtney, you've told us
about how rate optimization has
been very effective at General
Mills for saving costs using
CUDs or using BigQuery slots.
I'm curious.
What other ways have you
been saving costs around?
COURTNEY BORMANN:
Yeah, absolutely.
One of the other really
critical things that we've done
is start to look at
cost controls, Yasmin.
And it is, of
course, easy to get
excited about lowering the bill
and put some spend mitigation
on the back burner.
But unfortunately,
we learned this one
the hard way with one
project accruing over
$80,000 of anomalous
charges due to a lack
of partitioning and clustering
of a BigQuery data set.
The quotas that we now
have in place effectively
limit the amount of both
individuals and a team
can spend in a
rolling 24 hour basis.
Most of our projects
have $1,000 limit,
so obviously a much
lower ceiling if you do
have that oops moment.
My advice is really to
teams prioritize quotas now.
Prioritize cost controls.
Don't wait for a costly
financial mistake.
PATHIK SHARMA: Thanks for
sharing the lessons learned.
And I love how you strengthen
the overall process
while going through this.
We've been seeing
some of our customers
create a cookbook of recipes
such as these cost control
policies and design an
end-to-end automation
workflow to handle
these ops moments,
like this oops moments at scale.
Since we are on the
topic of BigQuery
and our customers
love using BigQuery,
I'm curious to see if you
find any other opportunities
in your data analytics workload.
COURTNEY BORMANN: Yeah,
absolutely, Pathik.
We are also heavy BigQuery
users at General Mills.
And one area where we have found
a lot of savings on BigQuery
is through optimizing
our data backup costs.
We started by digging
into policy requirements
and realized that we
had been capturing
a snapshot each day of the
month, so usually around 30.
By examining the policy as
well as our business need,
we were able to move to just
a rolling seven day backup
and then some weekly snapshots.
This allowed us to drop
nearly 20 additional backup
copies per month that
we had been using.
And the team has
really been awesome.
They've taken it even
one step further here
and are currently deploying the
table snapshot functionality,
which allows us to just capture
that incremental data that
has changed and
further reduce costs.
This work also has
the added benefit
of providing more
transparency to the teams.
We're moving from
one central backup
project to having that
distributed across
all the individual
consuming projects.
It's been a really big win that
way in terms of transparency
and really fabulous work by
our overall engineering team
to bring this to life.
And it's hard to argue
with savings of over 70%.
So really fabulous
work by the team.
PATHIK SHARMA: I love the
numbers you just shared,
and kudos to the
team for achieving
this impressive results.
Let me throw a
curveball question here.
We have been increasingly
seeing customers
use cloud-native architectures,
and serverless services just
play a very critical role there.
Have General Mills
adopted serverless?
Any thoughts you would
like to share us there?
COURTNEY BORMANN:
Yeah, absolutely.
So there's been a
couple of things
that I think we
can share on this,
a couple of quick
examples of teams that,
through a willingness to really
explore other tools to still
meet their business
deliverables,
have been able to achieve
some significant cost
savings, Pathik.
Two quick examples here.
The first is the
use of BI Engine
to redirect the majority
of one of our project teams
report query usage to BI
Engine from BigQuery database
processing.
The BI Engine provides
a fixed monthly cost
that was 80% less than
running the same processing
through BigQuery.
We have another group
who's also been working
with the new Composer
functionality that
has autoscaling built in.
And this team, through that
autoscaling functionality,
has been able to drive 60% out
of our Composer costs as well.
So another really
significant win
for the team through
that willingness
to think about trade-offs
in different services
that can still meet
our business need.
YASMIN MOWAFY: 60%, 80%, those
are great success stories.
Wow.
I love that you're using
all of these techniques
to ensure that you're
spending your money
on the right resources, like
to achieve the right business
results.
COURTNEY BORMANN:
Absolutely, Yasmin.
And the team has done some
really impressive work,
but we are still, I would
say, relatively early
in our Cloud journey.
So I'd love to hear from you.
What are other teams doing?
What are some other best
practices out there?
YASMIN MOWAFY: Absolutely.
So we see really
a whole spectrum
of cost optimization strategies
for customer with varying
levels of effort and savings.
What you see here on the
screen are the top 10 ways
our customers have
been leveraging
to lower their cloud costs.
Depending upon the use
case, some strategies
are going to provide highest
value with minimal effort.
Think about purchasing
flexible use
discounts or the regular
committed use discounts,
reserving BigQuery slots.
So similar to what you've been
doing at General Mills, really.
There are other
strategies that can
be employed during the
architecture and design phase.
So think before even deploying
your workload to the cloud,
autoscaling, choosing the
custom VMs to rightsize
your machines based on
your workload requirements.
Other strategies will work great
if you are into automation,
like turning off idle resources,
rightsizing your VMs post
deployment, and also setting up
appropriate storage lifecycle
management policies.
So Pathik, can you tell us
what else have you been seeing?
PATHIK SHARMA: Yeah.
I think-- I think you
covered a broad spectrum.
What we've been seeing
is a couple examples
come to my mind.
We've been seeing customers who
use fault-tolerant workloads.
They tend to leverage
Spot VMs, which
will help them save up to 91%
in cost savings for compute.
And just like General Mills, we
have been increasingly seeing
our Google Cloud
customers leverage
Google managed services,
serverless services,
like BigQuery, Spanner,
Bigtable, Cloud SQL, and more,
to not only reduce the
total cost of ownership,
but also instill and propel
that developer productivity
and go to market,
which is so much needed
in this dynamic environment.
Talking about more cost--
more ways to save
cost, Yasmin, can you
please tell us
what's coming out?
YASMIN MOWAFY: Oh, yeah.
So that's really exciting.
That's the fun part.
So I'm going to share with
you some products that we
are releasing and
announcing this Next.
So starting with products that
fit under the category of cost
visibility and insights, we are
introducing a new cost modeling
service to help customers model
out and forecast their cost
for their upcoming workloads
before deploying it
to the cloud.
We're also enhancing our
cost anomalies detection
capabilities to provide
proactive monitoring
and governance to
inform customers
about any out of the
ordinary spending
to avoid those oops moments
you've talked about, Courtney.
We're also introducing a new
Google Cloud Storage Insights
service that allows customers
to monitor the object, age,
and size trends to
forecast and control cost.
And finally, using BigQuery
partitioning and clustering
recommender, this
can help customers
save workload execution costs
by providing physical design
optimization recommendations
that customers
can apply to their tables.
So Pathik, can you walk us
through the new launches
in the cost optimization
recommendation section?
PATHIK SHARMA: Absolutely.
On the cost optimization
and recommendation side,
we are introducing
Autoclass, a new managed
service for Cloud Storage.
If I'm allowed to pick my
favorite, it would be this one.
Autoclass is a simple
bucket-level setting
that automates the lifecycle
management of objects.
It greatly reduces
the storage cost
by automatically
migrating objects
between warmer storage
and colder storage
with the most favorable pricing.
Next up is Cloud Spanner.
And this team had a lot
of great announcement
this year, starting with
granular instance sizing, which
allows customers
to provision 1/10
of the size of current
node at 1/10 of the price.
Now this is game changer, not
only for the bigger teams,
but also for the smaller
teams to unlock creativity,
especially those who
are tight on budget.
Next in Spanner is
increased storage capacity
per node, which helps you
achieve up to 50% savings
and compute costs for
storage intensive workloads.
And as part of our growing
portfolio of community use
discounts, Cloud
Spanner now supports
CUD for one year and
three year commitments,
which help you save up to 40%
compared to on-demand rates.
Finally in this category,
we are excited to introduce
Google Cloud Hyperdisk, the next
generation of block storage.
This is cool.
Cloud Hyperdisk decouples
the block storage performance
from the VM.
So you can tune your
storage performance
to your workload needs
to achieve higher IOPS
and throughput
performance independent
of virtual machines.
Cloud Hyperdisk can deliver
up to 60% better total cost
of ownership than the previous
generations of Persistent Disk.
I am looking forward to see
the reaction and feedback
from General Mills
and our customers
as they adopt these
exciting features.
YASMIN MOWAFY: That's right.
We're very excited here.
There's a lot of great
features coming out
on top of our already
expansive portfolio of ways
to save costs.
So Pathik, can you walk
us through how can we
make this an easy
journey for our customers
so it doesn't seem so daunting?
PATHIK SHARMA: Yeah, absolutely.
And this is where I'm
super excited to share
how Active Assist brings
the data, intelligence,
and machine learning to not
only proactively optimize cost,
but also improve the
availability, the performance,
the reliability, as well as go
green through sustainability.
To showcase how broad and
deep the portfolio is,
you can see that Active
Assist runs through nearly all
of Google Cloud.
Focusing on cost
intelligence, Active Assist
does a lot of heavy lifting
understanding your usage
patterns and assessing
the impact of billing
to provide you with the most
optimal cost recommendations.
For example, based on
your workload usage,
Active Assist can provide
you easy to understand
and actionable committed use
discounts recommendations
and BigQuery slots
recommendations.
Active Assist will also
service idle Cloud resources,
otherwise known as
Cloudbased, and rightsizing
opportunities for
virtual machines,
databases, and more, to
further lower your cost.
Now these recommendations are
available through Console, API,
and BigQuery export.
We have seen customers use
this for automation use cases.
The Active Assist team
has been hard at work
listening to our
customers rolling out
these relevant features.
I highly encourage you to check
out our public documentation
to keep up to date on this.
Now before we wrap up
this session, Courtney,
any final piece
of advice that you
would like to share
with us on driving
cost optimization at scale?
COURTNEY BORMANN: Yeah,
absolutely, Pathik.
Thank you for that.
And it's been really great to
share some of the early wins
that General Mills
is seeing and excited
about all the new features
that you and Yasmin covered
that are coming out.
But I do want to just take
a moment to really emphasize
the importance of culture.
And so as important as
it is to have real time
visibility and reporting, have
a central FinOps team that also
is going to be focused
on best practice sharing
and optimization practices
across organizations,
it's really all of the
individual users that
come together that make the
difference on whether or not
an organization will
be successful or not.
So when we're looking at this,
one of the critical things
to think about is how can you
incentivize the organization
to really adopt FinOps.
And one thing that
we've done around
that that's been successful
is a little bit silly,
but each month we recognize
a FinOps all-star.
And this all-star could
either be an individual
or a small team that's done some
great work driving waste out
of their cloud usage.
And we reward them with
a highly coveted Yeti mug
and put their faces, of course,
on a cheesy FinOps all-star.
But it's been such a fun
way to really incentivize
those new habits and just
make it an embedded part
of the General Mills culture.
PATHIK SHARMA: Agree.
Everyone has a
responsibility to play here.
And I love how you instilled
that cost-conscious behavior
through gamification.
Speaking of gamification,
if you are a developer,
don't forget to check
out the Developer Zone,
where we have immersive
experience for you
to engage with the community,
complete the challenges,
and earn badges that will
stay on your developer profile
forever.
I would like to thank
Yasmin and Courtney
for sharing your expertise.
And I hope you've
enjoyed this session.
Thank you for
listening to us, and I
hope you have fun learning
more at this Google Cloud Next.
[MUSIC PLAYING]
PRIYANKA VERGADIA: Hi.
I'm Priyanka Vergadia,
staff developer
advocate here at Google Cloud.
And you are watching
the session that's
going to teach you all about
the secrets of migrating
with speed, scale, and success.
And with me today we
have Jaspal Sawhney,
who is the VP of
SRE and Engineering
at Loblaw Technology,
and Daniel Dill,
who is the Senior Vice President
of Application Delivery
and Cloud Operations at
Global Payments Systems.
I'm very excited to have a
discussion with both of you
today about how you all migrated
your on-premise infrastructure
into Google Cloud.
But before we do that, we have
a few exciting announcements
to share.
We just recently
launched Dual Run,
which is a new
service that's part
of Google Cloud's mainframe
modernization solution.
It enables you to
simultaneously run workloads
on your existing mainframes
and on Google Cloud that
allows you to perform
real-time testing
and quickly gather data on
performance and stability
with no disruption
to the business.
So do check it out if you're
running mainframe systems today
and are looking to migrate
some of those mainframes
in near term into Cloud.
Now the next thing
we are announcing
is the Migration
Center, which focuses
on helping you reduce
your migration project's
complexity, time,
and cost by providing
a centralized, integrated
migration and modernization
experience.
If you are migrating,
these are the two tools
that are definitely going to be
great to have in your toolkit.
With that in mind, we have a
great segue to our main event
today that I am very,
very excited about,
which is our customer
panel on migration.
So with that, both Jaspal and
Daniel, I want to, one by one,
kick it off to you to explain a
little bit about your company.
Let's start with you, Jaspal.
Explain about Loblaw
Technology and why
did you decide to migrate
in the first place.
JASPAL SAWHNEY: Loblaw
is the biggest retailer
in Canada, 2,500 stores.
We are a hundred year
old plus company.
What we found as we were
trying to scale our businesses
is that we wanted to spend
our energy towards building
products instead of having
to manage infrastructure.
That's not our core competency.
And that kind of underpinned
the whole decision as to why go.
There was also an aspect
of developer experience,
which we wanted to get at.
We wanted to bring
our teams together so
that a developer and
a marketer should
be able to go have
the conversation using
the same set of tools and
get a faster time to market.
PRIYANKA VERGADIA: Daniel, I
would love to switch to you.
What was the motivation behind
migrating in the first place?
DANIEL DILL: Thanks, Priyanka.
Global Payments, we're
an industry leader
in payments, payroll, and
point of sale technologies.
We've grown a lot
through acquisition.
And with those app--
with that acquisition comes a
large number of data centers.
So one of the last
numbers I saw,
we had somewhere around,
I think, 70 data centers.
And that's just a large
capital investment,
both to keep those up to
date and also the staff
required to run those in a
secure, compliant manner.
Since we're in the
payments industry,
we have to follow all
the PCI guidelines,
and everything needs
to be super secure.
And so it takes a lot to keep
just equipment refreshed.
There's also a big driver
to get out of that rat
race of equipment refreshes.
It takes a lot of high risk
maintenance activities.
And so you're just
continuously churning,
doing the same
functions over and over.
And so we really wanted to get
our workforce, our engineers
out of that and let
them innovate and do
more interesting things than
just refresh in the life
equipment inside a data center.
PRIYANKA VERGADIA: OK, great.
So this question
is not a surprise.
I was definitely
going to ask this one.
Why did you pick Google Cloud?
Jaspal, why don't
we start with you.
JASPAL SAWHNEY: As I mentioned,
we wanted to keep our teams
and bring our teams closer.
So that was one.
The other aspect of it was
we wanted to build things
right from the get go and
not be just inheriting
ways of doing things and
replicating our auth structure.
I feel those were the
two main drivers for us
to pick up Google Cloud.
Daniel?
DANIEL DILL: We already had
a number of environments
running in Google Cloud.
We had a merchant
portal that's used
for servicing our customers.
We have a data lake,
a main API platform.
So we were already familiar with
the Google Cloud technology.
Also when we put out
the RFP for the program,
we were really
impressed with what
Google brought to the table.
They've invested a lot in
their Google Cloud technology.
And then also Google was
willing to use our service.
So we're processing
Google transactions
on Google, Google Cloud.
So Google on Google,
you might say.
And we're just
we're really excited
about the innovative capability
between the two companies
that we can use to help our
current merchants growth.
PRIYANKA VERGADIA: It's
like a 360 degree connection
from different angles.
Amazing to hear.
So now that we've
covered what made
you decide to migrate in the
first place, the background
of why to mi-- why migration
in the first place,
and then why picking
Google Cloud,
naturally, I want to
progress towards how did you
actually accomplish
the task of migrating.
Because that must have been
a long process in some ways.
Was it short?
I'm curious about that too.
But let's start
with you, Daniel.
How did Global Payments
migrate to Google Cloud?
DANIEL DILL: I think we have
taken every approach out there.
In some of our
first initiatives,
we used very much a
lift and shift approach.
So the end result
really looked and felt
like it was just another
one of our data centers.
It was just running
in Google Cloud.
The most recent initiative that
we just wrapped up last week
actually, we took a
little different approach,
where we had one large segment.
It was about 30 applications,
where we called it--
we did a lift and shine.
So we modernized some
services along the way.
We still use the m4
CE tool and we still
cloned some of the systems.
But we also left behind all of
that on-prem technology that
we're all used to--
we all know, and we
all manage it on-prem.
We left that behind.
So we did plenty in-- plenty
in Terraform's infrastructure
as code.
We also had another
application stack.
It was just perfect
for modernization.
So it was completely modernized,
all using paths services.
And at the end of the day,
we took this hybrid approach,
because that was what was
really best for the initiative
to exit that data center.
Jaspal?
JASPAL SAWHNEY: For us, I think
it was more driven from what
we were trying to achieve.
Part of that was we wanted
to keep our cost of ownership
still within our control.
So there was a whole premise
of that we would only do this
if it keeps us just at par with
what we are right now spending.
There was also an aspect
or a time in between
when we are migrating
wherein we tell
our teams that because you're
building net new systems,
do not replicate your
data center architectures.
Go and pick products which
are the right tools for you.
We have done a lot
of lift and shine.
We have used some of the
stacks currently around m4c
for what we're doing on stores.
But it's all around.
It's all over the
block, I would say.
PRIYANKA VERGADIA: Now
that you have actually
executed these migrations
and some of them
are still happening
now, what are
some of the most impressive
results that you've
seen so far?
JASPAL SAWHNEY: I think
two come to my mind.
2018, when we migrated our
grocery business right out
of the park, one week
after the migration,
we could see a 50%,
40% improvement
in just performance
of the stack.
The other example
which I would share
is a recent one around stores.
What we're doing, we migrated
our stores to Google Cloud.
And we have had
absolutely no incident.
And this is coming
from a world wherein
when stacks were
running in the store,
there was constant feeding
and nourishing which
was needed for those stacks.
Daniel?
DANIEL DILL: For us, it was
really two things as well.
One, the speed to deliver.
I've been very impressed
with how fast the teams have
been able to do migrations
and have a working
environment ready to go.
I mean, I'd be
hesitant to say that we
could do-- we could work at the
same speed in an on-prem data
center.
I just-- I don't
think we could do it.
The other is the
operational visibility.
It's amazing to me.
Not a week goes by that an
engineer doesn't come to me
and shows me something new, some
new widget, some new monitoring
technology, something that
helps them do their job better.
PRIYANKA VERGADIA: Great.
That is amazing insight.
Now if you were to--
there are lots of companies
trying to do migration.
So if they were to
take something away
from this session, what would be
one of the best or the biggest
piece of advice
that you would like
to offer to somebody who
is looking to migrate?
Daniel?
DANIEL DILL: I think
it's the assessment
and the planning is
the most important.
As much time as possible
that you can put in
to doing an assessment and
putting together the migration
plan, the more it's going
to pay off in the end.
Jaspal?
JASPAL SAWHNEY: For me,
it's actually the opposite.
I would say adhere to the
principle of failing fast.
So try create enough room
for your teams to stumble.
There's going to be surprises.
You would get to understand
your teams and your culture
via this activity very clearly.
And just start early
and then stay put.
PRIYANKA VERGADIA: Two very
different approaches again.
But I think it does
depend on the team
that people are working with
to apply whichever one that
makes sense in their scenario.
Great.
So with that, we are
close of our time.
I want to remind you that
the announcements that we've
made around Dual Run, which
is the mainframe modernisation
feature, and the
Migration Center, you
can see the links
to those if you
want to learn more about them.
There's a lot more
detail at those links.
And with that, I want to
thank you, Daniel and Jaspal,
for joining us today and sharing
such great insights about how
you all ended up migrating.
It was so unique
and so different
from both of your perspectives.
And I think that is what
made this discussion very
enlightening for me, as
well as for our audience.
And with that, I want to
thank you for listening,
and enjoy the rest of Next '22.
[MUSIC PLAYING]
SPEAKER 1: All of a
sudden, the entire world
is connected, and with it,
the opportunity for hacking.
SPEAKER 2: So this was a group
that we had been following
and that we knew was a threat.
SPEAKER 3: The attacker's
after something,
and you want to find
out what they're after.
SPEAKER 4: Remove their power,
contain them, and then put them
out.
SPEAKER 5: We want to
change the battlefield.
[MUSIC PLAYING]
PHIL VENABLES: Our
mission is to protect
the safety of all
the data we manage
for all of the billions of users
and customers of Google Cloud,
whether it's health,
energy, transport, finance,
public sector organizations.
We make sure that we defend
and protect that every day,
keep it secure, keep it private.
I'm Phil Venables.
I'm the Chief
Information Security
Officer for Google Cloud.
[MUSIC PLAYING]
PHIL VENABLES: When we think
about defending the cloud,
it's very much the
same as defending
all of the rest of Google.
We have various different
groups inside Google overall
that are working together
to protect our customers.
Threat Analysis Group
tracks attackers,
analyzing threat actors
that are developing
techniques against us.
SHANE HUNTLEY: There's
many other teams
that build defensive systems,
build software, manage
the firewalls, all
these other tasks.
Our job is to really understand
what the threats are,
provide that ground
truth that allows
us to really focus the security
efforts of the wider team.
PHIL VENABLES: When you
understand your attacker's
motivation, how their
techniques are evolving,
you can feel comfortable
that your defenses are
evolving to meet that
and stay ahead of that.
Then we have Detection
and Response.
Every single day monitoring
our entire environment,
looking for signs of attacks.
HEATHER ADKINS: Our focus is
on gathering the information we
need to put the story together.
Is there an attacker here?
And if there is, then we
activate our response team.
PHIL VENABLES: We
like to think they're
like a digital immune system.
The more you can get information
about what's going on
will be better defended.
HEATHER ADKINS: Every day,
every hour of the day,
100% dedicated.
PHIL VENABLES: Just all
about how we try and stay
ahead of that threat.
Red Team, it's really important
to aggressively test ourselves.
So we have some of the
world's best attackers
that are working for us.
How would they go
about attacking things?
DANIEL FABIAN: With every
exercise that we run,
the number of things
that an attacker can do
becomes less and less.
PHIL VENABLES: We all look at
the output of those exercises
and determine if
there are things
that we can build into
the cloud products
so that they can get defended
from the lessons learned.
And then we also
spend a lot of time
working with external
researchers, the so-called Bug
Hunters.
If they find an issue
with any of our products,
they can notify us of that.
We fix that.
CHRISTOPH KERN: In
order to prevent errors,
you have to study them.
Bug Hunters play
an important role
in looking for
bugs from all kinds
of different perspectives, which
is really, really valuable.
If you're coming
from the outside,
you might notice something that
somebody who's on the inside
might have actually not noticed.
PHIL VENABLES: If
that vulnerability
is discovered despite
the best efforts of all
of our organizations, you want
that discovered by somebody
that's going to tell you.
Then we have Project Zero,
active vulnerability research,
looking at where the
vulnerabilities exist,
not just in Google products,
but in other products as well.
TIM WILLIS: We don't
really care if you're
working on another platform.
Your security is
important enough to us
that we're going
to invest in that.
PHIL VENABLES: We have to
think about securing the cloud
overall, not just Google Cloud.
We're giving away our
hard earned experience.
We'd rather do that because
it defends everybody.
More and more organizations
are moving to the cloud.
Our job is to deeply
partner with our customers
and their IT and
their security teams
to help them secure
things in the right way
to get their
businesses operating,
their mission satisfied,
without having
to worry about the
detail of the technology
and how to defend it.
[MUSIC PLAYING]
GREGORY LEBOVITZ: Welcome.
So Nelly and I are going to be
spending the next 20 minutes
or so talking through some
key elements of how to secure
your cloud infrastructure.
And by that, we
mean your network,
your data, and your compute,
and how to do it the Google way.
So what do we mean by
that, the Google way?
So Google secures some of the
largest web apps and services
on Earth.
We've learned a few
things along the way
about how to secure this
cloud infrastructure.
And so in Google
Cloud, in particular,
we applied our learnings to
enable some infrastructure
security for you, some that
we've sort of built by default
for you, and other controls
that we've made available to you
optionally.
Today we're going to share about
some of those security controls
and how they work without
compromising performance.
And that's what we
mean by the Google way.
So we're going to cover
the topics in this order.
I'll cover networking,
and then Nellie's
going to cover data and compute.
Google Cloud offers a
robust set of security tools
that help you secure your
cloud infrastructure.
So first, at the
internet edge, we
provide this family of
load balancers and CDNs
for availability and
scalability and performance.
And then built in right along
that stack are several security
tools for access control and for
privacy and for authentication.
To these, we add tools
with deeper context
and specialized Layer 7 threat
understanding for things
like APIs, fraud from bots,
and secure application access.
And then inside your
cloud environment,
we have fully distributed set
of access control capabilities,
for both workloads and
for our Cloud Services.
And when I say
Cloud Services here,
I mean things like BigQuery
and Storage and Cloud Run.
And then we provide
a full complement
of network level telemetry.
So taking it a step further, we
have industry-leading advanced
threat detection capability.
But if raw packets
are what you want,
we provide full packet
inspection services
with our packet
mirroring capability.
And that can be sent to a number
of different third-party tools
that you can find
in our Marketplace.
All the alerts in the
telemetry from these products
are integrated into our threat
hunting, alert management,
and unified security systems.
Plus we also integrate with
third-party SIEM and SOAR
systems.
Of those, we're going to focus
in on three product areas
today, Cloud Firewall,
Cloud IDS, and Cloud Armor.
So let's start with
Cloud Firewall.
We are announcing some
significant enhancements
and expansions to our
Cloud Firewall offering.
To set the context,
Google Cloud Firewall
helps you achieve a zero trust
posture via a cloud native,
fully distributed
firewall service
with advanced
protection capabilities,
and granular controls.
You can even achieve
intra-subnet.
Yeah, intra-subnet,
micro segmentation that
is applied completely
independently
of the network structure.
We recently launched
some new enhancements
that simplify configuration and
deployment while simultaneously
helping you improve
network security posture
and do so with Cloud scaling.
We've introduced
these capabilities
in two service tiers,
Essentials and Standard,
which is in preview.
Essentials is the
service tier that
forms the foundation of
the Cloud Firewall family.
The enhancements here
include a new construct
for firewall rules, called
Network Firewall Policies.
We have both global
and regional of those.
Also here are
secure tags, objects
that can be used in your
rules that are IAM governed
and also address Groups.
Taken together, these
features simultaneously
help you increase
policy precision
and simplify rule
creation and operation.
You normally don't get
those two things together.
And then there is the new
Cloud Firewall Standard,
which as I mentioned,
is currently in preview.
Standard introduces
an expanded set
of source and destination
objects for firewall rules
that are dynamic.
They are built and
automatically updated by Google.
These are Google Cloud
Threat Intelligence,
which I'll cover in a
little bit more detail
in a moment, domain-name, or
FQDN, based dynamic objects,
and geolocation-based
dynamic objects.
The combination of IAM governed
secure tags in Essentials
and the dynamic objects
in the new Standard
tier and our existing
hierarchical firewall
rules help you run a very
dynamic, least privilege,
self-service environment
that enforces pinpoint policy
with greater simplicity and
decreased operational cycles.
So I just mentioned Google
Cloud Threat Intelligence
and how it's a set of objects
for rules in Firewall Standard.
Well, those same objects are
also available in Cloud Armor
and are going to be available
for future products.
Today Threat Intelligence
includes five types of objects.
Some are meant for blocking,
like IPs with poor reputation
and Tor exit nodes.
And some of them are
meant for allowing,
things like search
engines, public clouds,
and upstream providers.
And we'll be adding
many more types
to the Threat Intelligence
platform over time.
So speaking of Cloud Armor,
let's go deeper on that right
now.
Cloud Armor is our
service that protects
your websites, your web
services, and your APIs
against advanced, targeted,
and automated DDoS
and Layer 7 attacks and also
from fraud from bots using
reCAPTCHA.
You may leverage this
enterprise-grade ML powered
defense wherever your
application is deployed.
So that could be on
premise or in colo,
in any of our Google
Cloud regions,
or actually any public cloud
provider for that matter.
Cloud Armor attaches
to the load balancer
and provides Layer 3 and 4,
all the way up to Layer 7
application layer protection,
and access controls
to the backend services.
Cloud Armor security
policies can
be configured to enforce
rate limits, IP geo-
or ASN-based access, as well
as pre-configured, fully
customized WAF style
application protection rules.
This includes our
machine learning based
adaptive protection capability.
Now we've recently expanded
significantly the coverage
that Cloud Armor can provide.
And we've done so
in two big ways.
The first is for traffic that's
passing through our TCP and SSL
proxy load balancers,
so basically addressing
other types of TCP
applications that
might be not HTTP or HTTPS
or non-standard of those.
The second is for traffic
heading to our CDNs.
So this is going to include our
Cloud CDN and also our recently
announced Media CDN.
With these two types,
we significantly
expand the set of workloads
in your infrastructure
for which you can now get edge
protection from Cloud Armor.
So when I was describing
Cloud Armor just a minute ago,
I mentioned planet scale.
So you see here that DDoS
protection that Cloud Armor
customers receive, that
protection is provided
from techniques that our
SREs and our DDoS teams
have developed over
the past 20 years
in the effort of
protecting and ensuring
availability of our
own Google services,
things like Search and Map.
Due to the global nature
of Google's network,
where all of our customers are
and where all of our services
run, we're able to absorb and
dissipate or otherwise mitigate
attacks across
various components
in our global routing and our
load balancing infrastructure.
So here's a perfect example
of why this scale is so
important to you as a customer.
We recently announced that Cloud
Armor's machine learning-based
adaptive protection
capability combined
with the recently GA'ed
rate limiting feature
were instrumental in
mitigating a massive attack.
A Cloud Armor
customer was targeted
with a series of
HTTPS connections
which peaked at 46 million
requests per second.
That's the largest DDoS
attack of that nature to date.
76% larger than the previous
reported record attack.
So to give you a
sense of scale, this
is like all of the requests,
all the daily requests
to Wikipedia, hitting
in just 10 seconds.
And the customer service
remained available
throughout the attack.
The Cloud Armor product
team is actually
giving a dedicated
breakout session
to this attack, Sec 201.
So be sure to check it
out if you want to know
more about the details there.
So now let's turn our
attention to Cloud IDS.
This is our intrusion
detection system.
Cloud IDS detects malware,
spyware, command and control
attacks, and other
network-based threats.
Its security efficacy
is industry leading,
because it's built with Palo
Alto Networks' technologies
under the covers.
Being cloud native, Cloud
IDS is simple to deploy.
It's managed, and it
provides high performance.
A broad overview of it
kind of works like this.
So it copies traffic from
a customer's network,
as you can see with the little
packet mirrored icons that are
sprinkled throughout the VPC.
Any traffic you want you
can tell it via the packet
mirroring policy.
So this could be
compute engines.
This could be GCE containers
and GKE, all the traffic
within a VPC, within
a subnet even,
VPCs to the internet,
on-prem, Google services.
All of these things
can be expected.
North, south, east, west.
We have full visibility
and detection capability.
So everything in the blue
squares in the middle, that's
what we set up and manage for
you, taking the mirror traffic
it runs to the threat detection
technology, and then we let you
know what we find.
It's all out of band and
requires zero forwarding
changes to your infrastructure.
This dramatically simplifies
fulfillment of the IDS security
control requirements that are
common in several compliance
standards, for example PCI.
Cloud IDS also integrates
with SIEM and SOAR solutions,
enabling customers to set up
automated threat responses
to take automated actions in
things like Cloud Armor, Cloud
Firewall, for example, all just
based on Cloud IDS detections.
So it kind of works like this.
Looker blocks for
these applications,
like for example, Cloud
IDS and Cloud Armor,
are in the Marketplace.
You just download them and drop
them into your Looker instance.
The semantic layer
parses the JSON messages
and represents them to Looker.
The dashboards are
easy to export,
and there's no additional cost
for existing Looker users.
And again, you can
find these blocks
on marketplace.looker.com.
That's it for networking.
Nelly, I think everybody
is eager to hear about how
to better secure data.
NELLY PORTER: Thank
you so much, Gregory.
And our customers,
as you can imagine,
now understand how
to protect network.
And now let's remind everybody
that we protecting network
because customers bring in
data and workload to Cloud.
And to be able to
protect the data,
the first step in
their journey is
to figure out what kind of
data they bring into the cloud.
So the first thing is to
get a little more visibility
and understand where
the right data resides.
We have good tools
to help in this task.
And it's data loss prevention.
And data loss prevention is
not only preventing data loss,
but it's also helping to
classify and understand
what data is sensitive
and what data is not.
And it's also trying to
have additional toolset
of capabilities that
help customers to derisk
sensitive data.
What does it mean?
It's trying to remove
unnecessary PII data,
again with tokenization,
anonymization,
and other techniques.
But it's not all.
Let's assume that we
need to keep fully, fully
fidelity clean data.
You can't analyze
or tokenize it.
You have to have a tool.
And encryption is
very powerful tool.
It helping us to
focus more on keys
is it encrypting this data.
Also encryption helping us to
limit the access to this data,
to encrypt your data you trust.
And let me introduce a few
of them, a few options.
So we have what we call Cloud
Key Management Services.
And Cloud Key
Management Services
helping you to protect your
data is full of control
that you can apply
to those keys,
as those keys are
actually created and done
and all operation, encryption
operation, done in software.
We have Cloud HSMs, where
all operations and key
creation is done in those
hardware secure modules.
And we have additional
product in the market,
all of them in GA,
and this product
we called Cloud EKM,
External Key Manager.
And it's for those
customers that
want to have full sovereignty
of the keys, full sovereignty
of their key management
system and HSMs
and want to connect their HSM
system to the Cloud Services,
all of that in your possession.
So let's start with,
as a use case, when
you need to run your VMs.
So when we define and design our
confidential computing story,
first step we needed to
ensure that everything that's
running in this environment
is actually trustworthy.
And that is why we
introduced what we call
Shielded VMs two years ago.
Right now, this capability
available by default
on every single VM
you will start in GCP.
But not only VMs, but also
on Kubernetes, on our GKE,
on our Cloud,
Dataflow, Dataproc,
or any other services
around within VMs.
And shield it VMs
means protection
against malicious guest system
firmware and also malicious OS.
And all of these integrity check
and all of these monitoring
information we're
sending to Cloud Logging
so you can take
some actions if you
will find the situation that
your VMs or your systems
are not running as you expect.
We want to create this
confidentiality, this box
around your environment, and
provide cryptographic isolation
among all our tenants
running in Cloud.
And confidential VMs is
actually a normal VM.
You don't need to change
any line of your code.
You can do lift and shift your
applications from normal VMs
to confidential VMs
with simple click.
It's incredibly scalable.
But also from
security perspective,
it's one of the best
capabilities that we offer.
We used special
instructions on AMD CPUs
to generate the keys to
encrypt memory of your VMs.
But those keys is the
best keys in the world,
because they random, ephemeral.
They generated in the
hardware, but most importantly,
they not extractable.
It means not us,
no Google, no you,
would be able to
deal with this keys.
Confidential GKE
allow us to create
the whole entire cluster,
confidential cluster,
as confidential
with a simple flag.
But again, for
developers, they would
be able to add special label.
It's called Selector Node.
And this Selector
Node is something
that Kubernetes' audience
familiar very well.
And it is a way to indicate
to Kubernetes engines
that this particular
applications,
this particular container
need to be deployed
to confidential cluster.
So again, now we have
confidential VMs.
We have confidential GKE.
For very big analytical jobs,
we have confidential Dataproc.
And confidential Dataproc is
including Hadoop, manage Hadoop
and Spark for your clusters.
But a lot of our customers
are asking us the next step.
OK.
We know how to isolate our
workloads among ourselves.
But how we would be
able to collaborate?
How we would be able to
bring all of those data sets,
very private data sets,
together and execute workloads
or applications
that we agree upon,
without revealing the
private data to all
of the participants?
And that is why at Next, we
are introducing a new addition
to a confidential computing
portfolio called Confidential
Space.
It is a cryptographic,
very hardened box,
where all collaborators would be
able to put the data in and run
the application inside of
that with full guarantee
that none of them would be
able to access this data
or influence the execution
of their workload.
And it's incredible, big
step toward helping customers
to collaborate securely.
To summarize, Gregory
and I talk to you
about the security of
network, data, and compute
and how we can help you with
protecting network, data,
and compute in GCP.
The most importantly, we
also emphasize the fact
that all of those tools
incredibly easy to use,
performant, and scalable.
And it's exactly what
we mean the Google way.
Thank you so much
for being with us.
Bye, bye.
GREGORY LEBOVITZ: Thank you.
[MUSIC PLAYING]
CHELSIE CZOP: Hi.
Thanks for joining us
at Google Cloud Next.
Today we'll go through
how Google Cloud optimizes
infrastructure for
your unique workload.
I'm Chelsea Czop, Product
Manager for AI/ML and HPC
on Google Cloud.
And I'm joined by Ruwen
Hess, a Group Product
Manager of Storage.
And you'll hear from
him in a little bit.
Today we have two main topics.
We'll go through announcements
in Compute Engine as well
as Storage.
So let's dive in to talk
through more on Compute Engine.
A few years ago, we
introduced VM families.
And they were created
to be optimized
for all specific workloads.
So let's do a quick
refresher of each family.
Starting on the left
with general purpose,
we have cost-optimized
or efficient VMs, or E2.
They provide up to a 31%
cost savings compared
to our original N1 machines.
They provide
reliable performance
across Intel and
AMD, and they can
be provisioned with custom
machine types, or CMTs,
to only pay for what you need.
Going once to the right, we
have balanced or N2 and N2D,
providing a balance between
customization, performance,
and TCO.
They provide the widest feature
set and can also be provisioned
with custom machine types.
A quick little reminder for
you, the N2D denotes AMD, and N2
is Intel based.
Moving another one to the right,
we have the newest family,
and that is Tau VMs.
And they were
introduced last year
and provide the best
performance per dollar
and are optimized for
scale at workloads.
When we introduced
Tau VMs last year,
it was on the third generation
AMD EPYC Milan processor.
And earlier this
year, we launched
T2A, our first Arms instance.
And we'll dive into
more on T2A in a minute.
Now for workload optimize,
starting on the right,
we have compute-optimized,
which provides the highest
performance consistency
CPUs on Compute Engine.
And this is best suited for your
high-end web and app serving,
gaming, or even in
your HPC workloads.
And I'm really
excited to announce
one of our first third
generation VMs with you, C3.
Moving one over, we
have memory-optimized,
which provides the most
memory in Compute Engine
and is best suited for
your largest databases,
such as SAP HANA or
Windows databases.
And to round out our
workload-optimized fleet,
we have accelerator-optimized
VMs with high-end performance
GPUs, based on NVIDIA ampere
A100 tensor core GPUs.
We'll also get to another
exciting announcement
in our A2 VMs as well.
Now let's dive into
our recent launches.
Starting on the left
with general purpose,
we have Tau VMs.
Earlier this year, we announced
the addition of the Tau family
with Arm-based VMs.
And they're now generally
available in Google Cloud,
extending that rich choices
that we already have
to offer with Intel and AMD.
You can now use these
VMs in select regions
in North America,
Europe, and Asia.
And these VMs support
key Google products,
like Google Kubernetes Engine.
In addition, there's
support for broad ecosystem
of operating systems, databases,
programming languages, and so
many other tools as well.
Next up in our
compute-optimized VM family,
we're excited to introduce
C3, our newest machine
series powered by 4th generation
Intel Xeon Scalable processors,
or codename Sapphire Rapids.
C3 is more than just
another CPU update.
This is the first VM built on
our next generation platform
architecture, powered by
Google Cloud's custom Intel
infrastructure processing
unit, or an IPU.
This IPU is the outcome of a
deep multi-year collaboration
between Google Cloud and Intel.
C3 also features
new 200 gigabytes
low latency networking, as
well as high performance I/O
products, Cloud Hyperdisk,
which you'll soon
hear more from Ruwen about.
Contact your sales team to
join in on our private preview.
Because our clusters can
be scaled and paralyze
more densely, we're seeing
customers and partners,
like Ansys, Snapchat, and
Parallel Works, completing jobs
much, much faster
and also boosting
their productivity,
Snapchat, or Snap,
was able to obtain a 20%
increase in performance
over C2 for a key workload.
Parallel Works does
vital development
towards building of
Weather-Ready Nation,
including NOAA's multicloud
R&D computing environment.
They were able to
achieve a 10x faster
results running WRF on C3.
And Ansys, a leader in
engineering simulation,
was able to see a 3x performance
gain in C3 over C2 running
their flagship
mechanical products,
including Ansys Fluent and
Ansys Mechanical and LS-DYNA,
due to the higher memory
bandwidth and lower network
latency that you get with C3.
So let's move on to
accelerator-optimized.
In our accelerator-optimized
VM portfolio,
it's a VM that is optimized
to run the NVIDIA A100 GPUs.
We're really excited to announce
today the A2 ultra GPU VM
instance, which is based on the
NVIDIA A100 80 gigabyte GPU.
This is best suited
for largest models
with massive data
tables, like deep
learning recommendation models.
The A100 80 gigabyte
reaches up to 1.3 terabytes
of unified memory per
node and delivers up
to a 3x throughput increase over
the original A100 40 gigabyte.
For HPC applications
with largest data sets,
the A100 80 gigabytes
additional memory
was able to deliver
up to a 2x throughput
increase with Quantum Espresso,
a material simulation.
On a big data
analytics benchmark,
A100 80 gigabytes delivered
insights with to a 2x increase
over the A100 40 gigabyte.
This makes it ideally suited
for your merging workloads
with exploding data sets.
And we hear from our
customers and users
all the time that the AI demands
purpose-built infrastructure
for them to be successful.
AI Singapore is a
national program in AI,
supported by the National
Research Foundation
and hosted by the National
University of Singapore.
They were able to reduce a
large language model processing
time by 40% and also gain a
50% increase on text generation
throughput over the
A2-megagpu shapes,
based off the original
A100 40 gigabyte.
This allows other
teams to experiment
faster with increased
productivity.
And I'm also excited to share
that Nuro is looking forward
to experimenting with our
purpose-built hardware
for their platform.
Now I'll hand it off to Ruwen
to talk about new announcements
and storage.
RUWEN HESS: Thanks
so much, Chelsea.
Happy to be here.
Over the years, we've launched
a lot of performance and feature
improvements to Persistent Disk.
And while we're happy with where
we're going with Persistent
Disk, we've also taken the
time to step back and think
a bit about what does
block storage on the cloud
really ideally look like.
From customer
conversations, three areas
quickly crystallized.
Ideally, block storage should
be dynamically provisioned,
tuned to specific
performance and capabil--
capacity needs and decoupled
from instance type and size.
Customers should be able to
dynamically change performance
when requirements change.
Also block storage should
be at full spectrum,
meaning that it should cover
all the core Cloud workloads
and that for each
workload, it should
cover the entire spectrum
without efficiency or TCO
cliffs.
And then finally,
block storage ideally
should be managed at scale, with
aggregate storage management,
including capacity pooling, and
with policy-based management,
and ideally with
workload awareness,
allowing you to set performance
requirements in the workload
context rather than
tactically at the disk level.
Following these
insights, we set out
to build Google's next
generation block storage.
And we're now introducing
Google Cloud Hyperdisk.
Hyperdisk is built to leverage
our new IPU-based architecture.
It offloads and dynamically
scales out storage processing.
And by decoupling block
storage from the VM,
Hyperdisk allows you to
tune easily and dynamically
the storage to your
workload, achieving
higher performance, higher
flexibility, and higher
efficiency.
To bring all these
capabilities to you,
Google Cloud Hyperdisk is
an entirely new portfolio.
Hyperdisk Balanced is our
new general purpose volume
type, the best fit
for most workloads.
It covers a broad
range of workloads,
offering up to 150,000
IOPS and 2.4 gigabytes
a second of throughput.
It's joined on the high
end by Hyperdisk Extreme,
our new premium SKU focusing
on the highest performance
databases with
explicit performance
as always and up
to 320,000 IOPS.
For cost-sensitive throughput
oriented workloads,
we have Hyperdisk Throughput,
with up to three gigabytes
a second and also performance
and capacity provisioning
separately.
So all of these SKUs have
capacity and performance
provisioned separately and
dynamically, performance
decoupled from instance
types, and up to 512 terabytes
in capacity.
All of them will be available
for Compute Engine and GKE.
And all of them will also be
available in Hyperdisk storage
pools, where you can manage
at scale within provisioning,
data reduction, and
policy based management.
Let's have a look at what this
looks like with an actual case
study.
This is a case study of
the steps at a high level
that you might step through when
planning a SQL Server instance.
Today, constraints of how
performance and capacity
management works leads to
a complicated balancing
act when you try to meet the
requirements of the workload.
Capacity and performance
sizing frequently takes weeks,
and you need a detailed
understanding of the workload
to do this well.
Finally, mistakes are costly
and can require downtime to fix.
Hyperdisk allows you
to tune performance
to the workload without
instance restrictions.
You can size capacity freely
and have actual usage determine
consumption, without excessive
planning and [INAUDIBLE]
gymnastics.
And performance can
be changed at any time
if requirements change.
All of this leads to dramatic
real world impact, simpler
deployment and management, more
efficient use of resources,
and an ability to
dynamically adjust
to changing requirements
at any time.
So three key aspects
help you optimize
TCO for storage intensive
workloads with Hyperdisk.
You'll be able to choose
instance size based
on workload needs, resulting
in smaller instances.
You'll be able to provision only
the capacity and performance
needed for workloads, resulting
in less overprovisioning.
And for workloads with
predictable demand patterns,
you can further optimize
with dynamic provisioning.
If, for example,
your workload has
a peak that's associated with
end of quarter reporting,
you can adjust it with
dynamic provisioning
for that period alone and
then scale back as needed.
Overall, we estimate that
for common storage intensive
workloads, you'll see an
average of 50% lower TCO
with Hyperdisk.
We're very excited
about Hyperdisk
and look forward to making
it available in preview
later this year.
We have some great sessions
for you at Next this year.
MOD103 covers the top 10
ways to lower your costs
on Google Cloud.
MOD107 covers how to
protect your infrastructure
with a cloud environment
built to tackle
today's insecurity challenges.
And MOD300 is about
leveraging AI infrastructure
to optimize cost performance.
From Chelsea and me, thank
you for joining our session.
We hope you enjoy
the rest of the Next.
[MUSIC PLAYING]
TYSON SINGER: This isn't
just about some impact
at the corporate level.
This is really about having
an impact at a personal level.
For myself, I identify
as an engineer.
I really want to be able to
come into my company, Spotify,
and figure out how to have an
impact beyond recycling, biking
to work, all those
sorts of things.
They're great.
But when you can have an
impact that goes global,
the path to impact is as
important as the destination.
SPEAKER 6: OK,
guys, may I just ask
can everyone please
put their phones
in silent and airplane mode?
[INAUDIBLE]
TYSON SINGER: You may not
know that Spotify is, in fact,
a Swedish-based company.
And in Sweden,
sustainability is really
part of the cultural
consciousness.
For Spotify, we have decided
that we want to contribute
positively to climate change.
We started with the
Exponential Roadmaps
goal, which is to get to zero
carbon emissions by 2050.
Our goal is actually to get
to zero emissions by 2030.
In 2021, we released our Global
Equity and Impact Report.
And in that, we outline
our path towards Net Zero
for greenhouse gas emissions.
One of the things that
is part of that playbook
that I love the best is
our annual Hack Week.
So Hack Week is when the
entire company comes together
to figure out how to
innovate across the company.
So this year, we set
the theme of Hack Week
to making the planet cooler.
One of the first and
most basic things
is, of course, we have moved
everything to the cloud.
With Google, we
have the opportunity
to leverage a number of products
that help us in our technology
approach to climate change.
And we do autoscaling with
products like Bigtable,
like GKE.
There's a few things
that are really
at the core of our strategy.
One, we want to have a direct
reduction of our own greenhouse
gas emissions.
The cloud part of the
emissions was only a small part
of the equation.
It starts not just
from the cloud,
but it goes all the way out
to our end user devices.
The second part
is to take a look
at the emissions caused
by all the things
that are upstream
from us that we use.
Not only will we reduce
our own emissions,
but we'll have a side effect
to all the other companies who
are using these same providers.
And then the third part
is really to leverage
our unique position to be able
to influence people's behaviors
outside of Spotify,
in particular,
not surprisingly since I
lead a group of developers
and engineers, to really be able
to impact the environment as
well.
[MUSIC PLAYING]
MAX CHARAS: Hi, Tyson.
TYSON SINGER: Hi, Max.
Max, before we get
started, maybe you
could just tell us a
little bit about how
you got into engineering.
MAX CHARAS: I think it's
a very fascinating way
of trying to understand
and structure the world.
How did you get started
with engineering?
TYSON SINGER: I got started
with engineering as a young man.
My dad was actually
a software engineer.
MAX CHARAS: My dad was also--
or is an engineer as well.
So maybe that's a
common pattern that you
choose from your parents.
TYSON SINGER: I'd love to
hear what sustainability
means to you personally.
MAX CHARAS: Yeah.
I am personally worried
about where we are.
It just felt my anxiety increase
with regards to climate change.
And I realized that by taking
small personal steps, that kind
of [INAUDIBLE] my anxiety.
TYSON SINGER: So we
recently had our Hack Week.
And the theme was making
the planet cooler.
You participated in that.
But this was actually the
genesis of the climate
engineering handbook.
Just in a nutshell, can
you define what the climate
engineering handbook really is?
MAX CHARAS: Today,
it has two parts.
So it starts with
a theoretical part
that discusses where our
emissions primarily stem from,
device, CDN,
networking in Cloud.
So we discussed these chapters
from a theoretical perspective,
try to show where the
emissions come from in each
of these separate steps.
And then there is a second
part of the handbook which
is a much more hands-on
guide, discussing
what tangible steps you can take
in each of these disciplines.
TYSON SINGER: In
this overall process,
really sort of a
bottoms up process
to democratize access
and then action, tell us
a little bit about how you think
this will evolve over time.
MAX CHARAS: Amazing to see
how many people at Spotify
have actually cared
deeply about this topic.
I think the only thing
that's limiting us right now
is just people hearing about it.
TYSON SINGER: Well,
thanks, Max for being
a climate champion for Spotify
and hopefully for the world.
MAX CHARAS: Thank you.
[MUSIC PLAYING]
[INTERPOSING VOICES]
LAUREN CHANG: Hi, Tyson.
TYSON SINGER: How are you doing?
LAUREN CHANG: Great.
TYSON SINGER: First,
can you tell us
a little bit about what
you do here at Spotify?
LAUREN CHANG: So
here at Spotify, I
lead business development
for Backstage,
focusing on driving
strategic partnerships
and other opportunities that
improve the adopter experience.
TYSON SINGER: So
what is Backstage?
LAUREN CHANG: Yeah,
Backstage is an open platform
for building developer portals.
It was built internally at
Spotify, open sourced in 2020,
and we donated it to the Cloud
Native Computing Foundation.
TYSON SINGER: And what's
a developer portal?
LAUREN CHANG: A developer
portal is a single pane of glass
for your entire infrastructure.
So it unifies your tooling,
your services, docs, and apps
under a unified, consistent UI.
TYSON SINGER: So we have
these different plugins
that they work with Backstage.
There was a plug-in
that came out.
Can you tell us a little
bit more about what it does?
LAUREN CHANG: So
Cloud Carbon Footprint
is actually an open source
tool developed by Thoughtworks.
It leverages cloud APIs
to provide visualizations
of estimated carbon emissions on
usage across Google Cloud, AWS,
and Azure.
We want to empower
not just Spotify
internally, but the
broader developer community
to be able to immediately
measure, understand, and reduce
their carbon footprint.
TYSON SINGER: Do
you think that this
has opened the floodgate
for more possibilities?
LAUREN CHANG: There's
so much more to come.
[MUSIC PLAYING]
TYSON SINGER: We believe
that climate change isn't
going to come from a
single silver bullet
to solve the problem.
Instead, it's going to come
from people who are united
to make a lot of changes.
Engineers, at quite
a personal level,
they want to have a big impact.
It's not just about what we
can do to reduce our company's
global greenhouse
emissions, but what
we can do to help everybody
else have that same impact.
[MUSIC PLAYING]
SHAQUILLE O'NEAL: Hi.
I'm Shaquille O'Neal, and I'm
the founder of Big Chicken.
JOSH HALPERN: When you're trying
to build a national chain,
communication is so critical.
To do that, you need
a great partner.
We're really lucky that
partner is Google Workspace.
SHAQUILLE O'NEAL: Google
Calendar is my girlfriend.
I don't know anything I'm doing
unless I talk to my woman.
Google Workspace.
Productivity and collaboration
tools for all the ways
you work.
[MUSIC PLAYING]
MIKHAIL CHRESTKHA:
Thank you, everyone,
for joining our
Google Cloud Next 2022
session on how AI
infrastructure on GCP
helps teams accelerate
ML development
with optimized
performance and cost.
My name is Mikhail Chrestkha,
an outbound product
manager on Vertex AI,
Google Cloud's AI platform.
I'm excited to have three other
guests with me here today.
We have Kai from Uber,
who's a senior product
manager on Uber's ML
platform Michelangelo,
and Joanna and Sid from Cohere,
ML engineers building tools
to make natural
language AI capabilities
accessible to all developers.
We'll start with a quick intro
on why AI infrastructure is
important, walk through
AI infrastructure services
and solutions available on
GCP, hear customer stories
about leveraging AI
infrastructure on GCP,
and then wrap up
with a quick summary.
So let's start with why is
AI infrastructure important.
And the best way to start
really answering this question
is to give you a sneak peek
into our customers' stories
about how they're using AI
infra to accelerate development
and deployment.
Cohere is leveraging TPUs on GCP
to innovate faster and iterate
faster on large language models.
Uber has integrated Vertex
AI's TabNet and AutoML services
into their platform to allow
different teams to leverage
these new algorithms.
Credit Karma is leveraging
Vertex AI's Feature Store
to manage embeddings that feed
into their content engagement
and offer conversions engine.
And finally, Arbor
Biotech is leveraging
AlphaFold, running on Vertex
AI, to help their scientists
predict structures of protein
sequences in hours instead
of months.
These examples, and many more,
cannot be done on general
purpose infrastructure.
This requires robust
AI, purpose-built AI
infrastructure.
In addition to these
specific examples,
there is a broader trend
across data and AI.
Data is getting bigger.
It's getting multi-format.
AI models are getting larger.
And more developers are adopting
frameworks, like PyTorch,
TensorFlow, the Transformers
library, and JAX,
making AI research and
development more accessible.
So infrastructure
really needs to keep up
to make AI models performant
and cost effective.
And having access to
strong AI infrastructure
is becoming a
competitive advantage
to getting the
most value from AI.
And this is also exemplified
by these two quotes
from IDC and Forrester.
IDC shares that the lack of
purpose-built AI infrastructure
is now becoming one of
the leading causes of AI
infrastructure failing.
And Forrester shares
that AI infrastructure
is really crucial to
keep AI teams productive,
to keep them experimenting,
developing, and deploying,
rather than just waiting around
for large AI tasks to complete.
So now let's move into
what does AI infrastructure
on Google Cloud look like.
And we have three pillars
when thinking about this.
One, flexible and
scalable hardware
to support diverse ML workloads.
Two, manage
infrastructure to allow
practitioners to focus on
experimenting and deploying
models.
And this is offered through
Vertex AI, a fully managed Data
Science MLOps platform.
And number three,
really easily access
state of the art API with
optimized infrastructure
and software paired
together, really looking
to bridge the gap
between research
and applied AI, what we
call research to ready.
So with a focus of AI
infrastructure as part of an AI
strategy, the performance
and cost benefits
can be quite staggering.
Along the bottom, we
see some great results
where some workloads get
up to 6x cost reduction
by optimizing on
the right hardware.
And we've seen some
teams go from ideation
to experimentation to production
weeks rather than months
or quarters, with
many teams sharing
stories of 80% improvement and
overall velocity across the ML
lifecycle.
So now let's dig into some
of the specific services.
On the hardware side, we're
excited to welcome two new AI
accelerators onto GCP.
First the NVIDIA
A2-ultra GPU VM instance.
This machine has twice the GPU
RAM of the previous A2-mega
machine, providing 80
gigabytes of GPU memory.
This provides big performance
gains in throughput as well as
latency for use cases like
deep learning recommendations
and computer vision models, as
well as large language models.
Second, we're excited
to welcome the newest
generation of the tensor
processing unit, or the TPU.
This is infrastructure
designed by Google specifically
for machine learning, with
support for PyTorch, JAX,
and TensorFlow.
The fourth generation of
TPUs scale to a superpod
made up of over 4,000 chips.
And each chip has
big improvements
in Flops and FLOPs
per dollar versus
the previous third generation.
In addition to pure
horsepower, the data center
hosting the TPUs is operating
at 90% carbon-free energy,
with a big focus
on sustainability.
So now let's move on to some
of the more machine learning
specific tasks.
And large scale
training is really
one of the first ML tasks
that requires purpose-built AI
infrastructure.
Vertex AI provides a fully
managed training service
that eliminates the need to
provision and manage clusters.
Users can submit and forget
jobs with the Python SDK,
get out of the box
queue management,
access to on-demand
accelerators,
and built-in
hyperparameter tuning.
Moving on to the middle, Vertex
also provides a new distributed
training capability that works
with existing frameworks,
like Horovod or TensorFlow's
MirroredStrategy or PyTorch
Distributed.
This reduction
server architecture
provides a new way that
minimizes latency and data
transfer.
Benchmarks for language
models have shown up
to 30% to 40% reduction
in training time and cost.
And finally, we know
many times, data
feeding into these accelerators
can often be the bottleneck.
So we're launching
first class support
for Cloud Storage and NFS
Filestore as part of Vertex
to be easily able to mount these
into on-demand training jobs.
For Cloud Storage
specifically, this
allows you to use Cloud
Storage's new Autoclass
capability that automatically
moves data to lower and colder
storage classes
based on access time,
resulting in automated
cost savings.
Next, we move into the serving
layer with Vertex's managed
prediction service.
First, this is a fully
managed API endpoint service.
This allows users to
deploy their models
and to autoscaling endpoints
with a wide selection of CPUs
and GPUs with minimal
infrastructure
knowledge required.
Within this, we're excited to
announce two new capabilities,
model co-hosting,
that allows you
to share underlying resources
across models for cost
optimization, and custom
prediction routines, which
allows users to include pre-
and post-processing Python
code alongside your model binary
to simplify your architecture.
And then in the
middle, we're also
excited to produce an optimized
TensorFlow runtime only
available on Vertex.
This leverages model
precompilation and smart op
placement across GPUs and
CPUs to drive big improvements
in throughput and latency.
Early benchmarks showed huge
improvements, up to 8 and 6x
for throughput and
latency for tabular data
and material improvement for
BERT-based natural language
models.
And finally, deploying
models for serving
is only half the battle.
Vertex provides model monitoring
and explainable capabilities
to ensure your
models stay healthy
or are retrained and
redeployed as they erode.
Next, AI infrastructure
isn't just
about training and serving
machine learning models.
Managing features and embeddings
to feed into your ML models
is non-trivial.
As organizations scale
both the number of models
and the usage of complex,
unstructured data
as part of these models, feature
stores and vector databases
can become a challenge
to implement.
To support this newer
AI infrastructure need,
Vertex AI offers a fully
managed Feature Store
and Matching Engine.
And today, we're actually
excited to announce support
for streaming ingestion across
both these services, where
you can now have new
features and embeddings
synchronize and
updated in real time
to improve the accuracy and
timeliness of predictions.
And both these services support
a wide variety of use cases,
ranging from recommendations
engines, search engines, image
classification, ad
targeting, and much more.
And finally, we don't want to
stop at just providing hardware
and manage infrastructure.
We want to provide a path
to adopting leading edge
solutions coming from research
and partners that can drive
true business value through AI.
First, we have TabNet, now
available through Vertex AI's
tabular workflows.
This makes it easier to build
accurate and explainable models
on billion scale data sets
with built-in interpretability.
On Vertex, the TabNet
pipeline automatically
selects the appropriate feature
transformations, search space,
based on the input data,
data size, prediction type,
and your training budget.
We'll hear a little bit
more from Kai about Uber's
early experience with TabNet.
Second, we have an AlphaFold
batch inference pipeline
in Vertex AI.
This allows biotech
companies to embed AlphaFold
into their own workflows to
predict protein structures
at scale with
optimal architecture.
And third, we have a solution
from our partner NVIDIA called
Merlin.
Merlin is an
end-to-end framework
to design and deploy custom
large-scale recommender
systems.
We've partnered with NVIDIA
to build a reference solution
deployable on Vertex AI
and GPUs and GCP, which
includes data processing, model
training, and model inference.
Next we'll jump into some great
examples of enterprise teams
leveraging some of
these technologies.
And let's go ahead and
start with Kai from Uber.
KAI WANG: Thank you
so much, Mikhail.
Hi, my name is Kai.
I'm the product
manager for Uber's
internal end-to-end
virtual learning
platform, called Michelangelo.
Michelangelo powers 100%
of Uber's most business
critical machine
learning use cases,
such as ride's ETA, Eats
ETD, driver-rider matching,
and Eat's home feed
recommendations,
allowing our machine
learning developers at Uber
to focus on what
they are good at,
building and deploying
models, without worrying
about underlying
in complexities.
Our strategy of
building Michelangelo
is based on actively
evaluating and integrating
industry leading third-party
components, while selectively
investing in key platform
areas to build in-house.
So we architecture
Michelangelo in a modulized way
so that we can easily
integrate third-party tools
and components via
plug and play fashion.
When we explore the solutions
for large-scale model
architecture search
and training,
we found Vertex AI,
AutoML, and TabNet.
In the past year, we'll be
working closely with the Vertex
AI team to evaluate the
performance of AutoML
and TabNet with Uber's real
life use cases and data.
The evaluation results met
the POC success criteria
in terms of model accuracy,
training efficiency,
compatibility with
Michelangelo tech stack.
So we've decided to integrate
both tools with Michelangelo
and made them available to all
machine learning teams at Uber.
On the other hand,
along the way,
to meet Uber's extensive and
complex machine learning needs,
we've made multiple feature
requests and product
improvement suggestions to
Vertex AI team, which in turn
improves AutoML and TabNet.
One example use case is
UberEats Prep-time model,
which is used to estimate how
long it takes a restaurant
to prepare the food
after order is received.
This is one of the most
critical models at Uber,
with the highest QPS,
query per second.
We compared the TabNet results
with the baseline model,
and the TabNet
demonstrated a big lift
in terms of the
model performance.
Overall, the POC
results are promising,
and we are excited
to continue closely
working with the GCP
team to drive production
adoption within Uber.
With that, I'll pass
you on to Joanna
to talk about their
use cases at Cohere.
JOANNA YOO: Thank you, Kai.
Hey, everyone.
I'm Joanna from Cohere.
I'm very excited to
present how Cohere
is accelerating
large launch model
development with Google Cloud.
Cohere's mission is to unlock
NLP capabilities enabled
by large language
models and make
them accessible to all
developers with just an API
call.
To achieve this,
Cohere abstracts away
the heavy lifting
from the end user,
including collecting
and curating
a large corpus of
high-quality data,
training a pre-trained
learning language models,
and post fine-tuning
optimizations
for low latency inference in
a highly reliable environment.
Here, we outline
Cohere's achievements
in building a scalable
training, evaluation,
and inference stack.
Taken as a whole,
this technical stack
is a key competitive
advantage, which
has enabled Cohere
to rapidly scale
our models, while ensuring
quality, responsiveness,
and safety of our end
products, all of which
is implemented in
Google Cloud platform.
With that, I'll
pass it on to Sid.
SIDDHARTHA KAMALAKARA:
Hey, I'm Sid.
I'm a machine learning
engineer at Cohere.
I'll give a high level overview
of Cohere's training framework.
So Cohere has a proprietary
training framework called FAX.
It uses JAX and cloud TPU v4s
to train large language models.
TPU v4 pods are some of the
most powerful AI supercomputers
in the world.
A full v4 pod has 4096 chips.
In the diagram on the right, you
see a few typologies of slices
from the full v4 pod that are
available on the Google Cloud
platform and that can
be used by FAX to train
these large language models.
Again, Cohere uses FAX to train
our baseline models as well as
custom models that are
trained on customer data sets
using the fine tuning feature
on the Cohere platform.
FAX's job is to consume billions
of tokens and train models
as small as hundreds of
millions to as large as hundreds
of billions of parameters.
Coming to some concrete
benchmarks, in the middle
we see pipeline
parallelism, which
was the old way of
training large language
models at Cohere.
With FAX, the
scaling in the plot
is represented by a
tensor parallelism.
So for a fixed number
of tokens, the tensor
parallelism method of
training large language model
scales much better.
And on the right, we see
that with using 412 v4 codes,
the maximum model
size we can schedule
is 340 billion parameters.
And at a minimum batch size,
the step time is 6.21 seconds.
This enables us to train large
language models very fast
and bring those improvements
to customers right away.
Thanks for having us, Mikhail.
I'll pass it off to you.
MIKHAIL CHRESTKHA: Great.
Yeah, great to hear those
stories from both Uber
and Cohere really pushing
the boundaries of AI
and really leveraging the AI
infrastructure services on GCP.
I also wanted to end this with
actually two quick stories
from two other customers.
First is with Credit Karma.
For those not
familiar, Credit Karma
is a multinational
personal finance company
that continues to innovate
to provide more personalized
products and services.
And they have a mission
to make financial progress
possible to everyone.
One of their use cases
is to better recommend
financial content and financial
offers for their members.
Their approach was to
generate embeddings
from financial content and
mix those with features
from offers based
on user cohorts
to improve their input features
for their recommendation
system.
The diagram on the
bottom left gives you
a quick system overview of
how this architecture works.
We have financial content
on the left, where
they use a pre-trained
sentence encoder
model to extract embeddings.
On the right, they also bring in
features from financial offers.
They bring both these
different domains
into Vertex's Feature
Store that are then
leveraged for training and
serving their recommendation
engine.
And this really
had two benefits,
highlighted on the right side
with a few of their quotes.
One is really for the
internal Credit Karma
machine learning platform team.
Shout out to Debasish Das,
who's been working with us.
His team can now launch
richer and faster features
up to 3x faster.
And second is, of course,
the user experience,
the consumer
customer experience.
And using A/B testing, they've
seen significant increase
in content engagement
and offer conversion.
Next, we have a
customer, Arbor Bio,
leveraging DeepMind's AlphaFold
solution on Vertex AI.
Arbor Bio is a biotech company
based in Massachusetts,
discovering and developing
the next generation
of genetic medicine.
Arbor Bio is really developing
an extensive toolbox
of proprietary gene editors,
using both high throughput data
screening as well as AI.
And why do they really look
at Google Cloud and GCP?
Number one, they really look
to augment their toolkit
and processes with
DeepMind's AlphaFold model.
And they really wanted
to inject the capability
to predict structures of
proprietary protein sequences
in a scalable, robust,
and cost effective way.
Now Arbor Bio's data-driven
and AI-based approach
really has a lot of high
variability of hardware needs
across CPUs and GPUs.
And this was a great use case
for GCP and Vertex AI's managed
infrastructure.
And really, the
impact you can see,
with a quote in
the bottom right,
is that Arbor Bio scientists
are able to increase
their productivity and generate
actionable insights in hours
instead of months.
So let's go ahead and
end with a summary.
To summarize, we
truly believe that
to derive the most
value from AI,
AI infrastructure now needs
to be at the core of your AI
strategy.
We covered three pillars
of AI infra on GCP,
flexible and scalable
hardware, manage infrastructure
on Vertex AI, and accessible
state of the art AI solutions.
And you can see we can
continue to invest heavily
in each of these with lots of
new capabilities and launches
rolling out.
And finally, again, we're super
thankful for our customers
and partners leveraging
these services,
providing us feedback to
continue to improve these,
and sharing their stories
with the broader industry
so we can all learn
from each other.
So thank you so much
for joining us today.
Please enjoy the rest
of Google Cloud Next,
and have a great day.
[MUSIC PLAYING]
SAM GREENFIELD: So
the Office of CTO
is pulled in while
Etsy was considering
a variety of different clouds.
And we worked with
them to talk about some
of our advantages in
machine learning and search,
as well as touching on our
sustainability initiatives,
too.
MIKE FISHER: I think
that type of partnership
from the beginning made a
huge difference in ultimately,
selecting Google as
our cloud provider.
While we definitely know
we've reduced our energy
consumption, what we're missing
is the actual monitoring
of energy.
They've just kept coming
back and listening to us
and saying, what
do you need and how
do we partner with you
to solve these problems.
SAM GREENFIELD: I'd
say from the get go,
our relationship
with Etsy has always
been one of aligned values.
They've been leading the
industry in sustainability,
not just in the retail
space where they are,
but actually across
many dotcoms.
MIKE FISHER: What Google
has done with their cloud
infrastructure is be
very sophisticated
and actually apply
machine learning.
By doing that, their PUE, or
power usage effectiveness,
is much better than
a typical data center
that we would rent space in.
SAM GREENFIELD: It does
go beyond the environment.
It involves education.
It involves well-being
of your employees
and well-being of your
customers as well.
[MUSIC PLAYING]
SEAN DERRINGTON: Hi,
and welcome to Next.
I'm Sean Derrington,
Group Product Manager
here at Google Cloud.
Today I want to cover
three different things.
One, I want to
share some storage
best practices around data
analytics and Cloud Storage.
I then want to share a
little bit of the insights
around stateful
workloads in GKE,
as well as some considerations
for storage and data
protection, and actually
give you a couple of demos.
And then lastly, I wanted to
focus on critical applications
and different design
considerations
that you may have
for storage, as well
as, again, data protection.
So with that, let's
jump right in.
There are a lot of
opportunities to gain
more value from your data.
But you have to get the
data into Google Cloud
before you can begin to
run many of our analytics
options between Looker
and Vertex AI and BQ.
And so storage transfer service
enables you to do exactly that.
We have the opportunity
to move data
from on-premises as
three compatible object
storage into Google Cloud.
We also have the
opportunity to migrate
data out of other
cloud providers
into Google Cloud Storage.
So once you get the
data into Google Cloud,
now you have the
option to actually
make sure you know who's
accessing what data
and where they are.
And that's where the integration
into our IAM, the Identity
and Access Management,
comes into play.
You have the fine-grained
multitenant capability
between projects to define
who has access to what data.
But then we're also focusing
on with Cloud Storage
how can we make your
life simpler, simpler
in a variety of ways.
And that is, number
one, making it
easy from an
operational perspective
to get data into Cloud Storage.
Instead of going
through data migrations
and changing from structured
to unstructured or vice
versa, things like
Hadoop and Spark,
we actually give
you the opportunity
to use the same file
system structure, HDFS.
But now you can just transition
and simply use gs:// and begin
to run your analytics
from there, again,
making it really easy to
begin to gain value from
the information you have.
But we're also
focusing on how you
can take advantage of
optimizing your costs
across multiple storage
classes within Cloud Storage.
Whether you're looking
at Standard or Nearline
or Coldline or archive, you
have a single set of APIs
across all of those.
So from an application
design perspective,
you have one bucket,
one set of APIs,
regardless of which
storage class is actually
satisfying those requests.
But we're also focusing
on not only the cost
and the design application,
but also the response time.
One of the most important things
from an analytics perspective
is can I get consistent
millisecond response time
from Standard all the
way down to Archive.
And with Google Cloud Storage,
you can do exactly that.
The other thing we're
doing is focusing
on operational
simplicity, that is,
every organization is trying
to reduce costs over time.
With Autoclass, we give
you the opportunity
to hit the easy button, to
turn on an entire bucket level,
whether it be
millions, billions,
or trillions of objects.
You have the opportunity to turn
on Autoclass, which will now
migrate objects from one class
of storage to another by policy
without any intervention.
And as we migrate
storage from Standard
all the way down
to Archive, there
are no deletion fees
or retrieval fees
that you may have
from an early deletion
request of data that
might be resonant
for only 30 days instead of 60.
These are all things
that make it very easy
to optimize the
storage cost and where
those objects are over time.
And this is something that
we've recently announced,
and it'll be excitingly
available in Q4.
The other consideration
is how do you
get compute as close to
the data as possible.
And with Cloud Storage, you can
certainly have a single region
and run all the
analytics in that region.
But oftentimes, you want
higher levels of availability
if something impedes
or interrupts
that operation in
a given region.
And so with Cloud Storage,
we've had the opportunity
to define dual-region
replication for years.
Strongly consistent replication
with the recovery time
objective that you can basically
design a continental scale
bucket, regardless of where
the data is being served from,
the application doesn't know.
But what we've done recently
is actually introduce
enhanced options.
So now you have nine options for
regions within three continents
to choose for that
dual-region configuration.
And you can continue to use
that replication that's strongly
consistent if you want to.
So if your objects are in--
compute in region B, you need
to be retrieved from region
A, that can easily be done.
But if you need an extra
level of availability,
you can also look at the
optional Turbo Replication.
This gives you a
15 minute RPO SLA.
So now combined with
dual-region replication,
you have a recovery
time objective of 0
for your continental
scale bucket, but you also
have now a recovery point
objective of 15 minutes.
And last but not
least, as you look
at this design from
storage transfer
service on the left hand side,
getting the data into Google
Cloud Storage, looking
at how you replicate
Cloud Storage across
two different regions,
the different options
you have for analytics,
whether it be Vertex
AI or BigQuery,
you need to think
about performance.
Everybody wants their
data more quickly.
And that's where we come into
play with our Cloud Storage
option.
You have the option to scale
that performance almost as
much as you need to.
We have retail
customers today that
are using our Cloud
Storage with analytics
that are exceeding 10 terabytes
per second of throughput.
Now not everybody's going to
need that much throughput.
But if you do, it's
an option to design
your application that way.
And last but not
least, regardless
of what type of analytic
workload you're using,
BigQuery or Vertex
AI, this is all
supported with Cloud Storage.
So many times,
shifting gears to GKE,
many times organizations are
looking at stateless workloads.
And that's been the
way that most people
have been using that.
However, we're seeing more
and more customers thinking
about stateful workloads,
where if the pod goes down
or that namespace goes
down, you can't just
regenerate that data.
So you have to think
about things differently.
And that's where Backup
for GKE comes into play.
This is now an offering
that we've recently
made generally available.
It's integrated directly
within the Cloud console
that you can choose to protect
your Kubernetes environment.
In a few mouse clicks,
protect it locally, protect it
across another continent.
Just depends upon what
level of recoverability
you want to protect against.
And this is something that we do
from a very fine-grained policy
definition perspective.
So whether you're running and
supporting Persistent Disk,
you have the option to
protect a single application,
multiple namespaces, or the
entire cluster if you want to.
And we have application
consistency as well
for many of the options.
So you can have a crash
consistent backup for some.
But in some cases, you
actually want an application
consistent backup.
Some of the policies
that we've actually
introduced and give you
options for actually
protect against
ransomware as well.
So time lock is a very popular
feature that people use
to actually apply to a given
policy that says regardless
of who wants to
delete that backup,
it cannot be deleted until
a time lock expires of, say,
90 days.
So ransomware may try to,
but it won't be able to.
So now I want to introduce
Manu Batra, who's
going to do a demonstration
of backup for GKE.
MANU BATRA: For the Backup
demo, we're going to do--
we're going to create two
clusters, one a primary cluster
called Postgres cluster.
We're going to create the
cluster in a rapid channel
with 1.24.
Next, we'll enable the
Backup for GKE add-on.
We'll select specific
machine types,
in this case e2-standard-2.
And our cluster is created.
Now let's create the
secondary cluster.
Again, in rapid channel with
1.24 with similar machine
types.
And they will backup for GKE.
So now we have two
clusters, one for backup,
other we're going to
restore that backup.
Let's log in to our
primary cluster.
Deploy Postgres.
Let's log into Postgres, create
a table, and insert some data.
Let's insert
"learning GKE is fun.
Databases on GKE are easy."
So when we take a
backup and we restore,
we'll check if the same
table with the data
can be retrieved on
the restore cluster.
So now let's go
to Backup for GKE.
Create a new backup plan.
We have two clusters.
We select the cluster
we want to back up.
We will give this
backup plan a name.
It's a Postgres
plan, backup plan.
We select the target we
want to store the data.
And we have the backup plan.
Let's take an instantaneous
sort of quick backup.
This is a manual backup.
We'll give this backup a name.
And let's check it
if the backup's done.
Our backup has succeeded.
Now let's walk through
the process to restore.
Let's create a restore plan.
Let's select where
we want to restore.
We want to restore all the
namespaces in the backup.
We want to create new
volumes on Backup.
So we're selecting
all the options
for creating a backup plan.
Let's keep default
for restoration rules.
Now let's go back
to Backup for GKE.
We have a new Backup for GKE.
Restore plan for Postgres.
We have a new backup.
Now let's restore this
backup with the plan we just
created into the new cluster.
As you can see, our
restore was successful.
Now let's log in
to the new cluster
and see if the data
that we created
on the primary cluster
and all the config
has been recreated
into this new cluster.
So my Postgres workload is
recreated onto the Postgres
cluster restore.
All my configs, storage classes,
settings, [INAUDIBLE] claims
are also restored.
And the data that
I took a backup for
is already available.
So that was a quick demo.
Thank you so much.
SEAN DERRINGTON: Great.
Thank you so much, Manu.
Stateful workloads
obviously have
to be protected via Backup
for GKE, as we just described.
However, if you're thinking
about multi rider applications,
oftentimes pods need access
to consistent storage.
They need access to
persistent storage.
And that's where
Filestore comes into play.
Filestore is an NFS
managed storage service
that you can use for GKE.
And we have been able
to do this for a while.
We'll see it with
the CSI driver,
the container storage
interface driver.
We have some new news
that we've announced
regarding Filestore Enterprise
and some options there.
But if you think about how
you can have tens, dozens,
or even hundreds or thousands
of pods accessing the same data,
that's what we provide
with Filestore.
It provides highly
available storage.
So if pod one goes
down, pod two still
has access to the exact
same NFS mountpoint.
And from an operational
perspective,
Filestore supports
non-destructive upgrades.
So as we add more
features over time,
we incrementally
improve Filestore.
That upgrade is going
to be non-disruptive
to your environment.
You don't have to
plan for downtime
when you're doing this.
This just happens
in the background.
The other option,
depending upon the level
of applications you're
running and the availability
requirements, is Filestore
Enterprise actually
has a four 9 SLA.
That regional SLA
ensures the data
is replicated across three
zones within that region.
The new option that we've
introduced with Filestore
Enterprise is multishare.
This is the option for you
to take a Filestore instance,
let's say 1 terabyte,
and carve it up
into smaller shares to
provision across multiple pods.
So now you can provision as
little as 10 gigabytes per pod
that your admins need.
Very simple to do.
You create a storage class.
Then your GKE admins
can then create PVs
and begin to consume that.
But let's dive into
a demonstration
and take a look
at this in action.
MANU BATRA: In
this demonstration,
I want to show you
Filestore Enterprise
and the new multishare
capabilities
to use with your GKE clusters.
So let's get started.
So first of all,
we're actually going
to create a cluster in GKE.
We're actually going to use the
Autopilot capabilities to make
things simplified.
I'm going to call this
cluster "stateful-cluster."
I'm also going to use rapid
channel for the Filestore
multishare feature here.
This is going to enable us to
not only create the cluster,
but then make sure that the
cluster is properly connected.
So I'm going to
click Create here.
And now I'm going to begin
to configure the managed
storage with Filestore.
This is going to use a CSI
driver, the container storage
interface, for this.
It's going to make it
really simple to do,
also going to show
you the YAML file
here for the storage class.
This shows you
that storage class
is using Filestore Enterprise,
which is important,
but you also want to make sure
the multishare is set to true.
This is going to give
you that functionality
to have the smaller shares down
to 10 gigabytes, if you will,
within Filestore Enterprise.
Now once I do that, now
I'm going to actually run
the PVC YAML file.
And you'll actually
see that I'm going
to do this with a
single persistent volume
in this provisioning
process and eventually
be bound to the cluster.
Now, you can do this with
multiple PVs if you want to.
But in this instance,
I'm just going
to do one PB and 100
gig of the multishare
within Filestore Enterprise.
Now I'm not actually
going to show
you some of the YAML files
for the reader and writer
deployments.
The reader pods
are actually going
to be exposed by the load
balancer for users to view.
And this is what I'm going to
actually show you at the end
here, to show you how the multi
rider capability of Filestore
supports this.
We're going to just
choose 20 writer pods.
These will all simultaneously
write to the same shared
storage every 30 seconds.
So in this case, it's writing
to that one PB Filestore
Enterprise that I
created every 30 seconds.
I'll refresh it so you
can see that increases.
Also skip the YAML file
for the load balancer.
This basically just exposes
the reader pod to Filestore.
And here's the URL
that I clicked on
to actually show you that I've
got 20 writer pods writing.
And as I refresh
this, you can actually
see that it's recording
all the writers that
are writing to the shared NFS
file system, the enterprise
that I just created.
It will actually show
you, when I refresh this,
the same writer hostnames
match each other
as you go through this.
And that's as easy as it is
to use Filestore Enterprise
in the multishare capabilities.
SEAN DERRINGTON: Great.
The last area I want
to share with you
is some best
practices with storage
and critical applications.
One of the things that
we've been focusing on
with critical applications is
our block storage offering,
Persistent Disk.
We've recently
announced Hyperdisk.
Hyperdisk is the next
generation Persistent Disk
that we've kind of rethought
what block storage should
look like in the Cloud.
We think it should be able to
be dynamically provisioned.
With block storage, and
particularly Hyperdisk,
we want to give you three
dials, if you will, to turn.
IOPS, throughput, and capacity.
Based upon how you need
to size your application,
you can tune all of
those independently.
But we also want to
give you the ability
to tune those across a
wide variety of workloads,
from throughput-driven workloads
to IOPS-driven workloads.
And we want to make it easy
for you to manage capacity
at scale.
Managing 10 terabytes of data
is very different than managing
a petabyte of block storage.
And that's where we've recently
introduced Hyperdisk Extreme.
As the name implies,
it's going to satisfy
the most demanding
IOPS-driven database
workloads, like SAP HANA.
And that's going to be
suited for a percentage
of applications, but
really the top tier
percent of applications.
We're introducing and announcing
Hyperdisk Balance as well.
This is going to support the
widest range of applications,
from throughput-oriented
to IOPS-driven.
And last but not least, we
have Hyperdisk Throughput.
As the name implies, those
that need gigabytes per second
of performance, not necessarily
IOPS-driven performance
workloads.
So all of these we've
rolled out over time.
Hyperdisk Extreme will be
in preview this quarter.
But underlying all
these is really about
how you can optimize the storage
utilization with Storage Pools.
With Storage Pools, you can
think about thin provisioning
as we're used to in on-premises
world now brought to the Cloud.
So you can create
a pool of storage,
allocate that storage
to multiple servers,
and essentially
overprovision storage
and only use what
you need to use.
Thin provisioning brought to
the Cloud with Storage Pools.
But let's take a look at what
this looks like in a SQL server
provisioning environment.
Today with Persistent Disk,
on the left hand side,
there are a variety of steps
you need to go through.
Different Persistent
Disk options
have performance
characteristics.
You have to size
the VM accordingly.
And you have
licensing implications
oftentimes associated
with the vCPUs
that your provisioning
for that application.
And if you get it right
on day one, that's great.
But if it changes
over time, you have
to go back and rechange--
reprovision performance
and reprovision Persistent Disk.
That can oftentimes cause
a lot of complications.
If you look at the
right hand side,
Hyperdisk dramatically
simplifies this.
You have those three knobs,
IOPS, throughput, and capacity,
to turn on day one
to provision what
you need based upon the
application requirements
and size the VM to what
the application needs,
not what the block
storage needs.
Additionally, over time,
after month 6 or month 12,
you can actually
dynamically change
that performance
characteristic, IOP, throughput,
and/or capacity, as needed.
So one of the things that
Hyperdisk Extreme is very well
suited for is the most
demanding database workloads
like SAP HANA.
This is an architecture
for SAP HANA.
It's highly available
within a region,
but then also replicated to a
second region for full disaster
recovery.
As you see, the HANA
database is supported
with Hyperdisk Extreme,
doing synchronous replication
with HSR.
Now that's covering the
database side of things.
But what about the shared files?
Those combination
of media files,
app server, et cetera, binaries
that need to be used for SAP?
That's where Filestore
Enterprise comes into play.
Filestore has four 9s of a
regional SLA availability,
which means that
we're synchronously
replicating across three
zones within a region.
So if the media server
is in zone B goes down,
the media servers in zone A have
access to the exact same data.
Recovery time objective of 0,
recovery point objective of 0,
just like the HRS replication
of the database layer.
But thinking about the database
and the app and the shared file
use case is important.
We also need to think
about backup and DR
for the entire system.
And that's where Google Cloud's
Backup and DR that we recently
announced comes into play.
This is one of the
things that now within--
directly within
the Cloud console,
you can choose and set a
policy to choose GCVE, GCE,
other applications
and databases,
seamlessly protect
locally and/or remotely.
So combination of Hyperdisk,
Filestore, and Google
Cloud Backup and
DR, you can protect
the entire critical
application environment.
And with Backup and DR, we have
a continuous incremental backup
strategy.
So you're going to maximize
that storage utilization,
either in region
or across region,
in addition to having very fast
recovery time objectives based
upon that incremental
forever backup policy.
With that, thank you so
much for joining me today.
Please check out
Google Cloud's channel
on YouTube for additional
tech demonstrations
and deep dives into many of the
technologies and topics that
are touched on today.
[MUSIC PLAYING]
BRAD BONNET: Hello, everyone,
and welcome to our session
on modernizing your data center
and accelerating your edge
with Google Distributed Cloud.
I'm Brad Bonnet, Senior
Director of Product Management
for Google Distributed Cloud.
And with me today is my
partner in crime, Rohan Grover.
ROHAN GROVER: Hi, I'm
Rohan Grover, Director
of Outbound Product Management
for Google Distributed Cloud.
And I'm really happy to be here.
BRAD BONNET: Thanks, Rohan.
Today we are seeing
a transformation
that is happening across
multiple industry verticals.
We are seeing a lot of
use cases in retail,
with companies like Loblaw,
the largest retailer in Canada,
is joining a journey of
digital transformation,
evolving from a
traditional retailer
to a technology leader.
Loblaw is looking to use
Distributed Cloud features
to add new capabilities, like
self-checkout, and enhance
the future of in-store
customer experience.
The telco vertical
is another industry
where we are seeing
significant traction.
Bell Canada is looking to
drive operational efficiencies,
increase network automation,
and deliver richer customer
experiences for their 5G network
modernization initiative.
Using the Distributed
Cloud will enable
them to provide increased
speed and bandwidth
capacity to the Bell 5G network
and support applications that
respond faster and handle
greater volumes of data
than previous generations
of wireless technology.
Another key use case is
in the public sector,
where we see demand to use
innovative technologies
at the edge to increase
worker or public safety.
Later in this session, Rohan
will talk about Australia Post
and how they are using GDC
and our Vertex AI solutions
to improve worker safety
in their warehouses.
The really challenging
aspect of this transformation
is that the
expansion often leads
to operational turmoil
for the platform
and IT operators and teams
who build and operate
across these sites.
When we talk to IT
admins, enterprises
are increasingly concerned
with security, governance,
and managing its scale.
Complexity increases across
edges, sites, and clouds
due to disparate control
planes and generations
of legacy applications,
data, and infrastructure.
As a result, many
organizations plans
to modernize and
transform have stalled
or are not delivering
the results
needed for their business.
Standardizing development,
security, and operational tools
enables enterprises to
increase flexibility
where it matters, modernizing
infrastructure applications
and data.
To do so, however, requires
operating platforms
that can securely scale from
on-premise to edge to the cloud
while remaining open to change,
choice, and customization.
And that's why we built
Google Distributed Cloud.
GDC is the next evolution
of our edge journey.
With Anthos at its core, GDC
is a cloud-centric platform
that enables enterprises to run
modern applications anywhere,
consistently at scale.
We offer a wide
spectrum of solutions
from manage software
on your own hardware
to fully managed hardware
and software services
to a completely air-gapped
sovereign offering.
GDC enables customers to
standardize development,
security, and operational
tools to build and modernize
application infrastructure
and using VMs, containers,
and Kubernetes in
their data centers,
at their edge locations,
and in the cloud.
So they can run more
applications in more places.
It uses Anthos to
deploy Google's
leading cloud-centric
Kubernetes platform, GKE,
and adds enhanced management,
security, and compliance
monitoring features.
Flexible by design,
Google Distributed Cloud
brings Google technologies
to where you need it most.
Based on an open
source foundation
and a vibrant partner ecosystem,
Google Distributed Cloud
gives customers the flexibility
of solutions, operations,
and form factors to
meet their unique needs.
It has built in intelligence.
We bring leading cloud
services, like Vertex AI and ML,
to where the data is being
generated and consumed.
So customers can harness
real-time insights
across deployments.
It's secure by default. You
can scale with confidence
using the best of
Google security
across edges, devices, and
our planet-scale network
from the simplest to the
most sensitive workloads.
And last but not least, it
gives you consistency at scale.
It uses Anthos, our
cloud-backed control plane,
to provide a common
experience for developers
and IT admins across
any environment.
So GDC can run in and help
you manage your environment
in any way you choose.
We have a variety of
different offerings
to match the needs
of your business.
And it starts out
with our GDC Edge,
which gives you a fully
managed solution of hardware
and software in one
connected environment.
We also have GDC Virtual,
which are software-only
based solution, that allows
you to bring your own hardware
and manage it from our
connected control plane.
Last but not least, we
have our Hosted option.
With GDC Hosted, we
give air-gapped hardware
and software that can
be managed by Google
or a trusted partner for
the most sensitive workloads
in the world.
So our journey so far
has been an evolution.
It started with Anthos
launch in April of 2019.
And we've continued to
double down and innovate
on this platform.
And continuing on
that strong momentum,
today we're very
excited to announce
the general availability
of our GDC Edge
GPU-optimized configuration
for your AI and ML
graphics intensive workloads.
Many of our customers,
from retail, manufacturing,
and automotive
sectors, are already
testing this new
GPU-optimized SKU
to deploy visual inspection
and worker safety applications
in their facilities.
With the GDC Edge
GPU-optimized config,
you now get the horsepower
of 12 NVIDIA T4s
that can together
handle up to 300 camera
feeds at the same time.
This opens up a whole
new window and segment
of on-prem
applications that need
to return results with low
latency and high accuracy.
And so with the
GA GDC GPU-config,
we're very excited
about how customers
are going to innovate
with this new capability.
And now I'm going to
hand it off to Rohan
to talk about the GDC services.
ROHAN GROVER: Thank you, Brad.
It is so exciting to hear
about all of these innovations.
And I'm going to
talk to you about how
one of these customers
is using this innovation
in a few minutes.
But with that, I do want to talk
about our portfolio of Google
Distributed Cloud Services.
One of the core
value propositions
of Google Distributed
Cloud is our commitment
to provide a
variety of services,
both for Google
technologies, as well
as our open source and
third-party partner ecosystem.
With our commitment
to open source,
we believe that we can
innovate faster and allow
customers to deliver
their business outcomes.
For our public sector
customers, using our open source
foundation also provides some
guarantees of portability,
as well as meets their sovereign
and compliance requirements.
If I talk about
Google technologies,
I'm super excited to mention
that our industry-leading AI/ML
capabilities are now available
on Google Distributed Cloud.
We announced this back in June.
And this includes our Vertex
AI/ML set of solutions.
One of the key services
under that portfolio
is Translate API.
This allows us to translate
text in hundreds of languages
almost instantly and opens
up a whole host of use cases
in industries from finance
to manufacturing and public
sector.
Another key AI technology
that we are opening up
on the Google Distributed
Cloud portfolio is Vision ML.
And I'm going to
talk a lot about how
Vision ML enables one of our
customers as we move forward.
In addition, we're
also allowing customers
to use our database services
on Google Distributed Cloud.
One of the key use cases
for data on the edge
is sovereign requirements.
A lot of countries, a lot of
industries, including finance,
have requirements to
have data locally.
And because we've enabled
our new Omni database service
in Google Distributed
Cloud, they
can now use either our
PostgreSQL database
or an Oracle database
to do just that.
Our services
journey is evolving,
and over time, we will continue
adding additional Google
technologies, as well as open
source and third-party services
to the portfolio.
So to recap, Google
Distributed Cloud
has offers designed for greater
choice and customization based
on your unique needs.
Let's talk about a few
of the customers that
are adopting these solutions.
GDC Edge and Virtual, which
is our connected portfolio,
they're being used in
multiple industries.
Some customers in the telco
field that are doing this
include AT&T, Bell Canada,
Telenor, as well as
Reliance Jio.
We have customers
from other verticals,
including financials, like
HSBC, who are, again, utilizing
the capability of our platform
to provide unique value
to their customers.
And last but not least, we have
the media and entertainment
type of customers, like Major
League Baseball and Telegraph,
who are also using
this to provide
cutting-edge experiences
for their customers.
On the other end
of the spectrum,
we have Google
Distributed Cloud Hosted,
which provides air-gapped
hardware and software
solutions to meet
the most stringent
of sovereign requirements.
We have partnerships with the
likes of T-Systems in Germany,
as well as Thales in
France, so that we
can work with the public sector
agencies in these countries
and really meet all of their
compliance requirements.
We are super excited
with the traction
that our Google
Distributed Cloud portfolio
is seeing across the board,
across multiple verticals
and multiple countries.
And I now want to talk about one
specific use case that is using
one of these technologies.
So Australia Post,
as we talked about,
is an extremely large Australian
public sector organization.
It is a postal
mail and logistics
company that employs 35,000
workers across the country.
And it delivers
mails and parcels
to millions of their
customers across Australia.
This company has multiple
distribution centers,
or warehouses, that are
spread across Australia,
which they use to store all
of these millions of packages.
And one of the key concerns
that their execs had
was around worker safety.
As you can imagine, a warehouse
is a fairly chaotic place,
and lots of things can happen.
And the reality is 90%
of safety incidents
happen as a result
of human behavior.
So our goal was to minimize
the amount of safety risk
exposures.
And the way we did that is we
use one of our innovative AI
technologies, called Vision ML,
which is part of our Vertex AI
suite, and we put that on our
Anthos-based Google Distributed
Cloud solution.
This solution aggregates
feeds from all
of the cameras in
these warehouses
and then essentially pinpoints
worker safety issues.
One key worker safety issue is,
as you can see on the slide,
there's a forklift that's
going dangerously close
to an employee.
Now that could result
in injury or even death.
And our AI is able to
pinpoint those issues
and essentially
teach the workers
and teach the staff
at these sites
to avoid some of these issues.
Our objective was to provide
a 50% reduction in safety risk
exposures.
What we achieved in our
first pilot site deployment
over nine months
is an 83% reduction
in employee risk exposure.
It's a phenomenal result.
And as a result of
that, Australia Post
asked us to do this in
production in 29 major sites.
And with that, we achieved
a mindblowing 98% reduction
in safety risk exposures.
As you can see, this
technology is already
helping organizations
around the world.
You can see multiple
examples of this.
There are verticals
like manufacturing
where you can imagine
this technology being
used for the same use cases.
You could also see this
being used in retail stores.
Let's say there's a spillage.
And the AI technology can
help identify the spillage
and prevent a customer
or an employee
from causing injury
to themselves.
So with that, I
do want to hand it
back to Brad, who take us home
and summarize the session.
BRAD BONNET: Thank you,
Rohan, for walking us
through how our GDC services
and application ecosystem have
evolved over time, as
well as that deep dive
into Australia Post.
What a fantastic outcome.
So today, we started
off by talking
about how we're seeing the
world evolve and drive needs
for use cases outside
of traditional cloud
and how Google
Distributed Cloud is best
suited to meet those needs.
We talked about our portfolio
of solutions with GDC Edge
and Virtual for
connected use cases
and with GDC Hosted for our
disconnected sovereign use
cases.
Finally, we talked about
our services and application
ecosystem and how Australia Post
is using one of these services
to solve a real world problem.
I wanted to thank my
awesome co-presenter, Rohan,
for being with me today, as
well as all of you at home
for watching this presentation.
I hope you have a great rest
of your day, and thank you.
[MUSIC PLAYING]
SHAQUILLE O'NEAL: Hi.
I'm Shaquille O'Neal, and I'm
the founder of Big Chicken.
See, you got to do that then
when you say Big Chicken.
JOSH HALPERN: Big Chicken is
Shaquille O'Neal's emerging
fast casual
restaurant chain that
focuses on big fun, big
flavor, and big food.
When you're trying to
build a national chain,
communication is so critical.
To do that, you need
a great partner.
And we're really lucky that
partner is Google Workspace.
SHAQUILLE O'NEAL:
You know, Josh,
every time he does
a presentation,
he just loves Google Slides.
JOSH HALPERN: As the person
responsible for our marketing,
probably the best
at Google Slides.
SHAQUILLE O'NEAL:
His presentation
is like I'm at a
movie some time.
I'm just sitting there going--
MATTHEW SILVERMAN: I've
got some great, new chicken
sandwiches for you to try.
Brand new recipes.
Isn't there something
important we're
supposed to be talking about?
Good recipe development
comes with collaboration.
Using Docs in Google
Workspace gives us
the tools we need to
collaborate together.
JOSH HALPERN: Shaquille's
life gets crazy busy,
as does our entire board.
SHAQUILLE O'NEAL:
You want to talk
to me, make sure you put
on my Google Calendar.
Google Calendar
is my girlfriend.
I don't know anything I'm doing
unless I talk to my woman.
Google Workspace.
Productivity and collaboration
tools for all the ways
you work.
[MUSIC PLAYING]
TYLER: Farmers, they have an
interest in sustainability.
But what's been lacking
historically for growers
is having the data and
the tools that they need
to be able to do that well.
ADAM: And that's
the reason we're
so excited to work with farmers,
to give them the guidance that
will help them succeed.
TYLER: I'm Tyler.
ADAM: I'm Adam.
TYLER: And we're the
co-founders of Agrology.
We'd been kicking
around the idea
of how do we use a lot of
different sensors and machine
learning and sensor
fusion to deliver
predictive insights to our
growers to thrive in a changing
climate.
Traditionally, growers,
they try and do
these predictions themselves.
But they can't be on every
inch of every farm every day.
ADAM: They also have to
deal with a climate that's
changing on them that makes
every year harder and less
predictable.
We use TensorFlow
and Google Cloud
to train the models that
take our data streams
and turn it into predictions
for our customers.
TYLER: That aha moment
for us was when we first
saw the results and predictions
for the first growing season.
That was huge.
We can tell a grower if their
crops are at risk of stress
or if they're approaching
some irrigation thresholds.
We can warn them if they're
approaching wilting point
in a particular
area of their field.
ADAM: The process for preparing
to work with farmers is first
jumping onto Google Earth so
we can understand their land.
And then we have to
think about where
we could put our
devices in their fields
to deliver them insights.
Our app takes data from our
sensor arrays in the soil
and in the air.
And it runs it through our
machine learning models
to make predictions to
warn farmers about problems
before they emerge.
TYLER: The synthetic models we
generate, the forecast models
we generate, it's just
impossible without TensorFlow,
without Google Cloud taking
all the data from every block,
from every site, and forecasting
that three to four days out.
That's just something
that you're not
going to be able to duplicate
with a single human being.
Giving growers the
tools to actually see
what's happening
in their field is
the key to helping them
be more sustainable.
ADAM: It's thrilling,
and it makes
the journey and the
challenges worthwhile.
[MUSIC PLAYING]
SNEHAL PATEL: Hi.
I'm Snehal Patel.
I lead product management
for Anthos and GDC Virtual.
I'm really excited to talk about
Anthos and all the innovations
we are bringing
to help you solve
your most pressing problems,
help you solve your customers'
problems.
Today we're going to
talk about three things.
Number one, what is Anthos.
Number two, what's
new with Anthos.
And number three, how does
Anthos power GDC, as well
as the capabilities on
GCP and multi-cloud.
So let's get started.
Today customers want
a cloud-centric and
container-based operating
model everywhere.
Container adoption
continues to expand rapidly.
And that means more apps
and infrastructure are being
modernized in more locations.
Financial services companies are
creating advanced risk analysis
models.
Retailers are rebuilding
e-commerce platforms
and reimagining retail
edge applications.
Entertainment companies are
analyzing game statistics
in real-time in ballparks.
And healthcare companies
are using advanced AI/ML
to research new therapies.
This expansion too often
leads to more operations
total for the platform and
the operating IT operations
team, who build and
operate container platforms
for the developers and
the application operators.
And it's not only expansion
outside public cloud, hybrid,
and multi-cloud, but
growing use of clusters
in Google Cloud as well.
All these development tests
and production clusters,
plus all the different
clusters for tens
or hundreds of different
workloads, either newly built,
migrated, or
modernized from legacy,
can become difficult to
configure, monitor, secure,
and optimize.
Anthos was created to
address the growing
demand for hybrid and
multi-cloud solutions
for enterprise customers.
Anthos is our
cloud-centric platform
to run modern apps anywhere
consistently at scale.
This means Anthos
runs on Google Cloud.
Anthos runs on other public
clouds, such as AWS and Azure.
Anthos runs on-premises,
like data centers.
It also runs on premises
like edge locations, stores,
and restaurants.
Anthos is the platform that
brings all those pieces
together.
So you can have
one common platform
that simplifies your operations
and brings consistent security
and governance across the board.
So what problems
does Anthos solve?
Number one, it helps you
modernize apps and infra
in-place.
Next, it helps you scale large
multicluster applications.
This is one of the top
concerns from customers who've
deployed Kubernetes at scale.
Number three is to bring
consistent governance
and security in all
the environments.
Our customers identify this as
one of their top challenges.
What is included
as part of Anthos?
Right.
To start with, Anthos operates
in all the environments,
whether it's Google Cloud,
on-premises, AWS, Azure.
You also have something
called attached clusters.
We'll get to that second.
Next, what we want
to do is, Anthos,
we can manage all of it,
the entire cluster lifecycle
management from GCP.
What this means is
through GCP, you
can manage a full
cluster lifecycle
management in any environment.
This is what I call
Kubernetes as a service.
Next is Kubernetes at scale.
This is where you
can really help
our customers simplify
multicluster automation
and configuration.
Imagine hundreds and
thousands of clusters.
How do you manage them?
How do you deliver same
consistent governance
and security?
How do you create landing
zones for your developers
so they can focus
on developing apps
and not have to worry
about multiple clusters?
On top of our
multicluster automation
is compliance and governance,
allows our platform teams
to define policies, whether
they are organizational
policies or
standards-based policies.
So you can bring consistency
in every environment.
Next is to use service mesh to
bring microsegmentation as well
as visibility.
And third is their
operations, which
means understanding
your performance
SLAs for your applications,
whether we are meeting them
or not.
And then taking the remediation
action to get the SLAs we want.
We also integrate with developer
experiences and services,
whether they are
GCP or third-party.
We'll talk more about
it in the later slides.
So now that we know what
Anthos is all about,
let's talk about
what's new with Anthos.
All right.
So first, we made it simpler,
easier, and has less friction.
Number two, now we are
delivering consistency
at scale, meaning
multiple clusters,
helping your developers focus
on developing apps and not
the infrastructure.
And number three, now we
can expand Anthos and GDC
to your retail edge locations.
So let's start with the first
one and do a deeper dive.
First, it's a fully
hosted GCP service
that allows full cluster
lifecycle management no matter
where you run, meaning your
clusters can be running on GCP.
Your clusters can
be running on-prem.
Your clusters can be
running on AWS, Azure,
or other public cloud.
You can manage those
clusters directly
through GCP, either
through our consoles
or through our G Cloud.
It's a fantastic way
to automate this.
It's a fantastic way to
bring more capabilities
from an operations perspective.
Next is the attached clusters.
Remember we talked
about this before?
So if you already have
Kubernetes deployed
in other clouds or
other environments,
using other communities'
orchestration or platforms,
and you want to bring the same
consistency and same Anthos
solutions to those
clusters, what do you do?
A simple way is to attach
those clusters into Anthos,
what we call attach clusters.
What that means is that once
you attach them into Anthos,
takes a few minutes, then
you can apply same policies.
You can bring the
same observability.
You can also bring the same
fleet-wide capabilities
to all those clusters.
It's a fantastic way
to get started sooner
and delivering value
sooner to your dev teams.
Next, single pane of glass.
Everyone wants us.
We have it.
Right?
Number one, from Anthos on GCP.
You can see all your clusters
no matter where they are.
You can do a deeper dive into
them get better visibility.
All right.
That's enough about how
we've made it simpler.
Let's talk about how
we help you scale.
This is our number one
ask from our customers.
How do I scale?
How do I scale?
How do I scale?
Right?
So we are delivering
a lot of innovation
to help you get there.
Number one, why do customers end
up creating multiple clusters?
As a number of they want
to create a large operating
environment.
And once they create that
large operating environment,
they can create
different tendencies,
each tendency within the
same operating fabric
made up of multiple clusters.
This is something that we
call multicluster management
via fleets.
Now what does this
allow you to do?
Right?
Number one, you can create
different tendencies.
You can apply the same set of
security to all those clusters,
meaning now your
dev teams don't have
to worry about figuring
out how to secure them.
Your platform team can apply the
same policies to every cluster.
As soon as they
apply to a fleet,
all the clusters get the
same set of policies,
same security configurations.
So what does Anthos multicluster
management really do?
Number one, visibility.
All your clusters
across the entire fleet,
you can get the key status
and configuration information.
Number two, it's as simple
to connect into a fleet.
Go in there.
Connect.
Voila.
Done.
Third, you can start
assigning different features.
Let's say you want
to common config
management across all of them.
Let's say you want to apply the
same service mesh capabilities
to all of them.
Number three, you could also
do common identity services.
You can deploy Cloud Build
and Cloud Deploy as well.
This makes it super powerful,
simplifies the management
across multiple clusters.
This is our brand new dashboard
for fleet or multicluster
capabilities.
It helps you
fleet-wide visibility
for your container
infrastructure
across all your environments.
Includes key information
about resource utilization,
infrastructure,
policy violations.
It also integrates
with Cloud Ops
for deep analysis and alerting.
Compliance and governance.
Once you define your policies,
either standards-based or based
on your own
organizational policies,
the dashboard allows you
to get quick visibility
into which policy
violations are there,
whether they're in audit mode
or they're in enforcement mode.
It helps you better
understand what
you need to do to remediate
and get back into compliance.
That is something we
are super excited about.
A lot of our customers,
this has been our number
one ask from our customers.
All right.
Now let's talk about
expansion to the retail edge.
Who's excited about this?
I am.
All of our customers are.
They want to deploy
Anthos to retail edge
because this allows them
to fundamentally transform
the retail edge.
And it helps them bring
personalized offers, worker
safety, without having to
replace their existing store
infrastructure.
How good is that?
Right?
So what are our customers
trying to solve for?
Number one, well,
it's not easy, right?
Let's start with that.
Number one, the app
deployment cycle is long.
There is limited operational
visibility at scale.
Imagine hundreds,
thousands of locations.
There are multiple
failure dimensions.
You hear things like
someone by mistake
knocked the power cable, and
down goes the infrastructure.
You have other
things like you have
a heterogeneous
mixed infrastructure.
You have applications that
were built to run on VMs.
You have modern
applications that are
designed to run on containers.
Can't have multiple platforms.
You need one way to
manage all of that.
GD Anthos, powering
GDC Virtual, helps
you solve those challenges.
Number one, through Anthos,
you can deploy fast and secure
applications across the board.
You get unified
operational visibility.
You get resilient
architecture, meaning
if you lose connectivity, or if
you have limited connectivity,
your applications
still continue to work,
meaning you still get to
deliver value to your customers.
And number three, it leverages
your existing infrastructure
and hardware partnerships
that you might have.
To do this, we've recently
delivered Anthos for VM.
It's a unified
cloud-backed management
for containers and VMs.
So what does this really mean?
Imagine a deployment
where you've
got your modern applications.
But you have some
third-party apps
that are not going
to be containerized.
What do you do?
Anthos allows you to migrate
those apps to Anthos for VM,
and now you have
a common platform
to manage both of those.
Number one, it provides a
unified dev and ops experience
for your VMs and containers.
Number two, it's a developer
self-service provisioning
of VMs.
It's a declarative deployment
model for simplified DevOps.
It provides the same
policy enforcement
for consistent compliance.
We also have a tool
that we deliver
that allows you to
quickly determine
whether that VM-based app
will work on the Anthos
for VM runtime or not.
So that is amazing
amount of innovation
that we're delivering.
And I'm super excited about it.
So how does Anthos power all
of this stuff on GDC and GCP?
It's a lot of stuff
going on, right?
So number one, Google's Anthos
helps bring cloud operating
model everywhere, whether you're
on GCP, whether in Azure, AWS,
or on Google Distributed Cloud.
Anthos powers both of those.
It gives you a
common control plane.
It brings container
orchestration and management
across both of them.
It brings common policy
and security automation.
And it brings operations
in service management.
Anthos powers GDC in three ways.
Number one is Google
Distributed Cloud for the edge.
This is where Google delivers
a managed hardware and software
solution designed for low
latency and data resiliency
and hybrid workloads.
Number two, and GDC Virtual.
This is our
software-only solution
that works on your
hardware and your OS.
It's great for retail edge.
It's great for
environments where
you want to leverage your
existing hardware investments.
And the third one is a
completely air-gapped solution
called Google
Distributed Cloud Hosted.
On GCP and multi-cloud,
Anthos powers three ways.
Number one, Anthos runs on GCP.
It delivers an
integrated solution
that brings together GKE,
fleet, service mesh, policy
configuration, and
security on GCP.
Number two, Anthos
on multi-cloud.
It brings together
Google's Kubernetes,
fleet, service mesh, all
the other capabilities
we talked about, to AWS, Azure.
And finally, Anthos
attached clusters.
It's our-- it
allows our customers
to attach their existing
clusters into Anthos
so you can take advantage
of our service mesh policy
configuration and security for
all your existing Kubernetes
clusters.
So to recap, Anthos is our
cloud-centric container
platform to run modern apps
anywhere consistently at scale,
meaning it runs on Google Cloud.
It runs on other public clouds.
It runs on-premises,
data center,
and it runs on your edge.
We are delivering
three key innovations.
Number one, we've made
it simpler and easier.
Number two, we are delivering
consistency at scale.
Think hundreds of
clusters or thousands.
And Anthos now expands
to retail edge.
Thank you so much for your time.
I'm super excited with all
the innovations that we have.
I'm super excited to help
you innovate and deliver
new capabilities
to your customer.
Thank you so much for your time.
[MUSIC PLAYING]