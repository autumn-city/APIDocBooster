 Alright, let's now take a look at a code implementation
 regarding the character RNN that we talked about in the previous
 video. So in the previous video, I gave you some overview of the
 LSTM and the LSTM cell. And I prepared two notebooks. One is
 based on using I can maybe just scroll down for now on using the
 LSTM class and one is based on using the LSTM cell class.
 Personally, I think for this type of model, it makes more
 sense to use the LSTM cell, it's a little bit I think, more
 intuitive. So going back to what I talked about in the previous
 video. So here, that's the LSTM cell class, and we'll work, we
 will only consider one layer, I mean, we can easily extend it
 for multiple layers, but we will only have one layer. So let's
 blend that part out. And we will essentially, so if you consider
 this part, we will receive one initial hidden state, and one
 initial cell state, together with that input token, like
 this, it will receive one input token, hidden state cell state,
 and then produce one output, and this output will go to a fully
 connected layer to do the character prediction, then we
 will move to the next input. So here, the next input, it will
 receive the hidden state from the previous time step and the
 cell state from the previous type step, together with a new
 character, and then it will produce again, an output that
 goes to a fully connected layer to predict the next character.
 Then we will go on here again, it will receive receive the
 hidden state and the cell state from the previous time step, the
 current time step input, output something and so forth. So
 that's how the LSTM cell class works. All right, but before we
 get to this part, the LSTM cell class, let's start at the top.
 Let me even because it's quite fast to run. Let me even run
 this from scratch. We haven't done this in a long time. All
 right, so I mean, from scratch, I mean, by executing it as we go
 here. So we start by implementing by just importing
 some libraries. So here, I didn't use any helper files, I
 tried to keep everything in the notebook, because the code here
 is relatively short and simple. So here, I have some hyper
 parameters like the text portion size. So how long a typical text
 portion is the number of iteration for iterations for
 training. So here, we don't use epochs, we just use iterations.
 Learning rate, the size of the embedding and the size of the
 hidden layer. Okay, execute that. Running this on a CPU. I
 actually had problems running this on the GPU. I think there's
 a bug in pytorch. So it runs fine on the CPU, but on the GPU,
 it will restart the kernel. And if I run this in my terminal,
 without Jupyter Notebook, it gives a segmentation fault. And
 my suspicion is that it's a bug in pytorch. And because it's
 maybe related to the fact that it's loading the data too fast,
 and there's some, it's trying to access some memory in the GPU
 that is not free yet. So actually, this shouldn't happen.
 So I believe there's a bug somewhere. But anyways, it's
 quite fast, we can run this on the CPU. Okay, so we use this as
 our character set from the string Python library. So string
 is like a standard library in Python for string related
 things. And we will use all printable characters. So a bunch
 of them actually, how many are there? Let's see 100. So we will
 want to use the set of 100 printable characters. So numbers,
 lowercase, uppercase letters and special characters. Yeah, so
 actually, so we will use has the data set the COVID-19 FAQ from
 the University of Wisconsin website, I went actually to that
 website, and extracted all the FAQ questions here. So as you
 can see, there was a lot of text. So I just copied everything
 into a text file. So this is our training set. So COVID-19
 related questions on our university website. Okay, so
 here, I'm just opening and loading the text file. So in
 total, we have 84,000 characters in our text files like a small
 book, almost. Okay. So here, I have a function for getting a
 random portion of the text of size, text length. So we have
 this text portion, sorry, my text portion size 200. So it
 gets text of the portion size 200 from the whole text here
 randomly. So this will be our training batch. Okay, so this is
 a random portion. So you can see some letters are chopped off.
 But for our simple case here, it's good enough. So again, this
 is not perfect. So you may in real world application, you
 won't make me want to implement the function that it has like
 complete sentences or something like that. To just keep things
 simple. This is just a simple function, just getting 200
 characters at a time. Then I have a function converting the
 characters here to tensors. So here, this is just getting the
 index for the characters. So if we have 100 characters, it gets
 the index, right? So a would be index 10, and so forth. So it's
 converting the strings into numbers that we can work with
 in pytorch. And this is putting those things together. So this
 is for drawing a random sample for training. So this is just
 getting a random text portion and string format. This is for
 converting a string to integers. And this does both. It's getting
 the random portion, right? converts that into integers.
 And then it also gets our labels. So the labels are the
 inputs shifted by one character, because here, our task is yet to
 predict the next character in the sequence, right? So if I do
 that, and draw my random sample, so my random samples are 094,
 24, and so forth. And you can see, the target is shifted just
 by one, right? Because if we are here, and we want to predict
 the next character, the next character is 94. And from here,
 the next character is 24, and so forth, and integer
 representation. So this is our, our batch of features. And this
 is our, these are our labels. Okay, so here's now our RNN
 implementation. So I just have something to keep track of the
 hidden size. This is our embedding layer that goes from
 the integer, the character integer to the embedding vector,
 a real value vector of size, let me see of size 100. And the
 hidden dimension is 128. Okay, so we have the embedding size,
 and then the LSDM cell takes vectors of size 128, and has a
 hidden size of one, sorry, of 100, and has a hidden size of
 128. So if I go back to my slides, maybe using this
 representation here, so our text, we had here a one hot
 encoding. This is when we want to compute the loss, we use
 actually just an integer here. So here, this would be the
 integer two, the integer zero, and integer one, if you look at
 this here, right, so this one in this figure would correspond to
 an S, for example. And then the embedding layer will output 100
 dimensional vector, and the hidden layer will be a 128
 dimensional vector. So let's see how we use that actually in the
 forward pass. So in the forward pass, we first put the character
 through the embedding. So this will be accepting batch size
 embedding dimensionality, we use only one character at a time. So
 it will be one times embedding dimensionality, which is in our
 case, 100. Then we give to the RNN, which is an LSDM cell, we
 give the embedded vector, which is the one times 100, together
 with a hidden state, and the cell state from the previous
 iteration. So this, if we are here, essentially, or maybe use
 the other representation again, so if we are, let's say the
 first step here, so we are currently running here, this
 RNN, it will get the hidden state and the cell state from the
 previous iteration. This is these two. And these we provide
 them via by the forward pass as input. So these will go into
 this one, and these will return a new set of hidden and cell
 states. So here, these are the inputs. And then they return
 these outputs here, these two for the next round. And then
 this is computing our logits for the softmax for the cross
 entropy loss. So this one output here, this is essentially this
 one through a fully connected layer. So this will be it's
 clearer like this. So this will be like this. So we'll be giving
 us one output where here we have a fully connected layer in
 between, gives us one output. Okay, and we return the output
 because we use it for computing the loss. And also, when we want
 to take a look at the text so that we can generate some text,
 the next character. And then also, when we want to generate
 text, of course, we have to feed the output back into the input.
 So if I go back, there are too many slides here, when I go back
 here, this visualization, this is for training, but for
 generating text, we feed the output here. So we get this
 input, produce an output, and the output gets fed to the next
 time step is input, so that we can generate new text. Um, yeah,
 anything else? Yeah, just the dimensionality for reference.
 Anything else? This is not right here. I think this should be on
 definitely not this one should be the number of characters. Oh,
 but here I said hidden state output size would be the output
 size fixes. Okay. Yeah, then we so we are one more thing. We
 have this initialization of the zero state. So here, we have to
 start somewhere, right? So if I go back to my visualization,
 here, so we get rid of one of those. So here, we don't have
 initial input input input is here. So we have to have some
 some zero state here. And this is here, my zeros, just just some
 initial state. Okay, and all right, then let's get started.
 So actually, the output size, I should mention this is the same
 size as the input size. Yeah, so let's initialize the RNN. So as
 input size is the length of the number of characters, it's 100.
 So the output size would be also 100. In between, we have
 the embedding and hidden dimensions. I'm using Adam.
 Just simpler. He has an evaluation function. Let me get
 to the evaluation function in a second. Let me first run this.
 And this can already execute this. But I will talk about
 this after after talking about the main training loop, because
 the evaluation function here, it's actually used as it used
 here. So it's just one tiny part of the training function. So
 let's talk about the big picture training function first. So I'm
 just iterating 5000 times 5000 steps, then I have my
 initialization here. So this is for initializing my zero state.
 So this is for initializing here, these initial states, I
 can actually make this bigger, right? So this is for initializing
 these zero states here. This for each iteration, so each, so each
 iteration will go through one text portion of size 200. So for
 each text portion, we initialize it as zero, this is our
 beginning of the text. What we do is we set the loss to zero,
 draw a random sample. So again, the random sample will be like
 this, just some text, some random random text portion, and
 the targets shifted by one value here. We put it on the GPU. In
 our case, nothing happens because we use the CPU. Then for each
 character in the text portion size, this is where we do the
 step, putting it through the model. So here, this is just
 for making the dimensionalality right, because this is just one
 single value. But as you recall, we want one times sorry, batch
 size times one. So it should be a 2d tensor. So we are doing
 unsqueezed adding empty dimension. We provide the hidden
 from here, the cell state from here, these initial cell states.
 So in the first round, these will be the initial ones. But
 then we are also outputting them, right? So it will feed
 back right back. So in the next iteration, these will be used
 from the previous iteration. So it will feed right back. And we
 are computing the loss, we are just adding the loss here. So we
 are computing the loss between the outputs and the targets one
 at a time. And then we just normalize by the text portion
 size. So we have, it's just the average the mean, mean over the
 batch size, if you will, because we add, let's say 200 losses,
 and then we divide by 200, just averaging that's just so that
 it works better with a learning rate. So we can change the text
 portion sizes and shouldn't have to worry about changing the
 learning rate. Then we call backward, make a step and update
 step. And that is it. So here, we just have some logging, and
 some more logging, it will create a PDF with a plot, the
 loss plot, so we can take a look at it during training. There's
 also a tool called tensor board, which is actually pretty useful.
 But we already have so many things to talk about and code
 examples and everything, I don't want to make it more complicated.
 So we're just using matplotlib here. But yeah, the one last
 interesting part before I run this is the evaluation function.
 So instead of just printing out the loss, in addition to that,
 I'm also evaluating the model. So what do I mean by that? I'm
 letting it generate text. So let's take a look. This is my
 this is my evaluation function. So we initialize it to the zero
 state, then we build up some hidden state, we are priming
 this essentially. So what that means is, we are providing some
 prime character is like some starting character here, place
 holders a actually use th. So all texts will start with th that
 we are generating is just I mean, it's arbitrary could be
 anything. So all texts will start with th. And then here,
 we're priming it, we are building up some hidden states
 so that the model stabilizes. So we are doing this for the
 letters in this prime range. So we only have two letters, right?
 We can have actually more we can have some real words, spills up
 all our cell state. So it's just, it's just essentially
 running in this case, through two of these, right, one, two,
 and then we get to this part. And for each, in the prediction
 length, we are generating text of size 100. We are just running
 it as before. So again, there's nothing special, it's just
 running the model, similar to how we run it in the training
 loop. The new part now, though, is this part. So here could have
 also actually written it simpler like this, this is just a
 division. So we're dividing by a temperature. So what is the
 temperature? So first of all, outputs are our logits. So if
 we go back to our model here, these are just here, we don't
 use any softmax, because the softmax is usually used in a
 cross entropy function. So here, we have just our logits. And
 then we compute, I mean, we're not really computing the softmax
 as a normalization factor in the softmax, right? So we are usually
 normalizing by the sum of all of these for each class. Here, we
 are lazy, we don't do that, we just take e to the power of so
 essentially, it's e to the power of logits divided by the
 temperature. And the higher the temperature is, so maybe the
 other way around, if I have logits, and I have a small
 number, like the temperature is usually a value between zero.
 And one, and I should say, why, what's the temperature here,
 this like, I think it's inspired by energy based models, but
 which is in turn, in turn inspired by the Boltzmann
 distribution. But it's essentially how sharp our
 distribution is in this case. So if we have a small value here,
 like point one, all the values will become larger. Yes, we will
 get larger values. If we have a value 1.1, 1.0, they will be
 on softer. So we have here, there's a real main interesting
 part, we will here have this multinomial function, which is
 essentially a function on that randomly samples from a
 multinomial distribution. Using these, you can think of them as
 weights, or probabilities. And this is our the number of
 samples we draw. So here, we're drawing one, one character from
 the predictions. So outputs will be the logits for all 100
 characters, because we have 100 different characters. So logits
 will be all values. If we have softmax, these would be all the
 probabilities corresponding, corresponding to these
 characters. And here we are randomly sampling. So if we, so
 we expect expectation would be sampling, the character also, we
 would most often sample the character corresponding to the
 highest probability. And we can soften this by having a 1.0.
 This is the standard setting temperature of 1.0 is if we use
 no temperature, this would be our regular values. But if we
 use a smaller value, let's say 0.1, this will be a sharper
 distribution. So the most character with the highest value
 will be sampled more often. So even more often, so we can
 actually set the temperature very, very small. And then it
 will essentially be like not sampling at all, it will always
 sample the character with the highest probability. And that
 way, it's more like deterministic. So you can think
 of it like that, the higher the temperature, the more diversity
 in the output in the generated text you will get. So if you
 want to have more diverse text, you you can lower the
 temperature, it's it's kind of, you can increase the temperature
 if you want to have no randomness, you lower the
 temperature, higher temperature means more diversity, you can
 think of it also as it's kind of like heat in a biological system
 where you if you heat up the system, you have more kinetic
 energy, everything is more like wiggling around and things like
 that. So here we have like point eight, it's like a trade off,
 but it's a hyper parameter. If you have higher temperature,
 there will be more mistakes, but also more diverse text, it's
 like a trade off. Okay, can play around with that if you like,
 when you do the training, but let's just use the ones that I
 used to begin with. So it's running now, it gets a pretty
 high loss. And this is the initial text, notice that it
 starts with a th, but the rest is like, Ghibli Gok, there's
 nothing reasonable in here is just some arbitrary text. But
 yeah, it already finished the next iteration. Let's take a
 look at that. Still, not anything useful, you can see it
 learned something, right? So you can see, okay, these isn't an
 actual work here. So it's learning something. Let's take
 a look at the next one. Okay, we have some real words here. Kind
 of learning something. Now, you can see students. So again, this
 is trained on the covert FAQ, which smote mostly readable text.
 Okay, so it's getting, it's getting a little bit better. I
 mean, of course, this is like not real text, it's not a very
 sophisticated RNN, it's just one layer, very simple, very small
 text. Also, I mean, it just learned for one minute, right.
 Right. So but you can see that it actually seems to learn
 something, if you think about it started from this, had then went
 to this. And now it's kind of talking about a safer badger,
 badgers app notification system. So it's it's kind of learning
 things, right? Wisconsin COVID-19 testing. Yeah, we'll
 have to run way longer to get some sensible inputs. Again,
 there are also many hyper parameters to tune can have more
 layers, you can change the temperature and these types of
 things to get better results. But I think I feel like I mean,
 given that I'm on the CPU, just running it for two minutes, it's
 quite cool that it can, yeah, can create text that is not
 total nonsense. I mean, it's total nonsense, to be honest,
 but we can identify individual words. So that's actually quite
 cool. Okay, so let me let me stop it right here was a long
 video. And in the next video, we will take the next step talking
 about how we can outfit RNNs with the so called attention
 mechanism.