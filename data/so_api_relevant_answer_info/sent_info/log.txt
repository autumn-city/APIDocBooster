Logarithmic running time (O(log n)) essentially means that the running time grows in proportion to the logarithm of the input size - as an example, if 10 items takes at most some amount of time x, and 100 items takes at most, say, 2x, and 10,000 items takes at most 4x, then it's looking like an O(log n) time complexity.
It simply means that the time needed for this task grows with log(n) (example : 2s for n = 10, 4s for n = 100, ...).
And log2 12 = 3, which is a good apporximate to how many hits where needed.
If you plot a logarithmic function on a graphical calculator or something similar, you'll see that it rises really slowly -- even more slowly than a linear function.
This is why algorithms with a logarithmic time complexity are highly sought after: even for really big n (let's say n = 10^8, for example), they perform more than acceptably.
Divide and conquer algorithms usually have a logn component to the running time.
It should be noted that in Big-O notation, log is log base 2.
Edit: As noted, the log base doesn't matter, but when deriving the Big-O performance of an algorithm, the log factor will come from halving, hence why I think of it as base 2.
Then it takes log2(n) time.
O(log n) is a bit misleading, more precisely it's O(log2 n), i.e.
(logarithm with base 2).
The height of a balanced binary tree is O(log2 n), since every node has two (note the "two" as in log2 n) child nodes.
So, a tree with n nodes has a height of log2 n.
Another example is binary search, which has a running time of O(log2 n) because at every step you divide the search space by 2.
You can think of O(log N) intuitively by saying the time is proportional to the number of digits in N.
If an operation performs constant time work on each digit or bit of an input, the whole operation will take time proportional to the number of digits or bits in the input, not the magnitude of the input; thus, O(log N) rather than O(N).
If an operation makes a series of constant time decisions each of which halves (reduces by a factor of 3, 4, 5..) the size of the input to be considered, the whole will take time proportional to log base 2 (base 3, base 4, base 5...) of the size N of the input, rather than being O(N).
I cannot understand how to identify a function with a log time.
The most common attributes of logarithmic running-time function are that:
This is why, for example, looking up people in a phone book is O(log n).
O(log n): Given a person's name, find the phone number by picking a random point about halfway through the part of the book you haven't searched yet, then checking to see whether the person's name is at that point.
O(n log n): There was a mix-up at the printer's office, and our phone book had all its pages inserted in a random order.
O(n log n): We want to personalize the phone book, so we're going to find each person or business's name in their designated copy, then circle their name in the book and write a short thank-you note for their patronage.
O(log N) basically means time goes up linearly while the n goes up exponentially.
​It is O(log n) when we do divide and conquer type of algorithms e.g binary search.
Hence it  N O(log N)
O(log n) refers to a function (or algorithm, or step in an algorithm) working in an amount of time proportional to the logarithm (usually base 2 in most cases, but not always, and in any event this is insignificant by big-O notation*) of the size of the input.
The logarithmic function is the inverse of the exponential function.
O(log n) running times are very common in any sort of divide-and-conquer application, because you are (ideally) cutting the work in half every time.
If in each of the division or conquer steps, you are doing constant time work (or work that is not constant-time, but with time growing more slowly than O(log n)), then your entire function is O(log n).
It's fairly common to have each step require linear time on the input instead; this will amount to a total time complexity of O(n log n).
The running time complexity of binary search is an example of O(log n).
So you do approximately log(n)/log(2) steps.
The running time complexity of merge sort is an example of O(n log n).
This is because you are dividing the array in half with each step, resulting in a total of approximately log(n)/log(2) steps.
Thus, the total complexity is O(n log n).
Also by the [change of base rule (hyper-link)] for logarithms, the only difference between logarithms of different bases is a constant factor.
But what exactly is O(log n)
What it means precisely is "as n tends towards infinity, the time tends towards a*log(n) where a is a constant scaling factor".
Or actually, it doesn't quite mean that; more likely it means something like "time divided by a*log(n) tends towards 1".
"Tends towards" has the usual mathematical meaning from 'analysis': for example, that "if you pick any arbitrarily small non-zero constant k, then I can find a corresponding value X such that ((time/(a*log(n))) - 1) is less than k for all values of n greater than X."
it may have some constant startup time;  but these other components pale towards insignificance for large values of n, and the a*log(n) is the dominating term for large n.
time(n) = a + blog(n) + cn + dnn
What does it mean to say that the height of a complete binary tree is O(log n)?
Binary search is an example with complexity O(log n).
As a result, the complexity of the algorithm can be described as a logarithmic order.
Plotting log(n) on a plain piece of paper, will result in a graph where the rise of the curve decelerates as n increases:
The best way I've always had to mentally visualize an algorithm that runs in O(log n) is as follows:
That's why O(log n) algorithms are awesome.
What's logb(n)?
It is the number of times you can cut a log of length n repeatedly into b equal parts before reaching a section of size 1.
The explanation below is using the case of a fully balanced binary tree to help you understand how we get logarithmic time complexity.
And that's how you get O(log n) which is the amount of work that needs to be done on the above tree to reach a solution.
A common algorithm with O(log n) time complexity is Binary Search whose recursive relation is T(n/2) + O(1) i.e.
But what exactly is O(log n)?
For example, what does it mean to say that the height of a >complete binary tree is O(log n)?
I would rephrase this as 'height of a complete binary tree is log n'.
Figuring the height of a complete binary tree would be O(log n), if you were traversing down step by step.
I cannot understand how to identify a function with a logarithmic
  time.
Logarithm is essentially the inverse of exponentiation.
So, if each 'step' of your function is eliminating a factor of elements from the original item set, that is a logarithmic time algorithm.
Now if the first way is reaching in linear time O(n), the second one is O(log n).
Imagine an algorithm, which accepts an integer, n as input and completes in time proportional to n then it is O(n) or theta(n) but if it runs in time proportion to the number of digits or the number of bits in the binary representation on number then the algorithm runs in O(log n) or theta(log n) time.
log x to base b = y is the inverse of b^y = x
Walking a single path in the tree ~ O(d) = O(log n to base M)
These 2 cases will  take O(log n) time
In order to achieve O(log n) time complexity, the tree should be balanced, meaning that the difference of the height between the children of any node should be less than or equal to 1.
Therefore, trees do not always guarantee a time complexity O(log n), unless they are balanced.
This one works on balancing the tree while inserting data in order to keep a time complexity of (log n) while searching in the tree.
Now, if you can prove, that at every iteration of your algorithm you cut off a fraction of this space, that is no less than some limit, this means that your algorithm is running in O(logN) time.
That means, your program is still running at O(logN) time, although significantly slower than the binary search.
In information technology it means that:
The complexity in O-notation of this program is O(log(n)).
The logarithm of x (to the base of a) is the reverse function of a^x.
It is like saying that logarithm is the inverse of exponential.
Now try to see it that way, if exponential grows very fast then logarithm grows (inversely) very slow.
The difference between O(n) and O(log(n)) is huge, similar to the difference between O(n) and O(a^n) (a being a constant).
Algorithms in the Divide and Conquer paradigm are of complexity O(logn).
First, you'll want to have a general idea of Logarithm, which you can get from [https://en.wikipedia.org/wiki/Logarithm (hyper-link)] .
Natural science use e and the natural log.
Engineering disciples will use log_10 (log base 10) and computer scientists will use log_2 (log base 2) a lot, since computers are binary based.
Sometimes you'll see abbreviations of natural log as ln(), engineers normally leave the _10 off and just use log() and log_2 is abbreviated as lg().
All of the types of logarithms grow in a similar fashion, that is why they share the same category of log(n).
You can think of O(1), O(n), O(logn), etc as classes or categories of growth.
In the table below think of log(n) as the ceiling of log_2.
O(log(n)) - Logarithmic Examples:
Algorithm 3 - This acts like "log_2"
Algorithm 3 demonstrates an algorithm that runs in log_2(n).
Algorithm 4 - This acts like "log_3"
Algorithm 4 demonstrates log_3.
Algorithm 5 - This acts like "log_1.02"
Algorithm 5 is important, as it helps show that as long as the number is greater than 1 and the result is repeatedly multiplied against itself, that you are looking at a logarithmic algorithm.
O(n*log(n)) - nlog(n) Examples:
Think of this as a combination of O(log(n)) and O(n).
The nesting of the for loops help us obtain the O(n*log(n))
Algorithm 9 is like algorithm 8, but each of the loops has allowed variations, which still result in the final result being O(n*log(n))
The logarithm
Ok let's try and fully understand what a logarithm actually is.
The number of turns required to get any number is called the logarithm of the number i.e.
3 is the logarithm of 1,000, and 6 is the logarithm of 1,000,000 (base 10).
So what does O(log n) actually mean?
In our example above, our 'growth rate' is O(log n).
Now the example above did use base 10, but fortunately the base of the log is insignificant when we talk about big o notation.
If we had 128 numbers, we could also guess the number in 7 attemps but 129 numbers will takes us at most 8 attempts (in relations to logarithms, here we would need 7 guesses for a 128 value range, 10 guesses for a 1024 value range.
7 is the logarithm of 128, 10 is the logarithm of 1024 (base 2)).
A good rule of thumb to identify if an algorithm has a logarithmtic time is
  to see if the data set shrinks by a certain order after each iteration
What about O(n log n)?
You will eventually come across a linearithmic time O(n log(n)) algorithm.
The rule of thumb above applies again, but this time the logarithmic function has to run n times e.g.
You can easily identify if the algorithmic time is n log n. Look for an outer loop which iterates through a list (O(n)).
If the inner loop is cutting/reducing the data set on each iteration, that loop is (O(log n)), and so the overall algorithm is = O(n log n).
Disclaimer: The rope-logarithm example was grabbed from the excellent [Mathematician's Delight book by W.Sawyer (hyper-link)].
(you take the lg of each side, lg being the log base 2)
So when you say any algorithm is O(log n)
it means the execution time is log times the input size n.
O(logn) is one of the polynomial time complexity to measure the runtime performance of any code.
If you sum all the work done at each level you will end up with n(1+1/2+1/4....) and that is equal to O(logn)
Say you have the following commits in a text file named ~/commits-to-revert.txt (I used git log --pretty=oneline to get them)
You can always use the reflog as well.
git reflog  will display any change which updated the HEAD and checking out the desired reflog entry will set the HEAD back to this commit.
Every time the HEAD is modified there will be a new entry in the reflog
I found the answer from in a blog post (now no longer exists)
Choose the number of the HEAD(s) of git reflog, where you want revert to and do (for this example I choose the 12):
Run the Git log command with -n 4 from your terminal.
The number after the -n determines the number of commits in the log starting from the most recent commit in your local history.
You can search for the last working commit using git log and then run:
And make sure the password doesn't show up in access logs or application logs.
After the user logs in, the server sends the user a session cookie.
If you want to autologin your users, you can set a persistent cookie, but it should be distinct from a full-session cookie.
You can set an additional flag that the user has auto-logged in, and needs to log in for real for sensitive operations.
For example, when you return to visit Amazon, they show you a page that looks like you're logged in, but when you go to place an order (or change your shipping address, credit card etc.
Financial websites such as banks and credit cards, on the other hand, only have sensitive data and should not allow auto-login or a low-security mode.
We'll assume you already know how to build a login+password HTML form which POSTs the values to a script on the server side for authentication.
Unless the connection is already secure (that is, tunneled through HTTPS using SSL/TLS), your login form values (gasp!
including your password) will be sent in cleartext, which allows anyone eavesdropping on the line between browser and web server will be able to read logins as they pass through.
In essence, the only practical way to protect against wiretapping/packet sniffing during login is by using HTTPS or another certificate-based encryption scheme (for example, [TLS (hyper-link)]) or a proven & tested challenge-response scheme (for example, the [Diffie-Hellman (hyper-link)]-based SRP).
Given the perceived (though now [avoidable (hyper-link)]) cost and technical difficulty of setting up an SSL certificate on your website, some developers are tempted to roll their own in-browser hashing or encryption schemes in order to avoid passing cleartext logins over an unsecured wire.
There is no doubt that this is a real threat, however, there are ways of dealing with it seamlessly that don't require a CAPTCHA, specifically properly designed server-side login throttling schemes - we'll discuss those later.
Personally, I tend to find CAPTCHAS annoying, and use them only as a last resort when a user has failed to log in a number of times and throttling delays are maxed out.
Storing Passwords / Verifying logins
User databases are routinely hacked, leaked or gleaned through SQL injection, and if you are storing raw, plaintext passwords, that is instant game over for your login security.
So if you can't store the password, how do you check that the login+password combination POSTed from the login form is correct?
To verify a login, you run the same hash function on the entered password, this time passing in the salt and compare the resulting hash string to the value stored in your database.
Session data - "You are logged in as Spiderman69"
Once the server has verified the login and password against your user database and found a match, the system needs a way to remember that the browser has been authenticated.
Persistent Login Cookies ("remember me" functionality) are a danger zone; on the one hand, they are entirely as safe as conventional logins when users understand how to handle them; and on the other hand, they are an enormous security risk in the hands of careless users, who may use them on public computers and forget to log out, and who may not know what browser cookies are or how to delete them.
Personally, I like persistent logins for the websites I visit on a regular basis, but I know how to handle them safely.
If you are positive that your users know the same, you can use persistent logins with a clean conscience.
If not - well, then you may subscribe to the philosophy that users who are careless with their login credentials brought it upon themselves if they get hacked.
Of course, some systems can't afford to have any accounts hacked; for such systems, there is no way you can justify having persistent logins.
If you DO decide to implement persistent login cookies, this is how you do it:
And just to reiterate one of the most common pitfalls, DO NOT STORE THE PERSISTENT LOGIN COOKIE (TOKEN) IN YOUR DATABASE, ONLY A HASH OF IT!
The login token is Password Equivalent, so if an attacker got their hands on your database, they could use the tokens to log in to any account, just as if they were cleartext login-password combinations.
Therefore, use hashing (according to [https://security.stackexchange.com/a/63438/5002 (hyper-link)] a weak hash will do just fine for this purpose) when storing persistent login tokens.
A simple piece of trivia that anyone could lift from their blog, LinkedIn profile, or similar
Just like a password or a persistent login token.
A final note: always make sure your interface for entering the 'lost password code' is at least as secure as your login form itself, or an attacker will simply use this to gain access instead.
Making sure you generate very long 'lost password codes' (for example, 16 case-sensitive alphanumeric characters) is a good start, but consider adding the same throttling scheme that you do for the login form itself.
The National Institute of Standards and Technology (NIST) [Special Publication 800-63 (hyper-link)] has a set of very good suggestions.
Well, lots, but we can focus on the most important part: the fact that preventing large numbers of rapid-fire successive login attempts (ie.
And finally, login throttling: that is, setting a time delay between attempts after N failed attempts (yes, DoS attacks are still possible, but at least they are far less likely and a lot more complicated to pull off).
It is more like a timeout or refractory period during which login attempts to a specific account or from a specific IP address will not be accepted or evaluated at all.
That is, correct credentials will not return in a successful login, and incorrect credentials will not trigger a delay increase.
DoS attacking this final login throttling scheme would be very impractical.
And as a final touch, always allow persistent (cookie) logins (and/or a CAPTCHA-verified login form) to pass through, so legitimate users won't even be delayed while the attack is in progress.
Just as an aside, more advanced attackers will try to circumvent login throttling by 'spreading their activities':
That way, not only do they get around maximum-attempts measures like CAPTCHAs and login throttling, their chance of success increases as well, since the number 1 most common password is far more likely than number 49.995
Spacing the login requests for each user account, say, 30 seconds apart, to sneak under the radar
Here, the best practice would be logging the number of failed logins, system-wide, and using a running average of your site's bad-login frequency as the basis for an upper limit that you then impose on all users.
Say your site has had an average of 120 bad logins per day over the past 3 months.
Then, if the total number of failed attempts across all accounts exceeds that number within one day (or even better, monitor the rate of acceleration and trigger on a calculated threshold), it activates system-wide login throttling - meaning short delays for ALL users (still, with the exception of cookie logins and/or backup CAPTCHA logins).
Credentials can be compromised, whether by exploits, passwords being written down and lost, laptops with keys being stolen, or users entering logins into phishing sites.
Logins can be further protected with two-factor authentication, which uses out-of-band factors such as single-use codes received from a phone call, SMS message, app, or dongle.
Do NOT try to implement your own login form or database storage of passwords, unless 
the data being stored is valueless at account creation and self-generated (that is, web 2.0 style like Facebook, [Flickr (hyper-link)], etc.)
Just pick something you know you don't need and what sounds like something people would normally find logical to fill in into a web form.
In case of a bot: The bot will see a field whose type is text and a name email (or whatever it is you called it) and will logically attempt to fill it with appropriate data.
I believe this can also be used just fine with a login/authentication form.
This "authentication + authorization" service can be provided by several different technologies, such as LDAP (Microsoft OpenDirectory), or Kerberos.
In other words, you do not avail yourself of any "home-grown logic."
You can have a separate login form on a separate url executing separate code for requests that will grant high privileges.
One such that I've used is to actually scramble the login URL for admin access and email the admin the new URL.
$ git log
I had the same issue just now and I found [this answer (hyper-link)] easiest to understand (commit-ref is the SHA value of the change in the log you want to go back to):
Use git log to obtain the hash key for specific version and then use git checkout <hashkey>
performs a git log on the specified file and
See highlights of changes on [github blog (hyper-link)].
I had log4j available, so I was able to use the org.apache.log4j.lf5.util.StreamUtils.getBytes to get the bytes, which I was able to convert into a string using the String ctor
Better to first compress catalog on remote server:
NOTE: git whatchanged is deprecated, use git log instead
New users are encouraged to use
  [git-log[1] (hyper-link)] instead.
The
  whatchanged command is essentially the same as
  [git-log[1] (hyper-link)] but defaults to show
  the raw format diff output and to skip merges.
The command is kept primarily for historical reasons; fingers of many
  people who learned Git long before git log was invented by reading
  Linux kernel mailing list are trained to type it.
I generally use these to get the logs:
to let Git generate the patches for each log entry.
[git whatchanged -p filename (hyper-link)] is also equivalent to [git log -p filename (hyper-link)] in this case.
git log --follow -p -- path-to-file
In other words, if the file named bar was once named foo, then git log -p bar (without the --follow option) will only show the file's history up to the point where it was renamed -- it won't show the file's history when it was known as foo.
Using git log --follow -p bar will show the file's entire history, including any changes to the file when it was known as foo.
This has the benefit of both displaying the results in the command line (like git log -p) while also letting you step through each commit using the arrow keys (like gitk).
If you use Sourcetree to visualize your repository (it's free and quite good) you can right click a file and select Log Selected
If you use TortoiseGit you should be able to right click on the file and do TortoiseGit --> Show Log.
Right click the file and select 'Log' or press 'Ctrl-L'
In the [Sourcetree (hyper-link)] UI, you can find the history of a file by selecting the 'Log Selected' option in the right click context menu:
gives you the manpage of git log.
This works for both git log and [gitk (hyper-link)] - the 2 most common ways of viewing history.
More details about this are in my blog post here: [http://dymitruk.com/blog/2012/07/18/filtering-by-author-name/ (hyper-link)]
Show n number of logs for x user in colour by adding this little snippet in your .bashrc file.
You should use the [pickaxe (-S) (hyper-link)] option of [git log (hyper-link)].
Example (from [git log (hyper-link)]): git log -S"frotz\(nitfol" --pickaxe-regex
My favorite way to do it is with git log's -G option (added in version 1.7.4).
The commit is shown in the log if the before and after counts are different.
With the -G option, the commit is shown in the log if your search matches any line that was added, removed, or changed.
git log can be a more effective way of searching for text across all branches, especially if there are many matches, and you want to see more recent (relevant) changes first.
These log commands list commits that add or remove the given search string/regex, (generally) more recent first.
git log - Need I write more here; it shows the logs in chronological order.
--oneline - It compresses the Git log in one line.
--graph - It creates the graph of chronologically ordered commits.
Okay, twice just today I've seen people wanting a closer equivalent for hg grep, which is like git log -pS but confines its output to just the (annotated) changed lines.
So here's a diff-hunk scanner that takes git log --pretty=%h -p output and spits annotated change lines.
You're in the less program, which makes the output of git log scrollable.
The END comes from the pager used to display the log (your are at that moment still inside it).
In this case, as snarly suggested, typing q is the intended way to quit git log (as with most other pagers or applications that use pagers).
However normally, if you just want to abort a command that is currently executing, you can try ctrl+c (doesn't seem to work for git log, however) or ctrl+z (although in bash, ctrl-z will freeze the currently running foreground process, which can then be thawed as a background process with the bg command).
Beware, though, that IE9-IE11 and Edge prior to Edge 14 support let but get the above wrong (they don't create a new i each time, so all the functions above would log 3 like they would if we used var).
Beware, though, that IE9-IE11 and Edge prior to Edge 14 support let but get the above wrong (they don't create a new i each time, so all the functions above would log 3 like they would if we used var).
Using this same logic, it should be apparent that the value of i is also not collected until this point either.
Later those functions are invoked logging the most current value of i in the global scope.
let myFunc = (val, index) => { 
    console.log('val: '+val+'\nindex: '+index);
};
When you invoke the function , console.log("My value: " + i) takes the value from its Global object and display the 
result.
funcs[i] is a global function, and 'console.log("My value: " + i);' is printing global variable i
because of this twisted closure design of javascript, 'console.log("My value: " + i);' is printing the i from outer function 'createfunc(i)'
For example, you login a bash, and export A=1, if you exec bash, the A == 1.
For me what works when I change the PATH is: exec "$BASH" --login
When setting up a bare server (ubuntu 16.04), you can use the above info, when you have not yet set up a username, and are logging in via root.
It's best to create a user (with sudo privledges), and login as this username instead.
On my server, this was located at /home/your_username/.bashrc
(where your_username is actually the new username you created above, and now login with)
Assuming an interactive shell, and you'd like to keep your current command history and also load /etc/profile (which loads environment data including /etc/bashrc and on Mac OS X loads paths defined in /etc/paths.d/ via path_helper), append your command history and do an exec of bash with the login ('-l') option:
What I mean by that is that if you go to the trouble of logging a warning, you might as well fix the underlying issue.
Info - Generally useful information to log (service start/stop, configuration assumptions, etc).
I've always considered warning the first log level that for sure means there is a problem (for example, perhaps a config file isn't where it should be and we're going to have to run with default settings).
As a corollary to this question, communicate your interpretations of the log levels and make sure that all people on a project are aligned in their interpretation of the levels.
It's painful to see a vast variety of log messages where the severities and the selected log levels are inconsistent.
Provide examples if possible of the different logging levels.
And be consistent in the info to be logged in a message.
For your particular project, think of everything that you might want to log.
Info - user logged in/out, new transaction, file crated, new d/b field, or field deleted.
It also gives you a basis for comparison if something goes wrong in future (keep all logs, whether pass or fail, and be sure to include build number in the log file)).
I find it more helpful to think about severities from the perspective of viewing the log file.
Typically, a Fatal error only occurs once in the process lifetime, so if the log file is tied to the process, this is typically the last message in the log.
By filtering a log to look at errors and above you get an overview of error frequency and can quickly identify the initiating failure that might have resulted in a cascade of additional errors.
For example, expected transient environmental conditions such as short loss of network or database connectivity should be logged as Warnings, not Errors.
Viewing a log filtered to show only warnings and errors may give quick insight into early hints at the root cause of a subsequent error.
Info: This is important information that should be logged under normal conditions such as successful initialization, services starting and stopping or successful completion of significant transactions.
Viewing a log showing Info and above should give a quick overview of major state changes in the process providing top-level context for understanding any warnings or errors that also occur.
The best way to achieve this is by getting the dev team in the habit of regularly reviewing logs as a standard part of troubleshooting customer reported issues.
For example, it is often helpful to log user input such as changing displays or tabs.
In time, this makes log files almost useless because it's too hard to filter signal from noise.
That causes devs to not use the logs which continues the death spiral.
I'd recommend adopting Syslog severity levels: DEBUG, INFO, NOTICE, WARNING, ERROR, CRITICAL, ALERT, EMERGENCY.
See [http://en.wikipedia.org/wiki/Syslog#Severity_levels (hyper-link)]
They should provide enough fine-grained severity levels for most use-cases and are recognized by existing log-parsers.
Once you start aggregating logs and are trying to detect patterns across different ones it really helps.
I think that SYSLOG levels NOTICE and ALERT/EMERGENCY are largely superfluous for application-level logging - while CRITICAL/ALERT/EMERGENCY may be useful alert levels for an operator that may trigger different actions and notifications, to an application admin it's all the same as FATAL.
I like Jay Cincotta's interpretation best - tracing your code's execution is something very useful in tech support, and putting trace statements into the code liberally should be encouraged - especially in combination with a dynamic filtering mechanism for logging the trace messages from specific application components.
However DEBUG level to me indicates that we're still in the process of figuring out what's going on - I see DEBUG level output as a development-only option, not as something that should ever show up in a production log.
There is however a logging level that I like to see in my error logs when wearing the hat of a sysadmin as much as that of tech support, or even developer: OPER, for OPERATIONAL messages.
This I use for logging a timestamp, the type of operation invoked, the arguments supplied, possibly a (unique) task identifier, and task completion.
It's the sort of thing I want always logged, no matter whether anything goes wrong or not, so I consider the level of OPER to be higher than FATAL, so you can only turn it off by going to totally silent mode.
And it's much more than mere INFO log data - a log level often abused for spamming logs with minor operational messages of no historical value whatsoever.
As the case dictates this information may be directed to a separate invocation log, or may be obtained by filtering it out of a large log recording more information.
But it's always needed, as historical info, to know what was being done - without descending to the level of AUDIT, another totally separate log level that has nothing to do with malfunctions or system operation, doesn't really fit within the above levels (as it needs its own control switch, not a severity classification) and which definitely needs its own separate log file.
Here's a list of what "the loggers" have.
Apache log4j: [§1 (hyper-link)], [§2 (hyper-link)]
messages logged for the sake of de-bugging)].
messages logged for the sake of tracing].
Apache commons-logging: [§ (hyper-link)]
Expect these to be written to logs only.
Expect these to be written to logs only.
Apache commons-logging "best practices" for enterprise usage makes a distinction between debug and info based on what kind of boundaries they cross.
(See [commons-logging guide (hyper-link)] for more info on this.)
From RFC 5424, the [Syslog Protocol (hyper-link)] (IETF) - Page 10:
My two cents about FATAL and TRACE error log levels.
Event is logged as INFO
Event is logged as WARN
Event is logged as ERROR
Event is logged as FATAL
This is not about logging, because this message can be generated by some debugger and your code has not call to log at all.
So generally in your program you do DEBUG, INFO and WARN logging.
And when you are debugging application you will get TRACE logging from this type of software.
On Python, there are [only 5 "named" logging levels (hyper-link)], so this is how I use them:
It’s
permanent, it’s tied to my application logic, and it’s a more concrete
response than a 401.
they are not logged in or do not belong to the proper user group
If they are not logged in at all you should return 401 Unauthorized
If they are logged in but don't belong to the proper user group, you should return 403 Forbidden
If the user is not logged in they are un-authenticated, the HTTP equivalent of which is 401 and is misleadingly called Unauthorized in the RFC.
Meaning if you have your own roll-your-own login process and never use HTTP Authentication, 403 is always the proper response and 401 should never be used.
They do not refer to any roll-your-own authentication protocols you may have created using login pages, etc.
I will use "login" to refer to authentication and authorization by methods other than RFC2617
401 indicates that the resource can not be provided, but the server is REQUESTING that the client log in through HTTP Authentication and has sent reply headers to initiate the process.
It neither suggests nor implies that some sort of login page or other non-RFC2617 authentication protocol may or may not help - that is outside the RFC2616 standards and definition.
There seems to be a question on the roll-your-own-login issue (application).
In this case, simply not being logged in is not sufficient to send a 401 or a 403, unless you use HTTP Auth vs a login page (not tied to setting HTTP Auth).
It sounds like you may be looking for a "201 Created", with a roll-your-own-login screen present (instead of the requested resource) for the application-level access to a file.
When I'm building something like this, I'll try to record unauthenticate / unauthorized requests in an internal log, but return a 404.
401 if not logged-in or session expired
UNAUTHORIZED: Status code (401) indicating that the request requires authentication, usually this means user needs to be logged-in (session).
This can also happen after login if session expired.
[What is correct HTTP status code when redirecting to a login page?
If the user just needs to log in using you site's standard HTML login form, 401 would not be appropriate because it is specific to HTTP basic auth.
This leaves 403 as "you need to be logged in".
I think it is important to consider that, to a browser, 401 initiates an authentication dialog for the user to enter new credentials, while 403 does not.
Here are some cases under that logic where an error would be returned from authentication or authorization, with important phrases bolded.
For example, if you are using [Sourcetree (hyper-link)], you can simply [select any two commits in log view (hyper-link)].
which is pretty helpful if you're rebasing often because your feature logs will all be in a row.
Rename Log file
New log file will be recreated
Delete Renamed Log file.
I'm guessing that you are not doing log backups.
(Which truncate the log).
This will prevent log bloat.
To Truncate the log file:
Delete the transaction log file.
When the database is attached, a new transaction log file is created.
To Shrink the log file:
Backup log [DBName] with No_Log
Shrink the database by either:
Using Enterprise manager :-
Right click on the database, All tasks, Shrink database, Files, Select log file, OK.
Using T-SQL :-
Dbcc Shrinkfile ([Log_Logical_Name])
You can find the logical name of the log file by running sp_helpdb or by looking in the properties of the database in Enterprise Manager.
If you do not use the transaction logs for restores (i.e.
You only ever do full backups), you can set Recovery Mode to "Simple", and the transaction log will very shortly shrink and never fill up again.
If you are using SQL 7 or 2000, you can enable "truncate log on checkpoint" in the database options tab.
To my experience on most SQL Servers there is no backup of the transaction log.
Full backups or differential backups are common practice, but transaction log backups are really seldom.
So the transaction log file grows forever (until the disk is full).
In Full recovery mode this might not work, so you have to either back up the log first, or change to Simple recovery, then shrink the file.
This technique that John recommends is not recommended as there is no guarantee that the database will attach without the log file.
The SQL Server will clear the log, which you can then shrink using DBCC SHRINKFILE.
Backup log DatabaseName With Truncate_Only:
SP_helpfile will give you the logical log file name.
[Recover from a full transaction log in a SQL Server database (hyper-link)]
Use the DBCC ShrinkFile ({logicalLogName}, TRUNCATEONLY) command.
Remember though that TX logs do have a sort of minimum/steady state size that they will grow up to.
Depending upon your recovery model you may not be able to shrink the log - if in FULL and you aren't issuing TX log backups the log can't be shrunk - it will grow forever.
If you don't need TX log backups, switch your recovery model to Simple.
And remember, never ever under any circumstances delete the log (LDF) file!
Never ever delete the transaction log - you will lose data!
Part of your data is in the TX Log (regardless of recovery model)... if you detach and "rename" the TX log file that effectively deletes part of your database.
For those that have deleted the TX Log you may want to run a few checkdb commands and fix the corruption before you lose more data.
Check out Paul Randal's blog posts on this very topic, [bad advice (hyper-link)].
Rename Log file
Attach DB (while attaching remove renamed .ldf (log file).Select it and remove by pressing Remove button)
New log file will be recreated
Delete Renamed Log file.
DB Transaction Log Shrink to min size:
Backup: Transaction log
Shrink files: Transaction log
Backup: Transaction log
Shrink files: Transaction log
Rename the log file
(The system will create a new log file.)
Delete or move the renamed log file.
Most answers here so far are assuming you do not actually need the Transaction Log file, however if your database is using the FULL recovery model, and you want to keep your backups in case you need to restore the database, then do not truncate or delete the log file the way many of these answers suggest.
Eliminating the log file (through truncating it, discarding it, erasing it, etc) will break your backup chain, and will prevent you from restoring to any point in time since your last full, differential, or transaction log backup, until the next full or differential backup is made.
We recommend that you never use NO_LOG or TRUNCATE_ONLY to manually
  truncate the transaction log, because this breaks the log chain.
Use manual log truncation in only very
  special circumstances, and create backups of the data immediately.
To avoid that, backup your log file to disk before shrinking it.
Below is a script to shrink the transaction log, but I’d definitely recommend backing up the transaction log before shrinking it.
The transaction log contains a lot of useful data that can be read using a third-party transaction log reader (it can be read manually but with extreme effort though).
The transaction log is also a must when it comes to point in time recovery, so don’t just throw it away, but make sure you back it up beforehand.
Here are several posts where people used data stored in the transaction log to accomplish recovery:
[How to view transaction logs in SQL Server 2008 (hyper-link)]
[Read the log file (*.LDF) in SQL Server 2008 (hyper-link)]
“Cannot shrink log file (log file name) because the logical
      log file located at the end of the file is in use“
This means that TLOG is in use.
Making a log file smaller should really be reserved for scenarios where it encountered unexpected growth which you do not expect to happen again.
If the log file will grow to the same size again, not very much is accomplished by shrinking it temporarily.
Even if you are taking regular full backups, the log file will grow and grow until you perform a log backup - this is for your protection, not to needlessly eat away at your disk space.
You should be performing these log backups quite frequently, according to your recovery objectives.
For example, if you have a business rule that states you can afford to lose no more than 15 minutes of data in the event of a disaster, you should have a job that backs up the log every 15 minutes.
Now, once you have regular log backups running, it should be reasonable to shrink the log file to something more reasonable than whatever it's blown up to now.
This does not mean running SHRINKFILE over and over again until the log file is 1 MB - even if you are backing up the log frequently, it still needs to accommodate the sum of any concurrent transactions that can occur.
Log file autogrow events are expensive, since SQL Server has to zero out the files (unlike data files when instant file initialization is enabled), and user transactions have to wait while this happens.
Note that you may need to back up the log twice before a shrink is possible (thanks Robert).
So, you need to come up with a practical size for your log file.
Nobody here can tell you what that is without knowing a lot more about your system, but if you've been frequently shrinking the log file and it has been growing again, a good watermark is probably 10-50% higher than the largest it's been.
Let's say that comes to 200 MB, and you want any subsequent autogrowth events to be 50 MB, then you can adjust the log file size this way:
Note that if the log file is currently > 200 MB, you may need to run this first:
Putting the database in SIMPLE recovery mode will make sure that SQL Server re-uses portions of the log file (essentially phasing out inactive transactions) instead of growing to keep a record of all transactions (like FULL recovery does until you back up the log).
CHECKPOINT events will help control the log and make sure that it doesn't need to grow unless you generate a lot of t-log activity between CHECKPOINTs.
Next, you should make absolute sure that this log growth was truly due to an abnormal event (say, an annual spring cleaning or rebuilding your biggest indexes), and not due to normal, everyday usage.
If you shrink the log file to a ridiculously small size, and SQL Server just has to grow it again to accommodate your normal activity, what did you gain?
As per the example in the point-in-time recovery case, you can use the same code and logic to determine what file size is appropriate and set reasonable autogrowth parameters.
Back up the log with TRUNCATE_ONLY option and then SHRINKFILE.
Second, if you are in FULL recovery model, this will destroy your log chain and require a new, full backup.
Detach the database, delete the log file, and re-attach.
DBCC SHRINKDATABASE and the maintenance plan option to do the same are bad ideas, especially if you really only need to resolve a log problem issue.
Shrink the log file to 1 MB.
Unless your database is read only (and it is, you should mark it as such using ALTER DATABASE), this will absolutely just lead to many unnecessary growth events, as the log has to accommodate current transactions regardless of the recovery model.
Create a second log file.
You should deal with the problematic log file directly instead of just adding another potential problem.
Other than redirecting some transaction log activity to a different drive, a second log file really does nothing for you (unlike a second data file), since only one of the files can ever be used at a time.
[Paul Randal also explains why multiple log files can bite you later (hyper-link)].
Instead of shrinking your log file to some small amount and letting it constantly autogrow at a small rate on its own, set it to some reasonably large size (one that will accommodate the sum of your largest set of concurrent transactions) and set a reasonable autogrow setting as a fallback, so that it doesn't have to grow multiple times to satisfy single transactions and so that it will be relatively rare for it to ever have to grow during normal business operations.
Funny enough, these are the defaults for SQL Server (which I've complained about and [asked for changes to no avail (hyper-link)]) - 1 MB for data files, and 10% for log files.
The former is much too small in this day and age, and the latter leads to longer and longer events every time (say, your log file is 500 MB, first growth is 50 MB, next growth is 55 MB, next growth is 60.5 MB, etc.
Please don't stop here; while much of the advice you see out there about shrinking log files is inherently bad and even potentially disastrous, there are some people who care more about data integrity than freeing up disk space.
[A blog post I wrote in 2009, when I saw a few "here's how to shrink the log file" posts spring up (hyper-link)].
[A blog post Brent Ozar wrote four years ago, pointing to multiple resources, in response to a SQL Server Magazine article that should not have been published (hyper-link)].
[A blog post by Paul Randal explaining why t-log maintenance is important (hyper-link)] and [why you shouldn't shrink your data files, either (hyper-link)].
[Mike Walsh has a great answer covering some of these aspects too, including reasons why you might not be able to shrink your log file immediately (hyper-link)].
Database → right click Properties → file → add another log file with a different name and set the path the same as the old log file with a different file name.
The database automatically picks up the newly created log file.
The SQL Server transaction log needs to be properly maintained in order to prevent its unwanted growth.
This means running transaction log backups often enough.
By not doing that, you risk the transaction log to become full and start to grow.
Besides the answers for this question I recommend reading and understanding the transaction log common myths.
These readings may help understanding the transaction log and deciding what techniques to use to "clear" it:
From [10 most important SQL Server transaction log myths (hyper-link)]:
I don’t want to make SQL Server transaction log backups
One of the biggest performance intensive operations in SQL Server is an auto-grow event of the online transaction log file.
By not making transaction log backups often enough, the online transaction log will become full and will have to grow.
The busier the database is, the quicker the online transaction log will grow if transaction log backups are not created
Creating a SQL Server transaction log backup doesn’t block the online transaction log, but an auto-growth event does.
It can block all activity in the online transaction log
From [Transaction log myths (hyper-link)]:
Myth: Regular log shrinking is a good maintenance practice
Log growth is very expensive because the new chunk must be zeroed-out.
If you shrink the log, it will grow again and you are just wasting disk operation on needless shrink-and-grow-again game
It happened with me where the database log file was of 28 GBs.
Actually, log files are those file data which the SQL server keeps when an transaction has taken place.
Step 2: 
Right click on the database 
Task> Back up
Select back up type as Transaction Log
Add a destination address  and file name to keep the backup data (.bak)
Tasks> Shrinks> Files 
Choose File type as Log
Shrink action as release unused space
Check your log file 
normally in SQL 2014 this can be found at
Some of the other answers did not work for me: It was not possible to create the checkpoint while the db was online, because the transaction log was full (how ironic).
However, after setting the database to emergency mode, I was able to shrink the log file:
I went mostly from these instructions [https://www.sqlshack.com/sql-server-transaction-log-backup-truncate-and-shrink-operations/ (hyper-link)]
I had a recent db backup, so I backed up the transaction log.
Finally I shrank the log file, and went from 20G to 7MB, much more in line with the size of my data.
I don't think the transaction logs had ever been backed up since this was installed 2 years ago..  so putting that task on the housekeeping calendar.
git log --pretty=format:"%h%x09%an%x09%ad%x09%s"
Inspired by [stackoverflow question: "git log output like svn ls -v" (hyper-link)], i found out that I could add the exact params I needed.
In case you were curious what the different options were:
%h = abbreviated commit hash
%x09 = tab (character for code 9)
%an = author name
%ad = author date (format respects --date= option)
%s = subject
From [kernel.org/pub/software/scm/git/docs/git-log.html (hyper-link)] (PRETTY FORMATS section) by comment of Vivek.
tig is a possible alternative to using the git log command, available on the major open source *nix distributions.
(log is displayed in pager as follows, with current commit's hash displayed at the bottom)
By the way, tig is good for a lot more than a quick view of the log:
[Screenshots (hyper-link)] &
[Manual (hyper-link)]
%ad is the author date, which can be overidden by --date or the option specified in the [log] stanza in .gitconfig.
This is all of course in color, so it is easy to distinguish the various parts of a log line.
Also it is the default when typing git log because of the [format] section.
git log -g now contains the reflog selector.
Try git log --pretty=fuller, it will show you:-
Author:
Author Date:
Commit:
Commit Date:
[Git - git-log Documentation (hyper-link)]
[logger.exception (hyper-link)] will output a stack trace alongside the error message.
[@Paulo Cheque (hyper-link)] notes, "be aware that in Python 3 you must call the logging.exception method just inside the except part.
If you can cope with the extra dependency then use twisted.log, you don't have to explicitly log errors and also it returns the entire traceback and time to the file or stream.
One nice thing about logging.exception that [SiggyF's answer (hyper-link)] doesn't show is that you can pass in an arbitrary message, and logging will still show the full traceback with all the exception details:
With the default (in recent versions) logging behaviour of just printing errors to sys.stderr, it looks like this:
In most applications, you won't be calling logging.exception(e) directly.
Most likely you have defined a custom logger specific for your application or module like this:
In this case, just use the logger to call the exception(e) like this:
What if your application does logging some other way – not using the logging module?
If you use plain logs - all your log records should correspond this rule: one record = one line.
Following this rule you can use grep and other tools to process your log files.
After that (when you'll be analyzing your logs) you could copy / paste required traceback lines from your log file and do this:
You can log the stack trace without an exception.
[https://docs.python.org/3/library/logging.html#logging.Logger.debug (hyper-link)]
If true, stack information is added to the logging message, including the actual logging call.
Note that this is not the same stack information as that displayed through specifying exc_info: The former is stack frames from the bottom of the stack up to the logging call in the current thread, whereas the latter is information about stack frames which have been unwound, following an exception, while searching for exception handlers.
In your logging module(if custom module) just enable stack_info.
If "debugging information" means the values present when exception was raised, then logging.exception(...) won't help.
So you'll need a tool that logs all variable values along with the traceback lines automatically.
Out of the box you'll get log like
The pickaxe solution ( git log --pickaxe-regex -S'REGEX' ) will only give you line additions/deletions, not the other alterations of the line containing the regular expression.
[Since Git 1.8.4 (hyper-link)], git log has [-L (hyper-link)] to view the evolution of a range of lines.
Then, use git log.
You can mix git blame and git log commands to retrieve the summary of each commit in the git blame command and append them.
It lists branches in chronological order (newest first), and can also let you set a max age so that you don't list all branches (if you have a lot of them).
FYI, if you'd like to get a list of recently checked out branches (as opposed to recently committed) you can use Git's reflog:
I pipe the output from the accepted answer into dialog, to give me an interactive list:
I wrote [a blog post (hyper-link)] about how the various pieces work.
Do a git reflog or even just git log and note your commits.
Eventually, such dangling commits will be pruned through the garbage collection process (by default, they are kept for at least 2 weeks and may be kept longer by being referenced by HEAD’s reflog).
1
It is perfectly fine to do “normal” work with a detached HEAD, you just have to keep track of what you are doing to avoid having to fish dropped history out of the reflog.
The intermediate steps of an interactive rebase are done with a detached HEAD (partially to avoid polluting the active branch’s reflog).
(You will probably want to experiment with the log options: add -p, leave off --pretty=… to see the whole log message, etc.)
Then run git log, and you'll see that commit is now HEAD on this new branch.
If you want to push your current detached HEAD (check git log before), try:
In case you lost some access to previous commits, you can always run git reflog to see the history from all branches.
[In this Blog (hyper-link)] it's clearly stating
a Git repository is a tree-of-commits, with each commit pointing to its ancestor with each commit pointer is updated and these pointers to each branch are stored in the .git/refs sub-directories.
Either run basicConfig with stream=sys.stdout as the argument prior to setting up any other handlers or logging any messages, or manually add a StreamHandler that pushes messages to stdout to the root logger (or any other logger you want, for that matter).
Just get a handle to the root logger and add the StreamHandler.
Not sure if you really need stdout over stderr, but this is what I use when I setup the Python logger and I also add the FileHandler as well.
Then all my logs go to both places (which is what it sounds like you want).
You could also add a Formatter to it so all your log lines have a common header.
Here's a quick example reusing the assumed values and LOGFILE from the question:
[logging.basicConfig() (hyper-link)] can take a keyword argument handlers since Python 3.3, which simplifies logging setup a lot, especially when setting up multiple handlers with the same formatter:
handlers – If specified, this should be an iterable of already created handlers to add to the root logger.
Look at [LogRecord attributes (hyper-link)] if you want to customize the log format and add things like filename/line, thread info etc.)
You can use the logging from all other places in the codebase later like this:
Note: If it doesn't work, someone else has probably already initialized the logging system differently.
Comments suggest doing logging.root.handlers = [] before the call to basicConfig().
[https://github.com/acschaefer/duallog (hyper-link)]
Simply download the .py file and include it in your project, or install the whole package via pip install duallog.
Logging to stdout and rotating file with different levels and formats:
It supports logging to both console and log file, allows for different log level settings, provides colorized output and is easily configurable (also available as [Gist (hyper-link)]):
You can either configure the repository to allow all users to do this, or you can modify the log message directly on the server.
Log messages are kept in the
  repository as properties attached to
  each revision.
By default, the log
  message property (svn:log) cannot be
  edited once it is committed.
That is
  because changes to revision properties
  (of which svn:log is one) cause the
  property's previous value to be
  permanently discarded, and Subversion
  tries to prevent you from doing this
  accidentally.
The
  "pre-revprop-change" hook has access
  to the old log message before it is
  changed, so it can preserve it in some
  way (for example, by sending an
  email).
Once revision property
  modifications are enabled, you can
  change a revision's log message by
  passing the --revprop switch to svn
  propedit or svn propset, like either
  one of these:
where N
  is the revision number whose log
  message you wish to change, and URL is
  the location of the repository.
The second way of changing a log
  message is to use svnadmin setlog.
where REPOS_PATH is the repository
  location, N is the revision number
  whose log message you wish to change,
  and FILE is a file containing the new
  log message.
If your repository enables setting revision properties via the pre-revprop-change hook you can change log messages much easier.
Or in TortoiseSVN, AnkhSVN and probably many other subversion clients by right clicking on a log entry and then 'change log message'.
Its because Subversion doesn’t allow you to modify log messages because they are unversioned and will be lost permanently.
svn propedit -r N --revprop svn:log
But it probably won’t, because the svn:log revision property is unversioned and Subversion by default will stop you from overwriting it, either with the [hook script (hyper-link)] pre-revprop-change, or an error message that you don’t have such a hook.
If so, temporarily comment out the
part of it that aborts if you try to change svn:log.
In the working copy, run svn propedit -r N --revprop svn:log again
We decided they would be allowed to modify any log messages committed that day, to fix typo's etc.
right click in your project folder and choose "Show log"
in the Log Messages window, right click on a revision and choose "Edit log message"
I found a nice implementation of the server side pre-rev-prop-change hook at the svnforum: [https://www.svnforum.org/forum/opensource-subversion-forums/scripts-contributions/8571-pre-revprop-change-shell-script-allows-commiters-to-change-own-log-within-x-hours (hyper-link)]
[https://docs.spring.io/spring-boot/docs/current/reference/html/howto-logging.html (hyper-link)]
To log values:
if you hava a logback-spring.xml or something like that, add the following code to it
This the simplest way to print the SQL queries though it doesn't log the parameters of prepared statements.
And its is not recommended since its not such as optimized logging framework.
By specifying above properties, logs entries will be sent to the configured log appender such as log-back or log4j.
The problem with show-sql is that the SQL statements are printed in the console, so there is no way to filter them, as you'd normally do with a Logging framework.
In your log configuration file, if you add the following logger:
That's why the statement will be logged using parameter placeholders:
If you want to log the bind parameter values, just add the following logger as well:
Once you set the BasicBinder logger, you will see that the bind parameter values are logged as well:
Next, you need to set the net.ttddyy.dsproxy.listener log level to debug in your logging framework configuration file.
For instance, if you're using Logback, you can add the following logger:
Once you enable datasource-proxy, the SQl statement are going to be logged as follows:
All logging output is handled by the handlers; just add a [logging.StreamHandler() (hyper-link)] to the root logger.
Here's an example configuring a stream handler (using stdout instead of the default stderr) and adding it to the root logger:
Please see: [https://docs.python.org/2/howto/logging-cookbook.html (hyper-link)]
The simplest way to log to stdout:
The simplest way to log to file and to stderr:
You could create two handlers for file and stdout and then create one logger with handlers argument to [basicConfig (hyper-link)].
It could be useful if you have the same log_level and format output for both handlers:
Here is a solution based on the powerful but poorly documented [logging.config.dictConfig method (hyper-link)].
Instead of sending every log message to stdout, it sends messages with log level ERROR and higher to stderr and everything else to stdout.
For example: It's easy, you only need to set up two loggers.
[Complete logger configuration (hyper-link)] from INI file, which also includes setup for stdout and debug.log:
Set log_statement to all:
[Error Reporting and Logging - log_statement (hyper-link)]
In your data/postgresql.conf file, change the log_statement setting to 'all'.
make sure you have turned on the log_destination variable
make sure you turn on the logging_collector
also make sure that the log_directory directory already exists inside of the data directory, and that the postgres user can write to it.
This will affect logging until session ends.
Set (uncomment) log_statement = 'all' and log_min_error_statement = error in /var/lib/pgsql/9.2/data/postgresql.conf.
Find today's log in /var/lib/pgsql/9.2/data/pg_log/
#log_directory = 'pg_log' to log_directory = 'pg_log'
#log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log' to log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
#log_statement = 'none' to log_statement = 'all'
#logging_collector = off to logging_collector = on
Optional: SELECT set_config('log_statement', 'all', true);
Find current log in /var/lib/pgsql/9.2/data/pg_log/
The log files tend to grow a lot over a time, and might kill your machine.
For your safety, write a bash script that'll delete logs and restart postgresql server.
FYI: The other solutions will only log statements from the default database—usually postgres—to log others; start with their solution; then:
Ref: [https://serverfault.com/a/376888 (hyper-link)] / [log_statement (hyper-link)]
You should also set this parameter to log every statement:
I was trying to set the log_statement in some postgres config file but in fact the file was not read by our postgres.
Then you have to log in DB and run this command:
Ubuntu (old using upstart ) - /var/log/upstart/docker.log
Amazon Linux AMI - /var/log/docker
Boot2Docker - /var/log/docker.log
Debian GNU/Linux - /var/log/daemon.log
CentOS - /var/log/message | grep docker
Red Hat Enterprise Linux Server - /var/log/messages | grep docker
OSX - ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/log/d‌​ocker.log
Windows - Get-EventLog -LogName Application -Source Docker -After (Get-Date).AddMinutes(-5) | Sort-Object Time, as mentioned [here (hyper-link)].
If your OS is using systemd then you can view docker daemon log with:
Using CentOS 7.x or 8.x, logs are available using the command journalctl -u docker.
systemd has its own logging system called the journal.
The logs for the docker daemon can be viewed using journalctl -u docker
For Mac with Docker Toolbox, ssh into the VM first with docker-machine ssh %VM-NAME% and then check /var/log/docker.log
~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/log/d‌​ocker.log
If you want to see the docker daemon logs on commandline, just type:
Inside the Console App just choose system.log and type Docker into the search bar.
Now you should see all Docker related logs.
Add ways to find docker daemon log in windows:
You can copy the docker daemon log file to your local directory for analysis:
docker-machine scp default:/var/log/docker.log ./
Where default is the name of active your docker machine.
In my environment(docker for mac 17.07), there is no log file at ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/log/d‌​ocker.log
Instead I can find log file as below.
Check log file
/ # tail -f /var/log/docker.log
The location of docker logs has changed for Mac OSX to ~/Library/Containers/com.docker.docker/Data/com.docker.driver.amd64-linux/console-ring
Also you can see logs by this command:
Logs stored in: /var/lib/docker/containers/<container id>/<container id>-json.log
I was not able to find the logs under Manjaro 20/Arch Linux.
Instead i just stopped the docker daemon process and restarted daemon in [debug mode (hyper-link)] with $ sudo dockerd -D to produce logs.
John Resig also [blogged about it (hyper-link)].
The attr function used to have some convoluted logic around getting what they thought you wanted, rather than literally getting the attribute.
([http://blog.jquery.com/2011/05/10/jquery-1-6-1-rc-1-released/ (hyper-link)])
However, a best practice is to have a logger set up for your module.
In which case, you'll want the logger.exception function instead:
Which logs:
Which logs:
Logging levels and outputs can be adjusted, making it easy to turn off without touching the code.
Here's how to use it (assuming flaky_func is defined, and log calls your favorite logging system):
It's a good idea to catch and re-raise KeyboardInterrupts, so that you can still kill the program using Ctrl-C.  Logging is outside the scope of the question, but a good option is [logging (hyper-link)].
In addition to [Aaron Hall's answer (hyper-link)], if you are logging, but don't want to use logging.exception() (since it logs at the ERROR level), you can use a lower level and pass exc_info=True.
First, do not use prints for logging, there is a stable, proven and well-thought out stdlib module to do that: [logging (hyper-link)].
What log.exception is actually doing is just a call to log.error (that is, log event with level ERROR) and print traceback then.
If you use it, you will confuse anyone who reads your logs, they will be banging their heads against them.
Passing exc_info=True to log calls is just inappropriate.
But, it is useful when catching recoverable errors and you want to log them (using, e.g INFO level) with tracebacks as well, because log.exception produces logs of only one level - ERROR.
This is my solution to write the error in a log file and also on console:
The [for...in (hyper-link)] loop improves upon the weaknesses of the for loop by eliminating the counting logic and exit condition.
Does git log --oneline do what you want?
if you want to always use git log in such way you could add git alias by
git config --global alias.log log --oneline
after that git log will print what normally would be printed by git log --oneline
It's an alias for git log --pretty=oneline --abbrev-commit, and displays the "short sha" and "short description", for example:
You can define a global alias so you can invoke a short log in a more comfortable way:
git config --global alias.slog "log --pretty=oneline --abbrev-commit"
Then you can call it using git slog (it even works with autocompletion if you have it enabled).
Better and easier git log by making an alias.
[https://coderwall.com/p/euwpig/a-better-git-log (hyper-link)]
Advanced Reading.
[http://durdn.com/blog/2012/11/22/must-have-git-aliases-advanced-examples/ (hyper-link)]
git log --format="%H" -n 1
If you are not in the branch, then you can add the branch name to the "git log" command, like this:
The key option is --first-parent: "Follow only the first parent commit upon seeing a merge commit" ([https://git-scm.com/docs/git-log (hyper-link)])
just run git log origin/$BRANCH_NAME
Then either just hit l again to show commits from the current branch (with none of commits merged onto it) down to end of history, or, if you want the log to end where it was branched off from master, hit o and type master.. as your range:
Logging ssh user_name@host_name will work only for the default id_rsa file, so here is the second trap.
This was required for me even if password SSH login was disabled.
Log out and try log in in again!
You can change the log level of sshd by modifying file /etc/ssh/sshd_config(search for LogLevel, and set it to DEBUG) and then check the output in file /var/log/auth.log to see what happened exactly.
I have the home directory in a non-standard location and in sshd logs I have the following line, even if all permissions were just fine (see the other answers):
Just look in file /var/log/auth.log on the server.
Look in file /var/log/auth.log on the server for sshd authentication errors.
With this information I realized that my sshd_config file was restricting logins to members of the ssh group.
I had this problem when I added the group of the login user to another user.
Let's say there is an SSH-login user called userA and a non-SSH-login user userB.
The lead to the the described behaviour, so that userA was not able to login without a prompt.
After I removed the group userA from userB, the login without a prompt worked again.
Try phpinfo() and check for "error_log"
You should use absolute path when setting error_log variable in your php.ini file, otherwise, error logs will be stored according to your relative path.
Other solution would be writing simple script which would list all error logs files from directory tree.
PHP stores error logs in /var/log/apache2 if PHP is an apache2 module.
Shared hosts are often storing log files in your root directory /log subfolder.
According to rinogo's comment: If you're using cPanel, the master log file you're probably looking for is stored (by default) at
If all else fails you can check the location of the log file using
When configuring your error log file in php.ini, you can use an absolute path or a relative path.
A relative path will be resolved based on the location of the generating script, and you'll get a log file in each directory you have scripts in.
All access logs come under:
If you have build Apache and PHP from source, then the error logs by default is generated at your ${Apache install dir}/logs/error_log i.e generally /usr/local/apache2/logs/error_log.
Else, if you have installed it from repository, you will find it at /var/log/apache2/error_log.You can set the path in your php.ini also and verify it by invoking phpinfo().
How do find your PHP error log on Linux:
If You use php5-fpm log default should be under
I start by looking at /etc/httpd/conf/httpd.conf or /etc/apache2/httpd.conf and search for error_log.
It could be listed as either /var/log/httpd/error_log or /var/log/apache2/error_log but it might also be listed as simply logs/error_log.
In this case it is a relative path, which means it will be under /etc/httpd/logs/error_log.
Your virtual host could override it then with ErrorLog "/path/to/error_log".
whereever you want it to, if you set it your function call:
error_log($errorMessageforLog .
It appears that by default php does not log errors anywhere, the error_log key in php.ini is commented out in all the install's I've seen.
Search these files for the error_reporting value; 
Which should be set to whatever amalgamation of php log levels are enough for you., 
Eg:   E_ALL & ~E_DEPRECATED & ~E_STRICT
Check the error_log value to make sure it points to an actual place and is not commented out.
Probably /var/log/.
It can also be /var/log/apache2/error.log if you are in google compute engine.
The terminal will output the error log location.
The command prompt will output the error log location
NGINX usually stores it in /var/log/nginx/error.log or access.log.
Search the httpd.conf file for ErrorLog by running cat <file location> | grep ErrorLog on the command line.
Find the line that starts with ErrorLog and there's your answer.
Note: For virtual hosts, you can edit the virtual hosts file httpd-vhosts.conf to specify a different log file location.
Wordpress will direct error_log() messages to /wp-content/debug.log when WP_DEBUG_LOG is set to true.
[See Wordpress Documentation for WP_DEBUG_LOG (hyper-link)]
you can go in File Manager check logs folder.
check Log file in public_html folder.
check "php phpinfo()" file where log store.
cPanel Error logs are located in:
/usr/local/cpanel/logs/
/usr/local/apache/logs/
By default Apche logs are located inside:
/var/log/apache
/var/log/apache2
If anyone is using custom log location then you can check it by running this command:
cat /etc/apache2/conf/httpd.conf | grep ErrorLog
You are on share environment and cannot find error log, always check if cPanel has option Errors on your cPanel dashboard.
If you are not being able to find error log, then you can find it there .
On cPanel search bar, search Error, it will show Error Pages which are basically lists of different http error pages and other Error is where the error logs are displayed.
Other places to look on shared environment:
/home/yourusername/logs
/home/yourusername/public_html/error_log
For PHP-FPM just search config file for error_log:
/var/log/php-errors.log
for centos 8 var/log/httpd/error_log
On shared cPanel environment you cannot find error log, if your hosting provider don't provide any option in cPanel Dashboard.
Otherwise normally you will find a file called "error_log" on your public_html file, which have all the php error recorded.
I can guarantee you, I am not the only person who has been driven to madness at least once in a frustrating search for a log file.
A definitive guide on where php error log is stored would be a complicated bit of work.
The official php manual does not even try to address the whole topic, because there are dependencies on systems outside php, such as the operating system (linux vs. Windows, which distro of linux), including settings within Windows and linux that affect name and location of the php error log.
If you do both, you likely will find the error_log is in different places, depending on command line vs. web server use of php.
The default for error_log is no value
[ (hyper-link)]
Finding your way among them is confusing at first, but you do not need to deal with this to find your php log.
If the output from phpinfo() shows a full path to a file, that is where the log is.
Whether error_log is no value.
If it is, the log file location will depend on the operating system and the mode php is running.
If php is running as an apache module, on linux the log often is in /var/log/apache2/error.log  Another likely spot is in a logs directory in your account home directory, ~/logs/error.log
If there is a file name without a path, the location depends on whether the file name has the value syslog.
If it syslog, then the php error log is injected into the syslog for the server, which varies by linux distro.
A common location is /var/log/syslog, but it can be anywhere.
Even the name of the syslog varies by distro.
If the name without a path is not syslog, a frequent home for the file is is the document root of the website (a.k.a., website home directory, not to be confused with the home directory for your account).
This is related to a known issue with logging found in the [Xcode 8 Beta Release Notes (hyper-link)] (also asked an engineer at WWDC).
When debugging WatchOS applications in the Watch simulator, the OS may produce an excessive
  amount of unhelpful logging.
When debugging an app on the Simulator, logs are visible.
Open the system log (⌘ + /)
This will dump out all of the debug data and also your NSLogs.
To filter just your NSLog statements:
Prefix each with a symbol, for example: NSLog(@"^ Test Log")
This is still not fixed in Xcode Version 8.0 beta 2 (8S162m) for me and extra logs are also appearing in the Xcode console
• Xcode Debug Console shows extra logging from system frameworks when
debugging applications in the Simulator.
prefix my print logs with some kind of special character (eg * or ^ or !
Then use the search box on the bottom right of the console pane to filter my console logs by inputing my chosen special character to get the console to display my print logs as intended
My solution is to use the debugger command and/or Log Message in breakpoints.
To stop the Xcode 8 iOS Simulator from logging like crazy, set an environment variable OS_ACTIVITY_MODE = disable in your debug scheme.
There is a process that gets launched called configd_sim when the Sim starts that reads the plists in and prints debugging information if the plists specify they should be logged.
/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneSimulator.platform/Developer/SDKs/iPhoneSimulator.sdk/System/Library/Preferences/Logging/Subsystems
Now, build and run your application and observe the logs.
As far as NSLog being the dumping ground for system messages, errors, and your own debugging: a real logging approach is probably called for anyway, e.g.
[https://github.com/fpillet/NSLogger (hyper-link)] .
Drink the new Kool-Aid: [http://asciiwwdc.com/2016/sessions/721 (hyper-link)] [https://developer.apple.com/videos/play/wwdc2016/721/ (hyper-link)]
It's not surprising that there are some hitches after overhauling the entire logging API.
Anyway, NSLog is just a shim:
NSLog / CFLog
NSLog is now just a shim to os_log in most circumstances.
[Incorrect comment about NSLog removed]
Building on the original [tweet (hyper-link)] from @rustyshelf, and illustrated answer from iDevzilla, here's a solution that silences the noise from the simulator without disabling NSLog output from the device.
• Xcode Debug Console no longer shows extra logging from system
  frameworks when debugging applications in the Simulator.
In Xcode 10 the OS_ACTIVITY_MODE variable with disable (or default) value also turns off the NSLog no matter what.
So if you want to get rid of the console noise but not of your own logs, you could try the good old printf("") instead of the NSLog since it is not affected by the OS_ACTIVITY_MODE = disable.
But better check out the new os_log API [here (hyper-link)].
Please note that for iOS 14 Simulator, the OS_ACTIVITY_MODE=disable will not show any logs using the new Swift Logger.
No need to use any environment variables etc, otherwise using these as suggested by the previous posts would disable NSLOG output which isn't what you want in such case.
I made sure that all my NSLOG would contain a String that I would monitor later on, example:
(Optional Step) or (Easier), I wrote a function to channel (well; proxy) NSLog into another global function "Log" whereas all my logged would go to that function, and then the Log function would append the "[Admin]" string to the original logging string and also add logging-time and other items I needed etc (similar to WebLogic or any other JAVA logging), this ensure that all logs would follow one standard.
In other words, the annoying lines of logs that has nothing to do with your app will always be there, but you choose not to see them using the above command which I hope solves the issue.
Start mysql with the --log option:
Either one will log all queries to log_file_name.
You can also log only slow queries using the --log-slow-queries option instead of --log.
By default, queries that take 10 seconds or longer are considered slow, you can change this by setting long_query_time to the number of seconds a query must take to execute before being logged.
Create your log tables on the mysql database
Enable Query logging on the database
View the log
Disable Query logging on the database
For the record, general_log and slow_log were introduced in 5.1.6:
[http://dev.mysql.com/doc/refman/5.1/en/log-destinations.html (hyper-link)]
Selecting General Query and Slow Query Log Output Destinations
As of MySQL 5.1.6, MySQL Server provides flexible control over the
destination of output to the general query log and the slow query log,
if those logs are enabled.
Possible destinations for log entries are
log files or the general_log and slow_log tables in the mysql
database
Besides what I came across here, running the following was the simplest way to dump queries to a log file without restarting
Quick way to enable MySQL General Query Log without restarting.
Enable the log for table
View log by select query
Adding logging (example, I don't think /var/log/... is the best path on Mac OS but that worked:
You can replace gitk there with something like git log --graph --oneline --decorate if you prefer a nice graph on the console over a separate GUI app.
Personally I use my log-all alias which show me every commit (recoverable commits) to have a better view of the situation :
Once you know your sha1, you simply change your stash reflog to add the old stash :
Identify the deleted stash hash code: 
gitk --all $( git fsck --no-reflog | awk '/dangling commit/ {print $3}' )
gitk --all $(git fsck --no-reflog | Select-String "(dangling commit )(.
Because they don't yet know about or understand the reflog.
I would advise everyone with this question to just check the reflog (git reflog), not much more than that.
In the process you'll have learned about the reflog and useful options to various basic git commands.
For me, using --no-reflogs did reveal the lost stash entry, but --unreachable (as found in many other answers) did not.
List lost stashes
--> run this command for a project where all stashes were trashed:

git fsck --unreachable | grep commit | cut -d ' ' -f3 | xargs git log
--merges --no-walk
[HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Fusion!EnableLog] (DWORD) to 1
[edit ]:Save the following text to a file, e.g FusionEnableLog.reg, in
Windows Registry Editor Format:
By the way, don't forget to turn off fusion logging when not needed.
If you have the Windows SDK installed on your machine, you'll find the "Fusion Log Viewer" under Microsoft SDK\Tools (just type "Fusion" in the start menu on Vista or Windows 7/8).
Launch it, click the Settings button, and select "Log bind failure" or "Log all binds".
If these buttons are disabled, go back to the start menu, right-click the Log Viewer, and select "Run as Administrator".
I usually use the Fusion Log Viewer ([Fuslogvw.exe (hyper-link)] from a [Visual Studio command prompt (hyper-link)] or Fusion Log Viewer from the start menu) - my standard setup is:
Open Fusion Log Viewer as administrator
Check the Enable custom log path checkbox
Enter the location you want logs to get written to, for example, c:\FusionLogs (Important: make sure that you have actually created this folder in the file system.)
Make sure that the right level of logging is on (I sometimes just select Log all binds to disk just to make sure things are working right)
Set the log location option to Custom
Remember to turn of logging off once you're done!
The [Fusion Log Settings Viewer changer script (hyper-link)] is bar none the best way to do this.
If you already have logging enabled and you still get this error on Windows 7 64 bit, try this in IIS 7.5:
Instead of using a ugly log file, you can also activate Fusion log via [ETW/xperf (hyper-link)] by turning on the DotnetRuntime Private provider (Microsoft-Windows-DotNETRuntimePrivate) with GUID 763FD754-7086-4DFE-95EB-C01A46FAF4CA and the FusionKeyword keyword (0x4) on.
Just in case you're wondering about the location of FusionLog.exe -
You know you have it, but you cannot find it?
After move to .NET 4.5 number of version of FUSION LOG has exploded.
[There's so much wrong (hyper-link)] with the Assembly Binding Log Viewer (FUSLOGVW.exe) that I decided to write an alternative viewer named [Fusion++ and put it on GitHub (hyper-link)].
It uses the same mechanics internally but parses the logs for you.
You don't have to care for any settings at all, not even log paths
I found out how to configure requests's logging level, it's done via the standard [logging (hyper-link)] module.
I decided to configure it to not log messages unless they are at least warnings:
In this way all the messages of level=INFO from urllib3 won't be present in the logfile.
So you can continue to use the level=INFO for your log messages...just modify this for the library you are using.
For anybody using logging.config.dictConfig you can alter the requests library log level in the dictionary like this:
In case you came here looking for a way to modify logging of any (possibly deeply nested) module, use logging.Logger.manager.loggerDict to get a dictionary of all of the logger objects.
The returned names can then be used as the argument to logging.getLogger:
Per user136036 in a comment, be aware that this method only shows you the loggers that exist at the time you run the above snippet.
If, for example, a module creates a new logger when you instantiate a class, then you must put this snippet after creating the class in order to print its name.
Add urllib3 in loggers section:
Add logger_urllib3 section:
Kbrose's guidance on finding which logger was generating log messages was immensely useful.
For my Django project, I had to sort through 120 different loggers until I found that it was the elasticsearch Python library that was causing issues for me.
As per the guidance in most of the questions, I disabled it by adding this to my loggers:
Posting here in case someone else is seeing the unhelpful log messages come through whenever they run an Elasticsearch query.
Setting the logger name as requests or requests.urllib3 did not work for me.
I had to specify the exact logger name to change the logging level.
First See which loggers you have defined, to see which ones you want to remove
{...'urllib3.poolmanager': <logging.Logger object at 0x1070a6e10>, 'django.request': <logging.Logger object at 0x106d61290>, 'django.template': <logging.Logger object at 0x10630dcd0>, 'django.server': <logging.Logger object at 0x106dd6a50>, 'urllib3.connection': <logging.Logger object at 0x10710a350>,'urllib3.connectionpool': <logging.Logger object at 0x106e09690> ...}
Then configure the level for the exact logger:
This answer is here:  [Python: how to suppress logging statements from third party libraries?
You can leave the default logging level for basicConfig, and then you set the DEBUG level when you get the logger for your module.
However, occasionally you want to prevent this behavior from happening, typically because you want to maintain a specific branch topology (e.g.
shutdown -l — Logs off.
Instead of performing an action, it displays a GUI dialog.
The message will end up in the Event Log.
0 - LOGOFF
I would write this in [Notepad (hyper-link)] or [WordPad (hyper-link)] for a basic logoff command:
This is basically the same as clicking start and logoff manually, but it is just slightly faster if you have the batch file ready.
[LOGOFF (hyper-link)] - allows you to logoff user by sessionid or session name
[change/chglogon (hyper-link)] - prevents new users to login or take another session
rdpinit - logs you out , though I cant find any documentation at the moment
will log all output from the cron job to /var/log/myjob.log
By default cron logs to /var/log/syslog so you can see cron related entries by using:
[https://askubuntu.com/questions/56683/where-is-the-cron-crontab-log (hyper-link)]
There are at least three different types of logging:
The logging BEFORE the program is executed, which only logs IF the
cronjob TRIED to execute the command.
That one is located in
/var/log/syslog, as already mentioned by @Matthew Lock.
The logging of errors AFTER the program tried to execute, which can be sent to
an email or to a file, as mentioned by @Spliffster.
I prefer logging
to a file, because with email THEN you have a NEW source of
problems, and its checking if email sending and reception is working
perfectly.
For example, in a
simple common desktop machine in which you are not interested in
configuring an smtp, sometimes you will prefer logging to a file:
[code snippet]

I would also consider checking the permissions of /ABSOLUTE_PATH_TO_LOG, and run the command from that user's permissions.
The logging of the program itself, with its own error-handling and logging for tracking purposes.
On Ubuntu you can enable a cron.log file to contain just the CRON entries.
Uncomment the line that mentions cron in  /etc/rsyslog.d/50-default.conf file:
Save and close the file and then restart the rsyslog service:
You can now see cron log entries in its own file:
However, you will not see more information about what scripts were actually run inside /etc/cron.daily or /etc/cron.hourly, unless those scripts direct output to the cron.log (or perhaps to some other log file).
If you want to verify if a crontab is running and not have to search for it in cron.log or syslog, create a crontab that redirects output to a log file of your choice - something like:
Steps taken from: [https://www.cyberciti.biz/faq/howto-create-cron-log-file-to-log-crontab-logs-in-ubuntu-linux/ (hyper-link)]
GNU Awk) allow you to use the file name /dev/stderr for error messages, but this is not properly portable; in Perl, warn and die print to standard error; in Python, write to sys.stderr, or use logging; in Ruby, try $stderr.puts.








/path/file.sh > /pathToKeepLogs/logFileName.log 2>&1
It would be O(n log n) if you built the heap by repeatedly inserting elements.
The main idea is that in the build_heap algorithm the actual heapify cost is not O(log n)for all elements.
As you can see not all heapify operations are O(log n), this is why you are getting O(n).
Continuing in the same direction, you'll have log(n) comparisons for the root in the worst case scenario.
and log(n)-1 for its immediate children, log(n)-2 for their immediate children and so on.
So summing it all up, you arrive on something like log(n) + {log(n)-1}*2 + {log(n)-2}*4 + ..... + 1*2^{(logn)-1} which is nothing but O(n).
"The complexity should be O(nLog n)... for each item we "heapify", it has the potential to have to filter down once for each level for the heap so far (which is log n levels)."
Your logic does not produce a tight bound -- it over estimates the complexity of each heapify.
If built from the bottom up, insertion (heapify) can be much less than O(log(n)).
( Step log(n) ) The last n/2log2(n) = 1 element goes in row log(n) up from the bottom.
h=log(n), heapify filters log(n) levels down.
Also, notice that only a single element, the root, actually incurs the full log(n) complexity.
Why doesn't that same logic work to make heap sort run in O(n) time rather than O(n log n)?
Although both operations are O(log n) in the worst case, in a heap, only one node is at the top whereas half the nodes lie in the bottom layer.
Let h = log n represent the height of the heap.
The first term alone is hn/2 = 1/2 n log n, so this approach has complexity at best O(n log n).
If it is possible to run buildHeap in linear time, why does heap sort require O(n log n) time?
The complexity of deleteMax for a heap is O(log n).
This sum is O(n log n) just like the inefficient version of buildHeap that is implemented using siftUp.
In summary, the work for heap sort is the sum of the two stages:  O(n) time for buildHeap and O(n log n) to remove each node in order, so the complexity is O(n log n).
You can prove (using some ideas from information theory) that for a comparison-based sort, O(n log n) is the best you could hope for anyway, so there's no reason to be disappointed by this or expect heap sort to achieve the O(n) time bound that buildHeap does.
=~ O(n^(n + O(1))), therefore T =~ O(nlog(n))
So, to find the sum of height of nodes which is given by
          S = summation from i = 0 to i = h of (2^i*(h-i)), where h = logn is height of the tree
        solving s, we get s = 2^(h+1) - 1 - (h+1)
        since, n = 2^(h+1) - 1
        s = n - h - 1 = n- logn - 1
        s = O(n), and so complexity of buildheap is O(n).
In case of building the heap, we start from height,
    logn -1 (where logn is the height of tree of n elements).
For each element present at height 'h', we go at max upto (logn -h) height down.
As we know the height of a heap is log(n), where n is the total number of elements.Lets represent it as h
   When we perform heapify operation, then the elements at last level(h) won't move even a single step.
This gives S=2h+1{1+h/2h+1}                    =2h+1+h                    ~2h+has h=log(n), 2h=n
Therefore S=n+log(n) T(C)=O(n)
The basis of your original mistake is due to a misinterpretation of the meaning of the statement, "insertion into a heap takes O(log n) time".
Insertion into a heap is indeed O(log n), but you have to recognise that n is the size of the heap during the insertion.
In the context of inserting n objects into a heap, the complexity of the ith insertion is O(log n_i) where n_i is the size of the heap as at insertion i.
Only the last insertion has a complexity of O (log n).
Then its height would be Log(N)
Now you want to insert another element, then the complexity would be : Log(N), we have to compare all the way UP to the root.
Now you are having N+1 elements & height = Log(N+1)
Using [induction (hyper-link)] technique it can be proved that the complexity of insertion would be ∑logi.
log a + log b = log ab
This simplifies to : ∑logi=log(n!)
which is actually O(NlogN)
Or in mathematical terms the height of the tree is log2(n), n being the length of the array.
Based on this we get this formula for the Siftdown approach:
(0 * n/2) + (1 * n/4) + (2 * n/8) + ... + (log2(n) * 1)
First, Remember that this logfile can grow very large on a busy server.
To enable the query log, put this in /etc/my.cnf in the [mysqld] section
See [http://dev.mysql.com/doc/refman/5.1/en/query-log.html (hyper-link)]
With mysql 5.1.29+ , the log option is deprecated.
To specify the logfile and enable logging, use this in my.cnf in the [mysqld] section:
Alternately, to turn on logging from MySQL console (must also specify log file location somehow, or find the default location):
Also note that there are additional options to log only slow queries, or those which do not use indexes.
You can disable or enable the general query log (which logs all queries) with
It shows how to enable, disable and to see the logs on live servers without restarting.
[Log all queries in mysql (hyper-link)]
Create your log tables (see [answer (hyper-link)])
Enable Query logging on the database
(Note that the string 'table' should be put literally and not substituted by any table name.
View the log
Disable Query logging on the database
Clear query logs without disabling
In there you can enable the slow query log and general log, see a live monitor, select a portion of the graph, see the related queries and analyse them.
I use this method for logging when I want to quickly optimize different page loads.
Logging to a TABLE
You can then select from my mysql.general_log table to retrieve recent queries.
I can then do something similar to tail -f on the mysql.log, but with more refinements...
To enable the query log in MAC Machine:
Set the query log url under 'mysqld' section as follows:
Few machine’s are not logging query properly, So that case you can enable it from MySQL console
Enabling general_log really put a dent on MySQL performance.
I left general_log =1 accidentally on a production server and spent hours finding out why performance was not comparable to a similar setup on other servers.
Then I found this which explains the impact of enabling general log.
[http://www.fromdual.com/general_query_log_vs_mysql_performance (hyper-link)].
Gist of the story, don't put general_log=1 in the .cnf file.
Instead use set global general_log =1 for a brief duration just to log enough to find out what you are trying to find out and then turn it off.
I had to drop and recreate the general log at one point.
During the recreation, character sets got messed up and I ended up having this error in the logs:
[ERROR] Incorrect definition of table mysql.general_log: expected the type of column 'user_host' at position 1 to have character set 'utf8' but found character set 'latin1'
So if the standard answer of "check to make sure logging is on" doesn't work for you, check to make sure your fields have the right character set.
I also wanted to enable the MySQL log file to see the queries and I have resolved this with the below instructions
go to /var/log/mysql/ and check the logs
console.log works.
By default on iOS, it logs to the debug pane inside Xcode.
From there, use the Chrome Developer tools JavaScript console to view console.log
However, one of the log messages said:
And console.log messages are back!
Visual Studio Code has a decent debug console that can show your console.log file.
Use console.log, console.warn, etc.
As of React Native 0.29, you can simply run the following to see logs in the console:
adb logcat *:S ReactNative:V ReactNativeJS:V
react-native log-ios
react-native log-android
This shows all console.log(), errors, notes, etc.
The [react-native-xlog (hyper-link)] module, that can help you, is [WeChat (hyper-link)]'s [Xlog (hyper-link)] for React Native.
That can output in the Xcode console and log file, and the Product log files can help you debug.
It not only has live updates, but it will allow you to see the console logs in your terminal just like when developing for the web, rather than having to use the browser like we did with React Native before.
The following steps should get you to the Chrome Developer Tools, where you will be able to see your console.log statements.
Now whenever a console.log statement is executed, it should appear in Chrome Dev Tools.
You will see the logs inside the terminal.
Run the logger:
Development Time Logging
For development time logging, you can use console.log().
One important thing, if you want to disable logging in production mode, then in Root Js file of app, just assign blank function like this - console.log = {}
It will disable whole log publishing throughout app altogether, which actually required in production mode as console.log consumes time.
Run Time Logging
In production mode, it is also required to see logs when real users are using your app in real time.
One of them which I've used is by [Logentries (hyper-link)]
The good thing is that Logentries has got [React Native Module (hyper-link)] as well.
So, it will take very less time for you to enable Run time logging with your mobile app.
You can use the remote js debugly option from your device or you can simply use react-native log-android and react-native log-ios for iOS.
console.log() is the easy way to debug your code, but it needs to be used with the arrow function or bind() while displaying any state.
console.log() is the best and simple way to see your log in the console when you use a remote JavaScript debugger from your developer menu.
It's so simple to get logs in React-Native.
Use console.log and console.warn
This log you can view in the browser console.
If you want to check the device log or, say, a production APK log, you can use:
to get the log output
So you can see your log output in the console tab.
If you are on OS X and using an emulator, you can view your console.logs directly in [Safari (hyper-link)]'s web inspector.
console.log(): shows in the console
Just console.log('debug');.
And running it, you can see the log in the terminal/command prompt.
In that case,
console.log('data::', data)
and debug JavaScript remotely is the best option.
You're going to find it under Logcat in Android Studio.
There are a lot of logging messages that show up here, so it may be easier for you to create a filter for "ReactNativeJS" which will only show your console.log messages that are created inside your React Native application.
console.log can be used for any JavaScript project.
Debugging with console.log
Debugging code (logic) with [Nuclide (hyper-link)]
Debugging code (logic) with Chrome
Use the React Native debugger for logging and [Redux (hyper-link)] store - [https://github.com/jhen0409/react-native-debugger (hyper-link)]
Put console.log("My log text") in your code
write this command: React-native log-android
write this command: React-native log-ios
It will give you a lot more functionality than just logging.
console.log and view logging statements on, without opting out for the remote debugging option from dev tools, Android Studio and Xcode.
Or you can opt out for the remote debugging option and view logging on Google dev tools, Visual Studio Code or any other editor that supports debugging.
It is a simple-and-easy-to-configure tool that enables you to see each logging statement of different levels (error, debug, warn, etc.).
It provides you with the GUI tool that shows all the logging of your application without slowing down the performance.
For example, you can use console.log(), console.warn(), 
console.clear() etc.
You can use Chrome Developer to use the console command when you're logging while you are running your React Native app.
There are multiple ways to log.
console.warn() will go through the log in the mobile screen itself.
It can be useful if you want to log small things and don’t want to bother opening console.
Another is console.log(), for which you will have to open the browser's console to view the logs.
With the newer React Native 0.62+, you can see the log in node itself.
So they've made it much the easier to view logs in newer version.
Where you want to log data, use
And to print this log in the terminal, use this command for Android:
If you use an iOS simulator, you can open system console log on Mac.
⌘ + space, type "console", press Enter to open the system console log, and select your simulator.
You can go with console.log or the debugger available with React Native.
Mine currently is named LogCat - emulator-5554.
If yes, then log it as "error".
"Session" lifecycle events (login, logout, etc.)
login failed due to bad credentials).
We use "debug" level logs for entry/exit of most non-trivial methods and marking interesting events and decision points inside methods.
trace: we don't use this often, but this would be for extremely detailed and potentially high volume logs that you don't typically want enabled even during normal development.
Examples include dumping a full object hierarchy, logging some state during every iteration of a large loop, etc.
As or more important than choosing the right log levels is ensuring that the logs are meaningful and have the needed context.
For example, you'll almost always want to include the thread ID in the logs so you can follow a single thread if needed.
user ID) to the thread so it gets logged as well.
In your log message, you'll want to include enough info to ensure the message can be actionable.
A log like " FileNotFound exception caught" is not very helpful.
There are also a number of good logging guides out there... for example, here's an edited snippet from [JCL (Jakarta Commons Logging) (hyper-link)]:
Expect these to be written to logs only.
Expect these to be written to logs only.
Error: critical logical errors on application, like a database connection timeout.
log: logic information, like a result of an if statement
During a propagating failure, logging levels should help to identify both which components are affected and which are a root cause.
If you decide to log happy paths then I recommend limiting to 1 log message per significant operation (e.g.
For all log messages be sure to log useful context (and prioritise on making messages human readable/useful rather than having reams of "error codes")
In development I would advise using a combination of TDD and Debugging (where necessary) as opposed to polluting code with log statements.
In production, the above INFO logging, combined with other metrics should be sufficient.
A nice way to visualise the above logging levels is to imagine a set of monitoring screens for each component.
When all running well they are green, if a component logs a WARNING then it will go orange (amber) if anything logs an ERROR then it will go red.
This may also tangentially help, to understand if a logging request (from the code) at a certain level will result in it actually being logged given the effective logging level that a deployment is configured with.
Decide what effective level you want to configure you deployment with from the other Answers here, and then refer to this to see if a particular logging request from your code will actually be logged then...
"Will a logging code line that logs at WARN actually get logged on my deployment configured with ERROR?"
"Will a logging code line that logs at WARN actually get logged on my deployment configured with DEBUG?"
from [logback documentation (hyper-link)]:
In the following table, the vertical header shows the level of the logging request, designated by p, while the horizontal header shows effective level of the logger, designated by q.
So a code line that requests logging will only actually get logged if the effective logging level of its deployment is less than or equal to that code line's requested level of severity.
Back is supposed to go back in time (to the time when the user was logged in).
Plus, if you're not using HTTPS, then your page is vulnerable to login stealing in many other ways.
Below shows the raw logs of my tests:
where 'Login' is my id value that is -1 after logout (you could use something else, a boolean for example).
with this solution back click is enable on every page and disable only after logout on each page on the same browser.
Then, if it was working before, and if it wasn't asking for you username, it must be because you had stored your credentials (login/password) in a $HOME/.netrc file, as [explained here (hyper-link)].
This step will require you to enter login and password.
Enter you github user name a s login and github password as password.
It happens if you change your login or password of git service account (Git).
This happened to us after forcing "Two Factor Authentication" before login from Gitlab.
Using https which should prompt for login credentials but instead errors:
I had just enabled two factor authentication for my Github account, so I disabled two factor authentication and I was able to login with my UID and PWD
I don't have two-factor authentication enabled but whenever, I would login to a new PC I get email verification code to verify PC.
Solution:
You can login to GitHub on a browser and verify your identity, that will let GitHub know to trust that PC.
Afterwards, login with bash and this time your login should work as usual because now you are whitelisted.
Doing so will allow you to login when you make another git operation.
As of SLF4J 1.6.0, in the presence of multiple parameters and if the last argument in a logging statement is an exception, then SLF4J will presume that the user wants the last argument to be treated as an exception and not a simple parameter.
logback, log4j, etc) as well on how the underlying framework is configured.
In addition to @Ceki 's answer, If you are using logback and setup a config file in your project (usually logback.xml), you can define the log to plot the stack trace as well using
If you dont mind trying Spring AOP, this is something I have been exploring for logging purposes and it works pretty well for me.
It wont log requests that have not been defined and failed request attempts though.
Now annotate all your rest API methods which you want to log
You could use javax.servlet.Filter if there wasn't a requirement to log java method that been executed.
That said, you can override DispatcherServlet to accomplish logging of request/response pair.
And here's the sample of logs:
Spring Boot has a modules called [Actuator (hyper-link)], which provides HTTP request logging out of the box.
You can customize it to log each request, or write to a DB.
Hosting providers, Heroku for example, provide request logging as part of their service and you don't need to do any coding whatsoever then.
For example to log all requests to log (and still use default implementation as a basic storage for serving info on /trace endpoint) I'm using this kind of implementation:
Logging POST data
all uploaded file content will go to logs)
Here is sample code, but don't use it:
Don't forget to change log level of org.springframework.web.filter.CommonsRequestLoggingFilter to DEBUG.
I had defined logging level in application.properties to print requests/responses, method url in the log file
The [Logbook (hyper-link)] library is specifically made for logging HTTP requests and responses.
To enable logging in Spring Boot all you need to do is adding the library to your project's dependencies.
By default the logging output looks like this:
The library does have some interfaces for writing custom loggers.
In the meantime the library has significantly evolved, current version is 2.4.1, see [https://github.com/zalando/logbook/releases (hyper-link)].
Do NOT forget to set the log level to TRACE, else you won't see anything:
But as they have told upper it doesn't log bodies.
In order to log requests that result in 400 only:
Also, setting setIncludeHeaders to false is a good idea if you don't want to log your auth headers!
Made some changes from above referred solution , request and response will log in console and in file too if logger level is info.
Currently Spring Boot has the Actuator feature to get the logs of requests and responses.
But you can also get the logs using Aspect(AOP).
@Before logs the request, @AfterReturning logs the response and @AfterThrowing logs the error message,
You may not need all endpoints' log, so you can apply some filters on the packages.
All endpoints within this package will generate the log.
All endpoints within this package will generate the log.
All endpoints within this package will generate the log.
Here, using @ConditionalOnExpression("${endpoint.aspect.enabled:true}") you can enable/disable the log.
just add endpoint.aspect.enabled:true into the application.property and control the log
In order to log all the requests with input parameters and body, we can use filters and interceptors.
By using this we can decouple the logging mechanism from the application.
AOP can be used for logging Input and output of each method in the application.
If you have Spring boot Config server configured then just enable Debug logger for class :
Debugs will log all the requests and  responses for every request
As [suggested previously (hyper-link)], [Logbook (hyper-link)] is just about perfect for this, but I did have a little trouble setting it up when using Java modules, due to a split package between logbook-api and logbook-core.
You have to manually add CommonsRequestLoggingFilter like
Spawn each worker process such that its log goes to a different file descriptor (to disk or to pipe.)
Ideally, all log entries should be timestamped.
Your controller process can then do one of the following:


If using disk files: Coalesce the log files at the end of the run, sorted by timestamp
If using pipes (recommended): Coalesce log entries on-the-fly from all pipes, into a central log file.
(E.g., Periodically [select (hyper-link)] from the pipes' file descriptors, perform merge-sort on the available log entries, and flush to centralized log.
just publish somewhere your instance of the logger.
that way, the other modules and clients can use your API to get the logger without having to import multiprocessing.
One of the alternatives is to write the mutliprocessing logging to a known file and register an atexit handler to join on those processes read it back on stderr; however, you won't get a real-time flow to the output messages on stderr that way.
Yet another alternative might be the various non-file-based logging handlers in the [logging package (hyper-link)]:
SyslogHandler
This way, you could easily have a logging daemon somewhere that you could write to safely and would handle the results correctly.
The SyslogHandler would take care of this for you, too.
Of course, you could use your own instance of syslog, not the system one.
I just now wrote a log handler of my own that just feeds everything to the parent process via a pipe.
I would just substitute the Pipe for a Queue since if multiple threads/processes use the same pipe end to generate log messages they will get garbled.
It also does not support setting the formatter or anything other than the root logger.
Basically, you have to reinit the logger in each of the pool processes with the queue and set up the other attributes on the logger.
A variant of the others that keeps the logging and queue thread separate.
I have a solution that's similar to ironhacker's except that I use logging.exception in some of my code and found that I needed to format the exception before passing it back over the Queue since tracebacks aren't pickle'able:
All current solutions are too coupled to the logging configuration by using a handler.
You can use any logging configuration you want
Logging is done in a daemon thread
Communication to the logging thread is done by multiprocessing.Queue
In subprocesses, logging.Logger (and already defined instances) are patched to send all records to the queue
How about delegating all the logging to another process that reads all log entries from a Queue?
Simply share LOG_QUEUE via any of the multiprocess mechanisms or even inheritance and it all works out fine!
If you have deadlocks occurring in a combination of locks, threads and forks in the logging module, that is reported in [bug report 6721 (hyper-link)] (see also [related SO question (hyper-link)]).
However, that will just fix any potential deadlocks in logging.
Python docs have two complete examples: [Logging to a single file from multiple processes (hyper-link)]
For those using Python < 3.2, just copy QueueHandler into your own code from: [https://gist.github.com/vsajip/591589 (hyper-link)] or alternatively import [logutils (hyper-link)].
Each process (including the parent process) puts its logging on the Queue, and then a listener thread or process (one example is provided for each) picks those up and writes them all to a file - no risk of corruption or garbling.
Logging should be easy!
You can also inherit for other logging handlers (StreamHandler etc.)
Since we can represent multiprocess logging as many publishers and one subscriber (listener), using [ZeroMQ (hyper-link)] to implement PUB-SUB messaging is indeed an option.
Moreover, [PyZMQ (hyper-link)] module, the Python bindings for ZMQ, implements [PUBHandler (hyper-link)], which is object for publishing logging messages over a zmq.PUB socket.
There's a [solution on the web (hyper-link)], for centralized logging from distributed application using PyZMQ and PUBHandler, which can be easily adopted for working locally with multiple publishing processes.
Package:
[https://pypi.python.org/pypi/multiprocessing-logging/ (hyper-link)]
code:
[https://github.com/jruere/multiprocessing-logging (hyper-link)]
The reasons for this handler are discussed in detail [here (hyper-link)], but in short there are certain worse race conditions with the other logging handlers.
Choose a path to save the logs to such as /var/log/...
For whoever might need this, I wrote a decorator for multiprocessing_logging package that adds the current process name to logs, so it becomes clear who logs what.
This allows me to see which worker creates which logs messages.
As of 2020 it seems there is a simpler way of logging with multiprocessing.
This function will create the logger.
In the init you instantiate the logger:
Now, you only need to add this reference in each function where you need logging:
I'd like to suggest to use the logger_tt library:  [https://github.com/Dragon2fly/logger_tt (hyper-link)]
The multiporcessing_logging library is not working on my macOSX, while logger_tt does.
The [concurrent-log-handler (hyper-link)] seems to do the job perfectly.
Create a separate file with a function that returns a logger.
The logger must have fresh instance of ConcurrentRotatingFileHandler for each process.
Example function get_logger() given below.
Creating loggers is done at the initialization of the process.
Simple child process that inherits multiprocessing.Process and simply logs to file text "Child process"
Important: The get_logger() is called inside the run(), or elsewhere inside the child process (not module level or in __init__().)
This is required as get_logger() creates ConcurrentRotatingFileHandler instance, and new instance is needed for each process.
The do_something is used just to demonstrate that this works with 3rd party library code which does not have any clue that you are using concurrent-log-handler.
The main process that logs into file two times a second "Main process".
Same comments for get_logger() and do_something() apply as for the child process.
This uses the ConcurrentRotatingFileHandler  from the concurrent-log-handler package.
Just a simple example to test if loggers from 3rd party code will work normally.
You need a reference point for your log to show the right commits.
So even if git log master..mybranch is one answer, it would still show too many commits, if mybranch is based on myotherbranch, itself based on master.
Do git log -n1; the commit Id is the merge base between b2 and master
git log will show your log history of b2 and master
Use commit range, if you aren't familiar with the concept, I invite you to google it or stack overflow-it,
For your actual context, you can do for example
[code snippet]
The ".." is the range operator for the log command.
That mean, in a simple form, give me all logs more recent than commitID_FOO...
Look at point #4, the merge base
So: git log COMMITID_mergeBASE..HEAD will show you the difference
Think of it as equivalent to git log except it only shows the SHA1—no log message, no author name, no timestamp, none of that "fancy" stuff.
For example, let's look at the excerpt of git log --graph master on cakephp GitHub repo below:
I needed to export log in one line for a specific branch.
When doing git log --pretty=oneline --graph we can see that all commit not done in the current branch are lines starting with |
So a simple grep -v do the job:
git log --pretty=oneline --graph | grep -v "^|"
In my specific ase, the final command was:
git log --pretty="%ad : %an, %s" --graph | grep -v "^|" | grep -v "Merge branch"
git log <your_branch> --not $(git branch -a | grep -v <your_branch> | grep -ve '->' | sed "s/\s//g")
It's deriveration is quite simple:
see [http://en.wikipedia.org/wiki/Logarithm (hyper-link)] -> Group Theory
log(n!)
= log(n * (n-1) * (n-2) * ... * 2 * 1) = log(n) + log(n-1) + ... + log(2) + log(1)
log(inf) + log(inf) + log(inf) + ... = inf * log(inf)
Take logs throughout to establish the result.
You can find more information about it, with an example from the blog post [Passing data to an Activity (hyper-link)].
Logging.
//Your problem is you want to store session id after sign in and available that session id for each activity where you want to logout.
//The solution of your problem is you have to store your session id after successful login in a public variable.
and whenever you need session id for logout you can access that variable  and replace variables value to zero.
Suppose that I have 2 activities, LoginActivity and HomeActivity.
I want to pass 2 parameters (username & password) from LoginActivity to HomeActivity.
Here is how I pass the data in my LoginActivity
Whenever you are navigating to any other activity through your login activity, you can put your sessionId into intent and get that in other activities though getIntent().
LoginActivity:
You have to activate the query logging in mysql.
cat /tmp/mysql.log ( you should see the query )
By default, all log files are created in the data directory.
Run several queries, open the above file (/var/log/mysql/mysql.log) and the log was there :)
In mysql we need to see often 3 logs which are mostly needed during any project development.
The Error Log.
The General Query Log.
The Slow Query Log.
By default no log files are enabled in MYSQL.
All errors will be shown in the syslog (/var/log/syslog).
step1: Go to this file (/etc/mysql/conf.d/mysqld_safe_syslog.cnf) and remove or comment those line.
To enable error log add following
To enable general query log add following
To enable Slow Query Log add following
To enable logs at runtime, login to mysql client (mysql -u root -p) and give:
Finally one thing I would like to mention here is I read this from a blog.
Click [here (hyper-link)] to visit the blog
The MySQL logs are determined by the global variables such as:
[log_error (hyper-link)] for the error message log;
[general_log_file (hyper-link)] for the general query log file (if enabled by general_log);
[slow_query_log_file (hyper-link)] for the slow query log file (if enabled by slow_query_log);
To print the value of error log, run this command in the terminal:
To read content of the error log file in real time, run:
When general log is enabled, try:
In addition to the answers above you can pass in command line parameters to the mysqld process for logging options instead of manually editing your conf file.
For example, to enable general logging and specifiy a file:
Confirming other answers above, mysqld --help --verbose gives you the values from the conf file (so running with command line options general-log is FALSE); whereas mysql -se "SHOW VARIABLES" | grep -e log_error -e general_log gives:
Use slightly more compact syntax for the error log:
To complement [loyola (hyper-link)]'s answer it is worth mentioning that as of MySQL 5.1 log_slow_queries is deprecated and is replaced with slow-query-log
Using log_slow_queries will cause your service mysql restart or service mysql start to fail
Basic Authentication wasn't designed to manage logging out.
What you have to do is have the user click a logout link, and send a ‘401 Unauthorized’ in response, using the same realm and at the same URL folder level as the normal 401 you send requesting a login.
a blank username-and-password, and in response you send back a “You have successfully logged out” page.
In short, the logout script inverts the logic of the login script, only returning the success page if the user isn't passing the right credentials.
Edit to add in response to comment: re-log-in is a slightly different problem (unless you require a two-step logout/login obviously).
You have to reject (401) the first attempt to access the relogin link, than accept the second (which presumably has a different username/password).
One would be to include the current username in the logout link (eg.
/relogin?username), and reject when the credentials match the username.
With Ajax you can have your 'Logout' link/button wired to a Javascript function.
Then set document.location back to the pre-login page.
This way, the user will never see the extra login dialog during logout, nor have to remember to put in bad credentials.
If they see a successful basic auth request with any bogus other username (let's say logout) they clear the credentials cache and possibly set it for that new bogus user name, which you need to make sure is not a valid user name for viewing content.
An "asynchronous" way of doing the above is to do an AJAX call utilizing the logout username.
Have the user click on a link to [https://log:out@example.com/ (hyper-link)].
That will overwrite existing credentials with invalid ones; logging them out.
This log user out without showing him the browser log-in box again, then redirect him to a logged out page
That should "log you out".
- secUrl is the url to a password protected area from which to log out.
- redirUrl is the url to a non password protected area (logout success page).
All you need is redirect user on some logout URL and return 401 Unauthorized error on it.
No logout.
1) on you logout page you call an ajax to your login back end.
Your login back end must accept logout user.
Once the back end accept, the browser clear the current user and assumes the "logout" user.
2) Now when the user got back to the normal index file it will try to automatic enter in the system with the user "logout", on this second time you must block it by reply with 401 to invoke the login/password dialog.
3) There are many ways to do that, I created two login back ends, one that accepts the logout user and one that doesn't.
My normal login page use the one that doesn't accept, my logout page use the one that accepts it.
And the auth credentials aren't removed, so this doesn't works (for now) to implement basic auth logouts, but maybe in the future will.
Just add a "Logout" link and when clicked return the following html
Sending https://invalid_login@hostname works fine everywhere except Safari on Mac (well, not checked Edge but should work there too).
Logout doesn't work in Safari when a user selects 'remember password' in the HTTP Basic Authentication popup.
Sending https://invalid_login@hostname doesn't affect Keychain Access, so with this checkbox it is not possible to logout on Safari on Mac.
When a user navigates to a logout page the code is executed and drops the credentials.
And I use the Get method to let it know I need to logout,
When you have already logout, then you need to refresh (F5) the page.
What I wanted was to integrate it with the logging module, which I eventually did after a couple of tries and errors.
And to use it, create your own Logger:
Be careful if you're using more than one logger or handler: ColoredFormatter is changing the record object, which is passed further to other handlers or propagated to other loggers.
If you have configured file loggers etc.
you probably don't want to have the colors in the log files.
The script does hack the logging.StreamHandler.emit method from standard library adding a wrapper to it.
The downside of their solution is that it modifies the message, and because that is modifying the actual logmessage any other handlers will get the modified message as well.
This resulted in logfiles with colorcodes in them in our case because we use multiple loggers.
Just use the color variables $BLACK - $WHITE in your log formatter string.
[http://plumberjack.blogspot.com/2010/12/colorizing-logging-output-in-terminals.html (hyper-link)]
Quick and dirty solution for predefined log levels and without defining a new class.
The default log format shown in the above example contains the date, time, hostname, the name of the logger, the PID, the log level and the log message.
Now there is a released PyPi module for customizable colored logging output:
[https://pypi.python.org/pypi/rainbow_logging_handler/ (hyper-link)]
[https://github.com/laysakura/rainbow_logging_handler (hyper-link)]
Update: Because this is an itch that I've been meaning to scratch for so long, I went ahead and wrote a library for lazy people like me who just want simple ways to do things: [zenlog (hyper-link)]
Colorlog is excellent for this.
It's [available on PyPI (hyper-link)] (and thus installable through pip install colorlog) and is [actively maintained (hyper-link)].
Here's a quick copy-and-pasteable snippet to set up logging and print decent-looking log messages:
If you want to change anything else than the message you can simply pass the color codes to log_format in the example.
Some content has been sourced (with modification) from:
The post above, and [http://plumberjack.blogspot.com/2010/12/colorizing-logging-output-in-terminals.html (hyper-link)].
the first regex group (parens) matches the initial date in the logfile, the second group matches a python filename, line number and function name, and the third group matches the log message that comes after that.
Obviously this is flexible enough that you can use it with any process, not just tailing logfiles.
For this reason, I prefer colout to any custom logfile-coloring tool, because I only need to learn one tool, regardless of what I'm coloring: logging, test output, syntax highlighting snippets of code in the terminal, etc.
It also avoids actually dumping ANSI codes in the logfile itself, which IMHO is a bad idea, because it will break things like grepping for patterns in the logfile unless you always remember to match the ANSI codes in your grep regex.
You can import the [colorlog (hyper-link)] module and use its ColoredFormatter for colorizing log messages.
The code only enables colors in log messages, if the colorlog module is installed and if the output actually goes to a terminal.
This avoids escape sequences being written to a file when the log output is redirected.
Some example logging calls:
Logger("File Name").info("This shows green text")
Well, I guess I might as well add my variation of the colored logger.
This is nothing fancy, but it is very simple to use and does not change the record object, thereby avoids logging the ANSI escape sequences to a log file if a file handler is used.
It does not effect the log message formatting.
If you are already using the [logging module's Formatter (hyper-link)], all you have to do to get colored level names is to replace your counsel handlers Formatter with the ColoredFormatter.
If you are logging an entire app you only need to do this for the top level logger.
colored_log.py
app.log content
Of course you can get as fancy as you want with formatting the terminal and log file outputs.
Only the log level will be colorized.
We could even adapt the wrapper to take a color argument to dynamicaly set the message's color using logger.debug("message", color=GREY)
This may be applied to the names of each log level.
Note that your log formatter must include the name of the log level
Instantiate logger
[FriendlyLog (hyper-link)] is another alternative.
The idea is to use log record factory to add 'colored' attributes to log record objects and than use these 'colored' attributes in log format.
(handy trick I discovered recently: I have a file with colored debug logs and whenever I want temporary increase the log level of my application I just tail -f the log file in different terminal and see debug logs on screen w/o changing any configuration and restarting application)
ability to override the color for a specific log message
configure the logger from a file (yaml in this case)
logging.yaml
What about highlighting also log message arguments with alternating colors, in addition to coloring by level?
Another advantage is that log call is made with Python 3 brace-style formatting.
See latest code and examples here: [https://github.com/davidohana/colargulog (hyper-link)]
Sample Logging code:
Install the colorlog package, you can use colors in your log messages immediately:
Obtain a logger instance, exactly as you would normally do.
Set the logging level.
You can also use the constants like DEBUG
and INFO from the logging module directly.
Set the message formatter to be the ColoredFormatter provided
by the colorlog library.
This method also helps you to quickly scan and find logs directly in the source code.
%(asctime)s - Time as human-readable string, when logging call was issued
%(created)f - Time as float when logging call was issued
%(funcName)s - Name of function containing the logging call
%(levelname)s - Text logging level
%(levelno)s - Integer logging level
%(lineno)d - Line number where the logging call was issued
%(message)s - Message passed to logging call (same as %(msg)s)
%(module)s - File name without extension where the logging call was issued
%(msecs)d - Millisecond part of the time when logging call was issued
%(msg)s - Message passed to logging call (same as %(message)s)
%(name)s - Logger name
%(pathname)s - Full pathname to file containing the logging call
%(relativeCreated)d - Time as integer in milliseconds when logging call was issued, relative to the time when logging module was loaded
[Coloredlogs package (hyper-link)]
[Logging library (hyper-link)]
Use [logging.exception (hyper-link)] from within the except: handler/block to log the current exception along with the trace information, prepended with a message.
Now looking at the log file, /tmp/logging_example.out:
My job recently tasked me with logging all the tracebacks/exceptions from our application.
When tasked with logging all the exceptions that our software might encounter in the wild I tried a number of different techniques to log our python exception tracebacks.
At first I thought that the python system exception hook, sys.excepthook would be the perfect place to insert the logging code.
Either subclass Thread and wrap the run method in our own try except block in order to catch and log exceptions or monkey patch threading.Thread.run to run in your own try except block and log the exceptions.
The first method of subclassing Thread seems to me to be less elegant in your code as you would have to import and use your custom Thread class EVERYWHERE you wanted to have a logging thread.
However, it was clear as to what this Thread was doing and would be easier for someone to diagnose and debug if something went wrong with the custom logging code.
A custome logging thread might look like this:
The second method of monkey patching threading.Thread.run is nice because I could just run it once right after __main__ and instrument my logging code in all exceptions.
It was not until I started testing my exception logging I realized that I was going about it all wrong.
This was very frustrating because I saw the traceback bring printed to STDOUT but not being logged.
It was I then decided that a much easier method of logging the tracebacks was just to monkey patch the method that all python code uses to print the tracebacks themselves, traceback.print_exception.
This code writes the traceback to a String Buffer and logs it to logging ERROR.
I have a custom logging handler set up the 'customLogger' logger which takes the ERROR level logs and send them home for analysis.
Uncaught exception messages go to STDERR, so instead of implementing your logging in Python itself you could send STDERR to a file using whatever shell you're using to run your Python script.
You can log all uncaught exceptions on the main thread by assigning a handler to [sys.excepthook (hyper-link)], perhaps using the [exc_info parameter of Python's logging functions (hyper-link)]:
You can get the traceback using a logger, at any level (DEBUG, INFO, ...).
Note that using logging.exception, the level is ERROR.
This is of course only if you do not intend to restore any transaction log backups, i.e.
Rename or delete the Database and Log files (C:\Program Files\Microsoft SQL Server\MSSQL.1\MSSQL\Data...) or wherever you have the files;
I have had this problem when I also recieved a TCP error in the event log...
I had a similar incident with stopping a log shipping secondary server.
After the command to remove the server from log shipping and stopped the log shipping from primary server the database on secondary server got stuck in restoring status after the command
Then I copied both the Ldf and Mdf files of the database:
   [db name].mdf         and        [db name]_log.ldf
Started my MS SQL Management studio with normal login.
WITH RECOVERY option is used by default when RESTORE DATABASE/RESTORE LOG commands is executed.
If you are taking a tail-log backup, this issue can also be caused by having this option checked in the SSMS Restore wizard - "Leave source database in the restoring state (WITH NORECOVERY)"
In the log file, I found this:
The 'NORECOVERY' options, basically tells the SQL Server that the database is waiting for more restore files (could be a DIFF file and LOG file and, could include tail-log backup file, if possible).
No LOG backup are allowed in SIMPLE recovery model database.
Otherwise, if your database is set up with FULL or BULK-LOGGED recovery model, you can perform a FULL restore followed by NORECOVERYoption, then perform a DIFF followed by NORECOVERY, and, at last, perform LOG restore with RECOVERY option.
I believe, the catch is the to have the .mdf and .log file in the same directory.
Right Click database go to Tasks --> Restore --> Transaction logs
In the transactions files if you see a file checked, then SQL server is trying to restore from this file.
Log into the server itself and fire up the default SSMS program on the actual database server.
git log -10 --reverse would get 10 last commits then reverse list") has been clarified in Git 2.11 (Q4 2016):
Users often wonder if the oldest or the newest n commits are shown by [log -n --reverse (hyper-link)].
Use git log --graph or gitk.
(You could just use git log --format=oneline, but it will tie commit messages to numbers, which looks less pretty IMHO).
However, if you sometimes have to log in to a remote machine where you can't modify the config file, you could use a more simple but faster to type version:
I like, with [git log (hyper-link)], to do:
"--pretty=<style>" option to the log family of commands can now be  spelled as "--format=<style>".
You can also limit the span of the log display (number of commits):
To any of these recipes (based on git log or gitk), you can add --simplify-by-decoration to collapse the uninteresting linear parts of the history.
This makes much more of the topology visible at once.
99.999% of my time is looking at history by git lg and the 0.001% is by git log.
I just want to share two log aliases that might be useful (configure from .gitconfig):
I found [this blog post (hyper-link)] which shows a concise way:
Another git log command.
Take a look at [GitKraken (hyper-link)] - a cross-platform GUI that shows topology in a lucid way.
I have this git log alias in ~/.gitconfig to view the graph history:
In Git [2.12 (hyper-link)]+ you can even customize the line colors of the graph using the [log.graphColors (hyper-link)] configuration option.
As for the logs' format, it's similar to [--oneline (hyper-link)], with the addition of the author name (respecting .mailmap) and the relative author date.
The most rated answers are showing git log commands as favorite solutions.
If you need a tablelike, say column-like output, you can use your awesome git log commands with slight modifications and some limitations with the .gitconfig alias.tably snippet below.
compared to normal git log output this one is slow, but nice
To achieve this, add the following to your .gitconfig file and call your log alias with
git tably YourLogAlias:
If there are problems with your git log commands leave a comment.
I want to share my compact preset for git log command:
(green is my default console color)
[ (hyper-link)]
You can add it into your config file using these commands:
(note that they will change the date format for all git log formats!)
If you'd like to make any changes, see the [PRETTY FORMATS section (hyper-link)] of git log reference.
Apologies in advance for readability of the backticks ("`"), but these work in shells other than bash and are thus more pasteable.
Use a single python file to config my log as singleton pattern which named 'log_conf.py'
This is a singleton pattern to log, simply and efficiently.
Actually every logger is a child of the parent's package logger (i.e.
package.subpackage.module inherits configuration from package.subpackage), so all you need to do is just to configure the root logger.
This can be achieved by [logging.config.fileConfig (hyper-link)] (your own config for loggers) or [logging.basicConfig (hyper-link)] (sets the root logger).
Setup logging in your entry module (__main__.py or whatever you want to run, for example main_script.py.
and then create every logger using:
For more information see [Advanced Logging Tutorial (hyper-link)].
Best practice is, in each module, to have a logger defined like this:
If you need to subdivide logging activity inside a module, use e.g.
and log to loggerA and loggerB as appropriate.
See [here (hyper-link)] for logging from multiple modules, and [here (hyper-link)] for logging configuration for code which will be used as a library module by other code.
Update: When calling fileConfig(), you may want to specify disable_existing_loggers=False if you're using Python 2.6 or later (see [the docs (hyper-link)] for more information).
The default value is True for backward compatibility, which causes all existing loggers to be disabled by fileConfig() unless they or their ancestor are explicitly named in the configuration.
With the value set to False, existing loggers are left alone.
Now you could use multiple loggers in same module and across whole project if the above is defined in a separate module and imported in other modules were logging is required.
So LoggerManager can be a pluggable to the entire application.
Then in each module I need a logger, I do:
When the logs are missed, you can differentiate their source by the module they came from.
The reason is that the file config will disable all existing loggers by default.
Now the log specified in logging.ini will be empty, as the existing logger was disabled by fileconfig call.
While is is certainly possible to get around this (disable_existing_Loggers=False), realistically many clients of your library will not know about this behavior, and will not receive your logs.
Make it easy for your clients by always calling logging.getLogger locally.
So good practice is instead to always call logging.getLogger locally.
Also, if you use fileconfig in your main, set disable_existing_loggers=False, just in case your library designers use module level logger instances.
My main objective was to be able to pass logs to handlers by their level (debug level logs to the console, warnings and above to files):
created a nice util file named logger.py:
the application logger is always starting with flask.app as its the module's name.
This will create a new log for "app.flask.MODULE_NAME" with minimum effort.
The best practice would be to create a module separately which has only one method whose task we be to give a logger handler to the the calling method.
Save this file as m_logger.py
Now call the getlogger() method whenever logger handler is needed.
Then to use your logger in any other file:
The name of the logger for the entry point of your program will be __main__, but any solution using __name__ will have that issue.
A simple way of using one instance of logging library in multiple modules for me was following solution:
Here is a [blog post (hyper-link)] about this solution.
Comment logs:
Uncomment logs:
A constraint is that your logging instructions must not span over multiple lines.
I suggest having a static boolean somewhere indicating whether or not to log:
Then wherever you want to log in your code, just do this:
Now when you set MyDebug.LOG to false, the compiler will strip out all code inside such checks (since it is a static final, it knows at compile time that code is not used.)
For larger projects, you may want to start having booleans in individual files to be able to easily enable or disable logging there as needed.
For example, these are the various logging constants we have in the window manager:
I find a far easier solution is to forget all the if checks all over the place and just use [ProGuard (hyper-link)] to strip out any Log.d() or Log.v() method calls when we call our Ant release target.
Update (4.5 years later): Nowadays I used [Timber (hyper-link)] for Android logging.
Not only is it a bit nicer than the default Log implementation — the log tag is set automatically, and it's easy to log formatted strings and exceptions — but you can also specify different logging behaviours at runtime.
In this example, logging statements will only be written to logcat in debug builds of my app:
Then anywhere else in my code I can log easily:
See the [Timber sample app (hyper-link)] for a more advanced example, where all log statements are sent to logcat during development and, in production, no debug statements are logged, but errors are silently reported to Crashlytics.
All good answers, but when I was finished with my development I didn´t want to either use if statements around all the Log calls, nor did I want to use external tools.
So the solution I`m using is to replace the android.util.Log class with my own Log class:
The only thing I had to do in all the source files was to replace the import of android.util.Log  with my own class.
I would consider using roboguice's [logging facility (hyper-link)] instead of the built-in android.util.Log
Their facility automatically disables debug and verbose logs for release builds.
customizable logging behavior, additional data for every log and more)
Using proguard could be quite a hassle and I wouldn't go through the trouble of configuring and making it work with your application unless you have a good reason for that (disabling logs isn't a good one)
I have improved on the solution above by providing support for different log levels and by changing the log levels automatically depending on if the code is being run on a live device or on the emulator.
I have used a [LogUtils (hyper-link)] class like in the Google IO example application.
Per android.util.Log provides a way to enable/disable log:
Default the method isLoggable(...) returns false, only after you setprop in device likes this:
It means any log above DEBUG level can be printed out.
Checks to see whether or not a log for the specified tag is loggable at the specified level.
This means that any level above and including INFO will be
  logged.
Before you make any calls to a logging method you should check
  to see if your tag should be logged.
You can change the default level
  by setting a system property: 'setprop log.tag. '
SUPPRESS will turn off all logging for your tag.
You can
  also create a local.prop file that with the following in it:
  'log.tag.=' and place that in /data/local.prop.
So we could use custom log util:
logs will only be used in your debug ver, and then use
I then choose to replace all "Log."
with "//Log.".
This removes all log statements.
To put them back later I repeat the same replace but this time as replace all "//Log."
with "Log.
Just remember to set the replace as case sensitive to avoid accidents such as "Dialog.".
For added assurance you can also do the first step with " Log."
I like to use Log.d(TAG, some string, often a String.format ()).
Transform Log.d(TAG, --> Logd( in the text of your class
use DebugLog
All logs are disabled by DebugLog when the app is released.
[https://github.com/MustafaFerhan/DebugLog (hyper-link)]
I would like to add some precisions about using Proguard with Android Studio and gradle, since I had lots of problems to remove log lines from the final binary.
And if the integrator wants some log outputs, he can uses Logback for Android and activate the logs, so logs can be  redirected to a file or to LogCat.
If I really need to strip the logs from the final library, I then add to my Proguard file (after having enabled the proguard-android-optimize.txt file of course):
Type in Log.e(TAG, or however you have defined your Log messages into the "Text to find" textbox.
Android Studios will now go through all your files in your project and replace all the Logs with Timbers.
This will result in the app logging only when you are in development mode not in production.
You can also have BuildConfig.RELEASE for logging in release mode.
Timber is very nice, but if you already have an existing project - you may try github.com/zserge/log .
It's a drop-in replacement for android.util.Log and has most of the the features that Timber has and even more.
[his log library (hyper-link)] provides simple enable/disable log printing switch as below.
In addition, it only requires to change import lines, and nothing needs to change for Log.d(...); statement.
Logs can be removed using bash in linux and sed:
Works for multiline logs.
In this solution you can be sure, that logs are not present in production code.
Use proguard-android-optimize.txt and you don't need to worry about android.util.Log in your proguard-rules.pro:
I know this is an old question, but why didn't you replace all your log calls with something like
Boolean logCallWasHere=true; //---rest of your log here
I created [this Jupyter notebook (hyper-link)] to go over all java files and comment out all the Log messages.
2) select on one Log.d(TAG, "text"); the part 'Log.'
this removes all LOG calls at once in a java file.
If you want to use a programmatic approach instead of using ProGuard, then by creating your own class with two instances, one for debug and one for release, you can choose what to log in either circumstances.
So, if you don't want to log anything when in release, simply implement a Logger that does nothing, like the example below:
Then to use your logger class:
log.end is better because it asks node to close immediately after the write.
Here's a [video tutorial (hyper-link)] on the logic behind the script.
When you want to write in a log file, i.e.
Moreover, appendFile will write when it is enabled, so your logs will not be written by timestamp.
If you want an easy and stress-free way to write logs line by line in a file, then I recommend [fs-extra (hyper-link)]:
This got the benefit of low RAM usage (if that matters to anyone) and I believe it's more safe to use for logging/recording (my original use case).
You can subclass [HandleErrorAttribute (hyper-link)] and override its [OnException (hyper-link)] member (no need to copy) so that it logs the exception with ELMAH and only if the base implementation handles it.
This second version will try to use [error signaling (hyper-link)] from ELMAH first, which involves the fully configured pipeline like logging, mailing, filtering and what have you.
If not, the error is simply logged.
You may also have to take care that if multiple HandleErrorAttribute instances are in effect then duplicate logging does not occur, but the above two examples should get your started.
For more infomation check out my blog series on logging in MVC.
bear in mind that you may have to take care that if multiple HandleErrorAttribute instances are in effect then duplicate logging does not occur.
For me it was very important to get email logging working.
I faced the same problem, the following is my workable in my Erorr.vbhtml (it work if you only need to log the error using Elmah log)
There is now an ELMAH.MVC package in NuGet that includes an improved solution by Atif and also a controller that handles the elmah interface within MVC routing (no need to use that axd anymore)
The problem with that solution (and with all the ones here) is that one way or another the elmah error handler is actually handling the error, ignoring what you might want to set up as a customError tag or through ErrorHandler or your own error handler
The best solution IMHO is to create a filter that will act at the end of all the other filters and log the events that have been handled already.
The elmah module should take care of loging the other errors that are unhandled by the application.
Notice that I left a comment there to remind people that if they want to add a global filter that will actually handle the exception it should go BEFORE this last filter, otherwise you run into the case where the unhandled exception will be ignored by the ElmahMVCErrorFilter because it hasn't been handled and it should be loged by the Elmah module but then the next filter marks the exception as handled and the module ignores it, resulting on the exception never making it into elmah.
This setup allows you to set your own ErrorHandler tags in classes and views, while still loging those errors through the ElmahMVCErrorFilter, adding a customError configuration to your web.config through the elmah module, even writing your own Error Handlers.
Check your event logs (application and security) to see if any exceptions were thrown.
Use this FAQ page:  [Apache log4net Frequently Asked Questions (hyper-link)]
About 3/4 of the way down it tells you how to enable log4net debugging by using application tracing.
somewhere to make log4net read your configuration?
Also, Make sure the "Copy always" option is selected for [log4net].config
In my case, log4net wasn't logging properly due to having a space in my project name.
For me I moved the location of the logfiles and it was only when I changed the name of the file to something else it started again.
It seems if there is a logfile with the same name already existing, nothing happens.
Afterwards I rename the old file and changed the log filename in the config back again to what it was.
In my case I had to give the IIS_IUSRS Read\write permission to the log file.
[assembly: log4net.Config.XmlConfigurator(ConfigFile = "Web.config", Watch = true)]
Then, you have to register your Log4net config file to application.
After registering process, you can call below definition to call logger:
For me I had to move Logger to a Nuget Package.
See [https://gurunadhduvvuru.wordpress.com/2020/04/30/log4net-issues-when-moved-it-to-a-nuget-package/ (hyper-link)] for more details.
There are a few ways to use log4net.
The solution is described here: [https://www.hemelix.com/log4net/ (hyper-link)]
I think the default place for access logs is
Otherwise, check under IIS Manager, select the computer on the left pane, and in the middle pane, go under "Logging" in the IIS area.
Which will contain similar log files that only represents errors.
Try the Windows event log, there can be some useful information
I think the Default place for IIS logging is: c:\inetpub\wwwroot\log\w3svc
Enabling [Tracing (hyper-link)] may be a better alternative to the Windows Event Log.
The 100% correct answer for the default location of the log files is...
To be 100% sure, you need to look at the logging for the web site in IIS.
[https://docs.microsoft.com/en-us/iis/get-started/whats-new-in-iis-85/enhanced-logging-for-iis85 (hyper-link)]
Double-click Logging.
The location of log files for the site can be found within the Directory field
EDIT: As pointed out by Andy in the comments below you need to ensure when installing IIS that you elected to enable HTTP logging, otherwise HTTP logging won't be available.
I believe this is an easier way of knowing where your IIS logs are, rather than just assuming a default location:
Default, click on it, and you should see "Logging" to the right if logging is enabled:
I'm adding this answer because after researching the web, I ended up at this answer but still didn't know which subfolder of the IIS logs folder to look in.
Once you know the ID, let's call it n,  the corresponding logs are in the W3SVCn subfolder of the IIS logs folder.
So, if your website ID is 4, say, and the IIS logs are in the default location, then the logs are in this folder:
Answer by @jishi tells where the logs are by default.
Answer by @Bergius gives a programmatic way to find the log folder location for a specific website, taking ID into account, without using IIS.
I have found the IIS Log files at the following location.
C:\inetpub\logs\LogFiles\
C:\inetpub\logs\LogFiles
Here's the [explanation of reflog from the Pro Git book (hyper-link)]:
One of the things Git does in the background while you’re working away is keep a reflog — a log of where your HEAD and branch references have been for the last few months.
You can see your reflog by using git reflog:
The reflog command can also be used to delete entries or expire entries from the reflog that are too old.
From the [official Linux Kernel Git documentation for reflog (hyper-link)]:
The subcommand expire is used to prune older reflog entries.
To delete single entries from the reflog, use the subcommand delete and specify the exact entry (e.g.
git reflog delete master@{2}).
[git log (hyper-link)] shows the commit log accessible from the refs (heads, tags, remotes)
[git reflog (hyper-link)] is a record of all commits that are or were referenced in your repo at any time.
That is why git reflog (a local recording which is pruned after 90 days by default) is used when you do a "destructive" operation (like deleting a branch), in order to get back the SHA1 that was referenced by that branch.
git reflog expire removes reflog entries older than this time; defaults to 90 days.
git reflog is often reference as "[your safety net (hyper-link)]"
In case of trouble, the general advice, when git log doesn't show you what you are looking for, is:
"[Keep calm and use git reflog (hyper-link)]"
Again, reflog is a local recording of your SHA1.
As opposed to git log: if you push your repo to an [upstream repo (hyper-link)], you will see the same git log, but not necessarily the same git reflog.
git log shows the current HEAD and its ancestry.
To see a more representative log, use a command like git log --oneline --graph --decorate.)
git reflog doesn't traverse HEAD's ancestry at all.
The reflog is an ordered list of the commits that HEAD has pointed to: it's undo history for your repo.
The reflog isn't part of the repo itself (it's stored separately to the commits themselves) and isn't included in pushes, fetches or clones; it's purely local.
Aside: understanding the reflog means you can't really lose data from your repo once it's been committed.
If you accidentally reset to an older commit, or rebase wrongly, or any other operation that visually "removes" commits, you can use the reflog to see where you were before and git reset --hard back to that ref to restore your previous state.
git log shows a history of all your commits for the branch you're on.
If you want to see you commit history for all branches, type git log --all.
git reflog shows a record of your references as Cupcake said.
Try switching back and forth between two branches a few times using git checkout and run git reflog after each checkout.
You do not see these types of entries in git log.
References:
[http://www.lornajane.net/posts/2014/git-log-all-branches (hyper-link)]
Actually, reflog is an alias for
I like to think of the difference between git log and reflog as being the difference between a private record and a public record.
With the git reflog, it keeps track of everything you've done locally.
Reflog tracks it.
Reflog  tracks it.
Reflog tracks it.
Everything you've done locally, there's an entry for it in the reflog.
This isn't true for the log.
If you amend a commit, the log only shows the new commit.
If you do a reset and skip back a few commits in your history, those commits you skipped over won't show up in the log.
When you push your changes to another developer or to [GitHub (hyper-link)] or something like that, only the content tracked in the log will appear.
So yeah, I like the 'private vs public' analogy.
Or maybe a better [log vs reflog (hyper-link)] analogy is 'polished vs lapidary.'
The reflog shows all your trials and errors.
The log just shows a clean and polished version of your work history.
The reflog shows all of it.
Yet the log command makes it look as though there has only ever been one commit against the repo:
Also, since the reflog keeps track of things you amended and commits you [reset (hyper-link)], it allows you to go back and find those commits because it'll give you the commit ids.
Assuming your repository hasn't been purged of old commits, that allows you to resurrect items no longer visible in the log.
That's how the reflog sometimes ends up saving someone's skin when they need to get back something they thought they inadvertently lost.
git log will start from current HEAD, that is point to some branch (like master) or directly to commit object (sha code), and will actually scan the object files inside .git/objects directory commit after commit using the parent field that exist inside each commit object.
replace a721d with some of your commit code) and delete the branches rm .git/refs/heads/*
Now git log --oneline will show only HEAD and its commits ancestors.
git reflog on the other hand is using direct log that is created inside .git/logs
Experiment: rm -rf .git/logs and git reflog is empty.
Anyway, even if you lose all tags and all branches and all logs inside logs folder, the commits objects are inside .git/objects directory so you can reconstruct the tree if you find all dangling commits: git fsck
Searched for hours and then, at last, I found [this blog (hyper-link)].
Also in Android 12, the intent chooser bottom dialog shows the preview of the image you are sharing which is super cool by the way, but it can't load the preview from the scoped storage URI.
The console.log([...matches]) shows
Essentially you need to set up the applications exception handling to log, something like:
Cocoa already logs the stack trace on uncaught exceptions to the console although they're just raw memory addresses.
Is logarithmic.
Is N * log ( N ).
The running time consists of N loops (iterative or recursive) that are logarithmic, thus the algorithm is a combination of linear and logarithmic.
In general, doing something with every item in one dimension is linear, doing something with every item in two dimensions is quadratic, and dividing the working area in half is logarithmic.
The quicksort algorithm would be described as O ( N * log ( N ) ).
log(n) = O(n)
An algorithm is said to run in logarithmic time if its time execution is proportional to the logarithm of the input size.
O(log n) – Logarithmic Time
Algorithm that has running time O(log n) is slight faster than O(n).
O(n log n) – Linearithmic Time
This running time is often found in "divide & conquer algorithms" which divide the problem into sub problems recursively and then merge them in n time.
O(log N)
Ultimately, we look at O(log_2 N) individuals.
O(N log N)
If a large number of people came to the table, one at a time, and all did this, that would take O(N log N) time.
However, if everyone is sitting down at the table, it will take only O(log N) time.
Assuming the host is unavailable, we can say that the Inigo-finding algorithm has a lower-bound of O(log N) and an upper-bound of O(N), depending on the state of the party when you arrive.
O(Logn) Time Complexity of a loop is considered as O(Logn) if the loop variables is divided / multiplied by a constant amount.
[code snippet]
For example Binary Search has O(Logn) time complexity.
O(LogLogn) Time Complexity of a loop is considered as O(LogLogn) if the loop variables is reduced / increased exponentially by a constant amount.
The important thing about series (1/1 + 1/2 + 1/3 + … + 1/n) is equal to O(Logn).
So the time complexity of the above code is O(nLogn).
Logarithmic time has an order of growth LogN, it usually occurs
when you're dividing something in half (binary search, trees, even loops), or multiplying something in same way.
Linearithmic, order of growth is n*logN, usually occurs in divide and conquer algorithms.
I'm not qualified to comment on logging for .Net, since my bread and butter is Java, but we've had a migration in our logging over the last 8 years you may find a useful analogy to your question.
We started with a Singleton logger that was used by every thread within the JVM, and set the logging level for the entire process.
This resulted in huge logs if we had to debug even a very specific part of the system, so lesson number one is to segment your logging.
Our current incarnation of the logger allows multiple instances with one defined as the default.
We can instantiate any number of child loggers that have different logging levels, but the most useful facet of this architecture is the ability to create loggers for individual packages and classes by simply changing the logging properties.
We are using the Apache commons-logging library wrapped around Log4J.
After reading Jeffrey Hantin's post below, I realized that I should have noted what our internal logging wrapper has actually become.
It's now essentially a factory and is strictly used to get a working logger using the correct properties file (which for legacy reasons hasn't been moved to the default position).
Since you can specify the logging configuration file on command line now, I suspect it will become even leaner and if you're starting a new application, I'd definitely agree with his statement that you shouldn't even bother wrapping the logger.
We use Log4Net at work as the logging provider, with a singleton wrapper for the log instance (although the singleton is under review, questioning whether they are a good idea or not).
Nice number of log levels and configurations around them
So we have a library which wraps log4net and within our code we just need stuff like this:
Within the methods we do a check to see if the logging level is enabled, so you don't have redundant calls to the log4net API (so if Debug isn't enabled, the debug statements are ignored), but when I get some time I'll be updating it to expose those so that you can do the checks yourself.
By default we log at two locations:
We don't use the EventLog as it can require higher security than we often want to give a site.
I find Notepad works just fine for reading logs.
We use log4net on our web applications.
It's ability to customize logging at run-time by changing the XML configuration file is very handy when an application is malfunctioning at run-time and you need to see more information.
It also allows you to target specific classes or attributes to log under.
The event log we use for errors or exceptions.
We log most events to a database so that we can create custom reports and let the users view the log if they want to right from the application.
I have to join the chorus recommending log4net, in my case coming from a platform flexibility (desktop .Net/Compact Framework, 32/64-bit) point of view.
log4net.ILogger is the .Net counterpart of the [Commons Logging (hyper-link)] wrapper [API (hyper-link)] already, so coupling is already minimized for you, and since it is also an Apache library, that's usually not even a concern because you're not giving up any control: fork it if you must.
Using a global singleton logger (or equivalently a static entry point) which loses the fine resolution of the recommended [logger-per-class pattern (hyper-link)] for no other selectivity gain.
Failing to expose the optional Exception argument, leading to multiple problems:


It makes an exception logging policy even more difficult to maintain, so nothing  is done consistently with exceptions.
Failing to expose the IsLevelEnabled properties, which discards the ability to skip formatting code when areas or levels of logging are turned off.
I don't often develop in asp.net, however when it comes to loggers I think a lot of best practices are universal.
Here are some of my random thoughts on logging that I have learned over the years:
Use a logger abstraction framework - like slf4j (or roll your own), so that you decouple the logger implementation from your API.
I have seen a number of logger frameworks come and go and you are better off being able to adopt a new one without much hassle.
Use a framework that can be configured by external files, so that your customers / consumers can tweak the log output easily so that it can be read by commerical log management applications with ease.
Be sure not to go overboard on custom logging levels, otherwise you may not be able to move to different logging frameworks.
Try to avoid XML/RSS style logs for logging that could encounter catastrophic failures.
This is important because if the power switch is shut off without your logger writing the closing </xxx> tag, your log is broken.
Log threads.
If you have to internationalize your logs, you may want to have a developer only log in English (or your language of choice).
Sometimes having the option to insert logging statements into SQL queries can be a lifesaver in debugging situations.
You want class-level logging.
You normally don't want static instances of loggers as well - it is not worth the micro-optimization.
Marking and categorizing logged exceptions is sometimes useful because not all exceptions are created equal.
So knowing a subset of important exceptions a head of time is helpful, if you have a log monitor that needs to send notifications upon critical states.
Do you really want to see the same logging statement repeated 10^10000000 times?
Wouldn't it be better just to get a message like:
This is my logging statement - Repeated 100 times
We use a mix of the logging application block, and a custom logging helper that works around the .Net framework bits.
The LAB is configured to output fairly extensive log files included seperate general trace files for service method entry/exit and specific error files for unexpected issues.
The custom logging helper makes use of the Trace.Correlation and is particularly handy in the context of logging in WF.
At each of these invoke activities we log the start (using StartLogicalOperation) and then at the end we stop the logical operation with a gereric return event handler.
What log outputs do you use?
The text files are rolling logs that are rolled by day and size (I believe total size of 1MB is a rollover point).
What tools to you use for viewing the logs?
The logs are sent to a single directory which is then split into sub-dirs based on the source service.
This allows us to take a look at production logs without having to put in requests and go through lengthy red tape processes for production data.
As the authors of the tool, we of course use [SmartInspect (hyper-link)] for logging and tracing .NET applications.
We usually use the named pipe protocol for live logging and (encrypted) binary log files for end-user logs.
There are actually quite a few logging frameworks and tools for .NET out there.
There's an overview and comparison of the different tools on [DotNetLogging.com (hyper-link)].
It provides powerful, flexible, high performance logging for applications, however many developers are not aware of its capabilities and do not make full use of them.
There are some areas where additional functionality is useful, or sometimes the functionality exists but is not well documented, however this does not mean that the entire logging framework (which is designed to be extensible) should be thrown away and completely replaced like some popular alternatives (NLog, log4net, Common.Logging, and even EntLib Logging).
Rather than change the way you add logging statements to your application and re-inventing the wheel, just extended the System.Diagnostics framework in the few places you need it.
It seems to me the other frameworks, even EntLib, simply suffer from Not Invented Here Syndrome, and I think they have wasted time re-inventing the basics that already work perfectly well in System.Diagnostics (such as how you write log statements), rather than filling in the few gaps that exist.
This means no expensive calls to ToString() on parameter values until after the system has confirmed message will actually be logged.
The Trace.CorrelationManager allows you to correlate log statements about the same logical operation (see below).
VisualBasic.Logging.FileLogTraceListener is good for writing to log files and supports file rotation.
When using EventLogTraceListener if you call TraceEvent with multiple arguments and with empty or null format string, then the args are passed directly to the EventLog.WriteEntry() if you are using localized message resources.
The Service Trace Viewer tool (from WCF) is useful for viewing graphs of activity correlated log files (even if you aren't using WCF).
It is easy to track back to the specific code that logs/uses the event ids, and can make it easy to provide guidance for common errors, e.g.
A: Trace.CorrelationManager is very useful for correlating log statements in any sort of multi-threaded environment (which is pretty much anything these days).
You need at least to set the ActivityId once for each logical operation in order to correlate.
Start/Stop and the LogicalOperationStack can then be used for simple stack-based context.
LogicalOperationScope, that (a) sets up the context when created and (b) resets the context when disposed.
On creation the scope could first set ActivityId if needed, call StartLogicalOperation and then log a TraceEventType.Start message.
On Dispose it could log a Stop message, and then call StopLogicalOperation.
Whilst you probably want to consistently log all Warning & above, or all Information & above messages, for any reasonably sized system the volume of Activity Tracing (Start, Stop, etc) and Verbose logging simply becomes too much.
This way, you can locate significant problems from the usually logging (all warnings, errors, etc), and then "zoom in" on the sections you want and set them to Activity Tracing or even Debug levels.
You might also want to consider separate trace sources for Activity Tracing vs general (other) logging, as it can make it a bit easier to configure filters exactly how you want them.
Q: What log outputs do you use?
This can depend on what type of application you are writing, and what things are being logged.
(1) Events - Windows Event Log (and trace files)
If writing a server/service, then best practice on Windows is to use the Windows Event Log (you don't have a UI to report to).
In this case all Fatal, Error, Warning and (service-level) Information events should go to the Windows Event Log.
The Information level should be reserved for these type of high level events, the ones that you want to go in the event log, e.g.
"Service Started", "Service Stopped", "Connected to Xyz", and maybe even "Schedule Initiated", "User Logged On", etc.
In some cases you may want to make writing to the event log a built-in part of your application and not via the trace system (i.e.
write Event Log entries directly).
In contrast, a Windows GUI application would generally report these to the user (although they may also log to the Windows Event Log).
number of errors/sec), and it can be important to co-ordinate any direct writing to the Event Log, performance counters, writing to the trace system and reporting to the user so they occur at the same time.
If a user sees an error message at a particular time, you should be able to find the same error message in the Windows Event Log, and then the same event with the same timestamp in the trace log (along with other trace details).
(2) Activities - Application Log files or database table (and trace files)
Also, it is very common to use a specific Application Log (sometimes called an Audit Log).
Usually this is a database table or an application log file and contains structured data (i.e.
A good example might be a web server which writes each request to a web log; similar examples might be a messaging system or calculation system where each operation is logged along with application-specific details.
In these systems you are probably already logging the activity as they have important business value, however the principal of correlating them to other actions is still important.
As well as custom application logs, activities also often have related peformance counters, e.g.
In generally you should co-ordinate logging of activities across different systems, i.e.
write to your application log at the same time as you increase your performance counter and log to your trace system.
You don't want this stuff cluttering up the Windows Event Log.
Sometimes a database is used, but more likely are rolling log files that are purged after a certain time.
A big difference between this information and an Application Log file is that it is unstructured.
Whilst an Application Log may have fields for To, From, Amount, etc., Verbose debug traces may be whatever a programmer puts in, e.g.
One important practice is to make sure things you put in application log files or the Windows Event Log also get logged to the trace system with the same details (e.g.
This allows you to then correlate the different logs when investigating.
If you are planning to use a particular log viewer because you have complex correlation, e.g.
Provided you can correlated back to more structured logs at higher levels, things should be okay.
Q: If using files, do you use rolling logs or just a single file?
How do you make the logs available for people to consume?
A: For files, generally you want rolling log files from a manageability point of view (with System.Diagnostics simply use VisualBasic.Logging.FileLogTraceListener).
(Windows Event Log or Database Application Logs would have their own access mechanisms).
One interesting solution I saw for a Windows GUI application was that it logged very detailed tracing information to a "flight recorder" whilst running and then when you shut it down if it had no problems then it simply deleted the file.
Q: What tools to you use for viewing the logs?
A: If you have multiple logs for different reasons then you will use multiple viewers.
Notepad/vi/Notepad++ or any other text editor is the basic for plain text logs.
As I generally log high level information to the Windows Event Log, then it provides a quick way to get an overview, in a structured manner (look for the pretty error/warning icons).
You only need to start hunting through text files if there is not enough in the log, although at least the log gives you a starting point.
(At this point, making sure your logs have co-ordinated entires becomes useful).
Generally the Windows Event Log also makes these significant events available to monitoring tools like MOM or OpenView.
If you log to a Database it can be easy to filter and sort informatio (e.g.
When running a service in debug/test I usually host it in a console application for simplicity I find a colored console logger useful (e.g.
Note that the framework does not include a colored console logger or a database logger so, right now, you would need to write these if you need them (it's not too hard).
It really annoys me that several frameworks (log4net, EntLib, etc) have wasted time re-inventing the wheel and re-implemented basic logging, filtering, and logging to text files, the Windows Event Log, and XML files, each in their own different way (log statements are different in each); each has then implemented their own version of, for example, a database logger, when most of that already existed and all that was needed was a couple more trace listeners for System.Diagnostics.
For a professional application, especially a server/service, I expect to see it fully instrumented with both Performance Monitor counters and logging to the Windows Event Log.
You need to make sure you include installers for the performance counters and event logs that you use; these should be created at installation time (when installing as administrator).
When your application is running normally it should not need have administration privileges (and so won't be able to create missing logs).
If writing to the Event Log, .NET will automatically create a missing log the first time you write to it; if you develop as a non-admin you will catch this early and avoid a nasty surprise when a customer installs your system and then can't use it because they aren't running as administrator.
As far as aspect oriented logging is concerned I was recommended PostSharp on another SO question -
[Aspect Oriented Logging with Unity\T4\anything else (hyper-link)]
The link provided in the answer is worth visiting if you are evaluating logging frameworks.
A general best practice is to consider who will be reading the log.
So I log messages that gives them something they can act on.
You can use git log with the pathnames of the respective folders:
git log A B
The log will only show commits made in A and B. I usually throw in --stat to make things a little prettier, which helps for quick commit reviews.
git log -p DIR is very useful, if you need the full diff of all changed files in a specific subdirectory.
You are calling Logger.myLogger() more than once.
Store the logger instance it returns somewhere and reuse that.
Also be advised that if you log before any handler is added, a default StreamHandler(sys.stderr) will be created.
Your logger should work as singleton.
The logging.getLogger() is returns the same instance for a given name.
The problem is that every time you call myLogger(), it's adding another handler to the instance, which causes the duplicate logs.
The implementation of logger is already a singleton.
Multiple calls to logging.getLogger('someLogger') return a reference
  to the same logger object.
It is true for references to the same object;
  additionally, application code can define and configure a parent
  logger in one module and create (but not configure) a child logger in
  a separate module, and all logger calls to the child will pass up to
  the parent.
Source- [Using logging in multiple modules (hyper-link)]
Let's suppose we have created and configured a logger called 'main_logger' in the main module (which simply configures the logger, doesn't return anything).
Now in a sub-module, if we create a child logger following the naming hierarchy 'main_logger.sub_module_logger', we don't need to configure it in the sub-module.
Just creation of the logger following the naming hierarchy is sufficient.
Double (or triple or ..- based on number of reloads) logger output may also happen when you reload your module via importlib.reload (for the same reason as explained in accepted answer).
You are able to get list of all handlers for the particular logger, so you can do something like this
In the example above we check if the handler for a file specified is already hooked to the logger, but having access to the list of all handlers gives you an ability to decide on which criteria you should add another handler or not.
This is pretty convenient when debugging and the code includes your logger initialization
Meaning, if you call addHandler() within the code of your function, it will continue to add duplicate handlers to the logging singleton every time the function runs.
The logging singleton persists through multiple calls of you lambda function.
Bottom line for most cases when this happens, one only needs to call logger.getLogger() only once per module.
Both then will have their own full package name and method where logged.
I already used the logger as a Singleton and checked if not len(logger.handlers), but still got duplicates: It was the formatted output, followed by the unformatted.
Solution in my case:
logger.propagate = False
I have 3 handlers in one logger
resulting duplicated lines and duplicated handlers like this, 2 Stream Handlers, 3 Rotating FileHanders
While 1 Stream Handler + 2 Rotating FileHanders(1 for errlog, 1 for generic log)
This is done by
in every modules, issue resolved, no duplicated lines, 1 StreamHeader, 1 FileHandler for err logging, 1 RotatingFileHandler for generic logging
The details is in this document
[https://docs.python.org/3/library/logging.html (hyper-link)]
Note that Loggers should NEVER be instantiated directly, but always through the module-level function logging.getLogger(name).
Multiple calls to getLogger() with the same name will always return a reference to the same Logger object."
Loggers that are further down in the hierarchical list are children of loggers higher up in the list.
For example, given a logger with a name of foo,
loggers with names of
The logger name hierarchy is analogous to the Python package hierarchy, and identical to it if you organise
your loggers on a per-module basis using the recommended construction
When we use getLogger() without any argument, it returns RootLogger.
So if you invoke getLogger() at multiple places and add log handler, it will add those log handlers to the RootLogger (If you don't add log handler explicitly, it will add StreamHandler automatically).
Thus, when you will try to log messages, it will log messages using all the handlers added to RootLogger.
This is the cause of duplicate logs.
You can avoid this by just providing a different logger name when you invoke getLogger().
[error_log (hyper-link)] — Send an error message somewhere
You can [customize error handling with your own error handlers (hyper-link)] to call this function for you whenever an error or warning or whatever you need to log occurs.
Take a look at the [log_errors (hyper-link)] configuration option in php.ini.
I think you can use the [error_log (hyper-link)] option to set your own logging file too.
When the log_errors directive is set to On, any errors reported by PHP would be logged to the server log or the file specified with error_log.
Or update php.ini as described in [this blog entry (hyper-link)] from 2008.
You can just reverse your log and just head it for the first result.
git log --format="%h" | tail -1 gives you the commit hash (ie 0dd89fb), which you can feed into other commands, by doing something like
git diff `git log --format="%h" --after="1 day"| tail -1`..HEAD to view all the commits in the last day.
To see the full git log, with commit message, for just the first commit:
How I learned the first command above: [the main answer] [How to show first commit by 'git log'?
git shortlog is one way.
Git shortlog is one way to get the commit details:
Use git shortlog just like this
git shortlog -sn
# show contributors by commits
alias gcall="git shortlog -sn"
git shortlog by itself does not address the original question of total number of commits (not grouped by author)
However, with Git 2.29 (Q4 2020), "[git shortlog (hyper-link)]"([man (hyper-link)]) has become more precise.
Now that shortlog supports reading from trailers, it can be useful to combine counts from multiple trailers, or between trailers and authors.
This patch teaches shortlog to accept multiple --group options on the command line, and pull data from all of them.
to get a shortlog that counts authors and co-authors equally.
The caller in [builtin/log.c (hyper-link)] needs to be adapted to ask explicitly for authors, rather than relying on shortlog_init().
git shortlog now includes in its [man page (hyper-link)]:
git shortlog now also includes in its [man page (hyper-link)]:
For
example, git shortlog --group=author --group=trailer:co-authored-by
counts both authors and co-authors.
Print the total logs number grouped by author (git shortlog -s -n)
Example output
[code snippet]
This may apply to you if you apache log has following note:
You can create a Logger object yourself from inside any model.
Just pass the file name to the constructor and use the object like the usual Rails logger:
Here I used a class attribute to memoize the logger.
Remember also that you can inject the my_logger method directly into the ActiveRecord::Base class (or into some superclass of your own if you don't like to monkey patch too much) to share the code between your app's models.
A decent option that works for me is to just add a fairly plain class to your app/models folder such as app/models/my_log.rb
anywhere you could do Post.create(:title => "Hello world", :contents => "Lorum ipsum"); or something similar you can log to your custom file like this
I made a gem based on the solution below, called [multi_logger (hyper-link)].
The benefit is that you can setup formatter to prefix timestamps or severity to the logs automatically.
I would suggest using [Log4r gem (hyper-link)] for custom logging.
Log4r is a comprehensive and flexible logging library written in Ruby for use 
  in Ruby programs.
It features a hierarchical logging system of any number of 
  levels, custom level names, logger inheritance, multiple output destinations 
  per log event, execution tracing, custom formatting, thread safteyness, XML 
  and YAML configuration, and more.
The Logging framework, with its deceptively simple name, has the sophistication you crave!
Follow the very short instructions of [logging-rails (hyper-link)] to get started filtering out noise, getting alerts, and choosing output in a fine-grained and high-level way.
Log-rolling, daily.
Here is my custom logger:
Define a logger class in (say) app/models/special_log.rb:
initialize the logger in (say) config/initializers/special_log.rb:
Anywhere in your app, you can log with:
or, to tail the logs live
[Heroku log documentation (hyper-link)]
If you need more than a few thousand lines you can Use heroku's [Syslog Drains (hyper-link)]
Logging has greatly improved in heroku!
references: [http://devcenter.heroku.com/articles/logging (hyper-link)]
Heroku treats logs as time-ordered streams of events.
Accessing *.log files on the filesystem is not recommended in such an environment for a variety of reasons.
First, if your app has more than one dyno then each log file only represents a partial view into the events of your app.
Second, the filesystem on Heroku is ephemeral meaning whenever your dyno is restarted or moved (which happens about [once a day (hyper-link)])the log files are lost.
So you only get at most a day's view into that single dyno's logs.
As such, you won't find the log files for your other dynos that are running the actual http processes on the one spawned for heroku run.
[Logging (hyper-link)], and visibility in general, is a first-class citizen on Heroku and there are several tools  that address these issues.
First, to see a real-time stream of application events across all dynos and all layers of the application/stack use the heroku logs -t command to tail output to your terminal.
If you want to store the logs for longer periods of time you can use one of the many [logging add-ons (hyper-link)] that provide log retention, alerting and triggers.
Lastly, if you want to store the log files yourself you can setup your own [syslog drain (hyper-link)] to receive the stream of events from Heroku and post-process/analyze yourself.
Summary: Don't use heroku console or heroku run bash to view static log files.
Pipe into Heroku's stream of log events for your app using heroku logs or a logging add-on.
heroku logs -t shows us the live logs.
Zero configuration, and you get 7 days worth of logging data up to 10MB/day, and can search back through 2 days of logs.
E.g tail only your application logs
Or see only the router logs
Follow on [Heroku logging (hyper-link)]
To view your logs we have:
logs command retrives 100 log lines by default.
heroku logs
heroku logs -n 200
Show logs in real time
heroku logs --tail
heroku logs --app your_app_name
Whereas if you want to check from GUI so you have to log into your Heroku account and then select your application and finally click on view logs [ (hyper-link)]
To see the detailed log you need to put two lines in the production.rb file:
you can see the detailed logs.
You can leave the script running in background and you can simply filter the logs from the text file the way you want anytime.
You can access your log files using Heroku's Command Line
Interface ([CLI Usage (hyper-link)]).
You can also access the logs in a real-time stream using:
If the logs tell you something like this:
A complete log of this run can be found in:
/app/.npm/_logs/2017-07-11T08_29_45_291Z-debug.log
My solution is to get complete log the first time the application start, like:
for continuous logging, just iterate it using watch for every x minutes (or seconds).
I suggest using an addon, I use Logentries.
heroku addons:create logentries:le_tryit
Logentries allows you to save up to 5GB of log volume per month.
So to answer your question, by using this addon you ensure that your logs aren't lost anymore when you reach the 1500 lines that Heroku saves by default.
heroku logs -t shows us the live logs.
heroku logs -n 1500 for  specific number of logs
You can use 
heroku logs -n 1500
I would suggest you plug some logging tool.
( sumoLogic, paper trail n all ) as an add-on
They all have a free version( with few limitations, though  enough for a small app or dev env, which  will provide good insight and tool   to analyze logs )
You need to have some logs draining implemented and should be draining your logs there, to see all of the logs (manage historical logs as well):
First option - Splunk can be used: you can drain all your logs like:
And then login into your splunk server and search for any number of logs.
Timber.io
Sumo Logic
LogEnteries
Log DNA
Papertrail
You can also have a look at below options: If you want to have your
  logs in JSON format, as it will help if your are pushing your logs to
  external system like Splunk/ELK, it would become easy (performance
  wise also) to search in JSON.
[https://github.com/goodeggs/heroku-log-normalizer (hyper-link)]
The following command will tail the generating logs on heroku
The following comand will show the 1000 number of lines of logs from heroku
Note only 1500 latest lines of logs are available and rest of them gets deleted from heroku dyno.
More > View logs
The output of console.log(anObject) is misleading; the state of the object displayed is only resolved when you expand the Object tree displayed in the console, by clicking on >.
It is not the state of the object when you console.log'd the object.
Instead, try console.log(Object.keys(config)), or even console.log(JSON.stringify(config)) and you will see the keys, or the state of the object at the time you called console.log.
You will (usually) find the keys are being added after your console.log call.
console.log(data) showed the object as expected
console.log(Object.keys(data)) said ["constants","i18n"] as expected
Nothing helped... Then on the server side I wrote the data to the php log, and it revealed this:
This was not visible in my php editor or the browser console log.
Only the php log revealed this... That was a tricky one...
Console.log works because it executes after a small delay, but that isn't the case for the rest of your code.
When running console.log() on the whole object, all the document fields (as stored in the db) would show up.
Turned out that property accessors only works for those fields specified in my mongoose.Schema(...) definition, whereas console.log() and JSON.stringify() returns all fields stored in the db.
My initial logging made no sense:
The [i] is a little icon, when I hovered over it it said Object value at left was snapshotted when logged, value below was evaluated just now.
This might help somebody as I had a similar issue in which the JSON.parse() was returning an object that I could print on the console.log() but I couldn't acccess the specific fields and none of the above solution worked for me.
I quote "Logging Objects":
Don't use console.log(obj);,
  use console.log(JSON.parse(JSON.stringify(obj)));.
This way you are sure you are seeing the value of obj at the moment
  you log it.
Which is y in console.log(data) i get the content.
I was able to access all properties except the first property - but it would show up ok if I wrote the whole object using console.log.
So it looked exactly the same when you printed it to the log however whenever I tried to access the properties it gave me an undefined error.
In your fpm.conf file you haven't set 2 variable which are only for error logging.
The variables are error_log (file path of your error log file) and log_level (error logging level).
That will allow PHPs stderr to go to php-fpm's error log instead of /dev/null.
It still wasn't writing the log file so I actually had to create it by touch /var/log/fpm-php.www.log then setting the correct owner sudo chown www-data:www-data /var/log/fpm-php.www.log.
Once this was done, and php5-fpm restarted, logging was resumed.
I struggled with this for a long time before finding my php-fpm logs were being written to /var/log/upstart/php5-fpm.log.
So, if you setup nginx with php5-fpm and log a message using error_log() you can see it in /var/log/nginx/error.log by default.
A problem can arise if you want to log a lot of data (say an array) using error_log(print_r($myArr, true));.
If an array is large enough, it seems that nginx will truncate your log entry.
To get around this you can configure fpm ([php.net fpm config (hyper-link)]) to manage logs.
Uncomment the following two lines by removing ; at the beginning of the line: (error_log is defined here: [php.net (hyper-link)])
;php_admin_value[error_log] = /var/log/fpm-php.www.log
;php_admin_flag[log_errors] = on
Create /var/log/fpm-php.www.log:
$ sudo touch /var/log/fpm-php.www.log;
Change ownership of /var/log/fpm-php.www.log so that php5-fpm can edit it:
$ sudo chown vagrant /var/log/fpm-php.www.log

Note: vagrant is the user that I need to give ownership to.
Now your logs will be in /var/log/fpm-php.www.log.
in my case I show that the error log was going to /var/log/php-fpm/www-error.log .
Now I can see logs in the file specified by nginx.
In my case php-fpm outputs 500 error without any logging because of missing php-mysql module.
I started with trying to fix broken logging without success.
To search the commit log (across all branches) for the given text:
Finally, as a last resort in case your commit is dangling and not connected to history at all, you can search the reflog itself with the -g flag (short for --walk-reflogs:
EDIT: if you seem to have lost your history, check the reflog as your safety net.
The [git-ready reflog (hyper-link)]  article may be of help.
To recover your commit from the reflog: do a git checkout of the commit you found (and optionally make a new branch or tag of it for reference)
git reflog
For anyone who wants to pass in arbitrary strings which are exact matches (And not worry about escaping regex special characters), [git log takes a --fixed-strings option (hyper-link)]
Just a small addition to the git log -all --grep command:
In my case I had to escape the brackets inside the message:
You can use the Elmah.ErrorSignal() method to log an issue without raising an exception.
Direct log writing method, working since ELMAH 1.0:
Log method does not.
Raise is subscription based and is able to log one exception into the several loggers.
Then just call it whenever you need to log an error.
If you have legacy error logging you want to retain (I just happen to have a simple error logging mechanism that's tied into some UIs that I dont immediately have time to remove).
Elmah automatically unwraps exceptions so the underlying exception will still be reported in the log but the contextualMessage will be visible when you click on it.
I was trying to write custom messages into elmah logs using Signal.FromCurrentContext().Raise(ex); and found that these exceptions are bubbled up, eg:
Besides I don't see how elmah supports different levels of logging - is it possible to switch off verbose logging by a web.config setting?
To manually log errors with HttpContext (in controller) simply write:
You can using the EventLog class, as explained on [How to: Write to the Application Event Log (Visual C#) (hyper-link)]:
Use WriteEvent and WriteEntry to write events to an event log.
This is the logger class that I use.
The private Log() method has EventLog.WriteEntry() in it, which is how you actually write to the event log.
In addition to logging, this class will also make sure the message isn't too long to write to the event log (it will truncate the message).
Yes, there is a way to write to the event log you are looking for.
You don't need to create a new source, just simply use the existent one, which often has the same name as the EventLog's name and also, in some cases like the event log Application, can be accessible without administrative privileges*.
*Other cases, where you cannot access it directly, are the Security EventLog, for example, which is only accessed by the operating system.
I used this code to write directly to the event log Application:
As you can see, the EventLog source is the same as the EventLog's name.
Each log in the Eventlog key contains subkeys called event sources.
The event source is the name of the software that logs the event.
[https://msdn.microsoft.com/en-us/library/system.diagnostics.eventlog(v=vs.110).aspx (hyper-link)] ), checking an non existing source and creating a source requires admin privilege.
In my test under Windows 2012 Server r2, I however get the following log entry using "Application" source:
It might be possible to use the EventLogPermission class instead of this try/catch but not sure we can avoid the catch.
Then, using 'MyApp' in the method above will NOT generate exception and the EventLog can be created with that source.
When you open a terminal, the terminal starts bash in (non-login) interactive mode, which means it will source ~/.bashrc.
~/.bash_profile is only sourced by bash when started in interactive login mode.
That is typically only when you login at the console (Ctrl+Alt+F1..F6), or connecting via ssh.
or if wanna add logs via functions
The -l parameter tells bash to run as a login shell, this is required because .bash_profile will not run as a non-login shell, for more info about this [read here (hyper-link)]
You can configure rails, to use the systems log tools.
That way, you log to syslog, and can use default logrotate tools to rotate the logs.
Another option is to simply configure logrotate to pick up the logs left by rails.
On Ubuntu and Debian that would be, for example, in a file called /etc/logrotate.d/rails_example_com.
If you are using logrotate then you can choose either of the options shown below by placing a conf file in the /etc/logrotate.d/ directory.
Please note that copytruncate makes a backup copy of the current log and then clears the log file for continued writing.
The alternative is to use create which will perform the rotation by renaming the current file and then creating a new log file with the same name as the old file.
The reason why is that Rails may still keep pointing to the old log file even though its name has changed and they may require restarting to locate the new log file.
Enable to send logs to the loggly using rails logglier as following in my environments/production.rb file.
For Rails 5, this is what I had to do to limit log size and don't change server output in the console:
[According to the documentation (hyper-link)], if you want to limit the size of the log folder, put this in your environment-file ('development.rb'/'production.rb').
With this, your log files will never grow bigger than 50Mb.
The ‘1’ in the second parameter means that 1 historic log file will be kept, so you’ll have up to 100Mb of logs – the current log and the previous chunk of 50Mb.
For every log: Rails log, Rpush log, ...
You can use like this in your config file of service:
It means: only save 1 previous log file after split.
Main log size never over 20 MB.
For Rails 5, if you want daily log rotation, you only need this:
Simply log everything with one line format and tail the output:
Simply log everything reverse -1 means list one log
How the specify the commit index and log it.
By (no branch), you might be asking about the reflog rather than any given ancestry chain.
git log -g --pretty=oneline | tail -10
-g is --walk-reflogs Instead of walking the commit ancestry chain, walk reflog entries.q
add |cut -d ' ' -f 2|tr -d ':' > log to log only the reflog commit index.
git log --reverse --pretty=oneline | head -10 | cat -n
In case someone wants more than just git one-line log:
As [here (hyper-link)] points out git log --reverse -n 10 doesn't work as you need it to.
To fix it in my build of Chrome 60.0.3112.90 (Official Build) (64-bit) I opened the DevTools then navigated to the DevTools Settings then ticked 'Log XMLHttpRequests', unticked 'User messages only' and 'Hide network messages'
Chrome started logging this message when I changed network speed configuration to 3G fast/ 3G slow and again back to Online.
Fix:
When I tried selecting Offline mode and again Online mode, the logging issue disappeared.
The solution was to give the "Network Service" account read permission on the EventLog/Security key.
For me ony granting 'Read' permissions for 'NetworkService' to the whole 'EventLog' branch worked.
To give Network Service read permission on the EventLog/Security key (as suggested by Firenzi and royrules22) follow instructions from [http://geekswithblogs.net/timh/archive/2005/10/05/56029.aspx (hyper-link)]
Navigate/expand to the following key:
HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Eventlog\Security
However if you deploy your application to other machine(s), consider to register event log sources during installation as suggested in [SailAvid's (hyper-link)] and [Nicole Calinoiu's (hyper-link)] answers.
64 bit machines are using new even log - xml base I would say and these characters (set from string) create invalid xml which causes exception.
I ran into the same issue, but I had to go up one level and give full access to everyone to the HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\EventLog\ key, instead of going down to security, that cleared up the issue for me.
This exception was occurring for me from a .NET console app running as a scheduled task, and I was trying to do basically the same thing - create a new Event Source and write to the event log.
1) Goto your registry, locate: HKLM\System\CurrentControlSet\Services\EventLog\Application(???YOUR_SERVICE_OR_APP_NAME???)
is your application service name as you defined it when you created your .NET deployment, for example, if you named your new application "My new App" then the key would be: HKLM\System\CurrentControlSet\Services\EventLog\Application\My New app
Note2: Depending on which eventLog you are writing into, you may find on your DEV box, \Application\ (as noted above), or also (\System) or (\Security) depending on what event your application is writing into, mostly, (\Application) should be fine all the times.
When developing an application that writes anything into the EventLog, it would require a KEY for it under the Eventlog registry if this key isn't found, it would try to create it, which then fails for having no permissions to do so.
I had a very similar problem with a console program I develop under VS2010 (upgraded from VS2008 under XP)
My prog uses EnLib to do some logging.
(you may also refer to [http://www.blackwasp.co.uk/EventLog_3.aspx (hyper-link)], it helped me
The security log stopped working altogether because of a GPO that took the group Authenticated Users and read permission away from the key HKLM\System\CurrentControlSet\Services\EventLog\security
make sure this identity can read KEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\Eventlog (rigth-click, authorisations)
Credentials will be reloaded and EventLog reacheable
in [http://geekswithblogs.net/timh/archive/2005/10/05/56029.aspx (hyper-link)] , thanks Michael Freidgeim
The problem is that the EventLog.SourceExists tries to access the EventLog\Security key, access which is only permitted for an administrator.
A common example for a C# Program logging into EventLog is:
However, the following lines fail if the program hasn't administrator permissions and the key is not found under EventLog\Application as EventLog.SourceExists will then try to access EventLog\Security.
HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\EventLog\Application\dotNET Sample App
I also added a try/catch around log.source = "xx" in the app to set it to a known source if my event source wasn't created (This would only come up if I hot swapped a .dll instead of re-installing).
A simple answer is to create the log and the event source using the PowerShell command New-EventLog ([http://technet.microsoft.com/en-us/library/hh849768.aspx (hyper-link)])
Run PowerShell as an Administrator and run the following command changing out the log name and source that you need.
New-EventLog -LogName Application -Source TFSAggregator
I used it to solve the [Event Log Exception when Aggregator runs (hyper-link)] issue from codeplex.
A new key with source name used need to be created under HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\services\eventlog\Application in the regEdit when you use System.Diagnostics.EventLog.WriteEntry("SourceName", "ErrorMessage", EventLogEntryType.Error);
Run RegEdit and go to HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\services\eventlog
Right click in EventLog key and the select Permissions... option
3.Add your user with full Control access.
Repeat the steps from 1 to 3 for HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\services\eventlog\Security
Main disadvantages are inability to group by that event, and that you probably don't have an associated Event ID, which means the log entry may very well be prefixed with something to the effect of "The description for Event ID 0 from source .Net Runtime cannot be found...." if you omit it, but the log goes in, and the output looks broadly sensible.
git log
The others are (from git help log):
After a long time looking for a way to get git log output the date in the format YYYY-MM-DD in a way that would work in less, I came up with the following format:
In addition to --date=(relative|local|default|iso|iso-strict|rfc|short|raw), as others have mentioned, you can also use a custom log date format with
and your "git log" commands will show the human-legible format unless
you're scripting things.
With Git 2.32 (Q2 2021), "[git log --format=... (hyper-link)]"([man (hyper-link)]) placeholders learned %ah/%ch placeholders to request the --date=human output.
Change file type to "Log"
backup log logname with truncate_only followed by a dbcc shrinkfile command
This reduced my 14GB log file down to 1MB.
For SQL 2008 you can backup log to nul device:
And then use DBCC SHRINKFILE to truncate the log file.
Then simply delete the log file, or rename it and delete later.
In the attach window remove the log file from list of files.
The DB attaches and creates a new empty log file.
After you check everything is all right, you can delete the renamed log file.
I usually use scripts when interacting with the shell, so I wrote a basic Logging object that I "load(script)" into the shell, and then use the Logging object to call logging levels (debug,info,warn,error).
The Logger object does use 'print' and 'printjson' at it's core.
Configurable logging makes this a little better, but I really miss being able to step through the code.
EXPLANATION
You can check the logcat below carefully then I think you may know why this solution will work
A simple way of implementing that is checking whether user is logged in before going to the fragment.
Note that this will result in the color codes being written to the logfile as well, making it less readable.
I'm working in a Busybox environment without access to bash, and it does not understand the exec > >(tee log.txt) syntax.
This makes output from script go from the process, through the pipe into the sub background process of 'tee' that logs everything to disc and to original stdout of the script.
will not output bar to the terminal, only to the logfile, and
This can be
fixed by using two separate tee processes both appending to the same
log file:
(Note that the above does not initially truncate the log file - if you want that behaviour you should add
not even line-buffered, so in this case it is possible that STDOUT and STDERR could end up on the same line of foo.log; however that could also happen on the terminal, so the log file will be a faithful reflection of what could be seen on the terminal, if not an exact mirror of it.
If you want the STDOUT lines cleanly separated from the STDERR lines, consider using two log files, possibly with date stamp prefixes on each line to allow chronological reassembly later on.
Easy way to make a bash script log to syslog.
The script output is available both through /var/log/syslog and through stderr.
syslog will add useful metadata, including timestamps.
Alternatively, send the log to a separate file:
make tee=false the default instead, make TEE hold the log file instead, etc.
Now last 100 lines will be present in newLogfile
Retrieve last 100 lines logs
less +F my_log_file.log
once you start seeing logs you can do search, go to line number, search for pattern, much more plus it is faster for large files.
its like vim for logs[totally my opinion]
e.g tail -100 test.log
will fetch the last 100 lines from test.log
e.g tail -100 test.log > output.log
will fetch the last 100 lines from test.log and store them into a new file output.log)
You can [docker inspect (hyper-link)] each container to see where their logs are:
And, in case you were trying to figure out where the logs were to manage their collective size, or adjust parameters of the logging itself you will find the following relevant.
Fixing the amount of space reserved for the logs
This is taken from [Request for the ability to clear log history (issue 1083) (hyper-link)]):
Docker 1.8 and docker-compose 1.4 there is already exists a method to limit log size using [docker compose log driver (hyper-link)] and log-opt max-size:
Possible issue with docker-compose logs not terminating
[issue 1866 (hyper-link)]: command logs doesn't exit if the container is already stopped
A container's logs can be found in :
(if you use the default log format which is json)
To directly view the logfile in less, I use:
run as ./viewLogs.sh CONTAINERNAME
To see how much space each container's log is taking up, use this:
As of 8/22/2018, the logs can be found in :
On Windows, the default location is: C:\ProgramData\Docker\containers\<container-id>-json.log.
To see the size of logs per container, you can use this bash command :
Location of container logs can be found in
DOCKER_ARTIFACTS\containers\[Your_container_ID]\[Your_container_ID]-json.log
There is no log file.
By default it will log errors to STDERR and output to STDOUT.
You can change that when you run it from your shell to log to a file instead.
Alternatively (recommended), you can add logging inside your application either manually or with one of the many log libraries:
[log4js (hyper-link)]
It will run your .js-File 24/7 with logging options.
[Long Running Process]
  The forever process will continue to run outputting log messages to the console.
forever -o out.log -e err.log my-script.js
forever start -l forever.log -o out.log -e err.log my-daemon.js
  forever stop my-daemon.js
For nodejs log file you can use winston and morgan and in place of your console.log() statement user winston.log() or other winston methods to log.
After copying the above code make make a folder with name logs parallel to winston or wherever you want and create a file app.log in that logs folder.
Go back to config.js and set the path in the 5th line "filename: ${appRoot}/logs/app.log,
" to the respective app.log created by you.
If you don't want to integrate a framework like [Zend (hyper-link)], then you can use the [trigger_error (hyper-link)] method to log to the php error log.
You can use [error_log (hyper-link)] to send to your servers error log file (or an optional other file if you'd like)
Write to a log file
A lesser known trick is that mod_php maps stderr to the Apache log.
And, there is a stream for that, so file_put_contents('php://stderr', print_r($foo, TRUE)) will nicely dump the value of $foo into the Apache error log.
or if you want to print that statement in an log you can use
This a great tool  for debugging & logging php: [PHp Debugger & Logger (hyper-link)]
Since I am constantly looking at the developer tools for debugger, CSS layout, etc, it makes sense to look at my PHP loggon there.
function debug($name, $var = null, $type = LOG) {
O(n log n): quick or mergesort (On average)
O(log n): Binary search
A typical example of O(N log N) would be sorting an input array with a good algorithm (e.g.
A typical example if O(log N) would be looking up a value in a sorted input array by bisection.
O (n log n) is famously the upper bound on how fast you can sort an arbitrary set (assuming a standard and not highly parallel computing model).
O(logn) - finding something in your telephone book.
O(nlogn) - cant immediately think of something one might do everyday that is nlogn...unless you sort cards by doing merge or quick sort!
O(logN) - computing x^N,
O(N Log N) - Longest increasing subsequence
The factor of 'log n' is introduced by bringing into consideration Divide and Conquer.
These ones are supposed to be the less efficient algorithms if their O(nlogn) counterparts are present.
0(logn)-Binary search, peak element in an array(there can be more than one peak)
0(1)-in python calculating the length of a list or a string.
0(nlogn)-Merge sort.
sorting in python takes nlogn time.
so when you use listname.sort() it takes nlogn time.
Try downloading a Logcat app(eg aLogCat) on your device itself and see if you are having the same problem.
I know its not a complete solution, but this way you may be able to figure out whats going wrong by reading the application logs.
Follow these instructions (from denispyr's answer on  [Why doesn't logcat show anything in my Android?
Go to the Project Menu > Background Setting > Log setting and define the log availability (log switch) and level (log level setting).
Select "Enable All Logs"
It led to some MTKLogger settings screen, which is different than described in the answer, and didn't help at all.
In the first Telephony tab, click the Log Control item
Activate both switches (set Mtklog control to catch all log and adb radio log to catch radio log by adb)
Back in the previous screen, swipe to the Log and Debugging tab
Changing Log control
Just to clarify my original problem; logcat was showing some of my app's log entries, just not my own messages - I saw entries belonging to my process com.example.myapp, but never anything I was printing out myself, in my code, using Log.d("TAG", "hello world");
and selecting the option AP Log will be enough to display the messages in the LogCat.
Background Settings -> AP LOG Settings -> Open.
If you still can't see the logs, restart both your phone and Android Studio/Eclipse
No log output, neither in syslog nor in kern.log.
A tool like systemtap (or a tracer) can monitor kernel signal-transmission logic and report.
Typically in /var/log directory.
Either /var/log/kern.log or /var/log/dmesg
See the logs and google :)
The system logs did not mention any killing so I looked further and it turned out that the worker was basically killing itself because of a job that exceeded the memory limit (which is set to 128M by default).
Now the print statement will both echo to the screen and append to your log file:
You probably ought to parametize the log filename.
You should probably revert sys.stdout to <stdout> if you
won't be logging for the duration of the program.
You may want the ability to write to multiple log files at once, or handle different log levels, etc.
What you really want is logging module from standard library.
Create a logger and attach two handlers, one would be writing to a file and the other to stdout or stderr.
See  [Logging to multiple destinations (hyper-link)] for details
Here is a sample program that makes uses the [python logging module (hyper-link)].
This logging module has been in all versions since 2.3.
In this sample the logging is configurable by command line options.
In quite mode it will only log to a file, in normal mode it will log to both a file and the console.
As described elsewhere, perhaps the best solution is to use the logging module directly:
This is a way of redirecting stdout and stderr away from the shell using the logging module:
You should only use this LogFile implementation if you really cannot use the logging module directly.
another solution using logging module:
Also, you can use it in combination with logging module from Python if you want.
What I really wanted was to use the default python logging mechanism in every case where it was possible to do so, but to still capture any error when something went wrong that was unanticipated.
Obviously, if you're not as subject to whimsy as I am, replace LOG_IDENTIFIER with another string that you're not like to ever see someone write to a log.
To use this you can just call StdErrReplament::lock(logger) and StdOutReplament::lock(logger)
passing the logger you want to use to send the output text.
If you would like to also see the contents of the log.debug calls on the screen, you will need to add a stream handler to your logger.
While it would still saving this to the file my_log_file.txt:
When disabling this with StdErrReplament:unlock(), it will only restore the standard behavior of the stderr stream, as the attached logger cannot be never detached because someone else can have a reference to its older version.
If you wish to log all output to a file AND output it to a text file then you can do the following.
If you daemonize the thread that processes the output of the pipe, then that thread gets killed as soon as the main thread completes, but this does not guarantee that all outputs have been written to the log file.
O(log log n) terms can show up in a variety of different places, but there are typically two main routes that will arrive at this runtime.
As mentioned in the answer to the linked question, a common way for an algorithm to have time complexity O(log n) is for that algorithm to work by repeatedly cut the size of the input down by some constant factor on each iteration.
If this is the case, the algorithm must terminate after O(log n) iterations, because after doing O(log n) divisions by a constant, the algorithm must shrink the problem size down to 0 or 1.
This is why, for example, binary search has complexity O(log n).
Interestingly, there is a similar way of shrinking down the size of a problem that yields runtimes of the form O(log log n).
There are approximately log n digits in the number n, and approximately log (√n) = log (n1/2) = (1/2) log n digits in √n.
Because you can only halve a quantity k O(log k) times before it drops down to a constant (say, 2), this means you can only take square roots O(log log n) times before you've reduced the number down to some constant (say, 2).
That's interesting, because this connects back to what we already know - you can only divide the number k in half O(log k) times before it drops to zero.
Therefore, there can be only O(log k) square roots applied before k drops to 1 or lower (in which case n drops to 2 or lower).
Since n = 2k, this means that k = log2 n, and therefore the number of square roots taken is O(log k) = O(log log n).
Therefore, if there is algorithm that works by repeatedly reducing the problem to a subproblem of size that is the square root of the original problem size, that algorithm will terminate after O(log log n) steps.
Due to the way the vEB-tree is structured, you can determine in O(1) time which subtree to descend into, and so after O(log log N) steps you will reach the bottom of the tree.
Accordingly, lookups in a vEB-tree take time only O(log log N).
The maximum depth of the recursion is therefore O(log log n), and using an analysis of the recursion tree it can be shown that each layer in the tree does O(n) work.
Therefore, the total runtime of the algorithm is O(n log log n).
There are some other algorithms that achieve O(log log n) runtimes by using algorithms like binary search on objects of size O(log n).
For example, the [x-fast trie (hyper-link)] data structure performs a binary search over the layers of at tree of height O(log U), so the runtime for some of its operations are O(log log U).
The related [y-fast trie (hyper-link)] gets some of its O(log log U) runtimes by maintaining balanced BSTs of O(log U) nodes each, allowing searches in those trees to run in time O(log log U).
The [tango tree (hyper-link)] and related [multisplay tree (hyper-link)] data structures end up with an O(log log n) term in their analyses because they maintain trees that contain O(log n) items each.
Other algorithms achieve runtime O(log log n) in other ways.
[Interpolation search (hyper-link)] has expected runtime O(log log n) to find a number in a sorted array, but the analysis is fairly involved.
Ultimately, the analysis works by showing that the number of iterations is equal to the number k such that n2-k ≤ 2, for which log log n is the correct solution.
Some algorithms, like the [Cheriton-Tarjan MST algorithm (hyper-link)], arrive at a runtime involving O(log log n) by solving a complex constrained optimization problem.
One way to see factor of O(log log n) in time complexity is by division like stuff explained in the other answer, but there is another way to see this factor, when we want to make a trade of between time and space/time and approximation/time and hardness/... of algorithms and we have some artificial iteration on our algorithm.
For example SSSP(Single source shortest path) has an O(n) algorithm on planar graphs, but before that complicated algorithm there was a much more easier algorithm (but still rather hard) with running time O(n log log n), the base of algorithm is as follow (just very rough description, and I'd offer to skip understanding this part and read the other part of the answer):
divide graph into the parts of size O(log n/(log log n)) with some restriction.
Suppose each of mentioned part is node in the new graph G' then compute SSSP for G' in time O(|G'|*log |G'|) ==> here because |G'| = O(|G|*log log n/log n) we can see the (log log n) factor.
Compute SSSP for each part: again because we have O(|G'|) part and we can compute SSSP for all parts in time |n/logn| * |log n/log logn * log (logn /log log n).
But my point is, here we choose the division to be of size O(log n/(log log n)).
If we choose other divisions like O(log n/ (log log n)^2) which may runs faster and brings another result.
So may be we see more complicated stuffs than "log log n" in real working algorithms.
Don't forget that log[base A] x = log[base B] x / log[base B] A.
So if you only have log (for natural log) and log10 (for base-10 log), you can use
but also know that
math.log takes an optional second argument which allows you to specify the base:
[http://en.wikipedia.org/wiki/Binary_logarithm (hyper-link)]
If you are on python 3.3 or above then it already has a built-in function for computing log2(x)
If all you need is the integer part of log base 2 of a floating point number, extracting the exponent is pretty efficient:
Floors toward negative infinity, so log₂31 computed this way is 4 not 5.  log₂(1/17) is -5 not -4.
Floors toward negative infinity, so log₂31 computed this way is 4 not 5.
or you can generally use math.log(x, base) for any base you want.
It's very possible to do something like this in logback.
Then you'd setup two separate loggers, one for everything and one to log analytics data like so:
in my case I wanted to leave class names as log name
and as I had few such classes, so my logback.xml
You can have as many loggers as you wish.
But, it's better you have one for each package that you need to log differently.
Then all the classes in that package and its sub-packages will get the that specific logger.
They all can share the root logger and send their log data to root logger appender using additivity="true".
Eventually, it will be config.assets.logger = nil, but that part is currently stubbed on master (not done yet).
In config/environments add config.log_level = :error to the .rb files you want to change.
This will change the log settings to error only.
tail -f log/development.log | grep -vE 'asset'
For Ruby on Rails 3.2, add config.assets.logger = false to your development environment configuration file, typically found at config/environments/development.rb.
[Lograge (hyper-link)] for the win - it kills Ruby on Rails' annoying logger defaults out of the box (e.g.
logging assets, logging partial rendering) and is customizable if you want to add/remove specific items.
Many people are confused about the use of config.assets.logger = false.
Setting config.assets.logger to false will turn off served assets logging.
It only disables sprocket 'serving' logs, not Ruby on Rails actionpack request logs.
Taking example from the link, logs like this are disabled:
But logs like this are not
git log > log.txt
You seem to be using the [combined log format (hyper-link)].
LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-agent}i\"" combined
The same section of the documentation also lists other common log formats; readers whose logs don't look quite like this one may find the pattern their Apache configuration is using listed there.
I also don't under stand what the "-" means after the 200 140 section
  of the log
By default applicationhost.config file defines following two log file locations.
You can update above directory paths to change the log file locations.
C:\Users\ user_name \Documents\IISExpress\Logs\
From the firebug site
[http://getfirebug.com/logging/ (hyper-link)]
Calling console.dir(object) will log an interactive listing of an object's properties, like > a miniature version of the DOM tab.
In Firefox, these function behave quite differently: log only prints out a toString representation, whereas dir prints out a navigable tree.
In Chrome, log already prints out a tree -- most of the time.
However, Chrome's log still stringifies certain classes of objects, even if they have properties.
You can also see a clear difference with arrays (e.g., console.dir([1,2,3])) which are logged differently from normal objects:
While console.log([1,2]) gives the following output:
console.log prints the element in an HTML-like tree
Specifically, console.log gives special treatment to DOM elements, whereas console.dir does not.
console.log(input): The browser logs in a nicely formatted manner
console.dir(input):  The browser logs just the object with all its properties
Logs the following in google dev tools:
Of particular interest is depth, which (in theory) allows travering objects into more than the default 2 levels that console.log supports.
I wrote "in theory" because in practice when I had a Mongoose object and ran console.log(mongoose) and console.dir(mongoose, { depth: null }), the output was the same.
Well, the [Console Standard (hyper-link)] (as of commit [ef88ec7a39fdfe79481d7d8f2159e4a323e89648 (hyper-link)]) currently calls for [console.dir (hyper-link)] to apply [generic JavaScript object formatting (hyper-link)] before passing it to [Printer (hyper-link)] (a spec-level operation), but for a single-argument [console.log (hyper-link)] call, the spec ends up passing the JavaScript object directly to [Printer (hyper-link)].
Since the spec actually leaves almost everything about the [Printer (hyper-link)] operation to the implementation, it's left to their discretion what type of formatting to use for console.log().
Changing the log level is simple; modifying other portions of the configuration will pose a more in depth approach.
The changes are permanent through the life cyle of the Logger.
UPDATE: If you are using Log4j 2 you should remove the calls to setLevel per the [documentation (hyper-link)] as this can be achieved via implementation classes.
Calls to logger.setLevel() or similar methods are not supported in the
  API.
Equivalent functionality is
  provided in the Log4j 2 implementation classes but may leave the
  application susceptible to changes in Log4j 2 internals.
Log4j is able to watch the log4j.xml file for configuration changes.
If you change the log4j file, log4j will automatically refresh the log levels according to your changes.
See the documentation of org.apache.log4j.xml.DOMConfigurator.configureAndWatch(String,long) for details.
Another way to set the log level (or reconfiguring in general) log4j is by using JMX.
Log4j registers its loggers as JMX MBeans.
Using the application servers MBeanServer consoles (or JDK's jconsole.exe) you can reconfigure each individual loggers.
As described by Aaron, you can set the log level programmatically.
For example, you could have a GUI where the user or admin changes the log level and then call the setLevel() methods on the logger.
Log4j2 can be configured to refresh its configuration by scanning the log4j2.xml file (or equivalent) at given intervals.
See line 2 of the sample log4j2.xml file, which tells log4j to to re-scan its configuration if more than 5 seconds have passed since the last log event.
With log4j 1.x I find the best way is to use a DOMConfigurator to submit one of a predefined set of XML log configurations (say, one for normal use and one for debugging).
This answer won't help you to change the logging level dynamically, you need to restart the service, if you are fine restarting the
  service, please use the below solution
I did this to Change log4j log level and it worked for me, I have n't referred any document.
I used this system property value to set my logfile name.
I used the same technique to set logging level as well, and it worked
Sorry this won't dynamically change the logging level, it requires a restart of the service
in the log4j.properties file, I added this entry
If you would want to change the logging level of all the loggers use the below method.
This will enumerate over all the loggers and change the logging level to given level.
Please make sure that you DO NOT have log4j.appender.loggerName.Threshold=DEBUG property set in your log4j.properties file.
I have used this method with success to reduce the verbosity of the "org.apache.http" logs:
For log4j 2 API , you can use
Edit: it seems nginx now supports error_log stderr; as mentioned in [Anon's answer (hyper-link)].
You can send the logs to /dev/stdout.
[http://nginx.org/en/docs/ngx_core_module.html#error_log (hyper-link)]
RUN ln -sf /dev/stdout /var/log/nginx/access.log && ln -sf /dev/stderr /var/log/nginx/error.log
When running Nginx in a Docker container, be aware that a volume mounted over the log dir defeats the purpose of creating a softlink between the log files and stdout/stderr in your Dockerfile, as described in [@Boeboe 's answer (hyper-link)].
when logs are already collected by a central logging system).
Try git log tag1..tag2.
git log's pickaxe will find commits with changes including "word" with git log -Sword
While git log -G"regexec\(regexp" will show this commit, git log -S"regexec\(regexp" --pickaxe-regex will not (because the number of occurrences of that string did not change).
While git log -G"frotz\(nitfol" will show this commit, git log -S"frotz\(nitfol" --pickaxe-regex will not (because the number of occurrences of that string did not change).
One more way/syntax to do it is: git log -S "word"
Like this you can search for example git log -S "with whitespaces and stuff @/#ü !"
Do you have Enable Autogrowth and Unrestricted File Growth both enabled for the log file?
In the past, for special projects that temporarily require lots of space for the log file, I  created a second log file and made it huge.
Once the project is complete we then removed the extra log file.
Calling "checkpoint" causes SQL to write to disk all of those memory-only changes (dirty pages, they're called) and items stored in the transaction log.
This has the effect of cleaning out your transaction log periodically, thus preventing problems like the one described.
The following will truncate the log.
To fix this problem, change Recovery Model to Simple then Shrink Files Log
Database Tasks > Shrink > Files > Log
Then check your db log file size at
Database Properties > Files > Database Files > Path
To check full sql server log: open Log File Viewer at
SSMS > Database > Management > SQL Server Logs > Current
If your database recovery model is full and you didn't have a log backup maintenance plan, you will get this error because the transaction log becomes full due to LOG_BACKUP.
To overcome this behavior I advise you to check this [The transaction log for database ‘SharePoint_Config’ is full due to LOG_BACKUP (hyper-link)] that shows detailed steps to solve the issue.
I met the error: "The transaction log for database '...' is full due to 'ACTIVE_TRANSACTION' while deleting old rows from tables of my database for freeing disk space.
[http://www.nczonline.net/blog/2009/03/10/the-art-of-throwing-javascript-errors-part-2/ (hyper-link)]
But when you catch them and serialize them to console.log they are not serialized exactly the same way:
Console.log(e) of the above will produce 2 different results:
If I throw a new Error() and I am in development mode, I will get an error screen and a console log.
If I throw a string literal, I will only see it in the console and possibly miss it, if I am not watching the console log.
Throwing an error logs into the console and shows an error screen while in development mode (the screen won't be visible in production).
Whereas the following code only logs into the console:
Exception loggers will capture the stack trace for you
In this case the stack is being printed into the browser console but if you're using Javascript error logging tools like Appsignal or Bugsnag then that stack will also be available in them too.
Based on the Go docs, os.Open() can't work for log.SetOutput, because it opens the file "for reading:"
I prefer the simplicity and flexibility of the 12 factor app recommendation for logging.
To append to a log file you can use shell redirection.
The default logger in Go writes to stderr (2).
See also: [http://12factor.net/logs (hyper-link)]
The default logger in Go writes to stderr (2).
created a package called logger.go
[code snippet]

import the package wherever you want to log e.g main.go
[code snippet]
I usually print the logs on screen and write into a file as well.
I'm writing logs to the files, which are generate on daily basis (per day one log file is getting generated).
and "LogServer()" method is used to create a formatted log statement (contains : filename, line number, log level, error statement etc...)
Building on Allison and Deepak's answer, I started using logrus and really like it:
To help others, I create a basic log function to handle the logging in both cases, if you want the output to stdout, then turn debug on, its straight forward to do a switch flag so you can choose your output.
maybe this will help you (if the log file exists use it, if it does not exist create it):
I think that while not elegant, the [oplog (hyper-link)] could be partially used for this purpose: it logs all the writes - but not the reads...
This enables profiling and sets the threshold for "slow queries" as 1ms, causing all queries to be logged as "slow queries" to the file:
Now I get continuous log outputs using the command:
An example log:
Setting profilinglevel to 2 is another option to log all queries.
You can log all queries:
db.setProfilingLevel(2) means "log all operations".
I made a command line tool to activate the profiler activity and see the logs in a "tail"able way --> "mongotail":
[http://docs.mongodb.org/manual/reference/method/db.setLogLevel/ (hyper-link)]
Logs with only query projection
The logging happens in system.profile collection.
The logs can be seen from:
There are 3 logging levels ([source (hyper-link)]):
mongod always writes operations longer than the slowOpThresholdMs threshold to its log.
Where level refers to the profiling level and milliseconds is the ms of which duration the queries needs to be logged.
To turn off the logging, use
I wrote a script that will print out the system.profile log in real time as queries come in.
You need to enable logging first as stated in other answers.
[https://github.com/dtruel/mongo-live-logger (hyper-link)]
Try out this package to tail all the queries (without oplog operations): [https://www.npmjs.com/package/mongo-tail-queries (hyper-link)]
if you want the queries to be logged to mongodb log file, you have to set both 
the log level and the profiling, like for example:
(see [https://docs.mongodb.com/manual/reference/method/db.setLogLevel (hyper-link)])
Setting only the profiling would not have the queries logged to file, so you can only get it from
MongoDB profiler logs all the queries in the capped collection system.profile.
Start mongod instance with --profile=2 option that enables logging all queries
OR if mongod instances is already running, from mongoshell, run db.setProfilingLevel(2) after selecting database.
To view the logs I just need to tail it:tail -f ../logs/mongologs.txt.
This script can be started in background and it will log all the operation on the db in the file.
My code for tailable cursor for the system.profile collection is in nodejs; it logs all the operations along with queries happening in every collection of MyDb:
It cannot be used to log operations on a collection directly, instead use filter: 'ns': 'MyDb.MyCollection'
Another feature that i have added after this [logrotate (hyper-link)].
it logged all query info in mongod log file
tl;dr: shopt -s extglob && git log !
You now can log everything except a sub-folder content:
Adjust the DO_MATCH_LEADING_PATHSPEC logic to only get activated for positive pathspecs.
The access log files are stored relative to the path in the "Server path" field, which itself is relative to the workspace path.
The log files are stored in a folder realative to the path in the "Server path" field.
Also note you should be seeing the output to the log file in your Console view as you run or debug.
I'm not sure if you were after catalina.out or one of the other logs produced by Tomcat.
But, if you're after the catalina.out log file then follow the directions below:
Towards the bottom of the screen you can check the "File" checkbox and then specify a file that can be used to log your console (catalina.out) output.
The following system properties need to be set, so that the "logging.properties" file can be picked up.
[http://www.coderanch.com/t/442412/Tomcat/Tweaking-tomcat-logging-properties-file (hyper-link)]
Your logs are there.
Looks like the logs are scattered?
I found access logs under
<ProjectLocation>\.metadata\.plugins\org.eclipse.wst.server.core\tmp0\logs
If you want logs in a separate file other than the console: 
Double click on the server--> Open Launch Configuration--> Arguments --> add 
-Dlog.dir = "Path where you want to store this file" and restart the server.
You should have log4j or similar logging framework in place.
if you're after the catalina.out log and you are using eclispe with tomcat, this works for me:
In my case, I put it in logs directory of my tomcat install directory
e.g: /opt/apache-tomcat-7.0.83/logs/catena.out
I don't know if setLogLevel() will return in the final 2.0 version of Retrofit but for now you can use an interceptor for logging.
Here is an Interceptor that logs both the request and response bodies (using Timber, based on an example from the OkHttp docs and some other SO answers):
For those who need high level logging in Retrofit, use the interceptor like this
In Retrofit 2 you should use [HttpLoggingInterceptor (hyper-link)].
The above solution gives you logcat messages very similar to the old ones set by
Older Retrofit version might require an older logging-interceptor version.
I am not making some ads for them....but they are really nice guys :)
And the author replied to me very soon, with both Log method on Retrofit 1.9 and Retrofit 2.0-beta.
This is how to add logging method with the help of HttpLoggingInterceptor.
Also if you are the reader of that book I mentioned above, you may find that it says there is not log method with Retrofit 2.0 anymore -- which, I had asked the author, is not correct and they will update the book next year talking about it.
// In case you are not that familiar with the Log method in Retrofit, I would like to share something more.
Also should be noticed that there are some Logging Levels you could pick.
That's all of the Logging trick ;)
The main problem which I faced was dynamical adding headers and logging them into debug logcat.
One for logging and one for adding headers on-the-go (token authorization).
So here is working example with headers and logs:
I found way for Print Log in Retrofit
So add loggingInterceptor at the end, after your other Interceptors:
A best way to do this right in Retrofit 2 is to add the logger interceptor as a networkInterceptor this will print out the  network headers and your custom headers too.
The important thing is to remember that interceptor work as a stack and be sure u add the logger at the end of all.
Most of the answer here covers almost everything except this tool, one of the coolest ways to see the log.
This is the superb tool to monitor/log your app's network traffic on google chrome.
this will create a retrofit object with Logging.
implementation 'com.squareup.okhttp3:logging-interceptor:3.12.1'
While using Kotlin you can add Logging Interceptor like this :
Here is a simple way to filter any request/response params from the logs using HttpLoggingInterceptor :
I would recommend to use [Charles Web Debugging Proxy Application (hyper-link)] if you need logging your requests/responses.
I was also stuck in similar kind of situation, setLevel() method was not coming, when I was trying to call it with the instance of HttpLoggingInterceptor, 
like this:
Here is how I resolved it, to generate log for Retrofit2,
[https://github.com/square/okhttp/tree/master/okhttp-logging-interceptor (hyper-link)] )
I created a class with name AddLoggingInterceptor, 
here is my code,
Now you can see log generated in your Android Studio, you may need to search, okHttp for filtering process.
User logs in with 'keep me logged in'
You store a one-time token (huge string) which the user uses to pick-up their old login session.
Implementing a "Keep Me Logged In" feature means you need to define exactly what that will mean to the user.
Or at least to log them out.
This will mean that the user will be asked to login periodically.
follow the guidelines in [Improved Persistent Login Cookie Best Practice (hyper-link)]) and also recommend that you make sure your cookies are [HttpOnly cookies (hyper-link)] so they are not accessible to, potentially malicious, JavaScript.
When the user logs in, generate a large (128 to 256 bit) random token.
Your title “Keep Me Logged In” - the best approach make it difficult for me to know where to start because if you are looking at best approach then you would have to consideration the following :
User is logged on with Remember Me
Login Cookie issued with token & Signature
if not valid .. return to login page
If valid automatically login
User is logged in and remember me is selected
When a non-logged user visit the site  the signature, token and username are verified
Remember me login should have limited access and not allow modification of password, personal information etc.
The cookie can still be stolen because the user only gets the notification after the next login.
User Logged on to example.com with remember me
Attack can be prevented even before the next login like the other methods
Multiple Request to server just for a single login
When user logged in successfully create a string with this information:
If cookie is older than X days then redirect user to login page.
If password is changed after user's last login redirect user to login page.
IF agents are same go to next step, else redirect to login page.
Create new cookie if user re-logins.
Create a table to store "Remember Me" data in - separate to the user table so that I can log in from multiple devices.
On successful login (with Remember Me ticked):
a) Generate a unique random string to be used as the UserID on this machine: bigUserID
b) Generate a unique random string: bigKey
c) Store a cookie:  bigUserID:bigKey
d) In the "Remember Me" table, add a record with: UserID, IP Address, bigUserID, bigKey
If trying to access something that requires login:
a) Check for the cookie and search for bigUserID & bigKey with a matching IP address
b) If you find it, Log the person in but set a flag in the user table "soft login" so that for any dangerous operations, you can prompt for a full login.
On logout, Mark all the "Remember Me" records for that user as expired.
If you make your site support logging in with, say, your google+ account, you probably have a streamlined google+ button that will log the user in if they are already signed into google (I did that here to answer this question, as I am always signed into google).
If you get a validated response, use it as normal, then continue loading the logged in user as normal.
Otherwise, the login failed, but don't tell the user, just continue as not logged in, they will notice.
User 'joe' Logs in, escalates privileges, gets new Session ID and renews cookie 'session'.
Server sends Attacker to login page that runs an AJAX request to google to login.
Google server receives request, uses its API to see Attacker is not logged in currently.
Attacker's page receives response, script automatically redirects to login page with a POST value encoded in the url.
Login page gets the POST value, sends the cookie for 'keepmesignedin' to an empty value and a valid until date of 1-1-1970 to deter an automatic attempt, causing the Attacker's browser to simply delete the cookie.
Attacker is given normal first-time login page.
Note that there is no user field in this table, because the username, when logged in, is in the session data, and the program does not allow null data.
Either update this datetime on user login keeping it within a few days, or force it to expire regardless of last login keeping it only for a month or so, whichever your design dictates.
This massively reduces the likelihood that an attacker could obtain a valid token cookie and use it to login.
Some people will try to say that an attacker could steal the cookies from the victim and do a session replay attack to login.
If an attacker could steal the cookies (which is possible), they would certainly have compromised the entire device, meaning they could just use the device to login anyway, which defeats the purpose of stealing cookies entirely.
As long as your site runs over HTTPS (which it should when dealing with passwords, CC numbers, or other login systems), you have afforded all the protection to the user that you can within a browser.
Once a login has been achieved in this way, the server should still validate the session.
This is where you can code expectations for stolen or compromised systems; patterns and other expected results of logins to session data can often lead to conclusions that a system was hijacked or cookies were forged in order to gain access.
As a closing note, be sure that any recovery attempt, password changes, or login failures past the threshold result in auto-signin being disabled until the user validates properly and acknowledges this has occurred.
I apologize if anyone was expecting code to be given out in my answer, that's not going to happen here.
If a picture is worth 1k words I hope this helps others implement a secure persistent storage based on Barry Jaspan's [Improved Persistent Login Cookie Best Practice (hyper-link)]
If you have questions, feedback, or suggestions, I will try to update the diagram to reflect for the newbie trying to implement a secure persistent login.
I might be ungraving a dead topic but a simple solution is to check in your dependencies (Maven's pom for exemple) if you are including logback-core and logback-classic.
Whenever I built after a mvn clean, it wouldn't find log, or getXYZ(), or builder(), or anything.
CommonsLog
Flogger
Log
JBossLog
Log4
Log4j2
A simple thing but I figured it out is: I missed adding @Log to the class.
Try to create lombok.config file under project base directory and provide lombok.log.fieldName value.
Example: lombok.log.fieldName = LOG
or [https://developervisits.wordpress.com/2020/09/16/building-with-lomboks-slf4j-and-intellij-cannot-find-symbol-log/ (hyper-link)]
Log in->Home->screen 1->screen 2->screen 3->screen 4->screen 5
If you log in the user in screen 1 and from there you go to the other screens, use
Assuming you are finishing the login screen when the user logs in and home is created and afterwards all the screens from 1 to 5 on top of that one.
You can add an extra in the intent and read that in the home screen activity and finish it also (maybe launch login screen again from there or something).
I am not sure but you can also try going to login with this flag.
Then in your LoginActivity, overwrite onKeyDown:
Simply, when you go from the login screen, not when finishing the login screen.
And then in all forward activities, use this for logout:
When user click on the logout button then write the following code:
And also when after login if you call new activity do not use finish();
In my case, LoginActivity is the first activity in my program to run.
The above code clears all the activities except for LoginActivity.
Then put the following code inside the LoginActivity's onCreate(...), to listen for when LoginActivity is recreated and the 'EXIT' signal was passed:
For logout button on last screen of app, use this code on logout button listener to finish all open previous activities, and your problem is solved.
I have an application with a very similar design and I have a possible solution in terms of logic.
I have built my Login and Logout using shared preferences.
If I logout, data in my shared preferences is destroyed/deleted.
From any of my activities such as Home.java I check whether shared preferences has data and in this case it won't because I destroyed it when I logged out from one of the screens.
Therefore logic destroys/finishes that activity and takes me back to the Login activity.
You can replicate this logic in all your other activities.
in your log4j.properties (for alternative loggers, or log4j's xml format, check the docs)
Depending on your transaction manager, you can set the logging level of the spring framework so that it gives you more info about transactions.
Most interesting log informations of [JtaTransactionManager.java (hyper-link)] (if this question is still about the JtaTransactionManager) are logged at DEBUG priority.
Assuming you have a log4j.properties somewhere on the classpath, I'd thus suggest to use:
You could enable JDBC logging as well:
For me, a good logging config to add was:
log4j.logger.org.springframework.transaction.interceptor = trace
It will show me log like that:
[my own log statements from method com.MyClass.myMethod]
Here is some code I use in my Logback Layout implementation derived from [ch.qos.logback.core.LayoutBase (hyper-link)].
Whenever a new log line is printed out, getSpringTransactionInfo() is called and it returns a one-character string that will go into the log.
git log -p c -1 does just that .
That will give you the commit log, and then you'll have full control over all the git logging options for your automation purposes.
Of course you can filter out whichever events you see fit, and format the return as you wish via the traditional git-log commands which are well documented [here (hyper-link)].
where git log --grep='part_of_description' select the commits that contains 'part_of_description' and -p show the changeset of each commit
View Device Logs
All Logs
You probably have a lot of logs there, and to make it easier to find your imported log later, you could just go ahead and delete all logs at this point... unless they mean money to you.
I'm lazy so I just delete all old logs (this actually took a while).
Move all the above files (MyApp.app, MyApp-dSYM.dSYM and MyApp-Crash-log.crash) into a Folder with a convenient name wherever you can go using Terminal easily.
Symbolicated logs are on your terminal…
now what are you waiting for?
Go back to the crash log (in Devices window in Xcode)
The entire crash log should now be symbolicated.
If not, then right click and select 'Re-symbolicate crash log'
Click the "View Device Logs" button under the "Device Information" section on the right hand panel
The easiest process to symbolicate crash logs:
When the crash occurs, collect the crash logs from affected device.
If the crash log is in .ips format, just rename it to .crash.
open in xcode window->devices and simulators -> view device logs -> all logs -> drag and drop the .crash file.
Follow these steps in Xcode 10 to symbolicate a crash log from an app build on the same machine:
Click on the View Devices Logs button.
Switch to the All Logs tab.
The file should automatically symbolicate, otherwise use the right-click context menu item Re-Symbolicate Log.
Apple gives you crash log in .txt format , which is unsymbolicated
Download ".txt" file , change extension to ".crash"
  [ (hyper-link)]

Open devices and simulators from window tab in Xcode
select device and select device logs
drag and drop .crash file to the device log window
We will be able to see symbolicated crash logs over there
Please see the link for more details on Symbolicating [Crash logs (hyper-link)]
I was struggling to have the crash report symbolicated through atos but I was reluctant as the process seems cumbersome, But I found the crash report in the Xcode-> Window -> Organizer->Crashes(in left-side menu) Xcode will automatically download the crash logs and will symbolicate automatically, From there you can easily find the reason of the crash.
Instead of resetting passwords, allow sending a one-time password (that has to be changed as soon as the first logon occurs).
For logins just check if the stored value equals the value calculated from the user input + salt.
People can login without decrypting to plaintext
The page behind the link verifies that the parameter guid really exists (probably with some timeout logic), and asks the user for a new password.
If you need to have hotline help users, add some roles to your grants model and allow the hotline role to temporarily login as identified user.
Log all such hotline logins.
Any "Security Expert" that cannot point to sound risk analysis as the basis for their recommendations, or support logical tradeoffs, but would instead prefer to spout dogma and CWEs without even understanding how to perform a risk analysis, are naught but Security Hacks, and their Expertise is not worth the toilet paper they printed it on.
I'm less worried about some anonymous hacker, or even Cousin Fred squeezing in repeated suggestions to go back to Lake Wantanamanabikiliki, as I am about Aunt Erma not being able to logon when she needs to.
Again, I'm NOT worried about hacks, I just dont want silly mistakes of wrong login - I want to know who is coming, and what they want.
Not quite like your cat's blog's servers, but nor do they surpass some of the more secure banks.
The simplest way to allow users to recover a login is to e-mail them a one-time link that logs them in automatically and takes them straight to a page where they can choose a new password.
Here are a couple of blog posts I wrote on the subject:
[http://jamesmckay.net/2008/06/easy-login-recovery-without-compromising-security/ (hyper-link)]
If what they really want is a password to logon, why not have a routine that simply changes the old password (whatever it is) to a new password that the support person can give to the person that lost his password?
Of course all such resets should be logged somewhere and good practice would be to generate an email to the user telling him that the password has been reset.
That way when a user has a problem the support person can login to the account with the master key and help the user change his password to whatever.
Needless to say, all logins with the master key should be logged by the system as well.
Everything was logged and reviewed, plus all the operators had DOD secret clearances and we never had any abuses.
When you next log in it will ask you if you want to keep that password or change it to something you can remember more easily."
They can then be prompted to enter the creature's secret name to log in.
PS 2: for credit card information, like "one click buying", what I do is use the login password.
This password is hashed in database (sha1, md5, etc), but at login time I store the plain-text password in session or in a non-persistent (i.e.
If the user was logged in with a service like facebook, twitter, etc, then I prompt the password again at buying time (ok, it's not a fully "on click") or then use some data of the service that user used to login (like the facebook id).
Log them in automatically or provide a temporary password.
Use this when validating the next log-in.
If I log in to a news site to read newspapers, I want to type 1111 as password and be through!!
When logging the user in, allow both the user's password (after salting/hashing), but also allow what the user literally entered to match too.
Went on the log-in screen and clicked forgot my password.
Login to Bitbucket
you don't have a bitbucket password because you log with google, but you can "reset" the password here [https://bitbucket.org/account/password/reset/ (hyper-link)]
Access [https://id.atlassian.com/login/resetpassword (hyper-link)]
Don't click the Log in to my account button, instead, you want to click the small link bellow that says Alternatively, you can reset your password for your Atlassian account.
login to bitbucket
Click on Create App Password, here give permission to read and write and the login to GitHub desktop using the same password.
EDITED according to log4j2 version 2.4 FAQ
You can set a logger’s level with the class Configurator from Log4j Core.
EDITED to reflect changes in the API introduced in Log4j2 version 2.0.2
If you wish to change the root logger level, do something like this :
[Here (hyper-link)] is the javadoc for LoggerConfig.
Perhaps you should check JMX support given by Log4J2:
In JConsole look for the "org.apache.logging.log4j2.Loggers" bean
Finally change the level of your logger
More info: [http://logging.apache.org/log4j/2.x/manual/jmx.html (hyper-link)]
If you want to change a single specific logger level (not the root logger or loggers configured in the configuration file) you can do this:
I found a good answer here: [https://garygregory.wordpress.com/2016/01/11/changing-log-levels-in-log4j2/ (hyper-link)]
You can use org.apache.logging.log4j.core.config.Configurator to set the level for a specific logger.
The accepted answer by @slaadvak did not work for me for [Log4j2 2.8.2 (hyper-link)].
To change the log Level universally use:
To change the log Level for only the current class, use:
One un-usual way i found to do is to create two separate file with different logging level.
log4j2.xml and log4j-debug.xml
Now change the configuration from this files.
Most of the answers by default assume that logging has to be additive.
But say that some package is generating lot of logs and you want to turn off logging for that particular logger only.
Assuming following log4j2.xml is in classpath
I added a jvm argument: -Dlog4j.debug to my test.
This does some verbose logging for log4j.
I noticed that the final LogManager was not the one that I was using.
It does have an svn log --xml option, to allow you to parse the output yourself, and get the interesting parts.
If you save the above as svnLogStripByAuthor.py, you could call it as:
You can use Perl to filter the log by username and maintain the commit messages.
If you set this to the separator of the entries of the SVN log, Perl will read one record at a time and then you should be able to match the the username in the entire record.
Filters log messages to show only those that match the search pattern ARG.
Log messages are displayed only if the provided search pattern matches any of the author, date, log message text (unless --quiet is used), or, if the --verbose option is also provided, a changed path.
If multiple --search options are provided, a log message is shown if it matches any of the provided search patterns.
If --limit is used, it restricts the number of log messages searched, rather than restricting the output to a particular number of matching log messages.
Unfortunately, though, xsltproc is not a streaming processor, so you have to give log a limit.
svnLogFilter.xslt
svnLogText.xslt
Beginning with Subversion 1.8, you can use [--search and --search-and command-line options with svn log command (hyper-link)].
So it should be as simple as running svn log --search JohnDoe.
Another solution for the problem of permanently disabling pager specifically when using log subcommand:
for current repo only:
git config pager.log false
for your git installation (i. e. all repos on your machine):
git config --global pager.log false
Just use the same logic than in Objective-C but with some small changes
I wrote a
  class extension that can be added to your tool chain to handle this in
  a more logical fashion.
To not show such a screen to new users, just add the build number after the first login or when the on-boarding is complete.
have a look at [http://www.slf4j.org/api/org/slf4j/Logger.html (hyper-link)] for the full Logger api.
I imagine that the reason that this functionality is missing is that it is next to impossible to construct a Level type for slf4j that can be efficiently mapped to the Level (or equivalent) type used in all of the possible logging implementations behind the facade.
Concerning [@ripper234 (hyper-link)]'s [use-case (hyper-link)] (unit testing), I think the pragmatic solution is modify the unit test(s) to hard-wire knowledge of what logging system is behind the slf4j facade ... when running the unit tests.
They intend to implement piecemeal construction of logging events (with dynamic logging levels) in slf4j 2.0; see [https://jira.qos.ch/browse/SLF4J-124 (hyper-link)].
You could add other variants of log, say if you wanted generic equivalents of SLF4J's 1-parameter or 2-parameter warn/error/etc.
Try switching to Logback and use
I believe this will be the only call to Logback and the rest of your code will remain unchanged.
Logback uses SLF4J and the migration will be painless, just the xml config files will have to be changed.
Remember to set the log level back after you're done.
In my case, slf4j is configured with the java logging adapter (the jdk14 one).
Based on the answer of massimo virgilio, I've also managed to do it with slf4j-log4j using introspection.
But I think (a) the user should pass the Logger; and (b) AFAIU the original question was not asking for a convenient way for everywhere in the application, only a situation with few usages inside a library.
Since slf4j [allows a Throwable (whose stack trace should be logged) inside the varargs param (hyper-link)], I think there is no need for overloading the log helper method for other consumers than (String, Object[]).
I was able to do this for the JDK14 binding by first requesting the SLF4J Logger instance and then setting the level on the binding -- you may try this for the Log4J binding.
The method I use is to import the ch.qos.logback modules and then type-cast the slf4j Logger instance to a ch.qos.logback.classic.Logger.
To find out the possible Logging-levels, you can explode the ch.qos.logback class to see all the possible values for Level:
Logger is passed during invocation, so the class info should be ok, and it works nicely with @Slf4j lombok annotation.
It is not possible to specify a log level in sjf4j 1.x out of the box.
This will output a log like this:
Pro You can easily define variables, parameters and return types as LogLevel
It is not possible with slf4j API to dynamically change log level but you can configure logback (if you use this) by your own.
In that case create factory class for your logger and implement root logger with configuration that you need.
After you configure root logger (only once is enough) you can delegate getting new logger by
Remember to use the same loggerContext.
Changing log level is easy to do with root logger given from loggerContext.
The fluent API in SLF4J v2.0 introduces a new method, namely Logger.makeLoggingEventBuilder(Level) which an be used to accomplish the desired outcome.
The default implementation will return the singleton instance of NOPLoggingEventBuilder if the logger is disabled for the given Level.
This implementation of the LoggingEventBuilder interface, as the name NOP indicates, does nothing, preserving nanosecond execution time for disabled log messages.
Login as user which has permissions to modify the repository and navigate to the repository on your server.
Remove .git/logs/refs/remotes/origin/branch
In my case, removing the sub-folder in question in .git/logs/ref/remotes/origin resolved the problem, as the branch in question has already been merged back.
Namely I had a Java application whose main threw an exception (and something overrode the default uncaught exception handler so that nothing was logged).
(Another cause could be something in the app calling System.exit; you could use a custom SecurityManager with an overridden checkExit to prevent (or log the caller of) exit; see [https://stackoverflow.com/a/5401319/204205 (hyper-link)].)
Whilst troubleshooting the same issue I found no logs when using kubeclt logs <pod_id>.
Use [docker logs (hyper-link)].
docker logs --tail=50 <container id> for the last fifty lines - useful when your container has been running for a long time.
To directly view the logfile of an exited container in less, scrolled to the end of the file, I use:
run as ./viewLogs.sh CONTAINERNAME
This method has the benefit over docker logs based approaches, that the file is directly opened, instead of streamed.
sudo is necessary, as the LogPath/File usually is under root-owned
You can use below command to copy logs even from an exited container :
Steps that found the logs also listed in this [post (hyper-link)]
Substitute it in this command cat /var/lib/docker/containers/<container id>/<container id>-json.log
I don't think there is a runtime log, per se, but you can run it in debug mode.
running vim with the -V[N] option will do a pretty hefty runtime log, here N is the debug level.
would create a log of debug level 9 in the current directory with the filename myVim.log
Then create directory ~/.log/vim and call ToggleVerbose() to get your log in ~/.log/vim/verbose.log.
Use android.util.Log and the static methods defined there (e.g., e(), w()).
Look into [android.util.Log (hyper-link)].
It lets you write to the log with various log levels, and you can specify different tags to group the output.
The Tag is just used to easily find your output, because the Output of LogCat can be sometimes very long.
Log.v(TAG, "did something");
Recently I found this approach to writing logs in android, which I think is super awesome.
Please see the logs as this way,
You can use my libary called RDALogger.
With this library, you can log your message with method name/class name/line number and anchor link.
With this link, when you click log, screen goes to this line of code.
And than you can log whatever you want;
Text printed to stderr will show up in httpd's error log when running under mod_wsgi.
You can either use print directly, or use logging instead.
You can configure logging in your settings.py file.
Edit: the example above is from a Django 1.1 project, logging configuration in Django has changed somewhat since that version.
You can do this pretty easily with tagalog (https://github.com/dorkitude/tagalog)
For instance, while the standard python module writes to a file object opened in append mode, the App Engine module (https://github.com/dorkitude/tagalog/blob/master/tagalog_appengine.py) overrides this behavior and instead uses logging.INFO.
You could extend the module yourself and overwrite the log function without much difficulty.
Here's a Django logging-based solution.
see [https://docs.djangoproject.com/en/dev/topics/logging/ (hyper-link)] for details.
logging.conf:
This works quite well in my local.py, saves me messing up the regular logging:

otherwise, you can use Dart's built in [log (hyper-link)] function
The Dart print() function outputs to the system console, which you can view using flutter logs (which is basically a wrapper around adb logcat).
If you output too much at once, then Android sometimes discards some log lines.
log() from 'dart:developer'
So it comes helpful when you want to log the whole API response.
And also helps in dart dev tools to show formatted logging.
you can use the Logger package it's easy and simple
Alternatively, I have created a Logger for Flutter Apps: [https://github.com/hiteshsahu/Flutter-Logger (hyper-link)]
Simply copy AppLog.dart & add to yor project and use it like this:
AppLog.i("Info Message");  // Simple Info message
AppLog.i("Home Page", tag: "User Logging"); // Info message with identifier TAG
You can also set a filter for Log Levels
AppLog.setLogLevel(log_priority); // logs below log_priority will be hidden
VERBOSE<=log_priority<= FAILURE
Example
Hide all info and Debug Logs:
Feel free to use my logger or if you have some ideas for improvement please create PR or let me know I will improve it.
Take a look at logging.exception ([Python Logging Module (hyper-link)])
This should automatically take care of getting the traceback for the current exception and logging it properly.
However, I'd definitely recommend using the standard Python logging module, as suggested by rlotun.
Logging exceptions is as simple as adding the exc_info=True keyword argument to any log message, see entry for Logger.debug in [http://docs.python.org/2/library/logging.html (hyper-link)].
output (depending, of course, on your log handler config):
[https://devblogs.microsoft.com/premier-developer/getting-started-with-node-js-angular-and-visual-studio-code/ (hyper-link)]
A complete log of this run can be found in:
npm ERR!
C:\Users\admin\AppData\Roaming\npm-cache_logs\2018-11-20T07_38_56_733Z-debug.log
This is the directory where node stores its cache, locks, logs, global packages (unless you're using [nvm (hyper-link)]), and modules installed via npx.
Source- [https://www.codegrepper.com/code-examples/javascript/npm+ERR%21+Unexpected+end+of+JSON+input+while+parsing+near+%27...babel-plugin-istanbul%27+npm+ERR%21+A+complete+log+of+this+run+can+be+found+in%3A+npm+ERR%21+C%3A%5CUsers%5Csapho%5CAppData%5CRoaming%5Cnpm-cache%5C_logs%5C2020-08-26T20_37_45_303Z-debug.log+Aborting+installation (hyper-link)].
Wrap the whole thing in a try/catch and log the unhandled exception, then pass it on.
you can also create a custom version of  [HandleError] with which you can write error info and all other details to log
So make sure that you log and handle all exceptions in there:
Briefly, you add global exception loggers and/or global exception handler (only one).
This both prints the output to the STDOUT and writes the same output to a log file.
Note that this won't write stderr to the log file, so if you want to combine the two streams then use:
would send stdout and stderr output into the log file, but would also leave you with fd 3 connected to the console, so you can do
to write a message to both the console and the log file - tee sends its output to both its own fd 1 (which here is the LOG_FILE) and the file you told it to write to (which here is fd 3, i.e.
into the log file.
for log file you may date to enter into text data.
Log file will be created in first run and keep on updating from next runs.
In case log file missing in future run , script will create new log file.
Here echo outputs only to console, log outputs to only log file  and message outputs to both the log file and console.
In console
  Echoed to console only
  To console and log
For the Log file
In Log File
  Written to log file only
  This is stderr.
Written to log file only
  To console and log
The file /tmp/both.log afterwards contains
The /tmp/both.log is appended unless you remove the -a from tee.
I find it very useful to append both stdout and stderr to a log file.
I discovered that in zsh, the here-doc solution can be modified using the "multios" construct to copy output to both stdout/stderr and the log file:
It is not as readable as the exec solution but it has the advantage of allowing you to log just part of the script.
Of course, if you omit the EOF then the whole script is executed with logging.
I wanted to display logs on stdout and log file along with the timestamp.
Sample logs:
EDIT: to log all SQL queries to a file etc, then you will want to create some middleware.
Those are concerned with printing to the terminal, but it wouldn't be hard to adapt them to use python's logging library.
Django 1.3 logs all SQL statements to django.db.backends logger:
[https://docs.djangoproject.com/en/dev/topics/logging/#django-db-backends (hyper-link)]
Merge the following snippet with the LOGGING field in your settings.py:
To log SQL queries during testing, you need two things:
django.db.backends logger enabled and
Output the object with console.log from your code, like so: console.log(myObject)
Using "Store as a Global Variable" works, but it only gets the final instance of the object, and not the moment the object is being logged (since you're likely wanting to compare changes to the object as they happen).
Your process_exception() method can then perform whatever type of logging you'd like: writing to console, writing to a file, etc., etc.
Obviously James is correct, but if you wanted to log exceptions in a datastore, there are a few open source solutions already available:
1) CrashLog is a good choice: [http://code.google.com/p/django-crashlog/ (hyper-link)]
2) Db-Log is a good choice as well: [http://code.google.com/p/django-db-log/ (hyper-link)]
django-db-log, mentioned in another answer, has been replaced with:
If you just want to log everything to a simple text file here's the logging configuration to put in your settings.py
The lack of logging is sooo painful.
O( log* N ) is "[iterated logarithm (hyper-link)]":
In computer science, the iterated logarithm of n, written log* n (usually read "log star"), is the number of times the logarithm function must be iteratively applied before the result is less than or equal to 1.
log* (n)- "log Star n" as known as "Iterated logarithm"
In simple word you can assume log* (n)= log(log(log(.....(log* (n))))
log* (n) is very powerful.
1) Log* (n)=5 where n= Number of atom in universe
2) Tree Coloring using 3 colors can be done in log*(n) while coloring Tree 2 colors are enough but complexity will be O(n) then.
3) Finding the Delaunay triangulation of a set of points knowing the Euclidean minimum spanning tree: randomized O(n log* n) time.
The log* N bit is an iterated algorithm which grows very slowly, much slower than just log N. You basically just keep iteratively 'logging' the answer until it gets below one (E.g: log(log(log(...log(N)))), and the number of times you had to log() is the answer.
Depending on what platform you are running on and what other log viewing tools you have available, you can just use the appropriate log4j appender (syslog, Windows Event Logger) and just use your platform log viewing tools.
Are you trying to aggregate logs from several computers?
Or just view the logs from a single remote process?
You may want to use a custom log viewer that just works on files.
I like [Kiwi Log Viewer (hyper-link)] or Ganymede (an Eclipse plugin), but it's not hard to put a simple Swing app together that reads from the socket.
I've rolled out Splunk ([http://www.splunk.com/ (hyper-link)]) for log viewing and searching with great success.
The free version can be used locally and the paid version can collect all your logs into one location.
We use it mostly for Log4J logs but with lots of other formats as well.
Beyond tail and grep support (without needing to know grep...) it automatically indexes logs and allows easy analysis (e.g.
It's called LogExpert (see [http://www.log-expert.de/ (hyper-link)]) and is free.
You can try [logFaces (hyper-link)], it has fantastic real-time log viewer based on eclipse-like design.
Take a look to [http://jlogviewer.sourceforge.net/ (hyper-link)] or [http://sourceforge.net/projects/jlogviewer/ (hyper-link)]
Java log viewer is lightweight GUI to easily view the java application
logs generated by the "java.util.logging" package.
[LogSaw (hyper-link)] based on Eclipse and free.
Log4j log file analyzer, simple to use with easy filtering.
Supports several flavors of log4j log files: JBoss, Log4j pattern layout, Log4j XML layout, WebSphere.
After couple of hours googling and trying several recommended free log4j viewers, this one was pleasant surprise.
Have tried Chainsaw, BareTail, Insight, LogExpert, logview4j.
I'll add that for Windows, WireShark makes for a handy syslog viewer, ironically enough.
I've tried several other syslog tools, and really, Kiwi is the best for syslog out there, but the "free" version is a bit nerfed.
Others I ran into were either poorly programmed (crashing on minor issues -- logview4net), had a poor interface (Star SysLog Daemon Lite), or didn't even run (nxlog)
You can use WireShark's filter language to drill down on log data.
It's overkill, but until someone writes a free syslog viewer/collector for Windows and makes it decent, this is one field that will be a hard one for most people.
Consider to use [Log4j viewer eclipse plugin (hyper-link)] - that was fork of Ganemede plugin in the begging and now have more features and stability was improved significantly, and still in active development and free :)
You can use [MindTree Insight (hyper-link)], it is open source, efficient, and specific for that use case : analyze log4j files.
I'm using [OtrosLogViewer (hyper-link)].
You can mark log events [manually or using string/regular expression (hyper-link)].
Logs can be imported by [listening on socket (hyper-link)] or [connecting to Log4j SocketHubAppender (hyper-link)]
Disclaimer: I am the author of OtrosLogViewer
I am using Notepad++ with [my custom log file highlighting UDL (hyper-link)].
Alfa is a GUI tool for analyzing log files.
You open a log, press Ctrl-F and the "Next" button again and again, then reload the file as it was modified, and repeat the search.
Alfa maps a log file to a database allowing you to use standard SQL queries to get data without any superfluous actions.
[LogMX (hyper-link)] is a crossplatform tool that parses any log format from any source, then displays log entries with many features.
By default, it handles formats like Log4j, LogFactor, syslog,... and can read from local file or SFTP, FTP, HTTP... but you can write your own pluggins if your format is another one or if your logs cannot be accessed through classical protocols.
You can monitor logs in realtime like 'tail' or load a whole log file and stop monitoring it.
[www.logmx.com (hyper-link)]
Another good log viewer is Lilith ([http://sourceforge.net/projects/lilith/ (hyper-link)] and [http://lilithapp.com/ (hyper-link)]).
It is open source and works well with Logback, log4j & java.util.logging.
Just published a node module for color highlighting log output [log-color-highlight (hyper-link)].
Works well on unix/linux/windows and supports config file for complex logging scenarios.
2015July15 - the signin that was working last week with this script on login
if you handle your own logic without redirect link for web apps
[react-google-login (hyper-link)]: 5.0.2
Summary: React --> request social auth "code" --> request jwt token to acquire "login" status in terms of your own backend server/database.
The google sign in button is from react-google-login mentioned above.
The frontend stores that token and when it has to perform CRUD to the backend server, especially create/delete/update, if you attach the token in your Authorization header and send request to backend, Django backend will now recognize that as a login, i.e.
I had this problem using Meteor and Ngrok, while trying to login with Google.
Adjust the path if the daily log file you mention is not the standard Laravel log file.
I've then added my regular user to the www-data group so that running artisan commands as my regular user can still write to the log.
So if www-data is running PHP, any cache and log files it makes will be writeable by default by anyone in that user's main group, which is www-data.
Given that the user that creates the files is the one that has the permission to write to it by default, we can separate the logs by user as such:
If your www-data user were to create an error log, it would result in: storage/logs/laravel-www-data-2015-4-27.log.
If your root user were to create an error log, it would result in: storage/logs/laravel-root-2015-4-27.log.
Change the log used by your artisan command, in your php script.
If your class's name is ArtisanRunner, then your log file will be:
storage/logs/laravel-ArtisanRunner-2015-4-27.log.
Conclusion: Solution number 1 is better, given that it delineates your logs by user, and hence no errors will occur.
In our case we wanted to create all log files so that everything in the deploy group had read/write permissions.
For me this issue was much more than log permissions...I had issues with anything related to the bootstrap/cache and storage folders where one user would create a file/folder and the other would be unable to edit/delete due to the standard 644 and 755 permissions.
The dreaded log race conditions described above.
Purely for debugging purposes I found splitting the logs out into both cli/web + users was beneficial so I  modified Sam Wilson's answer slightly.
The best way I found is that fideloper suggest, [http://fideloper.com/laravel-log-file-name (hyper-link)], you can set laravel log configuration without touch Log class.

\Log::getMonolog()->popHandler();
\Log::useDailyFiles(storage_path('/logs/laravel-').get_current_user().
'.log');
It will store files like this: laravel-2018-01-27-cli-raph.log and laravel-2018-01-27-fpm-cgi-raph.log which is more readable.
It works with [Laravel Log Viewer (hyper-link)]
You [have to create a class (hyper-link)] for your logger:
Then, you have to register it in config/logging.php:
It will store files like this: laravel-2018-01-27-cli-raph.log and laravel-2018-01-27-fpm-cgi-raph.log which is more readable.
It works with [Laravel Log Viewer (hyper-link)]
In config/logging.php I just updated daily channel's path value with php_sapi_name() in it.
This creates seperate durectory for different php_sapi_name and puts log file with the time stamp into their perticular directory.
Log files are created under fpm-fcgi directory: Logs from website, owner: www-data
Log files are created under cli directory: from the artisan command(cronjob).
More info on Laravel 5.6 logging: [https://laravel.com/docs/5.6/logging (hyper-link)]
Here is my config/logging.php file:
Laravel version 5.6.10 and later has support for a permission element in the configuration (config/logging.php) for the single and the daily driver:
No need to juggle with Monolog in the bootstrap script.
$schedule->exec('chown -R www-data:www-data /var/www/**********/storage/logs')->everyMinute();
Laravel 5.8 lets you set the log name in config/logging.php.
So using previous answers and comments, if you want to name you log using both the actual posix user name AND the php_sapi_name() value, you only need to change the log name set.
Using the daily driver allows log rotation that runs per user / api combination which will ensure that the log is always rotated by an account that can modify the logs.
I also added a check for the posix functions which may not exist on your local environment, in which case the log name just defaults to the standard.
Assuming you are using the default log channel 'daily', you can modify your 'channels' key like so:
This will result in a log name that should be unique to each combination such as laravel-cli-sfscs-2019-05-15.log or laravel-apache2handler-apache-2019-05-15.log depending on your access point.
You could simply change the permission of the log file in your artisan command:
In other words, daily.log will always have www-data as its owner, even if you initialize the script as root user.
I prefer TailMe because of the possibility to watch several log files simultaneously in one window: [http://www.dschensky.de/Software/Staff/tailme_en.htm (hyper-link)]
I'm using Kiwi Log Viewer.
I haven't seen Log Expert anywhere among answers here.
It's customizable and is quite good for going around log files.
So far it's the best Windows graphical log viewer for me.
Graphical log viewers, while they might be very good for viewing log files, don't meet the need for a command line utility that can be incorporated into scripts (or batch files).
to view logs for a container: docker logs <containerid>
At the time of writing this the docker-compose run command does not provide a switch to see the logs of other services, hence you need to use the docker-compose logs command to see the logs you want.
Usage: logs [options] [SERVICE...]
-f, --follow        Follow log output.
--tail="all"        Number of lines to show from the end of the logs
for each container.
You can start Docker compose in detached mode and attach yourself to the logs of all container later.
If you're done watching logs you can detach yourself from the logs output without shutting down your services.
Use docker-compose up -d to start all services in detached mode (-d) (you won't see any logs in detached mode)
Use docker-compose logs -f -t to attach yourself to the logs of all running services, whereas -f means you follow the log output and the -t option gives you timestamps (See [Docker reference (hyper-link)])
Use Ctrl + z or Ctrl + c to detach yourself from the log output without shutting down your running containers
If you're interested in logs of a single container you can use the docker keyword instead:
Use docker logs -t -f <name-of-service>
To save the output to a file you add the following to your logs command:
docker-compose logs -f -t >> myDockerCompose.log
If you want to see output logs from all the services in your terminal.
: 
   Say you would like to log output of last 5 lines from all service
docker-compose logs -t -f --tail 5
If you wish to log output from specific services then it can be done as below:
docker-compose logs -t -f --tail 5 portal api
Where 5 represents last 5 lines from both logs.
Ref: [https://docs.docker.com/v17.09/engine/admin/logging/view_container_logs/ (hyper-link)]
Unfortunately we need to run docker-compose logs separately from docker-compose run.
In order to get this to work reliably we need to suppress the docker-compose run exit status then redirect the log and exit with the right status.
I wouldn't really mind about the performance of logging, at least not before profiling and discovering that it is a bottleneck.
Anyway you can always create a Handler subclass that doesn't perform flush at every call to emit(even though you will risk to lose a lot of logs if a bad exception occurs/the interpreter crashes).
    log2 (x) = logy (x) / logy (2)
where y can be anything, which for standard log functions is either 10 or e.
Consult your basic mathematics course, log n / log 2.
It doesn't matter whether you choose log or log10in this case, dividing by the log of the new base does the trick.
As stated on [http://en.wikipedia.org/wiki/Logarithm (hyper-link)]:
C99 has [log2 (hyper-link)] (as well as log2f and log2l for float and long double).
Basically the same as [tomlogic (hyper-link)]'s.
If you want to make it fast, you could use a lookup table like in [Bit Twiddling Hacks (hyper-link)] (integer log2 only).
Take also a look at possible duplicate [How to do an integer log2() in C++?
In my main program, I needed to calculate N * log2(N) / 2 with an integer result:
temp = (((uint32_t) N) * approx_log_base_2_N_times_256) / 512;
log2 (x) = logy (x) / logy (2)
I used setLogLevel(LogLevel.FULL).setLog(new AndroidLog("YOUR_LOG_TAG")), it helped me.
It appears that when overriding the log, the body is prefixed with a tag similar to
so it should be easy to log basic + body by adjusting the custom filter.
Logging In Retrofit 2
OkHttp 2.6.0 ships with a logging interceptor as an internal dependency and you can directly use it for your Retrofit client.
That’s why you need to manually import the logging interceptor.
Add the following line to your gradle imports within your build.gradle file to fetch the logging interceptor dependency.
Add Logging to Retrofit 2
While developing your app and for debugging purposes it’s nice to have a log feature integrated to show request and response information.
Since logging isn’t integrated by default anymore in Retrofit 2, we need to add a logging interceptor for OkHttp.
We recommend to add logging as the last interceptor, because this will also log the information which you added with previous interceptors to your request.
Log Levels
Logging too much information will blow up your Android monitor, that’s why OkHttp’s logging interceptor has four log levels: NONE, BASIC, HEADERS, BODY.
We’ll walk you through each of the log levels and describe their output.
further information please visit : [Retrofit 2 — Log Requests and Responses (hyper-link)]
no logging in Retrofit 2 anymore.
The development team removed the logging feature.
To be honest, the logging feature wasn’t that reliable anyway.
Jake Wharton explicitly stated that the logged messages or objects are the assumed values and they couldn’t be proofed to be true.
Even though there is no integrated logging by default, you can leverage any Java logger and use it within a customized OkHttp interceptor.
So add loggingInterceptor at the end, after your other Interceptors:
below code is working for both with header and without header to print log request & response.
For android studio before 3.0 (using android motinor)
[https://futurestud.io/tutorials/retrofit-2-log-requests-and-responses (hyper-link)]
[https://www.youtube.com/watch?v=vazLpzE5y9M (hyper-link)]
I hope this code will help you to logging .
[ZoomX — Android Logger Interceptor (hyper-link)] is a great interceptor can help you to solve your problem.
This is just another way to check it by using Android Logs.
Please add Interceptor in okHttpClient so you can get request and response log when calling API in retrofit android
Ideally, if you work with Timber it is better, as you can also do whatever you want with the log messages in your app globally, such as saving them to a file.
Here's how to make the Httplogginginterceptor work with timber:
You can also create your own implementation of the logger interceptor this way:
Besides what [Bert F said (hyper-link)], many commands, including log has the -r (or --revision) option.
For more info about these -r expressions refer to svn help log or the relevant chapter in the book [Version Control with Subversion (hyper-link)]
But svn log is still in reverse order, i.e.
the sorting order must be chronological.
The only command that does this seems to be svn log -r 1:HEAD but that takes much too long on a repository with some 10000 entries.
Display the last 10 subversion entries in chronological order:
svn log -r $(svn info | grep Revision | cut -f 2 -d ' '):HEAD -v
LE (thanks Gary for the comment)
same thing, but much shorter and more logical:
svn log -r BASE:HEAD -v
As you've already noticed [svn log (hyper-link)] command ran without any arguments shows all log messages that relate to the URL you specify or to the working copy folder where you run the command.
You can always refine/limit the svn log results:
[svn log --limit NUM (hyper-link)] will show only the first NUM of revisions,
[svn log --revision REV1(:REV2) (hyper-link)] will show the log message for REV1 revision or for REV1 -- REV2 range,
[svn log --search (hyper-link)] will show revisions that match the search pattern you specify (the command is available in Subversion 1.8 and newer client).
committers username),
date when the revision was committed,
revision comment text (log message),
list of paths changed in revision.
add, log, status ...), you can simply add the --help option to display the complete list of available options you can use with your subcommand as well as examples on how to use them.
The following snippet is taken directly from the svn log --help command output under the "examples" section :
I have solved my problem by running my Nginx as the user I'm currently logged in with, mulagala.
I had a similar issue getting Fedora 20, Nginx, Node.js, and Ghost (blog) to work.
I checked for errors in the SELinux logs:
[http://blog.frag-gustav.de/2013/07/21/nginx-selinux-me-mad/ (hyper-link)]
[https://wiki.gentoo.org/wiki/SELinux/Tutorials/Where_to_find_SELinux_permission_denial_details (hyper-link)]
[http://wiki.gentoo.org/wiki/SELinux/Tutorials/Managing_network_port_labels (hyper-link)]
Obtained using 
    sudo cat /var/log/audit/audit.log | grep nginx | grep denied 
as explained above.
Basically you can check the permissions set on setsebool and correlate that with the error obtained from grepp'ing' audit.log nginx, denied
You might want to look into something like Log4J which will automatically give you enough information to determine pretty closely where the logged code occurred.
You are better off using a logging framework like that in java.util.logging package or [log4j (hyper-link)].
Using these packages you can configure your logging information to include context down to the class name.
Then each log message would be unique enough to know where it came from.
As a result, your code will have a 'logger' variable that you call via
logger.debug("a really descriptive message")
I would recommend using a logging toolkit such as [log4j (hyper-link)].
Logging is configurable via properties files at runtime, and you can turn on / off features such as line number / filename logging.
Log4J allows you to include the line number as part of its output pattern.
See [http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html (hyper-link)] for details on how to do this (the key element in the conversion pattern is "L").
Here is the logger that we use.
it wraps around Android Logger and display class name, method name and line number.
[http://www.hautelooktech.com/2011/08/15/android-logging/ (hyper-link)]
In that method you can jump to your line code, when you double click on LogCat's row.
you can use -> Reporter.log("");
Below code is tested code for logging line no class name and method name from where logging method is called
One click on the underlined text to navigate to where the log command is
If you want to execute git log in folder B, type:
The underlying urllib3 library logs all new connections and URLs with the [logging module (hyper-link)], but not POST bodies.
which gives you the most verbose logging option; see the [logging HOWTO (hyper-link)] for more details on how to configure logging levels and destinations.
Depending on the exact version of urllib3, the following messages are logged:
urllib3 uses the http.client.HTTPConnection class to do the grunt-work, but that class doesn't support logging, it can normally only be configured to print to stdout.
However, you can rig it to send all debug information to logging instead by introducing an alternative print name into that module:
Calling httpclient_logging_patch() causes http.client connections to output all debug information to a standard logger, and so are picked up by logging.basicConfig():
The only thing missing will be the response.body which is not logged.
'urllib3' is the logger to get now (no longer 'requests.packages.urllib3').
Basic logging will still happen without setting http.client.HTTPConnection.debuglevel
When trying to get the Python logging system (import logging) to emit low level debug log messages, it suprised me to discover that given:
that only urllib3 actually uses the Python logging system:
Presumably redirecting stdout in some way might work to shoe-horn stdout into the logging system and potentially capture to e.g.
a log file.
To capture urllib3 debug information through the Python 3 logging system,  contrary to much advice on the internet, and as @MikeSmith points out, you won’t have much luck intercepting:
Here is some code which logs urllib3 workings to a log file using the Python logging system:
Remember this output uses print and not the Python logging system, and thus cannot be captured using a traditional logging stream or file handler (though it may be possible to capture output to a file by redirecting stdout).
To maximise all possible logging, you must settle for console/stdout output with this:
At this moment both request and response are fully defined, hence can be logged.
That's basically how to log all HTTP round-trips of a session.
For the logging above to be useful there can be specialised [logging formatter (hyper-link)] that understands req and res extras on logging records.
I'll show to use [Chronologer (hyper-link)] for that (which I'm the author of).
First, the hook has be rewritten to produce records that logging can serialise when sending over the wire.
Second, logging configuration has to be adapted to use [logging.handlers.HTTPHandler (hyper-link)] (which Chronologer understands).
Finally, run Chronologer instance.
Now if you open [http://localhost:8080/ (hyper-link)] (use "logger" for username and empty password for the basic auth popup) and click "Open" button, you should see something like:
I'm using a logger_config.yaml file to configure my logging, and to get those logs to show up, all I had to do was to add a disable_existing_loggers: False to the end of it.
My logging setup is rather extensive and confusing, so I don't even know a good way to explain it here, but if someone's also using a YAML file to configure their logging, this might help.
[https://docs.python.org/3/howto/logging.html#configuring-logging (hyper-link)]
Note: the [commit 5e1361c (hyper-link)] from [brian m. carlson (bk2204) (hyper-link)] (for git 1.9/2.0 Q1 2014) deals with a special case in term of log decoration with tags:
log: properly handle decorations with chained tags
git log did not correctly handle decorations when a tag object referenced another tag object that was no longer a ref, such as when the second tag was deleted.
The new 'YOUR_SYSTEM_USER' will have this auth plugin and you can login from the bash shell now with "mysql -u YOUR_SYSTEM_USER -p" and provide the password for this user on the prompt.
allowing a newly created MySql user to log into the MySql/MariaDB server:
Now you should be able to use a remote terminal client and securely log into mysql using the command:
Second step: login to your mysql default account
The correct way is to login to my-sql with sudo privilege.
Log in without a password.
so i relogin using sudo,
To enable access to root account, you need to login with your user name
Login as 'root'
Login into the MySQL in your machine using (sudo mysql -p -u root) and hit the following queries.
As this adds time, I added this string copy to the other two so that the times showed the difference only in the logic.
The problem is that calling [getLogger (hyper-link)] without arguments returns the root logger so when you set the level to logging.DEBUG you are also setting the level for other modules that use that logger.
You can solve this by simply not using the root logger.
this will create a new logger and thus it wont inadvertently change logging level for other modules.
Obviously you have to use logger.debug instead of logging.debug since the latter is a convenience function that calls the debug method of the root logger.
This is mentioned in the [Advanced Logging Tutorial (hyper-link)].
It also allows you to know which module triggered the log message in a simple way.
Conversely, you can use the getLogger() method to retrieve and reconfigure/disable the unwanted loggers.
I also wanted to add the logging.fileConfig() method accepts a parameter called disable_existing_loggers which will disable any loggers previously defined (i.e., in imported modules).
If you're going to use the python logging package, it's a common convention to define a logger in every module that uses it.
If a package uses this convention, it's easy to enable/disable logging for it, because the logger name will be the same name as the package (or will be a child of that logger).
You can even log it to the same file as your other loggers.
This disables all existing loggers, such as those created by imported modules, while still using the root logger (and without having to load an external file).
Note that you need to import all modules you don't want logged first!
Otherwise those won't be considered as "existing loggers".
It will then disable all loggers from those modules.
For more detailed examples using related options for configuration, see [https://gist.github.com/st4lk/6287746 (hyper-link)], and [here (hyper-link)] is a (partially working) example using YAML for config with the coloredlog library.
I was getting debug logs from matplotlib despite following the pretty straightforward documentation at the [logging advanced tutorial (hyper-link)]
and the [troubleshooting (hyper-link)].
I was initiating my logger in main() of one file and importing a function to create a plot from another file (where I had imported matplotlib).
This seemed counterintuitive to me so if anyone has insight into how you can set the config for a logger that hasn't been imported yet I'd be curious to find out how this works.
This will set my own module's log level to DEBUG, while preventing the imported module from using the same level.
Note:
"imported_module" can be replaced with imported_module.__name__ (without quotes), and "my_own_logger_name" can be replaced by __name__ if that's the way you prefer to do it.
I have a logging_config.py file which I import in all other py files.
In logging_config.py file I set root logger logging level to ERROR (by default its warning):
In other modules I import logging_config.py and declare a new logger and set its level to debug:
This way everything I log in my py files is logged, but stuff logged at debug and info level by imported modules like urllib, request,boto3 etc is not logged.
If there is some error in those import module then its logged, since I set root loggers level to ERROR.
Another thing to consider is the [propagate (hyper-link)] property of the Logger class.
logs logs about a module called sxbasics.py creationg a huge amount of logs
that because the propagation of the logs is True by default, setting to False, instead, i recovered 514MB of logs.
After trying various answers in this thread and other forums, I found this method efficient at silencing other modules' loggers.
[https://kmasif.com/2019-11-12-ignore-logging-from-imported-module/ (hyper-link)]
However, I found this to be a convenient and fast way to disabled other modules' loggers.
The accepted answer presents the git log --graph --all --decorate command, which is available as the glgga alias in oh-my-zsh.
In addition to the steps you have already taken, you will need to set the recovery mode to simple before you can shrink the log.
THIS IS NOT A RECOMMENDED PRACTICE for production systems... You will lose your ability to recover to a point in time from previous backups/log files.
Paul Randal has an exccellent discussion of this problem on his blog: [http://www.sqlskills.com/blogs/paul/post/backup-log-with-no_log-use-abuse-and-undocumented-trace-flags-to-stop-it.aspx (hyper-link)]
Within your database, locate the file_id of the log file using the following query.
In my instance, the log file is file_id 2.
Now we want to locate the virtual logs in use, and do this with the following command.
Here you can see if any virtual logs are in use by seeing if the status is 2 (in use), or 0 (free).
When shrinking files, empty virtual logs are physically removed starting at the end of the file until it hits the first used status.
This is why shrinking a transaction log file sometimes shrinks it part way but does not remove all free virtual logs.
To get around this do another transaction log backup, and immediately run these commands, supplying the file_id found above, and the size you would like your log file to be reduced to.
This will then show the virtual log file allocation, and hopefully you'll notice that it's been reduced somewhat.
Because virtual log files are not always allocated in order, you may have to backup the transaction log a couple of times and run this last query again; but I can normally shrink it down within a backup or two.
One of our heavily transacted databases grows a few hundred thousand records in a log table every day.
There are multiple log files that grow a few hundred GB every day.
We do take a full backup at the beginning and at the end of the process in order to overcome the issue of losing our ability to recover to a point in time from previous backups/log files.
Iterate through the log files and shrink each of them
If you want to change the default logging behavior, simply create a custom logger object that respond to all the Rails logger method:
[http://github.com/rails/rails/blob/9d7aae710384fb5f04129c35b86c5ea5fb9d83a9/activesupport/lib/active_support/buffered_logger.rb (hyper-link)]
Because it's your logger, you can decide to implement your personal logic.
Then, replace the default logger for every base class you want to customize.
You can easily create an initializer file called logger.rb and write there all your custom configurations.
In this way, the logger will be immediately replaced on Rails startup.
I use the rails ["exception logger" (hyper-link)], to log all problems to my database while my site is in production mode.
Are you satisfied with (1) having an access log in a DB (in real time), or (2) are you more interested in Rails/app-specific logging?
For (1), with Apache (at least), you can log to a database using piped logging.
[http://httpd.apache.org/docs/1.3/logs.html#piped (hyper-link)]
I wrote a program that runs in the background waiting for input, which it parses and logs to a Postgres DB.
My httpd.conf file pipes to this program with a CustomLog directive.
This is relatively simple to set up, and gives you all the obvious advantages of being able to analyze your logs in a DB.
However, you have to protect against sql injection, buffer overflows, and other security issues in the logging program.
If you want to log environment vars, or application data, or very selective bits of information, you could consider writing a web server module.
Depending on your exact needs, you could also get by with some combination of conditional logging directives and filtering in the logging program.
My company have been logging some structured traffic info straight into a MySQL log database.
However, our IT department have some growing concerns regarding to the scalability of the current setup and is suggesting that we offload the log info onto "proper" log-files.
The log-files will then be reinserted back into the same downstream database tables.
Here are some of pros and cons that I see regarding to the subject of log-files vs log-db (relational):
log-files are fast, reliable, and scalable (At least I have heard Yahoo!
makes heavy uses of log files for their click tracking analytics).
log-files are easy for sys-admin to maintain.
log-files can be very flexible since you can write almost anything to it.
log-files requires heavy parsing and potentially a map-reduced type of setup for data-extraction.
log-db structures are a lot closer to your application, making some feature's turn around time a lot shorter.
log-db can reduce logging noises and redundancies since log-files are insert only where as log-db gives you the ability to do update and associated-insert (normalization if you dare).
log-db can be fast and scalable too if you go with database partitioning and/or multi-log databases (rejoin data via downstream replications)
I think some stress tests on the log database are needed in my situation.
They can make the data-extraction process much simpler than parsing and map-reducing through gigs of log files.
[Storage of many log files (hyper-link)]
[Is writing server log files to a database a good idea?
[Using a SQL Server for application logging.
[Fast Search in Logs (hyper-link)]
[Separate production database for logging (hyper-link)]
[You Log to Your DB, Where Do You Log When Your DB is Down?
[rsyslog (hyper-link)] looks very interesting.
If you are using Ruby, you should have a look at the logging gem.
It provides multi-target logging capabilities.
i did develop a plugin to rsylog to save the logs not in files but at mongodb
the whole source code, from rsyslog + plugin is here [https://github.com/vpereira/rsyslogd-mongo (hyper-link)]
Having made the mistake of logging to a database recently myself, I feel I can offer one extremely good reason why you should not do this: Transactions.
Let's say you start a transaction, log a bunch of stuff during the course of the transaction, and ultimate you end up with an error condition.
You log the error condition, and oh hey.
Suddenly, everything you just logged is gone and you have no idea what happened or why.
In my case, the reason I logged things to the database was that I needed context-sensitive logs.
Essentially I needed to be able to look up all log entries related to a specific database model.
However, the right answer is to put those logs in some separate location that's a better fit for log data (and which, in my case, happens to be query-able).
You can also use git reflog to see what other commits your HEAD (or any other ref) has pointed to in the past.
This was motivated by the analogy to the shell idiom cd - to go back to whatever working directory one was previously in.
git reflog will show all the commits, but in this case, you just want the tip, so git checkout master.
git log --branches shows log of commits from all local branches
-1 limit to one commit → most recent commit
--pretty=format:"%H" format to only show commit hash
git checkout $(...) use output of subshell as argument for checkout
git log --branches shows log of commits from all local branches
-1 limit to one commit → most recent commit
--pretty=format:"%D" format to only show ref names
| sed 's/.
show all branches and commit 
git log --branches --oneline
show last commit 
git log --branches -1 --oneline
show before last commit 
git log --branches -2 --oneline
The simplest way to make a log-log plot from (probably) any seaborn plot is:
Also, git show accepts the same format string as git log:
Some of these fall into the category of general NLog (or logging) tips rather than strictly configuration suggestions.
Here are some general logging links from here at SO (you might have seen some or all of these already):
[log4net vs. Nlog (hyper-link)]
[Logging best practices (hyper-link)]
[What's the point of a logging facade?
[Why do loggers recommend using a logger per class?
Use the common pattern of naming your logger based on the class Logger logger = LogManager.GetCurrentClassLogger().
This gives you a high degree of granularity in your loggers and gives you great flexibility in the configuration of the loggers (control globally, by namespace, by specific logger name, etc).
Use non-classname-based loggers where appropriate.
Maybe you have one function for which you really want to control the logging separately.
Maybe you have some cross-cutting logging concerns (performance logging).
If you don't use classname-based logging, consider naming your loggers in some kind of hierarchical structure (maybe by functional area) so that you can maintain greater flexibility in your configuration.
So, you might request loggers like this:
With hierarchical loggers, you can configure logging globally (the "*" or root logger), by FA (Database, Analysis, UI), or by subarea (Database.Connect, etc).
Loggers have many configuration options:
See the [NLog help (hyper-link)] for more info on exactly what each of the options means.
Probably the most notable items here are the ability to wildcard logger rules, the concept that multiple logger rules can "execute" for a single logging statement, and that a logger rule can be marked as "final" so subsequent rules will not execute for a given logging statement.
If you roll your file daily, each file could be named "Monday.log", "Tuesday.log", etc.
It is easy and allows you to add your own context information to the log file via configuration.
For example, here is a layout renderer (based on NLog 1.x, not 2.0) that can add the Trace.CorrelationManager.ActivityId to the log:
Tell NLog where your NLog extensions (what assembly) like this:
See the NLog docs for more info on those.
Tell NLog to watch and automatically reload the configuration if it changes:
There are several configuration options to help with troubleshooting NLog
See NLog Help for more info.
NLog 2.0 adds LayoutRenderer wrappers that allow additional processing to be performed on the output of a layout renderer (such as trimming whitespace, uppercasing, lowercasing, etc).
Don't be afraid to wrap the logger if you want insulate your code from a hard dependency on NLog, but wrap correctly.
There are examples of how to wrap in the NLog's github repository.
Another reason to wrap might be that you want to automatically add specific context information to each logged message (by putting it into LogEventInfo.Context).
There are pros and cons to wrapping (or abstracting) NLog (or any other logging framework for that matter).
If you are considering wrapping, consider using [Common.Logging (hyper-link)].
It works pretty well and allows you to easily switch to another logging framework if you desire to do so.
Common.Logging does not currently support an abstraction for them, but it is supposedly in the queue of capabilities to add.
Basically, it buffers messages and only outputs those at a certain log level (e.g.
there has been an error, so the log level is >= Error), then it will output more info (e.g.
all messages from log levels >= Trace).
Because the messages are buffered, this lets you gather trace information about what happened before an Error or ErrorException was logged - very useful!
Note that the target-ref element used in the above-linked example cannot be used in NLog 1.0 (I am using 1.0 Refresh for a .NET 4.0 app); it is necessary to put your target inside the wrapper block.
Also note that the logic syntax (i.e.
&gt; and &lt;) or else NLog will error.
log (a log file or verbose message)
To send the data to the URL, I used NLog's [WebService target (hyper-link)].
Note: there may be some issues with the size of the log file, but I haven't figured out a simple way to truncate it (e.g.
[Nlog - Generating Header Section for a log file (hyper-link)]
The question wanted to know how to add a header to the log file.
Using config entries like this allow you to define the header format separately from the format of the rest of the log entries.
Use a single logger, perhaps called "headerlogger" to log a single message at the start of the application and you get your header:
Define the loggers:
Log each log level with a different layout
Similarly, the poster wanted to know how to change the format per logging level.
Configure NLog via XML, but Programmatically
Did you know that you can specify the NLog XML directly to NLog from your app, as opposed to having NLog read it from the config file?
Or, you could store your XML in a database, get it at app startup, and configure NLog directly with that XML (maybe checking back periodically to see if it had changed).
Log to Twitter
Based on [this post about a log4net Twitter Appender (hyper-link)], I thought I would try my hand at writing a NLog Twitter Target (using NLog 1.0 refresh, not 2.0).
PostMessageToTwitter is essentially the same as PostLoggingEvent in the orignal post.
Tell NLog the assembly containing the target:
Apparently, you can now use [NLog with Growl for Windows (hyper-link)].
Log from Silverlight
When using NLog with Silverlight you can send the trace to the server side via the [provided (hyper-link)] web service.
Easier Way To Log each log level with a different layout using Conditional Layouts
See [https://github.com/NLog/NLog/wiki/When-Filter (hyper-link)] for syntax
You are calling configure_logging twice (maybe in the __init__ method of Boy) : getLogger will return the same object, but addHandler does not check if a similar handler has already been added to the logger.
Or set up a flag logging_initialized initialized to False in the __init__ method of Boy and change configure_logging to do nothing if logging_initialized is True, and to set it to True after you've initialized the logger.
If your program creates several Boy instances, you'll have to change the way you do things with a global configure_logging function adding the handlers, and the Boy.configure_logging method only initializing the self.logger attribute.
Another way of solving this is by checking the handlers attribute of your logger:
Note: If you attach a handler to a logger and one or more of its
ancestors, it may emit the same record multiple times.
In general, you
should not need to attach a handler to more than one logger - if you
just attach it to the appropriate logger which is highest in the
logger hierarchy, then it will see all events logged by all descendant
loggers, provided that their propagate setting is left set to True.
A
common scenario is to attach handlers only to the root logger, and to
let propagation take care of the rest.
A call to logging.debug() calls logging.basicConfig() if there are no root handlers installed.
The default uses logging.BASIC_FORMAT that I didn't want.
It seems that if you output something to the logger (accidentally) then configure it, it is too late.
I believe this is because the first call to logging.warning creates a new handler automatically, and then I explicitly added another handler.
The problem went away when I removed the accidental first logging.warning call.
In my case I'd to set logger.propagate = False to prevent double printing.
In below code if you remove logger.propagate = False then you will see double printing.
I was getting a strange situation where console logs were doubled but my file logs were not.
Please be aware that third party packages can register loggers.
In many cases third party code checks to see if there are any existing root logger handlers; and if there isn't--they register a new console handler.
My solution to this was to register my console logger at the root level:
Namely, all log messages originating from any of the child processes got duplicated.
both in the child processes and also in the main process (since I wanted the main process to log stuff, too).
The reason this led to trouble (on my Linux machine) is that on Linux the child processes got started through forking and therefore inherited the existing log handlers from the main process.
Logger objects like the root logger root have a handlers property but it is undocumented.
The root logger gets modified by packages like pytest, so root.handlers and root.hasHandlers() are not very reliable to begin with.
Or to use an alternative to the logging package that doesn't rely on global state and instead requires you to do proper dependency injection but I'm digressing… :)
Yes, NSLog outputs on the device.
NSLog is written to device log in production release and you can check this by connecting your iPhone to your system and using Organizer.
Select your iPhone in the organizer, click Device Logs.
You would see all NSLog outputs in the log.
If you use Testflight SDK, you can capture all logs with their [Remote Logging feature (hyper-link)].
Just add this block of code in application:didFinishLaunchingWithOptions method in the app delegate file and it will create a log file in app document directory on iPhone which logs all console log events.
To get Logfiles :
Launch itunes, after your device has connected select Apps - select your App - in Augument Document you will get your file.
It pretty much gives you all the options to see or access logs of the device whether or not they are connected to your dev machine.
In Xcode 6.1.1, you can view the NSLog output by doing the following.
However, I'm not sure if it lets you see logs from too far back in time.
Just add this block of code in application:didFinishLaunchingWithOptions method in the app delegate file and it will create a log file in app document directory on iPhone which logs all console log events.
To get Logfiles : Launch iTunes, after your device has connected
  select Apps - select your App - in Augument Document you will get your
  file.
I know this is an old thread but you can also have access to the device logs going to:
I think in Xcode 9.3 the device log screen has been moved to a new location.Kindly refer the following link.
[Get device logs at runtime in Xcode (hyper-link)]
The external log deletion could happen while docker is writing json formatted data to the file, resulting in a partial line, and breaking the ability to read any logs from the docker logs cli.
after emptying the logfile, I get this error: error from daemon in stream: Error grabbing logs: invalid character '\x00' looking for beginning of value
Instead, you can have Docker automatically rotate the logs for you.
This is done with additional flags to dockerd if you are using the default [JSON logging driver (hyper-link)]:
Note, existing containers need to be deleted and recreated to receive the new log limits.
Similar log options can be passed to individual containers to override these defaults, allowing you to save more or fewer logs on individual containers.
For additional space savings, you can switch from the json log driver to the "local" log driver.
This allows you to store more logs in the same sized file.
The downside of the local driver is external log parsers/forwarders that depended on direct access to the json logs will no longer work.
You can either limit the log's size, or use a script to delete logs related to a container.
You can find scripts examples here (read from the bottom): [Feature: Ability to clear log history #1083 (hyper-link)]
Check out the [logging section (hyper-link)] of the docker-compose file reference, where you can specify options (such as log rotation and log size limit) for some logging drivers.
Jeff S. [How to clear the logs properly for a Docker container?
You can set up logrotate to clear the logs periodically.
Example file in /etc/logrotate.d/docker-logs

Find log file path by: 
$ docker inspect | grep log

Delete the log file content:
$ echo "" > log_file_path_from_step1
The first line gets the log file path, similar to the accepted answer.
The command we run is the familiar truncate -s0 $LOGPATH from non-Mac answers.
(And thus, deleting the log is probably unnecessary)
On my Ubuntu servers even as sudo I would get Cannot open ‘/var/lib/docker/containers/*/*-json.log’ for writing: No such file or directory
You can also supply the log-opts parameters on the docker run command line, like this:
Credits: [https://medium.com/@Quigley_Ja/rotating-docker-logs-keeping-your-overlay-folder-small-40cfa2155412 (hyper-link)] (James Quigley)
This grabs each Container ID listed by docker ps (will erase your logs for any container on that list!
), pipes it into xargs and then echoes a blank string to replace the log path of the container.
Here is a cross platform solution to clearing docker container logs:
To remove/clear  docker container logs we can use below command
$(docker inspect  container_id|grep "LogPath"|cut -d """ -f4)
or
$(docker inspect  container_name|grep "LogPath"|cut -d """ -f4)
If you have several containers and you want to remove just one log but not others.
Remove just that log:
> /var/lib/docker/containers/E1X2A3M4P5L6*/E1X2A3M4P5L6*-json.log (Replace E1X2A3M4P5L6 for your result !! )
As you can see, inside /containers are the containers, and logs has the same name but with -json.log at the end.
Thanks to [answer by @BMitch (hyper-link)], I've just wrote a shell script to clean logs of all the containers:
Save the current log, otherwise you may get problems coming back to the current version
$ git log > ../git.log
Since a different logarithm base is equivalent to a constant coefficient, it is superfluous.
That said, I would probably assume log base 2.
Big O notation is not affected by logarithmic base, because all logarithms in different bases are [related by a constant factor (hyper-link)], O(ln n) is equivalent to O(log n).
However, during the derivation of the O() polynomial, in the case of binary search, only log2 is correct.
Also, as a matter of my opinion, writing O(log2 N) is better for your example, because it better communicates the derivation of the algorithm's run-time.
Converting from one logarithm base to another involves multiplying by a constant factor.
So O(log N) is equivalent to O(log2 N) due to a constant factor.
However, if you can easily typeset log2 N in your answer, doing so is more pedagogical.
In the case of binary tree searching, you are correct that log2 N is introduced during the derivation of the big-O() runtime.
When deriving the polynomial to be communicated via big-O notation, it would be incorrect for this example to use a logarithm other than log2 N, prior to applying the O()-notation.
As soon as the polynomial is used to communicate a worst-case runtime via big-O() notation, it doesn't matter what logarithm is used.
When developing an intuition about tree structures, it's helpful to understand that a binary search tree can be searched in O(n log n) time because that is the height of the tree - that is, in a binary tree with n nodes, the tree depth is O(n log n) (base 2).
If each node has three children, the tree can still be searched in O(n log n) time, but with a base 3 logarithm.
so let f(n) = log base a of n, where a > 1 and g(n) = log base b of n, where b > 1
Now we get the following: log base a of n is said to be O(log base b of n) iff |log base a of n| <= C * |log base b of n| whenever n > k
Choose k=0, and C= log base a of b.
Now our equation looks like the following: |log base a of n| <= log base a of b * |log base b of n| whenever n > 0
Notice the right hand side, we can manipulate the equation: = log base a of b * |log base b of n| = |log base b of n| * log base a of b = |log base a of b^(log base b of n)| = |log base a of n|
Now our equation looks like the following: |log base a of n| <= |log base a of n| whenever n > 0
So log base a of n is O(log base b of n) and since a,b doesn't matter we can simply omit them.
You can read an article on it here: [https://medium.com/@randerson112358/omitting-bases-in-logs-in-big-o-a619a46740ca (hyper-link)]
Most of Scala's logging libraries have been some wrappers around a Java logging framework (slf4j, log4j etc), but as of March 2015, the surviving log libraries are all slf4j.
These log libraries provide some sort of log object to which you can call info(...), debug(...), etc.
I'm not a big fan of slf4j, but it now seems to be the predominant logging framework.
The Simple Logging Facade for Java or (SLF4J) serves as a simple facade or abstraction for various logging frameworks, e.g.
java.util.logging, log4j and logback, allowing the end user to plug in the desired logging framework at deployment time.
The ability to change underlying log library at deployment time brings in unique characteristic to the entire slf4j family of loggers, which you need to be aware of:
The way slf4j knows which underlying logging library you are using is by loading a class by some name.
I've had issues in which slf4j not recognizing my logger when classloader was customized.
Because the simple facade tries to be the common denominator, it's limited only to actual log calls.
In a large project, it could actually be convenient to be able to control the logging behavior of transitive dependencies if everyone used slf4j.
[Scala Logging (hyper-link)] is written by Heiko Seeberger as a successor to his [slf4s (hyper-link)].
It uses macro to expand calls into if expression to avoid potentially expensive log call.
Scala Logging is a convenient and performant logging library wrapping logging libraries like SLF4J and potentially others.
[Logula (hyper-link)], a Log4J wrapper written by Coda Hale.
[configgy (hyper-link)], a java.util.logging wrapper that used to be popular in the earlier days of Scala.
You should have a look at the scalax library :
[http://scalax.scalaforge.org/ (hyper-link)]
In this library, there is a Logging trait, using sl4j as backend.
By using this trait, you can log quite easily (just use the logger field 
in the class inheriting the trait).
Haven't tried it yet, but Configgy looks promising for both configuration and logging:
I pulled a bit of work form the Logging trait of scalax, and created a trait that also integrated a MessageFormat-based library.
A good example of this is Lift's logger
[Log.scala (hyper-link)]
[Slf4jLog.scala (hyper-link)]
If you have a separate trait that mixes this Log4JLogger into your class, then you can do
After using slf4s and logula for a while, I wrote loglady, a simple logging trait wrapping slf4j.
It offers an API similar to that of Python's logging library, which makes the common cases (basic string, simple formatting) trivial and avoids formatting boilerplate.
[http://github.com/dln/loglady/ (hyper-link)]
I use SLF4J + Logback classic and apply it like this:
but this approach of course uses a logger instance per class instance.
It's done with [system properties (hyper-link)], which you can set either by appending something like -Dorg.slf4j.simplelogger.defaultlog=trace to execution command or hardcode in your script: System.setProperty("org.slf4j.simplelogger.defaultlog", "trace").
For instance to set the logging level to "trace" in a specific run configuration in IDEA just go to Run/Debug Configurations and add -Dorg.slf4j.simplelogger.defaultlog=trace to VM options.
With Scala 2.10+ Consider ScalaLogging by Typesafe.
[https://github.com/typesafehub/scala-logging (hyper-link)]
Fortunately Scala macros can be used to make our lives easier: ScalaLogging offers the class Logger with lightweight logging methods that will be expanded to the above idiom.
In addition ScalaLogging offers the trait Logging which conveniently provides a Logger instance initialized with the name of the class mixed into:
I find very convenient using some kind of java logger, sl4j for example, with simple scala wrapper, which brings me such syntax
In my opinion very usefull mixin of java proven logging frameworks and scala's fancy syntax.
This is how I got [Scala Logging (hyper-link)] working for me:
Then, after doing an sbt update, this prints out a friendly log message:
If you are using Play, you can of course simply import play.api.Logger for writing log messages: Logger.debug("Hi").
I was really surprised that [Scribe (hyper-link)] logging framework that I use at work isn't even mentioned here.
What is more, it doesn't even appear on the first page in Google after searching "scala logging".
it is really fast, check comparison here: [https://www.matthicks.com/2018/02/scribe-2-fastest-jvm-logger-in-world.html (hyper-link)]
After catching an exception, start a new activity to ask the user to send
a log.
Extract the log info from logcat's files and write to your
own file.
Optionally, setup Proguard to strip out Log.d() and Log.v().
(1 & 2) Handle uncaughtException, start send log activity:
(3) Extract log (I put this an my SendLog Activity):
(4) Start an email app (also in my SendLog Activity):
(3 & 4) Here's what SendLog looks like (you'll have to add the UI, though):
You must specify "optimize" or Proguard will not remove Log.v() and Log.d() calls.
This tell Proguard to assume Log.v and Log.d have no side effects (even though they do since they write to the logs) and thus can be removed during optimization:
But one observation here, instead of writing into file using File Writer and Streaming, I made use of the logcat -f option directly.
Using File streaming gave me one issue that it was not flushing the latest logs from buffer.
Instead of using Logcat, the Throwable is sent to SEND_LOG via intent putExtra.
Then loop though the stack trace in SEND_LOG.
SEND_LOG
After a bit more research, I found out "why it works so strangely":
It turns out that when you don't specify a date format, [git log defaults to either the author's timezone or commit dates (hyper-link)], meaning for consistent behavior, it's useful to explicitly declare your date format with something like:
The three-argument Android log methods will print the stack trace for an Exception that is provided as the third parameter.
According to [this comment (hyper-link)] those Log methods "use the getStackTraceString() method ... behind the scenes" to do that.
I don't think you're running the logcat correctly.
/home/dan/android-sdk-linux_x86/tools/adb logcat
If you want to log this output then you need to follow the instructions "Viewing stdout and stderr" shown [here (hyper-link)]
You can make use of the helper function getStackTraceString() belonging to the android.util.Log class to print the entire error message on console.
Then replace your Write-host calls with LogWrite.
Add's timestamps - can't have a log without timestamps.
If you don't set a log destination, it simply pumps it out.
Gist with log rotation: [https://gist.github.com/barsv/85c93b599a763206f47aec150fb41ca0 (hyper-link)]
Using this [Log-Entry framework: (hyper-link)]
Log File (at D:\Apps\Logs\<computername>.log):
Log files are numbered with leading '0' but retain their file extension.
I keep the logger script in a central share and make a local copy if it has changed, or load it from the central location if needed.
First I import the logger:
Define the log file:
What I log depends on debug levels that I created:
I would probably consider myself a bit of a "hack" when it comes to coding so this might not be the prettiest but here is my version of logger.ps1:
You might just want to use the new TUN.Logging PowerShell module, this can also send a log mail.
Just use the Start-Log and/or Start-MailLog cmdlets to start logging and then just use Write-HostLog, Write-WarningLog, Write-VerboseLog, Write-ErrorLog etc.
to write to console and log file/mail.
Then call Send-Log and/or Stop-Log at the end and voila, you got your logging.
Or just follow the link: [https://www.powershellgallery.com/packages/TUN.Logging (hyper-link)]
Documentation of the module can be found here: [https://github.com/echalone/TUN/blob/master/PowerShell/Modules/TUN.Logging/TUN.Logging.md (hyper-link)]
This extension has lot more useful features which not only helps to generate report, but also help to log your work on daily basis with notifications, calendar integrations and lot more cool features which helps both managers and team members in their daily activity:
Options 1:
Create filter like following JQL:
worklogDate > startofWeek(-1w) AND worklogAuthor = john.smith
Then using worklog "Pie Chart" widget to sum.
Options 2:
 Use filter like given above to create Agile board and leverage "TimeTrackingReport" or "WorkLog" reports.
Please bear in mind that Worklog report can be narrowed by User but does not give much flexibility  [ (hyper-link)]
worklogs and displays the sums in the issue navigator, dashboard gadgets or custom fields
Also, as a note, if you try to log a message without passing the dict, then it will fail.
You could use a [LoggerAdapter (hyper-link)] so you don't have to pass the extra info with every logging call:
logs (something like)
produces a similar log record.
Another way is to create a custom LoggerAdapter.
And in your code, you would create and initialize your logger as usual:
Using mr2ert's answer, I came up with this comfortable solution (Though I guess it's not recommended) - Override the built-in logging methods to accept the custom argument and create the extra dictionary inside the methods:
As of Python3.2 you can now use [LogRecordFactory (hyper-link)]
You do not need to pass your logger around the application
It actually works with 3rd party libraries that use their own logger (by just calling logger = logging.getLogger(..)) would now have the same log format.
(this is not the case with Filters / Adapters where you need to be using the same logger object)
import logging;
class LogFilter(logging.Filter):
logging.basicConfig(format='[%(asctime)s:%(levelname)s]::[%(module)s -> %(name)s] - APP_CODE:%(app_code)s - MSG:%(message)s');
class Logger:
class Test:
    logger = Logger.getLogger('Test')
In the code below, I'm inducing an extra key called claim_id in the logger format.
It will log the claim_id whenever there is a claim_id key present in the environment.
In my use case, I needed to log this information for an AWS Lambda function.
The accepted answer did not log the format in logfile, whereas the format was reflected in sys output.
If you need a default extra mapping, and you want to customize it for ad-hoc log messages, this works in Python 2.7+ by creating a LoggerAdapter that merges a default extra dictionary with any extra from a given message.
I have also posted a detailed solution on my [blog (hyper-link)] which you may want to refer.
I know this is late but I ended up here with a search for my error 500 with DEBUG=False, in my case it did turn out to be the ALLOWED_HOSTS but I was using os.environ.get('variable') to populate the hosts, I did not notice this until I enabled logging, you can log all errors to file with the below and it will log even when DEBUG=False:
If you are having a 500 error after setting DEBUG=False, you can always run the manage.py runserver in the command line to see any errors that wont appear in any web error logs.
A look in the logs showed me the problem.
Moral of the story: always log errors and always check logs.
Thanks to @squarebear, in the log file, I found the error:
ValueError: The file 'myapp/styles.css' could not be found with <whitenoise.storage.CompressedManifestStaticFilesStorage ...>.
It wasn't allowed host (which should throw a 400), everything else checked out, finally did some error logging only to discover that some missing / or messed up static files manifest (after collectstatic) were screwing with the setup.
See the [Log Levels section (hyper-link)] of the reference guide.
You simply have to use the normal configuration for your logging framework (log4j, logback) for that.
Add the appropriate config file (log4j.xml or logback.xml) to the src/main/resources directory and configure to your liking.
You can enable debug logging by specifying --debug when starting the application from the command-line.
Spring Boot provides also a nice starting point for logback to configure some defaults, coloring etc.
the [base.xml (hyper-link)] file which you can simply include in your logback.xml file.
(This is also recommended from the default [logback.xml (hyper-link)] in Spring Boot.
Making sure Dave Syer tip gets some love, because adding debug=true to application.properties will indeed enable debug logging.
logging.level.=ERROR -> Sets the root logging level to error
...
logging.level.=DEBUG -> Sets the root logging level to DEBUG

logging.file=${java.io.tmpdir}/myapp.log -> Sets the absolute log file path to TMPDIR/myapp.log

A sane default set of application.properties regarding logging using profiles would be:
application.properties:
This will give you error only logging in production and debug logging during development WITHOUT writing the output to a log file.
In case you want to use a different logging framework, log4j for example, I found the easiest approach is to disable spring boots own logging and implement your own.
That way I can configure every loglevel within one file, log4j.xml (in my case) that is.
Please note, that this example only covers log4j.
That's all, now you're all set to configure logging for boot within your log4j config file!
If the only change you need to make to logging is to set the levels of various loggers then you can do that in application.properties using the "logging.level" prefix, e.g.
logging.level.org.springframework.web: DEBUG
logging.level.org.hibernate: ERROR
You can also set the location of a file to log to (in addition to the console) using "logging.file".
To configure the more fine-grained settings of a logging system you need to use the native configuration format supported by the LoggingSystem in question.
classpath:logback.xml for Logback), but you can set the location of the config file using the "logging.config" property.
Then you can set the logging level for classes inside your project as given below in application.properties files
logging.level.com.company.myproject = DEBUG
logging.level.org.springframework.web = DEBUG and logging.level.org.hibernate = DEBUG will set logging level for classes of Spring framework web and Hibernate only.
For setting the logging file location use
logging.file = /home/ubuntu/myproject.log
If you want to set more detail, please add a log config file name "logback.xml" or "logback-spring.xml".
The proper way to set the root logging level is using the property logging.level.root.
If you are on Spring Boot then you can directly add following properties in application.properties file to set logging level, 
customize logging pattern and to store logs in the external file.
These are different logging levels and its order from minimum << maximum.
Please pass through this link to customize your log more vividly.
[https://docs.spring.io/spring-boot/docs/current/reference/html/boot-features-logging.html (hyper-link)]
in spring boot project we can write logging.level.root=WARN but here problem is, we have to restart again even we added devtools dependency, in property file if we are modified any value will not autodetectable, for this limitation i came to know the solution i,e we can add actuator in pom.xml and pass the logger level as below shown in postman client
in url bar [http://localhost:8080/loggers/ROOT (hyper-link)] or  [http://localhost:8080/loggers/com.mycompany (hyper-link)]
and in the body you can pass the json format like below
I just want to share with you a new spring boot feature allowing to group logs and set logging level on the whole group.
Create a logging group
Set the logging level for group
You can try setting the log level to DEBUG it will show everything while starting the application
With Springboot 2 you can set the root logging Level with an Environment Variable like this:
Or you can set specific logging for packages like this:
We can also turn on DEBUG log via command line like below:-
Apparently there isn't an out-of-box way from AWS Console where you can download the CloudWatchLogs.
Perhaps you can write a script to perform the CloudWatchLogs fetch using the SDK / API.
The good thing about CloudWatchLogs is that you can retain the logs for infinite time(Never Expire); unlike the CloudWatch which just keeps the logs for just 14 days.
More information about the CloudWatchLogs API,
[http://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/Welcome.html (hyper-link)]
[http://awsdocs.s3.amazonaws.com/cloudwatchlogs/latest/cwl-api.pdf (hyper-link)]
The latest AWS CLI has a CloudWatch Logs cli, that allows you to download the logs as JSON, text file or any other output supported by AWS CLI.
For example to get the first 1MB up to 10,000 log entries from the stream a in group A to a text file, run:
That last part, if you set TAIL will continue to fetch log events and will report newer events as they come in (with some expected delay).
There is also a python project called awslogs, allowing to get the logs: [https://github.com/jorgebastida/awslogs (hyper-link)]
list log groups:
list streams for given log group:
get the log records from all streams:
get the log records from specific stream :
and much more (filtering for time period, watching log streams...
It seems AWS has added the ability to export an entire log group to S3.
I would add that one liner to get all logs for a stream :
I saved it as getLogs.sh and invoke it with ./getLogs.sh log-group log-stream
You can now perform exports via the Cloudwatch Management Console with the new Cloudwatch Logs Insights page.
Full documentation here [https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ExportQueryResults.html (hyper-link)].
I had already started ingesting my Apache logs into Cloudwatch with JSON, so YMMV if you haven't set it up in advance.
These
  queries count toward your limit of four concurrent CloudWatch Logs
  Insights queries.
Choose one or more log groups and run a query.
Choose one or more log groups and run a query.
[https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3ExportTasks.html (hyper-link)]
This laid down steps for exporting logs from Cloudwatch to S3
runaswslog -1w gets last week and so on
awslogs.sh:
parselogline.py:
This will log all executed collection methods and their arguments to the console.
You can also set debug logger parameters:
but this will only log queries, not insert or update statements.
or any other logger of your choice:
This is not currently a feature of the CLI since it just exposes the HTTP API for CloudWatch Logs.
Because CloudWatch logs can be delayed (i.e.
This script uses aws logs get-log-events for which you must specify a valid stream_name.
Or if you want to tail an entire log group, this script uses aws logs filter-log-events without a stream name:
Have a look at [awslogs (hyper-link)].
If you happen to be working with Lambda/API Gateway specifically, have a look at [apilogs (hyper-link)].
I've just discovered [cwtail (hyper-link)] and it works well (to watch a lambda function's CloudWatch logs).
To list log groups:
Then, once you've picked which log group to 'tail':
I was really disappointed with awslogs and cwtail so I made my own tool called [Saw (hyper-link)] that efficiently streams CloudWatch logs to the console (and colorizes the JSON output):
Got a Lambda you want to see error logs for?
Saw is great because the output is easily readable and you can stream logs from entire log group, not just a single stream in the group.
You can use [awslogs (hyper-link)], a python package to tail aws logwatch logs.
You can also filter logs with matching patterns.
See [project readme (hyper-link)] for more information on using awslogs.
To tail CloudWatch Logs effectively I created a tool called [cw (hyper-link)].
Those other tools mentioned above do provide a tailing feature, however, 
I tried all these tools, awslogs, cwtail and found them frustrating.
They were slow to download events, often unreliable and not helpful in displaying JSON log data and were primitive with query options.
I wanted an extremely fast, simple log viewer that would allow me to instantly and easily see application errors and status.
The CloudWatch logs viewer is slow and CloudWatch Insights can take > 1m for some pretty basic queries.
So I created SenseLogs, a free AWS CloudWatch Logs viewer that runs entirely in your browser.
SenseLogs transparently downloads log data and stores events in your browser application cache for immediate viewing, smooth infinite scrolling and full text queries.
SenseLogs has live tail with infinite back scrolling.
See [https://github.com/sensedeep/senselogs/blob/master/README.md (hyper-link)] for details.
Note that tailing an aws log is now a supported feature of the official awscli, albeit only in awscli v2, which is not released yet.
Tailing and following the logs (like tail -f) can now be accomplished by something like:
In addition to tailing the logs, it allows viewing the logs back to a specified time using the --since parameter, which can take an absolute or relative time
Usage: save the file, chmod +x it, and then run it: ./cloudwatch-logs-tail.sh log-group-name.
It queries CloudWatch Logs to get all log entries in the specified time interval, and displays those which do not match our list of event IDs.
The script polls every few seconds (set by LOG_INTERVAL in the script), and keeps polling for one more interval past the end time to account for the delay between log ingestion and availability.
Note that this script is not going to be great if you want to keep tailing the logs for more than a few minutes at a time, because the query results that it gets from AWS will keep getting bigger with every added log item.
AWS allows you to tail the logs now.
if you are using ElasticBeanStalk with app name myapp-prd and want to tail web1.log it would be
git-log takes a revision list.
To get a log containing just x, y, and z, try git log HEAD..branch (two dots, not three).
This is identical to git log branch --not HEAD, and means all commits on branch that aren't on HEAD.
To see the log of the current branch since branching off master:
git log master...
If you are currently on master, to see the log of a different branch since it branched off master:
git log ...other-branch
You can set sys.stdout = Logger() where Logger is a class whose write method (immediately, or accumulating until a \n is detected) calls logging.info (or any other way you want to log).
The log_to_stderr() function is the simplest solution.
multiprocessing has a convenient module-level function to enable logging called log_to_stderr().
It sets up a logger object using logging and adds a handler so that log messages are sent to the standard error channel.
By default, the logging level is set to NOTSET so no messages are produced.
Pass a different level to initialize the logger to the level of detail desired.
log rotation every day: Use a [TimedRotatingFileHandler (hyper-link)]
compression of logs: Set the encoding='bz2' parameter.
optional - delete oldest log file to preserve X MB of free space.
By setting the maxBytes parameter, the log file will rollover when it reaches a certain size.
The two parameters together allow you to control the maximum space consumed by the log files.
When you run the script below, it will write log files to /tmp/log_rotate*.
With a small value for time.sleep (such as 0.1), the log files fill up quickly, reach the maxBytes limit, and are then rolled over.
With a large time.sleep (such as 1.0), the log files fill up slowly, the maxBytes limit is not reached, but they roll over anyway when the timed interval (of 10 seconds) is reached.
All the code below comes from [logging/handlers.py (hyper-link)].
The other way to compress logfile during rotate (new in python 3.3) is using BaseRotatingHandler (and all inherited) class attribute rotator for example:
To copy the file, gzip the copied file (using epoch time), and then clearing out the existing file in a way that won't upset the logging module:
I created a new class inheriting logging.handlers.RotatingFileHandler class and added a couple of lines to gzip the file before moving it.
[https://github.com/rkreddy46/python_code_reference/blob/master/compressed_log_rotator.py (hyper-link)]
I think that the best option will be to use current implementation of TimedRotatingFileHandler and after renaming log file to the rotated version just compress it:
Here is my solution(modified from [evgenek (hyper-link)]), simple and does not block python code while gzipping huge log files:
I have added below one solution where i am basically zipping old
backup logs to zip with timestamp on it.
we have logs like this.
once count of backup logs hits to 5 it triggers to zip a.log.5 and a.log.4 to above zip and continues.
If it's just on your own local device, you can use LogCat.
Even if the device wasn't connected to a host machine when the crash occurred, connecting the device and issuing an adb logcat command will download the entire logcat history (at least to the extent that it is buffered which is usually a loooot of log data, it's just not infinite).
Make sure you are in the debug perspective (top right)
You may have to hit 'Resume' (F8) a few times for the log to print.
The crash log will be in the Logcat window at the bottom- double click for fullscreen and make sure you scroll to the bottom.
It's much more than just a crashlog, in fact it is much more: logging, reporting of problems by testers, crashlogs.
Here is another solution for Crash Log.
[http://kpbird.blogspot.com/2011/08/android-application-crash-logs.html (hyper-link)]
If you're just looking for the crash log while your phone is connected to the computer, use the DDMS view in Eclipse and the report is right there in LogCat within DDMS when your app crashes while debugging.
This is from [http://www.herongyang.com/Android/Debug-adb-logcat-Command-Debugging.html (hyper-link)]
It logs all the crashes occured along with the exact line of code that caused the crash along with automated markers that show you the steps the user took prior to the crash and more.
Try Carsh log app from android.
[Crash Reporter is a handy tool to capture all your crashes and log them in device locally (hyper-link)]
Plus it also provides API for capture Logged Exceptions using below method.
Here is a solution that can help you dump all the logs onto a text file
3) Logcat from that directory (in your terminal) to generate a constant flow of logs (for Mac):
./adb logcat
4) Open your app that crashes to generate crash logs
5) Ctrl+C to stop terminal and look for the logs associated with the app that crashes.
Most of the solution comes from 
[How do I configure Spring and SLF4J so that I can get logging?
Add the following line in log4j.properties : log4j.logger.httpclient.wire=DEBUG
Make sure spring doesn't ignore your logging config
There is no commons-logging library in your classpath : this can be done by adding the exclusion descriptors in your pom :
[code snippet]
The log4j.properties file is stored somewhere in the classpath where spring can find/see it.
If you have problems with this, a last resort solution would be to put the log4j.properties file in the default package (not a good practice but just to see that things work as you expect)
Besides the HttpClient logging described [in the other answer (hyper-link)], you can also introduce a ClientHttpRequestInterceptor that reads the body of the request and the response and logs it.
You might want to do this if other stuff also uses the HttpClient, or if you want a custom logging format.
Create LoggingRequestInterceptor to log requests responses
Assuming RestTemplate is [configured (hyper-link)] to use HttpClient 4.x, you can read up on HttpClient's logging documentation [here (hyper-link)].
The loggers are different than those specified in the other answers.
The logging configuration for HttpClient 3.x is available [here (hyper-link)].
Then instantiate RestTemplate using a BufferingClientHttpRequestFactory and the LoggingRequestInterceptor:
mjj1409 gets most of it, but conveniently avoids the issue of logging the response, which takes a bit more work.
Sofiene got the logging but with a critical problem: the response is no longer readable because the input stream has already been consumed!
The trick of configuring your RestTemplate with a BufferingClientHttpRequestFactory doesn't work if you are using any ClientHttpRequestInterceptor, which you will if you are trying to log via interceptors.
Long story short... just use this class in place of RestTemplate (note this uses the SLF4J logging API, edit as needed):
probably you will not be able to log the response if an Error comes .
Your best bet is to add logging.level.org.springframework.web.client.RestTemplate=DEBUG to the application.properties file.
Other solutions like setting log4j.logger.httpclient.wire will not always work because they assume you use log4j and Apache HttpClient, which is not always true.
or just logging.level.org.apache.http.wire=DEBUG which seems to contain all of the relevant information
my logger config used xml
It contains a LoggingRequestInterceptor class you can add that way to your RestTemplate:
and add an slf4j implementation to your framework like log4j.
In which case, you will have log those as well by implementing ResponseErrorHandler as follows.
You can use [spring-rest-template-logger (hyper-link)] to log RestTemplate HTTP traffic.
Ensure that debug logging is enabled in application.properties:
Now all RestTemplate HTTP traffic will be logged to org.hobsoft.spring.resttemplatelogger.LoggingCustomizer at debug level.
application.yml
logging:
  level:
      org.springframework.web.client.RestTemplate: DEBUG
Check the package of LoggingRestTemplate, for example in application.yml:
logging:
  level:
      com.example.logging: DEBUG
Check the package of LoggingRestTemplate, for example in application.yml:
logging:
level:
    org.apache.http: DEBUG
Than you can return that new ClientHttpResponse object to the rest template execution chain and you can log response using body byte array that is previously stored.
I apologize for all the missing semi-colons, this is written in Groovy.
Here's a rest template bean that's very agile and will log everything like the OP is looking for.
Custom Logging Interceptor Class:
Refer the Q/A for logging the request and response for the rest template by enabling the multiple reads on the HttpInputStream
This might not be the correct way to do it, but I think this is the most simple approach to print requests and responses without filling too much in logs.
By adding below 2 lines application.properties logs all requests and responses 1st line in order to log the requests and 2nd line to log the responses.
Instead of using the BufferingClientHttpRequestFactory when setting up the request, the interceptor itself can wrap the response and make sure the content is retained and can be repeatedly read (by the logger as well as by the consumer of the response):
logs in a more compact way
logs the status code identifier as well (e.g.
includes a request sequence number allowing to easily distinguish concurrent log entries from multiple threads
Example log output:
I was surprised that Spring Boot, with all it's Zero Configuration magic, doesn't provide an easy way to inspect or log a simple JSON response body with RestTemplate.
logging.level.org.apache.http=DEBUG
If you're using Spring Boot, you'll need to make sure you have a logging framework set up, e.g.
by using a spring-boot-starter dependency that includes spring-boot-starter-logging.
Although this doesn't meet the stated requirements of logging the body, it's a quick and simple way to start logging your REST calls.
Simply add the following line to your application.properties file (assuming you're using Spring Boot, and assuming you are using a spring boot starter dependency that includes spring-boot-starter-logging)
logging.level.org.springframework.web.client.RestTemplate=DEBUG
For logging to Logback with help from Apache HttpClient:
To log requests and responses, add to Logback configuration file:
Or to log even more:
org.apache.http.wire gives too unreadable logs, so I use [logbook (hyper-link)] to log application Servlet and RestTemplate requests & responses with payloads.
This will work, although logging the entire model will grow your log rather quickly.
The true in the second parameter of the print_r() method returns the information instead of printing it, which allows the Log facade to print it like a string.
Yo may wonder how's this possible, first param of debug method (as well as error, notice and other logging methods in Log class) accepts string as first param, and we are passing the array.
So, the answer lays down deep in the log writer class.
Also to clarify things little bit more, you can take a look into: 
[https://github.com/laravel/framework/blob/5.4/src/Illuminate/Log/Writer.php#L199 (hyper-link)] and you'll see that formateMessage method is formatting the message every time.
You can log either by print_r or json_encode.
At least in Laravel 8 there is no need to use print_r() nor json_encode() in your log statements.
I'd recommend that you have a look into logging solutions (Such as NLog, log4net or the Microsoft patterns and practices Enterprise Library) which may achieve your purposes and then some.
Replace Console.WriteLine with your Log method.
But you may need that for your Log(string msg) method.
The role you have created is not allowed to log in.
You have to give the role permission to log in.
One way to do this is to log in as the postgres user and update the role:
Once you are logged in, type:
Now there you can enable or disable login, roles and other options
Jailbroken device with syslogd + openSSH
So, to get the 2º one you just need to install syslogd and OpenSSH from Cydia, restart required after to get syslogd going; now just open a ssh session to your device (via terminal or putty on windows), and type "tail -f /var/log/syslog".
And there you go, wireless real time system log.
This is a open-source program that displays the iDevice's system log in Terminal (in a manner similar to tail -F).
What's particularly good about this solution is you can view the log whether or not the app was launched in debug mode from XCode.
Up pops a real-time display of the device's system log.
With it being a console app, you can filter the log using unix commands, such as grep
For instance, see all log messages from a particular app:
Taken from my blog at [http://pervasivecode.blogspot.co.uk/2012/06/view-log-output-of-any-app-on-iphone-or.html (hyper-link)]
Its idevicesyslog tool works similarly to deviceconsole (below), and it supports wirelessly viewing your device's syslog (!)
As an alternative, you can use an on-screen logging tool like [ticker-log (hyper-link)] to view logs without having (convenient) access to the console.
The file value can either be an absolute path like "c:\logs\log.txt" or a relative path which I believe is relative to the bin directory.
As far as implementing it, I usually place the following at the top of any class I plan to log in:
I think your sample is saving to your project folders and unless the default iis, or .NET , user has create permission then it won't be able to create the logs folder.
I'd create the logs folder first and allow the iis user full permission and see if the log file is being created.
For the log folder and file stuff, go with [@Bens answer (hyper-link)].
I will comment on the creating log part, though.
When coding loggers manually I do it the way you're doing it:
That said, I do not create the logger instances inside the classes these days, [I let my IoC container inject it for me (hyper-link)].
Log4net is saving into your project folder.
Something like: \SolutionFolder\ProjectFolder\bin\SolutionConfiguration\logs\log-file.txt.
If you want your log file to be place at a specified location which will be decided at run time may be your project output directory then you can configure your .config file entry in that way
and then in the code before calling log4net configure, set the new path like below
if you want to choose dynamically the path to the log file use the method written in this link: [method to dynamic choose the log file path (hyper-link)].
and then send this 'logFileLocation' to the method written in the link above like this:
I was developing for .NET core 2.1 using log4net 2.0.8 and found NealWalters code moans about 0 arguments for XmlConfigurator.Configure().
In your case, the log file will be in bin\Debug\netcoreapp3.1\ folder
and this may depends on your framework too.
Which means, one will need to match rawPassword that user will enter again upon next login and matches it with Bcrypt encoded password that's stores in database during previous login/registration.
When they login with remember me checked generate a new RememberMeToken (which invalidate any other machines which are marked are remember me).
When the user successfully logs in with Remember Me checked, a login cookie is issued in addition to the standard session management cookie.
The login cookie contains a series identifier and a token.
When a non-logged-in user visits the site and presents a login cookie, the series identifier is looked up in the database.
A new token is generated, a new hash for the token is stored over the old record, and a new login cookie is issued to the user (it's okay to re-use the series identifier).
If the username and series are not present, the login cookie is ignored.
Use it if you absolutely have to, but you should consider such a session only weakly authenticated and force a new login for anything that could be of value to an attacker.
Update 2013 - This was written around Node v0.2 and v0.4; There are much better utilites now around logging.
Update Late 2013 - We still use winston, but now with a logger library to wrap the functionality around logging of custom objects and formatting.
Here is a sample of our logger.js [https://gist.github.com/rtgibbons/7354879 (hyper-link)]
If this is for an application, you're probably better off using a logging module.
log4js [https://github.com/nomiddlename/log4js-node (hyper-link)]
You can now use [Caterpillar (hyper-link)] which is a streams based logging system, allowing you to log to it, then pipe the output off to different transforms and locations.
You could also just overload the default console.log function:
Above example will log to debug.log and stdout.
I often use many arguments to console.log() and console.error(), so my solution would be:
Overwriting console.log is the way to go.
To save yourself the trouble of writing log files, rotating and stuff, you might consider using a simple logger module like winston:
[Winston (hyper-link)] is a very-popular npm-module used for logging.
Here's a configuration ready to use out-of-box that I use frequently in my projects as logger.js under utils.
Then you can log the success as:
It also logs all the success-logs and error-logs in a file under logs directory date-wise as you can see here.
This implementation redirects both stdout and stderr to a log file:
and in the log file:
loglater.js:
I just build a pack to do this, hope you like it ;)
[https://www.npmjs.com/package/writelog (hyper-link)]
I for myself simply took the example from winston and added the log(...) method (because winston names it info(..):
Now you can simply use the normal log functions in your file and it will create a file AND log it to your console (while debugging/developing).
For simple cases, we could redirect the Standard Out (STDOUT) and Standard Error (STDERR) streams directly to a file(say, test.log) using '>' and '2>&1'
node test.js > test.log 2>&1
Step 2: '>' will redirect from  1 (stdout) to file (test.log)
This approach can help you (I use something similar in my projects) and works for all methods including console.log, console.warn, console.error, console.info
Is better than changing console.log, console.warn, console.error, console.info methods, because output will be exact the same as this methods output
Create a new file utils/logger.js
Then in  any file where you want to use logging import the module like
Use logger like the following:
>> file.log to redirect stdout to the file
2>> file.log to redirect stderr to the file
By the way, regarding NodeJS loggers, I use pino + pino-pretty logger
Adding to the answer above, a lit bit of an expansion to the short and efficient code overriding console.log.
Minor additions: set filename with date, wrapper function, also do the original console.logging to keep the console active with the info.
Usage: in the beginning of your code, run setConsoleLogToFile([FILENAME]).
Most logger is overkill and does not support the build in console.log correctly.
Hence I create [console-log-to-file (hyper-link)]:
if you are using [forever (hyper-link)] to keep your node app running, then typing forever list will show you the path to the log file that console.log is writing too
[http://example.com/?user%5Blogin%5D=username&user%5Bpassword%5D=123456 (hyper-link)]
[http://example.com/?user[login]=username&user[password]=123456 (hyper-link)]
The web server on the other end will likely take the user[login] and user[password] parameters, and make them into a user object with login and password fields containing those values.
Firstly, as Marcus did, check the source of the login form to get three pieces of information - the url that the form posts to, and the name attributes of the username and password fields.
Once you've got that, you can use a requests.Session() instance to make a post request to the login url with your login details as a payload.
Assuming your login attempt was successful, you can simply use the session instance to make further requests to the site.
Let me try to make it simple, suppose URL of the site is [http://example.com/ (hyper-link)] and let's suppose you need to sign up by filling username and password, so we go to the login page say [http://example.com/login.php (hyper-link)] now and view it's source code and search for the action URL it will be in form tag something like
Also replace the URL to point at the desired site to log into.
login.py
The use of disable_warnings(InsecureRequestWarning) will silence any output from the script when trying to log into sites with unverified SSL certificates.
Then create a link to this python script inside home/scripts/login.py
Close your terminal, start a new one, run login
The requests.Session() solution assisted with logging into a form with CSRF Protection (as used in Flask-WTF forms).
Some pages may require more than login/pass.
The most reliable way is to use inspect tool and look at the network tab while logging in, to see what data is being passed on.
Try Math.log(x) / Math.log(2)
so this would be applicable for log2.
just plug this into the java Math log10 method....
You can never know for sure what will (int)(Math.log(65536)/Math.log(2)) evaluate to.
For example, Math.ceil(Math.log(1<<29) / Math.log(2)) is 30 on my PC where mathematically it should be exactly 29.
I didn't find a value for x where (int)(Math.log(x)/Math.log(2)) fails (just because there are only 32 "dangerous" values), but it does not mean that it will work the same way on any PC.
Like (int)(Math.log(x)/Math.log(2)+1e-10) should never fail.
More demonstration, using a more general task - trying to implement int log(int x, int base):
If we use the most straight-forward implementation of logarithm,
It is slightly faster than Integer.numberOfLeadingZeros() (20-30%) and almost 10 times faster (jdk 1.6 x64) than a Math.log() based implementation like this one:
So with a 1.7 or newer server VM, a implementation like the one in the question is actually slightly faster than the binlog above.
To add to x4u answer, which gives you the floor of the binary log of a number, this function return the ceil of the binary log of a number :
To calculate log base 2 of n, following expression can be used:
Some cases just worked when I used Math.log10:
The key here is that you pass ax to the histogram function and you specify the bottom since there is no zero value on a log scale.
I'd recommend using the log=True parameter in the pyplot hist function:
There's also logx for log scaling the x-axis and loglog=True for log scaling both axes.
Logging could be one of them:
You would, however catch an exception to do some logic (for example closing sql connection of file lock, or just some logging) in the event of an exception the throw it back to the calling code to deal with.
Having try { } catch { log; throw } is just utterly pointless.
Exception logging should be done in central place inside the application.
exceptions bubble up the stacktrace anyway, why not log them somewhere up and close to the borders of the system?
DTO in one given example) just into the log message.
It can easily contain sensitive information one might not want to reach the hands of all the people who can access the log files.
Most of answers talking about scenario catch-log-rethrow.
Rethrowing exceptions via  throw is useful when you don't have a particular code to handle current exceptions, or in cases when you have a logic to handle specific error cases but want to skip all others.
Another cause of this issue is when the Take tail-log backup before restore "Options" setting is enabled.
On the "Options" tab, Disable/uncheck Take tail-log backup before restore before restoring to a database that doesn't yet exist.
In our case it was due to the Recovery Model on the primary database having been changed after we did the backup in preparation for setting up log shipping.
Ensuring the Recovery Model was set to Full Recovery before doing the backup and setting up log shipping resolved it for us.
in Files tab change Data file folder and Log file folder
The default logging level is warning.
Since you haven't changed the level, the root logger's level is still warning.
That means that it will ignore any logging with a level that is lower than warning, including debug loggings.
To change the level, just set it in the root logger:
In other words, it's not enough to define a handler with level=DEBUG, the actual logging level must also be DEBUG in order to get it to output anything.
Many years later there seems to still be a usability problem with the Python logger.
A common source of confusion comes from a badly initialised root logger.
Depending on your runtime environment and logging levels, the first log line (before basic config) might not show up anywhere.
It is a hierarchical set of five levels so that logs will display at the level you set, or higher.
So if you want to display an error you could use logging.error("The plumbus is broken").
This is a good article containing this information expressed better than my answer:
[https://www.digitalocean.com/community/tutorials/how-to-use-logging-in-python-3 (hyper-link)]
import logging 
log = logging.getLogger() 
log.setLevel(logging.DEBUG)
this code will set the default logging level to DEBUG.
For
  integral types, & computes the logical bitwise AND of its operands.
For bool operands, & computes the logical AND of its operands; that
  is, the result is true if and only if both its operands are true.
Time ~ O(log(d)) where d is number of binary digits
I will share another simple approach here since we know a power of two number have only one set bit so simply we will count number of set bit this will take O(log N) time.
Errors are stored in the nginx log file.
On Mac OS X with [Homebrew (hyper-link)], the log file was found by default at the following location:
Error logs, by default, before any configuration is set, on my system (x86 [Arch Linux (hyper-link)]), was found in:
My ngninx logs are located here:
You can also check your nginx.conf to see if you have any directives dumping to custom log.
The server could be configured to dump logs to /var/log as well.
find /usr/ -path "*/nginx/*" -type f -name '*.log', where /usr/ is the folder you wish to start searching from.
You can use lsof (list of open files) in most cases to find open log files without knowing the configuration.
Then search for open log files using lsof with the PID:
If lsof prints nothing, even though you expected the log files to be found, issue the same command using sudo.
Run this command, to check error logs:
Logs location on Linux servers:
I found it in /usr/local/nginx/logs/*.
It is a good practice to set where the access log should be in nginx configuring file .
Using acces_log /path/ Like this.
Then, you could find some default path for configuration and log files, in this case:
In this example all INFO and above are sent to Console, all WARN are sent to file and ERRORs are sent to the Event-Log.
you can use log4net.Filter.LevelMatchFilter.
other options can be found at
[log4net tutorial - filters (hyper-link)]
the accept on match default is true so u can leave it out but if u set it to false u can filter out log4net filters
You can do some interesting things with [Logback filters (hyper-link)].
logback.xml
I use logback.groovy to configure my logback but you can do it with xml config as well:
I apologize for my English!
click on stuff to take me to the log statements and
Your logback.xml file:
In this example WARN and ERROR levels are logged to System.err and rest to System.out:
Below is the configuration which logs different level of logs to different files
It looks like you can intercept all requests using an "interceptor", and log inside of it:  [https://github.com/mzabriskie/axios#interceptors (hyper-link)]
In this case a logging interceptor would be.
npm install --save axios-debug-log
require('axios-debug-log') before any axios call
By default, you'll see logs like the following:
Here's an NPM package for MySQL that let's you log all axios requests [https://www.npmjs.com/package/axios-logger-mysql (hyper-link)] , I hope this helps.
Use [axios-logger (hyper-link)]
When you send a request in nodejs, you need to show the log to the console.
You can use LoginManager.logOut()
Check out [https://developers.facebook.com/docs/reference/android/current/class/LoginManager/ (hyper-link)]
You can use LoginManager.getInstance().logOut();, even if you use LoginButton because
This UI element wraps functionality available in the LoginManager.
To handle it with the loginButton:
You can logout by using LoginManager but you have to use graph request also.
I am talking about log out completely so, that next time you can login with different account.
By the help of shared preferences here you can logout completely, and next time you can login with different account.
Where your log file is located is in your configs that you can access with:
The log file may not always be shown using the above.
The log file will be where the configuration file (usually /etc/redis/redis.conf) says it is :)
By default, logfile stdout which probably isn't what you are looking for.
If redis is running daemonized, then that log configuration means logs will be sent to /dev/null, i.e.
Summary: set logfile /path/to/my/log/file.log in your config and redis logs will be written to that file.
Check your error log file and then use the tail command as:
You can also login to the redis-cli and use the [MONITOR (hyper-link)] command to see what queries are happening against Redis.
Look for dir, logfile
So the log file is created at /usr/local/var/db/redis/redis_log with the name redis_log
No need to do this yourself, the Magento system has a built-in for cleaning up log information.
You can configure your store to automatically clean up these logs.
there are some other tables you can clear out:
documented here : [https://dx3webs.com/blog/house-keeping-for-your-magento-database (hyper-link)]
You can also refer to following tutorial:
[http://www.crucialwebhost.com/kb/article/log-cache-maintenance-script/ (hyper-link)]
[http://www.crucialwebhost.com/kb/magneto-log-and-cache-maintenance-script/ (hyper-link)]
[http://blog.magalter.com/magento-database-size (hyper-link)]
Cleaning the Magento Logs using SSH :
login to shell(SSH) panel and go with root/shell folder.
enter this command to view the log data's size
php -f log.php status
This method will help you to clean the log data's very easy way.
Cleaning Logs via Magento Admin Panel
To activate log cleaning option in Magento just do the following:
Log on to your Magento Admin Panel.
Under system you will see “Log Cleaning” option.
Fill the desired “Log Cleaning” option values and click Save.
Cleaning Logs via phpMyAdmin
If you are comfortable with mysql and queries then this method is more efficient and quicker than default Magento Log Cleaning tool.
This method also allows your to clean whatever you like, you can even clean tables which aren’t included in default Magento’s Log Cleaning tool.
log_customer
log_quote
log_summary
log_summary_type
log_url
log_url_info
log_visitor
log_visitor_info
log_visitor_online
You can also disable the logs by setting all events to disabled:
After clean the logs using any of the methods described above you can also disable them in your app/etc/local.xml
How Magento log cleaning can be done both manually, automatically and other Magento database maintenance.
Log Cleaning
To get more information [http://blog.contus.com/magento-database-maintenance-and-optimization/ (hyper-link)]
Login to your c-panel goto phpmyadmin 
using SQL run below query to clear logs
you can disable or set date and time for log setting.
System > Configuration > Advanced > System > Log Cleaning
So write except Exception, e: instead of except, e: for a general exception (that will be logged anyway).
Updating this to something simpler for logger (works for both python 2 and 3).
You can use logger.exception("msg") for logging exception with traceback:
After reading other answers and the logging package doc, the following two ways works great to print the actual stack trace for easier debugging:
or we can directly use logger.exception() to print the exception.
For formatted log rows like
Because I see there is still some people interested by this post, here is the final version (until next improvement) of my log interceptor.
With a current version of OkHttp, you can use the [HTTP Logging Interceptor (hyper-link)] and set the level to BODY
The logs generated by this interceptor when using the HEADERS or BODY levels have the potential to leak sensitive information such as "Authorization" or "Cookie" headers and the contents of request and response bodies.
This data should only be logged in a controlled way or in a non-production environment.
\xampp\apache\logs\error.log, where xampp is your installation folder.
If you haven't changed the error_log setting in PHP (check with phpinfo()), it will be logged to the Apache log.
Look in your configuration file and search for the error_log setting.
This might be a simple case of the [PHP error log being turned off (hyper-link)].
I found it in: 
\xampp\php\logs\php_error_log
For any one searching for the php log file in XAMPP for Ubuntu, its:
If you do not care about all the past logs you can empty the file easily by simply going to the terminal and then writing these three lines one by one:
And newer logs will be easy and fast to open now.
For my issue, I had to zero out the log:
sudo bash -c ' > /Applications/XAMPP/xamppfiles/logs/php_error_log '
You can also open the xampp control panel and click on the Button "logs":
\xampp\php\logs did not exist at all for me - for whatever reason.
I simply had to create a folder in \xampp\php\ called logs and then the php_error_log file was created and written to.
\xampp\apache\logs\error.log is the default location of error logs in php.
By default xampp php log file path is in /xampp_installation_folder/php/logs/php_error_log, but I noticed that sometimes it would not be generated automatically.
I am not sure, but I created the logs folder and php_error_log file manually and then php logs were logged in it finally.
As said above you can find PHP error log in windows.
In c:\xampp\apache\logs\error.log.
You can easily display the last logs by tail -f .\error.log
You can simply check you log path from phpmyadmin
now search for "error_log"(without quotes)
You will get log path.
So, "big-Oh of log(n)" means the same thing that I said above, except "some function of N" is replaced with "log(n)."
This is not binary search, and it is not big-Oh of log(N) because there's no way to force it into the criteria we sketched out above.
You can pick that arbitrary constant to be c=10, and if your list has N=32 elements, you're fine:  10*log(32) = 50, which is greater than the runtime of 32.
But if N=64, 10*log(64) = 60, which is less than the runtime of 64.
If your N=32, you can only do that about 5 times, which is log(32).
With all that background, what O(log(N)) usually means is that you have some way to do a simple thing, which cuts your problem size in half.
But, critically, what you can't do is some preprocessing step that would take longer than that O(log(N)) time.
So for instance, you can't shuffle your two lists into one big list, unless you can find a way to do that in O(log(N)) time, too.
(NOTE:  Nearly always, Log(N) means log-base-two, which is what I assume above.)
I have to agree that it's pretty weird the first time you see an O(log n) algorithm... where on earth does that logarithm come from?
However, it turns out that there's several different ways that you can get a log term to show up in big-O notation.
Interestingly, we also have that log2 16 = 4.
This took seven steps, and log2 128 = 7.
log2 n ≤ i
In other words, if we pick an integer i such that i ≥ log2 n, then after dividing n in half i times we'll have a value that is at most 1.
The smallest i for which this is guaranteed is roughly log2 n, so if we have an algorithm that divides by 2 until the number gets sufficiently small, then we can say that it terminates in O(log n) steps.
An important detail is that it doesn't matter what constant you're dividing n by (as long as it's greater than one); if you divide by the constant k, it will take logk n steps to reach 1.
Thus any algorithm that repeatedly divides the input size by some fraction will need O(log n) iterations to terminate.
Those iterations might take a lot of time and so the net runtime needn't be O(log n), but the number of steps will be logarithmic.
Well, since we keep cutting the array in half over and over again, we will be done in at most O(log n) iterations, since we can't cut the array in half more than O(log n) times before we run out of array elements.
Algorithms following the general technique of [divide-and-conquer (hyper-link)] (cutting the problem into pieces, solving those pieces, then putting the problem back together) tend to have logarithmic terms in them for this same reason - you can't keep cutting some object in half more than O(log n) times.
log10 (n + 1) ≤ k + 1
(log10 (n + 1)) - 1 ≤ k
From which we get that k is approximately the base-10 logarithm of n.  In other words, the number of digits in n is O(log n).
We do a total of O(1) work per digit (that is, a constant amount of work), and there are O(max{log n, log m}) total digits that need to be processed.
This gives a total of O(max{log n, log m}) complexity, because we need to visit each digit in the two numbers.
Many algorithms get an O(log n) term in them from working one digit at a time in some base.
There are many flavors of radix sort, but they usually run in time O(n log U), where U is the largest possible integer that's being sorted.
The reason for this is that each pass of the sort takes O(n) time, and there are a total of O(log U) iterations required to process each of the O(log U) digits of the largest number being sorted.
Many advanced algorithms, such as [Gabow's shortest-paths algorithm (hyper-link)] or the scaling version of the [Ford-Fulkerson max-flow algorithm (hyper-link)], have a log term in their complexity because they work one digit at a time.
Given the general structure of problems that are described here, you now can have a better sense of how to think about problems when you know there's a log term in the result, so I would advise against looking at the answer until you've given it some thought.
We call the time complexity O(log n), when the solution is based on iterations over n, where the work done in each iteration is a fraction of the previous iteration, as the algorithm works towards the solution.
The Log term pops up very often in algorithm complexity analysis.
==> 10 ^ d > X 
  ==> log (10 ^ d) > log(X) 
  ==> d* log(10) > log(X) 
  ==> d > log(X) // And log appears again... 
  ==> d = floor(log(x)) + 1
The minimum amount of information to uniquely identify a value in a range between 0 to N - 1 = log(N) digits.
This implies that, when asked to search for a number on the integer line, ranging from 0 to N - 1, we need at least log(N) tries to find it.
The minimum number of digits it needs to choose is log(N).
Hence the minimum number of operations taken to search for a number in a space of size N is log(N).
Its O(log(N))!
To correctly find the corresponding index in this range from 0 to n - 1, we need…log(n) operations.
The next element needs log(n-1) operations, the next log(n-2) and so on.
==> log(n) + log(n - 1) + log(n - 2) + … + log(1)Using log(a) + log(b) = log(a * b), ==> log(n!)
This can be [approximated (hyper-link)] to nlog(n) - n.  Which is O(n*log(n))!
Hence we conclude that there can be no sorting algorithm that can do better than O(n*log(n)).
These are some of the reasons why we see log(n) pop up so often in the complexity analysis of algorithms.
[Why does log(n) appear so often during algorithm complexity analysis?
See man git-log and search for the -G and -S options, or pickaxe (the friendly name for these features) for more information.
The -S option is actually mentioned in the header of the git-blame manpage too, in the description section, where it gives an example using git log -S....
You can use the following command to show a reversed git log.
Merge commits automatically have their changes hidden from the Git log output.
The file git log -p -- path/file history only showed it being added.
git log -p -U9999 -- path/file
git log --merges --pretty=format:"git diff %h^...%h | grep target_text" HEAD ^$(git merge-base A B) | sh -v 2>&1 | less
git log --graph --oneline A B ^$(git merge-base A B)
(A is the first commit above, B is the second commit above)
You click on the gear icon in the right bottom corner and uncheck Log XMLHttpRequests.
more recent (Chrome 55): open the console, right click the open area, uncheck "Log XMLHttpRequests".
OR click the vertical "..." in the top right, go to settings, and uncheck "Log XMLHttpRequests" under the "Console" header.
Then deactivate the option Log
.NET Core does not (and probably will not) provide a built-in ILoggerProvider implementation for file logging.
There is [a facade (hyper-link)] which makes trace source logging (the built-in logger framework originated in classic .NET) available for .NET Core applications.
As an alternative, you may try my lightweight ILogger<T> implementation which covers the features of the built-in ConsoleLogger and provides additional essential features and good customizability.
Issue [http://github.com/aspnet/Logging/issues/441 (hyper-link)] is closed and MS officially recommends to use 3rd party file loggers.
You might want to avoid using heavyweight logging frameworks like serilog, nlog etc because they are just excessive in case if all you need is a simple logger that writes to a file and nothing more (without any additional dependencies).
I faced the same situation, and implemented simple (but efficient) file logger: [https://github.com/nreco/logging (hyper-link)]
supports custom log message handler for writing logs in JSON or CSV
implements simple 'rolling file' feature if max log file size is specified
If you are using IIS, you can enable and view stdout logs:
Set stdoutLogEnabled to true.
Change the stdoutLogFile path to point to the logs folder (for example, .\logs\stdout).
Navigate to the logs folder.
Find and open the most recent stdout log.
For information about stdout logging, see [Troubleshoot ASP.NET Core on IIS (hyper-link)].
If you want to log Error, Warning and etc into a txt file in a .Net Core API project.
You can use what I used on my project which is called [Serilog (hyper-link)].
and you can follow the below blog to configure Serilog on your project.
[http://anthonygiretti.com/2018/11/19/common-features-in-asp-net-core-2-1-webapi-logging/ (hyper-link)]
Since an external NuGet is necessary anyway, worth mentioning that there is also a "standalone" extension of the Serilog rolling file sink that can be simply used as a logging provider for .net core, without completely swapping out the logging pipeline (pulls other dependencies, but I don't see that as an issue if you need the few extra features provided)
[Karambolo.Extensions.Logging.File  (hyper-link)]
[NReco.Logging.File (hyper-link)]
[Serilog.Extensions.Logging.File (hyper-link)]
Unfortunately, it not possible with the current version of ILogger.
As [@Vitaliy Fedorchenko (hyper-link)] mentioned in his post, the issue [http://github.com/aspnet/Logging/issues/441 (hyper-link)] has been closed after recommending to use 3rd party file loggers.
However, most of the file loggers out there are heavyweight (like Serilog) with limited configuration options.
Unlike those libraries, [Karambolo.Extensions.Logging.File (hyper-link)] provides an incredible functionality with a huge set of configuration options, which allows multiple logging providers with multiple files for each one.
This class library contains a lightweight implementation of the Microsoft.Extensions.Logging.ILoggerProvider interface for file logging.
Flexible configuration:

Two-level log file settings.
Fine-grained control over log message filtering.
Rolling log files with the customizable counter format.
Including log entry date in log file paths using templates.
Customizable log text formatting.
First, notice that .NET Core logging is more like Tracing in Full .NET Framework.
So you need to create both a TraceListener (ILoggerProvider), and a TraceWriter (ILogger).
Also, you need the create a LoggerOptions class, where you set the logfile-name, etc.
Furthermore,  you can optionally create a class which inherits from  ConfigureFromConfigurationOptions<T>, which can be called from ILoggingBuilder.TryAddEnumerable, I presume to configure your options from configuration entries.
Also, you need to create an an Extension-Method class, with which you can add ILoggerProvider to ILoggingBuilder.
The next stumbling-block is, that Microsoft has different "log-categories", e.g.
Now it will create a logger-instance for each of those categories.
Which means if you want to write your log-output to just one file, this will explode, because once the ILoggerProvider has created an instance of ILogger for ApplicationLifetime, and ILogger has created a FileStream and acquired a lock on it, the logger that gets created for the next category (aka Host) will fail, because it can't acquire the lock on the same file - "great" ...
So you need to cheat a little bit - and always return the same ILogger instance for all categories you want to log.
If you do that, you will find your logfile spammed by log-entries from Microsoft.
So you need to only return your singleton for the categories you want to log (e.g.
everything whose namespace doesn't start with Microsoft)...
For all other categories, ILoggerProvider.CreateLogger can return NULL.
Except that ILoggerProvider.CreateLogger CANNOT return NULL ever, because then the .NET framework explodes.
Thus, you need to create an IgnoreLogger, for all the log-categories you don't want to log... 
Then you need to return the same instance (singleton) of logger all the time, for all categories, so it won't create a second instance of Logger and try to acquire a lock on the already locked logfile.
Highlighs include, instead of using a singleton with locks, some file-loggers out there put log-statements in a queue, so they can have multiple instances writing to the same file by making the queue static, and periodically flushing that static queue to disk.
crashes) before the queue has been flushed, you will be missing the exact lines in the logfile which would have told you why your service crashed there (or did otherwise funny things)... e.g.
But although you tried to log that, you don't get the error log on that, because the queue hasn't been flushed before the program exited.
ILoggerProvider:
ILogger:
If you add a separate ScopeLock, you might get deadlocks by asynchronous calls  blocking each other due to logging.
So I have decided to share this easy way of logging to a file without using a third party library.
FileLogger:
FileLoggerProvider:
FileLoggerExtensions:
So now your logs will be written to the logger.txt file.
You can view the logger.txt file and compare the results with the console output.
If you don't like the implementation of my logging provider you can make your own using this info: [https://www.codeproject.com/Articles/1556475/How-to-Write-a-Custom-Logging-Provider-in-ASP-NET (hyper-link)]
As [described in the docs (hyper-link)], if warnings or errors occur during the parsing of the configuration file, logback will automatically print status data on the console.
Follow [http://logback.qos.ch/codes.html#layoutInsteadOfEncoder (hyper-link)] i.e.
the link mentioned by logback in its warning message.
Once you follow the steps mentioned therein, that is, if you replace <layout> element with <encoder>, logback will stop printing messages on the console.
The culprit is:
<layout class="ch.qos.logback.classic.PatternLayout">
If you have any configuration problems of level WARN or above, you will also get all status information logged to the console (including messages of level INFO).
My problem was due to multiple logback.xml files in my classpath.
When there is only one logback.xml file in classpath, there is no ambiguity and the problem is solved.
(...)if warnings or errors occur during the parsing of the configuration file, logback will automatically print status data on the console.
Once you get it right, there won't be any pollution in the first lines of your log anymore.
As of March 2015, in Logback 1.1.2, you need to use <encoder> sub-component - <layout> is now deprecated and if use it, error messages will appear.
You cannot control this, it´s Logback default behavior.
Here is the code snippet from their Errors Code Help page, which has the correct way to config the logger.
[http://logback.qos.ch/codes.html#layoutInsteadOfEncoder (hyper-link)]
there were a bunch of lines logged right at the beginning which were not related to my code.
Using the logback.groovy: statusListener(NopStatusListener) (in the src/test/resources/logback.groovy) works.
if working with ANT in Eclipse, using logback logging, groovy classes and unit tests where the unit tests take the src/test/resources/logback.groovy, but will also see the src/main/resources/logback.groovy (or similar) you cannot exclude (if ANT's classpath is said to use the projects classpath).)
in the logback and it succefully worked
I prefer to use status listener in order to switch off own logback logs:
So you can write your custom status listener and change log level for it manually:
Then use it in your logback.xml file:
[http://www.codeproject.com/Questions/163337/How-to-write-in-log-Files-in-C (hyper-link)]
Very convenient tool for logging is [http://logging.apache.org/log4net/ (hyper-link)]
Every time you add another log entry with +=, the whole string is copied to another place in memory.
Another thing to consider is what happens to the log entries that were added within the last 20 seconds of the execution.
Log4Net like Log4j(Java) - [http://www.codeproject.com/Articles/140911/log4net-Tutorial (hyper-link)]
Refer Link:
[blogspot.in (hyper-link)]
Add log to file with Static Class
The logging module uses handlers attached to loggers to decide how, where, or even if messages ultimately get stored or displayed.
You can configure logging by default to write to a file as well.
You should really read the [docs (hyper-link)], but if you call logging.basicConfig(filename=log_file_name) where log_file_name is the name of the file you want messages written to (note that you have to do this before anything else in logging is called at all), then all messages logged to all loggers (unless some further reconfiguration happens later) will be written there.
Be aware of what level the logger is set to though; if memory serves, info is below the default log level, so you'd have to include level=logging.INFO in the arguments to basicConfig as well for your message to end up in the file.
As to the other part of your question, logging.getLogger(some_string) returns a Logger object, inserted in to the correct position in the hierarchy from the root logger, with the name being the value of some_string.
Called with no arguments, it returns the root logger.
__name__ returns the name of the current module, so logging.getLogger(__name__) returns a Logger object with the name set to the name of the current module.
This is a common pattern used with logging, as it causes the logger structure to mirror your code's module structure, which often makes logging messages much more useful when debugging.
To find the logfile location, try instantiating your log object in a Python shell in your environment and looking at the value of:
log.handlers[0].stream
To get the log location of a simple file logger, try
Unfortunately, while they're easy to create, Loggers are difficult to inspect.
This gets the filenames of all Handlers for a logger:
Best bet is to check out my blog post on this:
[Timing things in Objective-C: A stopwatch (hyper-link)]
in the log...
I use very minimal, one page class implementation inspired by [code from this blog post (hyper-link)]:
Not currently, although [there seems to have been some discussion (hyper-link)] about supporting a -v option in the future, or making the current git log --not work for --author, --committer and --grep.
See also: [How to invert git log --grep pattern (hyper-link)].
[Stackdriver Logging (hyper-link)] is the preferred method of logging now.
Use [console.log() (hyper-link)] to log to Stackdriver.
Logger.log will either send you an email (eventually) of errors that have happened in your scripts, or, if you are running things from the Script Editor, you can view the log from the last run function by going to View->Logs (still in script editor).
Again, that will only show you anything that was logged from the last function you ran from inside Script Editor.
I was trying to use the Logger.log method to log some data whenever the onEdit function gets called, but this too seems like it only works when run from the Script Editor.
When I run it from the Script Editor, I can view the logs by going to View->Logs...
Peter Hermann's [BetterLog library (hyper-link)] will redirect all logs to a spreadsheet, enabling logging even from code that is not attached to an instance of the editor / debugger.
If you're coding in a spreadsheet-contained script, for example, you can add just this one line to the top of your script file, and all logs will go to a "Logs" sheet in the spreadsheet.
No other code necessary, just use Logger.log() as you usually would:
If you create a new script inside outside of docs then you will be able to export information to a google spreadsheet and use it like a logging tool.
When I'm working with GAS I have two monitors ( you can use two windows ) set up with one containing the GAS environment and the other containing the SS so I can write information to and log.
It's far from elegant, but while debugging, I often log to the Logger, and then use [getLog() (hyper-link)] to fetch its contents.
It grossly lacks the functionality of modern console.log() implementations, but the Logger does still help debug Google Scripts.
If you have the script editor open you will see the logs under View->Logs.
Then go to the script editor tab and open the log.
You will see whatever your function passes to the logger.
Basically as long as the script editor is open, the event will write to the log and show it for you.
2017 Update:
[Stackdriver Logging (hyper-link)] is now available for Google Apps Script.
From the menu bar in the script editor, goto:
View > Stackdriver Logging to view  or stream the logs.
[console.log() (hyper-link)] will write DEBUG level messages
Example onEdit() logging:
Then check the logs in the [Stackdriver UI (hyper-link)] labeled onEdit() Event Object to see the output
The dev console will log errors thrown by the app script, so you can just throw an error to get it logged as a normal console.log.
will show up in the console similarly to console.log('hello world')
Use console.log("Hello World") in your script.
Click on the header of the latest execution and read the log.
tail -f yourlog.csv
multitail looks better but the real way to burn CPU watching your log files is to use [glTail (hyper-link)].
tail -lf logfile.csv.
If you logged on to GUI, you can use mousepad to view the log dynamically.
This can be really helpful if you see something interesting in the log, but it scrolls off screen, or if you want to go back a bit to check on something you might have missed.
tail -F (that is capital -F, as opposed to lowercase -f) is a non-standard flag (available on Linux, Cygwin, MacOS X, FreeBSD and NetBSD), that works better for watching log files, which may be rotated occasionally; it's common for a process to rename a log file, and then create a new log file in its place, in order to avoid any one log file getting too big.
tail -f will keep following the old file, which is no longer the active log file, while tail -F will watch for a new file being created, and start following that instead.
vsConsole FileView may help if you prefer to monitor your logs via a web application.
Requires you run a java app server, deploy vsConsole to it, and run agents on the server containing the logs - so I'm guessing its a more heavy weight solution than what you need here.
(Its good for dev/testing teams who just want to click on a log file to see it rather than ssh, cd, tail etc)
As of Jellybean (4.1) you need the following permission:
<uses-permission android:name="android.permission.READ_CALL_LOG" />
This is method used to get the Call log.
Just put this method in you class and get the List of the Call Log.
This post is a little bit old, but here is another easy solution for getting data related to Call logs content provider in Android:
Example of class which responds for provide call logs:
Before considering making Read Call Log or Read SMS permissions a part of your application I strongly advise you to have a look at this policy of Google Play Market: [https://support.google.com/googleplay/android-developer/answer/9047303?hl=en (hyper-link)]
Probably the easiest is to just use one of the pre-baked --pretty formats, like git log --pretty=fuller - this will show both dates.
If you want to see only one date, but make it the commit date, you can use git log --format=<some stuff>.
All the [allowable codes (hyper-link)] for defining the format are documented in git help log.
lol is easier to type than log, and sounds better too.
Also gives you access to the regular git log if you ever need it.
It is not true for logger statement because it relies on former "%" format like string to provide lazy interpolation of this string using extra arguments given to the logger call.
Per the [Optimization (hyper-link)] section of the logging docs:
However, computing the arguments passed to the logging method can also be expensive, and you may want to avoid doing it if the logger will just throw away your event.
If you want to use fstrings (Literal String Interpolation) for logging, then you can disable it from .pylintrc file with disable=logging-fstring-interpolation, see: [related issue and comment (hyper-link)].
Also you can disable logging-format-interpolation.
There are 3 options for logging style in the .pylintrc file: old, new, fstr
for old (logging-format-style=old):
for new (logging-format-style=new):
for fstr (logging-format-style=fstr):
In my experience a more compelling reason than optimization (for most use cases) for the lazy interpolation is that it plays nicely with log aggregators like Sentry.
Consider a 'user logged in' log message.
If you interpolate the user into the format string, you have as many distinct log messages as there are users.
If you use lazy interpolation like this, the log aggregator can more reasonably interpret this as the same log message with a bunch of different instances.
Might be several years after but having to deal with this the other day,  I made simple; just formatted the string before logger.
That way there was no need to change any of the settings from log, if later on desire to change to a normal print there is no need to change the formatting or code.
Where is your logging.properties file located?
The .log file is in your \workspace\.metadata folder.
Your logging.properties file is not raken by the system, so point the properties file to the complete path as shown below then your log file will be generated in the place of directlyr where yor prefers it.
-Djava.util.logging.config.file=D:\keplereclipse\keplerws\NFCInvoicingProject\WebContent\WEB-INF\logging.properties
The root cause of the problem the questioner is having is that his logging.properties file is not being read.
The file specified in java.util.logging.config.file is not read from the classpath.
For example, running the following command java -Djava.util.logging.config.file=smclient-logging.properties SMMain will read the smclient-logging.properties from the current directory.
Once the correct java.util.logging.config.file is read, the logs are generated as specified in the file.
To find your logfile open the Java Console with your application.
There you will find your logfiles.
Location of log file can be control through logging.properties file.
And it can be passed as JVM parameter ex : java -Djava.util.logging.config.file=/scratch/user/config/logging.properties
To send logs to a file, add FileHandler to the handlers property in the logging.properties file.
This will enable file logging globally.
java.util.logging.FileHandler.pattern specifies the location and pattern of the output file.
java.util.logging.FileHandler.limit specifies, in bytes, the maximum amount that the logger writes to any one file.
java.util.logging.FileHandler.count specifies how many output files to cycle through.
java.util.logging.FileHandler.formatter specifies the java.util.logging formatter class that the file handler class uses to format the log messages.
SimpleFormatter writes brief "human-readable" summaries of log records.
To instruct java to use this configuration file instead of $JDK_HOME/jre/lib/logging.properties:
[Debug the your variable or logger.getHandlers(): just for the instances of FileHandler, and look for its private field: files (hyper-link)]
Make sure that your logger level including your log.
Make sure that your handler level including your log.
The log manager will be initialized during JVM startup and completed before the main method.
You can also re-initialize it in main method with System.setProperty("java.util.logging.config.file", file), which will call LogManager.readConfiguration().
I don't know if log4net supports this, but you could implement your own trace listener that did this.
The TraceListener doesn't have too many method that needs to be implemented and all you would do is to forward the values to log4net so this should be easy to do.
To add a custom trace listener you would either modify your app.config/web.config or you would add it in code using Trace.Listeners.Add(new Log4NetTraceListener());
According to Rune's suggestion I implemented a basic TraceListener which output to log4net:
To crudely deal with the issue (described in the comments to a previous answer) of internal log4net trace issuing from the LogLog class, I checked for this class being the source of the trace by inspecting the stack frame (which this implementation did already) and ignoring those trace messages:
From the description of Postgres' write ahead [http://www.postgresql.org/docs/9.1/static/wal-intro.html (hyper-link)] and VoltDB's command log (which you referenced), I can't see much difference at all.
Both sync only the log file to the disk but not the data so that the data could be recovered by replaying the log file.
Section 10.4 of VoltDB explains that their community version does not have command log so it would not pass the ACID test.
Command Logging as described here logs only transactions as they occur and not what happens in or to them.
The real difference that i see with this is that you cannot rollback to a point in time logically as with a normal transaction log.
Normal transaction logs (MSSQL, MySQL etc.)
Interresting question comes up - referring to the pos by pedz, will it always pass the ACID test even with the Command Log?
A DB snapshot is automatically created when the Command Logs fill up, to save you from big transaction logs and the IO used for this?
I'll rather stick to Transaction Logs for Transactional systems.
Why does new distributed VoltDB use a command log over write-ahead log?
Some basic terminology:
Write-Ahead Logging - central concept is that State changes should be logged before any heavy update to permanent storage.
Following our idea we can log incremental changes for each block.
Command Logging - central concept is to log only Command, which is used to produce the state.
Write-Ahead log contains all changed data, Command log will require addition processing, but fast and lightweight.
[VoltDB: Command Logging and Recovery (hyper-link)]
The key to command logging is that it logs the invocations, not the
  consequences, of the transactions.
By recording only the invocation, 
  the command logs are kept to a bare minimum, limiting the impact the disk I/O will 
  have on performance.
[SQLite: Write-Ahead Logging (hyper-link)]
[PostgreSQL: Write-Ahead Logging (WAL) (hyper-link)]
Using WAL results in a significantly reduced number of disk writes,
  because only the log file needs to be flushed to disk to guarantee
  that a transaction is committed, rather than every data file changed
  by the transaction.
The log file is written sequentially, and so the
  cost of syncing the log is much less than the cost of flushing the
  data pages.
Furthermore,
  when the server is processing many small concurrent transactions, one
  fsync of the log file may suffice to commit many transactions.
Command Logging:
Write Ahead Logging is a technique to provide atomicity.
Better Command Logging performance should also improve transaction processing.
[VoltDB Blog: Intro to VoltDB Command Logging (hyper-link)]
One advantage of command logging over ARIES style logging is that a
  transaction can be logged before execution begins instead of executing
  the transaction and waiting for the log data to flush to disk.
Another
  advantage is that the IO throughput necessary for a command log is
  bounded by the network used to relay commands and, in the case of
  Gig-E, this throughput can be satisfied by cheap commodity disks.
[VoltDB Blog: VoltDB’s New Command Logging Feature (hyper-link)]
The command log in VoltDB consists of stored procedure invocations and
  their parameters.
A log is created at each node, and each log is
  replicated because all work is replicated to multiple nodes.
This
  results in a replicated command log that can be de-duped at replay
  time.
Because VoltDB transactions are strongly ordered, the command
  log contains ordering information as well.
Since the invocations themselves
  are often smaller than the modified data, and can be logged before
  they are committed, this approach has a very modest effect on
  performance.
They log operations at the level of stored procedures, most RDBMS log at the level of individual statements (and 'lower').
One advantage of command logging over ARIES style logging is that a
  transaction can be logged before execution begins instead of executing
  the transaction and waiting for the log data to flush to disk.
They have to wait for the command to be logged too, its just a much smaller record.
Traditional RDBMS usually need to support ad-hoc transactions containing any number of statements, so procedure-level logging is out of the question.
given params+log+data always produce same output), which they would have to be for this to work.
With WAL, readers read from pages from unflushed logs.
With command logging, you have no ability to read from the command log.
Command logging is therefore vastly different.
VoltDB uses command logging to create recovery points and ensure durability, sure - but it is writing to the main db store (RAM) in real time - with all the attendant locking issues, etc.
Few terminologies before I start explaining:
Logging schemes: The database uses logging schemes such as [Shadow paging (hyper-link)], Write Ahead Log (WAL), to implement concurrency, isolation, and durability (how is a different topic).
The issue with this approach is a large number of fragmented tuples left behind which is randomly located, this is slower as compared to if the dirty pages are sequentially accessed, and this is what Write Ahead Log does.
Put quotes around cloglog for plot.survfit.
Even if it were possible, each AWS Lambda instance would still write to its own log-stream.
And though different invocations of the same lambda can write to the same log-stream (when the lambda instance is reused), this will definitely not be the case for different lambdas (since they must use different lambda instances).
As a result, you must have a tool that aggregates multiple log-stream.
If so, what's the problem with making it a little more generic, so that it can aggregate log-streams from different log groups?
Here's a snippet of two resources; the lambda function and the log group:
I found that the log group was created with a retention of 14 days as indicated.
When the lambda function runs, it does create log streams in this group.
When I deleted the stack, however, it seems that the log groups is not deleted, and the retention is now set to never expire.
Creating the log group as mentioned as one of the answers works.
First, to answer the original question, you cannot specify a custom log group name for the lambda function to write to.
The lambda log group name always follows this pattern:
The lambda function will first check if a log group exists with this name.
If it does not exist, it will create a log group with that pattern.
Hence, if you want to add settings, such as RetentionInDays and SubscriptionFilter, make sure your CloudFormation or SAM template creates the LogGroup before the lambda function.
If your lambda function is created first, an error will be thrown when creating the LogGroup saying that the LogGroup already exists.
So, the lambda function should have DependsOn: LogGroup instead of the other way round.
Also, make sure you are not using Ref or GetAtt to reference the lambda function inside the LogGroup because that creates an implicit dependency on the lambda function causing the lambda function being created before the LogGroup.
I attached LogGroup to Serverless function in SAM template as follows:
The only requirement is that the LogGroupName must be /aws/lambda/<FunctionName>.
SAM will create the log group before the lambda function as long as your are not referencing the function's logical ID in the LogGroup
Also, if you are adding LogGroup to exisiting function, you can do so by simply updating your template.yaml with the LogGroup resource like above and add FunctionName property to the function resource.
Try adding the $host variable in log_format:
If you want to log the full requested url, then my method is
A simplified version of the git-log synopsis given in [that command's man page (hyper-link)] is
In others words, git log is equivalent to git log HEAD.
If you're on a branch, called mybranch, say, this command is also equivalent to git log mybranch.
You want to limit the log to commits reachable from another branch, i.e.
The easiest way to do that is to explicitly pass the name of the branch of interest to git log:
"To have launchd start postgresql now and restart at login:"
What is the result of pg_ctl -D /usr/local/var/postgres -l /usr/local/var/postgres/server.log start?
Are there any error messages in the server.log?
/Applications/Server.app/Contents/ServerRoot/usr/bin/postgres_real -D /Library/Server/PostgreSQL/Data -c listen_addresses=127.0.0.1,::1 -c log_connections=on -c log_directory=/Library/Logs/PostgreSQL -c log_filename=PostgreSQL.log -c log_line_prefix=%t  -c log_lock_waits=on -c log_statement=ddl -c logging_collector=on -c unix_socket_directory=/private/var/pgsql_socket -c unix_socket_group=_postgres -c unix_socket_permissions=0770
But after checking the log file,
Check the log file every time.
You can verify this by looking at the logs of Postgres to see what might be going on: tail -f /usr/local/var/postgres/server.log
This gives you a nice screen in your System Preferences that allows you to launch, reboot, root, and launch at login.
Start postgres:
pg_ctl -D /usr/local/var/postgres -l server.log start
having installed Postgres with homebrew that is what I do to start postgres and keep it in foreground to see the logs:
I think it does not matter what is the base of the log as the relative complexity is the same irrespective of the base used.
So you can think of it as  O(log2X) = O(log10X)
Also to mention that logarithms are related by some constant.
So log₁₀(x) = log₂(x) / log₂(10)
The tree has a height of log₂ n, since the node has two branches.
1)I am confused over whether is it Log base 10 or Log base 2 as
  different articles use different bases for their Logarithm .
2) Does it make a difference if its Log base 2 or Log base 10?
3)Can we assume it mean Log base 10 when we see O(LogN)??
log₁₀(x) = log₂(x) / log₂(10) for all x.
1/log₂(10) is a constant multiplier and can be omitted from asymptotic analysis.
More generally, the base of any logarithm can be changed from a to b (both constant wrt.
n) by dividing by logₐ(b), so you can freely switch between log bases greater than one: O(log₁₀(n)) is the same as O(log₂(n)), O(ln(n)), etc.
An example consequence of this is that [B-trees (hyper-link)] don't beat balanced binary search trees asymptotically, even though they give higher log bases in analysis.
In Big O notation, O(log(n)) is the same for all bases.
This is due to logarithm base conversion:
1/log10(2) is just a constant multiplier factor, so O(log2(n)) is the same as O(log10(n))
I find it more useful to do the following, which is to enable Hibernate's logging to log the SQL along with bind variables (so you can see the values passed into your calls, and easily replicate the SQL in your editor or otherwise).
In your Config.groovy, add the following to your log4j block:
It avoids the performance problems of trace logging the Hibernate type package.
I got this from: [https://burtbeckwith.com/blog/?p=1604 (hyper-link)]
A) proxy your jdbc connection with log4jdbc or p6Spy.
Find out where you general_log_file is.
Active general log if no activated already.
Now everything is logged to you log file.
I know this was asked and answered long back .But I just happened to see this question and couldn't stop myself in answering or sharing our sql logging implementation approach in our project.
We are using "log4jdbc Driver Spy " to log sql.
[https://code.google.com/p/log4jdbc-remix/ (hyper-link)]
Option #1 add the following to logback.groovy
However this approach does not log the parameter values
Pure for reference only, but I use p6spy to log the SQL queries.
The exact query is logged as it would be send to the server (with parameters included).
If you have the [console (hyper-link)] plugin installed, you can get sql logging with this little code snippet.
And just like the other solutions that deal with logToStdout it only shows the queries and not the bind values.
A similar technique can be used to turn on logging for specific integration tests:
This will turn on sql logging for just the tests in this one file.
Source: [http://sergiodelamo.es/log-sql-grails-3-app/ (hyper-link)]
ssh {remotehost} tail -n0f {logfile}
has one plus - you can anytime CTRL-C and scroll back in log and start watching again with the 'F'.
Would show you the last 5 lines of the log file.
The git log --decorate will put by default:
But the [git log --format (hyper-link)] don't offer a way to display specifically the HEAD or remotes or branch: all three are displayed through %d, with one color possible.
git log –format now sports a %C(auto) token that tells Git to use color when resolving %d (decoration), %h (short commit object name), etc.
This [Atlassian blog post (hyper-link)] comments that this feature is part of several others focused on format (git rebase, git count-objects) and colors (git branch -vv)
In "git log --decorate", you would see the commit header like this:
The "log --decorate" enhancement in Git 2.4 that shows the commit at the tip of the current branch e.g.
The config option log.decorate can enable/disable default decorations in logs.
As of git 1.8.3 (May 24, 2013), you can use %C(auto) to decorate %d in the format string of git log.
This does create a log within the pycharm terminal using the Py terminal within it.
Instead of just putting in ram.log, use the full file path of where you would like the file to appear.
This assumes that ram.log resides in the same directory with the one that contains the above code (that's why __file__ is used for).
Luckily, I found a solution from a colleague's code by adding the following lines before logging.basicConfig().
See [logging.basicConfig (hyper-link)] parameters:
force: If this keyword argument is specified as true, any existing handlers attached to the root logger are removed and closed, before carrying out the configuration as specified by the other arguments.
Here I called logging.info() right before logging.basicConfig().
Further information can be found on the [MySQL Connector/Python 1.0.5 beta announcement (hyper-link)] blog.
From the [documentation (hyper-link)] of the /Log command line switch:
If LogFile is not specified, two files
  will be written to the current user's
  non-localized application data folder.
The two files are, by default, called
  ActivityLog.xml and ActivityLog.xsl.
The former contains the activity log
  data and the latter is an XML style
  sheet which provides a more convenient
  way to view the XML file.
To view the
  Activity log in your default XML
  viewer (e.g.
You will probably have to run devenv with the /Log switch for these files to be created.
Visual Studio doesn't seem to log anything by default.
The location of their log file is available on the about box of the add-in.
C:\Users\\AppData\Local\Temp\vmware-\vmware-vsid-2.log
I found the following entries in the log
Additional info: Visual studio 2010 logs are available at
%APPDATA%\Microsoft\VisualStudio\10.0\ActivityLog.xml %APPDATA%
  resolves to “C:\Users\user\AppData\Roaming”
Thanks to this blog post: [http://www.request-response.com/blog/CommentView,guid,9f383687-3e1e-4568-833b-ef80e0938337.aspx (hyper-link)]
According to [this (hyper-link)] post it uses the IE Cache for logging.
So it is a better idea to avoid clearing the browsing history all together and click on Settings->Enable Custom Log Path and choose a directory where you won't be sharing with IE.
If you're building an ASP.NET MVC application (or possibly any other application involving IIS) restarting IIS was required to start getting binding logs.
After much frustration I have found that by default, fuslogw does not log assembly binding failures!
You need to click 'settings' then select "log bind failures to disk", otherwise you won't see anything in the window
I use the script from [this blog post (hyper-link)] to enable fuslogvw.exe's logging, without the need to use the exe itself.
This script "enables" and "disables" custom settings for the Fusion
Log Viewer tool.
Create a log folder (default: C:\fusionlogs)
Add HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Fusion\LogPath and
set it to the log folder
Set HKEY_LOCAL_MACHINE\SOFTWAR\Microsoft\Fusion\LogFailures
to 1
Optionally set
HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Fusion\ForceLog to 1
Optionally set
HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Fusion\LogResourceBinds
to 1
Delete the log folder and its contents
Delete HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Fusion\LogPath
Set HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Fusion\LogFailures
to 0
Set HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Fusion\ForceLog to
0
Set
HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Fusion\LogResourceBinds
to 0
The Assembly Binding Log Viewer (FUSLOGVW.exe) has quite some glitches.
Like the fact that it does not bind errors by default when running or that you have to make sure to not add an additional backslash to a custom log path.
You cannot use the UI to go on error hunt you literally have to scan the file system to read logs.
There's so much wrong with it that I decided to write an alternative assembly binding log viewer named [Fusion++ and put it on GitHub (hyper-link)].
It uses the same mechanics internally but parses the logs for you.
You don't have to care for any settings at all, not even log paths
stash is a branch and you can list all stashes with git log -g stash.
Maybe you can write a script to show both git stash list and git log and use it with an alias.
Another easy way to do this is git reflog show stash
@msmt's answer gives you a log of the stashes, and you can use this to get the hashes to use in the git log.
git reflog show --format="%h" stash gives you just the hashes of all stashes which can then be passed to a git log command such as
git log --date-order --all $(git reflog show --format="%h" stash)
git log --oneline --graph --decorate --all $(git reflog show --format="%h" stash)
git log --oneline --graph --all $(git stash list --format="%H")
You'll find the referenced log file in the same directory containing the log referencing the other log file.
With a default installation of Tomcat, the directory is $CATALINA_HOME/logs.
The log file containing the information referenced will usually be the one named with the hostname on which Tomcat is running.
development environment) this is localhost.<datestamp>.log.
The additional error information can be found in this log file at the same timestamp as the noted reference in the referring log file.
Tomcat logs did not show full stack trace and application has more than one listeners, it may get challenging to figure out the issue.
Create a file logging.properties with the following content, place it inside Apache-tomcat\webapps\MyApp\WEB-INF\classes  and restart the Apache Tomcat server.
fileConfig is a mechanism to configure the log level for you based on a file; you can dynamically change it at any time in your program.
Call [.setLevel() (hyper-link)] on the logging object for which you want to change the log level.
Note that getLogger() called without any arguments returns the root logger.
It is certainly possible to use fileConfig() to change logging configuration on the fly, though for simple changes a programmatic approach as suggested in Martijn Pieters' answer might be appropriate.
Logging even provides a socket server to listen for config changes using the listen() / stopListening() APIs, as documented [here (hyper-link)].
To get logging to listen on a particular port, you use
Update: Due to backwards-compatibility constraints, the internal implementation of the fileConfig() call means that you can't specify disable_existing_loggers=False in the  call, which makes this feature less useful in certain scenarios.
Depending on your app, you first need to find a way for reloading that file or resetting the log level based on your own config file during execution.
I finally settled with using inotify and gevent to check for the file write operation, and once I know the file has been changed then I go and set the level for each logger I have based on the config.
Once I have the new log file config I can wire in the right logging level for each logger from config.
In addition to the accepted answer: Depending on how you initialized the logger, you might also have to update the logger's handlers:
Since Rails has already built with the similar method, we can just simply override it to append our logic:
I note the related [stack overflow question and the answers there (hyper-link)], and I wrote a much more verbose [blog post (hyper-link)] that summarises the various considerations.
And nothing I saw in the error logs pointed me in this direction.
I noticed that general.dll.js showed up in some of the hard-to-understand logs
No need to redirect logs.
Docker by default store logs to one log file.
To check log file path run command:
Open that log file and analyse.
if you redirect logs then you will only get logs before redirection.
you will not be able to see live logs.
To see live logs you can run below command
This log file /var/lib/docker/containers/f844a7b45ca5a9589ffaa1a5bd8dea0f4e79f0e2ff639c1d010d96afb4b53334/f844a7b45ca5a9589ffaa1a5bd8dea0f4e79f0e2ff639c1d010d96afb4b53334-json.log will be created only if docker generating logs if there is no logs then this file will not be there.
it is similar like sometime if we run command docker logs containername and  it returns nothing.
docker logs containername >& logs/myFile.log
It will not redirect logs which was asked for in the question, but copy them once to a specific file.
Assuming that you have multiple containers and you want to aggregate the logs into a single file, you need to use some log aggregator like fluentd.
fluentd is supported as logging driver for docker containers.
So in docker-compose, you need to define the logging driver
The second step would be update the fluentd conf to cater the logs for both service 1 and service 2
In this config, we are asking logs to be written to a single file to this path
/fluentd/log/service/service.
*.log
and the third step would be to run the customized fluentd which will start writing the logs to file.
Bit Long, but correct way since you get more control over log files path etc and it works well in Docker Swarm too .
docker logs -f <yourContainer> &> your.log &
-f (i.e.--follow): writes all existing logs and continues (follows) logging everything that comes next.
You can separate output and stderr by: > output.log 2> error.log (instead of using &>).
Bash script to copy all container logs to a specified directory:
To capture both stdout & stderr from your docker container to a single log file run the following:
Since Docker merges stdout and stderr for us, we can treat the log output like any other shell stream.
To redirect the current logs to a file, use a redirection operator
$ docker logs test_container > output.log 
docker logs -f test_container > output.log
$ docker logs test_container> /tmp/output.log
docker logs -f docker_container_name >> YOUR_LOG_PATH 2>&1 &
Create a shell script named as login.sh in your $HOME folder.
Paste the following one-line script into Script Editor: do shell script "$HOME/login.sh"
Finally add the application to your login items.
go to System Preferences -> Accounts -> Login items
While my solution is simple and working, the cleanest way to run any program or shell script at login time is described in [@trisweb's answer (hyper-link)], unless, you want interactivity.
so, asking to run a script or quit the app, asking passwords, running other automator workflows at login time, conditionally run applications at login time and so on...
Log in (or run manually via launchctl load [filename.plist])
Here's the specific plist file to run a script at login.
Save as ~/Library/LaunchAgents/com.user.loginscript.plist
Run launchctl load ~/Library/LaunchAgents/com.user.loginscript.plist and log out/in to test (or to test directly, run launchctl start com.user.loginscript)
Tail /var/log/system.log for error messages.
The key is that this is a User-specific launchd entry, so it will be run on login for the given user.
If you want a script to run on login for all users, I believe LoginHook is your only option, and that's probably the reason it exists.
After setting it, it worked and logged perfectly.
/frameworks/native/services/surfaceflinger/EventLog/EventLog.cpp
or mabye is not your APP's ErrorLog
It seemed quite hard to find this information, but eventually, I came across [this question (hyper-link)] 
You have to look at the 'System' event log, and filter by the WAS source.
Maybe interesting to mention is that you have to
  configure in which cases the app pool recycle event is logged.
You can do that in
  IIS > app pools > select the app pool > advanced settings > expand
  generate recycle event log entry – BlackHawkDesign Jan 14 '15 at 10:00
As link-only answers are not preferred, I will just copy and paste the content of the link of the accepted answer

It is definitely System Log.
Which Log file?
Well -- you can check the physical path by right-clicking on the System Log (e.g.
Server Manager | Diagnostics | Event Viewer | Windows Logs).
The default physical path is %SystemRoot%\System32\Winevt\Logs\System.evtx.
You may need first to enable logging of such even for a specific App Pool -- by default App Pool has only 3 recycle events out of 8 enabled.
To change it using GUI: II S Manager | Application Pools | Select App Pool -> Advanced Settings | Generate Recycle Event Log Entry.
Go to Logging and ensure either ETW event only or Both log file and ETW event ...is selected.
Enable the desired Recycle logs in the Advanced Settings for the Application Pool:
Go to the default Custom View: WebServer filters IIS logs:
... or System logs:
Windows Logs > System
You need to sort the arrays first before you plot it, and use the 'log' instead of the symlog to get rid of the whitespace in the plot.
Read [this answer (hyper-link)] to look at the differences between log and symlog.
[The only mathematical form that is a straight line on a log-log-plot is an exponential function.
Since you have data with x=0 in it you can't just fit a line to log(y) = k*log(x) + a because [log(0) (hyper-link)] is undefined.
Note: Using ax.set_xscale('log') hides the points with x=0 on the plot, but those points do contribute to the fit.
Before taking the log of your data, you should note that there are zeros in your first array.
In order to avoid loosing points, I suggest to analyse the relation between Log(B) and Log(A+1).
The code below uses the scipy.stats.linregress to perform the linear regression analysis of the relation Log(A+1) vs Log(B), which is a very well behaved relation.
You want to use the --follow option on git log, which is described in the man page as:
Git 2.9+ has now enabled this by default for all git diff and git log commands:
The end-user facing Porcelain level commands in the "git diff" and
  "git log" family by default enable the rename detection; you can still
  use "diff.renames" configuration variable to disable this.
It's still configurable at runtime, depending on your logger system, of course.
For example, if you use Apache Log (log4j/cxx) you could configure a dedicated logger for such URLs and then configure it at runtime from an XML file.
One concern is that POST data can be pretty large, and if you don't put some kind of limit on how much is being logged, you might run out of disk space after a while.
Note that mod_dumpio [stops logging binary payloads at the first null character (hyper-link)].
An easier option may be to log the POST data before it gets to the server.
You will now find your data logged under /var/log/apache2/modsec_audit.log
You can also use the built-in forensic log feature.
Log in Gradle Console
As in my log it clearly shows issues are with declaration of variables with butterknife.
Read log and try to found the answer of what went wrong.
In new Android Studio 3.1.+, you can enable/disable console log details by pressing "Toggle View" on "Build" tab.
you need to click on toggle view and see the logs in text format to see the error and if needed to Run with --stacktrace
See log for more details
press this button , you will see detailed logs with exact code that is preventing the project from compiling
See log for more details to me.
3 - Export logs from build and read them
java.util.logging was introduced in Java 1.4.
There were uses for logging before that, that's why many other logging APIs exist.
You can't always influence what logging framework your libraries use.
Therefore using SLF4J (which is actually just a very thin API layer above other libraries) helps keeping a somewhat consistent picture of the entire logging world (so you can decide the underlying logging framework while still having library logging in the same system).
If a previous version of a library used to use logging-library-X it can't easily switch to logging-library-Y (for example JUL), even if the latter is clearly superious: any user of that library would need to learn the new logging framework and (at least) reconfigure their logging.
Having said all that I think JUL is at least a valid alternative to other logging frameworks these days.
IMHO, the main advantage in using a logging facade like slf4j is that you let the end-user of the library choose which concrete logging implementation he wants, rather than imposing your choice to the end user.
Maybe he has invested time and money in Log4j or LogBack (special formatters, appenders, etc.)
and prefers continuing using Log4j or LogBack, rather than configuring jul.
Is it a wise choice to use Log4j over jul?
Disclaimer: I am the founder of log4j, SLF4J and logback projects.
For one, SLF4J allows the end-user the liberty to choose the underlying logging framework.
In addition, savvier users tend to prefer [logback which offers capabilities beyond log4j (hyper-link)], with j.u.l falling way behind.
In a nutshell, if logging is important to you, you would want to use SLF4J with logback as the underlying implementation.
If logging is unimportant, j.u.l is fine.
It follows that you should adopt SLF4J not because you are convinced that SLF4J is better than j.u.l but because most Java developers currently (July 2012) prefer SLF4J as their logging API.
Thus, holding "hard facts" above public opinion, while seemingly brave, is a logical fallacy in this case.
Except the end user could have already done this customization for his
  own code, or another library that uses log4j or logback.
j.u.l is
  extensible, but having to extend logback, j.u.l, log4j and God only
  knows which other logging framework because he uses four libraries that
  use four different logging frameworks is cumbersome.
By using SLF4J, you
  allow him to configure the logging frameworks he wants, not the one
  you have chosen.
Whenever I use these tools in a web-service type app the logging just disappears or goes somewhere unpredictable or strange.
Our solution was to add a facade to the library code that meant that the library log calls did not change but were dynamically redirected to whatever logging mechanism is available.
When included in a POJO tool they are directed to JUL but when deployed as a web-app they are redirected to LogBack.
Our regret - of course - is that the library code does not use parameterised logging but this can now be retrofitted as and when needed.
I ran jul against slf4j-1.7.21 over logback-1.1.7, output to an SSD, Java 1.8, Win64
jul ran 48449 ms, logback 27185 ms for an 1M loop.
You can find more conversion characters usage in log4j javadoc.For example, at [http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html (hyper-link)].
[http://tostring.it/2014/06/23/advanced-logging-with-nodejs/ (hyper-link)]
This will set up Winston to write a log to the console as well as a file.
What you need to do is have a single <logger> definition with a defined level of INFO, but in your two appender definitions, you set their thresholds accordingly, e.g.
You then add both appenders to your logger:
Log entries now going to the logger will get sent to both appenders, but since they have different independent thresholds, the ERROR_FILE appender will only log ERROR and above.
You need to use log4j filters:
That way you can create log files for each level separately.
If you are using log4j2 and config with XML format, [ThresholdFilter (hyper-link)] is a good solution.
Log only WARN
[code snippet]
Log from INFO to WARN
[code snippet]
I'd say you're probably fine with util.logging for the needs you describe.
For a good decision tree, have a look at [Log4j vs java.util.logging (hyper-link)]
Question One :
Do you anticipate a need for any of the clever handlers that Log4j has that JUL does not have, such as the SMTPHandler, NTEventLogHandler, or any of the very convenient FileHandlers?
Question Two :
Do you see yourself wanting to frequently switch the format of your logging output?
In other words, do you need Log4j's PatternLayout?
Question Three :
Do you anticipate a definite need for the ability to change complex logging configurations in your applications, after they are compiled and deployed in a production environment?
Does your configuration sound something like, "Severe messages from this class get sent via e-mail to the support guy; severe messages from a subset of classes get logged to a syslog deamon on our server; warning messages from another subset of classes get logged to a file on network drive A; and then all messages from everywhere get logged to a file on network drive B"?
If you can answer yes to any of the above questions, go with Log4j.
That said, pretty much every project these days seems to wind up including log4j, if only because some other library uses it.
I would go with log4j.
The possibilites with log4j is not obsolete at all!
I recommend using [Apache Commmons Logging (hyper-link)] as your logging interface.
That way you have the flexibility to switch logging implementations anytime you want without requiring any code changes on your end.
I recommend that you use the [Simple Logging Facade for Java (hyper-link)] (SLF4J).
It supports different providers that include Log4J and can be used as a replacement for Apache Commons Logging.
Log4j has been around for a long time, and it works very well.
I have no scientific study to back it, but based on what I've seen at a large number of clients, it is easily the logging framework that I see used more than any other.
It has been around for a long time, and not been replaced by the Next Big Logging Framework, which says something.
NTEventLogAppender
SyslogAppender
Additionally there is a great deal of flexibility in each of the appenders that allow you to control specifically what is output in your log.
One note, I had a series of classloader problems when I used apache commons logging in addition to log4j.
It was only for one specific application, but I found it simpler to use log4j alone, rather than to have the flexibility offered when using an abstraction layer like commons logging.
log4j is a much nicer package overall, and doesn't have some of the hiccups that java.util.logging contains.
I'd second that using log4j directly is easier than using the commons logging.
java.util.logging offers a comprehensive logging package without the excess baggage some of the others provide..
What happens, I expect, is that std::log simply delegates to ::log.
Unfortunately, ::log only provides a float overload, and you kindly provide a double overload, making yours a better match.
Unfortunately you are supplying an implementation for double log(double), so the linker will not use the one from the math lib.
EDIT: I claim it's a bug in libstdc++ because std::log should not suffer from interferences with the C library when you are explicitly asking for the std:: versions.
So g++ is allowed to declare and define double log(double) in the global namespace.
You, on the other hand, may not declare or define ::log in any way.
Where do I see the log?
In a log file or standard output, depending on your actual log handler configuration.
This can be set via a property file or directly via the logging API.
Does this mean that if I have 3 request level log...
So if your log level is SEVERE, only the SEVERE messages get logged.
If level is FINE, all 3 messages get logged.
This is very useful when in real production environment, you may want to log only the errors and possibly warnings (which are - hopefully - fairly rare, but you want to know about them), so you can set log level to WARNING.
debugging a problem, you want to see all information in the logs, even though it creates a large amount of log data and slows down the application.
So you set log level to FINE or FINEST.
Here is a good [introduction to Java Logging (hyper-link)].
Update: a simple example from the above page to configure the logger to log to a file at level FINEST:
To log to the console, replace the FileHandler above with a ConsoleHandler:
This is just an example though - in a real app, it is preferable to configure the logging via a config property file.
The [Java TM Logging Overview (hyper-link)] is quite interesting to answer all your questions on the Java Logger:
You will see your log where the Handler(s) associated with your Logger will generate said Log (in a Console, or in a Stream...).
The default configuration establishes a single handler on the root logger for sending output to the console.
Log Level:
Each log message has an associated log Level.
The Level gives a rough guide to the importance and urgency of a log message.
Log level objects encapsulate an integer value, with higher values indicating higher priorities.
The Level class defines seven standard log levels, ranging from FINEST (the lowest priority, with the lowest value) to SEVERE (the highest priority, with the highest value).
The log level is the exact reverse of what you said.
When you do a git pull and there are conflicts, the git log will show the updates to the conflicted files as coming from the user that resolved the conflicts.
For the main logfile/appender, set up a .Threshold = INFO to limit what is actually logged in the appender to INFO and above, regardless of whether or not the loggers have DEBUG, TRACE, etc, enabled.
If your goal is to have a single file where you can look to troubleshoot something, then spanning your log data across different files will be annoying - unless you have a very regimented logging policy, you'll likely need content from both DEBUG and INFO to be able to trace execution of the problematic code effectively.
By still logging all of your debug messages, you are losing any performance gains you usually get in a production system by turning the logging (way) down.
Thus, all info messages are written to server.log; by contrast, foo.log contains only com.example.foo messages, including debug-level messages.
I had this question, but with a twist - I was trying to log different content to different files.
I had information for a LowLevel debug log, and a HighLevel user log.
I wanted the LowLevel to go to only one file, and the HighLevel to go to both a file, and a syslogd.
My solution was to configure the 3 appenders, and then setup the logging like this:
The part that was difficult for me to figure out was that the 'log4j.logger' could have multiple appenders listed.
Demo link: [https://github.com/RazvanSebastian/spring_multiple_log_files_demo.git (hyper-link)]
My solution is based on XML configuration using spring-boot-starter-log4j.
The example is a basic example using spring-boot-starter and the two Loggers writes into different log files.
Here's a variation I consider somewhat cleaner, that still allows potential other logging from AR.
This might not be a suitable solution for the console, but Rails has a method for this problem: [Logger#silence (hyper-link)]
In case someone wants to actually knock out SQL statement logging (without changing logging level, and while keeping the logging from their AR models):
The line that writes to the log (in Rails 3.2.16, anyway) is the call to debug in lib/active_record/log_subscriber.rb:50.
That debug method is defined by ActiveSupport::LogSubscriber.
So we can knock out the logging by overwriting it like so:
I used this: config.log_level = :info
edit-in config/environments/performance.rb
I had to solve this for ActiveRecord 6, and I based my answer on fakeleft's response, but it wasn't quite right, since it was suppressing other logging such as the logging of nested views.
What I did was created config/initializers/activerecord_logger.rb:
The log subscriber in AR 6 has a sql event that we want to hide, so this is very narrowly targeted to skip that event.
I use activerecord 6.0.3.3 and I had to include ActiveSupport::LoggerSilence
Older stashes are saved in the refs/stash [reflog (hyper-link)] (try cat .git/logs/refs/stash), and can be deleted with git stash drop stash@{n}, where n is the number shown by git stash list.
stash@{0}, otherwise  must be a valid stash log reference of the form stash@{}.
You can find them in /var/log within your root Magento installation
There will usually be two files by default, exception.log and system.log.
If the directories or files don't exist, create them and give them the correct permissions, then enable logging within Magento by going to System > Configuration > Developer > Log Settings > Enabled = Yes
To create your custom log file, try this code
You can find the log within you Magento root directory under
there are two types of log files system.log and exception.log
you need to give the correct permission to var folder, then enable logging from your Magento admin by going to
system.log is used for general debugging and catches almost all log entries from Magento, including warning, debug and errors messages from both native and custom modules.
exception.log is reserved for exceptions only, for example when you are using try-catch statement.
To output to either the default system.log or the exception.log see the following code examples:
You can create your own log file for more debugging
These code lines can help you quickly enable log setting in your magento site.
Then you can see them inside the folder: /var/log under root installation.
[More detail in this blog (hyper-link)]
We log in as her, and give her a password.
Logout of psql by typing \q or ctrl+d.
my solution on PostgreSQL 9.3 on Mac OSX in bash shell was to use sudo to go into the data folder, and then append the necessary lines to the pg_hba.conf file to allow for all users to be trusted and be able to log in.
This method allows anyone that
  can connect to the PostgreSQL database server to login as any
  PostgreSQL user they wish, without the need for a password or any
  other authentication.
If a
user is already authenticated by the OS and logged in, PostgreSQL
won't require further authentication and will grant access to that
user based on whatever privileges (roles) are assigned to it in the
database.
allowed me to login successfully.
To check which line of configuration is use make sure to look at the default log file for messages.
In my case, solution here: (for people who concerned)
login to postgres:
First, log as user root then enter to psql by the following commands
One of the reason it can't be rooted is this phone had removed the
  "Logcat" mechanism from android.
I have complete logcat logging on my Huawei Sonic (U8650) now.
Find the line that says: rm /dev/log/main
Change the line to: # rm /dev/log/main (comments out the line, rm = remove, in case you were wondering)
Go to "ProjectMenu" -> "Background Setting" -> "Log Setting"
Open "Log switch" and set it to ON.
Open "Log level setting" and set the log level you wish.
My cellphone had a jellybean version oriented to performance (To be exact a Slim Bean ROM) with a Semaphore kernel (You can recognize it because when the cellphone is starting, the semaphore logo is shown).
These kind of ROM have by default disabled the logging so I had to download the Semaphore Manager application (You can download it from [here (hyper-link)]).
After that, in the section "Modules" I had to turn on the Logger option as it is shown in the picture
Restart and the logcat should be working now!
On Firefox you can use an extension called [FirePHP (hyper-link)] which enables the logging and dumping of information from your PHP applications to the console.
[http://www.studytrails.com/blog/using-firephp-in-firefox-to-debug-php/ (hyper-link)]
However if you are using Chrome there is a PHP debugging tool called [Chrome Logger (hyper-link)] or [webug (hyper-link)] (webug has problems with the order of logs).
But you can use [error_log (hyper-link)] for logging and [various I/O streams (hyper-link)] can be written to with [fwrite (hyper-link)].
Group console logs by request.
I have abandoned all of the above in favour of [Debugger & Logger (hyper-link)].
Log SQL queries
Some great answers that add more depth; but I needed something simpler and more like the JavaScript console.log() command.
The JavaScript console.log doesn't work in that case; it breaks the XML output.
Set the PHP error_log variable in the .ini file to write to that file
Use the error_log('myTest'); PHP command to send messages
And you can switch off less important logs by limiting them using the $debug value.
If you want write to the PHP log file, and not the JavaScript console you can use this:
error_log("This is logged only to the PHP log")
Reference: [error_log (hyper-link)]
For Chrome there is an extension called [Chrome Logger (hyper-link)] allowing to log PHP messages.
The Firefox DevTools even have [integrated support for the Chrome Logger protocol (hyper-link)].
To enable the logging, you just need to save the ['ChromePhp.php' file (hyper-link)] in your project.
For Ajax calls or XML / JSON responses, where you don't want to mess with the body, you need to send logs via HTTP headers, then add them to the console with a web extension.
It is super simple to use, allows you to pass as many arguments as you like, of any type, and will display the object contents in the browser console window as though you called console.log from JavaScript - but from PHP
NOTE: func_num_args() and func_num_args() are PHP functions for reading a dynamic number of input arguments, and allow this function to have infinitely many console.log requests from one function call.
I might be late for a party, but I was looking for an implementation of logging function which:
takes a variable number of comma separated arguments, just like javascript console.log(),
is distinguishable from a common javascript console.log().
It can be an issue for javascript as well (in a term of old browsers), because it creates a trailing comma after console.log() arguments – which is not legal until ES 2017.)
Second argument is a flag to differentiate normal logs against errors.
I think best solution is to use
error_log(content)
[This is output (hyper-link)]
Have you tried testing your Api\ApplicationEvents\LogWritter::write( $exception, 'Error'); code in isolation?
Sure, check [formatters (hyper-link)] in logging docs.
%(pathname)s  Full pathname of the source file where the logging call was issued(if available).
%(funcName)s  Name of function containing the logging call.
%(lineno)d    Source line number where the logging call was issued (if available).
On top of [Seb's very useful answer (hyper-link)], here is a handy code snippet that demonstrates the logger usage with a reasonable format:
To build on the above in a way that sends debug logging to standard out:
Putting the above into a file called debug_logging_example.py produces the output:
Then if you want to turn off logging comment out root.setLevel(logging.DEBUG).
For devs using PyCharm or Eclipse pydev, the following will produce a link to the source of the log statement in the console log output:
Different from the other answers, this will log the full path of file and the function name that might have occurred an error.
After the deletion, come back to gitbash and git pull should prompt the dialog for you to enter your credentials.
I could not clone a repository due to being logged on with other credentials.
Now start pushing the code and you will get GitHub popup to login again and now you are done.
Everything will work properly after successful login.
The --diff-filter option works with both diff and log.
that logs the warning and check its references and references of other projects that use the same common assembly that differs in version.
In my case warning was due to me adding a reference using Resharper prompt as opposed to the Add Reference dialog, which did it versionless even though both v4 and v12 are available to choose from.
In the MSBuild log with /v:diag verbosity it looked like the following.
These reference conflicts are listed in the build log when log verbosity is set to detailed.
These reference conflicts are listed in the build log when log verbosity is set to detailed.
As per the other answers, set the output logging level to detailed and search there for conflicts, that will tell you where to look next.
Check the output window(Ctrl+Alt+O) in VS to see the changes in the build log.
dotnet run --verbosity diagnostic >> full_build.log
Once the build is complete you can search through the log file (full_build.log) for the error.
You could use this.toString() to get a unique identifer for the specific class in which you print to the log.
For a while I was a fan of using the short class name, I mean LOG_TAG = MyClass.class.getSimpleName().
I found them harder to filter in the logs because there was less to go on.
I like to improve Yaniv answer
if you have the log in this format (filename.java:XX) xx line number you can link the shortcut the same way gets linked when there's an error, this way I can get direct to the line in question just by click on the logcat
Go to Android Studio -> preference -> Live Templates -> AndroidLog then select Log.d(TAG, String).
android.util.Log.d(TAG, "$METHOD_NAME$: $content$");
android.util.Log.d("$className$", "$METHOD_NAME$: $content$");
Now when you type the shortcut logd it will put
The following is the logging method:
Shorter than Log.d(TAG,
Won't forget to delete Log.d ever as I just have to delete the method and the locations of all logs get marked red.
The TAG has a prefix of CCC (a short, easy to type string) so that it is easy to list only your logs in android monitor in Android Studio.
A prefix like CCC helps as it gives you logs chronologically with the activity in which it occured
AndroidStudio has a logt template by default (you can type logtand press tab to have it expand to a sinppet of code) .
In order to Log the correct logging, errors and warns can be send to third party crash libraries, such as Firebase or Crashlytics.
note that this makes the Logs available only during DEBUG state and facilitates you the task of removing them manually for the launch on Google Play -
when release the app on the play store, we need to remove all Log statement from the app, so that none of the application data such as user information, hidden application data, auth-tokens are available to user in logcat as plain text
check out this article [https://medium.com/mindorks/better-logging-in-android-using-timber-72e40cc2293d (hyper-link)]
See the [git log manpage (hyper-link)] for details.
This lengthy log call will give you a nice view filtered by committer sans merge.
You need to call the console.log with the correct context (console):
thereafter my_log_alias is the same as console.log and can be called in the same way; Calling this from inside the function will report the line number for that function call, including the line inside of an alias or advice function where applicable.
Specifically, the line number Chrome provides will tell you the file the line is in, so what you are doing may be unneccesary; Consider reporting this as a bug/feature request in chrome that it provide this info in console.log.
The extra log content has to be calculated at bind time; it cannot be time sensitive or depend on the incoming log message in any way.
The extra log content can only be place at the beginning of the log message.
This may vary depending on setup, and how i got the splits was to just log the whole stack, and find the information I needed.
Here is the whole thing, preserving the native object logging.
You can also add nested context to you logging, e.g.
console.nest("my").log("test") will output >my> test.
My workaround if you are interested is to assign console to another variable and then wrap all my log messages in a function which lets me modify/style/whatever on the message.
Yes, it is possible to add information without messing up the originating line numbers of the log invocation.
Some of the other answers here came close, but the trick is to have your custom logging method return the modified logger.
log("DEBUG:")("A debug message"), log("INFO:")("Here is some info"), etc.
The only really import part about the function (in regards to line numbers) is that it returns the logger.
I ran into this issue as well about extending console.log() so that the application can extend, control and do fancy stuff with it in addition to logging stuff to the console.
Now the console gets log messages with their appropriate line information as well as our app contains an array of log messages that can be put to use.
For example, to display your in-app log using HTML, JQuery and some CSS the following simplistic example can be used.
This is a simplistic use, but once you have the mechanism in place, you can do whatever your imagination can come up with, including leaving the log lines in the code, but setting a threshold so that only warnings and errors get through.
An acceptable solution can be to make your own log-function that returns a console.log function bound with the log arguments.
This way the console.log call will be made from the correct line, and will be displayed nicely in the console, allowing you to click on it and everything.
Try setTimeout(console.log.bind(console,'foo'));
The solutions with a huge hack around throwing an Error for each log can show the right line, but it will not be a clickable link in your console.
And you can make a beautifulLog like this:
You should use xlog=TRUE, ylog=TRUE, instead of log="xy".
The names will show up and the log is not a graphical parameter warnings will disappear.
Anyway you have some 0 values, and you cannot plot log(0), that's the reason of the other warnings
Please check that the user account running PHP CLI has write access to /var/log/php_errors.log.
as a diagnostic you can attempt to force a write to the error log this way.
you should now see test 123 in your log
The logging/reporting behaviour of PHP is dependant on error_reporting too.
Some PHP frameworks (for example CodeIgniter) execute an error_reporting(E_STRICT) statement or equivalent, when in production mode, which will severely reduce the number/kind of logged errors.
This question and answer thread was very helpful to me while setting up PHP CLI logging on an Ubuntu 12.04 environment, so I wanted to post an answer that distills what I learned.
In addition to the great info provided by [David Chan (hyper-link)] as well as [George Cummins (hyper-link)] I have created a logrotate.d script to ensure the PHP CLI error log doesn’t grow out of control as well as set this up so multiple users will be able to log errors to the common PHP CLI error log.
First, the default behavior of the PHP CLI is to log error messages to standard output; logging to a file is not default behavior.
Which usually means logging to the same command line terminal session that is running the PHP CLI command.
While the PHP ini file does have accommodations for a specified error_log additional accommodations need to be made to truly make it work.
First, I had to create an initial php_errors.log file:
And in this case, I want the php_errors.log to be readable and writable by www-users I change the ownership of the file like this:
Yes, from a security standpoint having a log file readable and writable by anyone in www-users is not so great.
And besides, when PHP is run from the CLI, any user who can do that will need write access to the logs anyway to even get a log written.
Next, go into /etc/php5/cli/php.ini to adjust the default Ubuntu 12.04 settings to match this new log file:
Happily log_errors is enabled by default in Ubuntu 12.04:
But to allow logging to a file we need to change the error_log to match the new file like this:
Now that should be it, but since I don’t want logs to run out of control I set a logrotate.d for the php_errors.log.
Create a file called php-cli in /etc/logrotate.d/ like this:
And place the contents of this log rotate daemon script in there:
So check the actual php_errors.log for the test error message like this:
As you mentioned in a comment, you're using log4j.
The logger API actually takes Object as the first argument, not a string - and then it calls toString() on the argument.
(Your question is still unclear on whether your code is calling printStackTrace() or this is being done by a logging handler.)
The logger / handler being used has been configured to only output the exception's message string, not a full stack trace.
Your application (or some third-party library) is logging the exception using LOG.error(ex); rather than the 2-argument form of (for example) the log4j Logger method.
The exception that is being logged has overloaded some methods to obscure the stacktrace.
It turned out that, for some crazy reason, if a NullPointerException occurred at the same place in the code multiple times, after a while  using Log.error(String, Throwable) would stop including full stack traces.
Try looking further back in your log.
When that exception occurs often enough, the stack trace is not printed anymore, both to achieve better performance and not to flood the log with identical stack traces.
This blog post helped me efficiently detect size changes to DOM elements.
By default docker uses json-file logging driver.
This driver saves everything from container's stdout and stderr into /var/lib/docker/containers/<container-id>/<container-id>-json.log on your docker host.
The docker logs command just reads from that file.
By default postgresql logs into stderr #log_destination = 'stderr'
You enabled the logging collector which catches the logs sent to stderr and saves them into filesystem instead.
This is the reason why you don't see them anymore in docker logs output.
I don't see anywhere in postgresql documentation how to send logs both to file and stderr.
[https://www.postgresql.org/docs/9.5/runtime-config-logging.html (hyper-link)]
Containers should log into stderr and stdout.
process to log into file inside container's filesystem is considered bad practice.
If you insist your only chance is to change config line log_filename into something static like postgresql.log and create symbolic link (and rewrite the postgresql.log file created by psql) pointing it to it to stderr ln -fs /dev/stderr /var/lib/postgresql/data/log/postgresql.log.
I haven't tested the solution and I have certain doubts about loggin collector and its log_rotate capabilities.
I have no idea what happens with postgresql.log when log file is rotated.
You can try to disable log_truncate_on_rotation boolean to overcome this but I'm not sure if this would help tbh.
There are no request logging facilities available in elasticsearch 0.17.6.
The version 0.18.3 and above supports [logging of slow search operations (hyper-link)], which can be configured with threshold of 0ms to log all search requests for all shards.
If you are interested in logging all HTTP requests, the [elasticsearch-jetty (hyper-link)] plugin supports this functionality for elasticsearch 0.18.4 and above.
config/logging.yml
The preferred way to configure logging in Elasticsearch 5.x and above is using the API:
Also, Scott has a good blog post about filter interceptors on controllers/ actions [here (hyper-link)].
It will then log exceptions created at controller or page level.
It can be configured to log to various different places (like SQL Server, Email etc).
It also provides a web frontend, so that you can browse through the log of exceptions.
I still use log4net, but I tend to use it for logging debug/info, and leave all exceptions to Elmah.
You can also find more information in the question [How do you log errors (Exceptions) in your ASP.NET apps?
MVC3

Create Attribute that inherits from HandleErrorInfoAttribute and includes your choice of logging
Make the graph bigger and you'll see that O(n logn) isn't quite a straight line.
To see why, just take the logarithm of a few very large numbers.
So, to sort 1,000,000 numbers, an O(n logn) sorting adds a measly factor 6 (or just a bit more since most sorting algorithms will depend on base 2 logarithms).
In fact, this log factor is so extraordinarily small that for most orders of magnitude, established O(n logn) algorithms outperform linear time algorithms.
log(N) is (very) roughly the number of digits in N.  So, for the most part, there is little difference between log(n) and log(n+1)
Which is why O(nlog(n)) is so good!
Usually the O( n*log(n) ) algorithms have a 2-base logarithmic implementation.
For n = 1024, log(1024) = 10, so n*log(n) = 1024*10 = 10240 calculations, an increase by an order of magnitude.
So, O(n*log(n)) is similar to linear only for a small amount of data.
Tip: don't forget that quicksort behaves very well on random data and that it's not an O(n*log(n)) algorithm.
It has been shown to be asymptotically n α(n) where α(n) is the inverse of the [Ackermann function (hyper-link)] (though your usual algorithms textbook will probably only show a bound of n log log n or possibly n [log* (hyper-link)] n).
For any kind of number that you will be likely to encounter as the input size, α(n) ≤ 5 (and indeed log* n ≤ 5), although it does approach infinity asymptotically.
FYI, quicksort is actually O(n^2), but with an average case of O(nlogn)
FYI, there is a pretty big difference between O(n) and O(nlogn).
The derivative of log(x) is 1/x.
This is how quickly log(x) increases as x increases.
When thinking of O(log(n)), I think of it as O(N^0+), i.e.
The difference between logs of two different bases is a constant multiplier.
Look up the formula for converting logs between two bases:
(under "change of base" here: [https://en.wikipedia.org/wiki/Logarithm (hyper-link)])
The trick is to treat k and b as constants.
The messages are written both to the screen and the 'example2.log'.
shows that FileHandler() is not attached to the root logger on Python <3.3.
The docs for [logging.basicConfig() (hyper-link)] say that handlers argument is added in Python 3.3.
In the example below, you can specify the log destination based on its level.
For example, the code below lets all logs over the INFO level go to the log file, and all above ERROR level goes to the console.
Reusable logger function.
On Other python file, import the logger
You cannot just assume that the whole algorithms has a time complexity of log log n. Since your first loop has a complexity of log n, you are correct in saying that the second loop has a computation time of log(log n), the whole algorithm still has a time complexity of log(n) though.
If you have an algorithm with a computation time of 2500 log(n), it can still be considered log(n) in terms of Big O notation.
Note that for moderate values of N, the difference between log N and log log N is really unimportant !
Have you confirmed that the pages cannot be accessed before a login has occured?
Can you post the web.config settings and login code that you are using?
It is notoriously hard to get IE to flush it's cache, and so on many occasions, even after you log out, typing the url of one of the "secured" pages would show the cached content from before.
(I've seen this behaviour even when you log as a different user, and IE shows the "Welcome " bar at the top of your page, with the old user's username.
It could be that you are logging in from one subdomain (sub1.domain.com) and then trying to logout from a different subdomain (www.domain.com).
By tracing, control passes from RedirectToLoginPage() statement to the next line without to be redirected.
Either to modify FormsAuthentication.RedirectToLoginPage(); to be
It has been redirected immediately to the login url before hitting the break point.
But only in a specific case, where some other logic caused a redirect.
Here's an analogy for what seems to be going on... A new visitor, Joe, comes to the site and logs in via the login page using FormsAuthentication.
When he logs out, I abandon the Session so I don't have his name anymore.
I'm using mvc3 and it looks like the problem occurs if you go to a protected page, log out, and go via your browser history.
I just tried some of the suggestions here and while I was able to use the browser back button, when I clicked on a menu selection the [Authorize] token for that [ActionResult] sent me right back to the login screen.
Here is my logout code:
1. x64igor gave the example to do the Logout:
You first need to Clear the Authentication Cookie and Session Cookie by passing back empty cookies in the Response to the Logout.
Phil Haselden gave the example above of how to prevent caching after logout:
Just try to send a session variable when you press log in.
will effectively log out the user.
log(y) = k log(x) + q, so
y = exp(k log(x) + q) = exp(k log(x)) * exp(q) = exp(log(x^k)) * exp(q) = A x^k
You can try to use that one [https://github.com/jiaz/nginx-http-json-log (hyper-link)] - addition module for Nginx.
addition module for Nginx [nginx-http-json-log (hyper-link)]
Use any language as done in [nginx-json-logformat (hyper-link)] with example  /etc/nginx/conf.d/[json_log.conf (hyper-link)]
PS:
[The if parameter (1.7.0) enables conditional logging (hyper-link)].
A request will not be logged if the condition evaluates to “0” or an empty string:
You can test the output of your new logging format and make sure it’s real-and-proper JSON.
[http://nginx.org/en/docs/http/ngx_http_log_module.html#log_format (hyper-link)]
It looks like if you use the add_file_log helper it automatically calls scan_for_files.
You'll see that after it ran 10 times, Logs/ contains:
Check the specific version of SDK tools that is complaining (for me, it was 29.0.3 as shown above in the first screenshot) from the list, accept the agreement in the dialog that opens next and you're done.
I got this problem when i using AVPlayer Foundation on iOS Simulator (xcode 8.1).However it doesn't log anymore on iOS devices.
In my opinion it's a log bug.The player or the layer is destroyed.
i got this for you [fix unwanted log messages (hyper-link)]
This is a bug with logs in Xcode8 + iOS10.
However, if the purpose is simply to have a record of what happened—the most typical use of an audit log—then why not simply keep everything:
So just line up your business requirements for your audit logging.
We’ll create a small example database for a blogging application.
blog: stores a unique post ID, the title, content, and a deleted flag.
audit: stores a basic set of historical changes with a record ID, the blog post ID, the change type (NEW, EDIT or DELETE) and the date/time of that change.
The following SQL creates the blog and indexes the deleted column:
All columns are indexed and a foreign key is defined for audit.blog_id which references blog.id.
Therefore, when we physically DELETE a blog entry, it’s full audit history is also removed.
If all you want is to "log the progress of the page.evaluate to the console", then just
And use console.log in page.evaluate as usual, no more dependencies are required.
Also see [this nice tweak (hyper-link)] to remove multiple annoying warnings from log.
I like @[Vaviloff (hyper-link)]'s answer, but you will log the whole ConsoleMessage object when you may just want the text.
shows no error logs but only the log I created.
So when you are using page.evaluate() or page.evaluateHandle() and you want to get the console.log() text from the browser context back to node, use the following code and make sure to set the listener before any console.log() calls:
You can't see the console.log() text in your node console or set node breakpoints inside page.evaluate() or page.evaluateHandle(), because the code inside those functions is running only in the browser context.
If you would launch puppeteer in none headless mode you would see the console.log() message showing in the browser.
In most cases you don't really need to log inside the browser context and you can do the same work in the 'Console' tab of your browser 'Developer tools' section.
Print all the console outputs to stdout including warning, error, log:
[code snippet]
Print only logs (Ex: console.logs).
I suggest you take a look at logging.handlers.TimedRotatingFileHandler.
This piece of code will create a my_app.log but the log will be moved to a new log file named my_app.log.20170623 when the current day ends at midnight.
This above code will generate file like my_app.log for current day and my_app.log.20170704 for previous day.
The specified file is opened and used as the stream for logging.
A RotatingFileHandler allows us to rotate our log statements into a new file every time the current log file reaches a certain size.
Upon execution of this you should notice that every time app.log exceeds 500 bytes, it is then closed and renamed app.log.x where the value of x increments till it reaches whatever we have set backupCount to.
The specified file is opened and used as the stream for logging.
TimedRotatingFileHandler allows us to capture log files by a time slice.
Running this code will then create new log files every minute indefinitely.
We can set the backupCount parameter on our logHandler instance and it will cap the number of log files we create.
With the TimedRotatingFileHandler and the RotatingFileHandler it is possible to do the following things such as log all error messages to a rotating file, but all normal log files to a TimedRotatingFileHandler as we hope that we can expect far more of them than error messages.
Two levels of records are split out two different log levels: INFO and ERROR to two distinct places.
You should notice that 3 log files are created when you run this.
The error.log will contain only logs that are of level ERROR or higher.
The normal.log will contain a combination of all log messages logged out of our application.
These are just a few things I feel are important when implementing your own logging system.
In the above example, I have used dictConfig to configure the logger.
Please refer the link: [https://docs.python.org/3/library/logging.config.html (hyper-link)] to know the other ways of configuration.
Once the day change, the logger module creates a new file by suffixing the current file with the date.
The logging functions will convert the '%s' to string representation (and if the object happens to be a container, then repr() will be used for the contained objects)
How it shows in the log file:
Whereas the logging module would figure out a good representation for the object
and then log the string dictionary variable to the file
I came to this question when I wanted to log JSON lines for Cloudwatch.
I ended up using [python-json-logger (hyper-link)].
Install it: pip install python-json-logger
Use pythonjsonlogger.jsonlogger.JsonFormatter as the formatter class.
If that's the case, you can Python's built-in pprint to pretty format the dictionary before logging:
Boy you should post your error log with LANG=C or people can't get the real cause from your log.
The log above says: No such file or directory.
I have tried everything until I realized a new cygwin toolchain has messed up python logic.
Using Lua one can do pretty cool things, including logging all of the headers to say, Redis or some other server
Inspect your error log.
Based on @user1778602’s response the [set_by_lua_block (hyper-link)] can be used to set all headers into a variable to be consumed later at the log_format (see more details in [this answer (hyper-link)]).
The update your log_format (be sure to use the new format in e.g.
access_log):
PS: Beware of logging PII data may be violating GDPR :)
There are two more options to log headers in nginx.
Headers may be logged to error or access log.
Declare a variable to use in log_format directive (~1 line)
Add this variable to log format (~1 line)
Import js module, declare variable and add it to log_format:
By default strings in access log are escaped, so you will get something like this:
You can use escape parameter in [log_format (hyper-link)] directive to change how escaping is applied.
With njs you can use ngx.log or r.log (for older versions of njs ngx object is not available) in javascript function to log headers.
Enable logging in location:
For error log escaping is not applied, so you will get raw json:
Logging to error log may be useful if you don't want to mess up your access logs or you need to log headers only for specific location (for specific location access_log directive and separate log_format can be used too)
To log headers simple http server or just netcat oneliner may be used:
Because netcat doesn't reply to nginx, nginx will fill up error log with timeout errors, this errors do not affect clients.
This retrieves the log file:
Finally, if you get stuck with log4net add this to your <appSettings> section:
Since I already had a logger defined in the class I just used it.
I'm 99% sure that RollingFileAppender/DailyRollingFileAppender, while it gives you the date-rolling functionality you want, doesn't have any way to specify that the current log file should use the DatePattern as well.
You might just be able to simply subclass RollingFileAppender (or DailyRollingFileAppender, I forget which is which in log4net) and modify the naming logic.
I don't know if it is possible in Java, but in .NET the property StaticLogFileName on RollingFileAppender gives you what you want.
&quot;.log&quot; is for not letting the dateformat recognice the global date pattern 'g' in log.
Using log4j.properties file, and including [apache-log4j-extras (hyper-link)] 1.1 in my POM with log4j 1.2.16
this example will be creating logger for each minute, if you want to change for each day change the DatePattern value.
Even if u use DailyRollingFileAppender like @gedevan suggested, u will still get logname.log.2008-10-10 (After a day, because the previous day log will get archived and the date will be concatenated to it's filename).
So if u want .log at the end, u'll have to do it like this on the DatePattern:
log4j.appender.file.DatePattern='.
'yyyy-MM-dd-HH-mm'.log'
So, you are adding a bunch of things that are O(log n).
Well, about log(n) worth.
No, it's not O(log n).
It is O((log n)2).
The [grep (hyper-link)] command excludes (the -v parameter) every line which starts with seven symbols (which is the length of my Git hash for the git log command) followed by space.
I stumbled in here looking for a similar answer without the "git log" restriction.
(*) is because for 'high enough' values of n (n>N for some constant N), n/2log(2) < n/4log(n), so we reduce 'bigger' element - which result in lower outcome.
So, we got that log(n!)
>= 1/4*nlog(n)  - which gives us that it is in Omega(nlogn) by definition with c=1/4.
Regarding the first part, how we got to log(n/2) + log(n/2+1) + ... +log(n)
I think you are considering the proof that O(log(n!))
= O(n*log(n)) and if that is the case, then you need to use [Sterling's approximation (hyper-link)].
If you use this approximation you can show that for any value c < 1 there should be a value N such that for n >= N c * n*log(n) < log(n!
and taking the logarithm and dividing by 2
so that the lower bound has a leading term of 1/2*n*log(n) and the upper bound has a leading term of n*log(n).
I recommend that you don't use a control as your log at all.
Instead write a log collection class that has the properties you desire (not including the display properties).
Personally, I would put SendToEditControl and SendToListBox methods into my logging object.
You can update the UI log only as often as it makes sense, giving you the best possible performance, and more importantly, letting you reduce the UI overhead when the log is changing rapidly.
The important thing is not to tie your logging to a piece of UI, that's a mistake.
In the long run, a good UI for a logger is probably a custom control.
But in the short run, you just want to disconnect your logging from any specific piece of UI.
Our approach was to keep a ring buffer of the scrollback records and just paint the log text manually (with Graphics.DrawString).
Here is something I threw together based on a much more sophisticated logger I wrote a while ago.
This will support color in the list box based on log level, supports Ctrl+V and Right-Click for copying as RTF, and handles logging to the ListBox from other threads.
I'll store this here as a help to Future Me when I want to use a RichTextBox for logging colored lines again.
My solution to creating a basic log window was exactly as [John Knoeller (hyper-link)] suggested in his answer.
Avoid storing log information directly in a TextBox or RichTextBox control, but instead create a logging class which can be used to populate a control, or write to a file, etc.
The logging class itself, Logger.
The main form to demonstrate its use, LoggerExample.
First, the logging class:
The Logger class incorporates another class LogEntry which keeps track of the line number, timestamp, and desired color.
This form is created with two timers, one to generate log entries pseudo-randomly, and one to populate the RichTextBox itself.
In this example, the log class is instantiated with 100 lines of scroll-back.
The timer to generate text is at a 100ms interval while the one to update the log window is at 1000ms.
This logging class was only designed for a few hundred lines of scroll-back.
Adding to #2 above, some of my projects check if the log has any new entries before redrawing the RichTextBox content, to avoid unnecessarily refreshing it.
The timestamp on each log entry can be made optional and allow different formats.
There is no way to pause the log in this example, but many of my projects do provide a mechanism for pausing the scrolling behavior, to allow users to manually scroll, select, and copy text from the log window.
Spring Boot uses Commons Logging for all internal logging, but leaves the underlying log implementation open.
Default configurations are provided for Java Util Logging, Log4J, Log4J2 and Logback.
In each case loggers are pre-configured to use console output with optional file output also available.
From the [Spring Boot logging documentation (hyper-link)].
The default log configuration will echo messages to the console as they are written.
By default, Spring Boot logs only to the console and does not write log files.
If you want to write log files in addition to the console output, you need to set a logging.file or logging.path property (for example, in your application.properties).
Below codes in your application.properties will write the log into /home/user/my.log:
By default Spring Boot does not output logs to any file.
If you want to have logs written in a file (in addition to the console output) then you should use either of logging.file or logging.path properties (not both).
This will create a spring-boot-app.log file under /home/ubuntu.
For log, a helpful little ditty to add in the member variable.
The following code templates will both create a logger and create the right imports, if needed.
Log4J 2
Log4J
Also for logging:
log
The template for the logger declaration is great.
I also create linfo, ldebug, lwarn, lerror for the log levels that I use more often.
Logger setup
Log with specified level
Log and rethrow a caught exeption
With help of plugin: [http://code.google.com/p/eclipse-log-param/ (hyper-link)]
This requires a logger (called _logger: there is a very nice template for that in this thread as well).
My favorite template for a test case is one that logs exceptions and then rethrows them since I like to see the exceptions in the console rather than in JUnit's exception viewer.
Should you prefer System.out in your tests over log files, you can always use something similar like:
Post Java 7, a great way to set up loggers which need (or prefer) static references to the enclosing class is to use the newly introduced MethodHandles API to get the runtime class in a static context.
Verbose (Logv)
Debug (Logd)
Info (Logi)
Warn (Logw)
Error (Loge)
Assert (Loga)
This takes a lot of the grunt work out of printing / logging local values.
This prints an entire object (assumes you have already initiated a log4j LOGGER object):
One potential solution would be to list all the branches which contains your commit: "[How to know which branch a “git log” commit belongs to?
And then do a git log for each of those branches.
For the lambda function to create log stream and publish logs to cloudwatch, the lambda execution role needs to have the following permissions.
So if you just click 'test' button from Lambda console after you update your role policy in IAM, the cached Lambda instances will still have old role permissions, so you will still see no logs being written to Cloudwatch logs.
Just change your timeout by a second and click on 'save and test' button, and you will start to see logs in Cloudwatch.
For the lambda function to create log stream and publish logs to cloudwatch, the lambda execution role needs to have the following permissions
Just change your timeout by a second and click on 'save and test' button, and you will start to see logs in Cloudwatch.
I changed the timeout, saved and logs still did not work.
I assigned another role and logs still did not work.
This was it and logs started being generated but since I did not want to use a new role but my existing role, I simply assigned my existing role afterwards and it worked.
Maybe a bit late, but for those who still struggle with seeing the lambda logs in cloudwatch.
Note that the role must be assumable by Lambda and must have Cloudwatch Logs permissions."
So in IAM i granted " CloudWatchLogsFullAccess" to the role i assigned to my function.
then in cloudwatch, under logs, you'll see the logs for the functions assigned this role.
For the issue was I was trying to create a log group in the Cloudformation script by :
AWS::Logs::LogGroup
and then trying to push the Lambda log to this log group.
:P Novice
After careful reading , i found that Lambda creates its own log with the aforementioned format:
 /aws/lambda/
We just need to provide policy permission to this log group , or just a generic permission with resource as:
arn:aws:logs:::*
Apparently another necessity for logging to happen is the Lambda function must indicate completion; for instance in the Python context, the handler must return something other than None.
As other answers state you need to give lambda permission to post logs to cloud watch logs.
You can add this policy in your role which is assigned to your lambda and you should start seeing the logs.
If you do not want it you can create a custom policy with just the logs part.
It might already log, we just couldn't find the logs we expect...
CloudWatch Logs: (can you easily find the exact log below?)
Conclusion: too verbose to locate the original logs.
CloudWatch & CloudWatch Logs are different Permissions, you need add CloudWatch Logs to the policy which attached with your role.
There's a writeup called [How to Monitor AWS Lambda with CloudWatch (hyper-link)] with a section on "How to Use CloudWatch Logs with Lambda".
You might want add the Last Ingestion Time column to the log output.
Even the permissions are set appropriately so that lambda can log in cloudwatch, sometimes it is taking 3-4 hrs to reflect the log groups.
It took 4 hrs to get the logs reflected.
After 4 hrs without any action from my end, I was able to see the Cloudwatch logs.
Also make sure that you are seacrching the logs in the same region where your function is created.
Logs may not be in us-east-1, try looking for lambda edge logs in different regions !
In the first phases of component development log everything.
Usually I do all of my development on the console that way I can see all of my logic being output line by line if need be.
As you move forward logging errors is always a must and logging values after complex data manipulation can be helpful as well.
I recommend logging to a flat file over the database.
Log
I would err on logging more rather than little, and removing or filtering if/when this becomes a problem (as noted below, logging Gbs per day is probably counterproductive).
I've worked on numerous systems where it's been decreed that logging will be turned down or removed post development, and this never occurs, since it's so useful.
Some people object to logging on the basis that it affects performance.
You should be able to find suitable frameworks for logging (e.g.
[log4c (hyper-link)] for C, [log4j (hyper-link)] for Java) for your particular platform that allow appropriate filtering and destination selection.
That means you can log to files (and restrict log sizes), databases, remote monitors etc.
The right framework should require very little initially other than your inserting appropriate logging statements.
You shouldn't have to write much in the way of file management or other log management code.
Here's a [useful blog entry (hyper-link)] on this subject, with further guidelines.
Definetly log errors to flat files consistently formatted so that you can retreive them from your customer and read them into excel or a db for analysis if needed.
Logging is essential and useful.
Trust me, you will have a hard time figuring out what happened if you don't have logs.
They're pretty suited for logging purposes and you keep your code clean.
Regarding the type of logging: I usually work just fine with plain text files (with a certain max size), unless you have to keep track about all of your user's activities (legal reasons, whatever).
In such a case a DB log would probably be more appropriate.
Logging is always essential, because there may be errors that occur in production and information needs to be output about what happened.
Apart from errors, however, whether you log key entry and exit points, possible time consuming methods/operations etc.
If you don't have performance analysis built into your system, then you either need to log, or be able to turn on detailed logging, in order to track down performance issues.
Similarly, if you don't have real-time debugging tools, you need to log, or be able to turn on logging, for key entry/exit points.
If the system is handling millions of transactions, each with millions of operations, then you don't want to have this level of logging on all the time on every subsystem/process, otherwise your application quickly fills up all available diskspace - that's an issue.
For small, simple systems, it is probably sufficient to have a general default level of logging for production, and a debug level for tracking down problems.
This is a very general question... For starters, I'd say you should have several logging levels.
On the trace level, log everything you have in mind.
On the errors level, have only sever errors logged.
Make it possible to choose logging level on runtime, though the selection should not be available for end users.
Also, have in mind that if you make your logs available to people other than you (your support team, for example), make sure your messages are human readable and not something only you can understand.
There are a lot of logging infrastructures, look around to see what fits your development environment.
It's good to log actions or things that happend in programs(like errors)?
You should also classify them, so you can later turn off logging for a certain class of log messages (e.g.
In my case, it's good to log things?
With ever class you should also log relevant data.
What is the best way to log things(text files, databases...)?
But as long as you can access the logs the medium is the smaller problem.
I would recommend to use a logging framework, since logging is an essential part of your app and way to many people already came up with pretty good solutions.
Logging during development doesn't really matter--do it if it helps; stop doing it when it stops helping.
Good logs become important during production.
List the kinds of problems you'll run into and log the information you need to solve those problems.
Then log what you need to defend yourself.
Then log what you need to answer the questions and minimize your support costs.
It's easy to log too much (useless) information.
If you don't need it, don't log it.
Yes, and you can log to help diagnose those problems, but that's mainly a job for monitoring, not logging.
Syslog provides pretty good management and centralized control.
There are many logging frameworks that work about the same--it's really personal preference which one you use.
I use a structured, searchable log format on top of syslog.
All ad hoc development logging is removed before going live.
In the second example, Octave is interpreting 'a' as a char, converting 'a' to its ASCII representation (97) and then getting the natural logarithm.
Anyway newer version of Matlab and Octave have an error check for character input (there is little reason to compute the logarithm of the ASCII equivalent of a character).
You're reinitializing your log in each iteration of your while loop.
It seems that you need to pass a flag "-l, --log-file"
Kibana doesn't have a log file by default.
but you can set it up using log_file Kibana server property - [https://www.elastic.co/guide/en/kibana/current/kibana-server-properties.html (hyper-link)]
In kibana 4.0.2 there is no --log-file option.
If I start kibana as a service with systemctl start kibana I find log in  /var/log/messages
Kibana 4 logs to stdout by default.
So when invoking it with service, use the log capture method of that service.
One way may be to modify init scripts to use the --log-file option (if it still exists), but I think the proper solution is to properly configure your instance YAML file.
As for the --log-file option, I think this is reserved for CLI operations, rather than automation.
In general, one doesn't expand out log(a + b); you just deal with it as is.
(In fact, this identity is often used when implementing log in math libraries).
--log-group-name is not optional in aws cli, you can try using an * for --log-group-name value (in test environment)
aws logs delete-log-group --log-group-name my-logs
Reference URL: 
[http://docs.aws.amazon.com/cli/latest/reference/logs/delete-log-group.html (hyper-link)]
Hence we achieved this using a script where we first retrieved all the log streams of a log group and then deleted them in a loop.
Here is Script to delete all logs in a log group using python.
Just change the logGroupName to match your logGroup.
You can achieve this through using --query to target the results of describe-log-streams.
aws logs describe-log-streams --log-group-name $LOG_GROUP_NAME --query 'logStreams[*].logStreamName' --output table | awk '{print $2}' | grep -v ^$ | while read x; do aws logs delete-log-stream --log-group-name $LOG_GROUP_NAME --log-stream-name $x; done
aws logs describe-log-streams --log-group-name $LOG_GROUP --query 'logStreams[?starts_with(logStreamName,`2017/07`)].logStreamName' --output table | awk '{print $2}' | grep -v ^$ | while read x; do aws logs delete-log-stream --log-group-name $LOG_GROUP --log-stream-name $x; done
Delete All log groups - Warning, it deletes EVERYTHING!
aws logs describe-log-groups --query 'logGroups[*].logGroupName' --output table | awk '{print $2}' | grep -v ^$ | while read x; do aws logs delete-log-group --log-group-name $x; done
Clearing specific log groups
aws logs describe-log-groups --query 'logGroups[?starts_with(logGroupName,`$LOG_GROUP_NAME`)].logGroupName' --output table | awk '{print $2}' | grep -v ^$ | while read x; do aws logs delete-log-group --log-group-name $x; done
To delete all log streams associated with a specific log group, run the following command, replacing NAME_OF_LOG_GROUP with your group:
You may use -Force parameter on the Remove-CWLogStream Cmdlet in case you don´t want to confirm one by one.
If you have thousands of log streams, you will needed to parallelize.
For Windows users this powershell script could be usefull, to remove all the log streams in a log group:
The others have already described how you can paginate through all the log streams and delete them one by one.
I would like to offer two alternative ways that have (more or less) the same effect, but don't require you to loop through all the log streams.
Deleting the log group, then re-creating it has the desired effect: All the log streams of the log group will be deleted.
[delete-log-group (hyper-link)]
[create-log-group (hyper-link)]
CAVEAT: Deleting a log group can have unintended consequences.
For example, subscriptions and the retention policy will be deleted as well, and those have to be restored too when the log group is re-created.
However, probably the most important reason why one would want to delete all the log streams is to delete the logged data (to reduce costs, or for compliance reasons), and this approach achieves that.
I don't know of a way to mirror the Cypress logs to the console directly, but this is at least a workable alternative.
Expanding on @Joshua-wade's answer, you can overwrite cy.log to redirect all calls to it to the log task.
Cypress.Commands.overwrite('log', (subject, message) => cy.task('log', message));
Note: there's a small drawback to this: when you run the test using the Test Runner, instead of seeing LOG    my message in the command log, you'll see TASK    log, my message.
Cypress community is going to provide native support so that we don't have to do any workarounds to print the logs on non-GUI(headless) CLI.
Setting the [ELECTRON_ENABLE_LOGGING (hyper-link)] environment variable to 1 will cause all Chrome internal logging to be printed to the console.
Prints Chrome's internal logging to the console.
With this enabled, in addition to capturing any existing logging, this will also allow you to manually log within a test using console.log:
From reading [src/core/ngx_log.c (hyper-link)] I guess the general error log format seems to be
With PID and TID being the logging process and thread id and CID a number identifying a (probably proxied) connection, probably a counter.
Also see [Log4perl - how can I make sure my application logs a message when it dies unexpectedly?
The default log format in nginx is called "combined".
[Source: Module ngx_http_log_module (hyper-link)]
Default logs provided by both Apache and NGINX are pretty much identical.
You can replace O(log(log n)) with c log(log n) and be sure that there exists a constant c that 2 ^ O(log(log n)) < 2 ^ (c log(log n)).
Hence, we will have S = 2^ (c log(log n)) =  (2^(log(log n)))^c = log(n)^c.
However, you can't say S = O(log n).
Assuming a base-2 log, then
2log log N = log N,
but 210 log log N is also in 2O(log log N), and
210 log log N = (2log log N)10 = (log N)10
...and that is obviously not in O(log N)
log* n is the [iterated logarithm (hyper-link)], which for large n is defined as
Therefore, log*(log n) = (log* n) - 1, since log* is the number of times you need to apply log to the value before it reaches some fixed constant (usually 1).
Doing another log first just removes one step from the process.
Therefore, log(log* n) will be much smaller than log* (log n) = log* n - 1 since log x < x - 1 for any reasonably large x.
Another, more intuitive way to see this: the log* function is significantly better at compressing large numbers than the log function is.
Therefore, if you wanted to take a large number and make it smaller, you'd get way more efficiency by computing log* n first to contract n as much as you can, then use log on that (log (log* n)) to pull down what's left.
matomo) log imported you can issue the --debug option twice, which will spew the invalid line.
Here is an example of a script that shows it (but this is my preferred log format)
Have you tried git log > log.txt ?
If you want to format it to look nicer, you can use [parameters (hyper-link)] of git log.
You can make log report more clearly, by
(2) display long commit in one line
This command is display commit log in current branch:
Reference: [https://git-scm.com/docs/git-log (hyper-link)]
git log > /C/Users/<user-name>/Desktop/git-log.txt
git log --after="2020-3-20" --pretty=format:'Author : %an %nDate/Time :  %aD%nCommit : %s'   | paste  > log.txt
I write a log like this just now.
[Article of how generate a changelog (hyper-link)]
One tool that you could use is changelog
It will create a well formed CHANGELOG.md file.
Python3 logging has changed a bit.
This is an adaptation of [estani's answer (hyper-link)] to the new implementation of logging.Formatter which now relies on formatting styles.
After some searching around in  the logging class
Lib\logging__init__.py
I found that a data structure has changed from 3.3.3 to 3.3.4 that causes the issue
Instantiate logger
For more you can refer this post : [Advance Logging with Laravel and Monolog (hyper-link)]
Or [Custom Monolog Handler (hyper-link)]
In order to overwrite the default logging config, add the following in your AppServiceProvider:
Maybe this package is useful for your purpose: 
[https://github.com/Edujugon/laravel-log (hyper-link)]
Laravel 5.6 allows adding custom logging "channels" in config/logging.php.
[https://laravel.com/docs/5.6/logging#creating-custom-channels (hyper-link)]
; Default Log file name.
LogDefaultName=teraterm &h %d %b %y.log
; Default path to save the log file.
LogDefaultPath=C:\Users\Logs
; Auto start logging with default log file name.
LogAutoStart=on
"Log" tab
View log editor
Specify the editor that is used for display log file
Default log file name(strftime format)
Specify default log file name.
teraterm-&h-%Y%m%d_%H_%M_%S.log
The setting log_output doesn't affect the binary log.
It affects the general query log and the slow query log.
As I said in my comments it probably not possible to remove the file due to the nature of nginx, my suggestion would be using the same approach, but without actually removing the log file.
The first parameter is the string to be sent to the log.
The third parameter is the log file path.
You can use normal file operation to create an error log.
By default, the message will be send to the php system logger.
We all know that PHP save errors in php_errors.log file.
If we want to log our application data, we need to save it to a custom location.
We can use two parameters in the error_log function to achieve this.
[http://php.net/manual/en/function.error-log.php (hyper-link)]
print_r($v, TRUE) : logs $v (array/string/object) to log file.
3: Put log message to custom log file specified in the third parameter.
'/var/tmp/errors.log': Custom log file (This path is for Linux, we can specify other depending upon OS).
'/var/tmp/errors.log': Custom log file (This path is for Linux, we can specify other depending upon OS).
print_r($v, TRUE) : logs $v (array/string/object) to log file.
It logs exactly the same thing that would have been logged if you didn't catch the exception, minus the word 'Uncaught' at the beginning.
We use Monolog to do the logging in our application.
Monolog has a formatter that can print stack traces.
To log exceptions with traces, we use a LineFormatter and call includeStacktraces() on it.
If you have some data with either x=0 or y=0 you won't be able to print those points on a log-log-plot as [log(0) is undefined (hyper-link)].
There's a good explanation of the abilities of 'symlog' [here (hyper-link)].
There are infinite number of classes that are "between" O(logn) and O(loglogn), for example - look at the family of functions: { f_k(n) = (log(n))^k | k < 1 } (Note that this family of function alone is infinite).
For each f_k as defined above, it is easy to see that f_k is in O(logn) but is NOT in O(loglogn), and is actually defining a new class of functions O(f_k), which is an inequal subset of O(logn), but is not a subset of O(loglogn).
One example for such a function f_k is f_1/2 = sqrt(log(n)).
Proving the above claims is easy with introducing a new variable x=log(n), and then you get:
Looks like the logger is initialized twice.
First time, probably when the app loads and it couldn't resolve the ${log_file_name}.
If you start the app with -Dlog_file_name=*something* you can verify this behavior if it creates another log file with the name *something*
If all you need is to add a timestamp of the log file name, logback already [supports the timestamp element (hyper-link)].
I got this from here [http://logback.qos.ch/faq.html#sharedConfiguration (hyper-link)]
To separate/sift log messages to different files depending on a runtime attribute, you might want to use [ch.qos.logback.classic.sift.SiftingAppender (hyper-link)].
In a nutshell, this allows you to set up a FileAppender (or any other appender) with <file>${userid}.log</file> where ${userId} is substituted based on the [MDC (Mapped Diagnostic Context) (hyper-link)] (e.g., MDC.put("userid", "Alice");).
This is also very close to the output you'd get from svn status or svn log -v, which many people coming from subversion to git are familiar with.
--name-status is the key here; as noted by other folks in this question, you can use git log -1, git show, and git diff to get the same sort of output.
For the last commit, just fire this command: git log -1.
git log -[some #] only shows the log from the CURRENT position of HEAD, which is not necessarily the very last commit (state of the project).
You could view the entire git reflog, until reaching the entry referencing the original clone.
This question is already answered above which states the file names in last commit by git log / other commands.
git log --format=%B -n 1 $(git log -1 --pretty=format:"%h") | cat -
In your log4.properties - do you have this set like I do below and no other org.apache.http loggers set in the file?
Also if you don't have any log level specified for org.apache.http in your log4j properties file then it will inherit the  log4j.rootLogger level.
So if you have log4j.rootLogger set to let's say ERROR and take out org.apache.http settings in your log4j.properties that should make it only log ERROR messages only by inheritance.
Create a commons-logging.properties file and add the following line to it.
org.apache.commons.logging.LogFactory=org.apache.commons.logging.impl.Log4jFactory
Added a completed log4j file and the code to invoke it for the OP.
This log4j.properties should be in your CLASSPATH.
Here is some code that you need to add to your class to invoke the logger.
Commons HttpClient uses Commons-Logging for all its logging needs.
Commons-Logging is not a full logging framework, but rather, is a wrapper around several existing logging frameworks
That means that when you want to control the logging output, you (mostly) end up configuring a library other than Commons-Logging, but because Commons-Logging wraps around several other libraries, it's hard for us to guess which one to configure without knowing your exactly setup.
Commons-Logging can log to log4j, but it can also log to java.util.logging (JDK1.4 logging)
Commons-Logging tries to be smart and guess which logging framework you are already using, and send its logs to that.
If you don't already have a logging framework, and are running on a JRE that's 1.4 or above (which you really should be) then it will probably be sending its log messages to the JDK logging (java.util.logging)
Relying on Commons-Logging's autodiscovery mechanism is prone to error.
Simply adding log4j.jar onto the classpath would cause it to switch which logging mechanism it uses, which probably isn't what you want
It is preferable for you to explicitly tell Commons-Logging which logging library to use
You can do this by creating a commons-logging.properties file as per [these instructions (hyper-link)]
The steps you want to follow to configure the commons-httpclient logging are


Decide which underlying logging framework you want to use.
There are a number of choices, but probably log4j or java.util.logging are the best options for you.
Set-up the commons-logging properties file to point to the correct Log implementation.
to use log4j, put this into the properties file: org.apache.commons.logging.Log=org.apache.commons.logging.impl.Log4JLogger, or to use JDK logging set org.apache.commons.logging.Log=org.apache.commons.logging.impl.Jdk14Logger.
Configure the underlying logging implementation (e.g.
log4j) to ignore the messages you don't want, and output the messages you do want.
The developers at Apache-commons tend to assume you'll already have a logging framework configured, and they can work out which one it is by auto-discovery.
I put this into my log4j config file
like Matt Baker, I just want to shut off httpClient log without too much configuration.
As we were not sure which logging implementation underneath common-logging was used, My solution was to force it using log4j by throwing log4j jar file in the class path.
Default setting of log4j configuration shuts off common-httpclient debug output.
Of course, to make it more robust, you may create common-logging.properties and log4j.properties files to further define your logging configurations.
Update log4j.properties to include:
Note that if Log4j library is not installed, HttpClient (and therefore JWebUnit) will use logback.
In this situation, create or edit logback.xml to include:
Setting the log level to WARN with Log4j using the package name org.apache.commons.httpclient in log4j.properties will not work as expected:
This is because the source for HttpClient (v3.1) uses the following log names:
We use XML, rather than a properties file, to configure our logging output.
I fixed it by excluding logback and adding in slf4j-log4j12, like so:
Please notice that if you use binary distribution then Logback is a default logger.
To use log4j with JWebUnit I performed following steps:
removed Logback jars
add lod4j bridge library for sfl4j - slf4j-log4j12-1.6.4.jar
add log4j.properties
Probably you don't have to remove Logback jars but you will need some additional step to force slf4j to use log4j
It took far too long to find this out, but JWebUnit comes bundled with the [Logback (hyper-link)] logging component, so it won't even use log4j.properties or commons-logging.properties.
Instead, create a file called logback.xml and place it in your source code folder (in my case, src):
Logback looks to still be under development and the API seems to still be changing, so this code sample may fail in the future.
I guess HttpClient uses "httpclient.wire" as its logger name, not "org.apache.commons.httpclient".
Simple way Log4j and HttpCLient (v3.1 in this case, should work for higher, could require minor changes)
in your commons-logging.properties
For log4j, add the following to log4j.properties (in the application's source directory):
For logback, the following logback.xml will kill the noise:
It turned out the issue is that my project had a dependency on http-builder-0.5.2.jar which bundled a log4j.xml file within itself.
And sure enough, the log level for org.apache.http.wire was DEBUG!
The way I found it was just to go through all the jar files in my dependencies and do "jar tvf" and grepping for log4j.
While this discovery led to the eventual solution of upping the version of my http-builder dependency to 0.6, it still baffles me what must have gone through the developer's mind when bundling the log4j.xml file into the jar file.
The best solution I found was to use the maven enforcer plugin in order to prevent commons-logging from being used altogether.
Then I added the slf4j dependency for logging instead.
Add the below lines in the log4j property file and it will shut the http logs :- log4j.logger.org.apache.http=OFF
Setting OkHttpClientHttpRequestFactory should solve problem with trash logging.
Commons-Logging -> Logback and default Info while Debug will not be present;
You can use:
to define the information you want to log:like
[Final Result (hyper-link)]
like this.
Only the information I want to log will be present.
For apache 4.5.3, if you want to move the level for all apache http client logging to Warn, use:
The one soution that came the closest for me was the one suggesting creating a logback.xml.
That worked, however nothing got logged.
After playing around with the logback.xml, this is what I ended up with
Now All levels below DEBUG gets logged correctly.
Log2J 2 2.11.2
With 'httpclient' in the above example being a logical name you choose.
The solution which worked for me was creating a logback-test.xml
src/test/resources/logback-test.xml as in [https://github.com/bonigarcia/webdrivermanager-examples/blob/master/src/test/resources/logback-test.xml (hyper-link)] (ref - [https://github.com/bonigarcia/webdrivermanager/issues/203 (hyper-link)])
To view my logging infos, I replaced  logger name="io.github.bonigarcia" with my package name
Try 'log4j.logger.org.apache.http.headers=ERROR'
For me, the below lines in the log4j.properties file cleaned up all the mess that came from HttpClient logging...
I had to add log4j dependancies in my POM.xml and that resolved unnecessary loggings .
it's work for me with add "logback.xml" in class root path and below setting.
I created a logback-test.xml within src/test/resources with the following:
Started build after this, Junit runner picked up the above mentioned log level "info" and there are no org.apache.http logs.
Since Google also leads here even when searching for a log4j2 solution, here is what you can use.
Add the following piece of XML code into your log4j2.xml``within the Loggers` tags:
Working for org.apache.httpcomponents:httpclient:4.5.13 and org.apache.logging.log4j:log4-core:2.14.1.
[http://docs.crittercism.com/ios/ios.html#logging-handled-exceptions (hyper-link)]
It is not possible to log a caught exception in iOS using Crashlytics SDK.
CLS_LOG can be used to log custom messages, but these log messages will go to Crashlytics only with the next crash data.
If there is no crash, these log messages will never land in the Crashlytics dashboard.
Logging caught exceptions in iOS is there in their roadmap.
[https://docs.fabric.io/ios/changelog.html#january-7-2016 (hyper-link)]
You can use this to log any exception
Event if it's not it's intended usage exceptions are logged in the same way Android handled exceptions are.
This API is not intended be to used to log NSException objects.
You can now capture logged NSErrors in your iOS, tvOS, or OS X app.
This will let you capture a fair number of logged NSErrors per user session.
Logged errors errors are grouped by the error domain and code.
Take a look at the top of your log file and you'll see something like this:
In your case, and presuming you're running with the default log settings, the values would be:
If a field doesn't have a value in the log file then the missing value is shown as a hyphen -.
You can use logging tools like [Yahoo!
UI Library - Logger (hyper-link)] to log the errors/informative messages.
I've written about this for the JSNLog project, at:
[http://jsnlog.com/Documentation/GetStartedLogging/ExceptionLogging (hyper-link)]
(disclaimer: I am the author of JSNLog and jsnlog.com)
I sort of missed that in JavaScript Error object, so I created my own Exception object, also as part of the JSNLog project.
It is in the jsnlog.js file in the jsnlog.js Github project ([https://github.com/mperdeck/jsnlog.js (hyper-link)]).
Description is at:
[http://jsnlog.com/Documentation/JSNLogJs/Exception (hyper-link)]
Finally a shameless plug - the JSNLog project I'm working on lets you insert loggers in your JavaScript, and automatically inserts the log messages in your existing server side log.
So to log JavaScript exceptions with their stack traces to your server side log, you only need to write:
A quick example that will downgrade to console.log and log e if there is no e.stack:
Using named loggers in your modules:
you can set the log level for all the other loggers to error and for your loggers to debug:
You have to set this after importing all modules - it will disable all loggers that were created up to this point.
This will work most of the time, but some modules create their logger when you make a class instance for example (which would happen later in your code).
When you set up loggers according to the basic python tutorial they tell you to use logging.basicConfig(...).
This is a problem as this will set the handler (aka where the log will be routed to) to logging.lastResort which is stderr starting with Python 3.2 for all loggers globally in the process.
This means you now have enabled full logging for all modules.
So a better approach is to create a different logger only for your modules and give it some handlers of its own instead of using basicConfig().
This will give you the logger log that you can then use like log.error("Error found").
It will write to a new file called mylog.log and will also log so sys.stdout.
This will automatically also set 'disable_existing_loggers': True.
The error is thrown in Zend_Log::__call() magic method.
I checked my ZF installation (1.11.4) and on line 280 of Zend/Log.php file there is something else not related to this issue.
Adapted from the log4j documentation:
You can also do this from the log4j.properties file.
Using the sample file below I have added the system property ${logfile.name}:
The log file name can then be set two different ways:
As a command line, system property passed to java "-Dlogfile.name={logfile}"
In the java program directly by setting a system property (BEFORE you make any calls to log4j).
System.setProperty("logfile.name","some path/logfile name string");
Can be also done by this properties define in log4j.properties file
Just remove your log4j code and replace this code in your java class it's working good no need to add properties or xml file
The above code will log the logs for every minute.
winston itself does not support log rotation.
mongodb has a [log rotation use case (hyper-link)].
Then you can export the logs to file names per your requirement.
winston also has a mongodb transport but I don't think it supports log rotation out of the box judging from its API.
[Winston (hyper-link)] does support log rotation using a date in the file name.
Based on that documentation, and the tests for the log rotation features, you should be able to just add it as a new Transport to enable the log rotation functionality.
If you also want to add logrotate (e.g.
remove logs that are older than a week) in addition to saving logs by date, you can add the following code:
where my logger file is:
There's the [logrotator (hyper-link)] module for log rotation that you can use regardless of the logging mechanism.
I got the same kind of error in my C:\xampp\mysql\data\mysql_error.log when trying to start mysql.
Rename aria_log_control to aria_log_control_old
Rename below files from mysql/data
ib_logfile0
ib_logfile1
ibdata1
my.cnf 
innodb_buffer_pool_size to 200M as per your ram
innodb_log_buffer_size to 32M
Like, innodb_flush_method=normal in my.ini file and deleting ibdata1, ib_logfile1, ib_logfile0 files, and others but none of these works.
After starting mysql, I read error-log again and from there I came to know that one of my databases is responsible for this.
Transfer all MySQL projects Database, Data file & Log files
Now you have to copy the data file “ibdata1” & all log files “ib_logfile0, ib_logfile1 ” from data-old files folder to the data folder.
If you have many id_logiles then copied all of them.
First I open Logs for MySql in XAMPP panel.
If you are using Visual Studio 2008, use capital "/L" for the log option.
No, the transaction log is required.
Then on the restored copy change the logging to either bulk logged or simple, shrink the logs, do another backup operation on this new copy and then use that to restore to the target machine with the now much smaller transaction log.
Alternatively, perhaps the contact at the external source could shrink the transaction log before sending it to you (this may not work if the log is large due to a lot of big transactions).
Docs on the command to shrink the log file are [available here (hyper-link)].
This may not work since you have no control over the generation of the .bak file, but if you could convince your source to detach the database and then send you a copy of the .mdf file directly, you could then attach the .mdf and your server would automatically create a new empty transaction log file.
The transaction log is an integral part of the backup.
You can't tell SQL Server to ignore the transaction log, because there is no way to let's say restore and shrink the transaction log file at the same time.
I know this is an old thread now, but i stumbled across it while I was having transactional log corruption issues, here is how I got around it without any data loss (I did have down time though!)
Log into sql management studio and change the database mode to simple, then take a full backup.
Change the database type back again and once again take a full backup, then take a transactional log backup.
Right click on databases and click on restore, select the database name from the drop down list, select the later full database backup created (not the one taken from the simple mode) and also select the transactional log backup.
Click restore and it should put it all back without any corruption in the log files.
This worked for me with no errors and my backups all worked correctly afterwards and there were no more transactional log errors.
git rev-list outputs the commits that lead up to the $TAG similar to git log but only showing the SHA1 of the commit.
To omit those diffs, use git log -1 <tag>.
Just add the custom logRequest filter when you create your WebClient using WebClient.Builder.
Then just call myClient.send("get"); and log messages should be there.
The request logging filter will work anyway.
The only relevant part of the answer is logRequest() filter.
Some people also asked how to log the response.
To log the response you can write another ExchangeFilterFunction and add it to WebClient.
If you really need to log the body, you can make the underlying layer (Netty) to do this.
You don't necessarily need to roll your own logger, reactor.ipc.netty.channel.ChannelOperationsHandler does it for you.
Just configure your logging system for that class to log at DEBUG level:
To log headers or form body, set the above to TRACE level; however, that's not enough:
In response to a question in the comment that asked how to log request and response body, I don’t know if Spring has such a logger but WebClient is built on Netty, so enabling debug logging for package reactor.ipc.netty should work, along with [this (hyper-link)] answer.
If you don't want to log the body, then this is really easy.
The second line causes headers to be included in the log.
You can have netty do logging of the request/responses with by asking it todo wiretaping, if you create your Spring WebClient like this then it enables the wiretap option.
and then have your logging setup:
this will log everything for the request/response (including bodies), but the format is not specific to HTTP so not very readable.
@Matthew Buckett answer shows you how to get Netty wire logging.
But it can be easily customized via extending io.netty.handler.logging.LoggingHandler
If you want to suppress useless (for you) log entries like (note ACTIVE at the end):
The answer is based on [https://www.baeldung.com/spring-log-webclient-calls (hyper-link)]
I did not manage to get spring.http.log-request-details=true doing its job, and current [Spring WebFlux reference suggests (hyper-link)] that some coding needs be done to have headers logged, though the code example uses deprecated [exchangeStrategies() (hyper-link)] method.
There is still a replacement for the deprecated method, so a compact piece of code for getting headers logged at WebClient level may look like this:
It should be noted though that not all of the headers are available (do exist) at WebFlux ExchangeFunctions level, so some more logging at Netty HttpClient level may be essential too, as per [@Matthew's suggestion (hyper-link)]:
This will get bodies logged too.
If you are looking to log the serialized version of the JSON in the request or response, you can create your own Json Encoder/Decoder classes that wrap the defaults and log the JSON.
This is explained here: [https://andrew-flower.com/blog/webclient-body-logging (hyper-link)]
[@StasKolodyuk (hyper-link)]'s answer elaborates on the solution from [baeldung (hyper-link)] for logging the response body of a reactive WebClient.
Another non-deprecated way to add your custom LoggingHandler is (Kotlin)
Spoiler: So far the custom logging with the ExchangeFilterFunction does not support to log the body.
In my case the best logging is achieved with the solution from Bealdung (see [this (hyper-link)]).
Then the request log looks like:
If you are going to implement CustomLoggerHandler, don't forget to implement equals() and hashCode(), otherwise will be a memory leak [https://github.com/reactor/reactor-netty/issues/988#issuecomment-582489035 (hyper-link)]
There is a way to log request and response body using only [ExchangeFilterFunction (hyper-link)].
Based on [Stanislav Burov (hyper-link)]'s answer I made this logger, that logs all request/response headers, method, url and body.
That will — aside from all the other ProGuard optimisations — remove any verbose log statements directly from the bytecode.
The [Android Documentation says the following about Log Levels (hyper-link)]:
Debug logs are compiled in but stripped at runtime.
Error, warning and info logs are always kept.
So you may want to consider stripping the log Verbose logging statements out, [possibly using ProGuard as suggested in another answer (hyper-link)].
According to the documentation, you can configure logging on a development device using System Properties.
The property to set is log.tag.<YourTag> and it should be set to one of the following values: VERBOSE, DEBUG, INFO, WARN, ERROR, ASSERT, or SUPPRESS.
[More information on this is available in the documentation for the isLoggable() method.
A common way is to make an int named loglevel, and define its debug level based on loglevel.
Later, you can just change the LOGLEVEL for all debug output level.
Stripping out the logging with proguard (see answer from @Christopher ) was easy and fast, but it caused stack traces from production to mismatch the source if there was any debug logging in the file.
Instead, here's a technique that uses different logging levels in development vs. production, assuming that proguard is used only in production.
This technique makes use of commons logging.
Log4j or slf4j can also be used as logging frameworks in Android together with logcat.
See the project [android-logging-log4j (hyper-link)] or [log4j support in android (hyper-link)]
[Android Logger (hyper-link)] is the lightweight but easy-to-configure SLF4J implementation (< 50 Kb).
LOGBack is the most powerful and optimized implementation but its size is about 1 Mb.
In a very simple logging scenario, where you're literally just trying to write to console during development for debugging purposes, it might be easiest to just do a search and replace before your production build and comment out all the calls to Log or System.out.println.
For example, assuming you didn't use the "Log."
anywhere outside of a call to Log.d or Log.e, etc, you could simply do a find and replace across the entire solution to replace "Log."
with "//Log."
to comment out all your logging calls, or in my case I'm just using System.out.println everywhere, so before going to production I'll simply do a full search and replace for "System.out.println" and replace with "//System.out.println".
I know this isn't ideal, and it would be nice if the ability to find and comment out calls to Log and System.out.println were built into Eclipse, but until that happens the easiest and fastest and best way to do this is to comment out by search and replace.
If you do this, you don't have to worry about mismatching stack trace line numbers, because you're editing your source code, and you're not adding any overhead by checking some log level configuration, etc.
May be you can see this Log extension class: [https://github.com/dbauduin/Android-Tools/tree/master/logs (hyper-link)].
It enables you to have a fine control on logs.
You can for example disable all logs or just the logs of some packages or classes.
Moreover, it adds some useful functionalities (for instance you don't have to pass a tag for each log).
The attribute BuildConfig.DEBUG will be false in the production mode so all trace and debug logs will be removed.
In my apps I have a class which wraps the Log class which has a static boolean var called "state".
Throughout my code I check the value of the "state" variable using a static method before actually writing to the Log.
This means I can enable or disable all logging for the App in one call - even when the App is running.
It does mean that you have to stick to your guns when debugging and not regress to using the standard Log class though...
It's also useful (convenient) that Java interprets a boolean var as false if it hasn't been assigned a value, which means it can be left as false until you need to turn on logging :-)
I created a Utility/Wrapper which solves this problem + other common problems around Logging.
The usual features provided by Log class wrapped around by LogMode s.
Method Entry-Exit logs: Can be turned off by a switch
Use it like you use android.util.Log methods, to start with.
Use the Entry-Exit logs feature by placing calls to entry_log()-exit_log() methods at the beginning and ending of methods in your app.
For me it is often useful being able to set different log levels for each TAG.
Now just set the log level per TAG at the beginning of each class:
There is a tiny drop-in replacement for the standard android Log class - [https://github.com/zserge/log (hyper-link)]
Basically all you have to do is to replace imports from android.util.Log to trikita.log.Log.
Then in your Application.onCreate() or in some static initalizer check for the BuilConfig.DEBUG or any other flag and use Log.level(Log.D) or Log.level(Log.E) to change the minimal log level.
You can use Log.useLog(false) to disable logging at all.
We can use class Log in our local component and define the methods as v/i/e/d.
Another way is to use a logging platform that has the capabilities of opening and closing logs.
This can give much of flexibility sometimes even on a production app which logs should be open and which closed depending on which issues you have
for example:
[https://limxtop.blogspot.com/2019/05/app-log.html (hyper-link)]
For debug version, all the logs will be output;
For release version, only the logs whose level is above DEBUG (exclude) will be output by default.
In the meanwhile, the DEBUG and VERBOSE log can be enable through setprop log.tag.<YOUR_LOG_TAG> <LEVEL> in running time.
Because 0 is in your dataset, log(0) isn't defined, so the limits on your axis aren't clear and defaults to a narrow range around 10.
For simple cases, there is a global logger defined in the log package, [log.Logger (hyper-link)].
This global logger can be configured through [log.SetFlags (hyper-link)].
Afterwards one can just call the top level functions of the log package like [log.Printf (hyper-link)] and [log.Fatalf (hyper-link)], which use that global instance.
Create a single log.Logger and pass it around?
A [log.Logger (hyper-link)] can be used concurrently from multiple goroutines.
Pass around a pointer to that log.Logger?
[log.New (hyper-link)] returns a *Logger which is usually an indication that you should pass the object around as a pointer.
a copy of the Logger) and then multiple goroutines might write to the same [io.Writer (hyper-link)] concurrently.
Should each goroutine or function create a logger?
I wouldn't create a separate logger for each function or goroutine.
Goroutines (and functions) are used for very lightweight tasks that will not justify the maintenance of a separate logger.
It's probably a good idea to create a logger for each bigger component of your project.
For example, if your project uses a SMTP service for sending mails, creating a separate logger for the mail service sounds like a good idea so that you can filter and turn off the output separately.
Should I create the logger as a global variable?
In the previous mail service example, it would be probably a good idea to have one logger for each instance of your service, so that users can log failures while using the gmail mail service differently than failures that occured while using the local MTA (e.g.
I know this question is a bit old, but if, like me, your projects are made up of multiple smaller files I vote for your 4th option - I've created a logger.go that is part of package main.
This go file creates the logger, assigns it to a file, and provides it to the rest of main.
Note I have not come up with a graceful way to close errorlog...
I found the default log package ([https://golang.org/pkg/log/ (hyper-link)]) somewhat limiting.
For example, no support for info vs. debug logs.
After some poking around, settled on using [https://github.com/golang/glog (hyper-link)] .
This seems to be a port of [https://github.com/google/glog (hyper-link)] and gives decent flexibility in logging.
For example when running an application locally you may want DEBUG level log but might want to run only in INFO/ERROR level in production.
The list of full features/guide is, here [https://google-glog.googlecode.com/svn/trunk/doc/glog.html (hyper-link)] (Its for the c++ module, but for the most part translates to the golang port)
This is an older question, but I would like to suggest the use of [http://github.com/romana/rlog (hyper-link)] (which we developed).
It is configured through environment variables, the logger object is created and initialized when rlog is imported.
Therefore, no need to pass around a logger.
rlog has quite a few features:
Standard log levels (Debug, Info, etc.)
as well as freely-configurable multi-level logging.
On demand logging of caller info (file, line number, function).
Ability to set different log levels for different source files.
This is a simple logger
One of the logging module that you can consider is [klog (hyper-link)] .
It support 'V' logging which gives the flexibility to log at certain level
klog is a fork of glog and overcomes following drawbacks
glog presents a lot "gotchas" and introduces challenges in containerized environments, all of which aren't well documented.
glog  doesn't provide an easy way to test logs, which detracts from the stability of software using it
glog is C++ based and klog is a pure golang implementation
Logging and ecdf:
To log y-axis you need just:
I use coloredlogcat for viewing logcat logs on the console.
Have a look at this link [coloredlogcat (hyper-link)] for details.
stand alone log tool.
[http://androidforums.com/application-announcements/207057-tool-logviewer-android.html (hyper-link)]
I've tried [logview-0.19 (hyper-link)] - works fine for my 13Mb logs.
Make sure to mark ./logview and ./lib/logview.jar as executable when running in Linux.
Desktop tool for reading android logcat log file, the same as DDMS.
[image]
The purpose of this tool is to allow developers to quickly locate, analyze, problem-solving, rather 
than struggling in the log file.
Feature:
[http://code.google.com/p/androidlogcatviewer/wiki/KeyFeature (hyper-link)]
Download:
[http://code.google.com/p/androidlogcatviewer/downloads/list (hyper-link)]
Discuss-group:
[http://groups.google.com/group/androidlogcatviewer (hyper-link)]
I've tweaked log4j Chainsaw V2 and the (VFS)LogFilePatternReceiver to allow Chainsaw to read events from Android log files generated via logcat -v time
I like [LogExpert (hyper-link)].
So I have created a custom columnizer that parses LogCat logs.
It has some improvements to be able to parse my custom logs with method and line.
I found [Online LogCat Beautifier (hyper-link)] to which can colourize given LogCat for easy reading.
The link is no longer accessible, and luckily enough for us, Android Studio now supports pasting the LogCat into its "LogCat" tab (previously "Monitor").
Upon pasting, you'd have the options to search/click like a normal LogCat's log.
I went with [http://mlogcat.tistory.com/ (hyper-link)] now, since [Logcat Offline Viewer (hyper-link)] quit its job probably due to JAVA update.
Personally I like [TailExpert (hyper-link)] which I use together with logcatUDP to capture logs from multiple android targets, logcatUDP sends the log over the network to tailexpert where I capture it and view the side by side.
But you can also read the logs from file if you like, it's a tail program so it will add logmessages to the view when they arrive.
You can filter messages on keywords and use colors to identify certain logmessages.
The program is packed with features and I recently just started to discover the use of notifications to place bookmarks on the fly and halt the log when a certain logmessages passes by, which makes navigation so much quicker.
[Splinter Log (hyper-link)] works on MacOS and can do a few things better than DDMS, like grouping tags, resolving pids to package name, etc.
[https://splinter-log.com (hyper-link)]
the to redirect stderr to stdout append this at your command: 2>&1
For outputting to terminal and logging into file you should use tee
A year later, here's an old bash script for logging anything.
For example,
teelog make ... logs to a generated log name (and see the trick for logging nested makes too.)
Adapted from [here (hyper-link)]; the original did not, from what I can tell, catch STDERR in the logfile.
After running ./x.sh, the x.log only include the stdout, while the x.err only include the stderr.
file = /path/to/log/file/test-*.log
The config file is going to let you configure what log messages actually surface in the log, not what level each message is going be logged at.
The [javadoc (hyper-link)] has a method for each log level and a generic log method, which takes in a priority, so I'm not sure there's even a default to be set.
You can set a config file explictly on the command line via -Dlog4j.configuration=<FILE_PATH>, so you could set up a specific config for that test case.
log4j.properties  or log4j.xml.
file, the log file, e.g.
myApp.log
log4j.properties or log4j.xml.
class, any customized initialization class, like LogManager, should implement
the [org.apache.log4j.spi.Configurator (hyper-link)]
You can see more in [Apache log4j 1.2 - Short introduction to log4j (hyper-link)] on Default Initialization Procedure section.
Moreover, you can also use the methods that offers the [Logger (hyper-link)] class, like [public void setLevel(Level level) (hyper-link)], e.g.
It will set the threshold level of the root logger to TRACE.
If you're using Maven, you can have two log4j configuration files:
one in src/main/resources, containing your production logging config
one in src/test/resources, containing your test-time logging config
NB that the log4j2.properties file may include the line
You can waste an entire afternoon trying to figure out why your LOG.trace() statements aren't outputting anything!
I think this is a pragmatic approach; O(logN) will never be more than 64.
In practice, whenever terms get as 'small' as O(logN), you have to measure to see if the constant factors win out.
"With O(logN) your input size does
  matter."
I
  contend that the answer is no, there
  is not, and never will be, a data set
  for which logN will grow so fast as to
  always be beaten a constant-time
  algorithm.
Even for the largest
  practical dataset imaginable in the
  lifetimes of our grandchildren, a logN
  algorithm has a fair chance of beating
  a constant time algorithm - you must
  always measure.
about halfway through, Rich discusses Clojure's hash tries, which are clearly O(logN), but the base of the logarithm is large and so the depth of the trie is at most 6 even if it contains 4 billion values.
Here "6" is still an O(logN) value, but it is an incredibly small value, and so choosing to discard this awesome data structure because "I really need O(1)" is a foolish thing to do.
What good is an O(log2(n)) algorithm
  if those operations cause page faults
  and slow disk operations?
O(logn) says that the algorithm will be fast, but as your input grows it will take a little longer.
O(1) and O(logn) makes a big diference when you start to combine algorithms.
If you could do a join in O(1) instead of O(logn) you would have huge performance gains.
But with O(logn) you need to multiply the operation count by logn each time.
For large inputs, if you had an algorithm that was O(n^2) already, you would much rather do an operation that was O(1) inside, and not O(logn) inside.
With O(1) that constant overhead does not amplify the number of operations as much as O(logn) does.
Another point is that everyone thinks of O(logn) representing n elements of a tree data structure for example.
To clarify: we usually write f(x) = O(logN) to imply "f(x) is O(logN)".
But for O(logN), number of steps/time still grows as a function of the input size (the logarithm of it), it just grows very slowly.
O(log N) can be misleading.
The operations are O(logN) but rather complex, which means many low level operations.
When you take it in that context it becomes clear why an algorithm A ~ O(logN) and an algorithm B ~ O(1) algorithm are different:
if I run A on an input of size a, then on an input of size 1000000*a, I can expect the second input to take log(1,000,000) times as long as the first input
And if no constant-time algorithm exists, then the logarithmic-time one is usually the best you can get.
Because usually, a constant-time algorithm will be faster than a logarithmic one.
If I call your function, N times, then it starts to matter whether your function runs logarithmic time or constant, because the total complexity is then O(n lg n) or O(n).
So rather than asking if "it matters" that you assume logarithmic complexity to be constant in "the real world", I'd ask if there's any point in doing that.
Often you can assume that logarithmic algorithms are fast enough, but what do you gain by considering them constant?
O(logN)*O(logN)*O(logN) is very different.
Also a simple quicksort-style O(nlogn) is different than O(n O(1))=O(n).
The latter isn't 1000 times slower, it's 2000 times, because log(n^2)=2log(n)
Yes, log(N) < 100 for most practical purposes, and No, you can not always replace it by constant.
If, though, the program is O(N*logN), then it will take it ~2 secs to process 106 elements.
Imagine you have function f() working in O(logN), and on each iteration calling function g(), which works in O(logN) as well.
Then, if you replace both logs by constants, you think that your program works in constant time.
Reality will be cruel though - two logs may give you up to 100*100 multiplicator.
The difference between O(log N) and O(1) is then large, even with small N.
Ok, that took 30minutes, so when I run it on a dataset a hundred times as large it should still take 30minutes because O(logN) is "the same" as O(1).... eh...what?
If you think O(logN) is the same as O(1), then why would you EVER use a hash instead of a binary tree?
No one is saying that O(1) is always better than O(log N).
Computational biology.
Now, in the case of an indexing/searching algorithm, that log(n) multiple makes a large difference when coupled with constants.
The rules of determining the Big-O notation are simpler when you don't decide that O(log n) = O(1).
As krzysio said, you may accumulate O(log n)s and then they would make a very noticeable difference.
Imagine you do a binary search: O(log n) comparisons, and then imagine that each comparison's complexity O(log n).
If you neglect both you get O(1) instead of O(log2n).
Similarly you may somehow arrive at O(log10n) and then you'll notice a big difference for not too large "n"s.
O(log N) == O(1) is obviously wrong (and the poster is aware of this).
To answer the question of the poster: O(log N) != O(1), and you're right, algorithms with O(1) are sometimes not much better than algorithms with O(log N), depending on the size of the input, and all of those internal constants that got omitted during Big O analysis.
I do not believe algorithms where you can freely choose between O(1) with a large constant and O(logN) really exists.
In other words you can choose between inefficient O(n^2) algorithms or worse and O(n.logN) : it's a real choice.
If problem is O(n) it won't become O(logN) or O(1), you'll merely add some pre-treatment such that the overall complexity is unchanged or worse, and potentially a later step will be improved.
Say you want the smaller element of an array, you can search in O(N) or sort the array using any common O(NLogN) sort treatment then have the first using O(1).
Then your initial problem was truly O(NLogN), not O(N).
And it's not the same if you wait ten times or twenty times longer for your result because you simplified saying O(1) = O(LogN).
I'm waiting for a counter-example ;-) that is any real case where you have choice between O(1) and O(LogN) and where every O(LogN) step won't compare to the O(1).
Yes, in practical situations log(n) is bounded by a constant, we'll say 100.
However, replacing log(n) by 100 in situations where it's correct is still throwing away information, making the upper bound on operations that you have calculated looser and less useful.
Replacing an O(log(n)) by an O(1) in your analysis could result in your large n case performing 100 times worse than you expected based on your small n case.
You can make your analysis easier by crossing out the log(n) terms, but then you've reduced the predictive power of the estimate.
Quadrupling your disk requirements on your cached web page storage is worth caring about from a business perspective, and predictable if you don't cast out all the O(log(n)) terms.
Suppose in real time the O(1) operation takes a second on your architecture, and the O(logN) operation is basically .5 seconds * log(N).
You want to use the log(N) op for small datasets and the O(1) op for large datasets, in such a scenario.
As many have already said, for the real world, you need to look at the constant factors first, before even worrying about factors of O(log N).
That's O(N) instead of O(log N), which according to your lights would be significant -- but a linear search that moves found elements to the front may well outperform a more complicated balanced tree, depending on the application.
On the other hand, note that, even if log N is not likely to exceed 50, a performance factor of 10 is really huge -- if you're compute-bound, a factor like that can easily make or break your application.
If that's not enough for you, you'll frequently see factors of (log N)^2 or (logN)^3 in algorithms, so even if you think you can ignore one factor of (log N), that doesn't mean you can ignore more of them.
O(log(N)) means there is some A such that f(N) < AlogN for large N.
You might be interested in Soft-O, which ignores logarithmic cost.
Let's say you use an image-processing algorithm that runs in O(log N), where N is the number of images.
If running the algorithm on a single image would hypothetically take a whole day, and assuming that O(logN) will never be more than 100... imagine the surprise of that person that would try to run the algorithm on a very large image database - he would expect it to be done in a day or so... yet it'll take months for it to finish.
The observation that O(log n) is oftentimes indistinguishable from O(1) is a good one.
As we can see from this example, for all intents and purposes, an O(log n) algorithm like binary search is oftentimes indistinguishable from an O(1) algorithm like omniscience.
The takeaway point is this: *we use O(log n) algorithms because they are often indistinguishable from constant time, and because they often perform phenomenally better than linear time algorithms.
Index seek is O(log n).
if N is "the number of possible lottery combinations for a lottery with X distinct numbers" it suddenly matters if your algorithm is O(1) or O(logN)
If your input implies a sufficiently small constant factor, you could see great performance gains by going with a linear search rather than a log(n) search of some base.
This way one appender will only log to a certain level while the other to a different level even though they are defined by the same logger.
This tells the logger that if it reaches this line, don't log it.
That way it will log DEBUG messages, ERROR messages, and nothing else.
[http://www.codeproject.com/KB/dotnet/Log4net_Tutorial.aspx (hyper-link)]
It works the same as in Chrome (right click in the console window and select "Keep Log on Navigation").
Update: The setting is now back to "Preserve log" in the Network tab in the developer console in Safari 14+
back to the Console tab under the "Preserve log" checkbox.
The Network log has similarly moved back to the Network tab.
For old Safari versions, you can right-click on the console and select the Keep Log on Navigation[ (hyper-link)]
The Google's Internet Search Engine top result for phrase ""c++ log", fully answers your question :
double log (double x);
double log2  (double x);
float log2f (float x);
long double log2l (long double x);
double log10 (double x);
When trying to create a log-log plot using the pandas [plot (hyper-link)] function you must select [loglog=True (hyper-link)] rather than logx=True, logy=True within your keyword-arguments.
You just do not see .log extension in explorer because of its default settings:
You can't do without transaction logs in SQL Server, under any circumstances.
You CAN set your recovery model to SIMPLE on your dev machines - that will prevent transaction log bloating when tran log backups aren't done.
SQL Server requires a transaction log in order to function.
That said there are two modes of operation for the transaction log:
In Full mode the transaction log keeps growing until you back up the database.
In Simple mode: space in the transaction log is 'recycled' every Checkpoint.
The only point in using the Full model is if you want to backup the database multiple times per day, and backing up the whole database takes too long - so you just backup the transaction log.
The transaction log keeps growing all day, and you keep backing just it up.
That night you do your full backup, and SQL Server then truncates the transaction log, begins to reuse the space allocated in the transaction log file.
What's your problem with Tx logs?
In SQL Server 2000 or in SQL Server
  2005, the "Simple" recovery model is
  equivalent to "truncate log on
  checkpoint" in earlier versions of SQL
  Server.
If the transaction log is
  truncated every time a checkpoint is
  performed on the server, this prevents
  you from using the log for database
  recovery.
Backups of the transaction log are
  disabled when the "Simple" recovery
  model is used.
In cases where you are going to be doing any type of bulk inserts, you should set the DB to be in "BULK/LOGGED".
If we could get a performance boost by turning of ALL logging, we'd take it in a heart beat.
Also – having databases in full recovery mode can help you to undo accidental updates and deletes by reading transaction log.
[Read the log file (*.LDF) in sql server 2008 (hyper-link)]
If space is an issue on production machines then just create frequent transaction log backups.
The default error log as opposed to the scripts error logs usually has the (more) specific error.
so please first check your error log and then provide some more information.
The default error log is often in /var/log/httpd/error_log or /var/log/apache2/error.log.
The reason you look at the default error logs (as indicated above) is because errors don't always get posted into the custom error log as defined in the virtual host.
Check your php error log which might be a separate file from your apache error log.
Find it by going to phpinfo() and check for error_log attribute.
Absolutely nothing was being logged, but I kept getting a 500 error.
If you run into this particular issue, you can change the log level of mod_authnz_ldap like so:
That will use a log level of debug for mod_authnz_ldap but warn for everything else ([https://httpd.apache.org/docs/2.4/en/mod/core.html#loglevel (hyper-link)]).
Why are the 500 Internal Server Errors not being logged into your apache error logs?
By default, PHP does NOT log these errors.
Reason being you want web requests go as fast as physically possible and it's a security hazard to log errors to screen where attackers can observe them.
These instructions to enable Internal Server Error Logging are for Ubuntu 12.10 with PHP 5.3.10 and Apache/2.2.22.
Make sure PHP logging is turned on:
Change those lines so they look like this:
[code snippet]
What this communicates to PHP is that we want to log all these errors.
Warning, there will be a large performance hit, so you don't want this enabled on production because logging takes work and work takes time, time costs money.
Do what you did to cause the 500 Internal Server error again, and check the log:
[code snippet]
If your Internal Server Error information doesn't show up in log files, you probably need to restart the Apache service.
I've found that Apache 2.4 (at least on Windows platform) tends to stubbornly refuse to flush log files—instead, logged data remains in memory for quite a while.
In my case it was the ErrorLog directive in httpd.conf.
If your server use php-fpm you must restart php-fpm service

systemctl restart php-fpm

Check the log file in php-fpm log folder.
eg /var/log/php-fpm/www-error.log
Log-likelihood ratio
If that ratio
  is Λ and the null hypothesis holds,
  then for commonly occurring families
  of probability distributions, −2 log Λ
  has a particularly handy asymptotic
  distribution.
Many common test
  statistics such as the Z-test, the
  F-test and Pearson's chi-square test
  can be phrased as log-likelihood
  ratios or approximations thereof.
[http://www.knowledgerush.com/kr/encyclopedia/Log-likelihood_ratio/ (hyper-link)]
The only reason to use the log-likelihood instead of the plain old likelihood is mathematical convenience, because it lets you turn multiplication into addition.
and then total time is O(n loglogn).
Time Complexity of a loop is O(log log n) if the loop variables is reduced / increased exponentially by a constant amount.
If the loop variable is divided / multiplied by a constant amount then complexity is O(Logn).
Now log (2^64) = 64 and log 64 = log (2^6) = 6.
I think if the codes are like this, it should be n*log n;
Is your syslog.conf set up to handle facility=user?
You can set the facility used by the python logger with the facility argument, something like this:
To log to a specific facility using SysLogHandler you need to specify the facility value.
*    /var/log/mylog
in syslog, then you'll want to use:
handler = logging.handlers.SysLogHandler(address = ('localhost',514), facility=19)
and you also need to have syslog listening on UDP to use localhost instead of /dev/log.
the above script will log to LOCAL0 facility with our custom "LOG_IDENTIFIER"...
you can use LOCAL[0-7] for local purpose.
You should always use the local host for logging, whether to /dev/log or localhost  through the TCP stack.
This allows the fully RFC compliant and featureful system logging daemon to handle syslog.
This eliminates the need for the remote daemon to be functional and provides the enhanced capabilities of syslog daemon's such as rsyslog and syslog-ng for instance.
Retrying, queuing, local spooling, using TCP instead of UDP for syslog and so forth become possible.
From [https://github.com/luismartingil/per.scripts/tree/master/python_syslog (hyper-link)]
Create an file in /etc/rsyslog.d/ that ends in .conf and add the following text
Restart rsyslog, reloading did NOT seem to work for the new log files.
You can also add a file handler or rotating file handler to send your logs to a local file:
[http://docs.python.org/2/library/logging.handlers.html (hyper-link)]
I found [the syslog module (hyper-link)] to make it quite easy to get the basic logging behavior you describe:
In log cfg.yml:
Configured both syslog & a direct file.
Note that the /dev/log is OS specific.
The rsyslog service did not listen on socket service.
I config this line bellow in  /etc/rsyslog.conf file and solved the problem:
$SystemLogSocketName  /dev/log
I use JSON logging and wanted use SysLogHandler with UDP port 514.
[Edit]
To be clearer about what is going on here: this is only for python code, and using python's built in logging module.
The module allows format and destination(s) of the log messages to be configured.
One way to configure the log format and destination(s) is by using a JSON file that is used to configure the logging.
The above example allowed me to send log messages to a syslog daemon.
The example above sends python log messages to both syslog and the console.
The format of messages for destinations is different (syslog already prefixes each message with a timestamp).
For the syslog destination, the log uses facility LOCAL6.
There is no an option to include the source file with the standar org.apache.log4j.PatternLayout.
)* or any similar phrase matching across multiple lines, the greedy operator * means it'll try to match from the very first 7 digits to the very last timestamp on the entire log page, ignoring your start and end anchors.
pyplot.hist can "log" y axis for you with keyword argument log=True
pyplot.hist accepts bins keyword argument, but you have to "log" x axis yourself
This will give you the actual counts of how how many elements fall into each bin, plotted on a log axis (which is what people usually mean by a log plot).
I couldn't tell from your wording if you wanted this or the log of the count plotted on a linear axis.
rsyslog by default escapes all weird characters (ASCII < 32), and this include newlines (as well as tabs and others).
Simply add this to your rsyslog config to turn this off:
Alternatively, if you want to keep your syslog intact on one line for parsing, you can just replace the characters when viewing the log.
Another option would be to subclass the SysLogHandler and override emit() - you could then call the superclass emit() for each line in the text you're sent.
You might also want to set the limits for the axes, since the automatic feature does not work too well for log scales.
But, as you've noted, you're trading order for slower time to access an element: O(log n) for basic operations.
This implementation provides guaranteed log(n) time cost for the basic operations (add, remove and contains).
The reason why most use HashSet is that the operations are (on average) O(1) instead of O(log n).
Both should give Log(n) - it would be of utility to see if either is over five percent faster than the other.
HashSet is much faster than TreeSet (constant-time versus log-time for most operations like add, remove and contains) but offers no ordering guarantees like TreeSet.
guarantees log(n) time cost for the basic operations (add, remove and contains)
[http://tunatore.wordpress.com/2011/06/12/how-to-define-dailyrollingfileappender-in-log4j-properties-file/ (hyper-link)]
To make git log nicer by default, I typically set these global preferences:
Contributing this because it allows a and b logs to be diff'ed visually, side by side, if you have a visual diff tool.
In a cluster best practices are to gather all logs in a single point through an aggregator and analyze them with a dedicated tool.
For that reason in K8S, log command is quite basic.
Anyway kubectl logs -h shows some options useful for you:
Try kubectl logs -f pod --tail=10
You can also follow logs from the end if you are testing something:
To fetch tail lines from logs of a pod with multi containers.
To fetch tail lines from logs of  pods within an application:
(ex:if your application has 3 pods then output of above command can be 30 logs 10 of each pod logs)
In my opinion you have a problem of permission, try to change the path with /tmp/app.log for example.
Please check whether /home/mani/logs exists, because in other case it would not be created and the same error will be thrown.
Yes, since you can manipulate the terms in Big-O notation in familiar algebraic ways, O(Log A + Log B) = O(Log AB).
When talking about two sequential searches, though, it might be more intuitive to leave it at O(Log A + Log B).
You might want to simplify that to O(Log AB) if you're comparing to some other algorithm, or trying to find dominant terms.
Nope, O(log n log log n) is as simplified as that expression is going to get.
You sometimes see runtimes like O(n log n log log n) popping up in number theory contexts, and there aren’t simpler common functions that quantities like these are equivalent to.
You'll need to consider finding a way to reformat the server logs, or use a script of some kind to transform the data first.
There is a Sidekiq.logger and simply logger reference that you can use within your workers.
The default should be to STDOUT and you should just direct your output in production to the log file path of your choice.
That's why whenever url changes i need to call service method which returns logged in user information as per response i will go for further operations.
However, whenever it receives USER_LOGOUT action, it returns the initial state all over again.
Now we just need to teach the new rootReducer to return the initial state in response to the USER_LOGOUT action.
Now, whenever USER_LOGOUT fires, all reducers will be initialized anew.
Simply have your logout link clear session and refresh the page.
So when I dispatch a logout action creator I dispatch actions to clear state as well.
Logout action creator
Now, for logout you can handle the like below:
Every time a userlogs in, without a browser refresh.
A much better way will be that you have a LOGOUT_ACTION.
Once you dispatch this LOGOUT_ACTION.
This way store logout and reset will happen synchronously and your store will ready for another user login.
From a security perspective, the safest thing to do when logging a user out is to reset all persistent state (e.x.
While logging out of the application we can simple reAssign the default state and reducer will work just as new.
On Logout calling action for resetting state
Dispatch your 'reset' in your logout function
Then you would have to release them on logout like this:
add this code to logout action
In a Unix/Linux environment, you can printto a file, but 'tail' that file to get a constantly updating log.
You also could do it the other way; you could NOT use printto for the log, but then save your log after the program completes (either by just asking it to save via program commands in your program, or by attaching a macro to the run command.)
Finally, the one way you can truly do this is using the altlog system option.
Specifying ALTLOG in the shortcut or the config file allows you to send the log to an alternate location in addition to the screen.
There is an error in the blog the correct answer is:
The logarithm of zero is -Inf, which is sometimes represented as NA.
I suggest that you make sure that all your data points have positive values if you want to represent them on log scale.
or the [log4j API for the Level class (hyper-link)], which makes it quite clear.
When the library decides whether to print a certain statement or not, it computes the effective level of the responsible Logger object (based on configuration) and compares it with the LogEvent's level (depends on which method was used in the code – trace/debug/.../fatal).
If LogEvent's level is greater or equal to the Logger's level, the LogEvent is sent to appender(s) – "printed".
[Taken from [http://javarevisited.blogspot.com/2011/05/top-10-tips-on-logging-in-java.html] (hyper-link)]
DEBUG is the lowest restricted java logging level and we should write everything we need to debug an application, this java logging mode should only be used on Development and Testing environment and must not be used in production environment.
INFO is more restricted than DEBUG java logging level and we should log messages which are informative purpose like Server has been started, Incoming messages, outgoing messages etc in INFO level logging in java.
WARN is more restricted than INFO java logging level and used to log warning sort of messages e.g.
These messages and java logging level are almost important because you can setup alert on these logging messages in java and let your support team monitor health of your java application and react on this warning messages.
In Summary WARN level is used to log warning message for logging in Java.
ERROR is the more restricted java logging level than WARN and used to log Errors and Exception, you can also setup alert on this java logging level and alert monitoring team to react on this messages.
ERROR is serious for logging in Java and you should always print it.
FATAL java logging level designates very severe error events that will presumably lead the application to abort.
OFF java logging level has the highest possible rank and is intended to turn off logging in Java.
Hierarchy of log4j logging levels are as follows in Highest to Lowest order :
TRACE log level provides highest logging which would be helpful to troubleshoot issues.
DEBUG log level is also very useful to trouble shoot the issues.
You can also refer this link for more information about log levels : 
[https://logging.apache.org/log4j/2.0/manual/architecture.html (hyper-link)]
Going down the first column, you will see how the log works in each level.
To make it clear, there is no defined term like 'hierarchy' in log4j.
For troubleshooting you have to select which level you want the logs.
Older versions of git (say, 1.8.x) default to not decorating the output of git log.
In other words, with version 1.8.3. running git log I see:
I can get the same output with the older version of git using git log --decorate.
In other words, if you're running a modern version of git, there will be no difference in the output of git log and git log --decorate.
With Git 2.33 (Q3 2021), the documentation is clearer regarding git log --decorate default value:
There're two different default options for log --decorate: * Should --decorate be given without any arguments, it's default to short * Should neither --decorate nor --no-decorate be given, it's default to the log.decorate or auto.
git log now includes in its [man page (hyper-link)]:
Default to configuration value of log.decorate if configured,
otherwise, auto.
Take a look at Apache Chainsaw [http://logging.apache.org/chainsaw/index.html (hyper-link)] for your needs
It provides fast, Google-like searching across lots (terabytes) of logs, is easy to filter (e.g.
by log level or date), makes it easy to correlate into transactions of multiple related log events, etc.
There's a downloadable version that's free as long as you're indexing less than 500MB of logs per day.
You -can- load multiple log files into Chainsaw (by default, all events for a log file are placed on a logfile-specific tab).
You can also define a 'custom expression logpanel' which will aggregate events from all tabs into a new tab matching an expression you provided - similar to a database 'view', you could use the expression 'LEVEL >= WARN' to collect all warnings, error & fatal messages from any log file into a single view.
msg like 'User [a-z]* logged in'
msg ~= login || msg ~= logout
One of the useful features added to the upcoming release is a clickable bar to the right of the table (similar to Eclipse or Idea's bar showing syntax error indications) which will display color rule and search expression matches for the entire log file.
Mind Tree Insight is also a useful Open Source Log Analysis tool
you can also try an Online log file analysis-
[http://www.sharontools.com/tools/LogAnalysis/Main.php (hyper-link)]
You can try [LogSaw (hyper-link)], it's an open source software based on Eclipse and which is active right now...
Alfa is a GUI tool for analizing log files.
You open a log, press Ctrl-F and the "Next" button again and again, then reload the file as it was modified, and repeat the search.
Alfa maps a log file to a database allowing you to use standard SQL queries to get data without any superfluous actions.
Might come a bit late, but [LogMX (hyper-link)] does all this stuff, and is highly active for many years now.
[glogg (hyper-link)] is a simple but powerful tool.
In many cases it's now more succinct to use [Serilog.Sinks.Map (hyper-link)].
Usually for my use-cases I need multiple logs that will contain a minimum level for each.
This way I can later investigate the logs more efficiently.
For more recent versions of Serilog I would suggest to use this:
The setting log-queries-not-using-indexes will add any query that is not using indexes to the slow log.
SELECT * FROM mysql.slow_log WHERE query_time > 5
Your transformation of the power-law model to log-log is wrong, i.e.
Take your original model y=a*(x^b) and apply the logarithm on both sides, you will get log(y) = log(a) + b*log(x).
Thus, your model in log-scale should simply read y' = a' + b*x', where the primes indicate variables in log-scale.
The model is now a linear function, a well known result that all power-laws become linear functions in log-log.
Therefore, in log scale, the fit will minimise the relative error between the fit and the data, while in linear scale, the fit will minimise the absolute error.
The data you show certainly does not have a constant uncertainty in log-scale, so on linear scale your fit might be more faithful.
In that case, I would prefer the log-scale fitting, as the model is simpler and therefore likely to be more numerically stable.
Instead, you need to log individual properties separately, like
You can print any frame with this log.
Run adb logcat
adb logcat | grep 'redbox'
This way you don't have to scan the entire logfile.
[https://medium.com/@taufiq_ibrahim/using-adb-logcat-for-react-native-debugging-38256bda007c (hyper-link)]
It will tell you exactly what is crashing the app under LogCat tab.
In case react native app crashes without an error message checking the problem in Android Studio's Logcat works perfect.
Open Logcat
Click on "Edit Filter Configuration" in the right top of the logcat screen
My RN App works correctly before, suddenly one day it does not work, keeps crashing without error log.
For those exceptions, you can obtain the full US English version of the message by briefly switching the thread locale to en-US while logging it (saving the original user locale beforehand and restoring it immediately afterwards).
Where the ExceptionLogger class looks something like:
You should log the call stack instead of just error message (IIRC, simple exception.ToString() should do that for you).
In the exception logger you could log ex.GetType.ToString, which would save the name of the exception class.
For Logging purposes, certain applications may need to fetch the English exception message (besides displaying it in the usual client's UICulture).
My logging.cfg is pretty similar than your except the fact that the logger name si set in a constant module (a can do module rename without breaking the logging configuration)
To update from command line, you must have a mapping between your opts value and logging.Handler sub class name.
Here is an example of logging to multiple files with separate handles in dictConfig.
Log file said exception was caused by corruption in the project tree -- a very vague description.
After reboot, eclipse showed the same error " check log file".
--log-pmd was removed because of the way more useful and more powerful project
While --log-metrics was removed and for all things metrics you want to turn to the
Y2 = Y1 * 10**(a2 * log10(X1 / X2) );
The fact that you're using a log scale means that the length of a line on the plot will change based on where it's plotted.
Use some kind of Logger class/module (depending on language) that allows you to log at different levels (DEBUG, INFO, etc.)
Log very basic functions in LogLevel DEBUG
Log more complex functions in LogLevel INFO (e.g.
Log potential problems with LogLevel WARN
Log errors and exceptions with LogLevel ERROR
A seperate logging functionality allows you to turn logging on/off depending on effiency.
But also you have the capability to log everything.
Example: In Java there's log4j that provides excellent customization options.
You can define the logLevel and you can define which classes should have logging turned on.
This procedure highly depends on the language your working with (of course) but I think this "log4j" approach is a very good one.
Use a delegate function (or create a central log class with a log function) to log messages.
The parameters should contain the log message, log severity and log level.
This delegate function (or function from your log class) should be called everywhere where you need to log.
It allows you to replace the log mechanism easily, e.g.
change from file-based log to Windows event log.
In this function you can also deal with the log level, i.e.
Log when you start something, and when you've ended something, and always log errors which might occur.
Those log calls can be locally inside the function as shown in your example B.
If the language you're using supports that, write a function/method for logging exceptions: For example, in C# you can write an [extension method (hyper-link)] public static void Log(this Exception ex) { // ... logging code ... } for exceptions, which then allows you to simply call it in each catch block, like: try { ... } catch (Exception ex) { ex.Log(); }It can also be usefull to add an optional parameter to it like public static void Log(this Exception ex, string message="") { ... logging code ... }, so you can pass additional information to it like ex.Log("readDataFromIO() - read error occurred");.
your own centralized logging class) rather than re-inventing the wheel completely.
Additionally to what's been said before, I am using a somewhat generalized concept of logging.
I am using a class LogEvent (code is Java, but idea can be ported to most languages):
LogEvents can be "registered" using a LogEventCollection:
To get an overview of the inner events of my program, the LogEventCollection method report() gives shows a list of all LogEvents with their respective counts.
Additional locking code or a thread-safe collection are necessary for multi-threaded applications to prevent conflicts during concurrent access to the LogEventCollection.
Or the event logging can be disabled during production runs.
Of course, the best practice is to put logging where you need it.
:-)  But in your example, the best practice would be not to use logging at all.
Logging is generally not good for tracing program flow (which is what it appears you are trying to do), despite the existence of a TRACE level in a lot of logging frameworks.
The best use of logging is to log data flowing through the system, particularly data that causes problems.
When logging data, the data should be logged in the context where it can best be explained.
at the beginning of a function (logging the inputs),
at the end of a function (logging the outputs or timing), or
To find out where a fatal error occurs you should have an error handler that is informed of the fatal error and logs a stack trace showing where the error occurred.
You should not try to log the execution path in an attempt to localize the problem.
Best practice is to put the call to log() IN the function.
Log in methods
Keep the logging of each method in the method itself.
That way, when you re-use the method from other places, you will not have to add loggers in each place.
If the method happens to be a util that is called from many places, reduce the logger level or the logger priority for that prefix.
To track down the flow of the request or the source of the call, set a parameter in the logger or use thread name so that all subsequent logs have the tag and follow the logs for that tag.
Generally, its best to set the tags in logger as soon as the request is received.
To avoid duplicate logging
Catch exceptions and log at a certain logical stage in your code.
Here i would log at my module call level (corresponding to someProcess() in your code).
Alternatively, you could log the helper modules etc with a higher log filter.
Logging is a very subjective matter, and its got more to do with deciding one way and sticking to it everywhere.
You need to figure out your level of detail vs performance vs signal to noise in the logger you are using by tweaking the parameters little by little over time.
The simplest solution would be to make an overload of your methods, such that you may specify when to "log" and when not.
Rotate the logs yourself
[http://www.mongodb.org/display/DOCS/Logging (hyper-link)]
or use 'logrotate' with an appropriate configuration.
If you think that 32 megs is too large for a log file, you may also want to look inside to what it contains.
If the logs seem mostly harmless ("open connection", "close connection"), then you may want to start mongod with the --quiet switch.
This will reduce some of the more verbose logging.
You can use logrotate to do this job for you.
Put this in /etc/logrotate.d/mongod (assuming you use Linux and have logrotated installed):
Using logrotate is a good option.
while, it will generate 2 log files that fmchan commented, and you will have to follow Brett's suggestion to "add a line to your postrotate script to delete all mongod style rotated logs".
Some mongod logs may get lost.
Could check logrotate man page or refer to [this copytruncate discussion (hyper-link)].
You could write a script that sends the rotate signal to mongod and remove the old log files.
[mongologrotate.sh (hyper-link)] is a simple reference script that I have written.
Long answer: There appear two reasons __android_log_print can return a negative value.
The first is if it's unable to open any of the log files for writing (/dev/log/main, /dev/log/radio, or /dev/log/events).
After __android_log_print returns a negative value, is errno set?
If errno is zero, your log call probably wound up in __write_to_log_null and you should investigate your program's ability to open those /dev/log files.
If errno is set, it's possible that it was set when trying to open the log files.
You should be able to isolate this case by setting errno = 0, then calling __android_log_bug_write with a bad log_id (first argument).
For this to be useful, it'll need to be the first call to any of the logging functions - otherwise the file opening stage will have been and gone.
This answer comes to you courtesy of system/core/liblog/logd_write.c.
It works for me on Ubuntu...
cd var/lib/tomcat7 
sudo nano logs/catalina.out
Just logged in to the server and type below command
then move to logs
You will then see the live logs.
I have used this command to check the logs and 10000 is used to show the number of lines
I found logs of Apache Tomcat/9.0.33    version in below path:
In tail -f /opt/tomcat/logs/catalina.out
A commit log is a record of transactions.
disaster recovery - generally, all commits are written to the log before being applied, so transactions that were in flight when the server went down can be recovered and re-applied by checking the log.
I set the log manually not in the constructor and I don't give a category on the WriteEntry method:
You have the Event Source IvrService registered with the Application Log, not the IvrServiceLog.
The System.Diagnostics.EventLog.SourceExists verifies that the source exists, but not for a particular log.
My guess is that you originally registered this with the Application log and then later changed it to write to the IvrServiceLog.
Maybe you're using a different log4net DLL version than your colleagues?
"C:\\Users\\loganm\...".
Alternatively, you could try using forward slashes: "C:/Users/loganm/..."
One common mistake is to neglect to initialize the logger there.
It should have something like this at the bottom:
// Log4Net activation
[assembly: log4net.Config.XmlConfigurator]
I found the problem, and unfortunately it was just that logging had not actually been set up in the program and I was given incorrect information.
Give the following code in your application before you put your logging code:
make sure the log4net.config file, Copy to Output Directory property is not "Do not copy".
I had this issue also, amazing how much time I spent checking everything, finally a query on Google reminded me of a step I took when I last used Log4Net:  I had to add this to the AssemblyInfo.cs file!
I am using Log4Net in a C# Console application with the Log4Net settings in the app.config file.
[assembly: log4net.Config.XmlConfigurator(Watch = true)]
eg: running from test project does logging but running from web application won't
this will output log4net debug info (removed once issue is found out)
There was a bug in kubernetes that prevents logs obtaining for pods in CrashLoopBackOff state.
kubectl logs <podname> --previous
$ kubectl logs -h
-p, --previous[=false]: If true, print the logs for the previous instance of the container in a pod if it exists.
kubectl logs command only works if the pod is up and running.
In many cases, kubectl logs <podname> --previous is returning:
If you don't find the reason for the error with kubectl logs / get events and you can't view it with external logging tool I would suggest:
A )  Search for the failed pod container name in /var/log/containers/ and dump its .log file and search for errors - in most of the cases the cause of error will be displayed there alongside with the actions / events that took place before the error.
This attribute is normally provided by the logger, but you're using the logger_mt logger, which doesn't add any attributes and ignores the argument you provide to the BOOST_LOG_SEV macro.
You should use [severity_logger_mt (hyper-link)] or some other logger that provides support for severity levels.
This logger will add the severity level attribute to every log record made through it and will set the level to the value you specify in the BOOST_LOG_SEV macro call.
You can try using ggplot2 (this is a very nice and complete plotting package) in R. For example, consider looking at the page: [http://www.cookbook-r.com/Graphs/Axes_(ggplot2)/#axis-transformations-log-sqrt-etc (hyper-link)]
I suspect that you are being expected to plot "complementary log -log" which probably means you are being asked to plot the log of the negative log.
What I usually see in texts regarding survival analysis is a rising trend and one should see roughly parallel lines (with positive slope) for log(-log(survival)) plotted against time when the proptional hazards assumption is met.
As you can see in the image-1 there is a timestamp , make sure in your logs you have the folder/file with that timestamp as name ..
You are looking at UI, so first make sure you have log files created in the directory, in my case my log folder looks like
So my log URL is
When you go to your DAG, and select the GRAPH-VIEW, you can see a dropdown next to "RUN", select the appropriate run, and then in the graph-view below , select the appropriate task/operator and select view-log
Do you find /var/logs there?
I guess that some of your server blocks don't have the "error_log" directive.
Note that by default the error_log is always on.
Anything nginx notices before loading the config goes to the path for the error log defined at compile time.
Recompile nginx with sane path or symlink log to logs if you can't.
NOTE: You can also disable nginx logging:
[disable nginx logging (1) (hyper-link)]
[disable nginx logging (2) (hyper-link)]
In general you do not want to use the [diaglog configuration option (hyper-link)] as the diaglog binary files are both very verbose and also difficult to work with.
With this setting enabled you will get a sequence of files in your dbpath directory named diaglog.<time in hex>.
FYI, the diaglog option has actually been deprecated as at MongoDB 2.6 (ref: [SERVER-12149 (hyper-link)]).
To log queries you should instead be looking at the [Database profiler (hyper-link)] which can be enabled either globally (as you have done in your config with with profile=2) or per-database.
is there a way to configure mongo log file with java spring somehow (maybe xml)?
For example, git log --shortstat will tell you how many files changed, and how many lines were inserted and deleted.
You can see them in a git log -p.
Did you set the source on your EventLog?
You must set the Source property on your EventLog component instance before you can write entries to a log.
When your component writes an entry, the system automatically checks to see if the source you specified is registered with the event log to which the component is writing, and calls CreateEventSource if needed.
You might be forgetting to set the Source property on your EventLog.
Is it possible that you already used the source "myApp" when writing to the standard Application log?
If a source has already been mapped to
  a log and you remap it to a new log,
  you must restart the computer for the
  changes to take effect.
source registered in another log issue which I have encountered and don't want to manually have to remove sources from logs.
What I decided to do was check if the source exists, if it does check that its linked to the correct log, if it isn't delete the source, now that it doesn't exist or f it never did create the Log brand new.
The operating system stores event logs as files.
When you use EventLogInstaller or CreateEventSource to create a new event log, the associated file is stored in the %SystemRoot%\System32\Config directory on the specified computer.
The file name is set by appending the first 8 characters of the Log property with the ".evt" file name extension.
The problem only occurs if we configure Log4net with the same event source name as the endpoint name.
So we can have use different source name in our event log (in our case, <endpoint-name>.l4n).
If things go really pear-shaped, you can find the event log sources in your registry
From there, you can manually delete the event log sources that you no longer want.
This is the whole log4net configuration section to roll everyday, the same as @lazyberezovsky answered just adding the log4net and root for clarification
edit2
<file value="c:/paypal/logs/gateway_" /> this will create a file named 'gateway_' and at the end of the day (2012-04-27) it will be renamed as gateway_20120427.log and the next day (2012-04-28) it will create again the file gateway_ and at the end it'll create gateway_20120428.log.
And use datePattern value only to show when new log file should be created.
In this case all your files will have name like gateway_20120427.log.
[PreserveLogFileName (hyper-link)]
This setting keeps the extension of the file the same after you roll the log.
Virtual hosts, specific directories...), the error_log file can be set to a different path/name than the default one.
some Apache directive is messing with PHP's log path (or disabling the logging options, though that is very unlikely to be the case) - try setting an absolute path to the log file in php.ini (@Frosty Z has suggested something like this)
And while in theory there are sufficient permissions for a user in the http user group to write to the logfile - there's probably some suphp-like behaviour and when your script is accessed through the web it is executed with/as the username that is set as it's owner (file owner of the script that is) - try changing it.
In the past, I had no error logs in two cases:
The user under which Apache was running had no permissions to modify php_error_log file.
In this situation errors are logged to Apache error_log file.
Setting log_errors_max_len = 0 in php.ini worked for me.
Set the maximum length of log_errors in bytes.
In error_log
  information about the source is added.
This length is applied
  to logged errors, displayed errors and also to $php_errormsg, but not
  to explicitly called functions such as error_log().
And you will see no errors logged in normal log file, but all of them will go to ssl_error.log
This can also be caused by Apache's own LogLevel directive, which if set too high will override PHP's logging.
Checking the logpath on the right server helped for me... facepalm
In average, each partitioning divides the array to two parts (which sums up to log n operations).
In total we have O(n * log n) operations.
in average log n partitioning operations and each partitioning takes O(n) operations.
In this case, the number of times we can break it in half will be the base-2 logarithm of the number of inputs.
That's 7 levels of partitioning (and yes log2(128) = 7).
So, we have log(N) partitioning "levels", and each level has to visit all N inputs.
So, log(N) levels times N operations per level gives us O(N log N) overall complexity.
Introsort does so by keeping track of the current partitioning "level", and when/if it goes too deep, it'll switch to a heap sort, which is slower than Quicksort for typical inputs, but guarantees O(N log N) complexity for any inputs.
Well, it's not always n(log n).
To visualize 'n log n', you can assume the pivot to be element closest to the average of all the elements in the array to be sorted.
As in each step you go on halving the length of the array, you will do this for log n(base 2) times till you reach length = 1 i.e a sorted array of 1 element.
In-fact you need to find the position of all the N elements(pivot),but the maximum number of comparisons is logN for each element (the first is N,second pivot N/2,3rd N/4..assuming pivot is the median element)
There's a key intuition behind logarithms:
The number of times you can divide a number n by a constant before reaching 1 is O(log n).
In other words, if you see a runtime that has an O(log n) term, there's a good chance that you'll find something that repeatedly shrinks by a constant factor.
Therefore, a good intuition for why quicksort runs in time O(n log n) is the following: each layer in the recursion tree does O(n) work, and since each recursive call has a good chance of reducing the size of the array by at least 25%, we'd expect there to be O(log n) layers before you run out of elements to throw away out of the array.
@Jerry Coffin's excellent answer to this question talks about some variations on quicksort that guarantee O(n log n) worst-case behavior by switching which sorting algorithms are used, and that's a great place to look for more information about this.
Complexity of partioning is O(N) and complexity of recursive call for ideal case is O(logN).
For example, if you have 4 inputs then there will be 2(log4) recursive call.
Multiplying both you get O(NlogN).
here it is: [How to exclude certain messages by TAG name using Android adb logcat?
go to  Logcat console > Edit Filter Configuration > Log Tag(regex) and 
put this instead
note that EXCLUDE_TAG1 and EXCLUDE_TAG2 are Log Tag you exclude from logcat.
As we know we have different kernel level logs:
Console log level is something which is used to set log levels that can be displayed on console window with Log levels(printk) < Console log level(4 taken considering wrt your case).
i.e., it will print kernel messages with printk using log levels from 0,1,2 and 3. rest 4 to 7 will be logged in circular buffer maintained by kernel - can be seen issuing "dmesg".
Now if we move on to Default log level:
Whenever you use printk without any log level info, say for eg:
printk("Insmod my first driver\n"); // this log level will be set to "kern_warning"(as default log level is 4).
So the difference is console log is used to decide what is needed to be printed on console and default log level is used for what log level is to be taken by default if not mentioned by printk during kernel module programming.
[Winston (hyper-link)] is a pretty good logging library.
You can write logs out to a file using it.
You can also use npmlog by issacs, recommended in 
[https://npmjs.org/doc/coding-style.html (hyper-link)].
You can find this module here 
[https://github.com/isaacs/npmlog (hyper-link)]
The "logger.setLevel('ERROR');" is causing the problem.
logger.js
logger.test.js
When I run "node logger.test.js", I see only "ERROR message" in test.log file.
If I change the level to "TRACE" then both lines are printed on test.log.
I have looked through many loggers, and I wasn't able to find a lightweight solution - so I decided to make a simple solution that is posted on github.
[Log4js (hyper-link)] is one of the most popular logging library for nodejs application.
Coloured console logging
Replacement of node's console.log functions (optional)
File appender, with log rolling based on file size
SMTP, GELF, hook.io, Loggly appender
A logger for connect/express servers
Configurable log message layout/patterns
Different log levels for different log categories (make some parts
 of your app log as DEBUG, others only ERRORS, etc.)
Installation: npm install log4js
Configuration (./config/log4js.json):
[code snippet]
Observe that errorLogger is a wrapper around logger.trace.
But the level of logger is ERROR so logger.trace will not log its message to logger's appenders.
The fix is to change logger.trace to logger.error in the body of errorLogger.
A 'nodejslogger' module can be used for simple logging.
It has three levels of logging (INFO, ERROR, DEBUG)
The module can be accessed at : [https://www.npmjs.com/package/nodejslogger (hyper-link)]
Recently I used winston with with papertrail which takes the application logging to next level.
you can manage logs from different systems at one place.
this can be very useful when you have two backend communicating and can see logs from both at on place.
Logs are live.
you can see realtime logs of your production server.
you can create alerts to send you email if it encounters specific text in log.
[simple-node-logger (hyper-link)] is simple multi-level logger for console, file, and rolling file appenders.
change log levels on the fly
stats that track counts of all log statements including warn, error, etc
Messages can be logged by
PLUS POINT: It can send logs to console or socket.
You can also append to log levels.
This is the most effective and easy way to handle logs functionality.
I'll generally release with INFO being logged, but only if I know that log files are actually reviewed (and size isn't an issue), otherwise it's WARN.
I don't think there are any hard-and-fast rules; using the log4j-type levels, my 'rules of thumb' are something like:
INFO: Normal logging that's part of the normal operation of the app; diagnostic stuff so you can go back and say 'how often did this broad-level operation happen?
This is where you might log detailed information about key method parameters or other information that is useful for finding likely problems in specific 'problematic' areas of the code.
I need to log every single statement I execute to find this @#$@ing memory corruption bug before I go insane"
TRACE is intended to be the absolute lowest level of logging outputting every possible piece of information you might want to see.
To avoid a deluge of information, it is generally not recommended that you enable trace-level logging across an entire project.
Examine the build log in the intermediate files directory to see what actually executed.
The path and name of the build log is represented by the MSBuild macro expression, $(IntDir)\$(MSBuildProjectName).log.
EDIT: To appease our disgruntled -1er... You could obviously infer from this that you could add a <Execute Command="notepad.exe $(IntDir)\$(MSBuildProjectName).log"/> or similar if it needs to literally pop up, but that doesnt make sense to me.
Log file from Visual Studio is only supported for C++ projects.
Yes, you can check the logger level by
If you're using the root logger, for example because you called logging.basicConfig() then you can use
In your logger instance you can check it like this, as @Milán Vásárhelyi said:
myLogger.level
logging.getLevelName(myLogger.level)
As explained in [this walk through the source code (hyper-link)] logger.level is often wrong.
You want logger.getEffectiveLevel()
If you haven’t explicitly set a level on your logger object, and you’re depending on .level for some reason, then your logging setup will likely behave differently than you expected it to.
Use the limit parameter: hg log --limit 5
And I write something to the eventlog (otherwise you cannot retrieve the contents of the log file):
it will tell Laravel that you'd prefer to have multiple logs, each suffixed with the date of the when the error occurs.
In never versions of Laravel, the Logging config has been moved to it's own config file, as seen by @ShanthaKumara's answer ([https://stackoverflow.com/a/51816907/3965631 (hyper-link)]).
In the version of Laravel 5.6 that I am using, the configuration file for logging is config/logging.php
It will create log files for each day in the format laravel-2018-08-13.log in the logs directory.
The log directory will be like
After applying rotation configuration the directory is having the log file created for the current date (as circled one which is created for today 2018-08-13).
The laravel on daily logging is fine, but on 1st year, you will have 365 files of laravel.log.
This way you have organize the logs by each year/month/day.
2021/

06/

01/

laravel.log
You can install this [Jenkins Console log (hyper-link)] plugin to write the log in your workspace as a post build step.
With an additional post build step (shell script), you will be able to grep your log.
@Bruno Lavit has a great answer, but if you want you can just access the log and download it as txt file to your workspace from the job's URL:
Edit: 
The actual log file on the file system is not on the slave, but kept in the Master machine.
You can find it under: $JENKINS_HOME/jobs/$JOB_NAME/builds/lastSuccessfulBuild/log
Jenkins stores the console log on master.
If you want programmatic access to the log, and you are running on master, you can access the log that Jenkins already has, without copying it to the artifacts or having to GET the http job URL.
From [http://javadoc.jenkins.io/archive/jenkins-1.651/hudson/model/Run.html#getLogFile() (hyper-link)], this returns the File object for the console output (in the jenkins file system, this is the "log" file in the build output directory).
We use this to get the [http://javadoc.jenkins.io/archive/jenkins-1.651/hudson/model/Build.html (hyper-link)] for the upstream job, then call this job's .getLogFile().
.getLogFile().getParent() takes away all the pain.
You can get the console log file (using bash magic) for the current build from a shell script this way and check it for some error string, failing the job if found:
For very large output logs it could be difficult to open (network delay, scrolling).
This is the solution I'm using to check big log files:
Now you can save your big log as .txt file.
Open it with notepad++ and you can go through your logs easily without network delays during scrolling.
Log location:
Get log as a text and save to workspace:
To write the results of top to a file, use the -n 1 option (only one iteration) and redirect the output to your log file.
Be sure to click down to the nodpi folder in the dialogue, as the project view won't show you all drawable folders separately like it used to, so it won't be immediately apparent that it went to wrong one.
