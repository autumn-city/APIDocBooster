torch.nn.functional.cross_entropy function combines log_softmax(softmax followed by a logarithm) and nll_loss(negative log likelihood loss) in a single function, i.e.  it is equivalent to F.nll_loss(F.log_softmax(x, 1), y).
Yes, the cross-entropy loss function can be used as part of gradient descent. 
 So yeah, the  negative log likelihood and the binary cross entropy are  equivalent.  And in practice in deep learning, people just say  cross entropy, multi category cross entropy, which would be a  multi class version of the negative log likelihood, which  we will cover later in this lecture when we talk about the  softmax function.  So just to keep it brief, the negative log  likelihood that we just covered a few videos ago, is the same as  what people call the binary cross entropy, they were just  formulated in different contexts.  So negative log  likelihood comes more like from, I think it's like, it's probably  from a statistics context, I don't know the first paper, or  reference that mentioned that.  But this is something usually I  see in statistics papers, and the binary cross entropy thing  has originated from the field of information theory, or computer  science.  So we have actually seen that, or not, the cross  entropy, where we have seen the self entropy, or just entropy,  and statistics 451. 
 For those who took this class, in fall  semester, where we had used the entropy function in the context  of the information theory and decision trees, but we used a  lock to instead of the natural algorithm, but yeah, it's kind  of somewhat related, if you have taken any class where you  talked, for example, about the KL divergence, or callback  Leibler divergence, which measures the difference between  two distributions, the KL divergence is essentially the  cross entropy minus the self entropy. 
 And  there's also a multi category version is the multi category  cross entropy, which is just a generalization of the binary  cross entropy to multiple classes.  So in order to make  that negative log likelihood or binary cross entropy work for  multiple classes, we assume a so called one hot encoding, where  the class labels are either zero or one for some reason, it was  cut off here. 
