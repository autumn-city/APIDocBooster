Friends, welcome to my youtube channel. Dhanesh here. We were discussing with transformers
let's  continue with the sessions
I will be focusing on multi-head attention
in this video
Let's continue. We will close this and we will start
fine
Fine
Transformers
In this video my focus is multi-head attention . Scaled dot product attention models
are the building blocks of transformers. In the last video
We discussed in detail about those scaled dot product attention models.
But i'm covering here as well
Okay
you know these aspects
I discussed in the last video
Sequential data means ,order matters, order is very important
Convoluted neural network.
It's easy to parallelize.
But the drawback is , it cannot handle sequential data.
RNN can process  sequential data but not able to parallelize ,as they are connected ,each one
need the previous ones result . We discussed those aspects .
see
transformers
It's introduced in 2017
And like lstm, (long short-term memory) ,transformers is an architecture for transforming
one sequence into another one
with the help of two parts, encoder and decoder.
But it differs from the previously described
existing sequence to sequence models.
Because it does it does not imply any recurrent networks,
like gru & lstm. GRU means gated recurrent unit.
Lstm means long short-term memory .these units use the recurrent neural network architecture.
but our transformers  don't use
recurrent neural networks ,instead of that ,they use attention models
This is  a high level architecture of a transformer, it contains a multi-head attention module
and
a feed forward
network. It's a combination of multi-head attention plus feed forward network
Now what is this scaled dot product attention?
The basic building blocks of the transformers are scaled dot product attention units
transformer model learns three weight matrices
the attention function used by the transformer takes three inputs
Q query, K key and V is the value
uh
to understand this
The query key and value concept come from retrieval systems
for example
When you type a query to search for some video on YouTube, the search engine will map your query
against a set of keys, that is video title,
Description etc,
associated with the candidate videos in the database.
Then present you the best matched videos. That is values.
this is the architecture of the
scaled dot product self attention
architecture
It's a high level architecture
Here you have inputs at different time stamps
Query key and value are the inputs to the self attention module
And you are getting the output from the self attention module
This is the internal architecture of
the scaled dot product attention modules
See first you have the query tensor
and the
Key tensor, it's  applied to MathMul. That is a matrix multiplication
so
the tensors, the query and the key are multiplied
Then it is scaling down .Then you are doing a mask operation
Then the softmax function again. It's doing a matrix multiplication with the tensor Value
That's what happening here in this diagram. So I will explain you what is happening in each module
the dot product of query and key
See after feeding the query, key and value vector through a linear layer the queries and keys undergo
a dot product matrix multiplication to produce a score matrix.
Score matrix.
The score matrix determines how much focus should a word be put on other words.
So each word will have a score that corresponds to other words in the time- step.
The higher the score the more focus.
This is how the queries are mapped to the keys.
Next is scaling down the attention scores. How do you scale down the attention scores?
The scores get scaled down by getting divided by the square root of the dimension of query and key.
This is to allow for more stable gradients,
as multiplying values can have exploding effects.
You may be familiar with the vanishing and exploding gradient in
recurrent neural networks.
Next is the softmax of scaled score. That's what we need to do in the next step.
you take the softmax of the scaled score to get the attention weights, which gives you
probability values between 0 and 1
By doing a soft max, the higher scores get heightened and lower scores are depressed.
This allows the model to be more confident about which words to attend too
This is the equation for the soft max function you may be familiar .
Next ,multiply softmax output with Value vector
so that's what we do in the next step.
 
Then you take the attention weights and multiply it by your value tensor to get an output tensor.
The higher softmax scores will keep the values of words the model learns is more important.
The lower scores will drown out the irrelevant words
Then you feed the output of that into a linear layer to process
Next we are going to discuss about the math of
attention
as you know q
the matrix
That is the vector representation of the word in the sequence
the k the keys
Are v the values
the dimension of the key vectors used
This is the math of attention. This is the important equation.
This is what we discussed.  This is the vector multiplication of query and the key.
Then
You have divided it with the square root of dimension
Then you have taken the soft max
Then you are multiplying with the value. So this is like
Y =WX
X = V
 
This is the weight
 
 
Is the weight
Now we talk about multi head attention
So we have discussed about the scaled dot product attention. Then why we need multi-head attention
See this multi-head attention improves the performance of the attention layer in two ways
It expands the model's ability to focus on different positions
That is the first
advantage
the second advantage,
It gives the attention layer
multiple
representation sub-spaces.
As we see,
with multi-headed attention ,we have  multiple sets of queries,
keys,
values,
and multiple weight matrices
 
you know, the transformer uses eight
attention heads.
so we end up with  eight sets for each
encoder/
decoder.
Each of these sets is randomly
initialized . Then, after training each set is used to project the input embeddings
into a different representation subspace, so that's why
We use multi-head attention. To summarize,
It expands the model's ability to focus on different positions
And it gives the attention layer multiple representation sub-spaces. These are all the
 
advantages of multi-head attention.
Then I will tell you,
 
multi-head attention is just
several attention layers
stacked in parallel with different linear transformations of the same input.
 
 
 
one set of matrices is called attention head
 
 
 
 
 
Each layer in a transformer model has multiple attention heads
 
This is the architecture
the scaled dot product attention module
value, key and the query is passed to a linear layer
and after that it is passed to the
scaled dot product attention module. You can see
different layers are there.
Concatenation is happening and again pass to a linear layer.
Research has shown that many attention heads in transformers
encode relevance relations,
that are transparent to humans.
The multiple output for the multi-head attention layer are
Concatenated to pass into feed-forward neural network layers.
See
See that's all about the multi-attention head
we have covered
most of the aspects as you can see you understood .what is scaled dot product attention you understood
What is multi-head attention?
And you know the architecture also we discussed both scale dot product attention and multi-head attention
That's all about the attention models thanks for watching please like share and subscribe. Thanks a lot