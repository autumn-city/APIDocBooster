Hey there and welcome to this video! Today, 
we'll implement the GPT (or generative  
pre-trained transformer) from scratch. To be 
more specific we will try to replicate the  
behavior of an existing model from the wildly 
popular transformers library of hugging face.  
It is implemented inside of this modeling_gpt2.py 
module and this is the actual class. Anyway,  
to be able to fit everything into a single video 
we are only going to focus on the forward pass  
that means that we are not going to implement 
any training logic. Plus, we will make other  
simplifications since the goal is to create 
a minimal working implementation. Once we  
are done we will just download a pre-trained 
model from the hugging face model hub and copy  
manually the weights to our custom model to see 
how it performs. Anyway, this video is purely  
educational and I guess you should probably 
be using hugging face for any real use case.  
At the same time, their implementation is 
in my opinion very complex since it covers  
the training logic, parallelization and many other 
things and one can get easily lost in it. Also,  
our implementation is going to be very 
much based on this amazing repo by Andrej  
Karpathy. And yeah as always all the credit 
goes to the authors and the contributors of  
the aforementioned repositories and also to 
OpenAI. I'm not affiliated with any of them  
in any way and I apologize in advance for 
potential misunderstandings and bugs in my code.  
I hope you enjoy the video. I prepared a couple 
of slides to explain the most important concepts.  
First of all, let me try to explain what 
the main goal of the GPT is. The model takes  
in text as input and then it generates a new 
token that represents a plausible continuation  
and we can use a simple trick to actually 
generate text of arbitrary size which is called  
autoregression. We just take the single token that 
was generated in the previous iteration and we  
add it to the end of our input text and then just 
simply rerun the GPT model. And we can do this  
for as many iterations as we want. Now let us look 
at the forward pass without delving into details.  
As discussed this video is only going to be about 
the forward pass in the context of inference  
and at no point we will discuss the training. 
Anyway, let's say that our input text is "Today,  
we are" and first of all we will give this text 
to a tokenizer that is going to split it into  
four tokens. And what is really important is that 
these four tokens exist in a vocabulary and have  
a unique index assigned to them. The tokenizer 
itself is not something we'll discuss in this  
video and we will just use an existing hugging 
face implementation. The sequence of 4 numbers  
representing the 4 tokens then continues to what 
we will call the GPT module and I will describe it  
in detail in the next couple of slides. For now, 
let me just stress that it outputs a probability  
distribution over all tokens in the vocabulary 
and the higher the probability of a given token  
the more plausible it should be as a continuation 
of our input text. Now this is different to what  
I mentioned in the previous slide where the GPT 
model was spitting out a new token right away. I  
just wanted to make things simple. In reality, it 
does indeed give us a probability distribution and  
it is up to us to decide what let's say sampling 
strategy to use to generate the new token. Anyway,  
now that we have an understanding of the general 
pipeline let us focus on the GPT module. Note that  
in the following slides I will completely leave 
out dropout modules and some normalization modules  
since they don't change the shape of tensors 
and I don't want to clutter the slides too  
much. This is the first part of the explanation 
of what is happening inside of the GPT module.  
As discussed, it inputs the token indices 
generated by the tokenizer and these token  
indices are then dispatched to two different 
embedding modules. First of them is the token  
embedding and the second of them is the positional 
embedding. Note that I call them modules, however,  
in the background they are nothing else than a 
single two-dimensional tensor and by running a  
forward pass on them we simply query rows of these 
tensors. The token embedding module basically  
holds one learnable embedding for each token 
in the vocabulary and the positional embedding  
has one learnable embedding for each position the 
token can have in the input text. And finally we  
just take the token and the positional embeddings 
and we element-wise sum them together and we  
are done with the first part. Our tensor then 
goes through multiple so-called decoder blocks  
and I will talk about the decoder block in the 
next slide, however, the most important fact to  
point out is that the shape of the input tensor 
is identical to the shape of the output tensor.  
Then we continue to a linear module. What is 
actually happening is that we have a single  
linear module that is reapplied independently 
to each and every token or in other words  
row of the tensor and the output dimension of the 
linear module is going to be the vocabulary size.  
And then we will simply discard all the rows 
except for the last one and then we feed this  
last row of logits corresponding to the token 
"are" through the softmax and we will end up with  
a probability distribution. Note that at training 
time we wouldn't discard any rows, however,  
yeah again we just focus on the inference. Anyway, 
the final probability distribution is exactly the  
one we are going to be using to determine what 
the next token could be. Let us now zoom into the  
decoder block and try to understand what is going 
inside of it. The input tensor has a shape number  
of tokens times number of embeddings and first 
of all we run it through a layer normalization  
module and note that similarly to the linear layer 
we have seen before we actually reused the same  
layer normalization for each token. The output 
tensor has the same shape as the input tensor.  
Then our tensor continues to the masked self 
attention module. The resulting tensor is then  
summed with the very initial tensor or in other 
words the first row of this diagram is a residual  
block. Then again we apply layer normalization and 
finally we apply a multi-layer perceptron. Again,  
we sum the result of the residual block with 
the starting tensor and yeah that is our decoder  
block. Let us now have a quick look at the masked 
self attention module that we saw inside of our  
decoder block. First of all, this slide is only 
going to show what happens if we have a single  
head. Anyway, we take the input tensor and we feed 
it through a linear layer. What is special here  
is that the linear layer will have three times as 
many output features as input features and that is  
why its output can be thought of as three separate 
tensors of the same shape as the input. These  
tensors are called value, query and key and now 
the idea is that for each row of the query tensor  
we want to compute a probability distribution over 
the rows of the key tensor that will represent how  
similar a given query token is to a selected key 
token. This probability distribution is created  
by computing inner products and normalizing them 
with softmax as you can see in the formula. There  
is one crucial restriction and that is that we 
apply a no lookahead mask. That means that if we  
take for example the third query row we can only 
compute how similar it is to the first the second  
and the third key role and we manually set the 
similarity to the fourth key row to be zero  
this mask will prevent us from including 
information that is contained in the future tokens  
in the embedding of the current token. Anyway, 
once we have this probability distribution or  
we can also call it attention weights for every 
single row of the query tensor we then simply  
use them to compute linear combinations of the 
rows of the value tensor and we end up with a  
new tensor that again has the same shape - number 
of tokens times number of embeddings. And finally  
we feed it through a linear layer and that's 
it. Anyway, now we should have all the necessary  
let's say theoretical knowledge and we can start 
coding! So let us implement the neural network!
And first of all we will just start with a small 
technical detail let's say. Maybe you remember  
the decoder block had an MLP inside of itself 
and one of its layers let's say is going to be  
the Gaussian Error Linear Unit activation and 
we will just outsource this implementation to  
what hugging face transformers has. We 
can also use the inbuilt torch one but  
they seem to be slightly different and I just 
want to make sure that at the end our forward  
pass gives us exactly the same results for our 
implementation and the official hugging face one.
Now let us actually do something real and that 
is the implementation of the decoder block.
First of all, we choose the dimensionality of 
our embeddings and then we specify the number of  
heads for our multi-head self attention. Number of 
positions represents the maximum number of tokens  
we can have and this block needs to have this 
information because we will have a guarantee that  
we will never get more tokens than the 
maximum number of tokens. And then we  
will have two dropout probabilities/ Honestly, 
I could have left it out because in this video  
we will only focus on the inference 
mode and the PyTorch dropout module  
behaves like an identity function in inference 
mode so it's not really necessary but I just  
decided to keep it in the code for consistency. 
Finally, there's this hyper parameter for the  
layer normalization. Internally, we will 
have two layer norm modules, we will have  
one attention module and we will 
also have one multi-layer perceptron.
We instantiate our two 
layer normalization modules.
Here we instantiate the PyTorch multi-head 
attention module and we feed in all the relevant  
hyper parameters. We could have implemented 
this ourselves and actually the repo of Andrej  
Karpathy does contain the implementation 
but I just wanted to make things minimal.  
But anyway if you look at the constructor 
you can see that at no point we specified  
the mask. We will actually need to provide 
the mask whenever we run the forward pass,  
however, our mask is not really going to change 
for different samples and it's always going to be  
this triangular matrix representing the fact 
that we don't want to peek into the future.
Here we just registered the mask 
under a buffer. Yeah and maybe  
let me just quickly show you 
what this is actually doing.
So as you can see it's basically a lower 
triangular matrix and the elements that  
are equal to True will be those that are not 
going to be considered for the self-attention.
And as you can see if we 
provide a 2D array which we will  
it will be broadcasted across the batch and for 
a binary mask which is exactly our case a True  
value indicates that the corresponding 
position is not allowed to attend.
Finally, we prepared the multi-layer 
perceptron. We hardcore a couple of parameters,  
we use our custom Gaussian Error Linear Unit 
activation and also apply a dropout. This is  
the input to the forward pass. It has the 
shape of batch size, number of tokens and  
a number of embeddings and importantly the 
output has exactly the same shape which is  
in many ways convenient. And here you can 
see the sketch of the forward pass again.
First of all, we get the actual 
values of all three of the dimensions.
Then we start working on the first residual 
block which starts with a layer normalization.
We dynamically cut off our attention mask buffer 
and that is because the registered buffer actually  
has the maximum size of number of positions times 
number of positions, however, the chances are that  
for this given input tensor we will have fewer 
tokens than the maximum number of tokens.
Here we run the forward pass of the 
attention module. We basically provide  
3x our layer normalized tensor that will serve as 
the key, value and query and we also provide the  
prepared mask. And finally we tell torch that 
we don't need to have the attention weights  
since we are not really going to use them 
in this video. However, they are actually  
useful when it comes to trying to explain 
what the model was looking at and so on.
And we basically take the output of the residual 
block let's say and add it to the original tensor.
And here we just implement the second residual 
block in one line. We applied layer normalization  
and then we ran the tensor through the multi-layer 
perceptron and again we added it let's say to the  
input tensor and yeah that's our result. And 
now we can just write the entire GPT module.
We have seen a lot of these parameters before 
when we defined the decoder block but uh some of  
them are new and let me go through them. First 
of all, we have this vocabulary size and this  
number represents the number of tokens we have in 
our vocabulary. And the reason why it's important  
is because we need to have this number to be 
able to define the token embedding let's say  
table. Number of layers will represent 
how many decoder blocks we will have.  
Internally, we will have these two embedding 
modules - token and positional. We will have  
a dropout module and we will put all the decoder 
blocks in the sequential module. We will also have  
one final layer normalization and 
finally we will have the linear layer  
that let's say maps from the hidden 
embedding space to vocabulary space.
First of all, we save the number of positions 
internally and the reason is that we want to  
check whether the number of tokens isn't 
too high when we run the forward pass.
Here we define our two embedding modules. The 
first one is the token embeddings and basically  
for each token and our vocabulary we will have 
an embedding vector. And also we'll create the  
positional embeddings the reason why we need 
these positional embeddings is that we want to  
encode some information about the position of 
a given token in let's say a given sentence.
Here we define a dropout module 
and as mentioned since we're only  
doing inference in this video the dropout 
modules will behave like identity mappings.
And here very important we define a sequential 
module which will be a list of decoder blocks  
and note that these blocks are all let's say 
independent. They are separate instances.
Finally, we define a layer normalization module 
and a linear layer that will be the very last  
module. So the inputs and the outputs 
of the forward pass are really simple.  
There's a single input tensor that 
will have a shape of batch size  
times number of tokens. And we'll kind of 
assume that each of the elements in the tensor  
is between zero and the vocabulary size. And the 
tensor will be actually generated by our tokenizer  
and note that we will actually return logits 
for all the tokens and not just the last one  
as shown in the diagram at the beginning of the 
video. And also we are not going to apply softmax  
inside of this forward method. We will actually 
do both of these operations later when we  
implement sampling. Here you can see again the 
diagram of the first part of the forward pass.
Here we extract shape and device 
information from the input tensor.
Here we just make sure that if the 
user tries to provide too many tokens  
then we raise an exception.
We just on the fly create a tensor 
that represents the position.
And here we take our input token indices 
and we get their corresponding embeddings.
Then we take the positions and get the 
positional embeddings and we prepend it  
with an extra dimension and that is because 
we actually want to element-wise summit  
with the token embeddings and we want torch 
to do broadcasting over the batch dimension.
And here we sum the two embeddings and 
we apply a dropout. Here you can see  
the diagram of the second part of 
the forward pass. And let me just  
repeat that we are actually going to end 
the forward pass after the linear module  
and we will extract the last token and 
apply softmax in a different function.
Here we run our tensor through 
each of the decoder blocks we have.
And finally we get our logits over all tokens in 
the vocabulary by applying a linear module. So in  
theory I could end the video here because 
we implemented the forward pass, however,  
yeah I thought it would be kind of 
boring so what we're going to do now  
is that we will write a couple of utility 
functions to be able to copy paste weights  
from the hugging face implementation to 
our custom implementation. First of all  
let us write a function that is going to take one 
tensor and copy its contents to another tensor.
So in this function and also in some 
other functions that I'm going to write  
the first parameter or let's say the 
first object is always going to be  
something coming from the official model and 
the second parameter is going to be an object  
in our custom model and we want to populate it or 
replace it by the values from the official object.
Here we modify in-place the parameter ours and 
we just make sure it has the same values as the  
official parameter. Now with the help of this 
function we are able to copy parameters. Let's  
also write a utility function that's able to copy 
an entire decoder block that we just implemented.
And again the first parameter is something we take 
from the official model. In this case it is going  
to be the hugging face gpt2 block instance and we 
make sure that we copy all the relevant learnable  
parameters to our block instance. Here I just 
use shorter variable names to write less code.
We start with the layer normalization and 
it has two parameters weight and bias.
So when it comes to the attention module 
we want to copy the input projection.
The second learnable tensor 
in the attention module is  
the output projection which is 
just the linear module again.
Here we copy the parameters from 
the second layer normalization.
When it comes to the multi-layer 
perceptron we first of all copy  
the parameters from the first linear layer.
And here we copy the parameters from the second 
linear layer and yeah that should be it. And  
now finally we can write a function that is 
going to copy all parameters of a given model.  
First parameter is the official object 
which in this case is the GPT2 language  
modeling head model. And we will try to copy 
all its weights to our GPT model instance.  
We again shorten the variable 
names so that we write less code.
We copy let's say the two tables behind 
the positional and token embeddings.
We iteratively go through all 
the blocks and we copy them.
We copy the weights of the final 
layer normalization module and also  
the head linear module and it didn't have a 
bias so we don't need to copy it. Let us now  
quickly check whether we can really copy an 
existing official model to our custom model.
First of all, let us just list all the 
hyper parameters or let's say keyword  
arguments our GPT model needs to be instantiated.
Now let us choose the model we will 
download from the transformer hub  
let's just use the vanilla "gpt2".
I actually had the model downloaded already but 
now we actually instantiated it also. A small  
note. I set the tie_word_embeddings to False 
so that we get the same number of parameters.
This is how the official config looks like. I 
mean there are way more hyper parameters than  
we had but the idea now would be to 
just extract those hyper parameters  
that are both in the official 
model but also in our custom model.
And yeah now we actually have everything 
we need to instantiate our custom model.  
So the first sanity check we can do is 
to count up the number of parameters in  
our model versus the official model. 
Our model has 163 million parameters  
and the official one has exactly the 
same number of parameters which is  
good news. Let's set both of the models to 
the evaluation mode because we will be running  
the forward pass and we want to make sure 
that things like dropout are deactivated.
And now we don't even want to take 
real text and tokenize it. We can  
just generate some dummy indices and 
pretend they correspond to some tokens.
Let's try to get logits from both 
our model and the official one.
So first of all, we can see that the shape of 
the logits is identical or both the official  
and our model. We can see that we have a 
single sample in the batch then we have  
4 tokens that we provided and then the last 
dimension represents the vocabulary size. But  
what about the actual values of 
these tensors. Are they the same?
The answer is no and let us actually 
inspect what the biggest difference was.
The biggest per element difference was 13. I 
mean this is not surprising because we randomly  
initialized our model and we took a pre-trained 
model from the transformer hub. What we want now  
for this to work is to use our copy_model 
helper function that we just implemented  
and make sure that the trainable 
weights of our model are identical  
to the trainable weight of the official model.
So the weights are copied and 
let us rerun the same checks.
We compute the logits. I mean we did not have to 
recompute the official logits because we did not  
modify the official model but whatever. And as you 
can see these two tensors are virtually identical.  
There are some small differences. I did not 
really investigate too much where they come from  
but yeah. And as you can see the biggest 
element-wise difference is extremely small. Okay,  
so we verified that the copying works and we 
will use it later. Let us now investigate how  
one can take the logits that are the output of 
our model and use them to generate a new token.  
As you saw the forward pass outputs a tensor of 
logits and one can just simply run it through a  
softmax layer and get a probability distribution 
and then sample from it. However, there are  
multiple ways how to extend this approach to be 
able to control the generation process a little  
bit more. I will talk about applying a temperature 
and only considering top k elements. However, note  
that there are other techniques that are often 
applied when generating tokens like beamsearch but  
i'm not going to talk about them. First of all, 
here you can see a formula of the softmax with  
temperature and note that it is nothing else than 
a standard softmax, however, the logits we feed as  
input will be divided by a constant that is called 
the temperature. And what's interesting is that if  
the temperature is 1 you end up with the standard 
softmax. The second scheme that we will implement  
is the so-called top k and it is extremely simple 
since we only consider the top k tokens with the  
highest probability and we make sure that 
the probability of remaining tokens is zero.  
I prepared a small visualization to demonstrate 
how these two schemes influence the original  
probability distribution. I'm not going to talk 
about the implementation since we are going to  
implement both of these techniques in a few 
moments. So what you see on the left is let's  
say the original distribution and the thing on 
the right is a modified distribution. For now  
they are identical because the temperature is one 
and the top k let's say larger than the number of  
tokens we have. Let us increase the 
temperature to see what happens.
The distribution is getting closer 
and closer to a uniform distribution.  
One important implication of this is that 
if we sample from this higher temperature  
distribution we would expect the 
variability of samples to be high.
What if we decrease the value 
of the temperature below 1.
In a way we sharpen the distribution and 
the gap between low probability and high  
probability tokens is increasing. 
In our token sampling context this  
can be useful to make sure that our model 
only sticks to the most probable tokens.
And we see that for very low 
temperature values we basically  
end up with a distribution where the original 
argmax token has close to 100 probability.
Let us also quickly look at the top k hyper 
parameter. In this specific example our vocabulary  
consists of 8 tokens and if the top k is 
higher then this basically means that we  
are considering all tokens. However, 
let's try to make it lower than 8.
And we see that only the most probable tokens 
survive and the rest of the tokens is assigned  
a zero probability. And in our context it might 
be used to make sure that the model always goes  
for the most relevant tokens while not modifying 
their relative probabilities too much. Finally,  
let me just stress that even after applying both 
of the described techniques the probabilities will  
always sum up to 1. And also the argmax token 
will stay the same. Let us now just implement  
this function that is able to generate the next 
token given some initial sequence of tokens.
First of all, we provide our model instance. Then 
we provide a list of indices representing tokens  
that we are going to condition on or in other 
words this would be the input text. Here we have  
the temperature hyperparameter. Here we have 
a sample flag. If it's active it means that  
we are going to be sampling which means that 
inherently there will be randomness. However,  
if it's set to False we will just always take 
the argmax. Top k parameter if it's None then  
we don't modify the distribution at all, however, 
if it's a number (an integer) we only keep the top  
k most probable outcomes. And finally the only 
thing we return is the index of our new token.
So here we just make sure that we 
only include the tokens that were  
in our context window. So let's say if 
the user provided too many tokens that  
our model wouldn't be able to digest we just 
take the number of positions most recent ones.
And here we take the list of tokens and cast it to  
a torch tensor and we also prepend 
it with a dummy batch dimension.
We run the forward pass on our 
model and we get the logits.
However, what's important is that we 
take only the first sample because  
our batch was composed only of the first sample 
and also we take the very last token or let's say  
the embedding corresponding to the very last token 
and throw away all the remaining token embeddings.
And here we just divide by the temperature.
If we want to apply this top k clipping  
then first of all we find the top k biggest 
elements inside of our logic tensor.
And then we simply take the smallest of the 
top k values and we make sure that all the  
elements that are actually lower than this 
value will be set to minus infinity. And if  
you're wondering why minus infinity the reason 
is that in a couple of seconds we will actually  
take this logit tensor and we will run it 
through the softmax activation and after  
softmax all of these minus infinity logits will 
actually end up having exactly zero probability.
This is where we create the probabilities.
And yeah, if we want to have 
randomness we sample and  
internally this is done through the 
multinomial distribution in torch.
However, if we don't want to sample and 
don't want to have any randomness we just  
take the argmax which would correspond to 
the token with the highest probability.  
Then we just extract the actual integer 
and return it. And yeah, we're done!  
Now we just want to write a small command line 
interface that will do two things. First of all,  
it will do the copying that I demonstrated in 
the IPython session but also it will allow us  
to generate text using this generate_token 
utility function that we just wrote.
Here we configure logging and the reason why we 
want to use logging is to be able to monitor the  
timings of the new token generation. And 
now we just want to write the CLI parser.
Let me go through some of the arguments. 
Here we can specify the model name and  
there are four options. All of these models 
can be downloaded from the hugging face hub.  
The user is supposed to specify some initial 
text. This parameter we've seen it before it  
just represents whether we want to sample 
randomly and if not we will just take the  
argmax. This steps argument is an interesting 
one because the generate_token function only  
generates one token at a time, however, here 
we will also build a logic that iteratively  
calls this generate_token function and the 
number of steps represents the number of  
tokens that we are going to generate. And yeah, 
these two parameters we've already seen them.
We just let the user choose the verbosity.
And we instantiate a tokenizer that 
corresponds to the model that we selected.
Here we instantiate and 
possibly download the model.
We list all parameters that the 
constructor of our GPT model  
needs and we find the corresponding value 
in the config of the official model.
We instantiate our model using these hyper  
parameters and we also set 
it to the evaluation mode.
We copy all the trainable weights from 
the official model to our custom model.
We use the tokenizer to actually take 
our initial text and convert it to  
a sequence of token indices.
If the user wants we also set a random state and 
it will matter when we call the generate token  
function because there could 
be sampling happening inside.
We have a for loop whose goal is to 
generate multiple tokens. First of all,  
we generate a single token and we provide 
all the relevant hyper parameters.  
Then we use this autoregression trick 
where we just take the new token that  
we just generated and we add it 
to the end of the input sequence.
Once we are done with this autoregression loop we 
just ask the tokenizer to decode the token indices  
back to human readable text. And we just 
print it out to see what the result was.  
And yeah our CLI is ready to be used. 
So we have written our small entry point  
and now we would like to see whether 
it's able to generate some nice text.
So first of all we make it verbose. We set 
the number of steps to 80. Then we choose  
the gpt2 model but as you saw there are 
different ones. I have this one downloaded  
so we'll just stick to it. And the second 
positional argument should be the input text.  
So let me just stress that since we did not 
supply this sample flag we'll always just take  
the argmax which is the most likely token and 
therefore there shouldn't be any randomness.
As you can see it's literally just 
repeating the sentence we gave it  
which is not something we want. And 
the reason why this is happening is  
because we did not sample. We just 
went for the most likely option  
and in this case it seems to be repeating the 
same text again and again. Let's add sampling.
As soon as we add sampling it would be 
also nice to set the random state so  
that we can get reproducible results. 
I set it to 2 and let's run it again.
As we can see it actually generated some text, 
however, as you would probably agree it quickly  
changed the topic and then just started rambling 
about something. The problem this time is that  
even though we are sampling it can totally happen 
that every now and then we select a token that  
has a relatively low probability and let's say 
this completely distracts the model and leads it  
to change the topic. And we can actually fix this 
by only sampling from the top most likely tokens.
So I guess this time it stayed on the topic. One 
can argue that it's kind of repeating itself.  
At the same time the initial text that 
I came up with is very basic and maybe  
the GPT model just tries to replicate its 
style. Let's try a different random state.
Yeah the start was pretty good and then it 
again started repeating itself. Let's maybe  
make the top k a little bit larger.
Let's go maybe back to 5.
But try a different random state.
I guess it works. And let me also 
show you what happens when we provide  
extreme values of the temperature. 
Let's get rid of this top k for now.
And let's set the temperature to 0.01  
and by the way when we did not provide 
it specifically the default was 1.
And as you can see it repeats the same 
thing again and again which is exactly  
what was happening when we were just choosing 
the argmax because even though we are sampling  
we are sharpening the distribution so much with 
this low temperature that this argmax token has  
an extremely high probability whereas 
all the remaining tokens have a very  
low probability. And let us now do the other 
extreme where we set the temperature to 20.
And as you can see it generates 
random tokens and that is because by  
providing such a huge temperature we turn 
the distribution into something that's  
very close to uniform distribution. Which means 
that all of the tokens in the vocabulary will have  
more or less the same probability. And finally 
let me also show you what happens when we want  
to generate a lot of tokens. Here instead 
of 80 steps we want to generate 800 steps.
And as you can see the initial steps are done 
really quickly but then we are slowing down  
and the reason why it's slowing down is that 
our context gets bigger and bigger which in turn  
means that the number of tokens that enter the 
GPT model gets bigger and bigger which leads to  
slower inference. This given model allows for the 
context of around 1000 tokens which was dictated  
by the number of positions hyperparameter. Let me 
point out one interesting and important thing and  
that is that the amount of time it takes to do the 
inference is quadratic in the number of tokens.  
And the reason is that the GPT model has 
a bunch of self-attention layers inside.  
To illustrate this point I ran the CLI for even 
more steps and then generated the following plot.  
On the x-axis we see the steps and on the y-axis 
we see how many seconds it took. The red line  
represents the actual times whereas the green 
line and the blue line represent the best linear  
and quadratic approximations. And clearly we see 
that indeed the time complexity of the inference  
is quadratic. Finally, around the 1000th step 
the curve becomes constant and that is simply  
because we only consider the 1024 most recent 
tokens. And the older ones are discarded as you  
saw in the implementation. Anyway, that's it 
for today. I hope you enjoyed the video! Feel  
free to write any feedback in the comments below 
and if you want to see similar content like this  
definitely make sure to check out some 
of my other videos or to subscribe!  
Have a nice rest of the day 
and see you next time!!!