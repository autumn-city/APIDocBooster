[Music]
hello
and welcome back to the third lecture on
probabilistic machine learning
this term today we are
going to begin to move forward towards
real
applications of probabilistic reasoning
that you might even call a very basic
and elementary form of machine learning
we are here in the course
we've done lecture one and two in which
we found out that
probabilities are a way to distribute
truth
over not in a binary way but in a
continuous way over a space of
hypotheses
we learned that this way of distributing
truths
recovers certain
common sense aspects of our reasoning in
our daily lives
we also saw that doing so drastically
complicates computational processes of
inference beyond those
of propositional logic because we now
have to keep track of a potentially
combinatorially large space
of hypotheses we saw in lecture two that
one way
to simplify these reasoning processes is
that
probabilities can be independent
probabilities over variables
can be independent or conditionally
independent of each other and
when we find such conditional
independent structure inference can
become
drastically cheaper than combinatorially
hard
today we will do two things we will
um first complete our mathematical
machinery so that we can then actually
finally have a license to talk about
probabilities on all sorts of
interesting variables
and then we'll do an example an extended
example to really see
how inference works in
vaguely realistic applications that
first part fixing our machinery
consists of two things that i've so far
left out
and haven't talked about so much or
actually haven't talked about at all in
the previous lectures
which is that the way i've introduced
probabilities
makes it hard to talk about certain
kinds of variables
continuous variables and derived
variables we will start with the derived
variables
so just to remind you again of our
definition of probabilities
this they rest on essentially the notion
of sets
so to define probabilities we take a
space of elementary
events um and then define a collection
of so-called measurable sets on them we
call this collection the sigma algebra
these this is a collection of sets
which has the property that it is that
it includes both the entire space and
the empty set
and it is closed under intersection
union and differences
of sets this just makes sure that we
talk about sets
that we can talk about a collection of
sets in which we are allowed all the
operations we would like to do we can
think about joining intersecting
and separating from each other sets that
we think about to define probabilities
we take these to be a function
p which maps from the elements of the
sigma algebra
onto the the real numbers such that all
empty sets are assigned probability zero
the entire
set is assigned a probability of one
and everything in between naturally has
to have a probability between zero and
one because of our
third or in this case the second axiom
which is
really like the master idea of
probabilities which is so-called sigma
additivity which says that
the probability of unions of disjoint
sets
is assigned a probability that is equal
to the sum
of the individual probabilities of these
disjoint sets
this axiom ensures that we are not
inventing truth out of thin air when we
combine
sets with each other and also that we do
not lose truth
into thin air when we take intersections
or unions
of um of sets
now um this is all nice and well and in
all the examples we've had so far
the process of inference that i showed
you was that i
define this elementary a set of
elementary events this measurable space
or provel space
that consists of this elementary set and
the sigma algebra
and then assign probabilities to every
element of omega
and then we could do some reasoning on
them for example we did this
with the example of earthquakes and
alarms in last
week's lecture which essentially had
a set omega that consisted of binary
strings over these four
binary variables in this graphical model
that we looked at
but sometimes there are actually almost
all the time
there are reasoning processes in which
we want to talk about so-called derived
variables variables
that aren't directly the ones on which
we define the sigma algebra
to give you a trivial example or an easy
one that might be easy to follow
consider the following situation we have
one coin we're going to throw this coin
n number of times every time we throw it
it has a probability
f of coming up heads let's assume we
throw it independent of each other so
every time we throw it we create
a new binary variable that we will call
these binary variables x1
to xn if we throw the coin n times and
now a natural question you might want to
ask
is what's the probability
for this these n coin tosses
to produce exactly r heads
and n minus our tails naturally
now notice that here our definition of
probabilities breaks down
so the atomic event
is the set of atomic events is the space
omega which consists of all binary
strings
of length n if we say that we encode
heads and tails
as binary variables 0 or 1.
but the variable r that we talked about
isn't actually
an element of this elementary space
it's a real number sorry it's a natural
number that lies between 0 and n
of course right and that's a different
space than omega
so we haven't actually defined a rule
yet
that allows us to talk about variables
such as these
and of course these are important these
are actually the variables we will
typically
talk about derived variables that
form by taking sums or
other kind of algebraic operations on
these elementary events
these kind of variables will be called
random variables and we will construct
them
in a formal way that does exactly
what you want it to be so if you
um i mean if you just look at this
example for a moment you can stop the
video here if you like
and think about what the probability for
this random variable is
then you'll probably come up with
exactly the right rule and what i'll do
now is i'll just give you the formal
definition of what this rule actually
should be
to do so i'll define two concepts called
measurable functions and random
variables and then use this
to define this probability on this
derived variable which is called a
distribution measure
or a law of the probability of this
random variable sorry the law of the
random variable
so first of all we need to define a
measurable function let's consider two
measurable spaces so that's two spaces
which both have an atomic set and a
sigma algebra
on it in our example this space is the
space of all
binary strings and this space is the
space of the natural numbers between
zero and n
which have sigma algebras in both cases
these sigma algebras are trivial they're
just the power sets
of these sets and
now consider a function x so in our
example that was the function r
that maps between these two spaces this
such a function will be called
measurable
if for any element g of the sigma
algebra of this output space
the pre-image of this element is in the
sigma algebra of the input space
so this means in our example if
every um if the if the the pre-image of
any um
union intersection or difference
of these uh of these power sets over the
this finite length segment of the
natural numbers
is um a corresponds
to a measurable
set of binary strings in the so if every
for example in particular if every
variable n of a number of heads showing
up
maps it is reached from a
set of um configurations of the
of the coins that such that they're
the sum of their heads is actually n so
why do we
need such functions well because if a
function is measurable
then we're so we're going to use such
functions
to if you like push forward
the notion of probability from the
atomic
set of events coin tosses on to this
derived space of
numbers of heads and we need to make
sure that
the if we now talk about measurable sets
of this derived space
that they that it's such measurable sets
kind of inherit the good properties of
the original space
if um there is
a probability on the original space as
we've just defined in our example
then because because then we want to use
these measurable functions
to construct a new probability on the
derived space and we will call this
derived probability a measure a
distribution measure or the law
of the random variable x this
distribution measure
is defined in exactly the way you would
imagine it to be so i'm just going to
tell you what the definition is
and then we should i'll show you we'll
go back to the example and check whether
this definition actually makes sense
so consider a random variable then the
distribution measure or law
of at which we'll denote by p x of this
random variable x
is defined for any
element of the elementary events
of the output space as
and therefore also of course to every
element of the sigma algebra because we
can
then use sigma additivity to construct
probabilities of elements of the sigma
algebra
as the following function so that's p x
of g
the the law the probability of the set g
under the law of x
by taking the pre-image of g
which is an element of the sigma algebra
of the input space because we've assumed
that x is a random variable so therefore
this is a measurable set so x is a
measurable
function and this is a measurable set
and then just
look up what the probability of that
measurable set in the original space was
that's the um probability
of the set of
all elementary events which are in
the pre-image of
x under x of g okay that was a
complicated sentence
what does that actually mean let's go
back to our example with
coin tosses so here our
sigma algebra on the original space the
original space is the space of all
binary strings
from of length n the sigma algebra on it
is the power set
the random variable is called r it takes
values little r
from um 0 to n and
our law so we could write oh and i've
already shortened our notation a little
bit i should have written
p index r of r
equal to little r is what well
we take the
set of all configurations of heads and
tails
such that the total number of heads is
r and then just check what its
probability is
so under this generative process that
i've defined up here
this probability is just the product of
the individual probabilities because
i've assumed that we're throwing this
coin
independent of each other in a more
general setting it might be something
else you just have to look in the
original space
to see what in the original probability
space to see what the probability is
here it's just the probability to get
r heads and n minus r tails what is that
well it's just the probability to get
heads
r times times the probability to get
tails which is one minus f
n minus r times that's our probability
for r and we could
actually say write this also as a
conditional probability
with some variables f and n but we've
assumed that we know what f and n
is so maybe it's not so necessary to
actually write this and we will often
just drop
this kind of these kind of variables
from the from the notation
this is the law of this random variable
just to repeat again
the original space is the set of all
binary strings the sigma algebra on that
original space
is the power set of these binary strings
the random variable
is defined by the function r which is a
measurable function
and we can use it to construct the
distribution measure we're almost done
we just haven't actually done this sum
yet so we haven't constructed
what the what the actual law is to do
that we have to do again a little bit of
combinatorics
which is a bit tedious but all of you
have done it in high school so i'm not
going to dwell on it much
we just need to get out the sum so for
that we need to compute
the number of ways there are to choose
r heads from
the n possible uh from the end coin
tosses that we've done in total
so how many binary strings are there in
the set of all binary strings of length
n
such that they contain r ones
well that is just given by and to get to
this you need to basically think about
what you learned in high school
n factorial divided by n minus r
factorial times r factorial
which is this n choose r function that
you can just compute with your computer
and this gives rise to this law
of this random variable that looks like
this so this
is a probability distribution
as the name suggests that distributes
truth
over the values from 0 to n here i've
chosen n
to be 10 and f i've chosen to be
one-third
and this gives this kind of distribution
and what we'll actually typically do is
i've written this here as a node as well
it will abuse notation this is a very
common
kind of thing to do and we will first of
all drop
the subscript capital r because
typically we will know
what function we're talking about and i
won't write capital r
is equal to the value r so the random
variable r takes the value little r
but i said i'll just write little r i'm
allowed to do this
because we make this assumption that
probabilities
know the name of their input variable
actually
this seems sort of complicated like a
while ago this seemed really complicated
to do this or would be maybe a weird
notation
but in your generation it should be easy
to think about this way because you used
two programming languages that do that
as well so for example in python
python knows the name of the individual
variables you know you have this concept
of a named variable that can be entered
into the into each function
and there are also python functions that
just assume that the first input you
plug in
could be interpreted as a certain kind
of variable so
this is exactly what's happening here we
could either tell the function that
we're talking about the random variable
r
or we could just say you probably know
what i mean when i write
little r okay
this allows us to define this
distribution by the way there's a name
for this distribution it's called the
binomial distribution which is the
law of the random variable
given by the sum over successful
so-called bernoulli experiments but all
the experiments are coin tosses
okay that was problem number one we are
now allowed to talk about derived
variables which is obviously a very
powerful concept
and there's one more problem that we
briefly have to talk about and that's
continuous input spaces so in all the
examples i've shown you so far
the set of atomic events omega was
a countable actually it was typically a
finite
discrete set on such sets
it's trivial to define the sigma algebra
you just take the power set the set
of all subsets of omega
and it's easy you can show this yourself
that this power set is a sigma algebra
it fulfills all of the axioms
however in continuous basis which we
want to talk about as well of course
measurable sorry not all sets and in
particular not the power set in general
are measurable so of course we want to
talk in our applications about real
value variables we want to talk about
rates and velocities and positions
in time and space and these are all real
valued so they come from an uncountable
space
and there is a weird problem that in
such spaces
not all sets are measurable
if you haven't heard about this before
i'm not going to tell you how this works
because actually showing constructing a
non-measurable set
takes about 10 minutes itself and it's
really confusing these are typically
really odd
sets to construct but i if you want to
know more about it just go on wikipedia
and check and look up
non-measurable set and you can find out
for yourself these are
typically constructed in a very cautious
way such that they are end up
being non-measurable suffice it to say
that this problem exists
and therefore we can't really define
sigma algebras by using the power set
um earlier on in the 20th century by the
way these non-measurable sets were a
real problem and they caused all sorts
of big discussion
um today we just know that they exist
and we have to deal with them
so what that means is we
can't that this so just to be clear this
doesn't mean
that we can't define sigma algebras on
continuous spaces
it just means that we can't take what
would be the canonical sigma algebra on
discrete spaces which is the power set
so instead we have to find some other
way of constructing
sigma algebras and
one canonical way to do so or actually
the canonical way to do so
is works on continuous spaces that are
topological spaces
topological spaces are spaces that allow
the definition of open sets so that's
actually a circular sentence i just said
here is what a topology actually is
so consider a space omega
and consider a collection of sets on
that space
such a collection is called a topology
on the space omega if
it contains both the entire space and
the empty set
and for all its elements it holds that
potentially infinite
unions of these sets and finite
intersections of these sets are also an
element of the topology
that's an abstract definition that's one
of the advantages of topologies that
they are very abstract
but for typical applications for real
vector spaces
you can think of them as the canonical
neighborhoods
so um for of as the topology
for r to the d as this collection of all
sets
u which have the property that
if x is in u then
there exists a positive number
such that all real numbers which are
closer to x than epsilon strictly closer
are also an element of u so literally
intuitively this means for any real
number
the set of neighborhoods is the set of
all
sets around this real number that
include all points of
distance at most but not
an epsilon any arbitrary epsilon
we can do this on the real line because
of this the existence of this function
of this norm so we can compute
the um or this metric right
so we can compute the distance between
these two points in the natural way
so if if d is one in the univariate case
we can just take the distance between
two points so that's the difference
between these two real numbers and then
the absolute value of it
and call that the distance and in the
d-dimensional multivariate extension
we just take the sum of squares and take
the square root
of these distances right so
you might have noticed already staring
at this definition even if you haven't
seen the definition of topology before
you might have noticed that it actually
sounds quite a lot like the definition
of a sigma algebra
it's almost a sigma algebra it's just
missing a few key pieces
so here i have the two definitions next
to each other here's the definition of a
sigma algebra literally copied and
pasted from the previous slides
here's the definition of a topology a
sigma algebra
is a collection of sets so that
here it is it's a subset of the set of
all sets so that's a collection of sets
just like a topology a sigma algebra
contains the entire set just like a
topology
sigma algebras also contain all
potentially infinite unions of
their elements just like a topology
and sigma algebras also contain all
potentially infinite
intersections of their sets
this in particular also implies that it
contains the empty set just like a
topology
why does that imply that
do you know it implies that because you
can take
two sets that don't intersect and then
their intersection is the empty set and
it has to be by that rule
in the sigma algebra
now however the sigma algebra also
requires two additional things
first of all differences of
two sets have to be in the sigma algebra
that's not true for the topology by the
way
what's the difference between two sets
just
in case you haven't heard
this is the set a and this is the set b
the difference between a and b
is this part that excludes this bit
okay now so that this is
the complement in a of the intersection
of a and b so to define that we need
complements
of sets
and there's a very subtle difference
which you might have noticed looking at
this
that topologies allow finite
intersections and sigma algebras allow
infinite intersections
so if you think about this for a little
bit
you might come to the conclusion for
yourself
that to turn a topology into a sigma
algebra
we only have to add certain sets
so what we can do to construct a sigma
algebra is to take a topology
and then sort of check mentally
which sets we have to add to get to in
particular allow
all infinite intersections of elements
of the topology
and to get all differences of elements
of the topology
it turns out that those sets are always
available
and we can always add them to get a
sigma algebra
and the resulting sigma algebra is
called the borrel
sigma algebra here's the definition
consider a topological space the borel
sigma algebra is the sigma algebra that
is generated by the topology
that means you arrive at it by taking
the topology
and include all infinite intersections
of elements of the topology
and all complements in omega of elements
of the topology this
then ensures that you can build
differences between sets and therefore
you have a sigma algebra
now this sounds like a lot of abstract
nonsense
and maybe it is but it's just
a a permission for us
to talk about probability measures
on continuous spaces i haven't told you
yet how to actually construct these in
practice we'll do that
in a moment but what this sentence
actually says what this definition up
there says
is that on continuous basis there are
topological spaces
there exists a sigma algebra it's called
the borel sigma algebra
by the way the borel sigma algebra is
the smallest by definition sigma algebra
generate that which includes all the
open sets of the topology
and in this lecture we will just assume
that we have such a space
which means that we will only ever
define
random variables that map from
discrete typically even finite
or euclidean spaces euclidean spaces are
topological spaces
so typically r to the d or
binary strings essentially so discrete
finite spaces on both of these we now
know that there are sigma algebras
actually we have
identified which signal algebras we are
going to use either the power set
for discrete spaces or the broil sigma
algebra
and therefore we are allowed to define
probabilities and we can
we now know that the both the axioms of
probability theory hold
and the mechanisms base theorem in
particular and the sum and the product
rule
are allowed to be used and we can
be sure that all derived properties
of probabilistic inference
will actually hold by the way
on continuous sorry on the on
spaces that allow the construction of
corel sigma algebras there is a nice
result which i'm not going to prove
that if you have two spaces which
have parallel sigma algebras then any
continuous function
x is measurable and can just be used to
define a random variable
so this allows us to say
um and this is actually follows almost
by definition because that's how you
define
a continuous function that pre images of
open sets are open sets that's the
topological way of defining continuity
so this gives us basically a set of of
situations which we are now to allow to
consider and it's a very rich set of
situations it's a set or it's a
situations that can be described by
variables
that are either an element of a discrete
space or
an element of a topological space
actually a euclidean space
and if you allow functions that are
continuous and as you know the set of
continuous functions is very large
then any all
such functions all such continuous
functions can be used to define
random variables and therefore laws of
derived variables
we only have to be careful the only
alarm bell we have and i have to have in
our head
is when we start talking about derived
variables that
arise as non-continuous
functions
good now that we can define these um
these borel sigma algebras
we can have one gray slide that
summarizes what we just did i introduced
two notions the first one is that of a
random variable
the second one instead of a borel sigma
algebra a random variable is our
way of constructing probabilities
on derived variables such
derived probabilities are called
the law of the random variable or also
known as the distribution measure of
that random variable
and quite often i'm just going to say
the distribution of
this variable and i'm not even going to
say it's a random variable i'm not even
going to say that it's the law or it's
the distribution measure i'll just say
the distribution
or the measure and use that almost
interchangeably because in all of the
applications we will talk about
these technicalities really don't matter
it just was important to do it once
properly
and borrel sigma algebra is our license
our permission
to talk about probabilities on
euclidean or more generally topological
spaces
they allow us to talk about
probabilities that are defined on
continuous spaces
i'm specifically saying they allow us to
talk about them
because they don't actually tell us how
to talk about them to do that
we have to move to the next slide but
maybe this is your chance to take a
quick break
so this thought process has now shown us
that we can define probabilities on
continuous spaces
it just hasn't really told us how to do
so in practice
now it turns out that all
reasonable probability distributions if
you like
allow a representation that is
particularly convenient and this
representation is known
as a probability density function a
probability density function
is a function that satisfies the
following definition so
consider a parallel sigma algebra and a
probability measure on it now
some probability measures have the
property that there exists a function
p which um
is a non-negative measurable function on
the um the space that we've defined our
probability on
which satisfy that the fact that the
property that
for all elements of the sigma algebra
this probability can be written
as an integral over this particular
function and
these functions are then called
probability density functions
now so that what that means is
if p has such a density then all
probabilities
that you might be interested in so all
probabilities or arbitrary elements of
the sigma algebra
can be written as integrals now it turns
out that
not all probability measures have
densities in particular
there are measures that have point
masses and those can't be written with
densities
because then they don't allow integrals
however
it works the other way around all
non-negative measurable functions which
integrate to 1
on the entire domain are probability
density functions of some
probability measure which is then
defined
in exactly this kind of way so if we
have a density we can use it to define
a probability measure and it turns out
that
many interesting measures actually also
can be represented by a probability
density function so therefore
almost for the entirety of the course we
will talk a lot about pdfs probability
density functions
these are connected to another notion
called the cumulative density function
the cdf
the cumulative tensility function is
actually a more general notion
that exists for all
probability measures it's the at
probability measures on continuous
domains sorry
so consider a probability measure on
our borel probability space
then we can define a commodity at this
distribution function
c d f sorry i did not just say
cumulative density function that's wrong
a cumulative distribution function
cdf by defining
it as a function f that takes as its
input
a point in this space and then computes
the probability
for um
that is assigned to the set that
contains all values
x that are smaller in all elements
than x so in the univariate case it's
easier to think about this
it's the probability that is assigned to
the set
of all variables to the left-hand side
of x i hope this is the right way around
for you yeah left hand side
of x now if it's actually sufficiently
differentiable
which it isn't always but if it is
sufficiently differentiable
then p actually has a density and that
density is given
by the derivative of f that this is then
true
follows from the central theorem of
calculus
now this sounds like cdfs are the more
fundamental objects
and pdfs are derived if they exist so
cdfs always exist but pdfs only exist
if the cdf is sufficiently
differentiable however in practice the
pdf the probability density function is
actually the object we really care about
why because the pdf essentially
transfers the rules of probability
without change directly to the
continuous domain what i mean by that is
the following
pdfs have the property that they are
their
integral over the entire domain is equal
to one
that's true because um of the
of komogorov's fourth axis which ones
which is one which is the action that
says
probabilities of the entire elementary
domain
atomic set have to be one
secondly if we have a bivariate
space of random variables x1 and x2
with a density a pdf then
the marginal density so that's the
probability
density function or associated
with the measure on one of them x1
is given by what you expect the integral
over the other variable over the
bivariate pdf
and of course the other the same for the
other variable x2
so this is the sum rule the only thing
we've done is we've replaced the capital
piece with little key
lowercase ps and we've replaced the sums
with integrals
very natural to do right and conditional
densities
uh have the same form so if we have
a joint pdf over
a bivariate random variable then the
conditional distribution of one
given the other assuming that the
marginal distribution of the other
sorry the marginal density of the other
is non-zero
it can be written in this exact way i'm
not showing that this is the case
it's just the case and i'll just leave
it to you to think about why that's the
case
so therefore pdfs essentially fulfill
the sum rule and the product rule at
least intuitively right you can just
replace
all capital ps with lowercase ps and all
sums with integrals
and you get back the rules essentially
that we've already seen for
probabilities
again in probability densities that's
not true for cumulative density
functions
because they are like integrals from a
certain direction
up on through this space
so therefore we can also apply we can
also like because these two hold
and base theorem is a direct corollary
of them bayes theorem also applies
to pdfs and that's actually what we're
going to do
all the time when we talk about
probabilities we will not actually talk
about the probabilities themselves
we will talk about the densities and
operate on densities
by applying bayes theorem to them
why does this work you might think well
the reason it works is really
just intuitively that these are
densities we're talking about
probability density functions these are
just densities of masses so
you can think about probabilities as
mass as truth
distributed across a space so that each
part of the space contains a certain
mass
and the densities are the infinitesimal
versions of that and if you think about
your physical intuition then densities
transform
like masses the only
thing that separates a mass from a
density is
the amount of space over which you
integrate so
that will mean that that that will also
define the one difference between the
density and the probability
which is that if we change the space
over which
we integrate to get
our unit mass then we have to think
carefully about how the densities change
and we'll do that in a moment
before we do that though i want to show
you a bit of an intuitive picture
so here is a probability density
function in red
i've just chosen one so what a
probability density function is
is a potentially multivariate function
so it takes a potentially multivariate
input in this case over two variables
x and y and it assigns to every point
in this space a non-negative
real number such that the integral
over this entire space is unit is one
by the way this doesn't necessarily mean
that
this pdf never exceeds in value with a
value one
there might be locations with density
higher than one
because you might concentrate all of the
mass or a large part of the mass of the
entire probability
into a sub part a subsection of this
space
that has measure less than one
and once we have such a bivariate
distribution
then we can and density then
we can talk about both conditional and
marginal distributions
so this colorful thing in the middle is
the joint distribution
the marginal distribution over the
variable y is what we get if we take
this multivariate distribution and we
project it onto y so we integrate out x
on the other hand the conditional
distribution is
what we get if we
cut through the distribution at a
particular point
so for example
in here oops i'm sorry
from through this part of the
distribution if you take a cut through
here
then you get this leg line
and that's a conditional distribution 4x
so that's a function of x given
that we know that y is at a particular
point
so this is an intuitive picture
of what a probability density is and
probability density functions
are the objects we're going to
use almost exclusively when we actually
operate with probabilities because most
of the time
the variables we care about are not
going to be discrete but continuous so
we have to use them and
thankfully some product and base rule
apply to all of these to two pdfs
so therefore they're quite natural to
use there is however
one caveat with probability density
functions
and that is because they are constructed
essentially indirectly by taking the
derivative of the cumulative
distribution function
they do transform non-trivially
and this is something that is best just
written down
and i will give you a quick proof
just pictorially almost to give you a
feeling for how this comes about
and then just actually done in an
exercise which we'll do later on
in this term so consider
a probability density function
over a variable which we call
x a random variable capital x which
might take values little x
and it's defined over some domain from
the which has a left and the right hand
side
then we can construct a new random
variable
by constructing a function u
which um we require to be
monotonic and differentiable because
then it has an inverse
um and so this monotonic differential
function creates a
so this is itself a random variable that
defines
a new quantity y and this new quantity y
also has a probability density function
if
this transformation is monotonic and
differentiable
and this new pdf is given by
this object p of y so here i
why there are two y's in here right the
index is supposed to say that this is a
function
that takes as its input the variable
that we call y at the value y
which is given by the old pdf that we
just
know and then multiplied by
this absolute value of the derivative
of the inverse of v which actually for
these monotonic
differentiable invertible functions is
given by the inverse
of the derivative of the function u this
is
like an infinitesimal version
of the definition of a random variable
that we did earlier today
and why does it have exactly this form
well it has this form
because pdfs are defined
through cumulative distribution
functions through cdfs
and the cdfs are really the fundamental
objects because they they define the
probabilities
we've defined the entire theory around
probabilities not around densities
so we have to make sure that the
densities conform with what we've done
two probabilities why is this the
exactly right form
well here's a simple proof below just
consider
the derivative of this function which
exists because we've assumed so
and it's larger than zero because it's a
monotonic function
now we could consider two points this is
the situation here
you consider two points c1 and c2 in
x then y which is u of x
lies between the domain d1 and d2
where d1 is less than d2 because it's a
monotonic function
and we can now think about the
cumulative density function a
distribution function
cdf of y
this cumulative distribution function is
by definition given by this expression
so now we want to replace the y with x
we do that by first thinking about what
this actually means in x in terms of u
but u is invertible so we can also
directly talk about x
and this is basically what the
definition of what we previously did
for the definition of
random variables or we're essentially
using the definition of a random
variable
to replace the probability of
to write the probability of y in terms
of a probability on x
and then use the fact that x has a
density
to write this cumulative
distribution function of x as an
integral
now this is a way
of dividing the cumulative density
function of y
in terms of an integral over the pdf of
x
that's convenient because we now can
construct from here
the pdf of y which is just defined to be
the derivative
of this cdf
of y well what is that well we know that
it's this function
so we just take this function and take
its derivative with respect to y
so that's a simple form of calculus
which using the chain rule tells us that
we have to take this p
x of v of y and um
multiply with the derivative of this
function v with respect to y
now notice that v because u is a
monotonic function
its inverse is also a monotonic function
and it's also monotonically increasing
so if you could look at this picture you
can think of so this is u of x
you can mentally flip this around and
notice that it's still a monotonically
increasing function
so for more timely increasing functions
this works
how about monotonically decreasing
functions so for these
the situation is ever so slightly more
complicated but it's basically are not
analogous
so imagine that we have a function that
is
monotonically decreasing now the first
thing that actually changes now is so
i've taken drawn a little picture here
we have a decreasing function u of x so
the first thing that happens is that our
integration domains are
exchanged so if we have a look a region
in which
x goes from c1 to c2 then that actually
means that
y which is the map from x to y lies
between
d1 and d2 but d2 is less than d1
and the cumulative distribution function
of y is still given
by uh by definition by the
probability measure assigned to the
region to the left of y
we plug in the definition of y
in terms of x use the definition of a
random variable
and now we have to um there's a
larger than in here rather than a less
than so we need to do the integral the
other way around
we know what the integral is at infinity
because the integral
over the entire domain
from minus infinity to plus infinity is
one so
we can write this well it's not really a
cumulative distribution function but we
can write this kind of function
with this sort of probability over the
region to the right of v of y
as one minus the probability for the
region to the left of y
and plug in the definition in terms of
the density which works because p is
assumed to have a density
and again now do everything as before so
this
density we're looking for is given by
the derivative of the cumulative
distribution function
this if you compute it actually here
there's not going to be a minus here
showing up
other than that everything is the same
because the derivative of 1 with respect
to y is 0.
and what now changing is that we have
here derivative of
v of y which is the inverse of u and if
you look again
at this picture if you mentally flip the
axis of this function around
then the inverse of a monotonically
decreasing function is also
monotonically decreasing so it has a
negative derivative
and minus times minus is plus so
we get um we can write this expression
with the absolute value of the
derivative that's exactly what we're
looking for
so this was the univariate case for the
multivariate case
the situation's a bit more complicated
i'm just going to show you what it is
because this isn't a calculus class
so um we um i will just tell you what
the answer is
if you have a multivariate joint density
over multiple
random variables and consider a
continuously differentiable
injective function with non-vanishing
jacobian so that's the corresponding
concept to a monotonic function in the
one-dimensional case
then the derived variable y which is g
of x has a density that is given by
the density of the original variable at
its pre-image of y
under g times the determinant
of the jacobian matrix
of the inverse of the function of g
so this sounds really complicated
there's a determinant there's a jacobian
matrix if you don't know what a jacobian
is by the way then please look it up
it's a matrix of partial derivatives of
elements of the multivariate function
so this sounds complicated but it's not
going to be
that complicated in actual applications
so later this time
we will get to see we'll actually get
exercises where you have to do this
transformation
and if you actually do it mechanically
like if i give you a concrete function
then it's usually quite straightforward
to write down jacobians
to compute the determinants at least
numerically
construct the inverse and so on and then
you'll find that this just
does something non-trivial the only
thing you really have to keep in mind
and this is our next grey slide is
first of all that probability density
functions
are an important concept we're going to
use all over the place
they distribute probability across
continuous domains
not every measure has a density but
all probability density functions define
measures probability density functions
are non-negative
real valued functions which are
integrable such that their integral over
the entire domain is one
and they satisfy when we interpret them
as defining a probability measure
these three rules which are continuous
analogs
of the sum rule the product rule and
base theorem
and they actually look a lot like the
sum rule the product rule and base
theorem so therefore
we can really do probabilistic inference
using
these density functions rather than the
probabilities which are actually the
fundamental object
the only thing you have to be really
careful about with densities is if you
change the variable in a particularly in
a non-linear fashion i mean
also in linear fashion but particularly
in a non-linear fashion so if you
construct the derived variables from
underlying variables
then the densities transform in this
non-trivial way
that involves the jacobian of the
inverse
of your transform and its determinant at
least that's true
if you have a transformation that is
actually invertible
with that let's take a quick break and
then we'll do something much more fun
we'll do an experiment
i know that these very theoretical
derivations are
tedious they can be boring especially if
you're not
someone who's particularly excited about
mathematics so
let's end this lecture with a real
example to finally
start doing some ever so
simplistic machine learning
let's look at a real example let's say
i want to know how many people what the
what's the proportion of people in the
population
that wear glasses that's
maybe a bit of a stupid question to ask
but of course
it's a template for a fundamental kind
of question that you might want to know
like
ask questions that you might want to ask
about the population or the world at
large
so how do you do this using a
probabilistic or if you like a bayesian
approach now this is one of these points
where normally i would ask you questions
in the lecture hall
and then we would have a discussion
about it and we would slowly go through
it
and hopefully that would help your mind
to follow along
now unfortunately because of corona we
can't do this
so i implore you try to slow down the
video stop it here and there
think for yourself otherwise you're not
going to be able to follow this
this example so
how do we do this well
we're going to define a probabilistic
generative model
which allows us to
do patient inference and here is how
this works
we begin by introducing a random
variable up
for this quantity that we care about
let's call it pi the probability to wear
glasses
pi is maybe a bit of a weird symbol but
obviously i can't use p
because it's already used for
probability distributions so
we're going to use pi and say i want to
know this unknown probability
pi that's obviously a real number
between 0 and 1.
so for some of you it might already be
confusing that we're trying to learn a
probability with probabilities
but probabilities are just numbers
right real numbers between 0 and 1.
there's nothing
that forbids us from trying to learn a
probability with probabilities
so there is this number it lies between
zero and one if it's zero then it means
there are
there's no one in the entire population
who wears glasses
now if it's one that means everyone's
wearing glasses and if it's 0.5 that
means half the population is wearing
glasses
okay and now we're going to assume that
we can do experiments
now in any other year i would do this by
actually doing the experiment in the
classroom and go around and look at
people
that's one opportunity for us to finally
get to know each other in lecture three
we can't do that today so please imagine
me
walking around looking at individual
people and asking is this person wearing
glasses are they not wearing glasses are
they wearing glasses this is not wearing
glasses
and every single time you make such an
observation we are essentially
collecting
the value of one more random variable x
let's call it x i
x 1 x 2 x 3 and so on that's all we need
those are all the random variables we
care about and now we just have to
define
probability measures and because pi
is a real value we'll typically define a
probability density function over it
and then afterwards everything is just
bayesian inference
so how we do that well we're just going
to use bayes theorem right
so that's the mechanism we have agreed
to use we're never going to question it
that's just
how probability theory works because
that's what our axioms told us to do
and the only quick question is what are
the terms
in bayes theorem what's the prior and
what's the likelihood
so we need to assign actual values to
these
now let's start with the prior for
simplicity i'll do something very simple
because one of the arguments i want to
make is that the prior doesn't matter so
much
maybe i just let's just assign a uniform
prior so
pi is a number between 0 and 1 so it
ranges
from 0 to 1 and we could just just say
every single number in that domain
including 0 and 1
are equally probable if we don't know
anything yet so there's just a flat
distribution this works because it's a
bounded domain and the integral from 0
to 1 over the function one is just 1.
great that's our prior now the tricky
bit
is what is the likelihood what's the
probability
to observe someone wearing glasses
if the true probability of wearing
glasses is
pi now i know that this
weird seemingly recursive definition or
kind of object is confusing to many
people
so there's this probability which we
call pi we want to know about it but we
don't know it
that's the probability though that tells
us what the probability is
to observe someone wearing glasses
so actually this is one of these points
where please
stop the video and think about it for
yourself so that you can have
this insight for yourself
but now that you've maybe restarted it i
can tell you
that the likelihood to observe someone
wearing glasses
if the true probability or to observe
someone wearing glasses is pie
is just pie right and what's the
probability
of observing someone who is not wearing
glasses
if the probability to observe someone
wearing glasses is pi
it's one minus pi of course
great so that's it we're done
we just now we can do bayesian influence
oh actually almost done
there's this annoying normalization
constant the evidence that we have to
compute the probability
to observe um someone wearing glasses
so um for that we need to just integrate
this probability density function
because that's just now a function
right so we just have to do it and it
turns out that that's a function that
can be integrated and it's called the
beta function
and um it'll just allow our computer to
do that
now instead of doing this i'll show you
a demo
see if this works it's a little bit of a
trick to do this
it's different from my usual setup
so okay imagine me walking around the
lecture hall now
and saying we want let's assume the
people in
the in the lecture hall are a sample
from the population and
we are going to collect individual
samples
so this what you're currently seeing is
our prior all right
that's a function that maps from 0 to 1
uh onto the reals
and it assigns a probability density
function of one
to every single number between zero and
one that's a probability density
function because it's
non negative and it integrates to one
and it's integrable now let's say i see
the very first person in the front row
and they are varying glasses how lucky
are we
now what i need to do to
do bayesian inference is i need to
multiply this prior
with that likelihood to observe this
person wearing glasses
and that's as we just saw this function
which we call which is just the function
x or pi
actually so this is typically the most
confusing part of this experiment to
most people
so let's go slow the probability
if the true probability to wear glasses
is zero and i see someone wearing
glasses
and the probability of that happening is
zero right
it would be zero that's what the
likelihood tells us if
i the true probability to observe
someone wearing glasses is 30
and i observe someone wearing glasses
that the probability of that happening
is thirty percent if the true
probability of wearing glasses is ninety
percent
then observing someone with wearing
glasses happens with probability
um oops i'm sorry i'm pointing at the
wrong line that's the right line the
dashed one then the probability is
90 right so
notice that the likelihood is a function
of
the latent variable not the observable
the observed variable
is the thing that the likelihood assigns
a probability to the likelihood is a
probability over the data
but a likelihood for the latent quantity
so it's a function
of the thing we don't know and now we
can reason
about what the true probability might be
and to do that we do
base theorem which means multiply this
dashed line
with the straight line so prior times
likelihood and then
normalized by the evidence so what's the
evidence the evidence is the integral
over the product of these two functions
well okay so the prior is a uniform func
is a unit function so it doesn't do
anything
so the evidence is just the integral
over this
dashed red line from zero to one well
what's that integral
well that's again something you can
think about for yourself maybe
if you want stop the video
it's obviously one half right of course
because there is a rectangle here of
size one
and we've just drawn a straight line
through it dividing it in half so
of course it's one half right so we get
to
um multiply this by a factor of two so
that's one over one half
and we're left with that with the black
line that's our posterior density
function
great okay that's our posterior okay
that was patient inference done right
now of course there's not just one
person
in the in the in the audience right
now let's say i go and meet another
person
sitting right next to them and they are
not wearing glasses
okay so now how do i do bayesian
inference well actually
i already have a posterior from the
previous observation that posterior
is this probability density function so
the likelihood to observe the next
person varying or not wearing glasses
has nothing to do with the first person
other than the probability to observe
someone
wearing glasses so
you can already think about conditional
independence we'll talk about that in a
moment
so that means i can just multiply that
prior
which is my previous posterior right so
the posterior from from just before
is now my prior which i multiply with
the likelihood for this observation
to observe someone who's not wearing
glasses and we realized
before that that likelihood is one minus
that probability
so what i've done here is i've now
multiplied this prior density function
with this likelihood so that's just 1
minus pi
and then
divided by the normalization constant so
the normalization constant is the
integral over the product of these two
functions
these this product of these two
functions is some is some kind of um
parabola like this it's actually a
parabola
and
is a little bit further down i've just
re-normalized that as a probability
distribution
computing this integral under this like
this this area under this quadratic
function
is of course that ever so slightly
non-trivial problem but of course
i'm sure you can do that for yourself
maybe even in your head right
integrating a quadratic function is not
that hard
now let's say i make one more
observation and the next prairie person
is also not wearing glasses
what do i do now i hope that by now
you've understood what's going on
this is now our prior after two
observations
i get a third observation and i just
multiply with the likelihood for that
third observation
again that likelihood is again 1
minus pi because i've assumed i see
someone who's not wearing glasses
and this is now the posterior
distribution
the integral now gets a little bit
harder because we're now integrating
this complicated polynomial
um but let's just say we can do that i
mean we have a computer right computers
can do cool things so
um let's do that and now we're beginning
to do machine learning
right we let our computer do the
integration for us
and it's constructing a posterior
distribution for us now let's say the
next person is also not wearing glasses
i just keep keep multiplying in
likelihoods then there's a person who is
wearing glasses
and another one wearing glasses and one
wearing not wearing glasses and another
one wearing glasses and here's a person
wearing glasses and here's someone not
wearing glasses and another one not
wearing glasses another one not wearing
glasses and
someone wearing glasses and so on and so
on and so on
usually i actually do this by going
through like a significant part of the
lecture hall until we have something
like you know
20 people or so um that we've seen
and then the distribution looks
something like this so what you've just
seen here what you're currently seeing
here is
the most recent observation is someone
who is wearing glasses
and that was the prior before the
observation and this is the posterior
after this observation
and what this now tells us is we've
learned something we know
that that the probability to wear
glasses is definitely not zero and
definitely not one and it's also quite
unlikely to be 90 or 80
or 10 or 20 just because of the ratio of
people we've seen wearing glasses are
not wearing glasses
but after 21 people we're still not sure
right we really don't know what it is
because it might be any number
between let's say twenty percent and
seventy percent i mean of course it
could be any number between zero and one
other than zero and one it can't be zero
one because otherwise we couldn't have
seen both positive and negative cases
but those are now very unlikely and if
you keep doing this
then over time this distribution will
concentrate
around a value which actually tells us
what the probability is to wear glasses
now i know that people will have many
questions about this they always have
and that's great let's talk about it in
the flipped classroom
please if you're confused by this write
down your questions and we will talk
about it
i have to tell you where this experiment
comes from
it's one of the oldest questions of
statistics of course
right and it was actually discussed long
before statistics were a thing
it goes back and for this i need to
switch back to slides
to this wonderful man who you've seen
before
he's called piersimo de la place i'm
actually not sure whether he was already
a maki by the time that he wrote this
but this this text
and he wrote about this experiment in
his famous book theory analytic the
probability
now if you can read french here is his
real
citation normally i would do a simple a
silly joke in the in the lecture hall
and ask you whether someone can
translate this for us
and then when you're laboriously trying
to to drag out your high school level
french and try to understand what he's
saying
i say don't worry about it by now we
have cool machine learning technologies
that can translate
texts for us so we don't need you
anymore and here is the english
translation
as created by a deep network
the probability oops
the probability of most simple events is
unknown
laplace writes considering it a priori
which is what we just did
it seems susceptible to all values
between zero and unity right
we just put it a uniform prior between
zero and one but if one has observed a
result composed of several of these
events
the way they enter them makes some of
these values
more probable than others the posterior
starts contracting
until we get an a posteriorly
concentrated distribution
thus as the observed results are
composed by the development of simple
events
their real possibility becomes more and
more known
and it becomes more and more probable
that it falls within limits that
constantly tighten
would end up coinciding with a number of
simple events
became infinite if the number of simple
events became infinite
now obviously laplace writing in 1814
didn't have access yet to the wonderful
compact mathematical notation that we
use today
but he was thinking along the exact same
lines
so what we're going to do now is we're
going to
go through this exercise again more
slowly
to think about how this fits into our
cooking recipe for bayesian inference
for that let me just briefly switch
something
all right so let's go through this
experiment once more but a bit more
mathematically not staring at a demo
but trying to think about what just
happened
so um we've actually followed the
cooking recipe that i outlined in the
last lecture remember
i quoted david mckay always write down
the probability of everything
so here's our cooking recipe again how
do we build
a probabilistic machine learning method
it's actually i mean you could call it a
probabilistic inference scheme
but that's what it is it's a machine
learning method all
good learning machines are probabilistic
inference schemes
it's just sometimes hard to notice that
they are
so we're going to start by defining our
probability space
and now that we've talked about random
variables we can actually talk about
random variables
rather than this complicated sigma
algebra notion
so you you might notice that in many
texts people just define random
variables and that's because
random variables are a function and
because
even the most basic functions can be
thought of as being functions from an
even more basic
space and maybe even the same space
mapping onto itself
it's enough to just talk about random
variables as the objects of interest
here we have two kinds of random
variables one
of them is the probability to wear
glasses that's a real number
and the other ones are and the real
number lies between zero and one
sorry including zero and one and we have
the observations let's say there are
five of them or n
of them right and then actually n is
another parameter of this model
and the x i are the individual
observations the random variables
then um oh yeah i just told you this
right so these are binary variables that
lie
between zero and one and now we can draw
a graphical model if you like
right so this is actually the beginning
of step number two so
the first step is to write down or call
it well you could say the sigma algebra
to find the probability space or also
just
say what the random variables are and
now we can start thinking about
how the situation is going to look like
in terms of conditional independence
so we'll draw a picture like this which
says so there is this unknown variable
which is the probability to observe um
someone wearing glasses
and it's generating all the other ones
so we can draw from this probability
over and over again independently
for each of these observations and
identically
because each of these these variables is
drawn with the same probability
so these variables down here x1 to x5
will be said to be identically and
independently distributed iid
and so now comes a little bit of
simplification in the notation
so formally we would have to say there
is this random variable called y
a random variable being a function and
it takes values pi between zero and one
but we're just going to be talking about
pi because that's the value we actually
care about right so the function doesn't
matter so much is the value of that
function and the same for the capital x
i the random variables we'll just write
the lowercase x
and actually this is the last time
they're going to be so formal
in the future i will always just write
real numbers
for the values and i'm often not even
going to be talking about random
variables at all
just values to which you assign
probability density functions
then um we need for our generative model
we need a prior and a likelihood
together they make a joint distribution
so p of pi times p of x i given pi
gives a p of x i and pi comma pi
that's all we need once we have a
probability distribution everything else
is just mechanical bayesian inference
so to do bayesian difference we start
with a prior
you could think of this as the
probability for pi given that you know
nothing if you find priors weird and
then
um compute the probability to where for
someone to wear glasses after the first
observation
for that you multiply the prior by the
likelihood and you divide by the
evidence
bayes theorem we
are going to i'm going to to sort of to
drive home this point
of the structure of this equation i'm
going to call
this integral down here z1 that's a
normalization constant
i'm doing this to say that this down
here
is if if you're thinking about pi which
we do
then this is just a number just a real
number right it's an integral
over pi so it doesn't depend on pi
anymore once we've integrated it out
it's just a number so we might as well
write it in front
the interesting bits all the structure
is in this stuff behind it
the likelihood times the prior so
that's our posterior and now imagine we
see a second observation
so in the demo i said oh so now the
posterior becomes the prior and i just
get one more like you hold and we get up
and we get a posterior again why am i
allowed to do this
you might have thought for yourself
that's maybe one of the many questions
that come up
well i'm allowed to do this because of
the structure of this generative model
because the samples are iid so let's say
i see a second observation then
um so now we have we have seen two
numbers x1 and x2
so the um the proper mechanics of
probabilistic inference tell us that to
get
this posterior we need to use bayes
theorem and write down the joint
distribution of all three variables pi
x1
and x2 and then the normalization
constant so z2 you can think about this
for yourself what this is but it's an
integral
over pi over this expression down here
or up here maybe so um
what i've done here is i've already
factorized this distribution so of
course i could have written here p
of pi comma x 1 comma x 2 and then
because of the product rule
i'm allowed to rewrite this most
generally as
x 2 given x given x 1 and pi times x 1
given pi times
pi p of pi all right of course i could
permute
all the variables in here
if i wanted to because the product rule
is generally true for all permutations
of the variables
however this particular factorization is
useful
because i can now use the assumptions of
um of iidness of independence given pi
the graphical model on the previous
slide encoded a conditional independent
structure it's one of these fan
out elementary structures right um which
say that
when conditioned on pi then x2 is
actually independent of x1
so i can get rid of this x1 and this
expression and just write it like this
and now notice that this here before
up to the normalization which i've
observed absorbed into this real number
is actually just the previous posterior
times a new likelihood so normalization
doesn't matter
because after the after the inference
step we will always have a probability
distribution
even if so even if this thing that we
integrated over here isn't
a a joint probability distribution as
long as
it's a non-negative function that
integrates to a finite number
afterwards we'll have we're going to
have a normalized probability
distribution
so that's why we get to do our um
yesterday's posterior is today's prior
kind of thing
so once i've done that with more
variables i
will get more and more of these terms in
here and there will be all of these
individual observations
showing up in a product and of course if
i had n of these observations
then that would just be n of these terms
in this product
multiplied by a prior normalized and we
can do the normalization in every single
step as i did in the demo
or we can do it at the end it doesn't
really matter as long as we only start
talking about probability
distributions once they're normalized
everything's fine
so that was step number two step number
one was
let's define our generative model step
number two is now let's think about the
structure of this model a little bit to
understand how expensive it's going to
be to do the inference
now the third step is so far all of
these
are it's just abstract nonsense right
i've just written down some symbols
but these functions don't actually have
a shape yet so then i ask you so what is
the probability for someone to wear
glasses
given that you've observed someone where
glasses and we started thinking about
the structure of the prior and the
likelihood and actually assign values to
them
notice that what's happening there is
that we are
now imposing our domain knowledge or you
could also say our assumptions
onto the model so at this point this is
where the philosophical debate can start
up until now we've just done mathematics
and everything is just axioms
well okay you could you could of course
debate the conditional independence as
well and i'm sure we will
in the flipped classroom so maybe that's
part of the philosophy as well but
at some point philosophy comes or
philosophy
yeah at some point our our everyday
knowledge or our mathematical
assumptions about the process come in
and those are the ones that can be
debated
probability theory cannot be debated
it's just a set of axioms the
assumptions we make in a concrete
experiment
can be questioned so i said the
probability observed to observe someone
who wear glasses given that they wear
glasses
sorry no given the probability for
someone to wear glasses given
um now the probability to observe
someone wearing glasses given that
the probability with two where classes
is pi is pi and one minus pi the other
way around
that's actually an assumption and you
can think for yourself about
whether you want to question this or not
and maybe we'll talk about it in the
flipped classroom
and um oh yeah so now for for
convenience
i will let's introduce two new variables
we could call them random variables
um but there are there's things that
we're going to know so it doesn't matter
so much that they are random
we will actually know how many
observations we've made
let's um say um we say capital n for the
number of people with glasses we've seen
and capital m for a number of people
without classes we have seen
and they take values little m and little
m
and um we're going to use that to
simplify our notation because after
we've seen lots of observations we don't
want to plug in the individual terms we
want to somehow
condense them and we are allowed to do
this because we've defined the notion of
a random variable
so having made n uh
five observations n of which were
positive
and m of which were negative our
posterior is going to have the following
structure that will be the prior that we
haven't talked about yet so we'll have
to do that in a moment
in the demo i did that first but let's
say we leave that to the end
and here the likelihood will now take
the form
that there's only two kinds of terms in
here either it was a positive
positive observation then there's a pi
in here or it was a negative observation
then it's 1 minus pi in here
so we can get rid of the product symbol
and instead write um
pi to the n times 1 minus pi to the m
times the prior that's what our
posterior is going to be
and now we're left with only two
problems we have to define what a prior
is
and we have to get this normalization
constant
so this is now where people who
criticize probability theory might come
or bayesianists might come in and say oh
but now you're prior the prior is the
big problem right
how are you going to do with the prior
so here is how the plus deals with the
prior it makes some
a very interesting observation so what
we will need
to do bayesian inference is that we need
to compute
this normalization constant right we
need to integrate
over this function
now notice that um
well in the index in the experiment
actually that is still a simple thing i
said
let's just say that prior is uniform
it's just one right
then um we need to in the end solve an
integral that is something like the
integral
over pi to the n times one minus pi to
the m
now here's a super smart observation
laplace
which is hmm okay so to solve this
problem i need to be able to solve such
integrals
now notice that the unit function is
actually also of this form
it's just pi to the 0 times 1 minus pi
sorry
pi to the 1 times 1 minus pi to the
first power right because then it's just
pi times 1
minus pi
which is 1. so okay um
no i'm sorry that was wrong okay short
correction
it's just pi to the zero times one minus
pi to the zero which is one times one
which is one
yeah that's how this works okay good so
actually since we will have to solve
this
integral which actually was a huge
problem for laplace
let's talk about that in the flipped
classroom um but
since i have to solve this kind of
integral anyway i could actually
consider
any prior that is of this form so any
priority can be written like this
where we've introduced a minus one for
convenience so then um
this is uh that's why i was confused
just now right so if you set
a and b to one then you get our unit
prior
it looks like this right because
then if you have a prior that is of this
form then if you multiply it with the
likelihood
we'll still have to solve an integral
that is of this
general form so actually we can be
more general in our definition of a
prior than using a uniform prior
we can do we can we can choose priors
that are of this
general form because then we have to
still before and after always just have
to integrate this kind of function
and this thing has a name it's called
the beta function and
laplace actually couldn't do this
integral so when he did his computations
he actually used an approximation
and i think this is a fun thing to
discuss in the flipped classroom so
we'll talk about that then
and this integral was solved by leonardo
euler a little bit after the class
so um this is called the beta integral
and the
um so what this means is that
we can think of a prior as arising from
past observations and if you use a
uniform prior
that's a little bit like assuming that
we have no
prior observations at least that was
laplace's argument which is also maybe
where this minus one comes from
he in fact actually writes that so here
he is again
saying this um oh yeah it's in french
again sorry i can translate it into
german into english
so um he's writing that and again keep
in mind
it's 1814 this poor guy doesn't have
access to proper mathematical notation
yet
and all the concepts that we now use to
simplify stuff he doesn't even know what
a random variable is
so he has to write this very complicated
sentence
when the values of x considered
independently of the observed result
are not equally possible if we name
z the function of x which expresses
their probability
that's our prior it's easy to see
by what has been said in the first
chapter of this book so by what we've
just done basically
that by changing in formula one which is
what i had in the previous slide
y in y times that we will have the
probability that the value of x is
within the limits
between pi and pi prime this amounts to
assuming all the values of x equally
possible a priori
and to considering the observed results
at being first but as being formed by
two independent results
whose probabilities are y and z so he's
saying
to um we can actually consider
a more complicated prior and then our
data and think of the resulting
posterior as
resulting from two different data sets
some up a priori observations
multiplied with the with the likelihood
for the new observations that we
collected
these are priori observations are today
often called pseudo observations
we can thus reduce laplace writes all
the cases to the one where you assume a
priori before the event
and equal possibility to the different
values of x and by this reason
we will adopt this hypothesis and what
follows so he says if
you you already have some other prior
knowledge then you can include that
as if it were a data set rather than
actually calling it a prior
this is called this this sort of
algebraic structure we've used here
has a name and i'm just gonna tell you
about it now
and we will come back to it much later
in the course it's a really beautiful
concept
what we've just done here is we've
constructed something that's called a
conjugate prior
why have we done that and this is often
something people ask
in the lecture it's because it makes the
computation easy
and of course this might seem really
fishy to you
because if the point of probabilistic
reasoning is to express
everything we know and then do
mechanical inference and then not never
to question the mechanical process
anymore
it seems a bit dodgy that we're doing
this
uh we're sort of fiddling with the prior
to simplify this computation
and maybe it is we should talk about
that in the flipped classroom please
come with your questions
however there is um i mean the simple
answer is
think for yourself about or maybe try it
out with your machine
um what kind of shapes of priors you can
address
by using this kind of prior distribution
so basically
change a and b make a plot of this
function
set a to some anything from 0 or just
above 0
to a large number and b and see what
kind of shapes you can create
and if you think that that's an
interesting language in which to encode
prior information
then you've already bought the argument
and if you think there are some kinds of
priors that you can't encode with this
then think for yourself about what you
can do today
using your cool computer which
laboratory couldn't do
in 1814 to replace this prior
with something else okay with that we're
at the end
let me briefly summarize random
variables allow us to define derived
quantities
from atomic events parallel sigma
algebras can be defined in all
topological spaces allowing us to define
probabilities if the elementary space is
continuous
and probability density functions
distribute probability across
continuous domains pdfs are actually the
objects of interest
for anything real valued or continuous
valued
because they satisfy the rules of
probability we can
basically treat them as if they are
probabilities the only problem is
that we when we transform variables
in any way in particular also in a
non-linear way then they transform
non-trivially and we have to be careful
about these transformations but as long
as we don't transform our variables
everything's fine that was the tedious
mathematical part
of this of this lecture we had to do it
but
now we have all of our tools available
and we can start doing some actual
computations
with real numbers actual experiment and
think about how we do this with
computers
and we don't have to follow in the
footsteps of 1814
anymore and i'm hoping we'll see each
other again
in lecture four thanks for your time