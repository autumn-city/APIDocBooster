However, RMSProp does not keep a moving average of the gradient.  But it can maintain a momentum, like MomentumOptimizer.
 So, the algorithm  that we will talk about is what is RMSProp   which tries to address this problem  of Adagrad algorithm that is vanishing   learning rate as the time increases  as the number of iteration proceeds.  So, what is this RMSProp algorithm does  is instead of taking the accumulative sum   of squares of the gradients of the sum of the  squares of the past and gradients or this past   gradient starts from time t equal to 0. 
 So in   case of RMSProp, the scaling factor is not  the cumulative sum of gradient histories,   but it is the exponentially decaying  average of the squared gradients.  So, if I go to the updation algorithm is in  RMSProb, the updation algorithm will be like this;   you will find that you will compute the  gradient in the same way as we have done in   case of Adagrad, right. 
We are using gradient descent to calculate the gradient and then update the weights by backpropagation.  There are plenty optimizers, like the ones you mention and many more.
 So, your  basically the operation that was done in Adagrad   algorithm is r t, the scaling factor which is  1 upon square root of epsilon plus r t i.  So,   if you go for component wise this r t i is  nothing, but sum of g t i square of this   or let me put it as g tau square instead of  g t g tau square and you take the summation   of tau is equal to say 1 to t. So, you find  that this being a square term and which you   are going on adding.  So, r t goes on increasing,  it monotonically increases , it does not reduce. 
