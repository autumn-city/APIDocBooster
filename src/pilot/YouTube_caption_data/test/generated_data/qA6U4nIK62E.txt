good morning to everyone
today we are gonna see graph auto
encoders and
graph variational auto encoders and
first i'm gonna introduce you uh the
idea of auto encoders
then we will move to graph auto encoders
also known as
guy the theory then we will go to
some practice later i will show you
variational auto encoders the general
idea
and then we will move to uh variational
graph encoders
and some practice and a simple example
using pytorch geometric
also i want to show you tensorboard that
is a tool
really really useful in order to compare
models and you can do a lot of stuff but
we will see later
let's start okay what does a deep neural
network do
well uh the the real thing that
a deep neural network do it is that it
learns
important feature from the input what
does
important feature means features that
allow to do a specific
task on the data like classification
regression
whatever you want but the idea is that a
neural network
learn important feature to do some tasks
so the question is can you
can we compress our input data
like for instance suppose you have a
bunch of document
and you want to compress this document
in a low dimensional vector
or you have images and you and
you want to compress this this
images in a vector in a low dimensional
vector
yes we can do it with an auto encoder
the first thing i want to say is that
autoencoders
are neural networks that work in an
unsupervised manner what does it mean
it means that we don't need label it
data
so for instance if we have these images
of cat
and dogs we don't need to know a priori
which are cut
and which are dogs and it's quite
uh useful because uh it's easier to have
unsupervised data instead of supervised
data
labeled data that is how can
they work without any labeled data
well the idea is that our
input in this case is an image
goes inside the auto encoder and the
auto encoder
try to reconstruct our input
so as you can see the input here is a
dog
and the output is still a dog is
just ruined let me say
but this one is the idea okay how this
auto encoder is
done well is a neural network as
we said we have an encoder that take
takes in input our input our data our
image
document whatever you will whatever we
want
the uh information is encoded
in a small dimension then we have a
decoder
at the end and in the middle we have our
embedding that is an representation
of our data in a latent space
of a small dimension okay
and uh how it works i i just said we
have an input the input goes in the
encoder the encoder produce
an embedding like in this case one and
five for instance
and then the decoder try to reconstruct
the input
okay so uh how can we
train this model well we want to have
this some kind of similarity
between the input image and the output
image
and we want to maximize in this case the
similarity
between the input and the output
in other words we could want to minimize
the reconstruction error okay
okay how can we use a note encoder well
we can use it just to include
an image and then we can use the
representation
to do whatever we want instead of in the
domain of images
document but we can do in a real domain
of small dimension like we can do
clustering
visualization whatever we want but in
another domain
okay let's see how
uh they works first
oh sorry sorry uh okay we have done
and we can now move to uh
graph out encoders okay that we i think
we have understood the idea
of take the input and compress it
and then they call it to obtain uh this
uh embedding in a lot in space
let's talk about the graph suppose we
have
the structure is the same we have an
encoder an embedding
and a decoder okay let's
see uh as as you know uh
from the previous lecture we cannot use
a simple
convolution on graphs but we have to use
a graphical
neural network in fact our encoder
is one convolutional graph neural
networks
that produce a low dimensional embedding
representation of the input graph
let's see how it's defined
this we have a graph convolutional
network
that takes in input a the other
synthesis matrixes of the graph
and x x represents the
node feature of the mages and
if we apply a real activation function
we obtain this x bar
that is our representation of the graph
embedding
and as you can see we
we are not using uh the normal audio
synthesis matrixes
but we are using and
normalize at the synthesis matrix as you
can see we have the
inverse of the diagonal matrices
multiplied by a
and then multiplied again for the
diagonal matrixes
up to invert and so on okay have a look
at the embedding space
the embedding space uh sorry the
embedding of the graph
is just the uh
the output of a graph convolutional
network
okay let's see
uh as low uh
uh as as let's say
as
[Music]
okay we have this embedding space that
is just the output of the
graph convolutional network okay
let's see how they work okay suppose we
have our graph
with this small graph with some
node features that goes into the
our encoder and we have this node
embedding
in two in a latin space with two
dimensions what does it mean here our
embedding is an embedding
for each node of the network okay
so uh how can we build the decoder okay
let me just break up we have our graph
the graph goes into the
decoder and the decode the encoder
produced
a node embedding for each node of the
graph
then what we want we want to rebuild our
input graph
okay and here we are
how we we can recon reconstruct
the input well the first paper
that introduced graph out encoders
use the inner product between latent
variable z
what does it mean within a sigmoid
function
but what does it mean that the idea
sentences metrics is
the reconstructed other synthesis
matrixes
in position a and b is the sigmoid
function
of the product of the embeddings
of a and b and so on for each
of the nodes and here just
to show you what we have we have
everything we have our our
one convolutional neuron network to do
one embedding
we have our embedding that is just the
asp the output of
our graph convolutional network and then
we have
the inner product uh within a logistic
sigmoid
uh that reconstruct a the idea synthesis
matrixes
of our graph okay
go to the practice aside well
in order to talk about the gra the the
geometric
already implement the graph auto encoder
model
and it came from the same paper
i'm i i'm presenting here
it is uh okay simply what
how this model works we call this model
we give as an input an encoder and a
decoder
okay and the decoder by default
is uh is the
by default is the inner product
so we don't we we
we don't have to do so much
let's see this in practice
uh i didn't ask is is everything clear
so far or it's okay
yeah i think it was very clear perfect
thank you
okay let's start you can
download the both slides and this
notebook from the official website but
let's see how it works here you can find
the link to the paper
and the code the code that i'm using
came from the official
pytorch geometric repository and
it's just copy and paste and i'm here to
show you
the part okay first of all we are going
to download the data set
as you can see uh from
there we are done using site sir that is
a data set that is downloaded
and then as i already told you in the
previous section
is interesting because if i rerun this
uh
line we don't have to re-download the
data because
it has already stored in this folder
that i specified here
this name here okay uh have a look at
the data
we have an e a an edge index of
two of size uh two by nine
thousand so far we have a test mask
training mask and validation mask
because here
we are in uh we have a
unique graph okay and
we are doing the operation in a unique
graph so
we have just one graph we also have
x that that is our feature matrix
and in particular for each node we have
307 not 3703
features okay and also we have the
labels
okay we don't need the labels because
it's a we are using this data set in a
unsupervised
task so what we do first of all we
reset the train mask the validation mask
and the test
mask okay and
if we have a look at the data set now we
have
the same edge index we have x and y
actually we don't use y it's just here
but we don't use it
then what we do we call this function
that came
from pytorch geometric
uh uh utils
what does this function do well the
these functions
just split the edges of the data
producing a new data set actually the
same data set
but we have negative edges
positive edges for test for
train and for validation okay
uh what okay is it's obviously
that uh test and train and validation
we know what the what they
are used for why we have this negative
and positive well the negative are
no uh are edges that are not in graph
while the positive are edge that are
present
in the graph let me just say uh only
this
thing just to have an idea okay
now we have to define our uh the encoder
okay how the encoder is in this case we
have
this really really simple encoder
we just define the name of the class so
we want
we interacted by the torch and then
module we in assess initially
the superclass okay and then what we
have
we have two convolution the first
is a graph convolutional network
with as the input channel our input
channel
the number of feature then we have
an output channel with two multiplied by
so because we have two convolutional
neural network we goes from the input
feature
to the double of the output feature and
then
with the second convolution we go from
the double of the output feature
to the output the real output feature
the embedding we want
the size of the embedding we want to
produce okay
then we also have this cached equal
through
this can use only on transductive
learning so in our case because what
it does it caches the normalization of
the diasynthesis matrixes
so for instance if if we would have
many graphs we it won't
be useful to store this information but
in this case
in which we have only one graph is it
useful
okay and then we have the forward method
that it's just called the convolution
to x and the edge index apply a relu
okay and then i return the second
convolution
it's quite simple okay
now what we have we just called
our guy graph auto encoder model
and we define our model simply
calling guy and passing our encoder
okay the only uh the decoder as we said
previously is by default the inner
product
we specify previously i specified the
out channel
so the embedding in two dimension number
of feature
is taken from the dataset object
and the number of apex then what i do
i just move the data
and the model to the cpu if uh to the
gpu
if is available available in my case is
available so
both model data and edges are
moved to the device so to to the gpu
and the last thing i'm
is i'm run i'm uh
[Music]
initializing the optimizer easy
then i have these two functions the
train and the test
what does the train do well first of all
it puts
the the the the model in a training mode
i put the optimizer in in zero grad
i encoded the model so as you can see
uh well let me show you
if we have a look to our models so just
model you can see we have the encoder
and the decoder so what i'm doing below
i'm just saying model dot encode
and i pass the information to encode the
data
okay then i compute this loss
that is a reconstruct loss loss
and this uh function is already gave me
from the gaia model okay in fact if i do
uh it should work
i should have also this reconstruction
loss recon loss okay
so this one is a function given from the
the implementation by torch
then what they do just backward on the
loss
and a new and a a step in the optimizer
okay then
the test what does the test function
test function does
well we put the model in the evaluation
we set george nograd
okay and we encode
our positive edges
our training data and we test the
training data
with the positive and negative edges of
test in fact i in the
below when i'm calling the test
i'm passing the test positive add edge
index
at index and the test negative edge
index okay
it's quite easy as you can see
i'm running a 100 ebooks and
we have done okay
so how we can use we just
want to i don't know we want to see the
encoding in two dimension
and here we are we have encoded our
input in these information
and it's it's really done what we have
to do
but i have a question for you as you can
see
the model returns out that is uh
the curve under the rock
so just to show you is this one we have
the
true positive and the false positive and
the rock
curve and the area above
the rock curve is defined as the
out and then we have the average
precision but if you have a look to
these results at least to me is complex
to understand and
keep looking at each values and so on so
forth
for this reason we can use
tensorboard tensorboard is
really a useful tool
let me say so how we can use it
first of all we have to import
tensorboard
and then here i'm just cut and pasted
what i did previously it's just cut and
pasted them
really and then if you don't have
tensorboard installed you can just
install it
throat pip so pip install tensorboard
you have to specify a folder in which
you
want to write your experiment
results like for instance i call it
guy experiment should i mention 100
ebooks
because we have set the out channel
equal to two
after that i can run my model
so what's next next i can
move in the folder in which i have run
the experiment and in which i stored
this uh runs guy experiment
blah blah and i can open
a terminal and just digit the
tensorboard
minus minus log here equal
runs
okay now if i'm connecting to this uh
web app as you can see here i can see
the uh average precision in train
and the accuracy in train okay
or test or whatever i want and i can
zoom in uh in a portion
i can see bigger i can do a lot of stuff
but why is it so important to have
this kind of picture suppose that we
realize that two dimension are too few
to embed our data our nodes
so what i can do is just instead of two
right
20 rerun our model maybe
changing the name and instead of two
dimension right
20 dimension run it
run the training and then what happened
if i update the page you can see that
a new model appear guy experiment 20
dimension that is the blue one
and here you can compare the model okay
how the model performs changing
parameters of the model
and i i think it's really really cool to
see because we can see that the model
with 20 dimension has
really a huge improvement with respect
to two dimension well
it was kind of obvious let me say
but uh i don't know maybe looking at
this image
i can think uh
i can think maybe i have i can try
more e-books for instance so i can just
i don't know 1000 e-books
change the name to the model and run
again
and here again if i update the image
there is a new model that is the
red one and
okay
okay as you can see
the model tends to stabilize
to i don't know what is 0.88
okay and however the main idea of
of this tool is to visualize results
okay and i think it's practical
if we want to visualize the the output
of our models
you can do a lot more with this tool
like you can
build the computational graph you if you
have images you can visualize
the images it's
really really a cool
instrument but uh as everything
i want just to introduce you and then
uh last thing i want to say is that
there is this parameter that allows you
to make the function
more smooth or less smooth
if we want to see i don't know the tails
or i don't know
or we can see the trend of the function
increasing the smoothness of the of our
results
okay
it's done we did the
graph auto encoder okay
let's move to variational graph auto
encoder
what okay
okay variation of the encoders
okay so far what we see
we see that what we have seen we have
seen that if we have
images graphs whatever you want without
encoders
you can embed it in a specific point
okay what happened with the
variational encoders well instead of
moving an image in a specific
point we move it in a
multivariate gaussian distribution
what does it mean it means that we don't
have
the the specific point like
these two q5 coordinates but we have a
mu
and a sigma square okay
how it works again we have the encoder
but instead of producing an embedding in
a latin
space we have an um
[Music]
the parameter of a multivariate
gravitation distribution
okay let's go into details
what we want to do suppose we have x
that x is our input and z
that is the uh parameters of our
multivariate gaussian okay
and and this one is our encoder
the decoder is the opposite we
which is the probability of x given
z okay if we
uh open it so if we open
with bias we can see that the
probability of
x given z is equal to the uh
probability of z giving x
times probability of x divided by
the probability of z okay
where the probability of z is the
multivariate
revolution could the x is the
distribution of our
of the input and could it said
given x is something similar to our
encoder
okay so what we want
we want to build something we want to
that
q uh p d z given q
p d that given uh p did that given x
and q did that given x
will be as much similar as possible
and what we can do here is
called kl divergences
that measure measure the distance
between distribution
in particular we want to minimize
the kl divergency okay
is it done nope
because we cannot compute q
of x given z that given x
and moreover we want the reconstruction
and
our semi previous similarity
however it's it has been pro
proven from the auto outers of
this work that this function here
uh can be applied and which is the
difference that
the kl divergences is on p
z given x and q of
z and moreover we have this another term
okay and interestingly if we have a look
on the specific term we can see that the
first term
is a variational lower bound practically
is uh the reconstruction error how well
the network
is able to reconstruct reconstruct
our input okay and
the second part is the regularizer
in which we keep the distribution of
as much similar as possible okay
and we can use this loss here to train
our model
so uh this one is the model
we have our encoder instead of having
the
uh random embedding
we have this uh embedding in a
in a multivariate value gaussian
and then we random sample from this
multivariate
to generate something new okay
which is the problem back propagation
doesn't work
if we use random sampling but here we
have a trick
called the reparameterization trick how
it works
well zed is not
a random sampling but z is given by mu
plus sigma pairwise multiplication
of epsilon in which epsilon is a
random sample normalized normal sample
with mean
zero and sigma equal to one okay
and this trick is called the
reparameterization trick
and it allows you to do back propagation
uh can you see the right image so
we can do back propagation on each
sample
and without taking consideration
epsilon our random sample in this way we
can skip the problem
of that propagation okay
move to graphs because what i did so far
was the
an introduction to variational
autoencoders in the general framework
we have to move to graphs and what's
changed again we have our encoder
our latin space and our decoder
how the encoder works he has two
combinational
neural networks so instead of one we
have two convolutional neural networks
the first one is the same
we used before so the graph convolution
with a reload activation function and
with the uh
normalization of the idea synthesis
matrixes
and then we have a second graph neural
network
that generates mu and log
of sigma square so we have this graph
convolutional network mu
that takes in input x and a and it has
this
learning parameter w one okay
and then we have log of of sigma
square why we are using log
because in this way
we don't have the problem of negative
numbers
because the graph neural network can
produce a negative
number but with the logarithm
we put it in a positive way
okay uh
and again is a graph convolutional
network
uh with the same learning parameter wr1
to put together
we have this graph conclusion
network here that is the idea synthesis
matrix is
uh normalized multiplied by the real
activation function
of the uh normalized synthesis matrix is
multiplied by the input vector input
feature vector
and w0 and then we end w1
if you can see the the inner is the
first graphical emotional network
and the second the the the wall equation
is the second convolutional neural
network
and here we are then what we have
our proper marker parametrization trick
so mu plus sigma pairwise multiplication
of epsilon that is normally distributed
and then we have our decoder that is
identical to the previous one so the
inner product
between latin variable of that
and again to a sigmoid function that
tells
us if there is that or there is not the
edge among
that nodes okay move to the practice
what's changed well
nothing let me say instead of
having our graph convolutional network
encoder
here we have a variational graph
convolutional network encoder
let's see at the structure we again we
integrated
from torch and yen module
uh instantiate the superclass we have
the convolution one that takes in input
the input channel goes uh into
uh output channel doubled
output channel so okay so again these
previous tricks
in which we start from the number
feature
we reduce and then we go to the output
feature
okay then we have the convolution
mu and the convolution
log std and again we have
two multiplied the uh output feature
so the second neural network again uh
here and here okay
we have done the forward method in which
uh
first of all we use the first
convolution
the real activation function and then
what we
return is the output of
the we are returning two outputs
one is the convolution move and the
second
is the uh convolution of uh log
standard deviation okay
again as previous we can specify
the number of out channel the number
feature the number of e-books and
what's new nothing instead of
use a guy model from
torch python geometric uh
nn we import v guy variational graph out
encoder
and we instantiate the model passing our
encoder because again the decoder
is the inner product by default we move
the data to the
device so in my case again the cp the
gpu okay
and finally we initialize the optimizer
the train is
identical to the same the previous one
with the difference that the loss
is not only the number
of the reconstruction laws but is
it has been combined combined
with the kl loss kl divergences
okay oops so we just combine these
two kind of loss apply the backward and
the optimizer step and return the loss
the test is identical and the train
it's identical again let's just
run these lines
as you can see i i introduced this uh
i i i also use the tensorboard
here so that we can compare the results
in our case we are comparing two
dimension
100 features and again if i move to
tensorboard and i go down
i can see here variational graph auto
encoder
experiment two dimension 100 e-books
and i want to compare with the guy
experiment
two-dimension 100 e-books and done
it's done okay
maybe i have to decrease the smoothness
of the function
okay since here
seems that they uh our
guy model works better than
our variational graph encoder
but we can also try to i don't know uh
for instance
increase the number of ebooks
and it's too much let me just do
[Music]
uh i don't know 500
i didn't change the name
okay if we go again
here and suppose we want to compare
this model here
with i don't know something else okay
as you can see we are achieving a more
or less
the same uh in some sense
uh performance but it's it was just an
example to show you
uh how it easy to compare and you can
for instance zoom in or have them
have a more detail in some part uh
about fluctuation
it was just to show you uh the idea
of uh graphite encoders
and the power of uh
tensorboard okay
i think we have done uh
[Music]
okay are there any questions
um antonio uh i have
actually uh two questions yeah
uh the first one is uh correct me
if i'm wrong uh but um
so basically with uh graph auto encoders
you basically can use it only in a
transductive
setting why whereas with the variational
autoencoders
are you able also to use them uh in an
inductive setting
because since you're learning not the uh
the representation in the latent space
but you're
learning uh the distribution from which
to sample
is it true that maybe you can use it at
the variational
um auto encoders for
an inductive setting of graphs
why i well definitely you can use
variational autoencoder
in an inductive learning framework
but i'm asking why you think you cannot
use
the simple auto encoder in a inductive
uh
manner ah you said so that's right oh
sorry
it was my no no no no i'm i was saying
that
sorry it's my fault i was saying that
you can use
uh these only these so
yeah the cachet parameter
okay okay okay no no yeah you can
discussion parameter
is just to let me show you here that
it's quite
easier sorry
but probably i understood it wrongly no
maybe i i see something
okay it was just that
cache oh sorry
okay this parameter here cached
what does it do just instead of compute
every time the normalization it computes
once
because the input graph is just one okay
okay but that you can use uh
autoencoders in a inductive manner
okay okay okay um the second question uh
if you can go back to the slides sure
okay uh should be around
uh 77 uh
something like that
no here here here is one here
number 75 okay
okay so here um you're basically
learning this uh
multivariate motion yes um
so basically um again it's uh
unclear to me so basically uh here a mu
will be um
a vector right there yeah yeah
how do you go from having a matrix
because basically the
gcm produces a matrix of values right
which is a one vector for each
node yeah but
mu here is a
vector let me say the idea
is that
[Music]
the idea is that uh for each
uh from the encoder each uh the output
of the
commercial networks goes into
mu and it produces one mu
for each embedding space and one sigma
for each
is that you learn one mu for each
dimension
and one sigma for each dimension
um so for instance if the output
yes yes yes uh what i'm saying is that
uh
gcn outputs a matrix right
yeah one n times d n
is the number of nodes and d is the
dimension of the uh
of each node basically mu would have
basically dimension one d right and also
uh the the sigma yes uh
i see your point and uh
i was wondering if i mean it's just uh
actually you know
if the if you have any uh idea on how
you go from
uh having this matrix representation to
actually getting the
the menu you're right
you're right because uh uh you're
totally right
because here the output okay it's a
output channel but you are right one for
each node so
there is any some kind of uh
i mean maybe there is some pulling
operations
i i'm not aware of which kind of pooling
there is
but you are totally right yes
thank you for the presentation it was
very nice thanks
sorry yeah i totally missed a doubt yeah
but you are totally right
yes thank you and perfect
are there other questions