Thank you for being here on time I will have only 50 minutes today, so we are going to be trying to
Squeeze as much as I can without running too much. So there's gonna be like a small agreement between me and you
every time we start class so the agreement is the following I'm here for
Trying to communicate to you right so I'm here because I'd like to send a message across every time you don't have
Idea, what's going on? You think I'm confusing you cannot understand my
English you know, I have a very strong Italian accent, you know "mamma mia!". No, okay
Yeah, I make jokes, so don't take every word I say
as
Truthful if I'm talking about, you know
You know being aggressive or whatever. I'm joking. Most likely
If I'm talking about, you know deep networks and stuff. Usually it's reliable
Yeah
Okay, so the point is gonna be like every time you have no idea what's going on just stop me call me:
"Alf! I've no whatever idea was going on here. Stop, repeat, explain to me again"
I will explain to you as many times it's required
If you do not understand something it's almost
99.9% my fault because I haven't taken an account your background and you know, I'm skipping some steps perhaps because I'm not used to
You know, maybe this class or your background or whatsoever
So again, I'm here in order to educate in order to communicate and I can't do that if you don't help me, right?
So if I ask a question and you understand what I'm saying,
you should like nod your head because if you're like ...
Yeah not reacting to my jokes or questions
Then I have some issues understanding whether you are making any sense of what I'm saying
If you get distracted, I will try to
usually I throw chalks. We don't have a chalk here, right?
I think I can try with markers but
Maybe not right?
Just for an undergraduates
You're grownups. So you should be able to you know, keep yourself awake for the next
45 minutes more or less do not turn off because if you turn off
I can't come there and take you and shake you right again. You're not undergraduates anymore. You're grownups
Some of you are undergraduates, I guess so
Okay, I figure out where you are if I may come and shake you up
But for the others you should be able to stay awake for the next 50 minutes. I know it's like late night
so try not to eat before class so you don't have like *snore*
You know after dinner
thingy, whatever so
Yeah, so don't take me too seriously I'm Italian
Right so so so inspirational like why are we here?
To take an A maybe so it's a competitive class right at the end. We're gonna have a competition
Don't get too scared. We do not only
Like consider how you're doing well in the final ranking. We also consider,
How well is your work? Right so sometimes things don't quite work, but you still apply yourselves
So that's actually what matters nevertheless if you actually win the challenge you may have some
gifts and presents and something from us
Yep, that should be it
also, if you take a very nice grade, you may come and
you know join the lab perhaps with young and so on next semester, so
Just saying, you know take a nice grade so you can serve later as well. Okay. All right kind of joking
So neural networks, right? Did we see last time yesterday what neural networks are kind of know?
So I think we mentioned a few
applications one of those were classifications
So let me just give you a small recap about what classification is. So what is classification?
Let's say I take a picture of these of something right? Let's say I take a picture
if it is a one megapixel camera
So how many dimension will be my image? So if I take an image, I can consider my image which is gonna have
RGB plane and it's gonna be heavy like a thousand
Pixel in the vertical and one thousand pixels horizontally so you're gonna have overall 1 million pixels
So one megapixel you have RGB. So how many pixels how many values will you have?
3 million, right?
Ok, cool
So you can think about this image which can be thought as this kind of stack of three layers is one point
Living in this 3 million dimensional space and this 3 million dimensional space is really really really really large
right, you can move around and
Nothing happens, right? So let's say I take a picture of a dog and I have here a picture of a dog (rises right hand)
Let's say I take a picture of a cat
Where is gonna be a picture of a cat: here, here, or here (points far away, closer and next to right hand)
Sorry
Ok hands up for whose thinks, if this is my cat or dog
Who thinks the other one is here? (far away)
Ok
Maybe you were actually paying attention yesterday who were thinking who thinks the other point is gonna be here (closer). Okay
Who thinks the other point is gonna be here? (right next to each other)
Fantastic, ok, so you actually on the right?
So everything is just in the same damn little spot in the space (closes hand)
Ok and everything everything it actually makes sense is here. Everything else is just trash and so
We have to take the space go here take this point and move it here. How do you move point?
Have you taken linear algebra?
Yes, no
Ok about linear algebra
so as you may know or if you don't, now you do I do have Twitter and
I'm very annoying person on Twitter because I talk a lot
So, let's see Twitter. Ok open Twitter
all right, so
Here I cannot zoom so
twitter.com right
So then you go. Oh, there is no internet. See ha. Let's connect Wi-Fi
Yes, I'm connecting
All right, almost there
Okay, so you want to do a search and you do a linear algebra
You cannot see anything here. Okay. Damn. Let me zoom here
Okay, so you go top right, there is the search so you do linear algebra
Then you do parenthesis from me: "alfcnz", okay
So here's how you find out what are the questions for the midterm? Right? Just check on Twitter
I usually yeah leak stuff there. All right. So here you have these
Actually is very nice, this book
Introduction to linear algebra by Gilbert Strang so you may want to you know, look up this stuff
But the other one is this one here
So you'd like to check the third tweet here where I show you the essence of linear algebra by Grant
Maybe you know this guy Grant Sanders if you don't know, well, you should know now
Just check this link, right and this link is going to show you how you move stuff around. Okay?
Anyhow, it was the first link for today. So how do you move things around in a space?
For the ones that actually know how linear algebra works
You just talk it down raise your hands
Huh, okay matrix multiplication what does it do?
rotation
Cant hear. Okay first matrix multiplication is a what kind of variation
Linear transformation. Okay, what are the linear transformation? one:
rotation, two:
You sure
Stretching okay. Fantastic then, third:
Stretching, scaling, I guess let's see scaling/stretching is number two, then
If the determinant is negative, what do you have
(Alf turn around, showing its back)
What is it? reflection, third and then one more
Is it
Okay, I guess we have three for the moment this should be one more
Hold on so scaling right? So you have scaling this way (vertical). You have scaling in that way (horizontal)
Then you have rotation
Then you have reflection if there when you have reflection
No no no, identity does nothing, right?
So okay rotation means the matrix is orthonormal, right? cool second one you have scaling
When do you have scaling?
Okay
Well, I wish you in fantastic
So we have at the four, right? So we have rotation
we have zoom in this way you have shearing, right? and then you have the
Reflection whenever you have the negative determinant
Cool still I want you to move things around is motion, translation a linear operation?
No, yes, you can answer you don't have to rise hands
Adding scalars is adding scaler a linear operation
Hmm not sure
Because 0 where is the 0 mapped to, right? only if the 0 is mapped to 0 then you have a linear transformation. Anyhow
So you have translation which is the fourth one and a fifth operation if you use affine transformations
So all those nice things you're gonna be fine them there. Ok. So check those things are very useful
Anyhow, so finally we figure out that if I have everything up here
We have cat, hippopotamus, dogs and whatever. I want to take them down. I want to
Translate
Right. Yes talk to me. No, you don't talk yet. Ok. All right
So we want to use translation and then you translate this in the z-row
So you have all the things here then how do you separate those things apart?
So all of them are tall together, how can I (stretches space)
We mentioned what are the five different- four different transformation that a linear transformation does
Scaling right? How do you scale things with a
Diagonal matrix fantastic cool, right, so
Fantastic. So how do we diagonalize matrices? Ha that is the second question. You can figure out if you check Twitter
So we open a new page we go Twitter again
We go on top right should take a note about how to do this stuff. You do SVD parentheses from
"alfcnz". Okay, cool. Now you go if you go here is like or
SVD, so this is from also Gilbert, right? So you also like to check these on if you click here
It's gonna be more information about midterm right last year. Oh
Okay, you know too much now, okay. Anyhow, so you want to check this stuff out?
Anyhow, we'll figure out that by using matrices and scalars
You can move things around and then you can zoom right? And so again, I have things all collapsed here. I can't do anything
so I take it down with a
Translation, right? Yes, no? do like that with your head. No you following there?
Yes, no, okay, it's boring
No, okay
Alright, so you take this one, you put it down with translation and then you zoom it right with a matrix perhaps diagonal with some
Singular values right cool cool. Cool
And then what so I have this thing zoomed up though. So classification. What is it?
How do you perform classification now
You I'd like you'd like to assign labels, but how do you do that?
So whatever you would like to do is going to be basically moving these points in different regions
And then you slice them
Okay, and so this is problem is gonna be part of the next class
And now I'm just showing you how a neural network does this?
I hope you're gonna be enjoying this video
If I find my cursor, okay
So
All right, so we start from here
Full screen, perhaps. All right, so can you see anything? No, how do you turn off the lights? Can someone figure out how to oh
maybe here
Yes, no good night, hold on
All right, good night
so
At least then I'd turn it back
so here we start with these five branches of a spiral and you can see that each point is gonna be basically
Represented as what how can you represent each of those points without the color?
Okay, where do these points live
On a 2d plane, right? So this is a 2d plane this screen here. So each of those points is represented as a
As a tuple, right, that is representing the X and the Y coordinate, then the color there represents a third dimension
Where it's going to be basically
Representing the class to which each of those points belong and here we have five different branches of a spiral
So this is the input to my network is going to be a bunch of points without
Colors and then I ask my network to separate the points by color. Okay. Is it clear? What is the task I ask my network?
You can not rise your hands because I cannot see sh*t, right? *laughs*
So you have to shout back because I don't see you
understand right what's going on here if sorry not right now, so we start with this guy here, which is
Just five branches of a spiral the network doesn't see the colors right now and the network will try to separate the colors apart. So
This is what my network does
It takes the space the space space fabric and it performs like a stretching of the space fabric, right?
how cool does that sound and
You should do like oh, oh
See, okay. Yeah, we had to do multiple iterations. I think here this your first class. It's okay to be shy
Alright, so this is my network
Which is basically stretching the space fabric in order to get all those points that are belonging to the same color to the same class
To be in the same subspace of this final manifold such that
once we reach convergence, so whenever we reach at the end of this animation you
Have all your points that belongs to different spirals
Very separable
And so now you can just use logistic regression, right or like one versus all whatever regression is called
Yeah, I don't know. So I mean this is the last matrix is represented by that
That for five arrows there. So of those five arrows represent a matrix, which is how many rows how many columns
So what is my output dimension here of this network? What are we trying to infer in this case?
The classes how many classes do we have?
five, so the number of rows of this matrix will be
Five right because they are height of the matrix is they the same dimension where you shoot in two and the width of the matrix?
Represents the space where you're shooting from right?
And so what is going to be my width of this matrix?
Two, right?
so this matrix is going to be a high times two
because we are shooting towards five dimensions the five colors over here and then
And then we have two columns, right?
So it's gonna be basically the two coordinates of the tip of the arrow and those arrows are just centered in zero
We're going to talk about more about that intersection
I guess in later lessons if I actually may manage to make a video about that, but can you see that kind of cute?
intersection at the center
Have you ever seen that kind of intersection before?
Have you ever taken a bath?
I mean oh, yeah bubbles, right? So whenever you have multiple bubbles touching together, it looks like similar to that
Perhaps maybe not that I don't know. It looks to me
Alright, so that was the first Network
Okay
and this is how this network work these networks basically take the space fabric and then apply some kind of
Transformation which is still parametrized by matrices. So I have many many matrices
then I have
Nonlinearities, why do I need
Many matrices or why do I need nonlinearities? Can I turn on the light? Oh, you still like to watch? Okay
Yeah, otherwise you'll start sleeping
All right
So so
Here it basically this guy here is gonna be a network with two matrices my first matrix maps
My input which is living in which space?
Two dimensions to a intermediate layer of 100 dimensions
So my network architecture is the following so I have my two neurons here.
I met this one two, one two three, so on here, I have 100
Then I have some non-linearity
which is going to be just the positive part and
then from this guy here, I
Do something
Cute or funny or weird? I don't know depends. So from here I map down to
Okay, what should I do after let's say this is this is my network. What's gonna be the last layer of this network?
5 right?, but how many dimension has this screen?
So, how can I plot there? and don't say PCA
So I do this one
Okay
Such that I can show you there the linear interpolation
between this point here, which I call embedding in this case
but just a way I call this stuff right now
and my input
Okay
Alright that's pretty much it so this is my neural network has
one input layer one hidden layer
One kind of embedding layer, which does nothing and an output layer. Okay, so it has one hidden and overall
I count this as three layer neural network, because it is one two, and then this stuff is linear, right?
so why do we need nonlinearities and
What can one layer-, okay, he said:
Without non-linearity it would look like a single layer neural network and a single layer neural network what can it do?
Scaling,
Translation,
Rotation,
Reflection, and?
I guess, shearing, right? Okay, so guess what's going to be the next part of this class?
Let's let's check how one one linear one network with one layer works. Okay questions so far
Also there in the yeah
For just displaying stuff on the screen more questions again, don't raise hands you can just talk to me
Can you hear me the over there is everything okay? Awesome. Alright. Thank you for approval
So these two points here represent the coordinates of my input points, right
So this is gonna be my x-coordinate and my y-coordinate of the input space
This one is going to be my x-coordinate and my y-coordinate of these embedding space which is linearly separable
Given that I have, you know, I am
Obtained 100% accuracy with the training we haven't talked about training yet, but does it make any sense the answer what I gave you?
Okay, yeah
So this is just a way to visualize five dimensions in two dimensions instead of doing a PCA, I just do a PCA
Hold on hasn't finished
Because I'd like to see what the network output looks like in two dimensions, okay
Why is that true or is that false you see that next time it doesn't shrink because again those two
You can see multiply them together, right? It looks like one singular matrix one single matrix
And actually this works better
More about this later on in the class actually Yann is going to be talking about this
Okay, we still have some time all right, so how did I make these animations
First option: I'm a magician
Why you're laughing?, it's actually true. But anyhow, I know how to use matplotlib
I've taken this class three times and I never pass it
Okay, all of those are three
All the three are true. I never taken the class. But okay. I mean I've been on the other side
Yes, so this is not magic, right so this is just visualization using matplotlib free open source code in
an PyTorch, which is going to be our
library, which are going to be using for the
Right in these models. It's very it's very very convenient
was made by a student of mine back, I guess in 2016, so
You know be that cool. I'm just kidding. You can be even more cool. All right
more questions
No, okay, so now we actually can go to the more concrete part, right, so this was kind of abstract I gave you some
interesting
You know delicious I guess appetizer. Let's get on the main course and let's see how you can actually get started. Okay, so
Everything you're gonna be seen in my classes or in my github repository, which you are gonna be forced to star
Now I'm just kidding
But if you don't star it, I will take notice
All right, github.com/atcold
so it cold stays for something weird
So my Italian name is Alfredo, "fredo" can mean "cold" and "al" can mean "at"
So that "atcold" is gonna be like kind of transliteration of my name
So atcold
You go there
Yet, it's me how cute
and the first one with one thousand and three hundred stars, and I guess you have two hundred so I should be
1500 by tonight
It's not the class repository for the class. Right? And so you're gonna be
if you don't have so you should have like a
UNIX, actually no, PyTorch works or something on Windows
I don't use Windows so I have no idea. But if you have a Mac or a
Linux machine everything works. Just fine. If you have a Windows it should work as well. I haven't tried
So how do you access this stuff you go there?
Here you're gonna have the page website as soon as we actually make it
we are going to be actually looking at the
Repo, sorry,
The notebook number two right now
If we have time left, we can also check a bit of a more basic introduction to the tensor tutorial
How many of you have no idea
what numpy or numpy whatever you want to call it is ends up. Don't be shy
So
I'll change my question now
Do does everyone here in class know about numpy and has it used before?
Is it are you gonna be shaking your hand? Just shake your head, right?
So who is no whatsoever idea what numpy is and how it works and never use Python before you know, it happened
Don't don't laugh. Okay
Because if you don't know about that, there is also the zero zero class where you implement things from scratch
But I guess there's too much back
So I guess the number one it perhaps is not too worthy going over. We see if we have some time left
It basically gives you some introduction about how to use basic
pytorch routines for creating tensors which are basically
multi-dimensional arrays in an amp I
And so it creates tensors multiply tensors
it just
Initialize create random stuff. So it's it's not that different from numpy again
We can check them out if we can check this out if we have some time afterwards
Otherwise, it's gonna be like left as an assignment to go over this
It should be very easy gentle introduction to how to get started with the API. Ok
Complains about this
No, wow
Are you shy or you're too nice?
Ok
You're not laughing. Ok. All right, so
Ok, I'm joking, but you're not getting the jokes is fine. One more advertisement is type. Aura this type or a thing
I love it so much is a
markdown editor where you can write down your
Notes in latex and markdown, right so you can write for example
You know, whatever soft argmax which is something you're going to be learning about which I call it this way
So you can use latex in a markdown file. It looks very good
for
example I here I
annotate
Articles I I read okay. So this is TYPORA T Y  P O R A. Okay. That's just you know
Promotional message I don't get paid so, you know, it's Franco, okay
All right. So, how do we get started there?
we open the terminal we
Zoom such that you can see something so we can go for example inside or this is going to be different
Let's say we go inside that fall inside that repository
Icon somewhere else which is gonna be my notebooks and then my video something video lessons
Okay, so you're going to be basically running Conda activate
deep learning mini course and
Everything just works
If you have installed and you have followed the instructions in the readme, okay, so right now if you just type ipython
Ipython with pylab
It open you can do import
Torch for example
just to see whether everything works and then you can type for example torch dot R and and
Five and this one is going to be typing creating a tensor with five number
Which are randomly picked from zero to one uniform distribution, okay
So this is just to check that everything just works fine. If this works, then we can actually start and playing with the notebooks
You're not kind of I mean, you can try to follow along in class with the computer
But I rather I think it's better for you to actually follow and what I do here
you know, I'm a clown I try to engage with you if you isolate yourself with the
Computer maybe sometimes you may lose some of some of the nuggets I give you
Anyhow, so it looks like everything works here. So I'm gonna be just opening the Jupiter notebook
And things are black because I like them this way
Yeah, black is good. Alright, so we're gonna see this random projection to be starting
so
Yeah
I'm let me go fullscreen
You should be complaining if you don't see things, right?
I kind of know what you see what you don't know what you see what you don't see but if you
Provide feedback live like I can't read anything or can you turn off the light? That would be also useful
Okay, so do interrupt me to
Talk to me. Can you see is he okay?
Yes, no
Thank you
All right. Let me turn off that one - all right. Okay, so I do here. I just what I do here
I'm gonna just import some utilities. I find for plotting stuff
Then here I just import torch and then from torch I import this NN library
Which is simply I'm gonna be importing the matrix multiplication and the addition of our vector, okay?
so matrix multiplication is gonna be a linear operation plus the
Bias, but plus the translation we have the affine transformation
Which is the one we were talking about with the five different transformations, and then we're going to be important
I guess some nonlinear function somewhere
Moreover I import from applet leave those
Things for plotting things. I also important. Um pipe is NP
Non pine on PI, I don't know is it numpy number or Mumbai with numeric people complain about the atom?
Anyhow, so I use some default things and okay. This is the first line torch specific line. So this is gonna be
device
equal torch dot device
CUDA 0 if CUDA is available otherwise CPU. What is this if I am running these?
Examples on a chip on a machine which has a GPU
Automatically porch will run on the GPU memory
So your machine has a CPU which makes only sequential operations one after each other
Musty might be very very very fast, but it still sequential right? It's like your brain you can only do one thing at a time
If you try to drive and smoke and funk take a phone call
Yeah, you might not be here tomorrow so you can only do one thing at a time, right?
And it's like the CPU of a computer and then you have some local memory, which is the main memory is called RAM random access
Memory on the other side. If you would like to speed up things you can use something that is much slower
Which is called GPU so why would you use a GPU which is slower?
Than a CPU to speed up your computations
Because although it's very very much slower it can perform many many many more computation at the same time, right?
So if the CPUs might be like like running very quickly from one side to the other
It's gonna be beating your GPU so many times but the GPU does
Okay
Alright, so the GPU will have a memory which is called device memory, which is which lives on the GPU
I think right now they are planning to get some
Access to the main memory of the CPU from the GPU. I think they are working on that. I don't think it's yet out
Anyhow, if you work on the GPU, you will have to create your tensors in the GPU maintain the in the GPU memory
But you don't have to worry about that as long as you specify that thing over there
at the beginning of your code my torch will take care and will put your
Tensors in the right location for you. If you're going to be using a TPU a tensor processing unit from Google similarly
You're gonna have pi torch putting your tensors in the TPU memory
Which is close to the actual processing unit there which is going to make your operation, you know computed in a faster way
so this is a
Stupid single line, but it's important. Okay
Again about the Pytorch stuff you won't be
Tested in the midterm
Midterm is going to be mostly about math
This study is going to be essential for actually managing to succeed in the final project
Okay, so if you are trying to beat others while using CPUs, and they are using GPUs
You know, good luck
okay, and
yeah, I'm ironic but
You know people tried and
And then complain and then I don't want complaints. Okay. Alright, so here we create
1,000 points and
Actually, I do this one. Okay, so one underscore zero zero zero is the same as one thousand
But it's better to read and so here I have 1,000 points and my X capital X is gonna be my design matrix
It's gonna be having
1,000 rows and two columns. Okay, and it's sampled from our random and
Distribution, so if you go here you press shift tab
You're going to be having this thing opening up and you can click here. You're gonna see what is this stuff, right?
So R and R on the end is gonna be able to turn a tensor field with random numbers from a normal distribution
With mean 0 and variance 1 also called the standard normal distribution. Okay. So, how did I open the help?
Do you know this stuff?
Have you ever used notebooks before is the first time you saw this help, you know?
Some of you may have seen this the first time
Ok. So, how are these capital X going to be looking? Like how are these set of points?
What what is your expectation for this set of points?
You're supposed to interact
Small values near to zero. Okay. So you're going to have a cloud of points. What is going to be the radius of your cloud?
It's again
No
I'm just speaking since I don't like distributions
I'm gonna have like a bunch of points which are going to be basically somehow circularly distributed
I'd like to know what is the average radius of this?
blob
1/2 Warfare's more or less
Zero average
So you have the expectation of the square of the X square, right?
Square root of the expectation but whatever like expectation of the square root of the sum of the squares, right?
Okay. Someone said 1/2 cool guys once more who beats more
Say again?
that that or 1 so you have the standard deviation is gonna be 1
So are you gonna get points outside of the standard deviation or not? You know the bell curve right? Where is the one standard deviation?
Close to the center far away how much larger than the standard deviation is the bell?
Okay, roughly
Try to guess just shoot numbers, it's okay. I don't judge you yet
No you too shy okay, where is my student answer I
Can't find
You fine not fine, okay
There we go
What is this stuff it's cute I know
Can you see anything let me should I lower it okay. I just lowered the light whatever
people with Eric in the recording are gonna be complaining but
Okay, still I don't know how to turn off the first thing so you got get dark
So what are those red and green arrows there?
Okay, you can you can guess right I don't bite you
The axe is fantastic which axis?
What is the length of those things? Come on?
You need right? So those are the unit vectors Y. Y1 is red one while the other is green
Okay, fantastic
Which one is X which one is Y
X is the red one. Why is that because you have
RGB and right because you have taken some graphic just before this
So R is gonna be always right is gonna be always the x-axis green is gonna be always the y-axis
Those are unit vectors and therefore this cloud of points spends, roughly
What is the average radius here can you guess more or less
Three fantastic, that was the answer. Okay, so you have a cloud of points roughly large three, right?
Uniformly like circularly distribute the rain sort of all right cool. So we're gonna run now
You can try to put like this arrow here one more arrow here one more arrow here you have three arrows there, okay
More or less. I'm a physicist, right?
Yeah later. I will be mortician. Okay now I'm a mathematician so
Realizing, you know transformations see here. I compute the SVD singular value decomposition
Again, there's the video I pointed out before so I'm gonna be multiplying this
This cloud of point by a matrix, so what does the matrix do to these circular blob?
Come on, you know this stuff you will be repeated three times so far. What do you expect to? See you?
Have a circle here. What do you expect to see afterwards?
a
Potato yes, correct. So first potato and this is a very squished one. So why is he what's happen here?
What's happen in the y-direction?
All right, so how is this matrix
It's a cute matrix is the rotation matrix is an annoying matrix
How are the eigen what are the singular values look look here one singular value is almost close to 0, right?
So this is our almost singular matrix
So this is what's happening. It's gonna kill one dimension. This one didn't kill it yet
But basically you have that the dimensions in the diagonal right in the singular value decomposition
Represent the amount of zooming you have in the different direction and then the first two matrices the first and the last one
Represent the rotation that these metrics apply
This other one has a factor of two in one direction and the zero six in the other one
So you get this stuff here Oh something else. Hold on
So here you have blue green yellow red, right so our our G alone. No, there is no order here
We have blue green yellow red and then here you have blue red
Yellow green. So what's happened here? How's the determinant?
Negative why is that
Because you had a
Reflection that's correct. All right, I'll keep going you can see here different
Things right? So you have more potatoes here and this is actually zooming a lot. You have one point three
This also is kind of zooming a little
Okay, this one does nothing
Okay, boring, right, oh, okay what's happen here, how are these metrics
It's gonna be singular right almost you can see the one singular value is gonna be zero point zero three, it's very tiny
You can see these are the unit vectors. There are very big arrows here means it has squashed down, right?
The first one is 0 6 the other one is 0 0 3 so you can basically almost killed a dimension here
And so on. Okay, so no much
Stuff anyhow, you can do the same things with Pytorch now, so this can be the first instruction
You're gonna be seen about Pytorch
So here I do model is going to be a sequential
which is basically a
Container where I can put a few a few modules one after each other and my first module is going to be a NN linear
Which means what does and then linear means?
Doesn't mean a linear transformation
because these people were
Guest programmers so, you know, they don't know I guess this is an affine transformation
But actually it's a linear transformation because I said bias equals false. Okay. So this is actually just a matrix multiplication
So it is a linear transformation. Okay maps to 0 to 0
Then I do model to device I ship the model to the memory of the GPU and then I remove the gradients
You're gonna figure out this next week what it is
I'm gonna bet get here. My Y is gonna be the output of the model to which I input the X
Then I generate my figure and I plot this stuff. So this is to show you that
You can also get a singular matrix with the Pytorch. How cool is this?
No, it's boring, right?
I mean you can see you can multiply data by matrix matrices by using the Pytorch package
And so we had to create a model
Sequential container we put inside the linear module. We remove the bias the translation and you get this stuff
Why did I remove the translation?
Because otherwise I had to fetch it and it goes outside the screen so I keep it in the center. Okay? All right, so
Let's have this one. So right now I'm gonna just use the following I just use a matrix
That is I think I didn't pick curl. No, it's gonna be identity matrix
Which I scale both items in the diagonal by the same item by the same value
And then I apply a nonlinear function. So my nonlinear function is gonna be the following. It's a hyperbolic tangent
which goes to
Minus 1 roughly when you cross the minus 2.5 here at the bottom and it goes to to put goes to one
Roughly whenever you cross the plus 2.5 okay up here
so this stuff Maps, what the whole
Real line to wear
so plus infinity goes to
one and minus infinity goes to
Until you reach the linear region, which is roughly minus two point five to two point five
Why do I mention this name in these numbers? Because sometimes you're gonna put your nose inside
your model it is not training and you still want to figure out the order of
Magnitude and you know the rough the number which are kind of associated to the kink of these nonlinearities
Anyhow, so I just create this one which is very very similar to the thing. I just show you before
Which has
Where is it here? So here I have my sequential which is the sequence of
Operations I use a matrix first
so this linear to two then I applied the tanh and
Then I increased the I think the multiplier for that diagonal matrix
So that's what you get. So this is my cloud. What do you expect to see of my cloud?
So what is the range of this cloud we said it's roughly
It goes from minus 3 to plus 3 right? Where is the kink happening?
So everything that is
outside these two point five minus two point five two plus two point five box is going to be mapped to
one things that are inside
Stay roughly the same that's correct. So what do you expect to see after this image over here?
You expect to have inbox your data but what is going to be the size of this box
Are you sure so the the suggestion was from minus two point five to two point five?
The box is going to be 2 minus 1 2 plus 1 so minus 1 minus 1
I mean all the box from minus 1 to plus 1
But then the points inside are going to be mapped according to the mapping
We were mentioning before so things from minus two point five to two point five are getting linearly
Squeezed and then things outside at two point five
number are going to be squashed down to the
Basically edge of this box. Okay. Are you ready to see how to box a normal distribution?
Are you excited?
Okay, just two people just said that I mean are we excited come on give me some satisfaction
Okay, you're not playing with me okay, I see I'm trying to be nice to you oh
Oh, thank you at least at work, right? So that was first one
So in this case here, I'll just show you just my identity matrix. I didn't do anything and here things are kind of
What kind of distribution looks this? I mean what what does it look like? Come on? Oh
It's a uniform kind of between
Minus 1 to plus 1 fantastic in 2d, what does it happen now if I start doing a little bit
If I start multiplying instead of having identity matrix now, I have the 2x2 matrix or 3x3 or 4x4 5x5
Say again?
The points will go for further to the edges so you're gonna be seeing these and these and these and
Finally this huh? Oh, hey
Cool hah? So what happens here? We met by our cloud. It was like circular into this kind of boxy thing
How good is that now? Those little points are very very spread apart
And so you can actually tell them apart if you are for example
Classifying stuff finally in the last minute then I do let you go
It's going to be
using random matrices
but in this way, so here I'm gonna have my sequential I
Put like a linear layer
Which also has the bias terms is affine transformation I go from two to this hidden layer dimension
Which is going to be five, so I go to two to five
I applied the ReLU which is simply the positive part operator
And then I apply another affine transformation that goes to five to two such that I can display them on the screen
Okay, is it clear what I'm doing, so I create a sequential which is a sequence of modules
I have three modules
The first module is a matrix mapping
Matrix of height five with two so it shoots towards high dimensions it from a to two dimensional input space
There is a bias which is going to be five dimensional. Then you remove you set to zero everything. That is negative
And now you map this five dimensional space
Whatever down to two dimensions, so you're gonna have a matrix that has two rows and five columns
And then there is a bias of height like of size two. Okay. Did you follow?
If you didn't follow you can find in the recording because I'm gonna play press play. Are you ready?
Mmm, okay some engagement
Okay, it didn't work. Sorry. Okay. Okay fantastically. They know I did so this is the beginning right?
that's the starting point and then you get
Yes, you're allowed to do oh
It's fine. Oh, oh, this is all right. This is very spiky
So you're gonna figure out in the next class what happened here, but this is a very different thing right. This is this smooth transformations
Somehow smooth. This is very very
Angular, right?
and design and
How cute is this right so finally final remarks and then I let you go
I know it's dinner time for sorry for keeping you here to me if late so final remarks are the following then
Yeah, check the Piazza
so this is basically a untrain neural network those I just show you in the module with the three items the
linear transformation
Positive part the end transformation or affine transformation is your first linear deep neural network?
It's not that deep but still makes pretty damn awesome
Drawings I think in my opinion so I'm like a just initialize network an untrained Network
We start with this kind of transformation, but we have seen from the first video
I show you tonight that we are aiming for a
Transformation which is, you know instrumental to perform a specific task, which is for example
The classification of those points that are belonging to the different five spiral
So in the next class we're going to figure out how do I take this?
initial, you know arbitrary transformation and I can pull it apart and manipulate such that I achieve the
final goal, which is for example
Make my points linearly separable in the final layer. Okay, so this is gonna be for the next class
otherwise stay warm
And I see you next time, bye-bye