Hey everyone, my name is Jeffrey Mew and I'm a
program manager here at Microsoft for the Python data Science
and AI team in Visual Studio code.
Specifically, I'm working on all things related to Pytorch
in Visual Studio Code right now, and you might be
asking,
how did this all start?
Well, PyTorch has recently become actually the fastest,
if not the fastest growing Python machine learning framework out
there.
But the tooling portion hasn't really been kept up to
its growth,
and when we talked to the community of PyTorch developers,
we found that there is a huge gap and lack
of good tools to develop PyTorch within the market.
So because of this, we've been working with the PyTorch
team over at Facebook to make the development portion of
PyTorch in VS Code a first class experience.
And we want to make
VS Code the de facto tool for PyTorch.
So today I'll be going through a typical machine learning
project in VS Code,
and I'll be walking you through step by step throughout
the project from the data exploration phase of this machine
learning workflow to the training phase where we're going to
be trying to figure out a model and doing our
compute and maybe potentially tuning our model and finally to
the deployment and inferecing of your model so that
it can be used for other services and used by
other people.
And alongside this I'll be showcasing all the PyTorch and
data science features that VS Code has to offer.
Great so to get started with VS code if you
don't already have it installed,
you can just go to code.visualstudio.com right here and just
to download it for your computer.
It's free, open source and cross-platform so it works on
any machine,
but once you actually have this code installed,
let me just shift to my VS code.
you'll need to go into the extensions tab, so you
see this extensions over here like this. And then you
can search for the keyword Python.
So just search for Python And it should be the
first one made by Microsoft and then once you download
this.
This is where all the Python,
Jupyter notebook related functionality for VS Code live,
and that's where all the PyTorch and data science features
that I'm going to be showcasing today reside as well.
Great so now once we have everything installed and setup with
VS Code and the Python extension,
we can get started. So everyone is talking about stocks
nowadays and you probably heard of colleagues or friends talking
bout GameStop or Tesla stock.
So I decided to get into the fun of it
by building a simple PyTorch project to try and predict
and model the Microsoft stock price.
We'll just be using the historic stock price of Microsoft
as a data set and we will also be using
Pytorch with Jupyter notebooks to build and train the model.
Quick disclaimer. Not a financial expert here,
so this project is just for fun and if this
model worked 100% of the time, I'd probably be rich
by now.
But with that out of the way to get started,
I have my Pytorch project here already opened up in
VS
Code, and I already have my Jupyter notebook called Stocks.ipynb
opened up,
which is where most of my Pytorch code for this
first model is is living.
And although I'll be using Jupyter notebooks for this project
and demo, most of the features you will see today
can be found while you're working with a regular Python
file or Jupyter notebooks.
Whichever one you're more comfortable with.
And although I'll be stepping through the code quitw quickly,
since the focus here is mostly around the tooling,
feel free to pause the video anytime.
you can look at the code in more detail, and
I'll also be including a link to the GitHub repo
at the end of the video as well if you are more
interested.
Cool some coming back to the ML workflow diagram
here, we can see that one of the first steps
with any part or machine learning project is the data
exploration phase.
So here's where you'd be figuring out where data set
is, doing the data cleaning and preprocessing,
and loading it into your computer.
So that's going to be doing first.
I have the cell right here.
What it does is, I'm actually using the Yahoo Finance
API to get the Microsoft stock prices and using pandas
to load it into a data frame here.
So to run the cell I can click this run
button here or actually
just click shift + enter to run that cell.
you can see this right here,
and rather than printing out my dataframe 'cause I want
to see what my data looks like at some point,
and rather than just printing it out here and looking
at its raw values,
 I can actually use VS Code's built in data
frame viewer to look at my data in a much
more human readable format.
So to do so I just need to open up
the variable explorer first. To open up the variable explorer,
I can just click this variable explorer,icon in the
top right and then the variable explorer,will open up
in the bottom and variable explorer,
super powerful because they keep track of all your active
variables as well as their most up-to-date value.
So it just brings a lot of order to the
chaos that is your Jupyter notebook.
And then if I want to access the data viewer,
I can just open up the variable I just created
here so the I can just click.
I can just click this, and it will open up
what we call the data viewer.
So the data here, as you can tell,
is an Excel like interface for data frames or just
array like objects and within the data viewer I can
do a lot of powerful things so I can quickly
filter values, sort value. So for example I want to
look at the earliest date or the latest date.
you can see the earliest date here is 2016 March
15th and the latest date is today's date,
when I'm filming video. As well,
I can quickly sort values, search for values 
and then the more cool thing is I can actually
see all of my columns at a glance.
So here, makes it easier to figure out my game
plan is of how I want to clean the data
and which columns I need to get
rid of. So for this stock price prediction model,
I'm going to be looking at the column,
so I'm probably gonna be needing this Close column,
'cause that's where we're going to be predicting against. The
Close represents the closing price of the stock On this
date,
And I also need the date just 'cause it's referencing
the dates of when this close was as well.
So, knowing that I can go back to my code
and just,
write my code, run my code,
where I will only want to select the date in
the Close columns which I have here,
and again I can just hit shift enter to run.
And if I want to see my updated value, 'cause
I'm just responding to this data frame,
I can actually click this refresh icon here and it
gives me an up-to-date value of what my data is.
So it's really great sanity check to see that, I'm
actually grabbing the correct columns and my code is doing
the thing I expected to do.
OK, so I'm looking at my data and the data
viewer here is really nice,
but one thing you want to do with any PyTorch
or even data science project is to actually plot your
data to view how it transport visually.
So what's nice of VS Code notebooks is that they
support interactive ipywidgets and also nteract widgets such as Plotly,
which is one of the most popular ones,
or Python plotting libraries out of the box,
so you can see here,
I just loaded the Microsoft stock price over the last
few years and it's a polygraph.
So because there's a polygraph,
it's all fully interactive, so you can see the tooltips,
I can select different ranges I want to look at.
I can even export it and this is way more
powerful than just working with a traditional matplotlib static graph.
Cool, so now that I have a pretty good understanding
my data set,
it's time to actually process the data so that we
can prepare it for the model.
We will want to apply some transformation to the data
to better normalize it because you can see these numbers
are kind of like arbitrary, at least according to the
model, and
all over the place. So oftentimes we're applying several transformations
in a row as hard.
Hard to see if your transformations are giving you the
correct result.
And again, this is where the data viewer and the
variable explorer comes in
really handy. 'cause as I'm applying the transformations I can
actually see the size of the variable,
so I can make sure that it's expected size that
I want.
So not that like I'm dropping rows
I don't want to drop any percent of my rows
right,
or I can go into the data to make sure
that my data is correct as well.
So here I have some code that just applies some
simple transformations.
For example, just like we're removing null values and also
just normalizing values between the values of negative one and
one and I'm assigning to a new variable called DF_Microsoft.
So again, if I wanted to do a quick sanity
check,
I can just open it up in the data viewer
again and then I can leverage the powerful tools just
by clicking on the column name.
If you want to see the maximum value so I
can see my minimum value is -1 and the
maximum is 1,
So it's really another great way that I can leverage
this,
a sanity check to make sure that
my data is, or at least what applying to my
data is correct so far.
So finally in the last step we'll need to do
in the preprocessing is to actually convert the data to
tensors. And tensors are just the variable type that PyTorch
understands and works with and you can think about it
just like a matrix data type.
Again, I've written a basic function to do this,
but again we can leverage the data viewer to see
if it convert properly.
So here I'm on the cell and you can see
I'm splitting into the training and testing sets, and I
can find the corresponding value of variables here and I
can
see that there are now type tensor,
which is what I'm expecting and I can actually see
the distribution between the test and training sets, so you
can see the size of the training and test sets.
And as well, you can see,
even though it's 3 dimensional,
you can. You can actually even still.
We recently added support for both tensors,
 being viewed in the data viewer, but also multi dimensional
tensors.
So this is 3 dimensional and you might be asking
how does it actually render in this 2D grid?
Well, we added this feature for multidimensional data, so 3
dimensional or higher, where you can actually slice your data.
So sizing data essentially lets you take any 2 dimensional
slice of your multidimensional data and to just view that
place so it makes it way more human readable because.
If you didn't have that,
you'd just look at arrays within arrays,
which are kind of hard to read and understand and
visualize.
So having this makes it so much easier.
And again, you can leverage either 
This drop down so you can see this is in
sync
or you can use this text box if you want
to.
If you're working with your computer with that as well.
So for example, if I want to get some range,
I can get...
Sorry... if I want to get the first five, I
can just do this,
The first five rows. And again,
it's also another great sanity
check to make sure my values are still what I expected them to
be.
Cool, so now I'm going back to machine learning workflow.
We just did our data exploration step where we imported
our data and did a lot of data cleaning,
preprocessing on them from the data viewer,
as well as converting to tensors.
The next part is training,
where we will be going to be figuring out which
model we want to use and how we're going to
train it.
So there's several different models I can use.
The train stock price prediction,
but the two more common well known ones for this
type of problem space,
are LSTM or GRU based models,
and right now I'm not really sure which one will
perform better,
so I'm going to be testing out both models to
compare their accuracy.
But before I do so,
I just want to show up really quick.
a cool feature, which is our Intellisense support for Pytorch.
So as you can see,
here is my code for my LSTM model that showed
up.
It's a really simple model, but even if you're somewhat
familiar with PyTorch or new to PyTorch specifically,
you might find the API kind of daunting.
There's like a lot of API's,
and all the APIs have their own different parameters and
arguments,
and it's kind of hard to keep track of them
all.
So what's great is if I want to add another
layer to this model and then you will leverage the
PyTorch
package, so I can just do self.fc1 if I want
to add another fully connected layer, and then can see
as I typed in neural net dot
You can see it gives me suggestions of the top
APIs that are used by this 
PyTorch framework as well as it gives me the docstring,
so which is really nice,
'cause, for example, let's say I want to add like
a dropout layer.
If I go back to drop out,
it gives me examples of how it's used,
so I don't actually have to go back and forth
between like Google or my web browser.
just figure it out on Google or Stackoverflow how to
use it,
I can just see it here, and as well as
I type in "(" it will show the parameters that
I need to pass.
You can see it here and expect the float so
I can just give it a float of
what percentage it's going to be dropped out.
So having IntelliSense built into VS Code for PyTorch makes
PyTorch Development super super easy, and just takes the pain
out out of PyTorch development for you.
OK, so now getting back to the model development,
the code explained the models like Imagine Force right here,
but it's kind of hard to visualize to see its
architecture,
especially if you're not experienced with machine learning or Python itself.
So we'll be leveraging a tool called TensorBoard
to help us. And TensorBoard is a free and open
source dashboard companion that's made by Google to help Pytorch
and Tensorflow developers both visualize attributes and aspects of their
machine learning models and training.
And it's one of the more popular tools that you
use with PyTorch right now,
so through my code I actually have logging
information to the test log files,
which will then get read by this Tensorboard app itself
and then parse into the UI that you'll see soon.
So if PyTorch is like a car, you can think
of Tensorboard to be like the dashboard in front of
you.
And what's great is one of the newest PyTorch features
that's built into VS code is built and tested integration.
So to launch it, you can either click on the
code lens that you saw in the first cell.
So if you have any imports that have tensorboard, you
will see this code lens up here.
Or you can launch it anytime by opening the command
palette with Ctrl + shift + P or Command +
shift + P if you're on Mac, and searching for
the keyword
"launch tensorboard".
And we launch it. By default it will use your
current working directory,
'cause it knows when you have open in VS code
and it's smart
enough, it will actually search through its subdirectories
is to find log files so traditionally with tensorboard
you need to specify the exact folders in,
but here you will just automatically do it for you.
Or you can just select another folder.
So I'm gonna be launching this.
So because it's VS Code,
I can make this bigger,
so if I just close all this.
You can see it in full screen.
I can make it any size I want.Getting back in
problem of the model architecture we can actually navigate to
this graph tab which I created and
we can actually interact with this LSTM model in a
much more human readable format.
So you can see here.
I can see those two layers of my model best
LSTM linear layer,
so it goes from the inputs the LSTM linear output and you
can see where this is really useful is most of
the time you're going to be looking at
someone elses Pytorch code, whether it's from GitHub or your
colleagues for the first time.
And if you want to get better understanding of how
their model was built and how the model works.
this is just a way easier way to look at
it than typing to look at the code.
Cool, so now that we developed our models and had
a better understanding the architecture.
We can now actually run our training set against both
the LSTM and GRU models to see which one performs better.
I previously ran the training so everyone don't have to
watch my code run for hours on end,
but if you actually want to leverage faster compute, VS
Code supports connecting your notebooks or Python files to a
remote server via SSH.
So if you just have installed this extension calls remote
SSH.
you'll see this green thing here at the bottom left
and then here you can just enter your SSH and
click connect to host to enter the ID address
for the remote machine, whether that's on Azure or some
cloud compute, and then VS Code will leverage that compute
as a backend.
And once the training is complete,
we can go back to tensor board.
to see if the training on both models were trained
properly by seeing if the loss functions converged. So we
can kind of see X axis represents the number of
epochs
But as the training went on,
you can see that the loss of both models approach
0,
which is a good sign because it's showing that both
models are converging at some point.
Going back to our graphic and use it.
sorry, going back to our notebook we can use the
test set that we save to see a model performs
in the real world and we clearly see.
So this is our LSTM model.
It's not that great. We didn't expect that much,
'cause it's just a simple model,
but if we go the GRU section,
which is the exact same code but just using a
GRU based model, we can see that the stock price
performs much much better, the particular stock price and the
model performs much much better.
It's not like 100% accurate but there's no way to
get 100% accuracy
at this point. So based on this,
we're probably going to be sticking with the GRU model
over the STM.
And then finally, now that we know how our model
code works and we've been successfully able to train our
model,
we can try to optimize the training using the newly
released PyTorch profiler. The PyTorch profiler is a joint project
that's between Microsoft and Facebook. And to access the Profiler
you'll just need the latest version of Pytorch,
which at the time of this recording is just 1.8.1
installed, as well as the pip package named
pytorch_tb_profiler
which VS Code will install for you if you don't
have.
But that's the plugin for TensorBoard.
To actually view the profiler once you have both installed,
we will just show you the source code real quick,
but it's really simple to profile your code.
All you need to do is to import this torch.profiler
statement and then you just need to wrap your code
so you can see here.
This is my training code.
My training function. I just need to wrap it with
this one statement,
which is just telling the profiler to look at both
the CPU and GPU,
save the logs to this tab or this folder,
which is where my attention.
are already being saved, and then I'm just telling it
when the end of an epoch is so that the
profiler knows how long it is.
So once that's all done,
we can go back to that support tab and you'll
see this Pytorch Profiler tab,
which if you have that package like I mentioned earlier,
installed. And this is what the Pytorch profiler overview view
looks like.
Want me to switch to like another example,
but you can see
It gives you a step time breakdown of where each
of their time spent during each iteration of your training.
As you can see, this is the time it spent
loading data versus running on the compute etc., and also
gives you a nice performance recommendation if it detects that
there's any way where you can improve the speed of
your training,
'cause your CPU or CPU cycles cost you money.
So you obviously want to save money there.
So you can see this example.
It said that data loading took up around 22%
of time and it gives instructions on how to paralyze
that to speed that up.
So I did another training job where that's being done,
and you can see the purple which represents the data
loading is now essentially gone because we've improved that step
and there's no more performance recommendations,
which is really nice to see.
There's so many things you can do with this sensor
board.
Let's start with the PyTorch profiler,
so I'll just look for the documentation there.
If you want to learn more about it,
but the only other cool feature I want to point
out which is coming soon is the ability to jump
to source.
So if I go into this operator view,
which represents all the functions and how often they're getting
called by my training job,
let's say I want to see what is calling.
Let's  say I'm trying to find some inefficiencies in my
code.
I can see how many times,
for example, is calling. What's calling this add function so
often,
right? I can click on this "View,
CallStack", "View call frames" and if I find any user
code.
So let's say I see here there's some code that
I've written and click on this, and VS Code will
automatically jump to this source code error where you can
quickly just see what is causing all these calls and
make the edits necessary that you need to improve your
performance of your training.
So now that we've completed this essentially completed the training
job,
we fix the inefficiencies, the data loader if we want
to share this code with others,
or use git to deploy the code, VS Code has
native Git integration,
so we can do this.
Go into the source control tab.
You can see here and you can just click on
the notebook that you're working with and what's great is
rather traditionally,
if you ran a git diff,
you would see it on json,
which is kind of hard.
But VS code actually parses that fall into a notebook
like.
Interface and you can actually see here exactly what I've
changed so it's way nicer to read and makes it
a lot easier to track the changes that you made
in your Jupyter notebook.
Yes, that's mostly it. Last part is to just to
save our model.
So here I'm using the PyTorch save API and I'm saving
as this file,
and once that's actually saved,
I can now deploy it to Azure or deploy to any
cloud that I choose.
And what's great is that VS Code has direct built-in
integration with Azure services that you'll need for PyTorch.
So like Azure machine learning,
if you want to run experiments or Azure functions,
if you want to host the endpoint of your model or Azure storage,
if you want to store the model in the cloud
and that's all done through extensions.
So just for search for the Azure keyword,
you'll find it. And then from there I can just
upload it to Azure within VS code.
Great, so just to summarize everything we've done today,
I'll take it back to this machine learning workflow.
So we started off with the data exploration where we
import our data into our notebook.
Then we did some data cleaning, data preprocessing by using
the data viewer and we also converted to tensors and
made sure I'm using the data slicing  to see that
the values
are what we expected to be.
Then we moved to the training step where we figured
out which model to use between LSTM and GRU,
and we used TensorBoard to
try to figure out what not only what our model
looks like and how it works,
but also just figure out which one is performing better
and to see if they actually perform well of what
we expected to do.
We could have leveraged Remote SSH if we wanted to
make our training faster by leveraging remote compute,
and finally we leveraged the VS Code...
Sorry, the profiler integration with VS code to actually tune the 
model right by improving that
data loading efficiency. And we can also use jump to
source,
which is a feature that is coming out soon to quickly
make the changes as neede. And then finally for inferencing
we just have the Azure extensions installed and once that's
done it's super simple.
Right click -> deploy, and the key aspect of this entire
machine learning workflow as you saw was everything was done
in VS code.
I didn't need to switch between different tools to different
tabs,
everything was done in this context which is really nice
to see.
It's the all-in-one tool for PyTorch.
So what's next?
If you want to build your own stock price model
for the GitHub repo,
you can just go to 
aka.ms/pytorch-stocks
And if you want to try Pytorch development yourself with
this code,
you can just go to 
aka.ms/python-extension.
So thank you so much for watching today and we
have a lot more exciting new features for Pytorch and
data science coming up soon.
So if you want to keep track of that,
just follow us on Twitter @pythonvscode.
And thanks so much for watching.