okay what's up guys welcome to boot so
let's get started so basically we're
going discuss these five parameters
you know these frequently use parameter and
also if we see what are the parameters
available for this list in layer there
are a bunch of parameters.okay so,
here we are not going to discuss all of these parameters
because some of these parameters we
we never use them. So okay,
we will discuss the most used parameters of the keras LSTM layer
I'm going to make an assumption that you guys already know what is LSTM and why we use it.
so i'm not going to talk about that stuff
So, I basically explain what are these four parameters oops not four, five parameters
So, the first parameter is units and second parameter is input shape, return sequences, stateful and return state
what is this units parameter? in this
documentation is says the positive integer
dimensionality of the output space what does this mean?. so in order to get a better
understanding you should get our hands
this right now we get instead so let's
get some hands dirty by here I'm gonna
import these things that numpy and LSTM
input and sequential
so this will be included in our code.
I quickly made a sequential model and add a layer which is LSTM, LSTM layer
so this first parameter as you all know is units.
so what does this mean? as the
the document says it is the output space,
dimensionality of the output space.
so each lstm layer
output something right so let me see the
output it will look like this you can predict sample
data and get an output like this so that
means this vector is dimension of two. So if we specify another number here lets say four and run out model
then it will output a vector of dimension 4
that means this units means dimensionally
of the output space that's the only way that we
can explain this one so our next parameter is
the input shape so it will need two
parameters the first one is how many
times stamps do you have in your input
so let's say we want to input a word, not a word
a sentence with two words so this
one should be the length of that sentence
how many words that contain in that sentence
it so let's make this two and this one is how
many features are in one time
stamp so basically if we think about
if we represent a word by using  word2vec so
the vector size of the vector should come under here.
this is actually the feature size of a one time stamp
how many features there are in a one time stamp. This should be it
So, our next parameter is  return sequences
That one is very important, so if we, lest type it here
return, underscore,okay return sequencese  let's say it's true
So by default return sequences is false.That's why we get an out put like this
If we run this, Let's run this one and see the output
let's one this one also
oh, we received two vectors
okay, how did you received these two vectors?
let's go to our powerpoint
this one is the basic understanding of return sequences true diagram
and this one i basically extracted from the internet. So let's consider as this rnn as LSTM
basically, these two are equal you know
so this one is our first
time stamp and so this one is our last time stamp as you all know this
one is hidden state output right this one
is the one at the output of the hidden state
this one is another output of hidden state so the
cell state is by this direction to 1 LSTM
cell to another LSTM cell. okay you
guys know this right so leave the return
sequence true is the each output each
hidden state output is
so let's say we have in our case we have two
inputs right the length of one sentence is
two so we have two inputs to the layer
and we got two hidden state outputs okay so if
we have another layer below that, these hidden layers
output will be the input to the other Lane
like this okay.So if we if we make this false that means default one the return sequences false is the default one
so the last only the last
last hidden state of lstm layer is
returned.so if we have another layer bellow that
so this will be duplicated
and repeated vector repeated to all
other LSTM cell you got it that's why
we return  we have received two vectors with
the return sequence false
we receive only one vector.so that's good right?
if you want to have more data of your you know your sencence you colud go with this return sequences true one
and so it will look like this
it is three dimension.
so if we check the shape
three diamention
if we put this here false it will be a two diamentional one.
so you have manage the diamentionality and deal with is when you are using the return sequences true
so the our
next parameter will be stateful so we should quickly go through this documentation
here we can find the statefull.if true
true the last state for each sample at index I in
the batch will be used and the initials
is for the sample of index i in the following batch.
is back so if you go to our slides
let's describe this diagram so we have
batches like this inside the batch we have several samples, n samples  and let's take a 1
one i 'th sample here and on a sample
and when it goes to this our lstm layer
stateful is true layer it will
memorize this last cell state of that batch
so s1 s-1 then the next
batch comes the i'th
sample of that batch comes to the the LSTM layer
so the initial state to this you know the sample
will be the last cell state of the
previous batch i'th sample. okay okay
that's what the stateful means so we can
use this stateful true if our two batches have
some relationship
otherwise, we don't use this stateful true
if batches have some relationships and
one is depends with other batch then we use this stateful
true otherwise we don't use this okay so
so our nex one is return state that one is easy
okay let's specify return state true here
let's try to run this one
okay
it gives an error.it shows us all layers in sequence model should have one single output a single output tensor
so if we use try to use this return state true you cannot use it with this
sequential model, we have to move to keras
functional API okay so let's develop
another model with keras functional API so we import this one and we have imported input in oor top and you can specify return state true at this layer  so I assume that you also know
about keras functional API so if we run this one yeah
we will receive three outputs okay three outputs
so we have our input shape as 1,1 let's make this 2 like the previous one
let''s make this 2 okay
so we run this one our output will look like this
so it will output 3 tensors
what are these tensors the first one
is the hidden state of the last
LSTM cell so what are these things as you
see the first one and the second one are the same other one is different
so what this means
so let me go to our presentation
the LSTM layers actually looks like this
right so previously so one arrow here it's
actually two arrows so one is the hidden state this one is the hidden state and cell state.
so the hidden state and the cell state
both are passed to the next preceding cell
by the various cell okay so the returns
state what does the returns state that mean
is it returns the cell States to the
user to the next in next layer so if it's
consists of two things that the hidden
state of that one and the cell state so the reason for
these things to be similar we first we got the hidden state of this cell and
the cell state means hiddden state plus the cell state you can have
another name for this one so yeah that's
all the time
so hope you guys found something useful
Thanks