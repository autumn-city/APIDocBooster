hello everyone i hope you can hear me
fine
i think we can start
uh is everyone here just
uh wave a hello on the chat or unmute
yourself and just
let us know that you're here and ready
to start
okay perfect
okay awesome so um just a quick
presentation i'm hydra bermusian i'm an
instructor with the cod labs academy and
i'm doing my phd on automl and neural
architecture search i'm also working on
natural language processing and this
tutorial or this workshop will we will
like try to solve the question answering
tasks using some machine learning of
course and degrading okay
so
i think people are still coming so maybe
i'm just going to wait till
02 or 003
just to talk about the full workshop
agenda and then we can start
okay one quick remarks before we start
uh please stop me at any time if you
don't understand anything just unmute
yourself and ask your question or uh pop
a question on the chat i will look at it
uh while i will present
and yeah and don't don't hesitate to ask
any question you'd like even if it's
like basic machine learning because we
are going through nlp and transformers
which are a little advanced but don't
hesitate to ask any question you have
just in order to understand everything
and of course we're gonna have uh both
codes and presentations so i'm gonna
start by presenting the overall uh
what's machine learning what's nlp and
how can we use machine learning nlp and
then some of the models etc but also we
will have to code so if you want to code
with me just let me know otherwise i
have a notebook that is already
that is ready and that we can go through
and analyze each uh
yeah each line
out of context but can i connect with
you on linkedin yeah of course you can
look uh look for my uh name and uh first
name and last name of course and of
course you you also can connect with me
via the email i'm gonna send it here
collabs academy.com
so if you have any further questions
after the workshop you can of course
contact me on this email and i'll
respond
yes the recording will be available
later on our youtube channel
all right
if you don't have any further question
we can start
so in this workshop i'm gonna dis
i divided it into three different parts
so the first part i'm going to talk
about nlp in general and how do we deal
with text in uh in a computer so we know
that images are a bunch of pixels so we
know that okay intuitively we know that
pixels are numbers so we can deal with
them just like we deal with numbers but
how do we deal with text so we're gonna
see this and we're gonna try to create
some embeddings and play with embeddings
and what are the meaning of embedding so
basically as a first glimpse here
embeddings is how do we how we convert
words into our text into numbers or
vectors of numbers
and then uh which is really important
because
all deep learning deals with is numbers
and vectors of numbers so we need to
know how to convert all these texts and
words into numbers
and then in the second part we will
discuss what's
uh language modeling which is the
uh yeah which is the first task that
we're gonna solve with uh with deep
learning uh with deep learning models
and then of course we're gonna discuss
some architectures so in deep learning
we have these neural networks and then
the connection between each node in the
neural network and the operation within
the
the node
creates different architectures so we
have transformers we have recurrent
neural network we have the well-known
convolutional neural network for
computer vision but we also have
different uh yeah we also have different
architectures so we're going to discuss
which are the architectures that are the
most promising in uh in nlp and natural
language processing and the tasks that
we are going to solve which is question
answering
and we're going to discuss one of the uh
transformer model
which is bird which has like
state-of-the-art uh results but we are
going to discuss it deeper because we're
going to use it in the code etc
and then the third part we're gonna go
through question answering and explain
the task and solve it uh using uh
yeah using of course birth uh
architecture to solve the question
answer and we're going to also discuss
what kind of data sets are we going to
use what are the different types of
question answering etc
okay and then you you'll see here in the
slider qr code so what i wanted to do is
i wanted to create a small poll at the
beginning just to assess like uh your
knowledge of uh machine learning feel
free to answer the questions it's really
up to you i just sent the link on the
chat
i'm gonna send it again because some
people just came in but uh basically
what you can do is you can you go over
this uh slider and you just answer the
question uh so the first question i i
put here is machine learning is a subset
of
is it deep learning is artificial
intelligence is it data learning etc so
i'm just gonna assess what you're what
you know and what you what you already
know because it's it's really gonna help
me like go into the details or not of
some part of the talk
so please go into this link and try to
answer machine learning is a subset of
what is it a subset of data learning
deep learning artificial intelligence or
none of the above
okay
14 people answered 15 perfect
okay just gonna send the link again if
someone don't doesn't have it on the
chat you can check the link on the chat
you can also use the qr code on the
slide to access the um
yeah to access the poll
in the poll we are asking machine
learning is a subset of what is it a
subset of data learning deep learning
artificial intelligence or none of the
above
so i think i'm gonna show the answer
here uh we have already 16 people 17
people answered okay that's great
out of 28
come on people you can answer it
okay 19 people up to 28 that's good
okay i think i'm gonna stop here we have
20 people answered i'm gonna show you
the correct answer here
and the correct answer was
was artificial intelligence so machine
learning is a subset of artificial
intelligence i just launched another uh
another question but i will of course
explain this so we know that machine
learning deals with data and deals with
um
yeah of course deals with experience
from the experience we'll learn
something and then we apply it in the
real world or in new uh in new instances
and basically this is what machine
learning algorithm does learn from the
experience and then apply it in a real
life scenario or with real life
instances and artificial intelligence is
the science or uh yeah in globe all the
algorithms that uses uh that makes the
machine decide basically so
whether we have like a new finale an
expert system that tries that makes the
machine decide whether to turn off the
light or not this is artificial
intelligence but also machine learning
is a subset of artificial intelligence
and then we also have uh this
deep learning so deep learning are
machine learning algorithms so they also
learn from experience they also learn
from data but they use this special
neural network algorithmic like
algorithm model so this is what deep
learning is and this is what um yeah
machine learning and artificial
intelligence so to answer the question
machine learning is a subset of
artificial intelligence which most of
you got correct perfect
okay
and then we have another question that
pops up if you can go in and answer it
i'm gonna send the link again
so we are now 32 let's hope for 20
answers in the question of for the
question
so the question is pandas is used to
deal with tabular data
is it correct or not is it true or false
so we all know that pandas is a python
package but was it why do we use it a
lot in data science is it to deal with
tabular data is it to deal with images
is it used to deal with text is it used
to deal with all of them but if it's all
of them then it's it is used to deal
with tabular data so is it true or false
[Music]
all right so we have 20 answers that's
really cool and we have 90 for true
which is the correct answer perfect so
pandas is used to deal with tabular data
we have this great uh
yeah we can display our table uh really
fancy in our notebook and we can see all
the columns we can see all the features
what are the what we want to predict etc
so it's really cool to deal with uh
today today with tabular data
in like sql just like sql because we we
can have like we can query different
things in our
tables we can get different things from
it we can select different rows etcetera
so it's really cool to deal with tabular
data using pandas
all right last question and then we
start uh talking about nlp
okay if you can go inside again one last
time
and the question is in linear regression
we try to
minimize maximize nullify or change the
least square errors
of the model to identify the line of
best fit so is it to minimize is it to
maximize nullify or change
just give us your intuition your
knowledge about this so in a linear
regression we want to find a line that
fits the data
we call it the line of best fit because
it best fits the basically yeah it's it
is the line that best fits the data so
is this is this line when we try to
learn this line when we try to learn the
parameter the yeah the parameters of
this line do we uh minimize the least
square error or maximize it or nullify
it or change it
okay we are at 16 answers
can we go up a little bit to 20
come on
17 perfect
all right 19
just waiting for this 20
you know let's start
all right perfect i'm gonna show the
correct answer now and the correct
answer is to minimize so
usually the loss function we want to
minimize it and we use for that some
gradient descent or something like this
or some optimizer like this just to get
to the minimum to get to the parameters
that gives us the minimum of the least
square error and if you don't uh know
this least square error it's basically
the distance between all the points in
uh the true the true
the true uh values and the predicted
values so it's basically the distance
between all of these
all right so i think now we can start
with our machine learning in nlp
and one thing we can start off is what's
natural language processing so basically
we we know what is a natural language
a natural language is our language as we
talk so we have english
french arabic german we have different
uh languages and we want to process them
we want to have a computer that
understand it and maybe uh do something
with it so we have different tasks here
that i i i just cited a few here in this
slide but we have a lot of different uh
tasks we have sentiment analysis for
example in social media we can have um
yeah we can use it to know if this post
is uh
i don't know maybe if it's a hateful or
if it promotes love and compassion if
it's something that is hateful and we
want to reduce it and we we don't want
to show it we don't want to recommend it
or maybe we have test text
classification we we want to know if
this post talks about uh mental illness
or if this post talks about technology
and
security or anything like this
and then we have summarization this is a
cool a really cool task where we have a
lot really long text and we want to
summarize it we want to learn only
if it's a student he wants to learn only
the
the least the most important uh
information just before the exam and we
also know we have of course translation
where we translate from one natural
language to the other we have auto
completion so also completion basically
is what we found in all our
phone keyboards we we start by writing
the first letters of the words and it
suggests different things
uh we have intense detection where we
have two sentences and we want to know
if one is the cause of the other or one
is the consequence of the other so
basically we were doing this when we
were young in school and we would we
thought what if a computer can do it and
it will understand like the meaning of
each sentence and to get what's a
consequence and what a cause so
basically this is what intent detection
is and of course we have i i just missed
the explanation of two uh tasks here and
it's on purpose so we we did i didn't
explain what language modeling is and
basically as a first explanation
language modeling is to predict the next
word in a sentence so if i want if i
have a sentence um
okay natural language processing is what
we are
what we are and i stop here maybe uh the
machine
yeah the model will predict the next
word so it's gonna say learning what we
are uh dealing with or something like
this and of course we have a question
answering which is the task that we are
trying to solve here today and what's
what is uh question answering we're
gonna see it of course in a slide but
basically in one sentence we have a
question we want the machine to answer
it based on all the data that we gave
that we gave the machine and of course
there are plenty of different uh tasks
that you can think of here if you want
just send us some examples on the chat
and we're gonna discuss it
uh
can you let me know if you're hearing me
because uh jean paul de long
uh i am not getting any audio okay
you're getting okay perfect
okay perfect thanks
perfect
okay and then are you presenting your
desk i am presenting the slides
uh can you just confirm that you're
seeing the slides and you're on slide
three
okay okay
most of you are seeing the slide
carolina can you try to go
uh to disconnect and then connect again
maybe it will work
okay i can see the diagram perfect thank
you all for answering and then we have
some question language modeling is it
different from predictive analysis
uh
okay predictive analysis is
mostly
is mostly used for from data analysts
when they want to have like a direct
answer but here in language modeling we
want to create a model that really
understand the language and that can
predict any a next word in any context
and then what's the difference by the
way between auto completing and language
modeling okay that's a great question if
you if you see in your keyboard in the
phone keyboard if you start just one
letter it will try to also complete that
letter also complete the word
and that's what that's what's happening
in autocomplete
but in language modeling we don't get
this instant from one letter we try to
fill the blank but we try to fill the
sentence with other words it can be seen
as part of autocomp completion but
language modelling is much deeper than
that because auto completion for now in
uh in our phones for example doesn't
really use deep learning
and it's also you it only uses deep
learning as a keyword spotting just to
uh check which word is is used in this
in
which words you type you typed and then
uh based on that it will suggest other
things based on what the users are doing
but then in mod in language modeling we
are trying to have a model we are trying
to have a machine that will
automatically create its own text for
example generate its own
text and generate its own script
all right
uh let me know if you have any other
questions and i'm gonna continue here
hello i just heard someone yeah hello
yes so so i just want to know means
language modelling is more of a
probabilistic method
more
is it
it is based on a probabilistic method
but we are going to
learn it via a mod uh via deep learning
so it's true that the all the in machine
learning uh of course if we have a
sentence we have a probability to have
that next word for example if i have uh
if i have a noun i can count i can
calculate the probability of having a
verb after that noun or an adjective
after that noun and then based on that
probability i can predict of course
these probabilities will help us which
is used in globe for example globe use
occurrence and probabilities a
probabilistic model to predict all these
but then we are going to talk about bert
and burt uses deep learning to solve
this problem
uh is that answer your question
yes yes thank you
okay and then uh okay just a glimpse of
what's machine learning so machine
learning is an approach to achieving
artificial intelligence as we said it's
a subset of artificial intelligence uh
through of course experience we are
learning from experience to find some
patterns in a set of data
so for example before machine learning
this is an example to catch a spam
so what we are what we were saying if
the email contains the word free maybe
in capital like this
then it is categorized as a spam else we
look for more keywords uh i don't know
maybe 100 percent or all the words that
are in capital 70s with font size 70 and
in red or something like this something
really flashy and fancy uh we consider
it as a spam or other keywords of course
but using machine learning what we are
going to do we are going to write a
program that learns these patterns
with it from itself so we are not gonna
specifically say okay this is the
condition to have a spam we are gonna
give it a bunch of different emails and
we are gonna say okay these emails are
spams these emails are not spams and
it's up to you it's up to the machine to
find what is the pattern or what is the
condition that makes these a spam and
these not to spam so basically the
algorithm will look some will look like
something like this so we will have try
to classify the emails that we gave you
calculate the error because we know each
email is a spam and which email is not
and then we adjust based on that and we
repeat that a hundred of times our
number of epochs this is what we call
e-box here so we repeat that a number of
times until we learn until our model we
are satisfied with the result of our
model or until we minimize this
calculated error
so basically this is the difference
between what we are going to do with
machine learning and what is explicitly
done before machine learning using
explicit conditions
and then
what i said is we are going to start
with how to deal with text we know that
text is not um yeah we don't have like
numbers in text of course we treat text
as a language so how can we transform
these texts into vectors of numbers so
that we can deal with them using in our
codes
so one thing we have is let's say one
easy or intuitive way to do it we have
the vocabulary we know that okay i have
a vocabulary i have a dictionary of
different words and then i will assign
an index to each word so let's say have
is has an index of one a an index of two
word one is zero and i is 345.
dog is 1760
and then we we give like different
indexes to each word in our dictionary
if we have a hundred thousand of
different words we would have a hundred
thousand indexes
and then if i have a sentence like this
i have a dog i can create this vector
they are just by replacing each word by
its index so let's say i becomes 345
half becomes 1 etc
so this way i can transform each
sentence
into a vector of numbers so one question
i have here
is can you intuitively think
of a reason why
we can't use directly this vector and
this vector is not good can you think in
intuitively why this factor is not good
of course here each sentence will have a
vector of numbers so our goal is done so
we would have um
we would have like numbers in each uh
vector
numbers for each word so we would have a
vector for the sentence so the goal is
reached but
exactly so farian just said the correct
answer there is no semantic here
a sparse matrix why do you think it's
parse here
so we are replacing each word with with
each uh with its index so we don't get
really sparse mattresses but we we would
get sparse mattresses if we were if we
were to use one hot in cotton
so one hot in cotton is another uh way
to do it and in this case we would get
like a sparse mattress
but here we would we wouldn't have a
sparse matrix but exactly as fabian said
we would we would have no semantic so
that means if for example if i have uh
let's have two words that have the same
meaning okay if i have a verb and it's
past tense these these words have like
some semantic between them so they are
similar they they have the same meaning
except one is in the past and one is the
infinitive or one is in the present for
example
and these words i want my model to know
that this one is the past form of this
one and when to use the past form and
when you not use that past form
so if i want to do this and if i
classify my vocabulary like this i may
have like okay one the verb will be as
indexed one and the other one uh the
past form will be index 2 or will be
index 1000 or any number
so that means that the verb and its past
tense won't have any meaning between
them
and we need to find a way of course to
have meanings just to keep these
semantics together and to know what's
happening there
okay and this is where word embedding
comes into the play so what we want to
do we want to add some meanings to each
word we want to have some vectors that
represent each word and that has some
meanings inside so what we're going to
do is that we're gonna create so of
course we're gonna transform it to this
kind of vocabulary vector we're gonna
say okay i is the 345
word in um in my vocabulary have is the
first word in my vocabulary but i'm not
gonna present them like this to to the
model i'm gonna learn some embeddings
or some vector to represent each word so
i'm gonna have a vector for each word
and this vector will hold some
information that relates to the semantic
relates to the word itself etc
okay so ultimately what we want to get
and work back here is one of the most
promising uh
word embeddings used
so i'm i'm gonna start by the conclusion
and then get to the how to create word
to vec
so basically ultimately what we wanna
get is we wanna have the same relation
for example the same distance between
the two vectors of men and women and
king and queen because we know that the
relation is the same so we want to have
the same distance we also want to have
the same distance between walking and
walked swimming and swam because one is
the past one is the uh
present
and then we want to have the same
distance between each country and its
capital so we want to have we want to uh
we want the word embedding to hold some
information about the semantic
and one way to do that is to create some
machine learning model or deep learning
model
that learns the word embedding and then
use this word embedding in our model
that learns to do language modeling or
question answering so we have a previous
step a previous deep learning model
that will learn the word embeddings and
once we have the word embedding for all
our vocabulary so we would have a vector
for all these words in our vocabulary we
can use these to give our language model
or our question answering model to train
our uh yeah to train these models
now in this situation what we're gonna
do is that we're gonna learn we have to
kind of uh
types of word to fact so we can learn
the embeddings from a continuous bag of
words or from a skip gram i'm gonna
start by explaining the continuous back
of words but they are pretty similar
just in terms of inputs and outputs they
differ in terms of input and output
so basically in continuous back of words
what we are trying to do we are trying
to do a language modeling so we are
trying to predict the next word so we
give it i have a and then a pad and this
pad will reflect what we want to learn
and the output will be dark because we
want to uh complete this sentence so to
complete this sentence i will say i have
a dog and the embedding of dog what
would be the vector that represent the
embedding of dog would be the uh the
weight vector of the path from the
padding i have a pad so we would of
course in a linear regression we would
learn some parameters and these learned
parameters will represent the word dog
and similarly skip gram in skip gram as
i said the difference is only in the
input and output of the models so here
we would give it the what we want to get
we would give it the name the word uh
that that we want to search for the
embedding for so we'd give it dog and
then the output would be the sentence so
we reversed the input and the output so
we would give it stop we would also give
it like a length uh a specific length so
we want all our vectors
regardless of the length of the word to
all be four for example
and then we would give it like four
times dark and then
using the linear regression we would
learn something and then we would get
here i'm sorry i'm saying linear
regression but it's not linear
regression it's actually a linear layer
here it's different node that are fully
connected
and then we would have to uh
pre i have a pad and using this doing
this for all the vocabulary in our word
and for all the uh the sentences that we
have in our uh
in our data set then we would get to the
parameter then we would learn the
parameters here and we would give these
parameters to dog and basically dog
would would get this vector
okay i'm gonna stop here and ask for
some questions
we can go
what is the cost function for what to
wake
yeah so the cost function will be like a
direct class it's like a classification
because we know what we want to get
and it will be like a supervised uh way
of learning
so means it is i'm not able to
understand the nature of the data set if
you are saying supervised so what is the
label and
what is the data what is the data and
what is the target
yeah so we have the data the data set
will contain different sentences right
the data set will contain all the
sentences that we want if we have like
um all the different texts not only the
sentences but we are going to uh
tokenize them so we we are going to get
different sentences so the data set will
contain different sentences and we will
take we will uh mask different words in
each sentence and each time we mask
ascend a word we try to predict that
word so we have the label we have the
masked word and we are trying to fill in
that masked uh word in the sentence so
here i put like the pad in i have a dog
but the pad could be put like we could
put the pad in half and we would have
ipad a dog and we would try to
find the
find the word embedded for have
is it okay
so okay so you mean to say so suppose
the sentence is i have a dog
and so the data set data data set will
have a data point where i have
pad dog
sorry ipad a dog
and
exactly
okay so this is how you form the word
embeddings
yeah exactly
or is this is it is it exactly how word
to wake works
yeah this is exactly how word to fact so
where to back is the skip gram version
okay
so how do you decide means you to
randomly pad some words so like i have a
sentence which is a five letters
five words
so i can have five data points from that
sentence
so is there any some heuristic to decide
which words
which first word second but which word
to pad and all
yeah so
for example in bert we mask 15
of the word so they have like some
probability to say okay i'm gonna mask
uh 15 10 of the each sentence to create
some more to generate some more uh data
data points
okay okay okay okay thank you okay
so so means that means just to summarize
that means that
this entire training the the goal of the
trading is to uh learn uh predicting
someone but uh but because of this
training what embeddings also get
learned
exactly
this is exactly what's happening so we
have this go the training goal is to
predict the word but then we get the
word embedded from inside the model
okay okay good
okay
okay i'm just gonna stop presenting
because i cannot switch to
uh the visual code so maybe i'm gonna
present my entire screen like this
all right uh can you see my entire
screen
hello
oh yeah okay perfect
okay perfect
okay so i'm just gonna uh use the word
to vac models so what we have in the
literature and what you can find on
github you can also find them
in the literature but basically i'm
gonna send you the link on github uh to
find the models so
we are not going to train the skip gram
uh word to vac model we are just going
to use it and try to get some embeddings
from it so basically what you get is uh
you can try to load your model and
load your vocabulary so these
models were trained on wiki text wiki
text is a data set that contains
different wikipedia articles and
it's really rich because it contains
different words different context etc so
we can learn uh we can really learn from
each word like a really good embedding
don't forget that we also have some
words that have
um
similar
different meanings but similar uh
writing so
or spelling so we would get like
the same embedding but we would
try to
increase the the values of each vector
or the word embedded will hold different
uh different forms or different
semantics in it
okay so here i'm gonna load my models so
like this my models in on on the folder
on the waits folder so i'm just gonna
load my model and my vocabulary the
vocabulary of course is just a list of
all the different words that we have in
our data set
and then we're going to extract the
embeddings of the first model layer
because this is where we hold the
embedding
and then we are going to normalize them
and basically we would have uh 499
embedding with 300 and the
the dimension of each uh embedding is
300.
so i think you can find in the
literature different word embedding with
different dimensions if it's required by
a model but 300 is fine for us
and then we can visualize using tsne so
tsne basically is like a dimensionality
reduction uh
dimensionality reduction technique just
because we don't we cannot see 300
dimensions of course we will just reduce
it to two
dimensions just to plot all the
different words that we have in uh this
data set
so i guess we would get this
okay
and we would get different uh so all the
different words are here marked society
valued awarded charge etc
and all the numeric letters uh i printed
them on in green
so if you see an and a numeric value
like
which is a date or maybe we can see zero
here we can see 52 33 etc so we can see
different i'm just gonna pop to the chat
and see if there is any question okay
perfect
yeah we can see different uh numeric
values that are in green and all the
different words are in black here but we
have like a variety of different words
and different uh
yeah and if different words in our
dataset so what we are going to do is
that we have this function which gets
the top similar word so we are going to
verify what we did uh previous what we
claimed previously we we said that okay
we wanna have the same distance or we
want to get similar embeddings for all
the countries similar embeddings for all
the different uh capitals etc so maybe
what we're gonna do is that we have this
function
great stop similar and basically what we
are doing here is we are computing the
distance between the different
embeddings and the top ones we are just
going to print the words corresponding
to this uh top similar word
we are also going to print the
similarity so for example if i give okay
for words and similarity in get stop
similar this function will return a
dictionary and the dictionary will uh
the keys of the dictionary will be the
words and of course it will contain the
similarity so by how much we are similar
okay
okay now questions perfect so i'm gonna
just give it like the word germany and
i'm gonna try to print all the different
uh
yeah similar words and we have like
france italy netherlands which are all
uh close neighboring countries to
germany which is cool that we have this
kind of semantic
and then we have europe here
uh so that's cool
do you want to try any other word here
to get the similar words in the
dictionary
i'm just gonna check the
chats
okay
okay mother
let's go for mother
oh this would oh maybe it's because of
the
yeah it was because of the m
capital m everything is in
low level low letters so
the capital m is removed okay
we have father
weird we have husband at first and then
father daughter wife brother parents
okay that's cool childhood
india
all right
all right independence empire crochet
which is a country but indian
of course all these words depends on
which what kind of text we give to the
word embedding i don't know
if this model was trained enough on this
data set and it's if also this data set
contains all the articles in wikipedia
but it's cool to see like
what kind of uh words are similar to the
word india in the dataset okay
okay uh cannon i really don't know i'm
gonna send my code but if
i'm gonna ask and let you know okay
but i can send you the code of course
right and then we also have this cool
thing that we can do which is vector
equations so we can have like different
embeddings
and we can compute like okay we have
king men and women and we can compute
king minus man
plus women and we can do different
embeddings and then get to the top uh
similar words based on this
okay and then we get like king reigns
son women daughter
uh we would of course expect to have
queen here
when we do king minus men plus women
again i i didn't train the world to vac
and we would need to train it from
scratch but it would take like uh a day
in a decent gpu but
yeah of course in a cluster it would
take
much less but yeah
we would expect it of course in a good
embedding we would expect to get here
king minus men plus women we would
expect to get queen
uh it's a great uh yeah it's a really
great
uh
thing yeah it's more like related to the
word of course it's not it's not
similar because if we could if we get
similar because we are trying to learn
these embeddings from the context
so we would have like okay we have
mother and in the same context we always
have father so this is more like uh it's
not i'm i'm talking about similarity
because we compute the similarity
between the two embeddings so it's like
more a metric than uh similarity in
terms of semantic so
it's not yeah maybe it's not similar
word but related word and
more more specifically it's
words within the same context so words
that we found in the same sentences and
that we found related yeah related to
each other
it's a good remark thanks
all right let's go back to these slides
and we can go back to part two
so that was how we created the word
embeddings and what we played a little
bit with the word embeddings just so we
know that we can get that we can do some
vector uh some vector encoding we get
our words and then we can play with them
a little bit just to know
for example we can uh yeah we can do a
different operations on them and this is
what we are going to do uh yeah in deep
learning actually because we would we
would give all these different
embeddings and then we would get like
different
uh multiplications different uh
activation functions etc to get to the
output that we want to so we would
transform these and combine them in the
sentence etc
okay so now part two and in part two
we're gonna discuss what are the
different architectures that are used to
deal with these texts
so the first one i'm gonna start with
and it is a really important
type of neural network which is the
recurrent neural network
so
before transformers i know that now we
hear a lot about transformers but before
transformers we are we're using
recurrent neural network for texts or
sequence data in general so sequence
data is any uh any data that comes with
time so we have for example a video is a
sequence data because we have the first
frame second frame third frame etc as a
text of course is a sequence data
because we have steps we have the first
word second word third word etc
and we were using this recurrent neural
network so there are different types of
neural networks we the well-known
convolutional neural networks are good
for uh good for computer vision and
images although now transformers are
starting to uh
get really promising results in computer
vision as well
but let's not discuss that
uh
yeah recurrent neural networks are good
with sequence data because this they
have this kind of um
yeah they have this kind of recurrence
in the node so if i have this sentence i
have a dog of course we would get the
embeddings of each word as i said it's a
the word embedding is a preliminary step
before going to our language modeling or
question answering model and we are now
talking about our language modeling uh
model
so basically we would get the embedding
and then we would give the embedding to
a certain node
and this node would give us the the
first output h1
okay
and then the second node we would give
it the second word
but we would also give it this h1 the
result of the first node
and this is
this is yeah this is more likely
intuitively this is really easy to uh to
justify because if we want to predict
dog for example we would have to say
okay the node we would have to uh get
the information from i have and uh to
get to dog or maybe if we want to
predict have then we would have to get
the information from i and uh etc
but the recurrent uh the first recurrent
neural network we're like okay uh each
send each word in the sentence would get
the information from the first uh
then the previous words before it
so if i want to predict a we would get
the information from have which contains
the information from i etc
okay
so basically this is a recurrent neural
network because each each node would
have a recurrence uh each layer would
have like this recurrence uh in in it
because we would have the output of
another node inserted in it
okay
i'm just checking like the chat if there
is any question so please stop me if you
have any questions at any time
okay
and then these were of course as i said
if i was to predict have and i would get
only i it would be difficult for me to
get to get to have but if i have i and
the information of a dog it would be
easier to predict have and this is why
we also have bi-directional recurrent
neural network so we would have the
information from here to here and also
the information from here to here so
this is also happening and this was also
one of the models that was the most
prominent
and then inside this node we would get
like different types of operations so
basically the first ever recurrent
neural network we would only do a
hyperbolic tangent here so only one
activation uh layer and that's it
that was the first recurrent neural
networks and it worked uh yeah let's say
well but then there was like some
more prominent works of course and then
we have this famous lscm cell so the
lstm cell contains different activations
one after the other and
really in a a yeah in an adequate order
so that we get different gates and
different results
so basically i
so i just wanted to let to let you know
that we worked so previous to
transformers we worked on the inside of
this node trying to get more uh
more operations in it trying to get more
results and more information that goes
to the sec to the next node
okay thank you yes okay okay
all right
and then uh yeah of course the task that
we want to solve at first i'm gonna try
to i'm gonna justify why we want to
start with language modeling and
language modeling as i said is predict
the next word in a sentence
a cool test is a hugging face i don't
know if you know it or not but maybe we
can go inside and try some a bunch of
different text so one thing we can do is
this is using a language model
uh
gpt2
and we can use okay we can say
just give me a sentence give me a
starting sentence and we can ask it ask
the model to fill in the blanks
or to predict the next word do you have
any example you want to test
uh yes you can you can try uh we are
having a great learning experience you
want to predict experience we are having
a great
and
okay maybe learning and then experience
okay
so we have like curve in our community
and we have a great opportunity to do
that or experience with the with the way
i work and the way i work what okay
uh curve and learning curve oh okay
and i feel comfortable with the
direction which is oh this one is pretty
this one is pretty like
uh has like a a lot of sense we have
we are having a great learning curve and
i feel comfortable with the direction
okay
but we can try a bunch okay let's say
experience and then dot we can also ask
it to start the next sentence
uh
if we don't make progress for the year
we're going to be down
uh it's going to take a lot of hours to
get started on this it feels good to
have a great experience and a little bit
more experience what
experience
to enjoy working with the team to help
us develop better games okay
and just so you know this example and
this gpt-2 was like uh yeah it was like
deployed a few
years ago
so we can really like
uh
expect like more from gpt3 which is an
updated version of this model and expect
more from a real locally trained model
because these were uh trained to be
deployed on a server so maybe they were
trained with uh less
yeah with less number of epochs etc so
we we can expect a lot more from uh a
bigger model and a much more trained
model
yeah so i'm gonna just pause the link
here and if you have any uh sentence you
wanna test just uh feel free to play
with the transformers
all right so basically this is uh
language modeling so we predict the next
word in a sentence
and then uh yeah and then one thing i
didn't put a name on is self-supervision
so before i was saying okay we can mask
this word and then we try to predict
that word or we can remove that word and
uh yeah we can try to predict different
so we can create different data points
in our data set using one sentence by
removing or masking different word and
this i didn't put a name on it but
basically it's self supervision so self
supervision is when a model
creates its own data set so based on
different sentences we would give it
different sentences and then it will
train and create different version of
the data set try to fill in the labels
by itself etc
so here in bird which is the model that
we are going to use we are going to fill
of course this black box here it's just
a blue box here but we are going to know
what uh what is the content of what kind
of operations are applied in this model
but basically we have eye mask and then
a dog and a model we predict i have a
dog which is a masked language modeling
because the original language modeling
is only predicting the next word in a
sentence
so directly the next word of course here
in the example or in the in the
in this deployed transformer uh you get
dif a complete sentence because they
call the model different times so they
they they not they are not calling the
model only once if they call the model
only once we would get only one word but
you you call the model uh different
times to get a full sentence just for
the point and then once you get the
points you're fine
and then in the masked language modeling
we mask different or random uh
words in the sentence to create more
yeah to create more uh
more data points in our data set
another thing i want to say is why do we
start with language modeling we found
that if a model is trained with language
modeling so is pre-trained with language
modeling so we have a big model we train
it to do language modeling so to predict
the next word and then we just fine-tune
it for for uh question answering for
example or semantic analysis or any
other task it gives like really better
results because it kind of understood
the language and then we fine tune these
parameters to get to the right task that
we want to do
and this is exactly what we are going to
uh to do
so we are going to start with a model
that is already pre-trained on language
modeling a birth model that is already
pre-trained on language modeling and
then we are going to fine tune it
for the question answering task
okay
okay i'm going to just talk a little bit
here about self-supervision and how it
can be applied in nlp so we have the
masked language modeling that we just
talked about so we just mask a
percentage of the words in the sentence
and then we try to predict the entire uh
the entire sentence we try to fill in
the blanks
and then we have uh we also have another
case where we do a next sentence
prediction rather than doing a next word
prediction we can also do a next
sentence complete sentence so in this
case we have two consecutive sentences
in the text and we randomly remove one
of them so we randomly mask one of them
and then we have a gan-like
configuration which is generative
adversarial network uh so generative
adversarial network are the kind of
networks that are used for
um
yeah for example to generate new faces
to generate new data scenes in a game
for example generate something new uh
completely new
and we use uh
yeah and then we use this gun like
configuration to generate and replace
some random word in the in the in the
sentence and then ask the model to uh
predict which word was replaced
so we have a sentence i have a dog
and then i will generate a new word for
for dog for example and i would say a
german shepherd and then the i would ask
my model to know which word was replaced
by the gan model or not
just let me know if you still see my
screen please
okay perfect
yeah so these are the three kind of self
supervision that is used in nlp and self
supervision in nlp is really uh
really gives like great results and is
really used
every every time we want to train a
model
okay
so
one other thing i want to talk about is
okay sequence to sequence models we are
we are talking about the general
architectures of deep learning for nlp
one of the architectures that we studied
is recurrent neural network we just gave
an overview of it how it's uh
how everything is connected
another
really important model is the sequence
to sequence models
or uh transduction models and these
models contain two sub models the
encoder and the decoder
so let's say we have this sentence i
have a doc so each word has its own
embedding so i can create this mattress
here
and then i will i would give this matrix
to an encoder so this model can be a
recurrent neural network inside a
transformer inside any type of model
here that deals with uh text
and then this encoder will encode
everything and create one vector it will
like compress all the information in the
sentence into one vector
and then this vector is given to a
decoder the decoder also contains some
fully connected some recurrence some uh
transformers anything
and then it will output a new the same
sentence of course in meaning but in a
new language for example for translation
it can also output in the same language
but in uh for example if i want to uh
just paraphrase or change the sentence
or use some synonyms or anything like
this so i can also use it for this
purpose
or maybe summarize the sentence just to
get the summary here i have a paragraph
and i get to hear a summary of it
so this is what sequence to sequence or
transduction models are we have an
encoder a decoder the goal of the
encoder is just to compress all the
information in one vector and then the
goal of the decoder is to put the
put these information these most
important information into the right
format so for example translate it into
another language
okay
and then one other architecture
operation that is really used and
promising and it is uh it is the core
store stone of um
yeah of transformer models it's the
attention mechanism
so
one of the important thing okay let's
talk about sino let's words that have
the same spelling but different meaning
so let's say i left my laptop on the
left side of the desk
if i hold if i just do the embedding as
we said these words would have similar
or similar embeddings but then how my
model would get that this word
is completely different from this word
because this word means i leave from
leave infinitive of leave and then this
one is the left is a position
so we need to uh find a way to make the
model know the difference between the
two words
and also one of the other things of the
attention mechanism one of the problems
that the attention mechanism is trying
to solve is when we have a really long
sentence and i talk about uh
i i talk about let's say a name uh at
the beginning and then i refer to that
name by he or she at the end of this
sentence i need to know i need my model
to know that he or she referred to that
name which is at the beginning and
sometimes without the attention
mechanism we lose this connection
between the last word and the first word
of the sentence
so this is why we wanna this is what the
attention mechanism is trying to solve
and how it's uh how it's so how it
solves this problem is in the encoder
what you notice here is that we don't
have only one vector
but we have a matrix here and the matrix
will say okay the first word has a
certain relation with the second one the
second word has a certain relation with
the third one the fourth one and the
first one etc so it holds all the
relationships between the difference
between the different words in my
sentence and this way we can focus on
importance and uh words in the sentence
we can find some references as i said
the first word in the sentence that is
related to the last word for example and
we can understand similar words like i
left my laptop on the left side of my
desk
okay so this is the attention mechanism
and then it's it's a kind of okay
if you want uh some details here the
attention mechanism i just want to see
where can we get the recording it will
be in the youtube channel of codlabs
academy okay and then if you want some
more uh yeah more details here basically
the attention mechanism will get the
encoding of each word and then it will
compute some different uh yeah some
different vectors from the same encoding
and then using a soft max it will get
the probabilities of all the different
relationship between this word and also
the other words in the sentence so this
is like a really basic explanation of
the attention mechanism without getting
into the query value and keys
and then the transformers is the type of
model that is based only on the
attention mechanism so previously we
would have some recurrent neural
networks that contain the attention
mechanism but here the transformers came
and only uses
an attention operation with some feed
forward and some fully connected uh
layers
so basically if we see here the general
structure so on your right we can see
the general structure of uh the the
transformer model so we have on the left
the encoder this small part here can you
see let me use a laser here
okay so we have here on the
on the right the encoder and then here
we have the decoder
and in the encoder we have this
attention uh multi-hat attention which
is the attention mechanism where the
attention mechanism is applied and then
we have a feed forward add a normalize
is just applied to normalize the output
and of course we have a recurrent layer
here and then a recurrent uh yeah escape
connection here and the skip connection
here
okay and this encoder is applied n times
and then we get to the decoder and the
decoder will also use the output of the
previous uh words and also the output of
the encoder to get to the right uh yeah
to get to the output probabilities or
the right predicted words
so we still have an encoder and a
decoder and also this one was trained on
translation and also uh
yeah other transformer models were
trained was was pre-trained for language
modeling
uh one thing you can see also is here a
positional encoding so here we have the
input embedding so just get the
embedding from another network
and then we add the positional encoding
so one information that is missing in
the transformer because we are using
only
uh an attention layer and a feed forward
we are not doing a recurrent neural
network where each node gets the
previous uh information from the other
node we are just getting one layer that
can gets everything and then compute all
the different relationships so one thing
we are getting is one thing we are not
getting is the position of each word so
we don't know that the first word
which one is the first word which one is
the second etc and this is why we add
this positional encoding so some values
we change the values of the embedding to
also hold the position of each word in
the sentence
and then to get to the probability of
course in the multi-hand attention we
would get this kind of
attention uh operation and this
attention operation uses the query key
value so we would have three different
matrices for uh parameter matrices that
we are going to learn which are the
query the key at the value we multiply
each matrix by the input embedding so we
would get the
query key and value for each word
because we we have of course an
embedding for each word so we multiply
this embedding by these three matrices
we would get three uh three vectors one
for the query for the word one for the
key for the of the word and one for the
value of the word
and then we would multiply the query and
key and then uh multiply everything by
the value later with the
with the soft max and basically here the
query and key we get the query of the
word so we have the query of i
and i multiply it by the keys of all the
words in the sentence so i multiply it
by the key of i i also multiply it by
the key of half i also multiply it by
the key of a and a dot
and when i multiply it it's like
computing the relationship between the
two so between the query and the key of
all the different words
and and then this i will apply a soft
max on it to get a probability to know
what is the similarity between all these
different words or the relationship
between all these different these
different word and at the end i will
multiply it by the value to get my final
vector that represents this word so
basically this vector will hold the
information of
all the other
words with a certain probability so
maybe ninety percent of the first
embedding because it will hold all the
information by its uh of itself of
course every word is related to itself
and then maybe ten percent
for a dog or something like this
i'm just gonna check the chat
will there be certificate for this
section no i'm sorry there are one
this is a free workshop but then there
is certifications on the cod labs
academy
uh how is the attention mechanism
trained to know what to pay attention to
yeah so during the training we learned
this query key and volume mattresses
which are weight mattresses and by
learning these mattresses we would learn
the what's to pay attention to in each
word
okay
and the fact that we are creating a new
vector for each word
and this vector will hold all the
information from uh
from the vector itself from the free uh
the embedding of the word itself but
also from the embeddings of the words
related to it
uh is it clear asian or do you have any
other uh
okay so the attention mechanism is used
to
have like this
kind of
kind of computing the relationship
between all the different words so that
just we know if i has a relation with
this word uh have had the relation with
other words etc what is the percentage
of in this uh
this relationship but it's not
what's to pay attention to okay it's
just computing what is the relationships
between all the different words so i
want to have an embedded or a vector
that represents the word but also the
relationships uh between all the
different words
okay i hope it's clear and then uh one
prominent uh
model transformer model is gpt
so gpd was cr created in um
2018 i believe just after the
transformer the transformer model was in
2017
and then gpt came and just
modified the model a little bit as you
can see here it modified it by just
removing completely
the
the encoder part and just keeping a
masked multi-hat attention and just
keeping the
decoder part
and as you can see of course we can
apply this to solve multiple tasks like
classification entailment entailment is
uh the relationship between a premise
and a hypothesis
similarity between two sentences
and multiple choice answers so for
example if we have a question we want to
answer we we have multiple choice which
one is the right
answer to that question
now bert came also after
uh transformers of course and what they
did is they removed the decoder
completely they just kept an encoder and
with the repeating layers of course
and they changed the tokenization
process so they have like some and pro
tokenization is basically where we have
different texts with different sentences
and we uh extract all the different
words in the center so what are we going
to do with the punctuation for example
uh are we gonna remove it are we gonna
encode it into some sort of um
token or uh encoded into some sort of
word etc so this is the tokenization and
they use the special tokenization of
bird and they also use this only encoder
model
okay i'm going to stop here and check if
there is any questions
don't understand how training results in
calculating
v
k and q matrices do you have any
references that explain how this
training is done yes of course i
will
i believe that jlmr
attention
blog is one of the best illustrated
uh
i'm gonna send this
so basically you have these three
mattresses for the query key and value
and then you have okay here the input is
one sentence thinking machines you have
the embedding for thinking you also have
the embedding for machines and then you
multiply x1 by wq by wk and by wv so you
get the query for the word thinking the
key for the word thinking and the value
for the word thinking and the same thing
for machines
and then what you're going to do is that
you're going to multiply so for the
query you're going to multiply the query
multiplied by the key of thinking
but you're also going to multiply the
query of thinking by the key of machines
and you're gonna get two different
values and basically these two values
will hold the information of the
relationship between the two words
and then what you're gonna do is that
the you have the query and key you have
the query multiplied by the keys you
will divide it by a just to get some
dimensionality normalization and then
you will apply a soft max when you apply
a soft max you will get a probability
and this probability will say that
thinking
is 88 percent related to thinking
and 12
related to machines
and then you would multiply these
probabilities by the value of
thinking so basically you have this
value vector that you created previously
you would multiply 88 percent multiplied
by v1 plus 12 multiplied by v2 to get z1
which will represent uh the the thinking
uh vector
and then you do the same thing for
machines
and this way we train of course and of
course we get this for each sentence in
the in the data set we get to the
results and we we calculate the loss
function and we uh yeah using gradient
descent we adjust the weight mattresses
for the query and key and value to get
to the correct
way to calculate the relationship
i don't know if it's clear like this but
you can find all the details in this
blog i shared on the chat
okay is there any other questions on the
architecture of the models
i have seen v value key and query matrix
multiplication explanations but i don't
understand why
is there some intuitive explanation
so basically
the intuitions behind the query and key
uh value mattresses so okay let's say
you have we completely changed the
context let's say you have a page ranker
you have a search just like google
search or bing or anything okay you
wanna search for one thing this is your
query what you write in the input is
your query
and then you have a bunch of data set
where it contains all the different page
uh pages with all the different
information for the for these pages and
you can consider these as as the keys
and you want to compute the similarity
between your query and these keys in the
data set so you would multiply the query
by all these keys to get the similarity
but but then the result will contain all
the
uh yeah all the information
be uh from this similarity from this
multiplication you would get to yeah
from this multiplication you would get
the probability of having this page in
the results and then you would combine
all these to create your results and the
results that you're going to print to
the user
so basically this is what's happening a
little bit here this intuitively you
have the query this is what you are
asking for and then you have the your
different keys this is what you are
having your data set and then you would
compute the similarity between all these
to get to the uh
yeah to get to the to the pages that
you're gonna put on the on the search
results
is it clear for the architectures
okay i think i'm gonna continue and if
you have any questions just let me know
okay
one of the things that we can see from
the deep learning architectures over
time is this idea of bigger the better
which is clearly true
uh so one of the things that we have now
is gpt3 which is three billion in number
in the number of parameters we have
megatron which is eight billion in the
number of parameters etc we have really
big models and the numbers are really
going bigger and bigger now microsoft
has an uh has also unmodeled which is 17
billion which is really huge and can't
be deployed on any smartphone of course
and only on this server and a cluster of
different
um yeah a cluster of different gpus and
tpus and everything so it's really but
in terms of precision
training a large model for a few epochs
and then compress it is then is the new
best practice so before we were training
a smaller model
and then for a really
large number of epochs and then we would
uh fine-tune it and then compress it uh
a lot
so but now uh we we are trying to train
a large a really large when i say large
now it's 8 billion 17 billion the
numbers are really going up
for a few epochs only and then we try to
compress it to make it smaller to shrink
it by quantizing it or doing some
compression techniques
or optimization techniques to reduce the
size of this model and uh while keeping
of course the precision of that model
let me just go back to the uh
questions okay there are no further
questions okay let's go to the part
three and explain what's question
answering with just two slides and then
we go ahead and create our uh use birth
embeddings and create our question
answering model
yeah so question answer in question
answering you would get a a question
like this and a context
so you would get okay the question for
example is the atomic number of the
of the periodic table for oxygen and
then you would get an a context
a a certain the context is a paragraph
that should contain the answer to the
question
and then you have here in the context
oxygen is a chemical element with symbol
o and atomic number eight it is a member
of the collagen group etc
so basically what we want we want our
model to detect this atomic number eight
we want our result to be uh our answer
to be found in the text
and you can find this in the stanford
question answering data set so squad 10
uh 1.0 or 2.0 are really used in the
literature so you can uh it's a free
data set so you can download it and
explore it via the this link
and you have different types of question
answering so you have the extractive
question answering that we are going to
uh
code in this um yeah in this uh
workshop so basically you extract the
answer from the input and the input is
the context
and the yeah and the input can be like a
web page it can be anything really
anything and the goal is it it reads
this paragraph and it knows that it it
is looking for the atomic number so it
goes to this and print eight
and then you have open generative
question answering so here the model
generates new text from the context so
it is true that you have this context it
detects that you have atomic number
eight but it doesn't only give you okay
atomic number eight or or it doesn't
only highlights this it will give you a
full sentence like the atomic number of
oxygen in the periodic table is eight
this is like a generate a new um a new
text from the context
and then you have closed generative
question answering and in this case you
have no context that is provided so you
don't provide this you just provide a
completely um
yeah the the question answering model
will completely generate uh an answer by
the model so if you say okay maybe this
is this is the case in chat bots for
example where you ask it uh are you
funny or are you uh are you a robot or
something like this and then the chatbot
will completely generate its own answer
by its own
okay i'm gonna just check if there is
any other questions okay perfect and
then we have okay what we are gonna do
is here is that we are gonna use this
birth model but we are also gonna adapt
it to question answering and bert in
their paper in their original paper they
suggested to adopt it uh in different
ways so to to do some question answering
to do some entailments to do some text
classification etc but basically what
you have you have your two questions you
have the question and the context which
is a paragraph and as input of course of
the model and then you would have the
outputs here at the end of uh
of your output vector so at the end of
your output vector you would get the
final sentence because these other uh
outputs corresponds to other uh
yeah other tasks so for example c here
correspond to the class in the text
classification and then t1 to tn
correspond to another sentence if i want
to do something like okay i want to
predict two sentences or something like
this
so this is this is called this
corresponds to another task and then the
last uh sentence in our results will
correspond to the answer in question
answering and this is why we can use
bert really easily because it was all
already trained for language modeling
and we are just gonna fine tune it for
question answering for this task
all right so i think we can go ahead and
go to the code but if you have any other
question before i go just let me know
okay what about the question answering
system that depends on textbooks what
kind of domain are these systems open or
close it depends on the answer actually
if the answer if we want the answer to
be completely generated oh you you mean
from textbooks okay so it's it's an open
uh it's an open question answering so it
means that okay we have some textbooks
we want to find the answers in these
textbooks somewhere
but we don't want to generate a new uh
answer from scratch like uh i i i may
ask so in the closed question answering
i may ask the model okay what is the
atomic number of oxygen and it may
answer
uh
13 30 etc because just because i said
number so it detects that i need a
number but it doesn't know because it
doesn't have these textbooks so it
doesn't know that the atomic number is
eight
so if you have textbooks and you want to
find the answer in some somewhere in
these textbooks then it's open question
answering okay
okay
uh any other question
before we go to the uh final code and we
would okay
so while if you have any question please
ask otherwise i'm gonna start by
explaining what i'm gonna use in the
code so basically i'm gonna use uh first
are you seeing the collab
the notebook correctly or not just let
me know if i need to zoom in or do
something else
i'm gonna check the chats
okay maybe i can zoom in a little bit
okay
okay i think like this is fine okay so
what i'm gonna use is i'm gonna use two
special packages so the overall model
uses uh pie torch which is a deep
learning framework but i'm gonna use two
different packages that are helping to
create the models etc so we are not
going to code uh bert from scratch we
are not going to train it we are just
going to fine tune it train it for three
more epochs on the question answering
tasks
so we have the packet transformers so
transformers is a package that holds all
the different uh transformer models for
nlp and also all the different uh
weight pre-trained weights uh data
points so that you can take the file and
call it in a pre-trained way etc and you
also have this data set package and data
set really easily load uh squad version
two so we cannot directly use it and
it's really easy to use it to load the
data set create the labels etc
and then we have some other uh packages
that we are going to use just to display
the data set display some images etc
okay just before we start we are gonna
use squad we are not gonna use version
two we are gonna use uh version one
we could use version two easily just by
changing this boolean to true
and then it's just a matter of uh size
of the data set and training size
because the training would take a lot
longer
and then model checkpoint we would use
the birth base and cased
this is the just the name of the
checkpoint of bird in transformers
and then we just set the batch size to
16.
uh of course we load our data set using
this function which is directly from the
data sets package
okay and just here if if we put this to
true then it will load squad version two
as it will do squad version one
okay
and then we can check our data set so
the data set would contain some features
we have the train uh par the training
part and the validation part and in the
training part we have different features
per uh per data point so we would have
the id
we would have the title of the question
we'll also have the context as i said
the context is the paragraph where we
are looking for the answer
we'd have the question and of course the
answer
there may be different answers per
question so a number of answers is
normal
okay i'm going to check if there is any
question okay and in the validation
would have the same thing of course but
the number of rows is pretty different
so we would uh we have 87 000 around 87
000 uh question answers for the training
set and 10 000 for the validation set
we can take an example here uh
okay let's check the answer uh the
question to whom did the virgin mary
allegedly appear in 1858
in lords
uh friends
and then you can uh have like the answer
where's the answer okay the answer start
here stand bernadette uh superiors
okay and you can find of course this
answer in the text but we just wanna uh
yeah we just wanna extract it from the
text and put it here so it's in here
okay
and then yeah here i'm just gonna print
random elements from the data set i
think i uh
i'm gonna start the session again but
it's fine okay basically we're gonna
just uh print them in a
fancy way using pandas just to get all
the random uh random data points from
this data set but it's fine
okay uh oh i know why it's taken long
because i started the training before we
started the
yeah we started the workshop just to
get the training ready because it takes
a lot of time of course i i believe it
didn't finish yet but we are hopefully
it will finish when we arrive here okay
so basically this will want uh yeah this
cell won't run until uh the training
finishes okay it's fine
okay no question okay and then uh auto
tokenizer so also tokenizer uses the
birth tokenizer to tokenize everything
as i said tokenizer means dividing the
sentence into words and then getting the
inputs the getting the ids or the
indexes of each word from the vocabulary
or from the dictionary
so we
get the pre-trained tokenizer because
bert has a special tokenizer so we get
the pre-trained tokenizer we use it for
example in this sentence to i have a dog
and we get the output of this i have a
dog in this form
so we get the index of i have etcetera
and each sentence is padded so that you
have
a different kind of
length for each sentence
so here normally you would have like
different
yeah you would have different paddings
for the start of the sentence the end of
the sentence etc
yeah just uh just another basically you
have just another index here to
represent the start of the sentence and
another index here that represents the
end of the sentence and in between you
have the four words i have a dog
okay and then you just give the maximum
length the maximum length of a feature
so what is the dimension of each
embedding and also what's the doc stride
basically the doc stride is what are the
distances between two different
what's the collab link i'm gonna share
the code uh at the end of the workshop
i'm gonna ask them to
send it to you all
by email
okay
and then here we just like compute the
length of different uh results of the
tokenizer on different questions and
different contexts
uh just to check if we are doing
something special
okay and then here we apply everything
to uh all the questions and answers in
our uh in our in our data set basically
okay so this is for the tokenization i
don't want to go uh
a lot into the details because
we are almost running out of times so we
need to go to the model and how do we
use the model
okay
just checking if there is any other
question
okay and then basically what we are
going to do is to call this auto model
for question answering
and then we would give it the
pre-trained of course these pre-trained
is from the birth model and this bird
model was trained to do language
modeling and we are just gonna training
it fine tune it a little bit more for
the question answering tasks
and what we are gonna give it some
arguments because we need some training
arguments so we wanna give it okay
what's the train evaluation strategy
when do we stop we stop at certain
number of epoch what's the learning rate
what's the batch size the training batch
size of the evaluation batch size per
device so if we have like different
devices in our cluster we can use like
okay um different batch sizes for the
device so if i have like two gpus and i
have 16 in the bath size i could use
eight and eight in each one each one of
the gpus can use eight if they have
different performances then i would
maybe use like 12 and four depending on
the performance of the gpus that i have
and then the number of training ebooks
i'm not gonna put a lot it's just three
just to fine-tune the model on the uh
question answering tasks it's already uh
yeah we already trained the model for
the model uh language modeling so the
model is already trained we just want to
put the right weights or modify a little
bit of weights in order to uh
yeah in order to conform to the question
answering tasks and then the weights
decay and push to help is
is just for the hugging phase of the
transformer package they allow us to uh
push to the hub and get all the um
get all the results in their hugging
face uh website so i guess you i i'm
gonna write here
oops i'm gonna write here uh
the websites so that you can go inside
and just check it
so if you have like uh a website uh
yeah an account here you can check the
results on the hub after the model is
trained which is pretty cool
yep
and then yeah this is what we have here
and then of course we have a data
collector this is also like
uh the from where do we get this data
and uh just given uh what are we gonna
do with the data etc
and then we have the trainer so the
trainer we give it the model the
argument for the training the training
data set the evaluation data set and the
tokenizer
and this will and then yeah and then
just doing trainer.train
will run everything and will launch the
training and everything will run uh
hopefully smoothly
so basically we would get all the
results here uh the training is
almost done but it's already one hour uh
yeah one hour of training already for
three ebooks as you can see it's really
time consuming don't forget to put here
in the run time you change the rent time
to gpu as uh
to use the gpui training otherwise it
would take like days
in in the cpu so don't forget to change
it and yeah it takes a lot of time just
to fine tune the model so just imagine
how it takes if we were to train from
scratch
oh uh top strike is what we are using to
okay so dark stride is given to the
tokenizer just to know what's the length
of the length between two sentences so
for example if i have like um
multiple yeah multiple spaces or
multiple entries and go beyond that
space so i'm just gonna say okay
you find the first sentence and then you
go back and you take a lot of uh
yeah you take that number of sentences
within the same one etc so it's just uh
it's yeah it's just a
a hyper parameter that is given to the
tokenizer to say what is the stride
between two different sentences in uh in
the data set
so if i have a context what is the
stride between these two different
sentences
[Music]
all right it seems that this model is
not gonna finish on time but maybe we
can stop it
let's hope
okay let's just stop it and see
it did finish one ebook so at least we
have that
and it is running the evaluation so let
me just save my mobile like this
of course the results won't be good but
okay we're gonna uh
yeah i think we're gonna do without it
so basically here i'm saving the model
weights
into you just give like a name for the
weight of the
yeah for the file weights uh qa workshop
as i see even saving the model takes a
lot of time because it needs to go
in the model and save all the different
ways for all the different layers and
bird is pretty large
okay so basically once the model is
saved what you can do yeah it's upload
the pythos model okay it also sends
everything to the hub so that you can uh
check the hub again as i said you just
log into the website it's on the top of
this notebook i'm gonna send you this
notebook at the end so you just log in
here and then you get all the
information is your model trained or not
how how
does it perform etc so it's really it's
really cool to have this uh yeah all the
logs somewhere
but it takes time of course
okay and then what we are gonna do is
that we're gonna take this model and
just test it on new data set so
basically we're going to take the
evaluation data set get eval data loader
and then we are going to just
get the output for all the different
values in our evaluation data loader
and of course we're going to check the
shape if they are in the right format if
you have the same number of rows
with the the evaluation validation data
set
we're also going to go over uh over all
the results over all the outputs and
just print them in a correct way like
the score and the text of the uh answer
like how it is perf uh yeah what's the
answer basically because of course uh
don't forget here the answers are still
tokenized so we all we we have numbers
so we need to go back to the vocabulary
and convert the answers as well because
the answers are just uh yeah just a
bunch of numbers so we need to go back
to the vocabulary and get the answer in
a text format and put it here and
basically this function is what we do we
go into this uh valid answer um
yeah valid answer array and the array
will contain different uh
yeah different dictionaries so we fill
in the blanks in that array
yeah and then the row prediction on the
validation set we do the same thing and
we can compute of course the um
yeah we can postpo post process the qa
by computing for example the score and
everything and saying okay our model uh
performed this and this this accuracy on
the question answering on squad version
two or one as as you want
okay one thing i wanted to point out i
don't know if it's saved or not yet okay
not yet one thing i wanted to point out
is that uh
we can also have like a new answer
probability we can add that just to say
okay i want if the model is not
confident at
90 percent i want to answer the model to
say okay i don't know the answer to that
question or something like this so
that's pretty cool and it makes the
model uh know that okay sometimes you
can you can get it wrong or you you may
not know the answer
and we have this kind of confidence pro
score so we can know if our model is
confident enough or
from yeah we of the answer or not so
that's pretty cool
okay
so
i think this even the saving will take
too much time it's still on 48
so we're gonna stop here
and i'm gonna send you the notebook as
well as the slides and you can go
through the notebook and if you have any
questions please ask i'm gonna send here
my email again
okay i'm gonna put here my email again
yep can you also share the recording of
the question
oh the recording will be available in
the youtube channel
okay what is the youtube channel
name yeah so you just just look for
codelabs academy
thank you thank you
okay no problem no problem
all right
have a nice day
how is fine tuning captured in the model
does it change the vq mattress value
exactly so when when training the first
time we change the value key match
key and query mattresses in the
attention mechanism so we change all the
weights of the model the model contains
the attention and also the fully
connected so we change all the weight
mattresses and the value key and query
mattresses are weight mattresses so we
change all these in the training we
adjust them to the correct values for
the
language modeling so for the model yeah
the language modeling and then we want
to fine-tune it for the question
answering so we start with the right the
adjusted values and we train again so we
adjust for a small number of epochs here
we made we we put three so for this
small number of epochs we adjust for a
little bit more in order to fine-tune
these numbers or these values for the
question answering tasks
there is no neural network where it's
being adjusted also yeah of course so uh
yeah of course so after and let me just
share this screen maybe uh
yeah maybe i missed it in the
explanation so here in the encoder we
have an attention mechanism that is in
orange and we also have a feed forward
so we also have a fully connected neural
network after the attention so we apply
the attention and then a fully connected
attention fully connected
and these are the weights that are
adjusted during training
is it clear asian
still wondering how training works
so training works as as with all other
different networks so if you see in
in a standard neural networks in a fully
connected what you do is that you go you
do a feed forward you do a complete fee
forward so you get to the results you
compute the error you calculate the
gradient of each error
for each value of the weights and then
you adjust that value
uh using gradient descent for example
and it's the same thing that is
happening here it's not that it's not
different from it
so each of the weight mattresses we
calculate the um
yeah and and nothing is discretized is
uh during the training so everything is
still continuous so we still can compute
the gradients
this is why i don't uh
do
do you know how the training works for a
fully connected and you don't understand
it for this special case
so so why is the attention mechanism a
problem like the if if there was like
some discretization or anything else
then i would it's ex then i would
understand that the gradients
won't be computed anymore we can we
could not compute the gradient anymore
so we cannot adjust the weight but it's
not the case so here it's still
continuous
yeah just imagine it as having three
different linear layers
with three different mattresses and we
call these mattresses the value the key
and the query
so just imagine it as having three
different fully connected linear layers
and these linear layers can be
their weights can be adjusted because
everything the gradients can be computed
okay so don't hesitate please don't
hesitate to ask if you have any
questions even via email if you have a
code and you want to understand
something on it just us
all right uh so i think i'm gonna
terminate the call now
uh thank you everyone for
being here
uh if you need anything of course please
ask
this is one of the free workshops of the
cod labs academy but we also have a
certification
where you can learn more about data
science and deep learning
alright so have a nice day thank you