" In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.  This is numerically more stable. "
" Yeah, conceptually this is a tensor of integers, they can only be 0 or 1, but we, we�re going to be using a cross entropy style loss function, so we're going to actually need to do floating-point calculations on them.  That's going to be faster to just store them as float in the first place rather than converting backwards and forwards, even though they're conceptually an �int� we're not going to be doing kind of �int style calculations� with them. "
"We say the loss is minimized because the lower the loss or cost of error, the better the model. "
"The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss.  This terminology is a particularity of PyTorch, as the nn.NLLoss [sic] computes, in fact, the cross entropy but with log probability predictions as inputs where nn.CrossEntropyLoss takes scores (sometimes called logits)."
"For a special case of a binary classification, this loss is called binary CE (note that the formula does not change) and for non-binary or multi-class situations the same is called categorical CE (CCE). "
"Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs. "
"In your example you are treating output [0, 0, 0, 1] as probabilities as required by the mathematical definition of cross entropy.  But PyTorch treats them as outputs, that don’t need to sum to 1, and need to be first converted into probabilities for which it uses the softmax function."
"So H(p, q) becomes: "
"Translating the output [0, 0, 0, 1] into probabilities: "
"For example, suppose for a specific training instance, the true label is B (out of the possible labels A, B, and C). "
The one-hot distribution for this training instance is therefore: 
"You can interpret the above true distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C. "
"Now, suppose your machine learning algorithm predicts the following probability distribution: "
How close is the predicted distribution to the true distribution?  That is what the cross-entropy loss determines.
Where p(x) is the true probability distribution (one-hot) and q(x) is the predicted probability distribution. 
"The sum is over the three classes A, B, and C. In this case the loss is 0.479 : "
" I'm just  saying that we will use them in practice, we will actually use  the cross entropy in practice. "
" But in practice, this is more stable, I  recommend using this one. "
" So this particular function, which is identical to MNIST loss plus �.log� jhas a specific name and it's called binary cross entropy, and we used it for the threes vs. sevens problem, to, to decide whether that column is it a three or not, but because we can use broadcasting in PyTorch and element-wise arithmetic, this function when we pass it a whole matrix is going to be applied to every column. "
" Random numbers and movies by 5, okay.  And so to calculate the result for some movie and some user we have to look up the index of the movie in our movie latent factors, the index of the user in our user latent factors and then do a cross product.  So in other words we would say, Like oh okay, for this particular combination we would have to look up that numbered user over here and that numbered movie over here to get the two appropriate sets of latent factors.  Thanks everybody and I will see you next week or see you in the next lesson whenever you watch it "
" What happens   if we're not predicting which of five things  it is but we're just predicting “is it a cat?”   So in that case if you look at this approach  you end up with this formula, which it's   exactly… this is identical to this formula but  in for just two cases, which is you've either:   you either are a cat; or you're not a cat,  right, and so if you're not-a-cat, it's one minus   you-are-a-cat, and same with the probability  you've got the probability you-are-a-cat,   and then not-a-cat is one minus that. "
" So here's  this special case of binary cross entropy,   and now our rows represent rows of data, okay,  so each one of these is a different image,   a different prediction, and so for each  one I'm just predicting are-you-a-cat,   and this is the actual, and so the actual  are-you-not-a-cat is just one minus that. "
" And so then these are the predictions  that came out of the model,   again we can use soft max or it's  binary equivalent, and so that will   give you a prediction that you're-a-cat, and the  prediction that it's not-a-cat is one minus that. "
" So we're just going to decide, all right, we're  just going to decide the first 10 columns,   we're going to decide are the  prediction of what the disease is,   which is the probability of each disease.  So we can now pass to cross_entropy   the first 10 columns, and the disease target. "
"The cross-entropy loss that you give in your question corresponds to the particular case of cross-entropy where your labels are either 1 or 0, which I assume is the case if you're doing basic classification. "
"As to why this happens, let's start with the cross-entropy loss for a single training example x: "
"where P is the ""true"" distribution and ""Q"" is the distribution that your network has learned.  The ""true"" distribution P is given by your hard labels, that is, assuming that the true label is t, you'll have:"
which means that the loss above becomes 
"In your case, it seems that the distribution Q_s is computed from the logits, i.e. "
the last layer before a softmax or cost function which outputs a set of scores for each label: 
The traditional matrix multiplication is only used when calculating the model hypothesis as seen in the code to multiply x by W: 
"It is this measure (i.e., the cross-entropy loss) that is minimized by the optimization function of which Gradient Descent is a popular example to find the best set of parameters for W that will improve the performance of the classifier. "
" If we use negative  log likelihood loss or cross entropy in pytorch.  But  numerically, like stability wise on the computer, the cross  entropy one is more stable.  So and also for this one, really pay  attention to this one, it's taking the logits as input. "
 And the term binary  cross entropy and negative log likelihood are essentially the  same. 
