Machine learning is a subfield of AI
that allows computers to learn from data
Machine learning can be used in many fields
such as finance, biology, and marketing.
In this video, I'll cover XGBoost and show
a practical example
with a real-world dataset using an XGBRegressor
model.
First, I'm going to talk about tree-based
models.
The tree-based model is one of the most used
models in machine learning.
To understand better XGBoost, let's take a
look at what a random forest is.
A random forest consists of a collection of
decision trees.
The random forest is based on the bagging
technique.
In this technique, each tree is independent
and it's trained using a different subset
of features.
After that, the model combines all results
to get the final decision.
Alternatively, you can train models sequentially.
Each next model tries to fix errors from the
previous one.
This approach of combining models is known
as boosting.
The general idea behind boosting is to transform
weak learners
into strong learners by iteratively improving
upon errors.
Gradient boosting is a particular variation
of the boosting method.
It provides to minimize the errors of the
residuals.
So, what is XGBoost?
Let's discuss XGBoost.
XGBoost is short for Extreme Gradient Boosting.
You can use the XGBoost package to implement
gradient boosting.
It provides a parallel tree boosting
that solves many data science problems in
a fast and accurate way.
To install XGBoost, you can use the "pip install xgboost" command.
So far, what I have talked about
is the random forest, gradient boosting, and
XGBoost.
Let's move on and analyze a dataset with the
XGBoost.
To show XGBoost, let's load the bike rental
dataset.
First, I'm going to import Pandas.
Now let's load the dataset
with the read_csv method in Pandas.
df = pd.read_csv('bike_rentals.csv')
Okay.
We loaded the dataset.
Let's take a look at the first five rows with
the head method.
df.head()
Here you go.
This dataset related to bike rental such as
the date,
and temperature variables.
Awesome.
We saw the dataset.
Understanding the data is one of the important steps of data analysis.
After you explore the data, you perform data
preprocessing,
and decide the model you'll use.
With the describe method, you can look at
numerical statistics.
df.describe().T
Let me use the T attribute for performing
tranpose
You can see the numerical statistics in this
table.
Note that the mean and median are close to
one another,
we can say that the data is roughly symmetrical.
To understand the data, you can also use the info method.
Let me show you.
df.info()
You can see general information about the
columns and rows.
The dataset consists of 731 rows and 16 columns.
Here, you can also see column types and missing data.
You must handle missing data before building the model.
Let's look at the missing data of each column.
df.isna().sum()
Here you go.
As you can see, there is a little bit of missing
data in the dataset.
To handle missing data, you can use some strategies.
One common strategy is to replace missing
data with the median or mean.
Note that the median is often a better choice
than the mean for the dataset which includes the outliers.
Let's use the median of each column for filling in missing data.
First, let me create a variable that includes
the median of each column
values = {}
Now let's write first column that has missing
data.
"yr":
Let me find the median of this column.
df["yr"].median(),
Okay.
We handled the yr column.
Now let's handle the other columns.
To do this, I'm going to copy this code and
paste 5 times.
Copy this code and paste here
Copy this code and paste here
Copy this code and paste here
Copy this code and paste here
Cool, we created the values variable.
Let's pass this variable to the fillna method.
df.fillna(value=values)
To be permanent of changes, I'm going to use
the inplace parameter
inplace =True
Beautiful, we solved the missing data problem.
Let's check it out.
To do this, I'm going to use previous code
df.isnull().sum()
Here you go.
As you can see, there is no missing data in
the dataset.
Now, let's delete the columns that we won't
use when building the model.
df = df.drop()
the cnt column is the sum of casual and registered
columns.
So we can remove these two columns.
['casual', 'registered']
the dteday which represent the dates, I'm
going to remove this column.
'dteday' axis=1
Okay, we specified the columns.
In this analysis, our goal is to predict the
number of bike rentals.
Here, the target variable is the cnt column
which represents the number of bike rentals.
Let's assign this column to the y variable.
y = df
To select the cnt column, I'm going to use
the []
["cnt"]
Okay.
We created the target variable.
Now, let's create the feature variable.
X = df.drop('cnt', axis=1)
Cool.
We created the feature variable.
Before building the model,
we need to split the data into a training
and a test set.
You use the training set for building the
model.
And you use the test for evaluating the model.
Let's split our dataset
with the train_test_split function in scikit-learn.
First, let me import this function.
from sklearn.model_selection import train_test_split
Let's split the dataset with this function.
X_train, X_test, y_train, y_test
= train_test_split(X, y)
For reproducible, let me set
random_state=0
So far, we performed data preprocessing of
our dataset
and then split the dataset into the training
and test set.
Since our target variable is numeric, we can
use a regression model.
To build the model, I'm going to use the XGBoost
library.
Let's build an XGB Regressor model.
First, let me import the XGBRegressor class.
from xgboost import XGBRegressor
To build the model, I'm going to use the cross-validation
technique.
Let's import the cross_val_score function.
from sklearn.model_selection import cross_val_score
Now let's use this function for building the
model.
First, I'm going to create an object from
XGBRegressor class.
model = XGBRegressor()
Now, let's build the model with the cross_val_score
function.
xg_scores = cross_val_score()
First let me write the model
model,
Let me type the feature and target variables.
X, y,
Let me specify the metric.
scoring='neg_mean_squared_error',
I'm going to want to use 10 fold.
cv=10
After that let's find the RMSE by taking the
square root of the negative scores.
First, let me import numpy 
And then
rmse = np.sqrt(-xg_scores)
Let's look at this value rmse
You can see 10 values for using 10 fold.
Let's calculate the mean of these values.
np.mean(rmse).
To round to the first number of decimals,
let me use
round()
Here you go.
So we found the rmse metric.
Okay, we built an XGBRegressor model
and saw the performance of this model.
Let's compare this model with a regression
model.
To do this, I'm going to train a simple linear
regression model.
Let me import the LinearRegression class.
from sklearn.linear_model import LinearRegression
Let's create an object from this class.
lr= LinearRegression()
After that, I'm going to build the model
with the cross_val_score technique.
lr_scores = cross_val_score(lr, X, y)
scoring='neg_mean_squared_error', cv=10
Cool, the model is built.
Next, let's calculate the root mean_squared_error metric.
lr_rmse = np.sqrt(-lr_scores)
Since I used 10 fold, 10 rmse values are calculated.
Let's calculate the mean of these values.
lr_rmse.mean()
Here you go.
We would say, the performance of the XGBoost
model
is better than the performance of the linear
regression.
So far, I talked about what XGBoost is
and then I showed how to build an XGBoost
model.
Lastly, I compared this model with a simple
linear model.
That's it.
I hope you enjoy it.
Thanks for watching.
Don't forget to subscribe to our channel,
like the video,
and give a comment.
See you in the next video.
Bye for now.