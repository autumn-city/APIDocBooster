So yes, lr is just starting learning rate. 
However, note that, by default, decay parameter for Adadelta is zero and is not part of the “standard” arguments, so your learning rate would not be changing its value when using default arguments. 
learning_rate: A Tensor or a floating point value. 
If you really want to use Adadelta, use the parameters from the paper: learning_rate=1., rho=0.95, epsilon=1e-6. 
A bigger epsilon will help at the start, but be prepared to wait a bit longer than with other optimizers to see convergence. 
