all right
so yeah we're gonna talk about something
new today
right so uh it's called deep
contextualized representations
and i put the transformer here in the
parenthesis
because um the transformer based
model is the most typical example for
these
so-called deep contextualized
representations and actually
i mean nowadays people will not
use the deep context light
the term deep context light
representation sold often
but will people will directly call
transformer based models transformer
based encoders
or birds like models gpt models and so
on right
but i think it will be it will make more
sense
if we follow the the line of history
like
to look at how these type of models are
you know
gradually developed okay
i i said gradually but it's actually a
very uh
rapid uh very rapid uh very rapid
evolution okay uh so
this type of technique is basically to
take care of uh text data
which is to understand the
the the meanings of not just short
pieces of text but longer pieces of text
longer sentences longer paragraphs how
to understand the semantic meanings of
different entities in
text so it's designed to solve such a
more complex text
text understanding and the text
generation tasks
okay but uh the basis of it
i would say it's uh originates from the
the so-called embedding and the rn based
technique so that's why i will put this
chapter as the next one uh exactly that
immediately follows uh
uh follows the uh r and the embedding
technique
okay and so let's look at some
first of all some uh
milestones okay so it's actually in
2018 which is some of the major
achievements
that have been developed so the first of
all the the
birds model which is short for
bi-directional encoder representations
from transformers
right so you see the the nature of the
text
is actually lies here within the
transformers
okay and of course the bi-directional
architecture also is important okay so
those are the benchmark scores
in that are commonly used in nlp tasks
okay so in that year a lot of
improvements has been achieved okay
and
most importantly those percentage
numbers made
doesn't make uh like very
straightforward
like it doesn't make very specific sense
to uh
readers but actually these are very
significant improvements because some of
the tasks
the models can outperform human
performance right by a significant
factor
in terms of accuracy okay so which is
which means this is a very amazing
uh achievements okay
so ants other than birds we have some
variants of it
actually those type of models can
actually are developed
uh simultaneously they they came out in
parallel
and by different companies the the gbt
model generator pre-training model
which also uses transformers but it's
developed
by open ai it was also released in 2018
and then gpd2 in the next year and now
we have gd3
right so those models also
uh this dvd model also
uh improves the accuracies by
very significant amount of tasks so like
those
big companies they have very strong
competitions uh ongoing right
and what's behind gb2 and dbt and
bird models are also actually i rely on
the same technique
uh called transformers okay
so i think this is a example
showing what kind of particular specific
nlp tasks look like so it's an example
from the stanford question answering
task
squad okay so the
data sets contains a paragraph
that have some knowledge for the
for uh model to read right
and it comes with uh several questions
like
which an nfl team represents afc at
super bowl 50 right
so the model should be able to
generate correct answers among such
options
right so it's like a basic multiple
choice
question answering task right so the
more basically the model should able to
um
generate those uh tasks automatically
based on some pre-trained um
pre-trained model your example training
model okay and the pre-training task
the pre-training technique is someone we
will focus on
this in this in this lecture today and
on on thursday's lecture i don't think
we will have time to cover
everything today okay all right
and let's look at the outline
so the first
uh technique that we will
cover uh today actually uh it's uh
have a stronger connection to the word
embedding techniques
we just finished yet okay just finished
uh
talking about so the first type of
technique is called
elmo okay which is also developed in
2018
and it's uh it is in this work in an
almost
in an elmo paper that's the
term deep contextualized pre-training
or deep contextualized representation is
first developed okay and the difference
from these elmo
from other gpt of birds or other uh
more fancier network is that we
the model used here is significantly
significantly smaller which is uh lstn
but not
transformer or transformer based models
okay so i think it makes sense to
directly to look at the
first step from word embeddings to elmo
let's look at
how that transitioning happens
okay so the
word embedding as we know it's uh
it's a way to represent words
with uh dense vectors through some
unsupervised laws
right because we just use the data alone
to train itself
right we for example we use negative
sampling right
to sample some words outside a fixed
window
and in together with the positive
example we construct
some loss functions which is similar to
multiple logistic regression tasks so
that will result in the word
vectors that can be used
that can be plugged into some
downstream supervised nlp tasks such as
[Music]
sentiment sentiment classification such
as
question answering right so we have
shown that
the word embedding is successful in
increasing the performance for those nlp
tasks okay
so it has very significant advantages
over one hot encoding right
we will represent all those words with
those embedding vectors
right so these are just the example that
we already know
right so so we will talk about
the shortcomings of the word embeddings
uh in traditional models
okay so in the word of act right we will
orange
we will use the word orange for example
as the context to predict the target
word in within the within the short
uh window right and so each word is
converted into a embedding vector right
300 by one
vector right right so this
uh conditional probability using the
context word predicts word is modified
is uh modeled by this uh soft max
uh regression probability and then which
is also approximated with negative
sampling
but we know the details of the technique
right and
we know that this per train the network
pipeline is successful in coming up in
in
obtaining meaning for what what
embedding vectors but also
it comes with disadvantages of um
with the words in bats where embeddings
trained in such a way
because first of all
each word you know is represented by a
single embedding vector
right which is a problem right because
we know that
um the um
it's very common that the same word has
multiple sentences that are not
even closely related right so
i think it's more so in morphologically
rich
languages okay
so that is the problem that's
traditional
work to react models doesn't
handle really well okay
and in that case because
we will show that uh with a particular
example
for the word orange because orange is a
color an orange is also a fruit
so in that case if we don't def uh
distinguish those different uh you know
different
meanings then the mo
the vectors learned is actually a
mixture of static semantic meanings
it's just a mixture of different
meanings right
so in linguistics i think it's called uh
the the phenomenon is called
uh ploy uh sammy okay which means
the word meanings may be different
depending on the context right so
the orange juice means fruits but paint
this part oranges means color
okay so actually the
if we think of the nature of water
actually where to veg
knows the orange the word orange can be
colored or fruits
right but it actually does not know when
to use which
no so because the whole sentence
when this whole sentence is fed into a
word
uh word to back model the model knows
that the word
orange co-occurs with the word paint for
example
okay so actually the model captures
this core occurrence relationship it
knows that the word
orange means paint okay but
and also in another language in another
word
the model knows the uh word orange
co-occurs with the word juice right it
also knows the orange
has some meanings with related to juice
but it doesn't really know
which semantic meaning to choose right
it only has one representation
okay it doesn't have separate separate
representations so
here's the just the um
the shortcomes of the traditional world
embedding methods
okay so um
here we have a a new target on your goal
which is to um come up with more context
sensitive representations
right which is to we will help the
models to be able to
you know dynamically adjust the word
meanings
by the context it reads okay we hope
that
when the model reads the
word paint for example it should able to
adjust
the embeddings for orange so that's
it's tuned towards a a
vector that represents colors meaning
okay and when the
model uh have the juice
within within the neighbor of were
orange you should be able to adjust the
meaning of orange
so that it's closer to the fruit type
okay so we wish the models to be dynamic
to be flexible in doing this
dynamic adjustments so that's why we
developed first people first developed
this first type of
deep contextualized pre-training or deep
contoxide model
called elmo okay
and it utilizes the uh lstn
uh uh structure
okay so uh we'll take a look at
this take a look at this
um interesting model okay so it's called
embeddings so the elmo is from
embeddings from
language models okay so the
within each cell here
it is a uh lstm
uh cell okay so the lstm cell carries
like a an activation cell and the memory
cell right
and also some a bunch of gates within
each
uh within each of those box so you see
um if we only look at uh
one components like if you only look at
this
left uh part without looking at those
right
parts okay so it looks like
it's a it's just a bi-directional
uh it's actually just one directional
but two layer lstn
right it's uh one directional right so
one direction is here
and the above is just the second layer
of one directional so it's just a
an kind of a lstm model
and the embedding for the first word
second word the last word in a sentence
so this is a whole sentence is fed as
the input
to those two layer nstm network
right and the internal activations is
used
as the input for the second layer
and the output from the second layer is
used in the
final output right so that is the one
direction
okay and this the the
model also creates a second um
direction which is in this way right
so the blue color is second direction so
it's a little bit different from the
bi-directional
lstm but uh eventually
the the the the part on the right
is from modeling the sentence from the
right to the left
okay right and the output from this
second layer lstm is
used as output and these two outputs are
concatenated or sum up as the outputs
for the final token
okay so this t1 t2 and tn are the
output representations for each input
tokens
so that is the so-called elmo model
and the iomo representation technique
okay so these
tokens here t1 t2 to tn are used
in the prediction tasks okay okay
so the um
when when the model is being trained the
left to right language model
highlighted by this color actually it's
a language model so it's uh
uh maximizes the conditional probability
given the current words uh given the
previous words
uh for the current word right so it's a
language task
and the the right parts
it's it is a right to left language
model
and its task is to predict the next word
by
maximizing the probability of the next
word given the context on the right okay
so this is the context that have greater
than one
right k plus one k plus two which is the
all the words on the
on the right of the model okay so having
these two models
uh training trained together
we are able to obtain
uh some meaningful representations
from the model's activation layers
okay so on the next slide we're gonna
show the
a simple uh showcase for the one layer
lstm model right
and the internal is just like this okay
and the multiple layer is just
the stack of multiple uh multiple
uh activations okay
so uh the training of this elmo model
is actually a joint training which means
that we are training the
both the forward which is the left to
right language model
and the right to left language models at
the same time
so we have two uh
probabilities right two probabilities
uh that are maximized so these two
probabilities are maximized at the same
time with uh
they are fading to the same uh they are
constructed they're used to construct
the one
cost function okay
so uh after the model
uh is changed
uh so uh so so far any questions about
this
uh on architecture model architecture
okay so i'm gonna carry on with the
next supervised learning task okay so
these joint training with forward and
backward
language models is just at the
pre-training stage which means
at this stage we are training the models
using
uh unstructured data with unlabeled data
it's a it's unsupervised
uh task right and then
after the models are trained and all the
parameters are
fixed now we're gonna use the models
for supervised nlp tasks like where
we want to for example given the
sentence
uh given an input sentence that has n
words
here right we're gonna for example
um
feed the model uh
into into the feed the input centers
into the model
and we will have three types of
representations first one for the word
end we will have
the a word embedding right so that is a
raw word embedding
okay and at the first layer we'll have
some
first layer lstm activation and then
will have some second layer lstn
activation so these are the uh
representations returned by the model
and then we can
combine somehow collapse them into one
representation
and then feed them feed the collapsed
representation to the downstream tasks
which can be a
a binary clarification classification or
it can be a
multi-modal soft max classification
models
okay depending on what tasks we are
interested in okay
all right
so um the
example here here's an interesting
example showing how the
um bi-directional language models using
lstm can actually
determine actually decide actually
differentiates the word senses
based on the context
so for example when the input is like
this one uh chicoris played a
spectacular play made of spectacular
play on
a lucix grounder okay so
these um
word play have a
different meaning as this word play
right a bro this is this the second play
means a a broadway play right it's a
more a drama update right
so a good model should be able to
assign different meanings to these two
plays okay
so in the example that's this study has
shown that
when we try to find the nearest
neighbors of the play
the the blue play is associated with
oops
uh
the blue play here is associated
with this play right as well as his
all-around excellent plate
okay and then the the the orange play
here the drum drama play
is associated with this play which is a
more you know
of course uh looks um
more like a a uh play in the in
a sense in a context of uh dramas right
so this can be achieved
by the lstm by the
elmo model but for the
same task if we only use the glove
vectors
which is just the raw vectors to find
the
nearest neighbors of this wordplay then
we can
see that a lot of those words are
mixed together because it doesn't
represent it doesn't
differentiate with the playing the multi
the football doesn't represent doesn't
uh distinguish the
musical play or the sport uh sports play
or whatever other plays
are there right so um
the model is successful
in using the context information in a
sentence to determine the
the meanings of a word right so
uh that's why the model is uh you know
important uh i mean is as a really
a impactful
model in the year 2018
so the
weakness of elmo compared to the
same uh the the model that uh
came out in the same year like birds and
gbt
it's also um very obvious
that first of all the lstm is
way less powerful as an encoder than
transformers it uses much less
parameters okay and also the two-way
concatenation
of the bi-directional language model
might not sb
as a an all-directional fusion okay
so yeah so that's why people
uh you know develop uh very much more
complex models like transformers so
we're gonna spend some time on
transformers uh
uh next when we uh in our thursdays
class
so we're gonna go through the details
some of the details of those
uh would step inside the the
transformers model and let's
uh basically reveal the uh
the secrets of the performance uh
behind okay behind the performance we're
going to see how
those different building blocks can
actually capture the semantic meanings
really successfully
all right so i think
that's so much for today we have
made this transition from raw rolling
bedding models
to this uh deep contextualized
world deep contextualized in
representations
which uh will be the basis for uh
more advanced transformer based models
okay so
uh i think that's it uh
i'm glad to take questions if you have
any questions
uh this isn't a question about the
lecture but uh i would just have a
question about
like the final and stuff like that uh
since it is coming up in the next month
um
you know like are you gonna be giving
like a study guide for that or
uh yeah yeah i'm gonna i'm gonna prepare
a a lecture we're gonna talk about
what's gonna how what will be tested and
how the
questions will be like questions like
that and how you can prepare the
final exams all right sounds good
okay
all right um