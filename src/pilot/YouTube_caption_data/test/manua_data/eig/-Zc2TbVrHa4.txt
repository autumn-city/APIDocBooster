Friends welcome to my youtube channel, Dhanesh here
See today, I am going to discuss about principal component analysis
Let's start
Machine learning models create wonders when the dataset provided for training is large.
Having a good amount of data
allows us to build a better predictive model as we have more data to train the model.
However, using a large data set has its own drawbacks.
The biggest limitation
Is the problem of dimensionality.
To get rid of this a process called dimensionality reduction was introduced.
Dimensionality reduction techniques can be used to filter only a limited number of significant
features needed for training and
this is where PCA or  principal component analysis comes into picture.
Let's discuss in detail about principal component analysis.
As mentioned
PCA is an unsupervised
dimensionality reduction technique that enables you to identify
correlations and patterns in a data set so that it can be
transformed into a data set of significantly
lower dimension,
without loss of any important information.
The main idea behind
PCA is to figure out patterns and correlations
among various features in the data set.
On finding a strong correlation between different variables,
a final decision is made
about reducing the dimensions of the data in such a way that
the significant data is still retained.
Dimensions are nothing but features that represent the data.
Now we will discuss about what is principal components.
See here.
We are transforming the variables or features or dimensions to a new set of variables,
which are known as the principal components or simply
the PCs.
The principal components are computed in such a manner that the newly obtained
variables are highly significant,
and independent of each other.
To understand the principle component analysis in a better way,
you need to understand the mathematical concept,
eigenvalues and eigenvectors which are related to matrix.
See.
Eigenvector of a matrix  A
Is a vector represented by a matrix X such that
when X is multiplied with matrix A,
then the direction of the resultant matrix remains the same as vector X.
mathematically the above statement can be represented as shown in the diagram.
where A is any
Arbitrary matrix and lambda are the eigen values
And X is an eigen vector corresponding to each eigen value.
See to understand the principal components,
you need to understand this equation.
Determinant
A minus lambda I equal to 0.
The principal components are the eigenvectors
of a covariance matrix
and
they are orthogonal.
I will explain you what is covariance matrix.
We will be discussing in detail. What are all the steps to be followed?
For this principle component analysis
These are all the different steps
we need to follow in principle component analysis. Normalize the data, compute the covariance matrix,
Calculating the eigenvectors and eigenvalues,
choosing the components and forming the feature vector,
forming the principal components,
now let's
discuss the first step , normalize the data.
If you are familiar with data analysis and processing,
normalization of data is the first step.
Normalization is a technique often applied as part of data preparation for machine learning . The goal of
normalization is to change the values of numeric columns in the data set
to a common scale, without distorting differences in the range of values.
Step two
is ,
you know ,computing the covariance metrics.
See as mentioned earlier,
PCA , principal component analysis helps to identify the
correlation and dependencies among the features in a data set. A covariance matrix
express the correlation between the different variables
In the data set.
It is essential to identify
heavily dependent variables because they contain biased and redundant information,
which reduces the overall performance of the model.
Mathematically a covariance matrix is a P by P matrix where p represents the dimensions of the data set.
Each entry in the matrix represents the covariance of the corresponding variables.
Consider a case where we have a two dimensional data set with variables a and b,
the covariance matrix is a two by two matrix  as shown in the figure .Here covariance
of a,a
represents the covariance of a variable with itself here.
 
which is nothing, but the variance of the variable a.
Covariance of a , b
in the matrix represents the covariance of the variable 'a' with respect to the variable 'b'.
And since the covariance is
commutative, covariance of 'a' , 'b'
is equal to covariance of 'b' , 'a'.
You may be familiar with statistics. Covariance value denotes how
co-dependent two variables are with respect to each other . If the covariance value is negative,
It denotes the respective variables are
indirectly proportional to each other. That means as one variable increases the other decreases.  A  positive
covariance denotes
that the respective variables are directly proportional to each other.
Step three : Calculating the
eigenvectors and eigenvalues.
We already discussed this step.
we calculate the
eigenvalues and eigenvectors
by using the equation
Determinant A minus lambda. I equal to 0 as discussed earlier
Step four : Choosing the components and forming a feature vector.
We order the eigen values from largest to smallest,
so that it gives us the components in order  or significance.
Here comes the dimensionality reduction part.
 
If we have a data set with n variables then we have the corresponding n
eigenvalues and eigenvectors.
It turns out that the eigenvector
corresponding to the highest eigenvalue
Is the principal component of the data set
and it is our call as to
how many eigen values we choose to proceed our
analysis with.
uh
 
To reduce the dimensions we choose the first P eigen values and ignore the rest.
We do lose out some information in the process, but if the
eigenvalues are small, we do not lose much.
Next we form a feature vector, which is a matrix of vectors in our case, the
eigenvectors. In fact only those
eigenvectors which we want to
proceed with.
uh
 
Since we just have two dimensions in the
running example,
we can either choose the one corresponding to the greater eigen value or simply take both.
Next is step five : forming principal components .
This is the final step where we actually form the principal components using all the
math we did till here.
For the same we take the
transpose of the feature vector
and
left multiply it with the  transpose of the scaled version of the original data set.
Here new data is the matrix consisting  of the principal components.
And feature vector is the matrix, we formed using the eigenvectors
we
choose to keep . ScaledData is the scaled version of the original data .
T in the superscript  denotes the
transpose of a matrix ,which is formed by interchanging the rows to columns
and vice versa. In particular a 2 by 3 matrix has a
transpose of size 3 by 2.
Next we are going to discuss about the applications of principal component analysis.  Principle component analysis is
predominantly used as
dimensionality reduction techniques as discussed in domains like
facial recognition,
computer vision and image compression.
It's also used for  finding patterns in data of high dimension in the field of finance,
data mining,
bioinformatics,
psychology, etc. It's used in spike -triggered covariance analysis in neuroscience,
Quantitative finance and medical data correlation.
That's all for principle component analysis. Thanks for watching. Please like share and subscribe. Thanks a lot