 In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.  This is numerically more stable. 
You can interpret the above true distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C. 
 So this particular function, which is identical to MNIST loss plus �.log� jhas a specific name and it's called binary cross entropy, and we used it for the threes vs. sevens problem, to, to decide whether that column is it a three or not, but because we can use broadcasting in PyTorch and element-wise arithmetic, this function when we pass it a whole matrix is going to be applied to every column. 
 So here's  this special case of binary cross entropy,   and now our rows represent rows of data, okay,  so each one of these is a different image,   a different prediction, and so for each  one I'm just predicting are-you-a-cat,   and this is the actual, and so the actual  are-you-not-a-cat is just one minus that. 
The cross-entropy loss that you give in your question corresponds to the particular case of cross-entropy where your labels are either 1 or 0, which I assume is the case if you're doing basic classification. 
