welcome back
today we're going to be looking at by
torch
in python and unfortunately i won't be
able to make you an expert in play torch
in just a single video but hopefully
you'll learn some of the basics some of
the features it has and you'll be able
to go away and do
your own research and create your own
project based on what you learn if
you're interested in it and if you're
not then you can just do other stuff
so apply to watch
it is a python package and it has these
two features
and the first one is tensor computation
so this is the ability to do maths on
high dimensional equivalence of matrices
and it also has gpu acceleration so
things can happen very quickly
the other one is this neural networks
uh
based on this tape based system which is
slightly different to other ways of
doing automatic differentiation but uh
that's a very useful thing that it
provides fire torch also has a lots of
sub libraries so the first one is torch
which is
like numpy and it does this
tensor
maths and tensor operations then you've
got autograd which is their automatic
differentiation system
then you've got jit which is used to
compile
things that can be run on your gpu
torch.nn
which allows you
to have neural network modules
and uh integrated with their autograd
system so that you can do back
propagation and gradient descent among
other things
multiprocessing which allows you to do
machine learning on multiple machines
so you don't just have to have
one gpu for example you could have
several and distribute them using the
multi processing
and then the utools which has a bunch of
utility functions
so the way to install pytorch depends
quite a lot on your operating system so
i would recommend going to the apply
torch page
and installing it from there
and
usually you can do it with pip but
that's not always the case especially if
you want gpu support
so the first thing we need to do is
import torch
and all these related libraries so we're
using torch vision data sets because the
data set we're going to use
then we've got matplotlib
and numpy as kind of utilities and this
is the magic line which makes everything
print
in the right place and then we're just
resizing everything so that it's the
right shape
next we're going to get some data and
we're going to download the nist dataset
which is very famous
and
i've never used before but it's very
common for introductory tutorials so i
thought why not
and it's going to be saved in the data
directory we've got the train
and the test part of our data set
and the classes here are just a list of
the numbers from zero to nine because
they are handwritten digits and if we
want to view the data
you can see it's not very easy to see
exactly what's there but fortunately we
can just display it with matpot lib
and so we pick five random digits and
they happen to be an eight a two or
three
a seven under one and you can see
they've got these labels
and technically you know we don't need
the classes because it's going to return
an
integer anyway
with the right number but if we just put
a string here and that's a bit nicer and
if you had different
class names then you could do that
so now we're going to wrap everything in
a data loader
and the data load is very useful because
it can preload things by running in the
background it puts stuff in batches it
puts stuff
in a tensor rather than in just
a numpy array or another kind of data
format
you can see that our data is in batches
of size 64.
and each image is one channel saying
it's black and white and then 28 by 28
and the size of the labels is 64. so
it's just a list of 64 numbers
so now we're going to create
our module
and we just need to define two things
the network itself
so we've got this
convolutional part
which
kind of slides a window
over
each part of the image and does some
calculations
and then this max pool 2d which then
looks at the maximum of these
compilations
and then we have the flattening bit
where we turn our image into a single
list
and then here we perform
two linear layers where we classify
everything into the correct label
and you can see that when we pass it
forwards we just do those layers in
order and we return log it because
they're not probabilities yet
we also set up cuda because i have cuda
available otherwise it will run on your
cpu
and it should tell us
that we are using cuda and here i just
put the model onto my gpu so it's now
sitting there and can run whenever i
pass things through it
to actually use our model and train it
we need a loss function so we're using
cross entropy loss and an optimizer i'm
using adam which is fairly common
and i'm using all the default settings
but i've passed the model parameters
because they're the things we're trying
to optimize
now our training loop
so we look through all the batches
and we have an x and y and we put them
both on the gpu because you need to put
everything on the same device as the
model to pass it through the model
then we calculate predictions
calculate our loss
we backward propagate anything
and then we say here that every 100
batches we're going to print the loss
the current which is how far through we
are
and uh show our progress
then when we test it we're also going to
calculate the test loss
but we're also going to calculate our
accuracy which is the number
where
what the column we've predicted so the
digit is the same as the actual column
then we just cast it and calculate their
sums
then we divide by the number of batches
for our loss and the number of items to
get our accuracy
then in each epoch we're just going to
train it
and then test it and at the end print
done
and because of all these parameters it's
quite a small model we only need two
epochs really to get everything working
so we can then just let it run
and usually you wouldn't let watch your
model run you just leave it for a few
hours but because it's only going to
take
a few seconds we can just watch it and
see if the loss decreases
and you can see we've done pretty well
we've got 97 accuracy the loss has been
decreasing
another thing that's often important to
do is save your model
for us it's not unnecessary because
we're never going to use this model
again but it does mean once you've
trained your model put all that
computational power into training it you
can then use it somewhere else
rather than deleting it at the end of
your session which isn't great
and then you know we can just load it
back again and this is a way that means
we have to have the model defined when
we load it but there are
other ways of doing it so you don't have
to do that
now we're going to evaluate our model
and what we'll do is we'll take five
different
things in our test data set i just make
predictions and see what the model says
and uh
to make a couple of things clearer here
we're just displaying the image
here we're making sure that we have that
extra dimension because you remember
initially there were 64
in batch sizes but now we're just doing
batches of one
and then here we make predictions and we
get what it actually is and then just
add that as the title
and so you can see here we have a one
which we've got correct and eight which
we got correct an a which we've got
correct a two which you've got correct
and what looks like an eight
which
we predicted
correctly
so you can see that has been hugely
successful we've managed to go through
mnist which is quite a large data set
and generate some pretty good
predictions using pi torch
and nothing too sophisticated on top of
it
if you want a challenge you could try
re-running this tweaking parameters
seeing if you can get a higher
percentage
but uh anything really over 95 is good
enough for this data set because it's
not perfect
and there are lots of things
that you might disagree with in the test
data set
you've enjoyed this video
learn the basics of pie torch and
hopefully have a flavor of what it's
capable of so if you ever need to do
machine learning in the future and you
think fire torch is the best way of
doing it and you can just go ahead and
look at the documentation look at some
of the tutorials
and you know roughly what you're going
to be able to do
so hopefully you enjoyed this video
seen some cool stuff and i'll see you
again same time next week till then