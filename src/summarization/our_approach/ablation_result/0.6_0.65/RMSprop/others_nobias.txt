Vanilla adaptive gradients (RMSProp, Adagrad, Adam, etc) do not match well with L2 regularization. 
Adam is a recently proposed update that looks a bit like RMSProp with momentum.
In   particular, when combined with adaptive gradients, L2   regularization leads to weights with large gradients   being regularized less than they would be when using   weight decay.
Although the expression "Adam is RMSProp with momentum" is widely used indeed, it is just a very rough shorthand description, and it should not be taken at face value; already in the original [Adam paper (hyper-link)], it was explicitly clarified (p. 6): 
There are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running average of first and second moment of the gradient. 
