It is simple: BatchNorm has two "modes of operation": one is for training where it estimates the current batch's mean and variance (this is why you must have batch_size>1 for training).  The other "mode" is for evaluation: it uses accumulated mean and variance to normalize new inputs without re-estimating the mean and variance. In this mode there is no problem processing samples one by one.
 Alright, so batch normalization, or in short, batch norm goes  back to a paper published in 2015, called batch  normalization, accelerating deep network training by reducing  internal covariate shift. 
Pytorch does its batchnorms over axis=1.  But it also has tensors with axis=1 as channels for convolutions.
 Yeah, and also  note that now when we use batch norm, batch norm has learnable  parameters.  So if we use batch norm in a given layer, it has we  have an additional two vectors that have the same dimension as  the bias vector, right.  So if we have, we use batch norm here in  this layer, we will have two, four dimensional vectors, like  this bias vector here would also be four dimensional, right,  because there's one bias for each in layer activation. 
 So  yeah, here, that's the first step of batch norm, there are  two steps.  So the first step is to normalize the net inputs.  So the j is the  feature index again.  So you can actually use batch norm for any  type of input.  So we will also see there is a two dimensional  version for that for convolutional networks later on. 
