many to many vs. many to one: In keras, there is a return_sequences parameter when your initializing LSTM or GRU or SimpleRNN.
Often, LSTM layers are supposed to process the entire sequences.
You can achieve many to many with a simple LSTM layer, using return_sequences=True:
Now, this is not supported by keras LSTM layers alone.
[Predicting a multiple forward time step of a time series using LSTM (hyper-link)]
[https://github.com/danmoller/TestRepo/blob/master/TestBookLSTM.ipynb (hyper-link)]
If you want details about how steps are calculated in LSTMs, or details about the stateful=True cases above, you can read more in this answer: [Doubts regarding `Understanding Keras LSTMs` (hyper-link)]
Refer this blog for more details [Animated RNN, LSTM and GRU (hyper-link)].
The figure below gives you a better view of LSTM.
It's a LSTM cell.
An example of one LSTM layer with 3 timesteps (3 LSTM cells) is shown in the figure below:
** A model can have multiple LSTM layers.
units = It's a positive integer and determines the dimension of hidden state and cell state or in other words the number of parameters passed to next LSTM cell.
LSTM in its core, preserves information from inputs that has already passed through it using the hidden state.
Unidirectional LSTM only preserves information of the past because the only inputs it has seen are from the past.
Using bidirectional will run your inputs in two ways, one from past to future and one from future to past and what differs this approach from unidirectional is that in the LSTM that runs backwards you preserve information from the future and using the two hidden states combined you are able in any point in time to preserve information from both past and future.
What they are suited for is a very complicated question but BiLSTMs show very good results as they can understand context better, I will try to explain through an example.
Lets say we try to predict the next word in a sentence, on a high level what a unidirectional LSTM will see is
And will try to predict the next word only by this context, with bidirectional LSTM you will be able to see information further down the road for example
Forward LSTM:
Backward LSTM:
Another use case of bidirectional LSTM might be for word classification in the text.
Adding to Bluesummer's answer, here is how you would implement Bidirectional LSTM from scratch without calling BiLSTM module.
This might better contrast the difference between a uni-directional and bi-directional LSTMs.
As you see, we merge two LSTMs to create a bidirectional LSTM.
You can merge outputs of the forward and backward LSTMs by using either {'sum', 'mul', 'concat', 'ave'}.
In comparison to LSTM, BLSTM or BiLSTM has two networks, one access pastinformation in forward direction and another access future in the reverse direction.
BiLSTM or BLSTM
However, we can also use LSTM in this but Bidirectional LSTM will also do a better job in it.
From: [https://keras.io/getting-started/sequential-model-guide/ (hyper-link)] (search for "stacked lstm")
We need to add return_sequences=True  for all LSTM layers except the last one.
Setting this flag to True lets Keras know that LSTM output should contain all historical generated outputs along with time stamps (3D).
So, next LSTM layer can work further on the data.
If this flag is false, then LSTM only returns last output (2D).
Such output is not good enough for another LSTM layer.
In case you are using LSTM for time series then you should have Dense(1).
If I understood it right, you just want to know how to create models with LSTM?
Using LSTMs
There are many ways to do this, but, suppose you want only one LSTM layer:
But here, since the input has shape (None,20), you must first reshape it to some 3-dimensional array in order to attach an LSTM layer next.
Acually, it's not necessary to use "return_sequences" or even to use LSTMs, but you may do that.
You could work in many other ways, such as simply creating a 50 cell LSTM without returning sequences and then reshaping the result:
What I often see about LSTMs for generating sequences is something like predicting the next element.
Let's device a simple LSTM
Rather than using tf.nn.rnn_cell.LSTMStateTuple I just create a lists of lists which works fine.
Then unpack it and create a tuple of LSTMStateTuples before using the native tensorflow RNN Api.
And in second case it is just one layer of LSTM with works in usual manner.
The first model has 2 LSTM layers, which are stacked on top of each other.
The first LSTM layer takes a single input parameter and outputs 256 parameters, the second LSTM layer has 256 input parameters and it returns the same number as parameters as its output, so the input to the final layer has a width of 256 parameters.
The second model has a single LSTM layer that takes a single input parameter and outputs 512 parameters, which act as the input to the final layer (and not 256 as in the first model).
Stacking LSTM layers makes the model deeper, and potentially allows the hidden state at each level to operate at different timescales.
If you want to learn more about stacking LSTM layers, you will find the following link to a post by Jason Brownlee very informative:
Sigmoid specifically, is used as the gating function for the three gates (in, out, and forget) in [LSTM (hyper-link)], since it outputs a value between 0 and 1, and it can either let no flow or complete flow of information throughout the gates.
LSTMs manage an internal state vector whose values should be able to increase or decrease when we add the output of some function.
The GRU cousin of the LSTM doesn't have a second tanh, so in a sense the second one is not necessary.
Check out the diagrams and explanations in Chris Olah's [Understanding LSTM Networks (hyper-link)] for more.
The related question, "Why are sigmoids used in LSTMs where they are?"
Basically, the unit means the dimension of the inner cells in LSTM.
Because in LSTM, the dimension of inner cell (C_t and C_{t-1} in the graph), output mask (o_t in the graph) and hidden/output state (h_t in the graph) should have the SAME dimension, therefore you output's dimension should be  unit-length as well.
And LSTM in Keras only define exactly one LSTM block, whose cells is of unit-length.
Does that mean there actually exists N of these LSTM units in the LSTM layer, or maybe that that exactly one LSTM unit is run for N iterations outputting N of these h[t] values, from, say, h[t-N] up to h[t]?
In that Keras LSTM layer there are N LSTM units or cells.
If you plan to create simple LSTM layer with 1 cell you will end with this:
[ (hyper-link)]
And this would be your model.
Alternatively, you can consider that in an LSTM with units=1 the key values (f, i, C, h) are scalar; and with units=n they'll be vectors of length n.
Same way LSTM(100) will be a layer of 100 'smart neurons' where each neuron is the figure you mentioned and the output will be a vector of 100 dimensions
[LSTMCell (hyper-link)] is a cell that takes arguments:
A tuple of LSTM hidden states of shape batch x hidden dimensions.
[LSTM (hyper-link)] is a layer applying an LSTM cell (or multiple LSTM cells) in a "for loop", but the loop is heavily optimized using cuDNN.
Optionally, an initial state of the LSTM, i.e., a tuple of hidden states of shape batch × hidden dim (or tuple of such tuples if the LSTM is bidirectional)
You often might want to use the LSTM cell in a different context than apply it over a sequence, i.e.
make an LSTM that operates over a tree-like structure.
LSTM: the argument 2, stands num_layers, number of recurrent layers.
LSTMCell: in for loop (seq_len=5 times), each output of ith instance will be input of (i+1)th instance.
If we set num_layers=1 in LSTM or add one more LSTMCell, the codes above will be the same.
Obviously, It is easier to apply parallel computing in LSTM.
In keras document, mentioned the input is [batch_size, time-step, input_dim], rather than [batch_size, time-step, hidden_unit_length], so I think 64, 32 coorresponding the X-input's has 64 features and LSTM-32 has 32 features for each time-step.
Having a stateful LSTM in Keras means that a Keras variable will be used to store and update the state, and in fact you could check the value of the state vector(s) at any time (that is, until you call reset_states()).
is the amount of units your LSTM layers have.
PS: I'm not sure output_dim is useful in the LSTM layer.
You can simply set the go_backwards argument as True to reverse the traversal of the input vector by the LSTM layer.
I have defined the smart_merge function below which merges the forward and backward LSTM layers together along with handling the single traversal case.
In this case, does that means the length of the input sequence into
  the LSTM layer is 128?
Whatever you supply as 128 to the [LSTM layer is the actual number of output units of the LSTM (hyper-link)].
(more like 92 lstm cell)
This is the LSTM formula on [PyTorch documentation (hyper-link)]:
In Keras LSTMCell:
The two-bias LSTM is what's implemented in cuDNN (see the [developer guide (hyper-link)]).
In Keras, the CuDNNLSTM layer also has two bias weight vectors.
In your case, embedding single words into a vector of dimension 32 might be enough, but the LSTM will process a sequence of them and might require more capacity (ie dimensions) to store information about multiple words.
But still here is a way to implement a variable-length input LSTM.
Just do not specify the timespan dimension when building LSTM.
An LSTM state has two tensors in it, of shape (batch, hidden); when you run your
LSTM in both directions, this will add two more states (the backward pass).
Consider a Numpy data array x of shape (samples, timesteps,features),
to be fed to an LSTM layer.
before the LSTM layer:
Since you are stacking multiple LSTM layers on top of each other, you need to use return_sequences=True on the first two layers.
Otherwise, their output would have a shape of (batch_size, n_units) and therefore would not be a sequence and cannot be processed by the following LSTM layer.
Check out the documentation on [LSTM (hyper-link)].
In a 1-layer LSTM, there is no point in assigning dropout since dropout is applied to the outputs of intermediate layers in a multi-layer LSTM module.
If we want to apply dropout at the final layer's output from the LSTM module, we can do something like below.
According to the above definition, the output of the LSTM would pass through a Dropout layer.
Timesteps in LSTM models: [https://machinelearningmastery.com/use-timesteps-lstm-networks-time-series-forecasting/ (hyper-link)]

I think that you need a TimeDistributed layer after a LSTM with return_sequences=True
You can create 2 [LSTMCells (hyper-link)].
The inner one operating on characters calling the first LSTMCell and resetting the state after each word.
The outer one operating on embedded words (the output from the inner loop) and calling the second LSTMCell.
Then you can use some of the [standard embedding tools (hyper-link)] and a single LSTMCell with the dynamic_rnn.
One loss for the "word embeddings" produced in the decoder (by the inner LSTM), with respect to the word embeddings produced in the encoder (by the outer LSTM applied on characters).
Heres an example of applying CNN and LSTM over the output probabilities of a sequence, like you asked:
By the way, a better approach would be to employ the LSTM over the last hidden layer, i.e to use the CNN as feature extractor and make the prediction over sequences of features.
This thread might interest you: [Adding Features To Time Series Model LSTM (hyper-link)].
By transforming your data into (samples, 5, 500) you are giving the LSTM 5 timesteps and 500 features.
The [LSTM (hyper-link)] input is (samples, timesteps, features).
So if your rows represent timesteps in which 5 measurements are taken, then you need to permute the last 2 dimensions and set input_shape=(500, 5) in the first LSTM layer.
check [this link (hyper-link)] about returning output sequences and states of an LSTM.
input in any rnn cell in pytorch is 3d input, formatted as (seq_len, batch, input_size) or (batch, seq_len, input_size), if you prefer second (like also me lol) init lstm layer )or other rnn layer) with arg
[https://discuss.pytorch.org/t/could-someone-explain-batch-first-true-in-lstm/15402 (hyper-link)]
A) as Vanilla LSTM
B) as Encoder-Decoder LSTM
By choosing lstm_out[:,-1,:], you actually intend to predict the label associated with the input text only using the last hidden state of your LSTM network, which totally makes sense.
The last hidden state of the LSTM network is the propagation of all previous hidden states of the LSTM, meaning that it has the aggregated information of previous input states that it has encoded (let's consider that you're using uni-directional LSTM as in your example).
So for classifying an input text, you will have to do the classification based on the overall information of all tokens within the input text (which is encoded in the last hidden state of your LSTM).
That is why you feed only the last hidden state to the linear layer that is upon your LSTM network.
Note: If you intended to do sequence-tagging (such as Named-entity recognition), then you would use all the hidden state outputs from your LSTM network.
Each of these series enters the same LSTM as independent series (which is equivalent to having 3 copies of the LSTM) - Shape (slidingWindowSteps, 3, 1000, 1)
You want the LSTM that processes this 3 step sequence to also return a 3 step sequence
(You can use many LSTMs in this first stage, if you want more layers.
The Time Distributed LSTM works like this:


TimeDistributed allows a 4th dimension, which is groupsInWindow.
The LSTM with return_sequences=False will eliminate the windowStride and change the features (windowStride, the second last dimension, is at the time steps position for this LSTM): 
result: (totalSamples, groupsInWindow, intermadiateFeatures)
The other LSTM, without time distributed, will not have the 4th dimension.
But return_sequences=True will not eliminate the time steps as the first LSTM did.
Result: (totalSamples, groupsInWindow, arbitraryLSTMUnits)
The learning_phase object branches out to every single layer, but the LSTM itself does not.
Here's what the insides of my LSTM_1 layer looked like in my Tensorboard graph:
First, you have a problem in your implementation of encoder using Common LSTM, the [LSTM layer of keras (hyper-link)] take inputs with shape (batch, timesteps, channel) by default, so if you set your input_shape=(32, 15360) then the model will read as timesteps=32 and channel=15360 which is opposite of what you intend to.
Because first layer of encoder using Common LSTM described as:
At each time step t, the first layer takes the input s(·, t)(in this
sense, “common” means that all EEG channels are initially fed8 into
the same LSTM layer)
So the correct implementation of encoder using Common LSTM would be:
Second, because encoder using Channel LSTM and Common LSTM described as:
The first encoding layer consists of several LSTMs, each connected to
only one input channel: for example, the first LSTM processes input
datas(1,·), the second LSTM processess(2,·), and so on.
In this way,
the output of each “channel LSTM”is a summary of a single channel’s
data.
The second encoding layer then performs inter-channel analysis,
by receiving as input the concatenated output vectors of all channel
LSTMs.
As above, the output of the deepest LSTM at the last time step
is used as the encoder’s output vector.
Since each LSTM in the first layer only deal with one channel, so we need the number of LSTM equal to number of channel in the first layer, following code show you how to build one encoder using Channel LSTM and Common LSTM:
The function LSTM(X,Y) in which you create your model is shadowing the Keras LSTM layer.
You can control the input that goes from the layers incase of lstm you can use return_sequences or return state.
The problem was that my dataset is too niche to be easily autoencoded by LSTMs.
[How to implement deep bidirectional -LSTM (hyper-link)]
Now designing BiLSTM is easier.
As you can look at above image, LSTMs have this chain like structure and each have four neural network layer.
The simplest thing you can do is to pipe the output from the first LSTM (not the hidden state) as the input to the second layer of LSTM (instead of applying some loss to it).
TLDR: Each LSTM cell at time t and level l has inputs x(t) and hidden state h(l,t)
In the first layer, the input is the actual sequence input x(t), and previous hidden state h(l, t-1), and in the next layer the input is the hidden state of the corresponding cell in the previous layer h(l-1,t).
In PyTorch, multilayer LSTM's implementation suggests that the hidden state of the previous layer becomes the input to the next layer.
I've created a [gist (hyper-link)] with a simple generator that builds on top of your initial idea: it's an LSTM network wired to the pre-trained word2vec embeddings, trained to predict the next word in a sentence.
Besides, it's definitely not the bottleneck -- LSTM training takes much longer.
Don't forget that LSTM is used for processing sequences such as timeseries or text data.
Now the problem in your case is that the preprocessing step you have used is not the proper one for a LSTM model.
Therefore, you are completely ignoring the order of appearance of words in a sentence, which LSTM layer is good at modeling it.
There is also another issue in your LSTM model, considering the preprocessing scheme you have used, which is the fact that Embedding layer accepts word indices as input and not a vector of zero and ones (i.e.
I guess you can reach at least 80% training accuracy with an Embedding layer and a single LSTM layer.
(h_n, c_n) comprises the hidden states after the last timestep, t = n, so you could potentially feed them into another LSTM.
The output state is the tensor of all the hidden state from each time step in the RNN(LSTM), and the hidden state returned by the RNN(LSTM) is the last hidden state from the last time step from the input sequence.
a single LSTM cell hidden state
several LSTM cell hidden states
I just verified some of this using code, and its indeed correct that if it's a depth 1 LSTM, then h_n is the same as the last value of the "output".
(this will not be true for > 1 depth LSTM though as explained above by @nnnmmm)
So, basically the "output" we get after applying LSTM is not the same as o_t as defined in the documentation, rather it is h_t.
In Pytorch, the output parameter gives the output of each individual LSTM cell in the last layer of the LSTM stack, while hidden state and cell state give the output of each hidden cell and cell state in the LSTM stack in every layer.
Multi-Layered LSTM
Bi-Directional Multi-Layered LSTM
LSTM is a recurrent layer
LSTMCell is an object (which happens to be a layer too) used by the LSTM layer that contains the calculation logic for one step.
Usually, people use LSTM layers in their code.
Or they use RNN layers containing LSTMCell.
An LSTM layer is a RNN layer using an LSTMCell, as you can check out in the [source code (hyper-link)].
Alghout it seems, because of its name, that LSTMCell is a single cell, it is actually an object that manages all the units/cells as we may think.
In the same code mentioned, you can see that the units argument is used when creating an instance of LSTMCell.
When you are using cells LSTM, GRU, you don't have the notion of layers per se.
I suggest that you read: [http://colah.github.io/posts/2015-08-Understanding-LSTMs/ (hyper-link)] really carefully before you start implementing this kind of stuff.
Here's a snippet of code I did to get some insights behind the LSTM implementation.
The "layer" thinking here should be applied to the number of times the LSTM will be unrolled, in this case 30.
It would typically refer to the number of LSTM cells.
So the LSTM can be considered as a map function:
$R^d$ by $R^d$ by $R^d$ ---> $R^d$ by $R^d$
(c,h) = LSTM(x,pc,ph)
pc and ph can be thought as two "hidden variales".
and the lstm_size mean the dimension d.
A possible approach is to implementing bidirectional LSTM is to reverse the input prior to processing it by a second LSTM layer.
Then reverse the output of the second LSTM again and concatenate with with the output of the first LSTM layer.
model.add(LSTM(100)) and model.add(LSTM(units=100)) are equivalent.
LSTMs contains one Bias tensor (despite conceptually it has many biases, they seem to be concatenated or something, for performance), and for the batch normalization I add "noreg" in the variables' name to ignore it too.
Here is the neural network I was working on: [https://github.com/guillaume-chevalier/HAR-stacked-residual-bidir-LSTMs (hyper-link)]
Right now you are running your LSTM forward for 5 timesteps, and returning the hidden state that resulted at each timestep.
This is a bit more involved and involves two LSTMs, one for encoding the input sequence, the other for decoding the output sequence (see EncoderRNN and DecoderRNN implementations in the pytorch tutorial linked above).
the final state of the LSTM after consuming the input sentence, then use that state to initialize a separate LSTM decoder, from which you sample autoregressively - in other words, you generate a new token, feed the token back into the decoder, then continue either for an arbitrary number of steps that you specify, or until the LSTM samples an "end of sentence" token, if you've trained the LSTM to predict the end of sampled sequences.
Simply setting return_sequences=False in your first bidirectional LSTM and adding as before RepeatVector(23) works fine
The LSTM on the other hand, expects the input to have size [seq_len, batch_size, num_features] (as described in [nn.LSTM - Inputs (hyper-link)]).
You can either change the dimensions of your input, or you can set batch_first=True when creating the LSTM, if you prefer having batch size as the first dimension, in which case batch_size and seq_len are swapped and your current input size would be the expected one.
Also, you should feed your input to the LSTM encoder or simply set the input_shape value to the LSTM layer.
Always use return_sequences=True within the LSTM layer when feeding a Bidirectional one.
Note that the returned value from Bidirectional(LSTM(..., return_state=True)) is a list containing:
The output for the LSTM is the output for all the hidden nodes on the final layer.
hidden_size - the number of LSTM blocks per layer.
In total there are hidden_size * num_layers LSTM blocks.
Note: for this answer I assumed that we're only talking about non-bidirectional LSTMs.
hidden_size - the number of LSTM blocks per layer.
Therefore each of the “nodes” in the LSTM cell is actually a cluster of normal neural network nodes, as in each layer of a densely connected neural network.
Hence, if you set hidden_size = 10, then each one of your LSTM blocks, or cells, will have neural networks with 10 nodes in them.
The total number of LSTM blocks in your LSTM model will be equivalent to that of your sequence length.
This can be seen by analyzing the differences in examples between nn.LSTM and nn.LSTMCell:
[https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM (hyper-link)]
[https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell (hyper-link)]
An LSTM cell is what maps the input (x) to output (h) and includes all the mathematical relations and gates to perform the mapping.
The figure labeled "The repeating module in an LSTM contains four interacting layers."
shows an LSTM cell.
An "LSTM with 50 neurons" or an "LSTM with 50 units" basically means that the dimension of the output vector, h, is 50.
The LSTM layer will do the same, take an entire input and output an entire tensor.
Is it correct that the LSTM will now process then entire time steps (~81) on the 32 features of the conv1d instead of the 3 features of the input?
The first LSTM will take an input shape of (None, 38,32).
This means this LSTM will process:
For the second Bidirectional-LSTM, by default, return_sequences is set to False.
If you want to get the output of each time_step, then simply use model2.add(Bidirectional(LSTM(10, return_sequences=True , recurrent_dropout=0.2))).
For attention mechanism in LSTM, you may refer to [this (hyper-link)] and [this (hyper-link)] links.
Also, if you want to use stateful lstm it doesn't make sense to shuffle the training data (which is the default).
I know that this doesn't answer directly your question, but to me it confirms what I was thinking : when a LSTM is not stateful, the state is reset after every sample.
Keras LSTM resets state after every batch.
Here is a good blog: [https://machinelearningmastery.com/understanding-stateful-lstm-recurrent-neural-networks-python-keras/ (hyper-link)]
Read LSTM State Within A Batch and Stateful LSTM for a One-Char to One-Char Mapping topics in this blog.
What you can do is basically to regard your batch as 7 succesive batches of shape (m*1*3), and feed them progressively to your LSTM, recording the hidden state and prediction at each step.
allowing to save the whole LSTM intermediate state in a persistent way
It is interesting to take a look at the guts of the LSTM object.
[this link (hyper-link)] describes the Keras LSTM implementation as follows:
Note that the output of a LSTM layer over a single sequence is determined by its weight matrices, which are fixed, and its internal states which depends on the previous processed timestep.
Now to get the output of LSTM layer for a single sequence of length m, one obvious way is to feed the entire sequence to the LSTM layer in one go.
However, as I stated earlier, since its internal states depends on the previous timestep, we can exploit this fact and feed that single sequence chunk by chunk by getting the state of LSTM layer at the end of processing a chunk and pass it to the LSTM layer for processing the next chunk.
Feed the timesteps 1 and 2 to the LSTM layer; get the final state (call it C1).
Feed the timesteps 3, 4 and 5 and state C1 as the initial state to the LSTM layer; get the final state (call it C2).
Feed the timesteps 6 and 7 and state C2 as the initial state to the LSTM layer; get the final output.
That final output is equivalent to the output produced by the LSTM layer if we had feed it the entire 7 timesteps at once.
So to realize this in Keras, you can set the return_state argument of LSTM layer to True so that you can get the intermediate state.
Since you need this chuck processing capability in inference time, we need to define a new model which shares the LSTM layer used in training model and can get the initial states as input and also gives the resulting states as output.
The following is a general sketch of it could be done (note that the returned state of LSTM layer is not used when training the model, we only need it in test time):
Instead, it can just remove the return_sequences=True for the last LSTM layer.
Now, with this trained model, you can simply create a new model exactly the same way you created the trained model, but marking stateful=True in all its LSTM layers.
There are a lot of [great posts (hyper-link)] to build lstm with Pytorch, and you will understand the benefit of dynamic graph once you see them.
It is an LSTM layer with` 512 units.
BasicLSTMCell implements the abstract class RNNCell.
A common way of creating the LSTM layer together with the unrolling for Back Propagation Trough Time is the following one:
In particular, for LSTM, it is a named tuple with two elements, c and h (the two states of a LSTM).
Iterate through tf.trainable_variables and find the variables associated with your LSTM.
An alternative, more complicated and possibly more brittle approach is to re-enter the LSTM's variable_scope, set reuse_variables=True, and call get_variable().
Your intuition is correct; what you need (and what you have described) is an embedding to translate your input vector to the dimension of your LSTM's input.
Then pass the output of that linear layer as the input of your LSTM when you create your LSTM via the rnn_decoder() function [in Tensorflow's seq2seq.py library (hyper-link)] or otherwise.
Or you could have Tensorflow create this embedding and hook it up to the inputs of your LSTM  automatically, by creating the LSTM via the embedding_rnn_decoder() function at line 141 of the same seq2seq library.
(If you trace through the code for this function without any optional arguments, you'll see that it is simply creating a linear embedding layer for the input as well as the LSTM and hooking them together.)
[LSTM Fully Convolutional Networks for Time
Series Classification (hyper-link)]
here is my proposal where I substitute the last block with a simple LSTM layer with 1 output and a sigmoid activation
For understanding recurrent neural networks and LSTMs from scratch, I think the best blog for this is the [Colah (hyper-link)] blog.
Finally, to understand LSTM layers think of them as a simple Dense layers with units as the size of the layer.
Yes, there is a difference, as dropout is for time steps when LSTM produces sequences (e.g.
sequences of 10 goes through the unrolled LSTM and some of the features are dropped before going into the next cell).
After the LSTM you have shape = (None, 10).
A dropout as an argument to the LSTM has a lot of differences.
(You can see the [LSTMCell (hyper-link)] code to check this).
From the docs, it seems like [LSTM (hyper-link)] isn't even intended to take an input_shape argument.
First you should know, method of solving video classification task is better suit for Convolutional RNN than LSTM or any RNN Cell, just as CNN is better suit for image classification task than MLP
Those RNN cell (e.g LSTM, GRU) is expect inputs with shape (samples, timesteps, channels), since you are deal inputs with shape (samples, timesteps, width, height, channels), so you should using [tf.keras.layers.ConvLSTM2D (hyper-link)] instead
In the example, I merge before the final Dense layer to aid the predication along with the timeseries features extracted with the LSTM.
Nico's [simple LSTM (hyper-link)] has a link to a great paper from Lipton et.al., please read this.
I think it makes sense to talk about an ordinary RNN first (because LSTM diagram is particularly confusing) and understand its backpropagation.
LSTM picture and formulas look intimidating, but once you coded plain vanilla RNN, the implementation of LSTM is pretty much same.
My question is how is LSTM backpropagation different then regular Neural Networks
In case of LSTM, it's slightly more complicated: d_next_h = d_h_next_t + d_h[:,t,:], where d_h is the upstream gradient the loss function, which means that error signal of each cell gets accumulated.
But once again, if you unroll LSTM, you'll see a direct correspondence with the network wiring.
batch_input_shape in your LSTM layer, and batch_size in model.fit() and model.predict().
Embedding is pretty important for NLP models and as you see, the LSTM layer expects you to give it the resulting embedding dimensions.
Given your problem statement, you will have to use LSTM for making a classification rather then its typical use of tagging.
The LSTM is unrolled for certain timestep and this is the reason why input and ouput dimensions of a recurrent models are
So the input to our LSTM model are the names fed as one character per LSTM timestep and output will be the class corresponding to its language.
Toy character LSTM model code
The LSTM model requires a 3D input in the form of [samples, time steps, features]
Your code is giving you a network with 3-layers (num_layers), where each layer contains an LSTM with a hidden state of length 2 (n_hidden).
There are not weights between the three LSTM layers: each LSTM feeds its output to the input of the next LSTM.
The operations like forget and update in the LSTM execute some function on a linear combination of the inputs to the network and the network's previous hidden state.
Let's take a look at the LSTM network architecture.
Basically a single LSTM cell maintains a hidden state that represents its "memory" of what it has seen so far, and at each update step, it decides how much new information it is going to blend with the existing information in this hidden state using "gates".
I recommend also looking in to the [gated recurrent unit (GRU) network (hyper-link)], which performs similarly to LSTM but has a slightly easier structure to understand.
and additional output layer weights/biases that you add on top of the last LSTM layer
the working of LSTM 2 being influenced by the output of
  LSTM 1.
Passing the final state of LSTM1 as the initial state of LSTM2 creates the dependency you are looking for.
In order to LSTM2 be able to take into account output1 it's more natural to just concatenate output1 to the input2 and let LSTM2 learn to abstract features from [Input2, Output1].
The solution that concatenates the output of LSTM1 to input2 can be described like this:
As LSTM1 return a sequence (return_sequence=True) you can just concatenate the output of the LSTM1 (seq_len, num_units) to imput2 (seq_len, in_features2) resulting in (seq_len, num_units + in_features2).
num_units is the number of hidden states within a LSTM cell not the number of LSTM cells
A LSTM Layer -> model.add(LSTM(num_unit, input_sequence))
consists of LSTM cells equivalent to your specified input_sequence
An LSTM cell in Keras gives you three outputs:
and you can see an LSTM cell here:
[ (hyper-link)]
The cell state is information that is transported from previous LSTM cells to the current LSTM cell.
When it arrives in the LSTM cell, the cell decides whether information from the cell state should be deleted, i.e.
The carry state is carried through all LSTM cells, i.e.
We can also verify this using the [source code of the LSTM cell (hyper-link)], where a forward step is given by
In add_loss we specify the loss of our interest (in our case: mse) and set the layers used to compute it (in our case: the LSTM output and model predictions)
What you need is to find a method by which you can compute the loss tensor of intermediate layers or output of the hidden activation functions (e.g first LSTM and last Dense in your case).
You're interpreting the hidden state of LSTM1 as a sentence embedding (rightfully so).
And you now want to pass that sentence embedding into LSTM2 as prior knowledge it can base its decisions on.
If I described that correctly then you seem to be describing an encoder/decoder model, with the addition of new inputs to LSTM2.
If that's accurate, then my first approach would be to pass the hidden state of LSTM1 in as the initial state of LSTM2.
That would be far more logical than adding it to the input of each LSTM2 time step.
You would have the further benefit of having an extra gradient path passing from LSTM2 through the state of LSTM1 back to LSTM1, so you would be training LSTM1 on not only the loss function for LSTM1, but also on its ability to provide something that LSTM2 can use to improve its loss function (assuming you train both LSTM 1&2 in the same sess.run iteration).
Another question, what if I wanted to introduce a LSTM3 who's output
  should also effect LSTM2.
In this case, would I just sum LSTM3 and
  LSTM1 hidden state and set that as the initial state for LSTM2?
You control the hidden state size of LSTM2, it should just have a larger hidden state size.
One of the things I didn't mention
  earlier was that sometimes LSTM1 will receive no input, and obviously
  since it's input is a sentence, LSTM1 will receive different input
  every time.
Would this impact the error updates for LSTM1 and LSTM2?
In this case, if LSTM1 has no input (and thus no output state), I think the logical solution is to initialize LSTM2 with a standard hidden state vector of all zeros.
LSTM units argument means dimensions of LSTM matrices and output shape.
If your LSTM layer follows CNN - then its input shape is determined automatically as CNN output.
The first category assumes that you create a separate LSTM encoder and LSTM decoder.
the LSTM hidden size - rnn_size in your code).
As already mentioned by the first comment, a LSTM network is maybe a little bit of an overkill in this case.
I would also call the LSTM a part of the NN, though.
The output of the LSTM is 300-long vector describing the Description field.
[This Answer (hyper-link)] explains the difference between LSTMs and LSTMCells.
LSTMCell is just an Object that is used by the LSTM layer.
For nn.LSTM in Pytorch , as per docs [https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM (hyper-link)]
You can refer this link , is has really nicely explained about LSTM in pytorch , it also has one sample example of SentimentNet model
I think the TimeDistributed Layer expects and returns a 3D array (kind of similar to if you have return_sequences=True in the regular LSTM layer).
Try adding a Flatten() layer or another LSTM layer at the end before the prediction layer.
LSTM layers are designed to work with "sequences".
An LSTM layer requires input shapes such as (BatchSize, TimeSteps, Features).
In regard to the specific design, the LSTM layer has 60 nodes which are densely connected to the 2 final nodes.
That means that each of the 60 LSTM's are directly connected (with appropriate weights) to both of the final Dense nodes.
On the input side, you have defined a single time step with 6 features to the LSTM layer via the input_shape=(1,6).
So, the full input to the LSTM layer would look like:
I believe, each feature then connects to each LSTM node.
You can use the [Flatten (hyper-link)] layer to flatten the 3D output of LSTM layer to a 2D shape.
As a side note, it is better to use dropout and recurrent_dropout arguments of [LSTM layer (hyper-link)] instead of using Dropout layer directly with recurrent layers.
The main purpose of return_sequences is to stack LSTMS or to make seq2seq predictions.
In your case it should be enough to just use the LSTM.
Its not very clear what you're asking, but from my understanding you would like to initialize an LSTM with a given set of weights.
However, the last lstm layer, you have return sequences as false, meaning that the LSTM is returning a single 32 long vector and sending that into the dense layer.
Furthermore, the use of multiple LSTMS here seems to lack purpose, though it does not necessarily harm anything.
In order to fit your presumed data, you would want the last lstm to have return_sequences as True, and have the number of neurons in that lstm not 32, but rather 5, as in the final dimension of your y data.
You could also not have it at all (since you already have two lstms before that, and instead make the second lstm only have 5 neurons and have the final lstm layer be removed entirely.
I happened to be in the same situation where I already had tensors and did not want to use embedding layer before LSTM.
Your interpretation of what the LSTM should return is not right.
Concretely, the first argument of [keras.layers.LSTM (hyper-link)] corresponds to the dimensionality of the output space, and you're setting it to 1.
model.add(LSTM(k, input_shape=(3,4), return_sequences=True))
Try with return_state=True in the LSTM layer.
It allows you to get the last h and c computed by the LSTM.
So you can use them in the following LSTM as initial_state :
LSTM(20, input_shape=(3,20), return_sequences=True) takes as input shape (100,3,20) and returns (100,3,20).
LSTM expects inputs to be shaped as (batch_size, timesteps, channels) (or (num_samples, timesteps, features)) - whereas you're feeding one timestep with nine channels.
It's long, but could save you hours of debugging; this information isn't available elsewhere in such a compact form, and I wish I've had it when starting out with LSTMs.
the input has 9 time lags that need to be fed to the LSTM in sequence, so they are timesteps
Also, make sure to respect the order in which data is fed to the LSTM.
Since the LSTM reads the input from left to right, the 9 values should be ordered as: x_t-9, x_t-8, ...., x_t-1 from left to right, i.e.
If they are not oriented as such you can always set the LSTM flag go_backwards=True to have the LSTM read from right to left.
lstm_clf.add(LSTM(100, input_shape=(1, max_seq_len)))
lstm_clf.add(Dense(7, activation='softmax'))
lstm_clf.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
If you want more processing before throwing 3072 features into an LSTM, you can combine or interleave 2D convolutions and LSTMs for a more refined model (not necessarily better, though, each application has its particular behavior).
You can also try to use the new [ConvLSTM2D (hyper-link)], which will take the five dimensional input:
I'd probably create a convolutional net with several TimeDistributed(Conv2D(...)) and TimeDistributed(MaxPooling2D(...)) before adding a TimeDistributed(Flatten()) and finally the LSTM().
This will very probably improve both your image understanding and the performance of the LSTM.
You have explained the structure of your input, but you haven't made the connection between your input dimensions and the LSTM's expected input dimensions.
That means the input_size of the LSTM needs to be 768.
The hidden_size is not dependent on your input, but rather how many features the LSTM should create, which is then used for the hidden state as well as the output, since that is the last hidden state.
You have to decide how many features you want to use for the LSTM.
The image passed to CNN layer and lstm layer,the feature map shape changes like this
BCHW->BCW
in rnn ,shape name changes,[batch ,seqlen,input_size],in image,[batch,width,channel],
**BCW->BWC,**this is batch_first tensor for LSTM layer(like pytorch).
The batch_input_shape parameter of the Embedding layer should be (batch_size, time_steps), where time_steps is the length of the unrolled LSTM / number of cells and batch_size is the number of examples in a batch.
There is an excellent [blog post (hyper-link)] which explains stateful LSTMs in Keras.
Also, I've uploaded a [gist (hyper-link)] which contains a simple example of a stateful LSTM with Embedding layer.
Since LSTM is a layer, a layer can have only one output in keras (correct me if I am wrong), you can not get two output simultaneously without modifying the source code.
Yes, when using a BiLSTM the hidden states of the directions are just concatenated (the second part after the middle is the hidden state for feeding in the reversed sequence).
The output tensor of LSTM module output is the concatenation of forward LSTM output and backward LSTM output at corresponding postion in input sequence.
And h_n tensor is the output at last timestamp which is output of the lsat token in forward LSTM but the first token in backward LSTM.
Tensorflow now comes with the [tf.contrib.rnn.LayerNormBasicLSTMCell (hyper-link)] a LSTM unit with layer normalization and recurrent dropout.
If you want to use batch norm for RNN (LSTM or GRU), you can check out [this implementation (hyper-link)] , or read the full description from [blog post (hyper-link)].
For the second question, did you mean "I want to LSTM read one sentence at each time step".
If your intention is to work with sentence sequence, I believe that is what you want to do as you are using LSTM, then you need to define the sequence length (number of sentences will be processed in one particular sequence).
Your program is a regression problem where your model consists of 2 lstm layers with 18 and 50 layers each and finally a dense layer to show the regression value.
LSTM requires a 3D input.Since the output of your first LSTM layer is going to the input for the second LSTM layer.The input of the Second LSTM layer should also be in 3D.
so we set the retrun sequence as true in 1st  as it will return a 3D output which can then be used as an input for the second LSTM.
Your second LSTMs value  does not return a sequence because after the second LSTM you have a dense layer which does not need a 3D value as input.
In keras by default LSTM states are reset after each batch of training data,so if you  don't want the states to be reset after each batch you can set the stateful=True.
If LSTM is made stateful final state of a batch will be used as an initial state for the next batch.
how do we connect each of 32-dimensional output produced by each of
the 10 LSTM cells to the next dense layer?
The output shape of this layer is (10, 15), and the same weights are applied to the output of every LSTM unit.
of LSTM cells required for the input(specified in timespan)
of LSTM units required in the output?
You either get the output of the last LSTM cell (last timestep) or the output of every LSTM cell, depending on the value of return_sequences.
how each of the 32-dim vector from the 10 LSTM cells get connected to TimeDistributed layer?
a size-32 vector for each of the 10 LSTM cells.
Hence, dense_outputs has size (10, 15), and the same weights were applied to every LSTM output, independently.
Keras will repeat LSTM, TimeDistributed, etc.
Yes, the output of the LSTM layer is the last hidden unit.
LSTM works way better with that!
In most LSTM Autoencoder structures I observe use of keras.layers.RepeatVector() after the encoder and keras.layers.TimeDistributed(keras.layers.Dense()) after the decoder.
You can simply stack several LSTM layers, for example via the Sequential module:
Short Answer:
If you are more familiar with Convolutional Networks, you can thick of the size of the LSTM layer (128) is the equivalent to the size of a Convolutional layer.
In the left image, a LSTM layer is represented with (xt) as the input with output (ht).
So when your summary is:
    lstm_1 (LSTM)                (None, 10, 128)           91648
It means that your input sequence is 10 (x0,x1,x2,...,x9), and that the size of your LSTM is 128 (128 will be the dimension of your output ht)
The return_sequences=False parameter on the last LSTM layer causes the LSTM to only return the output after all 30 time steps.
If you want 30 outputs (one after each time step) use return_sequences=True on the last LSTM layer, this will result in an output shape of (None, 30, 1).
For a more detailed explanation of LSTMs in Keras, see [here (hyper-link)].
But that has nothing to do with LSTM functioning, but has deal with task itself.
I want sincerly commend you for your efforts in understanding how LSTM works, but the code which you pointed gives example which is applicable to all kinds of NN and explains how to work with text data in neural networks, but not explains how LSTM works.
First up, LSTM, like all layers in Keras, accepts two arguments: input_shape and batch_input_shape.
LSTM layer is a recurrent layer, hence it expects a 3-dimensional input (batch_size, timesteps, input_dim).
numHiddenUnits is the dimensionality of the LSTM hidden state.
For instance, if you set numHiddenUnits = 5 , then the LSTM output is a 5-dimensional vector.
So, it doesn't represent the number of LSTM cells.
Your model works like this: At each time LSTM receives an input and processes it BUT doesn't output until the last time step.
At the last time-step, your LSTM outputs a vector and sends it to the fully-connected layer and gives you the regression value.
Your sketch would be ok if only your last LSTM cell sends an output to the fully-connected layer, not all of them.
No, you still have one LSTM layer with four LSTM Neurons.
I know [Jeff Donahue (hyper-link)] worked on LSTM models using Caffe.
He has a [pull-request (hyper-link)] with RNN and LSTM.
Update: there is a [new PR (hyper-link)] by Jeff Donahue including RNN and LSTM.
To unroll LSTM (or any other unit) you don't have to use [Jeff Donahue (hyper-link)]'s recurrent branch, but rather use NetSpec() to explicitly unroll the model.
These values are what your what your LSTM model predicted as the most likely outcome.
I think the primary confusion is on the terminology of the LSTM cell's argument: num_units.
of LSTM cells" that should be equal to your time-steps.
(Please note this) output_size = num_units; if (num_proj = None) in the lstm cell 
where as, output_size = num_proj; if it is defined.
Now, typically, you will extract the last time_step's result and project it to the size of output dimensions using a mat-mul + biases operation manually, or use the num_proj argument in the LSTM cell.
encoder has in its last layer lstm with two dimension (number_batch, number_features) instead of (number_batches, number_timesteps, number_features).
But what you want to do is the same, as what you do with your decoder: You apply the RepeatVector layer to make the input shape 3 dimensional and therefore able to be feeded into a LSTM Layer.
To feed the encoder output to the LSTM, it needs to be sent through a RepeatVector layer.
In other words, the last output of the encoder needs to have [batch_size, time_steps, dim] shape to be able to be fed into a LSTM.
try
from tensorflow.python.keras._impl.keras.layers.recurrent import LSTM
Use this one:
from tensorflow.keras.layers import LSTM
To understand LSTMs, I'd recommend first spending a few minutes to understand 'plain vanilla' RNNs, as LSTMs are just a more complex version of that.
[This is a famous explanation of LSTMs (hyper-link)] if you want to make the leap from basic RNNs to LSTMs, which you may not need to do - there are just more complicated weights and states.
I cannot understand how those 300 hidden units used for the LSTM cells and how the output comes out.
Are there 20 LSTM cells and 300 units for each cell?
It is simpler to consider the LSTM layer you have defined as a single block.
lstm_7 (LSTM)                (None, 300)               362400
The output shape of the LSTM layer is 300 which means the output is of size 300.
Keras recurrently ran the LSTM for 20 time-steps and generated an output of size (300).
Now, if you add the Dense layer after LSTM, the output will be of size 20 which is straightforward.
Lastly, avoid Dropout and use LSTM(recurrent_dropout=...) instead (see linked SO).
Since Python 3 you can omit the arguments to super to get the same result (as you have done in the LSTM class):
In my case, training a model with LSTM took 10mins 30seconds.
Simply switching the call from LSTM() to CuDNNLSTM() took less than a minute.
I also noticed that switching to CuDNNLSTM() speeds up model.evaluate() and model.predict() substantially as well.
Here, CuDNNLSTM is designed for CUDA parallel processing and cannot run if there is no GPU.
But LSTM is designed for normal CPUs.

keras.layers.LSTM
Intel i5-4690 CPU only:
612235/612235 [==============================] - 3755s 6ms/step - loss: 2.7339 - acc: 0.5067 - val_loss: 2.1149 - val_acc: 0.6175

keras.layers.CuDNNLSTM
RTX 2070 & Intel i7-9700K:
612235/612235 [==============================] - 69s 112us/step - loss: 1.9139 - acc: 0.6437 - val_loss: 1.8668 - val_acc: 0.6469
15x gain over traditional(non Cuda) LSTM implementation!
In TensorFlow 2.0, the built-in LSTM and GRU layers have been updated to leverage CuDNN kernels by default when a GPU is available.
With this change, the prior keras.layers.CuDNNLSTM/CuDNNGRU layers have been deprecated, and you can build your model without worrying about the hardware it will run on.
Since the CuDNN kernel is built with certain assumptions, this means the layer will not be able to use the CuDNN kernel if you change the defaults of the built-in LSTM or GRU layers.
go_backwards just reverses whatever sequence that you give to the LSTM.
Question 3: If I was trying to use the model below and wanted to
  reverse the corpus data, would I set the go_backwards flag to true in
  both LSTM layers or just one?
Not sure why you would want to reverse the input for the decoder(second) LSTM.
So I would recommend to just apply go_backwards to the first LSTM.
Adding an LSTM after a CNN does not make a lot of sense, as LSTM is mostly used for temporal/sequence information, whereas your data seems to be only spatial, however if you still like to use it just use
If you are talking of spatio-temporal data, Use Timedistributed on the Resnet Layer and then you can use [convlstm2d (hyper-link)]
Example of using pretrained network with LSTM:
The main feature of LSTM is the state that transformed between steps.This state is the memory of LSTM that can change the effect of input and can be changed by input and previous output.The Core Idea Behind LSTMs section describes this concept.
The size of this vector is very important.Accordingly, the dimensions of all matrices, named by uppercase W, is affected by the size of LSTM memory because their duty is data transformation and controlling inter-effect between memory and other values.
An LSTM doesn't have 4 layers but 4 weight matrices due to its internal gate-cell structure.
If this is confusing, it is helpful to read some resources on how an LSTM works.
2 seems u need a embedding layer or stuff like that to let lstm accept your data by, either shrink them to low degree, or reduce somehow
keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, go_backwards=False, stateful=False, unroll=False)
here is a example on kaggle:
[https://www.kaggle.com/divrikwicky/fast-basic-lstm-with-proper-k-fold-sentimentembed (hyper-link)]
However, given that you have a large amount of data a 2-layer LSTM can model a large body of time series problems / benchmarks.
LSTM parameters can be grouped in 3 categories: input weight matrices (W), recurrent weight matrices (R), biases (b).
Part of the LSTM cell's computation is W*x + b_i + R*h + b_r where b_i are input biases and b_r are recurrent biases.
That's why cuDNN's LSTM has more parameters than Keras.
Let's go into the real deal(I attached a practice code to look-up for this, so please refer [EMA_LSTM.py (hyper-link)]).
Shall we say, there is a network function containing LSTM:
You just indirectly declares the LSTM variables as 'net4' in the 'network()' function.
[https://github.com/talolard/MarketVectors/blob/master/preparedata.ipynb (hyper-link)] (you will have to replace fc layers with lstm, and fiddle with inputs)
Create LSTM layer to recieve series of inputs.
Create output layer (usually fully connected) to take last lstm state and predict output of your desired size.
in which case you may have 2 lstms outputing a state to your fully connected layer...
nn.LSTM is implemented with nn.RNNBase which puts all the parameters inside the OrderedDict: _parameters.
Also, to know what are the keys value in that OrderedDict, you can simply do:
print(lstm._all_weights).
lstm.weight_ih_l0 does the job as well.
The issue seems to be in the shape of X_train_count you are taking in LSTM input shape is always tricky.
In the LSTM layer, the input_shape should be (timesteps, data_dim).
Whenever cont is zero, the LSTM layer forgets its memory and resets it to the initial value.
At the same time, LSTM retains its memory over multiple subsequent solver steps (forward/backward cycles), so the first entry in each timeseries MUST have it set to zero.
While building LSTMs, are all the Hidden layer units an LSTM cell?
I mean if I have 100 units in a hidden layer, does it mean that there are 100 LSTM cells?
I recommend reading [Understanding LSTM Networks (hyper-link)].
I would say [this code (hyper-link)] has one LSTM layer with 128 units.
One LSTM unit consists of multiple nodes.
In addition, RNNSharp won't get updated for any new feature, you could try Seq2SeqSharp which supports both bi-LSTM and transformer models.
Some modifications you could do to improve the graph are to represent it horizontally (forward pass from left to right), add some style to the individual layer titles and descriptions, replace the word neurons with the word units when referring to the LSTM units, and add some colors to accentuate the difference between features, units, and neurons.
tf.contrib.rnn.LSTMCell objects have a [property (hyper-link)] called variables that works for this.
At least this is the case when using a single LSTMCell.
So for instance, to access the LSTM variables in the above example, lstm_cell.variables suffices.
It represents how many hidden states there are for this layer and also represents the output dimension (since we output a hidden state at the end of each LSTM neuron).
[Understanding Keras LSTMs (hyper-link)]
[What exactly am I configuring when I create a stateful LSTM layer with N units (hyper-link)]
[Initializing LSTM hidden states with
Keras (hyper-link)]
Actually timestep number does not impact the number of parameters in LSTM.
However, I couldn't find anything about horizontal LSTM cells, in which the output of one cell is the input of another.
Having said that, there is no need to implement your own even if your dataset and/or the problem you are working on is new LSTM.
Pick from the existing library (Theano, mxnet, Torch etc...) and modify from there I think is a easier way, given that it's less error prone and it supports gpu computing which is essential for training lstm within a reasonable amount of time.
There are many possibilities your LSTM did not work, even if you implemented it correctly.
The best way to test an LSTM implementation (after gradient checking) is to try it out on the toy memory problems described in the original LSTM paper itself.
For example, when you want to feed in x2 into your LSTM network, you will have to use h1 from the previous cell (i.e., the output from previous timestep) along with the vector for x2.
is an RNN (LSTM/GRU) cell that has a bunch of trainable parameters such as weight matrices and biases.
TLDR: Layers like LSTM/GRU have weights and states, where layers like Conv/Dense/Embedding have only weights.
What reset_states() does is that for an LSTM it resets the c_t and h_t outputs in the layer.
These are the values you normally obtain by setting LSTM(n, return_state=True).
Just the sequential layers like LSTMs and GRUs.
In Keras, all recurrent layers, including [ConvLSTM2D (hyper-link)], have a settable stateful attribute - I'm not aware of any other.
which version of Grid LSTM cells are you using?
GridRNNCell was created some while ago, at that time all the LSTMCells in Tensorflow was using concat/slice instead of tuple.
Since the LSTM layer has two states (hidden state and cell state) the value of initial_state and states is a list of two tensors.
Input shape: (batch, timesteps, features) = (1, 10, 1)
Number of units in the LSTM layer = 8 (i.e.
Input shape: (batch, timesteps, features) = (1, 10, 1)
Number of units in the LSTM layer = 8 (i.e.
Note that for stateful lstm you need to specify also batch_size.
With a Stateful LSTM, the states are not reset at the end of each sequence and we can notice that the output of the layer correspond to the hidden state (i.e.
lstm.states[0]) at the last timestep:
if you set return_sequence=True in your LSTM (which is not your case), you return every hidden state, so the intermediate steps when the LSTM 'reads' your sequence.
So if you define your LSTM layer like this :
If 32 is your time dimension, you will have to transpose the data before feeding it to the LSTM.
You can use two different approaches to apply multilayer bilstm model:
1) use out of previous bilstm layer as input to the next bilstm.
2) Also worth a look at another approach [stacked bilstm (hyper-link)].
Let's say you want to create a stack of 3 BLSTM layers, each with 64 nodes:
The LSTM code is fine, it executes with no errors for me.
If your model only contains LSTM, you should get an error that calls out the LSTM layer that it conflicts with.
you can also consider TimeDistributed(LSTM(...))
You got it right, the hidden state in the LSTMs is there to serve as a memory.
Okay, so the fix is very simple, you can just run the first timestep outside, to get a hidden tuple to input in the LSTM module.
Thus, putting it all together, your final code might be (with some adjustments to make it fit the LSTM):
Try printing the shape of input vectors being generated by TimeseriesGenerator and compare it with input shape of your LSTM layer.
UserWarning: Update your LSTM call to the Keras 2 API: LSTM(1, input_shape=(1, 5))
Then you feed that to an LSTM() which will return the whole sequence of hidden vectors of length 20 so you get a shape = (None, 100, 20).
Depending on which version of tensorflow you are using (you can check the version info by typing import tensorflow; tensorflow.__version__ in python interpreter),  in version prior to r0.11, the default setting for the state_is_tuple argument when you initialize the rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0) is set to be False.
If you installed r0.11 or the master version of tensorflow, try change the BasicLSTMCell initialization line into:
lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=False).
The input format for the LSTM should have a shape (sequence_length, input_dim).
However, I don't understand why you return the whole sequence for the second LSTM?
Return sequence means that you will return the whole sequence, so every hidden output at each step of LSTM.
I would set this to False for the second LSTM to only get a "summary" vector of shape (4,) that your Dense layer can read.
Anyway, even for the first LSTM it means that with an input of shape (4,3), you output something which has shape (4,4), so you will have more parameters than input data for this layer... Can't be really good.
It doesn't make really sense to use a softmax out of LSTM's and the Dense before the last.
I implemented an architecture like this for multi-channel time series data, by unrolling the convolutional network in time and feeding the concatenated output tensors into the LSTM network.
The LSTM network is created by standard tf.contrib.rnn.LSTMBlockCell, tf.contrib.rnn.MultiRNNCell and tf.nn.dynamic_rnn.
First, to clear some confusion: i, j, f and o tensors are not weight matrices; they are intermediate calculation steps that depend on particular LSTM cell input.
All the weights of the LSTM cell are stored in variables self._kernel and self._bias, and in a constant self._forget_bias.
and find that the operation which produces i, j, f, o tensors has name 'rnn/basic_lstm_cell/split', while kernel and bias are called 'rnn/basic_lstm_cell/kernel' and 'rnn/basic_lstm_cell/bias':
The tf.contrib.rnn.static_rnn function calls our basic lstm cell 7 times, once for every timestep.
When Tensorflow is asked to create several operations under the same name, it adds suffixes to them, like this: 
rnn/basic_lstm_cell/split,
 rnn/basic_lstm_cell/split_1, 
..., 
rnn/basic_lstm_cell/split_6.
this will change the shape of input_merged to (None, 2400, 60) and thus acceptable by LSTM layer
So I just tried an experiment, and discovered that it was the output shape of the LSTM, and having a smaller output length and then expanding it with a Dense Layer removes the error.
model.add(LSTM(100, input_shape=(8, 1), return_sequences=True))
Note that before batching and passing it into LSTM you need to convert the strings to vocabulary indices and call embedding lookup on the indices.
If you are sending data without making sequence, which is of dims as (batch, input_dims), then use can use this method [RepeatVector (hyper-link)], which repeat the same weights by n_steps, which is nothing but rolling_steps in LSTM.
I probably won't be able to answer all of your questions but what I can do is provide more information on the LSTM cell and the different components that it's made of.
I hope this clarifies how a LSTM cell works.
I invite you to read tutorials on LSTM as they use nice schemas, step by step examples and so on.
you should see three tensors: lstm_1/kernel, lstm_1/recurrent_kernel, lstm_1/bias:0
One of the dimensions of each tensor should be a product of
That is because each tensor contains weights for four LSTM units (in that order):
We still technically concatenate all incoming hidden vectors, same happens in the stacked/MD-LSTM.
This is the definition of LSTMs that is later used, so we can assume the same thing holds for Grid-LSTMs.
keras.layers.LSTM as with all layers that are direct or indirect subclasses of [keras.engine.base_layer.Layer (hyper-link)] has a add_loss method that can be used to set a starting value for the loss.
I suggest to do this for the LSTM layer and see if it makes any difference for your results.
Now, if you want the output to have the same number of time steps as the input, then you need to turn on return_sequences = True in all your LSTM layers.
The output of an LSTM is:
construct an asymmetric autoencoder, using the time distributed layer and dense layers to reduce the dimension of LSTM output.
1) Remove the MaxPooling1D layer, add the padding='same' argument to Conv1D layer and add return_sequence=True argument to LSTM so that the LSTM returns the output of each timestep:
Although I have no idea why you are doing this, to use the output of the convolution layer (which is (?, n_timesteps, n_features, n_filters), one solution is to use a LSTM layer which is wrapped inside a TimeDistributed layer.
Your LSTM is not returing sequences  (return_sequences = False).
But even if you do the  Conv1D and MaxPooling before the LSTM will squeeze the input.
So LSTM itself is going to get a sample of (98,32).
Your LSTM is returning a sequence (i.e.
Therefore, your last LSTM layer returns a (batch_size, timesteps, 50) sized 3-D tensor.
The other alternative is, if you really have only 1 output value per data point, you need to use return_sequences=False on the last LSTM.
If you mean the specific usage in the implementation rather than their use in the model, you should inspect the [LSTMLayer (hyper-link)] implementation, which uses these variables.
Actually, your problem is closer to the 'generic' use of LSTMs.
The real potential of LSTM models for time series forecasting can be exploiting by building a global model using all the time series, instead as a univariate model, which actually ignores any cross series information available across your time series.
Here, you basically produce multiple input and output tuples for the given set of time series you have and then pool them to together for the LSTM training purposes.
Basically, lstm takes the size of your vector for once cell:
In your example, I guess the lstm_size is 256, since it's the vector size of one word.
Please see this example: [https://github.com/nlintz/TensorFlow-Tutorials/blob/master/07_lstm.py (hyper-link)]
I ran for 1000 epochs, and my results are not as bad as yours, the LSTM seems to make some effort to follow the line, though it seems to just be hovering around the running mean (as one might expect).
It seems to me, a time series should be given to the LSTMs in this format:
Try adding dense layers, or multiple LSTM layers, or fewer LTSM nodes
Did you create nn.LSTM maybe with batch_first=True?
We can add an exclusive InputLayer in the model and specify the dimensions there instead of doing it in the LSTM layer.
Use bidirectional LSTMs, in this way you can get info from forward and backward pass of LSTM (not to confuse with backprop!).
In case of LSTMs there are non-linearites, that's one point.
One last point, depending on the length of sequence, LSTMs are prone to forgetting some of the least relevant information (that's what they were designed to do, not only to remember everything), hence even more unlikely.
The code works because input_shape is passed as a keyword argument (the **kwargs), then these keyword arguments are passed by the LSTM constructor to the Layer constructor, which then proceeds to store the information for later use.
[this (hyper-link)] is a good tutorial on how to reshape your data for lstm models.
The solution is to add the input shape to the LSTM layer:
A lot of complex models, especially LSTMs are unstable.
Although it is possible to migrate the code for keras-rl to use eager execution and therefore an LSTM.
LSTMs need to be updated with a whole episode of learning to prove accurate something which keras-rl does not support.
An LSTM network is used to predict a sequence.
Studying LSTM formulae deeper and digging into the source code, everything's come crystal clear.
Apply BatchNormalization to LSTM's inputs, especially if previous layer's outputs are unbounded (ReLU, ELU, etc)

If previous layer's activations are tightly bounded (e.g.
Why do stacked LSTMs diverge faster?
In addition to feeding ReLUs to itself, LSTM feeds the next LSTM, which then feeds itself the ReLU'd ReLU's --> fireworks.
first, i do not see lstm in your model, its just 4 convo to 3 full connected right?
LSTM over frames i would do instead of  first full connected right after flatten.
If I had scrolled down a little more, I would have found ConvLSTM2D in the docs and this should solve my problem.
For more reference and explanation about LSTM, look at: [this video (hyper-link)]
The LSTM takes as input vectors, not numbers.
add return_sequences to your LSTM code:
By default LSTM only returns it's final output after the last element of a sequence.
If you want to chain two together then you need to pass the output after each element of the sequence from the first LSTM to the second.
The input shape of LSTM supposed to be a 3D tensor such as [batch, timesteps, feature].
You can add CNN and LSTM layers in one model, with Keras.
First, the circle in blue is the essence of LSTM and it will never be disabled or you will not have a rnn/lstm at all.
That arrow means that whatever the value you get from the last rnn/lstm cell, you will pass it to the next rnn/lstm cell and it will be processed together with the next input.
The only difference between rnn and lstm is just that a simple rnn does not have that blue-circled arrow, only the black arrow below while lstm has that arrow as a gate for short/long term memory.
Second, for return_sequences, it is typically used for stacked rnn/lstm, meaning that you stack one layer of rnn/lstm on top of another layer VERTICALLY, not horizontally.
Horizontal rnn/lstm cells represent processing across time, while vertical rnn/lsm cells means stacking one layer across another layer.
It means that if you want to stack one rnn/lstm layers on top of another, you need to set it to true.
If you want to get the output of your LSTM layer "out" given input of "inp" in a keras Sequential() model called "model," where "inp" is your first / input layer and "out" is an LSTM layer that happens to be, for the sake of this example, in the 4th position in your sequential model, you would obtain the output of that LSTM layer from the  data you call "lstm_input" above with the following code:
You can check the pytorch documentation for that: [https://pytorch.org/docs/master/generated/torch.nn.LSTM.html (hyper-link)]
You can pass the initial hidden state of the LSTM by a parameter initial_state of the function responsible to unroll the graph.
In the documentation of [LSTMCell (hyper-link)] this tuple corresponds to c_state and m_state, which [previous discussion (hyper-link)] points out that this represents the cell state and hidden state, respectively.
I think the problem is in the training code since you are using LSTM you are supposed to flush down the hidden and cell state after every epoch and detach it from the computation graph after each batch.
While this is technically possible (you would have to write your own LSTM implementation or extend the existing one, however), it doesn't seem like a good approach towards this problem.
With an LSTM layer, it works the same.
Although an LSTM layer already expects a time dimension in its input shape: (batch, timeSteps, features), you can use the TimeDistributed to add yet another "time" dimension (which may mean anything, not exactly time) and make this LSTM layer to be reused for each element in this new time dimension.
LSTM - expects inputs (batch, timeSteps, features)
TimeDistributed(LSTM()) - expects inputs (batch, superSteps, timeSteps, features)
In any case, the LSTM will only actually perform its recurrent calculations in the timeSteps dimension.
There is no official or correct way of designing the architecture of an LSTM based autoencoder...
The only specifics the name provides is that the model should be an Autoencoder and that it should use an LSTM layer somewhere.
The default behaviour of the LSTM layer in Keras/TF is to output only the last output of the LSTM, you could set it to output all the output steps with the return_sequences parameter.
In this case the input data has been shrank to (batch_size, LSTM_units)
Consider that the last output of an LSTM is of course a function of the previous outputs (specifically if it is a stateful LSTM)
It applies a Dense(1) in the last layer in order to get the same shape as the input.
PyTorch 1:

They apply an embedding to the input before it is fed to the LSTM.
This does not defeat the idea of the LSTM autoencoder, because the embedding is applied independently to each element of the input sequence, so it is not encoded when it enters the LSTM layer.
The author used a number of units in the LSTM layer equal to the input shape.
LSTM is generally used where we deal with the sequences.
Suppose that, you provide one english sentence as input to your LSTM network.
Actually, by adding more modules (like the second LSTM network you just added), you're increasing the model parameters that have to be trained ––also, you will have to give more time to your network to get trained.
units for LSTM, which is last)
In other words, the first output returns LSTM channel attention, and the second a "timesteps attention".
Keras LSTM expects input of shape [batch_size, timesteps, features].
The convolutional layers are supposed to automatically learn the features from the data, and them feed the LSTM layers with them.
There are several possible architectures, DeepConvLstm is one of them:
[DeepConvLstmArch (hyper-link)]
DeepConvLstm paper: www.mdpi.com/1424-8220/16/1/115/htm
When working with LSTMs you have to be careful when creating the dataset splits to further allow the model to generate more accurate sequence predictions, especially when the dataset size is small in each trial created.
If you want to predict a label for each line instead of one label for all lines you need to pass return_sequences=True to the LSTM layer.
Units are the number of cells in your LSTM layer.
Implies that you are adding an LSTM layer that has 32 LSTM cells that are connected to the previous and next layer.
The LSTM model that I use is this one:
RMSE means that on average your LSTM is off by 0.12, which is a lot better than random guessing.
Usually accuracies are compared to a baseline accuracy of another (simple) algorithm, so that you can see whether the task is just very easy or your LSTM is very good.
The RMSE from your LSTM should be smaller than the RMSE from your naive forecast, but even a smaller RMSE is no guarantee that your model is good.
If you need some "old-style" tensorflow stacked LSTM, you can use tf.nn.rnn_cell.MultiRNNCell (now it's deprecated and replaced with tf.keras.layers.StackedRNNCells):
LSTM is best suited for sequence models, like time series you said, and your description don't look a time series.
Any way, you may use LSTM for time series, not for prediction, but for classification like [this article (hyper-link)].
Therefore, you can train your LSTM as a multivariate predictor for your 6th variable, that is the sample label and compare with the ground truth during testing to evaluate its performance.
I think you are using Keras to develop your LSTM model.
[https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/ (hyper-link)]
Your model only has 1 LSTM layer, add a second one to benefit from its "memory":
Try less LSTM units, data_dim is way too much.
Now the LSTM will process 2 timesteps of flattened image features.
So the initial state for the unrolling of the LSTM is always zero.
If you would like to manually update the LSTM's state / self._initial_state you need to define it as a Variable instead of a Tensor.
You don't have to Flatten a LSTM output.
LSTM outputs tensor with shape (batch_size, units).
You can just add a Dense layer after your LSTM layer, without setting 'return_sequences' to False (this is only needed if you have a second LSTM layer after another LSTM layer).
I made an LSTM model recently to predict some future values, depending on the history of that variable.
LSTMDim parameter is the output dimension of that very layer
You can see that LSTMDim and cellDim are set to be 25.
You must set int inDim = 10; in the beginning of the LSTMTimeSeries form as well, to get it started.
You can just replace the GRU with an LSTM.
A LSTM need a state, which consists of two components, the hidden state and the cell state, very good guide here: [https://arxiv.org/pdf/1506.00019.pdf (hyper-link)].
For every layer in the LSTM you have one cell state and one hidden state.
The problem is that Tensorflow stores this in a LSTMStateTuple which you can not send into placeholder.
Then you can use the built-in Tensorflow API to create the stacked LSTM layer.
LSTM Call arguments [Doc (hyper-link)]:
Finally, the input_shape of the LSTM layer can be (None, 50).
Here, the length of twice the input comes from having a bidirectional LSTM.
It looks like you'll need to return [_whatever_previous_lstm_state, hh] in the step_fprop function
LSTM layer accepts a 3D array as input which has a shape of (n_sample, n_timesteps, n_features).
array (15, 4)) is a feature map where there is a local spatial relationship between its elements, say like an image patch, you can also use [ConvLSTM2D (hyper-link)] instead of LSTM layer.
Otherwise, flattening the timesteps and using LSTM would be fine.
But, if you want to use a batch size other than 1, you’ll need to pack your variable size input into a sequence, and then unpack after LSTM.
your output is 2D so set return_sequences=False in the last LSTM cell
LSTM expects 3D data.
In this way, u have 3D data to use inside your LSTM
The input to an LSTM model should be three dimensional, in the format [samples, timestep, features].
Keras LSTM layer expects the input to be 3 dims as (batch_size, seq_length, input_dims), but you have assigned it wrong.
Find details on data preparation for LSTMs [here (hyper-link)].
LSTMs map a sequence of past observations as input to an output observation.
Take away: Now your shapes should be what your LSTM model expects them to be, and you should be able to adjust your data shape to your needs.
Let me copy a function I used for preparing my data for LSTM:
Likewise, if my input looks something like (1000,10,5) and I run it through an LSTM like LSTM(7); then I should know (automatically) that I will get something like (...,7) as my output.
Now the second thing to learn about LSTMs.
Notice the 30 is the TIME dimension used in your LSTM model.
Another common misunderstanding, notice that the LSTM expects you to provide each SAMPLE with it's own COMPLETE PAST and TIME AXIS.
That is, an LSTM does not use previous sample points for the next sample point; each sample is independent and comes with it's own complete past data.
Maybe your LSTM should be an LSTM(2) or LSTM(5) instead of 50...etc.
Then a LSTM(50) would use your 8 long feature space and change it into a 50 long feature space while going over the TIME AXIS of 10.
you may not need to use a TimeDistributed layer at all:
first, just remove the resturn_sequences=True argument of the LSTM layer.
After doing it, the LSTM layer would encode the input timeseries of the past in a vector of shape (50,).
Alternatively, if you would like to use a TimeDistributed layer and considering that the output is a sequence itself, we can explicitly enforce this temporal dependency in our model by using another LSTM layer before the Dense layer (with the addition of a [RepeatVector (hyper-link)] layer after the first LSTM layer to make its output a timseries of length 12, i.e.
number of LSTM layers, the dimension of LSTM layers, etc.)
The [logistic regression (hyper-link)] is used to project the real-valued state of LSTM to a class, as well as to define loss function.
You can create model that uses first the Embedding layer which is followed by LSTM and then Dense.
Now here is the confusing bit, when we say LSTM(100) it means a layer that runs a single LSTM cell (one like in Colah's diagram) over every word that has an output size of 100.
Let me try that again, you create a single LSTM cell that transform the input into a 100 size output (hidden size) and the layer runs the same cell over the words.
Now we obtain (samples, 100) because the same LSTM processes every review of 500 words and return the final output which is of size 100.
Take away lesson is that the LSTM layer wraps around a [LSTMCell (hyper-link)] and runs it over every timestep for you so you don't have to write the loop operations yourself.
In the LSTM (after the embedding, or if you didn't have an embedding)
If each Xt is a 100-dimension vector represent one word in a review, do I feed each word in a review to a LSTM at a time?
No, the LSTM layer is already doing everything by itself, including all recurrent steps, provided its input has shape (reviews, words, embedding_size)
Understanding LSTMs deeply:
If you set return_state=True, then LSTM(...)(X) returns three things: the outputs, the last hidden state and the last cell state.
So instead of X = LSTM(128, return_sequences=False, return_state=True, dropout = 0.5)(X), do X, h, c = LSTM(128, return_sequences=False, return_state=True, dropout = 0.5)(X)
By default you don't have to specify an initial state for the LSTM layer in keras.
If you want to specify the initial state you can do it like this LSTM(units)(input, initial_state), where the initial_state is a list of tensors [hidden_state, cell_State].
For more about how to handle LSTM initial state and sequence to sequence in keras see [this link (hyper-link)]
Each node with name h in Figure 1 represents one LSTM cell.
However, different superindices represent different LSTM cells.
A single-layered RNN network consists of one LSTM cell.
In the multi-layer RNN case, input of an intermediate LSTM cell is output of the previous LSTM cell.
In Figure 1, the data sample x is also fed along with the LSTM output.
Quick Answer: Use [TF-api for LSTM (hyper-link)] which has a parameter called initializer to be used for initializing weight and projection matrices.
Method call to evaluate the multiplications: [https://github.com/tensorflow/tensorflow/blob/3686ef0d51047d2806df3e2ff6c1aac727456c1d/tensorflow/python/ops/rnn_cell_impl.py#L576 (hyper-link)]
lstm_matrix = _linear([inputs, m_prev], 4 * self._num_units, bias=True)
As I know Keras LSTM  reset its state after each batch.
So when your batch size if 1, LSTM resets its memory.
An Alternative would be to flatten the video batch to create an image batch, do a forward pass from CNN and reshape the features for LSTM.
I had the same problem with a LSTM in pytorch.
Using 4 LSTM layers with large sizes means a lot of parameters to learn.
I suspect that your model only learns the weights of the Dense Layer properly, but not those of the LSTM layers below.
As a quick check, what kind of performance do you get when you get rid of all LSTM layers and replace the Dense with a TimeDistributed(Dense...) layer?
ValueError: Error when checking input: expected lstm_2_input to have 3 dimensions, but got array with shape (6, 2)
These are different dimensions of your input to the LSTM.
Based on the question you ask, Convert BasicLSTMCell to bidirectional LSTM - You can use the Bi-Directional RNN wrapper directly as shown in the code below.
Do clarify how you are modifying the LSTM layer class that is causing the error you are facing.
Since state_is_tuple is True by default, you need to pass in lstm.zero_state(batch_size, tf.float32) to the state variable.
Also, make sure you pass LSTMStateTuple objects to the initial_state argument.
Since the next layer is LSTM, this is no problem ... it will happily consume variable-length sequences as well.
Finally, note that when you express the dimensionality of the LSTM layer, such as LSTM(32), you are describing the dimensionality of the output space of that layer.
the first LSTM has 32 cells/units
the second LSTM has 16 cells/units
Yes this code is from Jason’s LSTM tutorials.
Warning: Data for LSTM should have shapes like (batch_size_or_samples, time_steps_or_length, features).
You can just search Google images for stacked LSTM.
This one is a good minimalistic  example of 2 LSTMs:
[ (hyper-link)]
For my understanding, the two LSTM-Cells are connected as usual.
The graph in Figure 2 is probably hard to interpret, but it should be a LSTM with the input of hidden_layer and the state.
LSTMs are a subclass of recurrent neural networks.
Then, your full data can be described by a 3rd order tensor of shape (num_samples, 7, 1) which can be accepted by a LSTM.
Remember the purpose of the RNN you are training is to predict the next character in a string and you have one LSTM for each character position.
If num_unrollings = 5 then instead of only stepping one lstm unit forward every time you step num_unrolling=5 lstm units forward in every time-step.
and the  next time next is called next returns the input and labels for the next 5 lstms
This is so as in each time-step in the RNN learning algorithm you feed in num_unrolling input characters and you only calculate on the corresponding lstm's, while the (hidden) input from the previous part of the sequence is stored in variables that are not trainable.
Remember that the LSTM job is to encode a sequence into a vector (maybe a Gross oversimplification but its all we need).
The number 10 controls the dimension of the output hidden state (source code for the LSTM constructor method can be found [here (hyper-link)].
If you want to control the number of LSTM blocks in your network, you need to specify this as an input into the LSTM layer.
timesteps controls how many LSTM blocks your network contains.
For LSTMs, the hidden dimension is the same as the output dimension by construction:
[ (hyper-link)]
This is certainly the case for Keras LSTM as well and is why you only provide single units argument.
The default value of recurrent_activation is 'hard_sigmoid' for Keras LSTM layer.
BatchNormalization can work with LSTMs - the linked SO gives false advice; in fact, in my application of EEG classification, it dominated LayerNormalization.
Try both: BatchNormalization before an activation, and after - apply to both Conv1D and LSTM
If your model is exactly as you show it, BN after LSTM may be counterproductive per ability to introduce noise, which can confuse the classifier layer - but this is about being one layer before output, not LSTM
If you aren't using stacked LSTM with return_sequences=True preceding return_sequences=False, you can place Dropout anywhere - before LSTM, after, or both
recurrent_dropout is still preferable to Dropout for LSTM - however, you can do both; just do not use with with activation='relu', for which LSTM is unstable per a bug
Don't forget to set return_seq=True on your previous LSTM layer or you will get an error
Now, coming to the other part of your model, since you are working with text, i would tell you to go for tanh as activation function in LSTM layers.
And you can try using LSTM's dropouts as well like dropout and recurrent dropout
You can try adding convolution layer as well for extracting features or use Bidirectional LSTM But models based Bidirectional takes time to train.
You're right that Keras is expecting a 3D tensor for an LSTM neural network, but I think the piece you are missing is that Keras expects that each observation can have multiple dimensions.
A document would be a single sample in a Keras LSTM.
If you're looking for sample code, on the [Keras Github (hyper-link)] there are a number of examples using LSTM and other network types that have sequenced input.
Below is an example that sets up time series data to train an LSTM.
A simple Unrolled LSTM model with 3 steps is shown below.
Each LSTM cell takes an input vector and the hidden output vector of the previous LSTM cell and produces an output vector and the hidden output for the next LSTM cell.
LSTM models are sequence to sequence models, i.e, they are used for problems when a sequence has to be labeled with an another sequence, like POS tagging or NER tagging of each word in a sentence.
There are two possible ways to use LSTM model for classification
Coming to the implementation using Tensorflow, lets see what is the input and output to the LSTM model.
Each LSTM takes an input, but we have 3 such LSTM cells, So the input (X placeholder) should be of size (inputsize * time steps).
So the Input of LSTM will be (batchsize * inputsize * time steps).
A LSTM cells is defined with the size of hidden state.
The size of output and the hidden output vector of the LSTM cell will be same as the size of the hidden states (Check LSTM internal calcuations for why!).
We then define an LSTM Model using a list of these LSTM cells where the size of the list will be equal to the number of unrolling of the model.
I have skipped lots of things like how to handle variable length sequence, sequence to sequence error calcuations, How LSTM calcuates output and hidden output etc.
Coming to your implementation, you are applying a relu layer before the input of each LSTM cell.
I dont understand  why you are doing that but I guess you are doing it to map your input size to that of the LSTM input size.
LSTM does a set of operations on the input and previous hidden output and given an output and next hidden output both of which are of size hidden size.
You can look my toy example which is a classifier using bidirectional LSTM for classifying if a sequence is increasing or decreasing or mixed.
[Toy sequence_classifier using LSTM in Tensorflow (hyper-link)]
In the original code for the [LSTM class (hyper-link)], line 510, it also seems that hidden/cell states are initiated at zero if no values are explicitly passed.
The LSTM layer takes a 3 dimensional input, corresponding to (batch_size, timesteps, features).
The LSTM layer is adapted to sequences formats (sentences, stocks prices ...).
When specifying 'input_shape' for the LSTM layer, you do not include the batch size.
[Source: Keras RNN Layer, the parent layer for LSTM (hyper-link)]
Another possibility is that LSTMs are very strong at context and position based reasoning, and from what I glean about your task it seems that you are looking more for keywords and less for long distance relations.
Apart from the hidden state, LSTM also has cell state, C. Therefore, a tuple is passed I think.
See [https://pytorch.org/docs/stable/nn.html#lstmcell (hyper-link)].
Note that this is the case for LSTM, GRU or RNN do not have C.
The last LSTM layer must also be set to return sequences, since you want a result for every timestep and not just the last one.
I have a dummy LSTM layer to showcase how the architecture works, feel free to stack more layers.
Therefore, do choose your next layers accordingly as 512 LSTM cells may be too much :)
For details of how the input vector is connected to the hidden state vector, please google LSTM cell and you'll find some good explanations.
The problem is that the way you are reshaping your input is incompatible with an LSTM layer.
An LSTM layer expects an input with 3 dimensions: (batch_size, timesteps, features).
So, it makes more sense to leave out the Reshape layer and simply use input_shape (84600, 6) for your LSTM layer:
For LSTM based RNN, the input should be of 3 dimensions (batch, time, data_point).
Q1: What is the meaning of units in LSTM?
[model.add(LSTM(units, ...))]
For this, the parameter return_sequences of your last LSTM layer should be False.
eli5's scikitlearn implementation for determining permutation importance can only process 2d arrays while keras' LSTM layers require 3d arrays.
I understand this does not really answer your question of getting eli5 to work with LSTM (because it currently can't), but I encountered the same problem and used another library called [SHAP (hyper-link)] to get the feature importance of my LSTM model.
For example, while creating decoder_model we use decoder_lstm layer which was defined as a part of full model,
decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs),
If you check the [documentation (hyper-link)], LSTM requires the input of shape seq_len x batch_size x input_size.
If you declare LSTM with batch_first = True, then LSTM would expect an input of shape batch_size x seq_len x input_size.
You can split 1000 records into small batches and feed them to the LSTM.
So, you can feed the 5 feature values to LSTM and use the output vectors to predict the price value.
According to the [LSTM cell documentation (hyper-link)] the outputs parameter has a shape of (seq_len, batch, hidden_size * num_directions) so you can easily take the last element of the sequence in this way:
Thus output from Embedding layer can be transfer to LSTM layer in Keras LSTM layer which accepts 3D tensor.
The problem turns out to be the misunderstanding of the batch size and other features that defining an nn.LSTM.
Using LSTM is a good approach.
However, LSTM will only learn the noises you have fed it with.
A paper titled Learning when to trust distant supervision: An application to
low-resource POS tagging using cross-lingual projection has done a similar approach on correcting common mistakes on linguistics tagging using LSTM.
To circumvent over fitting with your LSTM architecture try the following things in this order:
Then you can try to modify the architecture of the LSTM, here you already added dropout (maximum value 0.5), I would suggest to try 0.2, 0.3.
The LSTM layer will by default reset its state after it has processed one batch of data.
Although, You could also add return_sequesnce=True to your lstm layer and the network will run as is.
According to [this (hyper-link)] Keras Sequential Model guide on "stateful" LSTM (at the very bottom), we can see what those three elements mean:
Here is the thing about LSTM, as elegantly designed as it is, it still suffers from vanishing gradients and/or exploding gradients.
However, this does not mean LSTM is free from exploding/vanishing gradients when you feed an infinitely long sequence into it.
You can try training you LSTM on your segmented data subsets.
LSTM default for return_sequences is False, so the LSTM layer will output only the last cell output -> (None (batch), units=MAX_SLIDER_VALUE).
If you want to output (None, 495, MAX_SLIDER_VALUE), change return_sequences to True in the LSTM initialization (units does not have to be MAX_SLIDER_VALUE), and the Dense units to MAX_SLIDER_VALUE.
A cell in the Tensorflow universe is called an LSTM layer in Colah's universe (i.e an unrolled version).
Therefore, in order to understand num_units in Tensorflow, its best to imagine an unrolled LSTM as below.
So if you have X_t as a 1D array in TensorFlow, then in the Colahs unrolled version each LSTM cell x_t becomes a scalar value (Please observe the capital case X (vector/array) and small case x(scalar) - Also in Colah's figures)
If you have X_t as a 2D array in the Tensorflow, then in the Colahs unrolled version each LSTM cell x_t becomes a 1D array/vector (as in your case here) and so on.
Tensorflow uses the implementation of LSTM cell as defined in Colahs universe from the following paper:
your data are in 3d format and this is all you need to feed a conv1d or an LSTM.
if your target is 2D remember to set return_sequences=False in your last LSTM cell.
using a flatten before an LSTM is a mistake because you are destroying the 3D dimensionality
The amount of cells of an LSTM (or RNN or GRU) is the amount of timesteps your input has/needs.
For example, when you want to run the word „hello“ through the LSTM function in Pytorch, you can just convert the word to a vector (with one-hot encoding or embeddings) and then pass that vector though the LSTM function.
And each input can even have a different amount of timesteps/cells, for example when you want to pass „hello“ and after that „Joe“ the LSTM will need different amount of iterations (5 for hello, 3 for Joe).
So what happens in this (outputs, hidden = lstm(input, hidden)) line?
Is it clear now how what the LSTM function does and how to use it?
It is usually faster, check available layers [here (hyper-link)] keras.layers.CuDNNLSTM is what you are after.
On top of that you can still employ LSTM normally.
On the upside: you will not encounter vanishing gradients (could be mitigated a little by Bidirectional LSTM as well).
[https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model (hyper-link)]
Simply put, the reason there are so many parameters for an LSTM model is because you have tons of data in your model and many weights need to be trained to fit the model.
the input dimension is 6 and the hidden neurons in the first LSTM layer is 64.
so the first LSTM layer takes input [64 (initialized hidden state) + 6 (input)] in this form.
But LSTM has 4 FFNN, so simply multiply it by 4.
If your inputs are time series, you should use Conv1D:
model = Sequential()
model.add(Conv1D())
model.add(LSTM())
model.add(Dense(softmax))
If your inputs are sequences of images or videos, you should use ConvLSTM2D.
You want to build a Stacked LSTM network with multiple features ( what you name parameters is often called features ), this is described in [https://machinelearningmastery.com/stacked-long-short-term-memory-networks/ (hyper-link)] and [https://machinelearningmastery.com/use-features-lstm-networks-time-series-forecasting/ (hyper-link)] and [https://datascience.stackexchange.com/questions/17024/rnns-with-multiple-features (hyper-link)]
RNNs and so LSTMs are only able to handle sequential data, however this can be expanded by a feature vector with more than one dimensions ( your ensemble of parameters  as described in the answer in [https://datascience.stackexchange.com/questions/17024/rnns-with-multiple-features (hyper-link)] )
The displayed structure of the 6 LSTM cells in 2 layers is  a Stacked LSTM network with 2 layers feature_dim = data_dim=6 (or 7) ( number of your parameters / features ) and timesteps=3 ( 2 layers with 3 unit in each layer ) cf section Stacked LSTM for sequence classification in [https://keras.io/getting-started/sequential-model-guide/ (hyper-link)] and [How to stack multiple lstm in keras?
Setting the accurate input shape is vital cf [Understanding Keras LSTMs (hyper-link)], your network is many-to-many case.
The shape of the input passed to the LSTM should be in the form (num_samples,timesteps,data_dim) where data_dim is the feature vector or vector of your parameters
When creating the LSTMCell like above, these are the same.
Also see the [LSTMCell documentation (hyper-link)].
This has the advantage that the DeepCore object supports the start state methods required for dynamic_rnn, so it's API compatibe with LSTM or any other single-timestep module.
Initial weights for LSTM are small numbers close to 0, and by adding more layers the initial weighs and biases are getting smaller: all the weights and biases are initialized from -sqrt(k) to -sqrt(k), where k = 1/hidden_size ([https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM (hyper-link)])
If you try LSTM with bias=False, you will see that output getting closer and closer to 0 with adding more layers.
