hello everyone and welcome back to
another video by endless engineering
today we're gonna be implementing
logistic regression in Python from
scratch to do some handwritten digit
classification we're gonna be using the
great ascent algorithm and if you're not
familiar with this algorithm I recommend
you go back and watch my video that
details all of the innervation for this
algorithm because we're I'm gonna be
using the same notation here and you
might not be able to follow along so
essentially what the great incentive
algorithm does is it updates this
parameter theta using this term right
here which comes which is derived using
the approach of maximizing the log
likelihood function and this is the log
likelihood function which we which we
derived in a previous video using a
Bernoulli distribution and H here is the
sigmoid function which is our hypothesis
theta are our parameters and X bar is
the feature vector augmented by one and
essentially in this video what we're
going to be implementing is the batch
gradient descent or batch gradient
descent they're really the same
algorithm and this algorithm basically
loops through a number of math of
iterations and in every iteration it's
going to go through the entire data set
and keep updating the parameters and in
a previous video when I talked about
gradient descent with linear regression
I mentioned this not might not be the
greatest algorithm to use for large data
sets however you'll see today that it
pretty much suits our need and that's
really the key of using any machine
learning algorithm is that you pick the
algorithm that works for you and for
your problem and recall that the
logistic regression problem is
essentially a binary classification
problem but we have ten digits that will
classify 0 1 all the way to 9 so this is
a multi-class classification problem and
the way we're gonna be doing that is
what I like to call the brute-force
approach but it's referred to as the one
vs. all or one versus rest approach
where if you have n classes in our case
we have ten different classes then we
will train and different classifiers in
this case ten classifiers and when we
get a new data point we want to predict
what class it belongs to we run all of
our ten different classifiers and we
pick the one that has largest
probability but it doesn't matter in
this case because we're going to build
one
kind of classifier with loose tech
regression classifier and we can then
reuse it for whatever number of classes
that you want now this might not be the
best way to do multi-class
classification but you'll see in this
video that it's simple enough but it
gets pretty good results so let's start
so I'm starting out here by some code
that I've written before I'm importing
the digits data set which is in SK learn
data sets and I'm also importing the
shrink test split from SK learn as well
this is gonna help us split our data set
according numpy of course and PI plot so
we can plot stuff you're going to this
is our first plot here where we're going
to see the digits themselves so I just
basically took a couple of data sets
here and plotted them to show you what
they look like so they're not really
that clear to us and again remember
these are 16 by 16 images right so
they're very small they're not really
great quality but again this is an
example that shows you this is real
actual real world data where we're going
to take handwritten digits that have
been digitized into images and we're
going to write a logistic regression
class here that is going to allow us to
classify these digits so I've prefilled
out some functions here so that we can
code them together so this is a class
that I'm gonna create this called ristic
regression and it's going to basically
allow us to train on the data set and
then predict however we want and we're
also going to be able to measure the
accuracy of our classifier so let's
start out with this set values function
so there's a bunch of values that we
need to set essentially for them for the
classifier to be able to do its job the
first is going to be the parameters so
the parameters here are passed in so I'm
gonna store them as a member variable
here in params and also alpha which is
my step size in the gradient descent
algorithm I'm gonna store that in here
as well there's also the maximum number
of iterations that is defined by the
user so we have it in our interface as
well and here I have another parameter
called class of interest and I want
stop here and talk about this for a
second so class of interest in this case
is basically because this logistic
regression classifier is a binary
classifier so you can either tell you if
the class that it's classifying is a 0
or a 1 so it can either tell you that
whatever data point you gave it is the
class that it knows how to classify or
not and in this case let's say we're
gonna train to learn the digit number 8
so the class of interest that we would
pass through this classifier would be 8
and that way it knows that it's going to
learn number 8 and anything else that it
sees is going to be a not 8 essentially
and this goes to the concept of one
versus all right when we train eight
different classifiers or ten different
classifiers they're all going to be
trained on a class of interest and
they'll be able to identify that class
of interest and essentially when you
pass it a image and it's going to
classify it then it's going to tell you
the probability that it is the class of
interest or not and then you pick the
one with the largest probability out of
all of your classifiers so our next
function here is a sigmoid so the
sigmoid function is very important for
our hypothesis right so I'm just gonna
code it right here so it's 1 divided by
1 plus the exponential 2 minus X right X
here is the input to this function so it
doesn't have to be the actual X of
features that we have I'm just calling
it X and I also have a predict function
now this function is going to let us use
the sigmoid function so that we can
predict the output or the probability
that something belongs to some input
belongs to a certain class and I'm going
to use the sigmoid function sigmoid
function here and I'm going to do a dot
product I don't know I can't type today
numpy dot product and that's going to be
between params and X the input right so
remember here also actually this won't
be X this will be
bah right so to predict we're gonna have
like x-bar and we're gonna do a dot
product with X and X bar and we're gonna
take the sigmoid on that and that's
gonna give us the probability that
something belongs to a certain class
right that's a prediction for a
probability okay great
our next function will compute the cost
and remember the cost here is
essentially the likelihood log
likelihood cost which we defined earlier
let me scroll up back there this is the
cost that we have right it's the sum
over Y which is the label itself
multiplied by the log of our hypothesis
plus 1 minus y log of 1 minus hypothesis
right it's a very nice function it comes
from it assumes a Bernoulli distribution
it allows us to do this binary
classification so let's go ahead and
write that here so say we have our input
variables and our output variables are
params and remember this is the entire
data set right every time we want to
compute the cost we pass in the entire
data set so let me let me say that we
have to loop we have to loop over the
data set itself zip with input input
fair and output well in this case the
output variables are going to be the
labels themselves and say X bar here
remember we have to do we have to
augment our input from the we have to
argument our input of the features with
the 1 on top right so we can have a a
bias term right that we trained not that
we trained and we find ok so now we have
Y hat which is our prediction right
we're using the function that we wrote
to predict X bar and params
all right so that gives me my predicted
value and now for me to do the to do to
compute the cost itself right I need to
do a sum so outside of this loop let's
just define a cost equal to zero right
and then we will add to that why okay so
this is important here let me call this
Y binary and multiply it with let's
write out the log-likelihood first and
then I'll explain what that binary why
binary is essentially 1 minus y binary
that multiplied by the log of 1 minus y
hat right so just to clarify what I mean
by Y binary here so this is essentially
the binary class that we are trying to
classify right and what do I mean with I
remember what I told you that we're
passing in the pram class of interest so
this is the class of interest of tranked
and again this is a binary
classification problem so for us to know
what Y binary is it can either be a 0 or
a 1 right so we'll see the Y binary is a
is a 1 if Y is the class of interest
right else this is going to be 0 right
so in this case if say I'm trying to
learn the digit 2 then if Y is equal to
the class of interest 2 which is coming
from the label then my binary Y is equal
to 1 otherwise it's going to be 0 so
it's 1 means it's definitely the class
of two digit 2 otherwise it's going to
be 0
and I'm going to use that in my cost
which is the binary y times the log of
the predicted value of the probability
time plus 1 minus y binary times log of
one minus the probability right so this
I'm gonna basically keep summing over
this cost and I return this cost over
here right okay so let's recap real
quick what did we do we basically
started out this class as setting values
for my initial parameters for my alpha
for my max iterations class of interest
that I want to train on i define a
sigmoid function and I defined a
prediction function that allowed me to
do the prediction of the probability
given an input that it belongs to a
certain class and I'm computing the cost
using the log likelihood function okay
so now we need to write a function that
allows us to Train based on passed in
data right and this is again coding the
batch gradient descent right so let's
start out with an iteration
let's say we're in iteration one we're
going to start from there and let's say
that while my iteration is less than
less then my max iterations right that's
how many times I'm gonna loop I'm not
going to go more than a consideration
then what I'm gonna do is I'm going to
loop over my data set correct because
every time I get an iteration I'm going
to loop over my data set and I'm going
to say okay give me my give me my
predicted value and tell me what my
gradient is what I can update what that
is so zipping my input and my labels
right I'm gonna loop through them and
I'm going to say give me my X bar which
is augmenting again here now you could
write a function that allows you to do
this I'm just gonna write it here it's
just I feel that it's quicker
right so what is this doing basically
I'm inserting a one in front of my input
variables and then I'm computing my
prediction right I predict with x-bar
and my params right and then I need to
compute my gradient and that's going to
be equal to Y minus y hat times X bar
right but again here what is y
it's the same concept as this Y binary
over here right we can't go and pick Y
as the actual label itself so what we
have to do is we have to find out y
binary and in this case Y binary we get
it from x y1 right so if this X y1 which
is essentially the label if it's equal
to the class of interest it's been
defined by the user then the Y binary is
1 otherwise the Y binary is 0 and I
compute my gradient using that Y binary
binary right and now I can update update
my params to be I'm gonna use my step
size alpha and multiply with the
gradient right and after this is done
after I loop over this I have to
increment my iteration right okay so
basically what we're doing here is we're
getting an input and an out on the end
labels to these inputs and we have I'll
tell you what this is in a second we
iterate from one to the maximum
iterations and in every iteration we're
looping over the day
a set and we are defining x-bar which is
a numpy array that's inserting a 1 in
front of all my inputs and then I'm
predicting my probability that this X
bar belongs to a certain class or not
and then I'm computing my Y binary which
tells me that in fact the state is this
data point is labeled as the class of
interest or not and I'm using that to
compute my gradient which I then used to
update my prams actually this needs to
be plus equals I'm incrementing and
going back to this print iteration so
basically what I want to do is I want to
print my cost when I'm training so that
I can see that I'm doing the right thing
so instead of printing every iteration
because we're gonna have a large number
of iterations we're going to print every
so often iterations right so these print
iterations are set to 5000 is default
and the user can can change them and
what we're gonna do is we're gonna print
the iteration and we're gonna say that
and we're also going to print the cost
right and the cost is I can compute my
cost right from the function that I
broke so and I need to pass to compute
cost my input there and my labels also
my params
alright ok so let me also print like a
newline like uh so that we can know you
know every other region looks nicer this
way ok so now we have a function that's
going to allow us to train our model but
we also want to test how well our model
is doing right how well our classifier
is doing and the only way to do that is
to compute what's known as the accuracy
so let's say we are going to have we're
gonna split our we're gonna split our
datasets and we're gonna have a training
and a test sense right so let's just
make a few variables here so I have the
total classifications as zero and the
correct classifications is zero what
we're gonna do here is we're going to
run our model on the test data and we're
gonna classify every point so let me
loop over the input and labels right for
test and when I loop over this data what
I'm going to do is exactly the same
thing as I did up here right what I'm
gonna do is I'm going to compute my X
bar and then I'm going to compute my Y
hat and then I'm gonna compute my binary
so let me just copy this here and edit
it right so X bar is going to be like
this and then Y hat is going to be X bar
and params of the predict and then my Y
binary is going to be one if my y is
equal to the class of interest otherwise
it's going to be 0 right and so this
every time I run in this every time I do
this I'm going to have to increment my
total classifications right because
every time I loop I over one one data
point I'm going to do one classification
and now that I've done this
classification I need to check if what I
did if what I classified is correct so
what I can do is I can say if my Y hat
is greater or equal to some threshold
let's say point five I so let's say that
I'm I want upset this first threshold
and say if I classify something as 50%
that means that it is in fact the class
right if I get a probability that
something is 50% and it's in fact the
class itself and you can you know you
can pick that threshold however you want
let's just start with this and let's
just say Y binary is 1 right so that
means that I did get a probably that
it's 50% or more that this is a 1 binary
right so it is the class I'm interested
in so that means that I made a correct
classification so I will increment my
correct classification right so this is
basically a correct classification of
class of interest let's put a note here
say this is a correct classification of
class of interest right but there's also
other cases right there is the case
where I have a probability that is less
than 0.5 right so I have a low
probability below the threshold and my Y
binary is not equal to 1 so this is not
the class of interest right this is also
a correct classification of another
class right so this means that my Y
binary tells me that you know the label
tells me that this is not the class
you're interested in and my probability
is that less than 50% that it is a class
you're interested in so that's also a
correct classification right so and now
to compute accuracy what we do is we
divide done to compute accuracy we
divide the correct classifications by
the total number of classifications
right so that basically tells you the
ratio of how many times you got a
collection versus what your total
classifications were alright so let's
say that you did 100 classic
and you got ten correct that means you
have a ten percent accuracy right okay
let's just look at this again and see
what's doing so this is the test
function we're passing in the test data
the test input and label and we're
saying that we have 12 classifications
on correct classifications we loop over
the entire data set and we do a
classification every time by computing X
bar and then we are doing a prediction
here of what this X bar is using our
parameters and we're computing the Y
binary again we talked about this before
and then I'm saying that if my
probability that this is in fact the
class of interest is more than 50% and
the label tells me there's a cost of
interest that means I got a correct
classification and if the probability
that it is not the class of interest
what is the probability that it's of
class of interest is less than 50% and
it is in fact labeled as not the class
of interest this should be a zero right
oh no that's correct this should be one
not equal to one then I do get a correct
classification and to compute the
accuracy of this classifier we divide
the correct number of classifications by
the total number of classifications okay
we have an error here line 26 ok so
there's an okay so now we have written a
logistic regression class that allows us
to train on data and test our accuracy
so what I'm going to do is I'm going to
split the data set that we got from SK
learn on the digits we saw this data set
over here right I'm gonna just split it
and randomly assign it and randomly
shuffle it that's what the test train
split does and I've written some code
here to allow me to use the class that I
just defined to define a digit
regression classifier for the digit 0
right and I've set the maximum number of
iterations to 10,000 and my alpha to
point hole 1 so let's go ahead and run
this okay so now that I've run this I
can see that I have throughout my
iteration my lord likely
cost is increasing because this is again
graininess m so it starts from negative
number and it keeps going up obviously
the maximum it could go to is zero so we
got pretty close right so this is good
now what I can do is I can use my tests
a test data and I can check what my
accuracy is right and just to make a
note here I'm dividing my input values
by sixteen because the if you look at
the data in the digits data set then you
will see that they basically have a
maximum value of sixteen right and it's
usually good when you work with images
to normalize the value that makes makes
it easier for the algorithm to work
because when you have a summation in the
log likelihood and when you keep
iterating over your when you keep
iterating over your parameters then you
might get into problems with your
gradient becoming too shallow and things
like that but anyway what I'm going to
do here is I'm basically gonna give the
classifier that I just trained using
training data right I passed in training
data here I'm going to test its accuracy
using test data so this is data it's
never seen before and basically what
this is going to do it's gonna run my
test function and what my test function
does is it takes all the eight test data
and it loops over it and it classifies
and he uses the 50% threshold to say
that you know whether this was
classified correctly or not and we're
gonna get the accuracy so let's run this
and you can see here I get an accuracy
of a hundred percent that means I
classified every digit that was a zero
correctly and every digit that was not a
zero correctly now to be honest with you
when I first got this result I was kind
of astounded I didn't think I would get
this good of a result but I checked the
I checked data I checked my algorithm
and I also plotted some stuff in another
notebook and I looked at it and it seems
really legit so what I'm gonna do here
is I'm gonna train a bunch of other
classifiers that are going to give me
the classifier for one for
in other digits and let's look at what
their accuracy is so now I've trained
classifiers to classify the one digit so
in this case you can see that we start
out we have a cost and that cost is you
know negative relatively high it goes
down a little bit but not a lot so it
doesn't go down as much as we did for
the zero digit right but we still get a
ninety seven point five percent accuracy
on the test data and I'm using the same
number of iterations as before ten
thousand in the same step starts so this
is pretty good this is a pretty good
result so basically you can see the cost
is still going down so if I increase the
number of iterations and I played around
my hopefully I could probably get a
better result than this and I also
strain classifier for the two digit and
for the eight digit and for the two
digit I got a 100 percent accuracy on
classification and you can see that the
cost here went considerably done and
remember here we're training on data and
we're testing on entirely different data
so the data that we test on is David
this classifier has not seen or these
classifiers have not seen before so
these are pretty good results really now
for the eight digit I tried you know I
just tried 0 1 & 2 so I jumped up to 8
we get a 96.1 accuracy we get a 96.1
accuracy and here again I'm using only
10,000 iterations and the alpha is 0.01
so potentially I could play around alpha
increase the number of iterations
because I can see that my cost is still
going down here when my glass iteration
exited and so again this is taking a
little bit of time to run it takes
somewhat of around maybe 2 minutes to
run per classification so a lot of stuff
that we wrote would like and if you look
at the class we wrote using for loops
and things like that that could be
optimized by potentially using numpy
matrix multiplication and things like
that I just wanted to write it here in a
way that makes it clear from an
algorithmic point of view but there's
definitely a lot of software
enhancements that can be done here that
could make it faster and it could train
faster and you can iterate and do a lot
of things but I'm really happy with the
results that we got we're getting around
96 97 percent to 100 percent accuracy on
some digits and this just goes to show
you that with machine learning it's
really all about picking the correct
algorithm for you to do a certain task
now if you know one algorithm you know
if you only have one hammer everything
looks like a nail right but if you learn
multiple algorithms and you really
understand the basics and concepts of
you know the algorithm itself and how
it's derived and how it works then you
can really pick the best algorithm for
your data now I'm not saying that you
know logistic regression is the best
algorithm for handwritten digit
recognition especially for a multi-class
classification problem I'm just showing
you that a simple algorithm like this
can go a long way in at least a first
pass solution at a problem and that's
what you should strive for is picking an
algorithm that works for you that is
based on your data set and based on the
problem that you're trying to solve I
hope you enjoyed this video on
implementing logistic regression from
scratch and Python I'm gonna leave this
notebook up on github and you'll find a
link for that in the description below
if you enjoyed this video hit that
thumbs up button and think about
subscribing to endless engineering and
supporting the channel also maybe hit
the notification bell that way you get a
notice every time we roll out a new
video thank you for watching
you