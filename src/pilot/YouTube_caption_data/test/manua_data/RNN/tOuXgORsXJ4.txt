We are going to learn about GRU or
gated recurrent unit today.
This version of RNN is newer than LSTM, 
this was invented in 2014 and is gaining
a lot of popularity.
So let's see how this works. You all know
that the basic RNN
suffer from a short-term memory problem.
For example
when you are trying to make an
autocomplete for a statement where it
says
Dhaval eats Samosa almost every day it
shouldn't be hard to guess that this
favorite cuisine
is let's say after is you want to make a
guess.
So in order to guess that it's an Indian
cuisine
you have to remember samosa but
traditional RNN
has shorter memory. So for example when
you are at the statement is,
hypothetical let's assume it remembers
only last two words.
So when it has cuisine is it cannot
predict
that this is an Indian cuisine it has to
remember samosa.
So if you have a network like this which
can remember
remember the important keywords
okay, so let's say when I say Dhaval eats
it doesn't remember anything because
it's not important and when it comes to
samosa
it remembers that and it carries that
long-term memory all the way
then then when you may want to make a
guess
on what that cuisine is, you can use that
memory
and you can deduce that this is an
Indian cuisine.
Now in last video we learned about LSTM
and LSTM can do this
it can remember long term memory.
So if you have not seen that last last
video
uh please go and watch LSTM video
because it will
give you understanding on certain
concepts. So it has
long term memory and short term memory.
GRU is a
modified or a lightweight version of
LSTM
where it combines long and short-term
memory into
its hidden state okay? So you see the
basic difference:
LSTM has two like cell state and
hidden state, here it has only hidden
state which can combine both long and
short term memory. If you look into GRU
box
it will have two gates, LSTM had
three gates input output and forget,
GRU has only two gates update get and
reset get
and the function of update gate is to
remember
is to basically it knows how much of
pass memory to retain. Whereas reset gate
knows how much of pass memory to forget.
Now they sound
little similar but actually there is
some subtle difference and we'll look
into it
using an example. And there is certain
other mechanism which I'm going to show
and then in the end you get like
output as well as output hidden state. So
let's go back to our original uh
NLP problem of completing
an english sentence. So when you have
sentence like this
of course the answer is Indian you might
have seen
gmail right when you type something it
tries to auto complete so
we are trying to solve the same problem
and
it knows that it's an Indian cuisine
based on this word samosa.
But when you have a longer statement
like this where
you start a second statement saying that
his brother
loves pasta and cheese, then the
answer of this autocomplete would be
Italian.
So now watch this carefully what happens
is when you are until this word
okay when you are say cuisine is,
then you want to remember about samosa
because
you have that much in context okay? And
when you
go further and when you come here
till pasta now you know the context has
changed.
So when it comes to pasta you want to
forget about samosa okay?
Similarly when it comes to cheese
you want to retain the memory of pasta.
So now you already see
kind of the difference that here you
want to forget about samosa when pasta
comes,
but when you want to go to end and
cheese and words like that, you want to
retain the memory of pasta
such that you can auto complete it
saying that it's an Italian cuisine.
So in GRU now now this cell that I'm
showing you is GRU
the first thing is when pasta comes you
want to forget about samosa
and the way it is handled is using a
reset gate. 
A reset gate takes hidden state ht minus
1
and the current word xt and it will do
this mathematical operation like
weighted sum applied sigmoid activation
on top of it,
and the rt value that you get is your
reset gate value. When it comes to
cheese for example you want to retain
the memory of pasta
and same applies to the word end okay
and that is done by update gate. So what
update gate will do is
exactly same as reset gate. But it will
have
different type of weights like if you
look at this mathematical equation
this w z like previously it was you see
w r
so the weights are different but
mathematical equation is still same.
Weighted sum of x d w z
ht minus 1 uz and you apply
sigmoid activation, so zt that you get
here
is the value of your update gate. So
so far it looks little simple that
you are taking update gate value and
reset gate value.
Now get a little bit complex you will
use this value
ztrt and then ht minus 1 xt
to produce your new hidden state as well
as your output.
So this is how the final GRU looks like
here this multiplication operation that
you're seeing is hadamard
product so it's a product between two
matrices and if you look at the
wikipedia article on
on you know you can search for hadamard
product
you will see you have two matrices and
it's just taking element by element and
just
you know multiplying them a1 b11c
a1 b1 and then
a 1 2 b 1 2 you see
a 3 3 b 3 so element wise element
product and if you look at the
mathematical equation they look like
this. So
I know it's little complex but maybe you
can consume this
information you can look at this
mathematical equation
and you kind of get an idea on what this
is.
But overall the crux of this whole GRU
unit is
it combines short-term memory, long-term
memory into
one state and it is a little bit
lightweight than LSTM.
So let's see the differences between the
two: so LSTM has three gates,
GRU has only two. LSTM is more accurate
on longer sequences
it takes little more time or a little
less efficient.
There are cases where they both perform
equally well
but GRU is overall more efficient
computation wise and it's getting more
popular it was invented in 2014,
and LSTM is little old was invented
between 95 and 97
and GRU is gaining more popularity. So I
hope this video
gives you some understanding of GRU.
This was all about theory, in the future
I will try to cover
coding as well. Uh if you like this video
please give it a thumbs up and
share it with your friends who want to
learn about GRU.
Thank you!