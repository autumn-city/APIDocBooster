so welcome and ah i take over at this point
from where i left in the had left in the earlier
one so in the earlier lecture you had seen
about how to train down ah a sparse auto encoder
and a simple network which had just we were
taking down the mnist ah digits input over
there and you had twenty eight cross twenty
eight or seven hundred eighty four neurons
which connected down to just four hundred
hidden layers and then from there for the
auto encoder you again reconstructed back
to seven eighty four and then for classification
you just went down to ten classes right good
so ah two lectures earlier ah what we had
done on lecture eighteen was actually to get
you introduced to ah two different dual concept
one was parse another was denoising and then
we said that these are independent of each
other while in the sparsity you actually learn
to have over complete representation in denoising
you become ah more robust to removing out
noise so one one good aspect was that now
you can have a neural network which can cleanse
an image you can remove out noise from the
image and we did also realize from some of
the other papers that ah while you are introducing
more and more noise your network tends to
learn better and better features over them
and and in fact the number of ah unique features
it learns in an over compute representation
and the better number of features it lasts
is much higher when going down with a denoising
auto encoder
so here we take the standard example building
up on top of what we had finished off with
a sparse encoder in order to get into creating
something called as a denoising sparse in
auto encoder for mnist classification so ah
they are all standard ah stand alone runs
so that you don't get confused between any
of them and i am not reusing any of my ah
i am not going to ask you to reuse any of
the codes everything is a standard release
so ah my header gets loaded all the functions
which i need to call then i load down my ah
data set over here my training and testing
and then ah my gpu so by now a lot of you
guys might get bored around because i keep
on repeating the same thing and then ah yeah
so since you are watching a new video every
day so let's let's keep on repeating the same
thing so here comes down my first part which
was about the l one penalty loss and the whole
aspect was that ah i needed to have this extra
weight ah extra amount of loss incorporated
in my whole loss and ah since my l one penalty
is not something which is predominantly defined
ah ah in in py torch based on whatever releases
we are recording with so it's not yet defined
maybe in future releases or if you are viewing
these videos beyond the release of this mooc
l one penalty might come down as a standard
release and you can use that as well
now till then ah what we do is ah we have
two parts which we need to define one is the
forward pass over ah the network and other
is the backward pass over the network and
these two methods have been explained in the
earlier class and i am i am just going to
reuse them and and then not go on to any further
now here comes down my auto encoder definition
over here so what i do is ah my network is
defined as seven eighty four neurons to four
hundred neurons and then four hundred neurons
one to seven eighty four neurons ok
forward pass of the data is defined as a forward
pass over the encoder then an ah l one penalty
with a row factor or the penalty factor or
or target sparsity factor actually target
sparsity factor set down to ten percent or
point one ok and then you have your decoder
coming down and then it returns down the output
over there and then i print down my network
i just do a copy of all the weights available
over there and ah this is what my final network
looks like
and since the l one penalty over there is
just a functional implementation it's not
a pretty layer which comes down on the architecture
that does not have any any impact in terms
of how this architecture looks like and how
its defined so now i will have to start by
training my auto encoder now here comes an
interesting aspect one is ah we shifted over
from the vanilla gradient descent on to an
adam optimizer so i i did have used it in
the earlier case as well ah but i said that
let's just hold on to a later on ah lecture
where i speak more details about optimizers
and there is where i will be explaining you
what adam is but today the reason we took
it out because ah other than using an adam
optimizer it becomes really tough ah with
just a simple gradient decent in order to
show you the efficacy of denoising and in
fact there is a good amount of relationship
between why a batch learning with a denoising
criteria works out really good with an adam
optimizer ah as as compared to a gradient
descent optimizer
so we come down to them in the later on lectures
when i would be going more details into the
theory and and linking it forward over there
now on the start part over there you see that
ah i am going to iterate it over twenty iteration
so let's just keep this one running and and
i can do the rest of the speaking and that
time so ah if you guys are also watching this
and at the same time executing the code and
we just keep on set the whole thing to run
and ah by the time i finish it off you can
actually see the results on your side of pc
as well ah my learning rates over here i changed
down to ten power of minus three and that
is quite consistent with the kind of an optimizer
which i am using over here
now what i need to do in my denoising is that
you remember that for within an epoch whenever
i am taking a sample i add some noise on to
that sample and that is what is given to the
network as an input the version which is coming
out of the network is compared down with the
unknown or the original version of the sample
together and then i get down my mse loss so
i need to define the property of noise and
what i choose to do is just add standard white
gaussian noise which is a drawn from a distribution
from a gaussian distribution which has a mean
of point one and standard deviation of point
two ok
you have your standard relationship between
your noise power to the mean and standard
deviation of my so you can use any of your
your preferable whatever you want to use your
mean and standard deviation and still keep
on doing except for keep one thing in mind
that the moment you start changing these noise
powers over larger ranges and there are drifts
and since your original images in a dynamic
range of a zero to one so try to keep down
your noise also in the similar dynamic range
and don't just ah scatter out and come down
to a much higher change your value and that's
why you do not keep a zero mean but we have
kept a non zero mean a higher value over there
of point one ok
so now within each epoch as it keeps on ranging
so what i do is ah i will have to add the
some noise coming down over here so what we
do is we create these noisy very noisy ah
and data over there and ah again ah keeping
in the same tandem as we were now if you see
over here what i am doing is basically i am
drawing down from a normal distribution with
a particular mean and a variance i convert
that onto a double data type or double precision
number system point over there and then ah
what i do is i have that one ah made into
the same size as that of my input and that's
also converted onto my standard container
called as a variable so that that's what we
were doing down in any of our examples which
we have been doing with pi torch to make it
into a variable otherwise it does not workout
it is just a static pointer
ok now ah what i do is that my inputs are
modified as my ah outputs plus my noise ok
so output is ah or what i call as ideal output
is the unnoisy version of my input ok and
now what i choose to do is i would add down
the noise and make that as my input over there
so this is how my ah x plus that noise eta
this is getting generated over here as my
input
now what i do is i have my optimizers gradients
all zero down so that i don't have any issues
coming down and then i take my outputs coming
down from my network now you would see that
what i do is on my loss function which is
my l two norm or mse loss function my ah difference
is being taken from my outputs which are produced
by the network and the ideal outputs which
are expected so this ideal output was the
un noisy version so you had this ideal output
which was actually the input variable which
was given down over there whereas your input
is the noise added version over this ideal
output so that is x plus noise which is over
there
and then ah with this coming down over here
i have my ah derivative over the loss computed
and then i set my optimizer running down so
the whole update rules and everything is what
happens within my within one unit step operation
of my optimizer and then i just look into
my loss functions so this is typically how
you see the loss going down and typically
you see that there has been a significant
drop in my ah mean square error as compared
to when i was using just a gradient descent
in fact i have even not even able to reach
down the third the decimal point over there
in terms of mse when we are just using a gradient
descent whereas here
so one is you have an adam optimizer which
is running down the next part is you also
have a denoising attribute given done which
is making the whole system learn down much
robust features as compared to what it was
learning when this noise was not there and
and that's one of the findings which we had
read in the paper as well
so now what i would like to do is ah just
look into the performance of my auto encoder
so say that this is where the kind of a noisy
input which was given down this is what it
gets reconstructed over here so sorry and
yeah yeah so i have this output being created
for for this kind of an input which is given
done and you can see it pretty much alot of
these noise is gone down so you can one example
is now you know how to create a new electric
in order to remove noise from your images
provided and then you can change provided
you have like really unnoisy versions of your
ideal inputs available to you and now let's
look into the visualizations for this one
so it's the same way that we had the weights
copied down before the training and then after
the training so this were sort of your initial
weights which connect down seven eighty four
neurons onto your two hundred onto your four
hundred neurons the same logic that there
are seven eighty four or twenty eight cross
twenty eight ah elements which make up this
weight matrix and you have twenty cross twenty
such weight matrices or four hundred and now
here is what you start looking into this train
weights
now clearly look i mean these weights which
come down at the center these kind of some
patterns as if wave lets some jagged kind
of wave let patterns they are much more meaningful
ah there are distinct features which you see
over here on these kernels as come to what
you were seeing down with a ah simple ah one
in the earlier case with just using sparsity
or even without using sparsity
so there you could see that there were changes
when we were looking into the change matrix
over here which was this ah the weight update
which comes down but when you are looking
at trying to find out if there is something
significant to be understood from the weight
matrix you don't see though a lot of times
it ah these weight matrices might not make
any sense to be physically interpreted but
this is you need to keep something in mind
that operating with these weight matrices
is what tries to boost up the features and
give you all the features so it's not necessary
that these kernels would have some physical
significance meaning
so it's the same way as trying to look into
a sobel kernel i mean that might not necessarily
give you any meaning coming out of it but
the moment you start convolving a sobel kernel
with an image and then you see you will either
accentuate your gradients along the horizontal
or the vertical direction based on what kind
of a kernel you are using so here also it's
the same thing based on this if you are doing
a correlation operation and then your non
linear
so the whole thing which happens in the first
layer you would start to get down some meaningful
outputs from there and then ah this brings
us to the point that it was easier to learn
down ah more significant and prominent bits
ah with noise incorporated now also remember
one thing that this is still a ah sparsity
incorporated auto encoder so we also need
to look down into the decoder side of it and
really try to find out if i am able to get
down the sparse matrix coming down
so this is what my initial bits are there
one which connects down my four hundred neurons
to seven eighty four neurons now after my
training you see a majority of these actually
turn up as zero bits and whatever you see
over here is non zero bits these also have
a significant amount of ah zeros over there
so there are just few dotted points which
are these connections ah of high negative
or high positive value and rest everything
is your value and this is ah exactly what
you were trying to look at any point of time
so you can save this image zoom into it and
really look into it or or just play around
with the code you can extract each simple
patch over there and look into it and a greater
detail
now this is what i was ah inferring about
so once you have your sparsity incorporated
over there and if you end up learning a much
better over complete representation in your
earlier layer connecting down seven eighty
four neurons to four hundred neurons in the
subsequent next layer where you have an l
one sparsity in position given down you would
be having a matrix which has a majority of
values which are zeros and that's what comes
down over here as you learn down
so if you look into your weight updates they
will also be open down a similar kind of a
form because you had to bring down those very
high values on to zero values also so the
updates are of the similar range now comes
the next part of it which is just to modifying
your auto encoder in order to do a classification
now here what we do is we ah chunk off ah
the last layer just delete this last line
over there the last pointer which was four
hundred neurons to seven eighty four neurons
and then connect down four hundred neurons
to ten neurons and then i have a log soft
max for my classification output over there
ah if i have a gpu it gets converted to a
gpu and then ah this is my architecture
so what i essentially did was no no further
changes anywhere over there but ah keep one
thing in mind that while i am trading down
a classifier i no more would need to make
use of the noisy data in any way right so
and then the noise is of no use and in fact
what i choose to do over here is i even do
away with my optimizer of an adam the adam
optimizer used and i can use a simple gradient
descent the vanilla gradient descent in order
to solve it out because by now i have my weights
which i learned on which are really good in
shape ok and ah this is a newer network whose
forward pass is defined in a very different
way you don't even have the l on sparsity
coming down in this particular kind of a network
as well you had all everything incorporated
over there
so now let's ah run the ah classifier so i
see this this is ah what comes down as my
final network of the classifier now if i run
train start training of the classifier which
i needed to do actually so let's wait for
sometime yeah so you start somewhere at a
training accuracy ah starting accuracy of
eighty seven percent now remember in the earlier
case we trained it down for ten epochs and
over ten epochs you went down to an accuracy
of something ninety one point seven percent
or something over twenty epochs you just barely
cross down ninety two percent whereas here
when we start with this one you start with
the training accuracy of eighty seven percent
and this is also quite concurrent ah that
while we were just training down the auto
encoder for denoising purposes
so let's get down over here so your your starting
error mse error was also quite low ok as as
well as your mse error at the end was was
really low so as network which has been able
to really reconstruct itself ah whatever data
is given down in a really good way that is
a network which has learned to actually learn
down features in a real good way and that's
what we see when we come down to this classification
so the starting point of my classification
is what starts with an accuracy of eighty
seven point one percent and then we go down
and then quite within the tenth epoch we come
down to an accuracy of ninety two percent
now one thing you might ask is my saturation
accuracy does not change in any way and and
that's like really something to do around
with the whole aspect of statistical machine
learning because your saturation accuracy
is basically the upper bound of how good you
will be able to classify and you cannot just
break that it's a function of the data which
you have it's a function of the amount of
training [sam/samples] samples you have it's
a function of the kind of a architecture or
the way you are actually trying to model down
a discriminator it's a function which is also
dependent on what is the kind of a loss function
you are using it's a cumulative effect of
all of them which come down and what we essentially
have played down is just one single aspect
so i have not increased my training data although
i was putting down noise but that's just small
jitters around the training data there has
been a significant increase in my training
data as such i have not changed my architecture
i have not changed the way i am evaluating
a network or helping it learn which is my
cost functions
the only thing which has been changed are
slight modifications to the my cost function
or or the data which comes down now that ah
given having said that the only point where
it has a significant impact is the starting
point and then that it it it has the lead
in the whole race to finishing it off the
final point of the race is still the same
you can keep this running for a longer duration
of time change out your learning rates you
can still see
but we have our experiences so there is something
which i will come down in a later lecture
which is with actually trying to look into
your learning rules and there we will be introducing
something called as a weight decay factor
so your learning rates over then we will start
on decaying then they will they will change
after a fixed number of epochs and that will
actually help you come down very close to
the saddle so you remember your saddling aspect
and in the cost function plane so it comes
down to the global minima but then since the
gradient is really high on account of this
learning rate eta so it keeps on oscillating
around over there
now if i keep on changing this oscillation
factor i keep on reducing my learning rate
so it would gradually oscillate and come down
to the global minimum point and that's what
we will be doing but that comes down slightly
in a later on lecture so let us look into
what happened with the performance in a per
class basis ok
so the last number five yes nothing increased
significantly but we do see that class number
zero and one they come down to a point of
ninety seven percent which was really high
so in the earlier case we did not see that
high result going down to ninety seven percent
we had something close to around in in case
of class zero it was about ninety seven percent
but then in class one it was still restricted
to about ninety to one ninety three percent
so these are changes which come down by introducing
that extra noisy factor which helped it learn
down
so that brings us to a point that noise at
certain aspects is also good it's not necessary
that you need to always remove out noise and
in fact for a lot of these applications as
was in the paper and then what we had studied
in the earlier lectures and today's example
you do see that adding noise actually create
so much robust system and a much better way
of learning down these weights and representations
so that's all what we have for lecture twenty
and this whole family of water encoders which
we had done so that would bring us to a concrete
conclusion of this first four weeks and the
first month of the class in the next month
onwards we are going to start with the next
version which is called as convolutional ah
neural networks and then that's the next generation
of neural networks which come down which don't
necessarily have a connection which is called
as a fully connected but we are going to play
around with how neurons are connected to the
inputs in a very different way so till then
stay tuned and wait and watch for the exciting
ones in the coming weeks as well