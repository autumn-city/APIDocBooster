>> You're not going to want
to miss this episode of
The AI Show where we dive into what
exactly is PyTorch both technical
and community with our
good friend Dmytro.
Make sure you tune in.
[MUSIC]
Hello and welcome to
this special edition of
the AI Show where I've
got a special guest,
Dmytro Dzhulgakov who is going to
talk a little bit about PyTorch.
Why don't you introduce
yourself, my friend?
>> Hi, Seth. Thanks a lot for
having me. My name is Dmytro.
I'm one of the core developers of
PyTorch deep learning framework.
That's basically what we're
going to talk about today.
A little bit about myself.
I work at Facebook,
I have been in the space
of Machine Learning infrastructure
for the last maybe six,
seven years, and
particularly deployment
frameworks and PyTorch specifically
for the past three, four years.
I'm currently working on a
lot of core abstractions
in the PyTorch development framework,
making sure that all
the new features in
which community or
[inaudible] fit together.
It's a nice and coherent
product experience.
>> Well, I'm super
excited because I started
using PyTorch maybe
nine, 10 months ago
and I was super fascinated
with how it was built.
From your perspective as a
core contributor to PyTorch,
what makes PyTorch different?
>> The way I like to formulate
the fusion is that PyTorch
tries to really embrace
the principles that Machine
Learning is programming.
It doesn't try to take control
from you and business
huge boxes system.
Instead, it tries to be
just a set of libraries.
By that, it host
programming language.
Well, as name suggest, it's Python.
Because of this approach,
it's really easy to get started.
It's really easy to use
parts of PyTorch with unions
but maybe not use some
libraries which we don't need.
So far, it's very easy to
debug and just connect
with whatever machine or
industry we are going,
so pretty much any other part of
your workflow or data science,
or even a general
programming ecosystem.
That's because Python
ecosystem is really rich and
powerful and probably has a library
for anything you can imagine.
>> Because some people think that
PyTorch is just for deep
learning, is that right?
How would you describe
PyTorch just in general?
>> I will say core focus
of PyTorch is to be
like developer environment,
like [inaudible] developer
platform primarily
for deep learning but
also a little bit
broader for numerical computation.
You can think at its core,
it's basically like NumPy with
different devices support.
First of all, GPS and
automatic differentiation
and all that rest is
basic libraries on top.
Because of that, obviously
it's very popular in
deep learning community
and related applications.
But we actually have users
also from traditional,
scientific configuration
community in physics,
biology and stuff like that.
There are folks like it's certain,
for example, is in PyTorching stuff.
So pretty much you
can think of it users
who would use NumPy and
numerical computing
in Python has pretty
much the same audience.
>> It's pretty cool because
here's the thing that
I started to discover
as I use PyTorch.
Just the way that it
stores computation
and allows you to do
things for example,
automatic differentiation,
makes it a general purpose
library for doing complex
mathematical things.
Would you say that that is
the case or am I stretching?
>> Yeah, I think
you're actually right.
At its core, it's basically
linear algebra tensor.
A tensor library which allows you to
run those operations efficiently
across a number of devices.
The CPUs, GPUs, or
maybe even more obscure
mobile embedded devices
and stuff like that.
You can think of everything else
basically being layers on top of it.
I have tensor computations,
I want to compute gradients,
so that's auto gradient layer on top.
Now I want to build
my neural network.
So I probably want to have a
standard library of layers
that's Torching in and all the
libraries related to that.
I want to run the optimization loop,
read my data, deploy the models,
and like all the further
questions in the usual workflow
of Machine Learning deep
learning practitioner.
Basically, whatever the step is,
there is usually an
independent sub library
within PyTorch package which
allows you to do that.
But what's cool is basically
those are structured as
like libraries in an SDK.
They work well with each other but
they are modular and pluggable.
If you want to have
your own data reader or
you don't need something
which is PyTorch related,
you don't have to use it
and you don't have to
pay abstraction price for that.
You can bring your own or build
your own project on top,
doing something with it.
>> This is cool. At the center then
is this tensor computational graph.
Then you wrap layers around
it is what you're saying.
Let's do this if you don't mind.
Can we take a look at it
as a Machine Learning
computational framework type thing
and start with the whole onion?
Let's start with the tensors
and let's talk about the data
around it and how you
actually move data in there.
Then let's move to the models,
the optimization and the deployment
because you mentioned
each of those things.
Let's start with the
data at its foundation,
tell me how PyTorch deals with data.
>> Yes. I mean, Machine
Learning is usually
be inside 90 percent
of data preparation
and 10 percent maybe of
actual deep learning.
Obviously it's very important.
Again, as I said,
PyTorch tries to embrace
Python ecosystem to its maximum.
If you basically get
started, you can think of
how would you write your
workflow in Python.
You'll probably have some
iterator over your data
which may be reading from
local CSV file or from
local folder with images.
Or maybe more production
seconds like streaming from
some bucket and storage in the Cloud
or some database or whatever.
Basically you wrap it in the
form of tensors and you do
your neural Networkish computation
or computation on those.
Basically, PyTorch the
core package comes with
big and simple abstractions
to make it easy for
you to get started and
handle typical tasks.
That's what's called
Torch Data Loader.
If you have typical data sets,
set of images somewhere,
something like this,
you can probably just
grab existing class
and be able to read it efficiently in
a multithreaded fashion
and have a library of
preprocessing tool so you can
delete data augmentation,
and some professional
re-scale images like
cropping the text preprocessing
depending on your domain,
and basically produce an
iterator of tensors which is
your mini batches of data going into
training and pretty much
take it from there.
PyTorch is not like opinionated
on how exactly it's structured.
There is libraries within
the Torch Data Loader within
PyTorch's shelves
which makes it easier
to get started and use
it for common cases.
But also if you're operating in
some very specialized workflow
like your data is in
some proprietary,
crazy database from which
you want the stream.
Again, because it's all just Python
and a general programming ecosystem,
it's pretty easy to link.
We have extension mechanism,
you can just basically build your
function in whatever language.
One thread like return data
in a multidimensional array,
wrap it in PyTorch tensor and
pretty much that will work.
It mirrors the philosophy of PyTorch,
basically to give
you as a user and as
a developer the right tools to do
your workflow but not be extremely
opinionated that you have
to do it exactly one way.
Because workflows
differ and especially
in research or bigger
production settings,
there's a lot of variability
between these cases so
the customizability and
modularity are important.
>> Look the first time I
learned how to do this,
obviously when you're doing
machine learning type of stuff,
your goal is to get any piece of data
into some multidimensional array,
I guess a Tensor in this case.
But then the first abstraction
that I looked at was the data set.
I was like, "Okay, so now I
need to inherit from data set."
I went to look at the base data
set class and it was basically like,
"Well, you just need to
tell us how many are
there and how to get
one from your set?"
>> Exactly.
>> I was like, "Whoa,
this is literally nothing."
I can literally do whatever I want,
which was really cool to me.
Then once I had that
base abstraction,
I could literally just
piggyback onto a data loader
to get me the batch size that I want.
>> Yeah.
>> I could put
transformers in there to
transform the data as
it came out if I want,
or I could have put
that in the data set.
To me the whole data to Tensor
to data set to data loader
was super freeing in that
the structure was there if I wanted
it but I didn't necessarily need it.
>> Exactly.
>> That's what important for me.
>> Yeah. To extend your example,
you have image folder dataset which
would implement what are there.
Whatever file size in the directory,
how to get one, just read that file.
That works for small and
medium size use cases.
If it's probably something
like super large scale,
there are libraries out there
connecting PyTorch data loader,
dataset implementations
to Cloud providers,
like Azure or AWS or whatever,
to just stream data from some
storage bucket somewhere.
That's also like you have to pay
for what you've actually used.
If something doesn't
suit here, as you said,
you can always inherit
from that class,
or inherit from different
extension point and basically
implement whatever logic you
might need and optimize
it for your settings.
>> Let's move over
now. Now that we've
talked about data, because again,
to me I just had this notion
of tensor computational graph,
the data set, the data loader.
Let's talk about
building actual models.
Is it the same philosophy in PyTorch?
The same approach to
build your own models?
>> Yes. The general idea is the same
as decomposed like necessary
building blocks to
their minimal useful components
and give you some control
to each of those they
want to use or not.
For example, if you are following
a tutorial how to build
a neural network,
you usually learn about how to
do linear algebra operations.
Then you learn about
gradients and how to do
automatic differentiation with
usually reverse mode
Autograd and chain rule.
Then you learned about
different layers
and how to construct
actual training loop,
which reads data in
like pass forward,
pass backward, pass
and calls optimizer.
This is exactly a structure which
abstractions in PyTorch follow.
you have general computations
which we talked about.
You have confined called Autograd,
which is basically
actually similar to
original Autograd projects
coming from 1980s.
The idea is you can flip a flag on
a tensor saying I'm interested in
computing gradients
for this one and now,
like whatever operation you're
going to do with these tensors.
But you should be remembering
which steps were taken,
software that you
can take some result
which is usually lost
in different cases,
say the backwards, and
automatically get gradients
for the original tensors,
which in this case usually will
be weights of the neural network.
Again, Autograd is
completely pluggable.
It doesn't try to enforce what
the structure of your program is.
Like your program might have been
arbitrary Python control flow,
it can have recursive
function in the middle.
It can have a bunch of
lambdas and whatever.
It doesn't really matter
like you can use whatever.
Python has available Autograd,
just like basically trace
whatever tensor operations
were happening in your program
and let you to compute gradients.
That makes it very pluggable
and flexible, especially
for research.
If you look at usually
for vision models,
it's usually pretty simple.
Everybody likes to
look at [inaudible].
But if you go in especially
crazier NLP research,
people do fun stuff like
tree-structured to STM, whatever.
The way your model looks like,
it's literally recursive
function trying
to parse the sentence
and stuff like this.
Because Autograd doesn't try
to import the structure
of your program,
it just records operations,
this tensor that
happened, that works.
You can still differentiate tensors
flowing through your program.
We talked about Autograd.
Usually next layer is grabbing
actual standard neural
network layers,
some convolutional layer
or different activation functions.
PyTorch, from the beginning comes to
this standard library of
neural networks layers,
torch and N. It's directly
in the core package,
and that's pretty
much all you need to
construct your regular neural network
and what those layers really combine.
They basically tie in state.
You can think of neural network layer
presented as a module in PyTorch,
it's basically as an
object through it,
and its weight is a state
which you're going to train.
It has a forward function
which when I'm going
to call this module,
basically apply this layer to some
inputs to produce some output.
Again, it's pretty
much embracing like
traditional software
engineering wisdom
of object-oriented programming.
So very powerful abstraction
for constructing your networks.
If you're trying to do something
like weight sharing where you have
the same module applied into
different places in your network,
it's very convenient to express
for programming
experience perspective
because you just create an
object and you call it twice.
Just how you would do it in
regular Python or C++ program.
With neural network library,
you can basically construct your
typical neural network structure.
Again, because it's just Python,
you can have like
arbitrary control flow,
you can have loops and
branching and whatever
you want in the middle.
You can still compute the
derivatives of parameters
by turning on Autograd,
like passing your inputs
through the network.
That's pretty much what you
need for forward and backward
to pass a typical neural
networks training loop.
That pretty much gives you
necessary components to construct
the entire training loop and
train your first neural network.
You would take data loader,
which you talked about,
which grabs data from somewhere
and packages to the tensors.
You would construct your model,
their hierarchy of modules.
You would call it on the inputs.
It will produce your loss,
you would call Autograd backwards
computing their
derivatives of parameters.
You would call optimizer layers
from the standard libraries.
So you want to play SGD,
Adam, or Autograd or whatever,
your favorite optimization method
is and pretty much do
all of that in the loop.
Keep reading your data and updating
parameters and that would be
your classical training
loop for a neural network.
But again, because it's
like very explicit
and following the
programming abstractions,
it's very easy to recreate.
Now if you are trying to
do against, for example,
generative adversarial
neural networks where
training loop looks like two
separate parts of the loop,
it's very easy to understand
where to plug it in.
Now you have two modules,
just call one or another.
If you have something
more complicated
like lists of tree structures, again,
it's very easy to express it in
this program and really this
simplicity and decompose it
into individual components
which work well together.
Gives both good user experience,
but also a good flexibility
for more advanced use cases.
I guess it's one of the
reasons why PyTorch was
even from the initial versions
very popular and these
days very popular especially
in research environment
where people need a lot
of this flexibility,
how to tune, how to trick their
model structure in their training,
their regime and stuff like that.
>> This is cool because basically
I asked you about models,
but you actually started
with computational graphs,
which when you think about it,
it's like, yeah, that's
basically what a model is.
It's a forward computation
that you pass forward,
but when you're trying
to optimize it,
you're just basically going
backwards and that's
all just built in.
You mentioned something about,
and I think it's NN.modules.
Is that the right place
where these things are?
>> Yeah. The actual library with
standard layers is like torch.nn.
So you will have a torch.nn linear,
or torch.nn comms2-D or
something like this.
Also like you mentioned
about conditional graphs.
Again, PyTorch, if you
wanted to do backward pass,
you actually record like the
graphic table stuff which happens.
But in forward you actually don't
have to construct the graph like so.
Which is just a Python program.
You can call arbitrary Python
function in the middle
if you wanted to,
which basically means
your model structure
is not constrained to what
a graph should be like.
You don't need to learn your fancy
constructs of PyTorch itself.
If you want to write an IF statement,
you basically write Python
IF statement. That's it.
>> That's really cool.
Because like I said,
when you're trying to
optimize these things,
which is what you weigh in to,
the cool bit about this is in
other frameworks you have to be very
cognizant of what's happening.
But in this framework,
because of the way the
things flow forward,
if you do an IF statement,
it just doesn't go through
that part of the graph and it
doesn't compute the gradients
if you don't need them.
If you don't need the
gradients at all,
you just say don't
compute the gradients,
which to me it was
really cool as well.
>> Then the other thing that was
interesting is everyone knows about
AECOM 2D rider or linear layer.
You can actually make your own layers
however you like by just sub-classing
the right thing and then putting it
in the other structures, correct?
>> Yeah, exactly.
PyTorch and library,
is probably the most common
and the most standard layers,
which pretty much everybody doing
deploying usually agrees upon.
But if you look at more
domain-specific libraries,
either part of PyTorch,
media tele-systems such as
Torchvision or Torchtext,
Torchaudio for respectfully
like different domains.
They will have more
specialized modules.
For example vision,
the detection style of
red layers like
non-medium suppression.
If you're trying to do detection,
classical stuff like that,
if you look at some projects
doing 3D detection,
they would have their
own layers implemented
and imagery of cases
like very easy to
understand how it's
done because it just
inherit from a Python class
or maybe right there.
Call somebody and ultimately, again,
if you want to extend that Autograd,
you just inherit from
Autograd function specify,
what is my forward part,
what is my backward part.
That kind of places, that
rest of the program.
>> As I said, to me it
was really cool because
now I could literally
make any structure
that I wanted and optimize it.
That's why it doesn't
have to be reserved
or just deep learning.
It could be any
mathematical optimization
[inaudible] in theory.
You could totally do this.
The optimization loop to me was
super cool because basically,
you got the batch,
you put it in, you measure the loss,
you take the gradients,
and then you update the weights,
and then you keep doing
that until you're done.
Which is basically the
whole optimization problem.
>> Yeah. I mean,
if you want to take it further,
another example of molarity,
the support, for example,
higher-order gradients, when
you're doing your backwards,
you can turn on recording
gradients on the [inaudible].
You will basically compute
gradients of gradients.
It's second partial derivative,
which is how you compute the Hessian,
the whole Jacobian matrix,
people doing research on
second-order optimization methods
or we can make many
problems perform better
than first-order
optimization method on.
There's very natural
way of doing that.
There are examples of how
modularity bring stoke
natural user experience.
>> That's really cool
and I'll tell you,
it does my heart good and it
pains me because when
I was in grad school,
I actually had to
calculate the Hessian for
the problem I was doing
and I couldn't do it.
I had to do some numerical
approximation thing
and I wrote it in five different
languages or whatever.
It was just not very cool.
I didn't know that you could actually
do higher-order derivatives in there,
which is super good,
especially if you're trying
to do different kinds of
optimizations that actually tells you
which direction to go
on, which is cool.
>> Yeah, exactly. Also, you can
do it directly this pieces a
little bit and actually edit
higher level Autograd APIs.
More optimized implementations
for computing locations.
Secondly, because I
mean, they're jointly or
how some of the tricks how you
can optimize computations there.
You can check out
some recent releases.
There is extra stuff being edited on
the Spark as well as one
of the many directions.
>> That's something I didn't
know about. Thank you for that.
Let's get into now.
Just a couple more
things to finish up.
It's great that we can do all
these cool mathematical things.
But when it comes to time
to actually take what we've
learned and deploy and put these
things out into production.
What does PyTorch offer?
>> Yeah, I'm actually
focused on bringing
this research to production is
a core part of the project.
In a sense because
Machine Learning field
is evolving really quickly,
and deep learning in particular.
Even from used cases,
which we see at Facebook
or other companies,
frequently you take the
model which I cannot
just got published and
people want to put it in
production application
in weeks or months later.
How electric supply chain is,
simply avoiding rewriting
the whole thing
from scratch is really important.
The developers in the
past few years focus on,
in a similar philosophy,
how to give you tools for
packaging and deploy new models in
optimizing for high-performance in
France became a core
part of the project.
Some other version is make sense of
the decompose into several pieces.
What does Deploy Machine
Learning Models mean?
They're different. It's
pretty wide field.
There are different kinds of settings
and regimes where you might deploy.
If you're trying to deploy something
on server-side in the Cloud,
then frequently even just running
your Python program that directly
taking your model class and say,
hey, this is my part
of the model we share onto
the playful inference.
This is my train checkpoint.
This, whatever we are
going to just take and
put in some server somewhere.
There is Project Management
Ecosystem called the third CRF,
which would actually
build with partners
from Amazon and Microsoft,
MIT gives you the ability to
easily package the model
and attach rest API to it.
You can just deploy
it with few commands and
start sending requests.
You can do it. By default you can do
it basically just like Python.
Implementation as a model.
In many cases, it might
be undesirable to
actually run Python directly.
I don't server-side because high load
and polarization problems of
Python, global interpreter lock.
Or especially if you're
trying to deploy for
embedded devices for like on
mobile phones for example.
Writing Python
interpreter is not good.
They actually have some component of
the PyTorch ecosystem called
Torchscript, which is,
you can think of it
as an implementation
of subset of Python language,
which is really like tailored to
the experience a writing
models in PyTorch.
Select your typical model.
You can basically train it in
full Python this PyTorch
how you would do.
Then take just the model,
say Torchscript on it
and what it will do it
basically go look at your NN model.
Go find all the pieces of code
which is necessary to run it.
Basically, package it in one file,
which we can run without the
Python as our own interpreter.
But you'll literally go and
parch the Python source code.
Like in for some
reasonable subset of it,
very dynamic stuff will not work,
but majority of stuff which people
have in their models will work.
Basically that piece of
[inaudible] deploy it now,
embedded in some SQL specification
without Python being in it.
Or it can be brought to
mobile device and whatever.
That's the packaging and
deployment part of story.
I guess second big ingredient for
efficient deployment of production is
basically how to optimize
performance of models.
If they're running, usually,
an inference on a huge scale,
you'd probably don't want to waste.
They're usually pretty strict
on those capacity requirements or
latency requirements,
stuff like that.
That's where set of techniques
utilized and been up,
but the neural networks, something
like quantization,
pruning, specification.
Maybe just like combination
like go to low precision or
actually doing some things
like techniques that
preserves the accuracy,
or maybe just doing
some local optimization
on some models so
you can replace some
models or modules,
like more efficient implementations.
PyTorch by itself, actually
gives you this set
of ingredients to optimize the
models for efficient inference.
You have combination libraries
and workloads as a
part of core PyTorch.
Basically, there are some optimized
implementations operators,
you can build your own.
That's why they also deploy into
wide range of devices who
comes in because you'll
usually train on GPUs or maybe
some Cloud accelerators.
But in terms of deployment,
there's a wide range of
devices you might deploy.
There's server-side CPUs and GPUs.
There is all kinds of server-side
accelerators these people have.
There are all the kind of Zoof,
different mobile devices and
DSPs and stuff like that.
Our approach there is that,
for the most popular platforms,
we try to provide implementations to
all these tensor operations
on them in the core PyTorch,
like how you do the CPU, GPU.
We're now working on bigger
coverage for mobile CPU like
ARM CPUs and something
like mobile GPUs,
like open jailbreak-ins.
But then there's
this long tail of different devices.
It's also important to think how
you can take your model
deployed to that.
Again, depending on how
restricted your devices
computationally, it might not
be very automatic process,
but it's still important to give you,
as a developer, tools
to make it smoother,
so there is a number of
technologies in better
to support that tool, ranging from,
for example, ONNX export,
so you can export it
to the format which
a lot of these runtimes
can import for inference.
There is also a direct
integration points in
PyTorch for integrating
different backends.
If you want to run optimization
on some part of your module,
delegate to some runtime
something like NVIDIA
[inaudible] or like
ONNX from Microsoft,
you can even basically can,
by taking part of your model
and exporting it to that form.
>> That's a cool bit. It seems
you-all have thought a lot
about the framework itself,
but you've also thought a lot
about how do we get these
things out in an efficient way.
You can just do it or
you can optimize it with
Torch script and embed it
into C++ style application.
I'm a fan of ONNX, the ONNX runtime,
as well as the ONNX format for
getting multiple folks
to be able to use it.
So just to finish up,
any last things that
make PyTorch special
and then where can people
go to find out more?
>> Yeah. We talked a lot about
technical merits of PyTorch,
especially on the core framework.
Probably, the most
important reason why
PyTorch is a developer platform,
is so popular, it's friendly,
it's actually not technical but
more on the community side.
From basic had an amazing
community and we can relate,
spend effort to nurture it and
support different members.
Core PyTorch contributors
basically from Day 1
includes multiple industry companies
like university labs and just
like individual
open-source developers.
Those contributions range from
contributing to the core framework to
just doing awesome work
in supporting and answering
questions in PyTorch forums,
which a lot of people say
that is one of the main
reasons why they love PyTorch.
It's actually very responsive forums,
and obviously, this huge ecosystem
of projects built on top of PyTorch.
If you pretty much look at any
subfield of deep learning,
just go search that keyword, PyTorch.
Google it to find a
bunch of GitHub repos,
which are all awesome projects,
work we spent time
building up on PyTorch,
but implement special layers of
functionalities, algorithms,
and models for that
particular domain.
Being it something more standard
like computer vision or
maybe more obscure applications
like biology and stuff like that.
That ranges both on type
of domains and type
of emergent research areas being it
like privacy preserving
Machine Learning
or being it like different
optimization methods
or being applications
in different domains.
That's really the most
amazing thing to see how
empowering community allow
us to basically create
this ecosystem, where you can find
many different projects and
many different tools for
pretty much any job.
A lot of ideas also start in
this independent projects
and become also like share
the cross manual processors
like very intelligent people to
contribute those ideas and models,
whatever back to PyTorch core so they
can benefit even broader
set of use cases.
That's very organic
and natural process.
Yeah, I guess that probably
would be the main,
not just technical factor
of PyTorch success.
You asked where to learn
more about PyTorch,
basically go to pytorch.org.
There are tutorials, there are
documentation, there is, basically,
a set of pointers to
ecosystem projects,
then instructions on how to
download and start play and visit,
or finding the project which
made you fit in domain
you are working in.
If you're new to deep learning,
actually, one of your
previous speakers,
Jeremy Howard, was
like, "How's the TA?"
Basically, Jeremy and
other folks build
this amazing library as an
intro to deep learning as well.
So actually, it was PyTorch
and Jeremy is involved.
That's also an example of
ecosystem, community projects.
That's probably the stuff,
and as you start using
PyTorch you also go to
forums and also Lincoln's website.
Ask your question, and usually
people will be happy to
help you out if it gets dark or
point you in some direction.
Of course, if you want the
contribute to PyTorch itself,
all our development
happens on GitHub,
so go grab some simple issue,
maybe send DPR and [inaudible]
>> Well, I'm a huge fan, Dmytro.
Thank you so much for
spending some time with us.
Again, if you want to
learn more about PyTorch,
go to PyTorch.org,
there's some pretty
cool tutorials there,
that I'm excited about.
We've been learning
all about PyTorch,
not just from a
technical's perspective,
but from a community perspective.
Thanks so much for
being with us, Dmytro.
>> Thank you so much for
having me. It was fun.
>> Awesome. Again, thank
you so much for watching.
This has been another episode of
the AI Show where we
learned all about PyTorch.
We're going to have to get
you on again, my friend.
So watch for Dmytro, hopefully,
in later episodes to show us
some code or some cool sample.
Again, thanks for watching, and
hopefully, we'll see you
next time. Take care.
[MUSIC]