" That means, it tries to make the learning rate inversely proportional to a sane cumulated history. "
RMsprop keeps the exponentialy decaying average of squared gradients. 
RMSprop uses a momentum-like exponential decay to the gradient history.  Gradients in extreme past have less influence. It modiﬁes AdaGrad optimizer to perform better in the non-convex setting by changing the gradient accumulation into an exponentially weighted moving average.
" So, that is the problem with the Adagrad  algorithm and in case of RMSProp instead   of using this cumulative or acumulative sum of  squared gradients the RMSProp takes exponentially   decaying average of the squared gradient.  And, it  does not consider the extreme past histories while   accumulating the sum of square gradient.  And, as  a result of this the algorithm converges rapidly,   once it reaches once your vector reaches  locally convex ball short of surface that .  And,   what you can do is we can assume this point once  it reaches that locally convex error surface,   that as if your Adagrad algorithm is initialized  at that point within that locally convex ball. "
"However, RMSProp does not keep a moving average of the gradient.  But it can maintain a momentum, like MomentumOptimizer."
" So, Adagrad got stuck when it was close to convergence because the learning rate was killed and it was no longer able to move in a direction of b, but for RMSProp, it overcomes this problem by not growing the denominator very aggressively, ok. Now, can you think of any further modifications? "
" Actually it is taking a moving average of; there is the same as the momentum base role, right? "
" This is the same as what RMSProp suggested that you divide the learning rate by a cumulated history of gradients, right? "
" So, in today’s class we will  talk about two more algorithms,   one of them is RMSProp and the other  one is Adam and we will also see a   very closely related algorithm which  is very closely related to RMSProp. "
" So, the algorithm  that we will talk about is what is RMSProp   which tries to address this problem  of Adagrad algorithm that is vanishing   learning rate as the time increases  as the number of iteration proceeds.  So, what is this RMSProp algorithm does  is instead of taking the accumulative sum   of squares of the gradients of the sum of the  squares of the past and gradients or this past   gradient starts from time t equal to 0. "
" So in   case of RMSProp, the scaling factor is not  the cumulative sum of gradient histories,   but it is the exponentially decaying  average of the squared gradients.  So, if I go to the updation algorithm is in  RMSProb, the updation algorithm will be like this;   you will find that you will compute the  gradient in the same way as we have done in   case of Adagrad, right. "
" So, in  case of RMSProp we did not have any concept   of momentum. "
"We are using gradient descent to calculate the gradient and then update the weights by backpropagation.  There are plenty optimizers, like the ones you mention and many more."
The optimizers use an adaptive learning rate.  With an adaptive loss we have more DoF to increase my learning rate on y directions and decrease along the x direction. They don't stuck on one direction and they are able to traverse more on one direction against the other.
Adam with�beta1=1�is equivalent to RMSProp with�momentum=0.  The argument�beta2�of Adam and the argument�decay�of RMSProp are the same.
A detailed description of rmsprop.  maintain a moving (discounted) average of the square of gradients  divide gradient by the root of this average  can maintain a momentum
" So, your  basically the operation that was done in Adagrad   algorithm is r t, the scaling factor which is  1 upon square root of epsilon plus r t i.  So,   if you go for component wise this r t i is  nothing, but sum of g t i square of this   or let me put it as g tau square instead of  g t g tau square and you take the summation   of tau is equal to say 1 to t. So, you find  that this being a square term and which you   are going on adding.  So, r t goes on increasing,  it monotonically increases , it does not reduce. "
