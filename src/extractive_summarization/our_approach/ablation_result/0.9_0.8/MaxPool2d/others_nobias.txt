Pooling is of MUCH MORE IMPORTANCE in convnets. 
And, by doing so, achieves one of the most interesting things about convnets; robustness to displacement, rotation or distortion of the input.  Invariance, if learnt, is located even if it appears in another location or with distortions. It also implies learning through increasing scale, discovering -again, hopefully- hierarchical patterns on different scales. And of course, and also necessary in convnets, pooling makes computation possible as number of layers grows.
 Here, I am going to use, sure you have a five by five input and we're going to apply max pooling with a filter size that's three by three.  So f is equal to three and let's use a stride of one.  So in this case, the output size is going to be three by three.  And the formulas we had developed in the previous videos for figuring out the output size for conv layer, those formulas also work for max pooling.  So, that's n plus 2p minus f over s plus 1.  That formula also works for figuring out the output size of max pooling.  But in this example, let's compute each of the elements of this three by three output.  The upper left-hand elements, we're going to look over that region.  So notice this is a three by three region because the filter size is three and to the max there.  So, that will be nine, and then we shifted over by one because which you can stride at one. 
 So if you need to know more about max pooling, or you just need a refresher, same thing with padding here for zero padding, activation functions, anything like that, then be sure to check those episodes out and the corresponding deep learning fundamentals course on depot's or.com. 
