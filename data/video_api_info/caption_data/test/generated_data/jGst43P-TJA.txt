what's going on guys hope you're doing
amazing in this video I want to show you
how to implement a bi-directional lsdm
in pi torch so what we're gonna do is I
have some code set up here which it's
just some very very simple code some
training loop check accuracy and the
loading demonstate I said this is from a
previous video if you want to check this
out and what we want to do is we want to
create a bi-directional elysium and what
we're gonna do is we're gonna do class
let's call it bi-directional BRM and
we're gonna inherit an end module and
we're gonna do define in it and yeah let
me just mention here that so what we
have is we have the amnesty the set and
Emily's data set we can view as having
28 sequences which each has 28 features
so the image is a 28 by 28 grid so we
can kind of unroll it row by row and we
can feed that to the RNN sequentially so
that each we have to Nate time steps at
each time step just takes a single row
of the of the image and then we just
have some number of layers that we're
gonna use we have the hidden size of the
RNN just some hyper parameters and
number of classes learning rate etc but
we're gonna send in to the
bi-directional Ren it's just input size
the hidden size the number of layers and
the number of classes we're going to do
super br9
self and then dot in it
and then we're gonna do self that hidden
size is just hidden size self that num
layers
it's just naam layers now we're gonna
define our LST m and we're going to N n
dot LST n input size and then hidden
size number of layers that's just sort
of the order where we send it in and
then we're gonna do batch first equals
true for for the M meanest data set
which has batches as they as the first
access and then just to make it
bi-directional all we have to do is
bi-directional equals true so easy you
know I guess and then we're gonna do
self that fully connected is Anand that
linear and we're gonna do hidden size to
number of classes and then so we know
forward just one thing that we need to
keep in mind is that well we need to
define our hidden state and also the
cell state that we're gonna send into
lsdm and yeah so let's do that
h0 will be our hidden state torch that
zeros and we're gonna do self dot numb
layers and then we're going to times
that by two and why we do that is that
so we have seven number of layers but we
also have let's see so we have so we
have several number layers and then we
need to times it by two since we have
one of them going forward and one of
them going backward but they're all
going to get concatenated into the same
for the same specific hidden state so we
need to just expand this this tensor by
two in this axis and then we're gonna do
exit size of zero just the number of
examples we send in into the batch size
and then self dot hidden size and then
just dot two device okay and actually we
need to do that here as well so the
nonlinear will take the hidden size by
times two since for the same argument we
have one going forward one going
backward
and they're gonna get concatenated and
that's a sort of the hidden hidden state
for that particular time sequence just
some background from bio direction or
LCM I'm sort of assuming in this video
that you know the theory behind it and
this is just a sort of the
implementation of it and yeah so we can
just copy this for the cell state and
then we all need we just need to sort of
run the LST m and H 0 comma cell state
in a tuple and what would be the output
here is the is this is the hidden state
and then the cell state but we're not
really gonna use those so we can just
remove or we we don't have to look at
that output and then we're just gonna do
out is self that FC of out and we're
just gonna take the last hidden state to
send into the Tillinger layer and then
we're just gonna return out and yeah all
we need to do here is we need to
initialize the network with the br9 and
this should be ready to run so I'm gonna
run this for I guess to epochs and let's
see what kind of results we get so
training for two epochs
we get nine about 97% accuracy which is
I guess not too great but yeah we just
train for two epoch so you could I guess
expand this as well and you would
probably get a lot better accuracy but
yeah that was that's pretty much it for
the bi-directional STM if you have any
questions leave them in the comment
otherwise thank you so much for watching
your video and I hope to see you in the
next one
[Music]