hi everyone my name is Dan banger
welcome and thanks again for spending
your evening with us today my name is
Dan banger I do lead a small team of
business about managers are eight of us
and what we do is that we do focus on AI
platforms and engines essentially
everything that can enable their
scientists to be successful with their
journey with machine learning and today
specifically I have the honor and
pleasure to have my friend and former
colleague from Facebook Joe Spisak
joining me today thank you gotta be here
and you guys are in for a treat there's
a lot of content to cover so we're going
to be going a little bit faster but just
a little bit of housekeeping
one is we will be covering some some
code as part of the demo the code is
available for you to copy and and play
with today on github so don't worry
about trying to figure everything out
right now the slides as well will be
available immediately for you to take
home and then play around with so don't
worry too much about that piece and also
this is a foreign level session today
what we what we decided to do Joe and I
is to dig deeper into the Python
experience for the develop or data
scientists and try to come up with a
message that really puts aside all the
definitions and everything that you
might probably want to read on your own
and then dig really deep into the best
practices the opportunity of piperwai
piperj what is the strategy that
Facebook had in mind when they built
Piper watch as well as what we do at
Amazon Web Services that enables fight
or to essentially be a very compelling
deep learning framework on the cloud so
it's going to be a deep dive like I said
so far in the level session I have some
code on the side and I also have a lot
of slides to cover so I'll dig right
away just for my own calibration
who here is familiar with pike porridge
and who here is probably running a team
or part of a team that is using higher
torch pretty much every day all right
okay better than I hoped perfect thanks
a lot for the calibration again my name
is Dan banger today with me is Joe who
would dig into the pythor experience
from a Facebook's perspective I started
with thank you and one of the key
reasons one of the key drivers that we
all hear today and I believe that you
you aware of that as well is that we we
all into technology and we all into
trying to touch the the next level the
next stage as far as technology is
concerned what are the top ten digital
trends that are out there that are
really giving AI and machine learning an
opportunity today and so I decided to
put this to distend I mean I got them
from from Gartner but it's interesting
to look at where the world is going in
terms of technology where we're building
autonomous things we're dealing with
blockchain smart spaces digital and
digital ethics
sorry digital ethics and privacy quantum
computing and an empowered edge where
computing is coming closer and closer to
end-users as possible and if you dig
deeper into all of these new paradigms
and technology we realize that all of
them involve some form of AI machine
learning I know that AI is very visible
nowadays with all the examples that you
get you know from a marketing standpoint
but it's also this optimization game
that is a very back-end related
artificial intelligence practice that we
don't get to talk about a lot but that
actually exists right so what we see at
Amazon Web Services for the past you
know a couple of years working with tens
of thousands of active developers
building machine learning models
adopting different frameworks of machine
learning on the cloud is that there's a
250% growth year over year that we're
capturing simply because of the
opportunity to leverage cloud
infrastructures and leverage cloud skill
and leverage cloud paradigms and
leverage you know platforms like sage
maker to accelerate the process through
which developers and their scientists
deploy and then build and then train
machine learning models we also seeing 8
out of 10 typical machine learning and
deep learning workloads running on AWS
today including the benchmarks and so I
believe you familiar with most of these
customers into it it's using AWS H maker
to build fraud detection models we're
having very good use cases coming from
companies like Sony in media and
entertainment if you're using sucked up
to go to the doctor you're probably
using a tensor flow model built on top
of AWS in the backend another example of
a customer that I liked holding now the
pretty recent one is Siemens we're using
tensor flow and sage maker they were
able to bring the computational time
that they use to basically build machine
learning models from 12 hours to 30
seconds it's really incredible and in
sports analytics as well we're seeing a
lot of movement from folks like the NFL
from folks like Formula One handling
terabytes and terabytes of data piping
that over to the cloud and then using
that to provide a different experience
that the folks that are watching the
games or the folks that are watching the
races are having simply because of the
opportunity of deep learning or AI and
the combination of that and in streaming
analytics so our approach at AWS is very
simple well first of all with customer
focus 90 to 95 percent of our roadmap is
driven by our customers and then
secondly we're very aggressive at the
pace at which we do in vein right so we
aggressively listen to our customers and
then we go deeper and deeper in terms of
breadth and depth to make sure that we
talk to everything that would build is
speaking directly to our customers
demand and of course with that we what
we've observed what we've observed is
that customers are coming to us and
asking us to provide fighters for
example as a deep learning framework it
doesn't matter whether Amazon created
that are not in the case of fighters for
example we've worked with Facebook to
make sure that title which is optimized
for the cloud and running on AWS and of
course we secure that and we inject
research and development in everything
that we build if you look at the word of
AI today
typically you have developers partnering
with data scientists in order to build
real-time applications where you get a
developers or a DevOps person that is
getting essentially the build of
quantities or the requirements that a
data scientist has in order to put the
real product in production and what I
want to challenge you to think about or
what I want to get all of us who think
about today is the whole concept that
are packaged into AI driven development
and by a are driven development I mean a
combination of bringing AI development
tools together with algorithms and
end-to-end machine learning platforms
and overall machine learning models and
templates combined all together in some
form of workflow a process that is
captured by a single platform and that's
the opportunity that we have today the
opportunity that we have today is to
package all the best practices in terms
of platform automation in terms of model
and templating and packaging SDKs and
libraries and examples and everything
that developers have to use on a day to
day basis in a single platform and of
course our Odom's as well so that's what
we built sage maker and th maker is
essentially a suit of platforms and
services capabilities that enable
machine learning developers and knitter
science is to build trained tuned and
deploy machine learning models every day
who here is not familiar with sage maker
ok just a few people perfect then we
don't have to get into that so it's H
maker you can have Python alongside many
other deep learning frameworks as well
as the infrastructure that comes around
with it and yeah and some higher level
capabilities like an integration with
the deep lense
device which is a developer first camera
that makes it possible to deploy machine
learning models on the field and all of
that is available behind an API so if
you think about Amazon sage maker it's
really that environment that template
Isis and package is a capability to spin
up infrastructure that comes
pre-configured with all these machine
learning and
learning frameworks including pythons
which is a focus of a conversation today
and then builds that end-to-end machine
learning workflow behind an API and if
you pay attention tomorrow and day after
tomorrow you might hear much more
capabilities that are coming to the
company's age maker so digging deeper a
little bit into that so sage maker has
this environment that enables data
scientists to build machine learning
models so Jupiter notebooks as a service
you guys heard about that I'm not gonna
get into the details the second thing it
provides is a training environment which
is again available behind an API in a
managed platform to just spin up machine
learning trading jobs and then process
them a scale including hyper parameter
optimization and speaking of hyper
parameter optimization the difference
between 92 percent accurate model and a
95 96 percent accurate model is probably
how you tweak the exact same model and
how does a platform enable you to get
that capability available and the last
thing about the platform is the ability
to deploy machine learning models at
scale and of course all of that is
packaged with metadata capability
pay-as-you-go and compliance and
end-to-end VPC and encryption support
now the reason I take us through this
process is because the platform was
built and designed to work with deep
learning frameworks like Pi coach in a
way that is seamless in the way that
makes the life of the developer and data
scientist easier and that's what we're
gonna jump into right now so to dig into
the PI torch experience on top of a deep
learning on top of the stage make a
platform I decided to walk you through
their anatomy what I call the anatomy of
a deep learning framework on top of
Amazon tej maker if you look at the
developer's experience well it usually
starts either on the developers laptop
and by developer here I mean data
scientist developer AI researcher pretty
much anyone that has something to do
with a machine learning or deep learning
framework so they would start with the
laptop or Jupiter notebook somewhere and
then when they start
acting with Amazon sage maker or on AWS
cloud they have access to the three core
capabilities of a platform that I pulled
out earlier the notebook instance the
training and tuning capability and so
the the radish or the arrows over there
represent the control capabilities that
the developer or the data scientist has
can either spin up a jupiter notebook
environment or model training
environment or model hosting environment
with the controls of encryption and the
control of access control and all these
things that are provided by sister
services and the relationship with a
cloud storage is provided with Amazon s3
which is an infinite infinitely scalable
cloud storage capability making it
possible for developers to essentially
store data and then seamlessly
communicate with a platform and extract
that and so this is where PI quartz and
sage maker and other deep learning
frameworks come in where the
relationship of sage maker with Amazon
ECR makes it possible that we can store
our gotham's in the form of docker
containers and then use this docker
containers to kick off what we call
machine learning training jobs or tuning
jobs all these are the capabilities in
that environment and of course we need
we use AWS carwash logs to store the
logs externally so that we don't have to
maintain that maintain that
infrastructure running at all time and
it is possible for you to run a training
job and then kill it off at the end of
the training job execution and so if we
want to dig deeper slightly into the
jupiter notebook instance pretty simple
it's a machine that is attached to an
EBS volume and a number of jupiter
notebook examples there to get ramped up
and get started faster and there's a lot
of high thoughts related examples there
as well to get started now from a model
training perspective it becomes
interesting so this is an informal
cluster that spins up a game with a
Python container and then it's attached
to an EBS volume in which we download
the data from the cloud storage with the
possibility of streaming the data in
directly to the machines to start
training the models and then we use that
to train the machine learning model and
then after we're done you have the
possibility to host that either in a
managed real time and
environment or in a batch inference
endpoint environment so I believe you
familiar with that now uh the last thing
as far as the workflow and the
infrastructure is concerned is that you
can consume your machine learning model
using it'll be lambda and API gateway so
I hope from here you see how HT maker
fits in the middle of the execution of
basically building training and
deploying machine learning models as
scaled now where we want to focus our
attention now is what happens within the
container once a Python container for
example is pulled in so the first thing
to note within that environment is a
data agent that exists in stage maker
basically the data agents role is to
communicate with the cloud storage and
then go fetch the data from there on
behalf of of the customer so what
happens is that when a training job
comes on the data is handed over to the
docker containers by the data agent the
other thing is the love metric agents
that runs within the container that can
again you know handle the logs if you
have multiple machines that are working
together different deep learning
frameworks it's possible to go get these
logs from all these machines aggregate
them and push them to the class storage
and if you start going up the stack then
you have things like the distributed and
included GPU capabilities that are
installed there if in case
are you using a deep learning a GPU
based machine and we start going up the
stack from there then you have the deep
learning libraries with the deep
learning framework and additional
libraries that are related to that and
then you go further up the stack you
have the sage maker Python SDK and the
sage maker Python SDK is very very
powerful it's actually the core of the
best experience or deep learning
practitioner can have on Amazon
changemaker today it makes a lot of
thing is things easier things like
spinning up their environment
configuring that handling hyper
parameters and all these things I'll get
into some of the details and then at the
very top of the stack you get the
algorithm the algorithm is now what you
would write as a script for example if
you would want to create a deep learning
distributive a job with PI porridge
wouldn't have to write the GPU code you
wouldn't have to write the data handling
capabilities the data is handed over to
you by the data agent you wouldn't have
to worry about where to find a different
log files because the logs are handled
by the log agent and so on and so forth
now if you use the sage maker SDK and
you do importation maker you get direct
access to a decent number of
capabilities that we are going to jump
into right now
Oh something that I have actually done
so I figured before continue I figured I
could share my screen and then get the
job started let me run the screen
you seeing my screen
is the resolution okay it's pretty small
is it big enough okay so this is
essentially the example that we're going
to go through as a demo sometime later
but I wanted to make sure that I keep
the job off and then it starts training
even though even though even though I
ran it already and I will show you the
results immediately but at least you
could see that I'm kicking off as a
shoemaker job from my own laptop from my
own vehicle environment and then I'll
walk you through the details here and
what does that mean and so on and so
forth if it lets me oh I'll kick up
another job so that we can see there
okay perfect now back to the slides okay
so once you start digging into the sage
maker Python SDK things look like this
we're familiar with the stack already
the anatomy of a typical piece of code
you're leveraging the sage Maker Python
SDK makes it so easy to get the job
started and I'm gonna walk you through
what you need to do what does that mean
and so on and so forth so of course it
starts it starts by importing sage maker
which is a Python SDK that gives you
access to the PI torch container and so
when you do import sage makeup I torch
and that gives you access to a few
classes one of them is PI torch which is
basically the estimator who here is
familiar with scikit-learn okay perfect
a good deal of people so you can think
of the sage maker estimator as a weak
analogy of scikit-learn
where you can essentially have an
environment and leveraging that
environment you have a context and then
through that context if you do if you do
a fit or fit deploy you have the
capability of training a machine
learning model given that context so the
experience is almost similar when you do
import the stage makeup I poured
estimator it gives you the capability of
specifying your main script for example
the script that you want to use for
training your machine learning model and
we alongside other parameters things
like the training instance type the one
that you want to use the training
instance count and a framework version
in this case I specify pythons 1.0 the
the development branch right now the
face book is is pushing to to get to to
release candidate and then you also have
to specify the hyper parameters now if
you look at this carefully you haven't
specified the way that the data was
going to be distributed to the different
machines you haven't specified the way
that the logs were going to get handled
you haven't specified all of these
distributed titled mechanisms and then
those are the kind of things that the
platform handles in the back end you
haven't specified how much data you want
to send to one machine or the other
sharding is handled
matically whether you want to fully
replicate the data or you want to send
part of the data to a machine or the
other so there are some of these backing
capabilities that I handled by the
platform but it understands PI torch
well enough to know that if you want to
do this video training with Piper words
then you have access to different
back-end capabilities as far as
disability training is concern and it
provides that to you now once you have
an estimated object the next thing you
need to do is to say hail given my
scimitar I want you to fit that on this
data set now note here that the data set
comes with two in the dictionary with
two types of parameters one is the
training parameter which is analogous to
the training data set and it's pointing
to Amazon s3 and the other one is the
test what we call channel that is still
pointing to Amazon s3 so once you do
this what the platform does is okay you
want to use PI porridge you've given me
this estimator I know what your main
function is and I know where your data
is so with your permission I'm going to
go download that entire data set bring
it down to the platform in a specific
magic folder that I'm going to make
accessible to your deep learning
framework and then your deep learning
framework and pick that up and leverage
the distribution capabilities in the
backend that I provide and so once the
model is trained the environment also
knows where to go pick up the model and
then send it back to the cloud storage
and where to go pick up the logs and
then send it back to the cloud locks
capabilities and so the next thing that
is left for you to do is to essentially
deploy the model in the production
environment so with de piel or y6
letters except specifically you can
instruct and summon sage maker and say
hey bring me a machine of this type mlc
for extra large for example and then I
want three of these machines and then
within these three machines I want you
to host the REST API and then host my
model there for me and that's the entire
that's the entire story so if you take a
step back and then look at what it means
to kicking up a machine learning
training job with three computers in
that environment it looks
like this you have access to your data
again and then the data agents in these
three machines are all going into the
cloud storage and fetching the logs
sorry fetching the data back into into
the machine and then so it puts that
into a magic folder called up to ml data
and then at the distribution layer the
platform understands how to make these
due to pythons disability capability is
working together in order to share
things like checkpoints in order to
share things like global steps in order
to share things like that are required
for running a distributed training job
and then the next thing that we're going
to get to talk about in details right
now is the experience so yeah after the
model is done training the data is
pushed the model is pushed back to the
cloud storage and then the logs are also
pushed to the to the cloud logs and so
we're going up to stack into what is
happening now into the deep learning
capabilities and what is pythons and
what are the different benefits that you
have once you're using Patras and for
that I like to have joke giving us some
of these details cool thank you Dan
okay so I'm not Jeff Smith but Jeff
Smith is my colleague here won't see a
repeat of this talk he's gonna come on
Friday and I think give it about 10 a.m.
but I am Joe Spisak product manager for
face book AI
platform so that includes PI torch as
well as another project called
onyx and then our kind of broader open
source strategy as with respect to AI
one thing that's really cool is like one
of the reasons why PI torch is so
popular is because of its you know
customer obsessed nature like the the
customers in this case our researchers
and really that's over the last new year
or year and a half year has shifted more
towards being a production framework so
all those researchers that have been
doing great things for the framework are
now wanting to put ship those into to
large-scale production and so that's
kind of the story I'll talk about here
you know as a project it's only maybe a
little less than two years old so it was
started in roughly early 2017 so let's
start with the higher level of why
actually Facebook cares about pie
tourism and the thing
we use it for so of course things like
ranking algorithms for recommendations
if you ever been on on messenger and
you've had a recommendation from a from
someone or been chatting in a different
language if you have friends that's
because language doesn't like to chat we
do the all of these translations in real
time talk more about translations later
on and where Facebook is invested there
and and and the code that we've
open-sourced
in collaboration actually with with
Amazon it's also things like
accessibility so if I'm for example
someone who can't see very well I may be
blind or so on you can actually have
accessibility features where it can
actually do things like text-to-speech
based on the content within images and
that's been really powerful for for for
the viewing impaired and using Facebook
things like box and assistance so we
have you know em and that's obviously
powered by AI and things like generating
content so when you upload your your
movies onto Facebook we have algorithms
that we'll go through and basically
generate a preview automatically for you
based on you know a number of features
within the within your video air effect
so has anyone in f8 this year okay no
whatever every it's very small so it's a
it's Facebook's annual conference but we
with our mobile app we're able to
generate you know AR effects instead of
having kind of this static flat map you
just overlay your phone it gives you
this visual effect which is really
powerful and it kind of gives you a more
immersive experience and of course VR
hardware so we have oculus as part of
the family of platforms within Facebook
and AI is a big big part of that so all
hand tracking is machine learning based
you know things like lip sync movements
are all tracked and using machine
learning and of course there's a lot of
bad things that happen on our platform
so things like share baiting which we
try and hope you never ever see but
sometimes you see it that is
hopefully removed ahead of time using
machine learning we can tag that remove
it also things like suicide prevention
that we do within our platform as well
and all up we do you know over 300
trillion predictions a day so we run a
really really large scale so we actually
we need a framework that can basically
take a lot of this research that's
cutting edge work that we're doing and
scale it up into to large-scale
production very very quickly you can
imagine we take a lot of the data from
some of those previous applications we
want to train that on algorithms and
then get that deployed very very quickly
because you know that type of
adversarial problem say like share
baiting the data has a time value so we
want to deploy that very quickly we also
deploy on phones so instead of sending
data for example from your phone over to
our infrastructure you can actually run
locally and do predictions so we have a
runtime as part of always previously
called cafe to runs locally out over a
billion phones and it's very compact so
things like at the AI camera things like
style transfer you know augmented
reality effects all that is actually
using deep learning and running actually
locally on your phone and we actually
have a team that makes sure that even if
you're using a fairly low end say
Android phone all the way up to so the
high end iPhone it's gonna work pretty
well for you and so one thing that that
we take is a really a full stack
approach and so we we talk about the
research we do we talk about the
products we we develop we actually open
source all of this as well so all the
way down from the hardware so we do
things like open hardware designs so if
anyone's heard of OCP open compute
platform has anyone heard of P okay good
so a few folks so basically open source
all of our design for our hardware we
have a kind of a consortium around that
and we do fleet wide metrics and
benchmarking Zoar we're constantly
looking at how is our fleet of servers
doing how efficient they are etc we have
a number of compiler projects so you
know how do you generate the best
performance based on that hardware of
course Patrick
is our are chosen framework and we use
that across a number of workloads and
then libraries so on top of PI torch so
there's a number of different libraries
like the one I'll talk about here in a
second run translation that are built on
top of hi gorge so things like detect
Ron which is probably the the cutting
edge most cutting-edge computer vision
suite of algorithms it's out there it's
open source the models are available
it runs extremely fast for different
things like object detection
segmentation etc models we open source a
ton of models based on opens to open
datasets and then of course datasets
themselves so basically we do everything
at every level of the stack but then we
also open source or open up all the
things that we do so it's a very unique
place Facebook is in this regard so I
was chatting with some folks before the
the talk here and a few folks hadn't
heard of what haven't heard pi torch so
I didn't really know what PI torch was
so I figured given you know dance talk
is jumping right into you know how you
use it a sage maker I figured I'd dive a
little bit deeper into the code and show
you like what is the purpose of a deep
learning framework like how does it help
you but first it's you know it's popular
so pie chart one to give you an idea is
it's the number two fastest-growing
open-source project on github to give
you an idea so it's a very popular
project almost three X number of
contributors in the last 12 months so I
think the only other more popular
project on github was as your Docs which
I don't know who's using and
contribution to add your Docs but okay
but PI sources is it is really really
popular I get people come up to me all
the time saying I love the project thank
you this is amazing
and so I pass that on of course the dev
team but you know when you look at the
the project itself so what is it it's in
a lot of ways you can think about a
framework as a number of api's and
libraries
for convenience so if anyone has taken
say like Andrew and Steve learning AI
class or anything actually that takes
you from really from scratch like
writing things and say like numpy you'll
know that it's pretty painful when you
start to implement a lot of these
yourself so what a framework really does
is gives you a set of really nice api's
to do different things and accelerates
them on different hardware
you know backends and so if you look at
some of the api's that are supported and
packed which now go into the code here
in the in the next slide but things like
torch tienen is a suite of api's that
allows you to define layers or define
groups of layers writing those from
scratch would be really painful
optimizers so things like SGD or atom
those are implemented for you and of
course they're open source so you can
look at how they're implemented or or
they might be calling a library
underneath
Dayna having you write your own data
interrater or data loader from scratch
is painful so being able to call simple
API and load data is really helpful
autograph this is kind of the heart and
soul of Pi torch and this allows you to
do auto differentiations so when you for
example define a tensor I can actually
just call you know say like lost out
backwards and actually it does an auto
differentiation for me writing those
derivatives writing the chain role is
kind of painful so but you know it works
like PI torch do this for you makes life
easy torch vision or torch vision gives
you nice easily accessible models and
data and then of course something that's
really new to the framework is what
we're calling JIT or just-in-time or
really a head of time compiler and this
is something we'll talk about later on
but really this is the core of how you
take your code from say a research
exploration mode and refactor it so that
you can then take it to the large-scale
production and this is actually what we
do at Facebook this is how we we take
our exploration because not all models
are gonna go to production maybe less
than 1% probably less than 1% of 1% go
into production but the ones we do want
to take in production we want to take
them gracefully we want to take them
frictionless and scale the model so
hopefully you can read this code but
this is a very this is your canonical
amnesty example but it really kind of
hits home how easy things get with
things like the the libraries themselves
so for example n n modules so I'm
defining a
a very simple network here basically two
fully connected layers and you call that
torch ten dot linear and I can define
you know the parameters around those
those layers very simple for me to have
to do that manually would be a lot of
work I can also call things like
nonlinear functions so when you get into
the the forward function when I want to
actually take a forward propagation of
that defined network
you know rectified linear units are are
kind of the state of they are some
variant of rectified linear units are
pretty much what are used today dropout
that's all implemented for you sigmoid
these are all functions that basically
if you call torch dot you're going to
get that it's it's implemented it might
actually be calling something like who
DNN or mkl underneath but it's very
simple the other cool thing about high
torch is something we announced about a
month or so ago we announced a C++
front-end and you know you can really
it's actually kind of hard to tell the
difference between the Python and C++ so
if you if you look at they're they're
very very similar there's a few more
colons obviously in the C++ but
otherwise you know besides the include
statement really there's there's not a
lot of difference this is actually
empowered a lot of folks inside Facebook
as well as a lot of high-performance
researchers that you know wanted to
squeeze that extra bit of performance
out of their framework so you know for
example our Starcraft team that competes
and competitions with their BOTS their
application is largely C++ for them
writing this code fits right into their
bigger application very straightforward
and then when you start to get into the
training it's very easy so in this case
I'm loading data so I have this nice
utility called data loader in this case
I'm going to bring in a computer vision
a data set for computer vision for M
nests so I can just basically call the
torch data sets torch vision data sets
API I can specify my optimizer in this
case stochastic gradient descent very
simple and then from here I can
basically set the number of epochs and
train my model
so things like you know actually doing
backpropagation is as simple as calling
los backwards and it actually goes in
and does a backdrop for me and then at
the end there I'm just doing a check
point based on modulo if there's a
remainder or not and you can see the
analogous C++ code so it's it's actually
very very simple if you had to implement
all this yourself in something else it
would be quite painful so that's really
what a framework brings for you so let's
let's talk about the the problem of
resource production because this is the
problem that's near and dear to me it's
something that it's actually how I'm
judged to Facebook how fast we can get
researched into production and how in a
production scale and solve some of these
problems so if you remember there's this
project called onyx is everyone heard of
Audax so onyx okay a few folks onyx is
open neural network exchange this was
something we developed actually in
partnership with Amazon a little over a
year ago and it's a project that's still
going it's still got a lot of momentum
and we actually use it in large scale
today at Facebook for a number of
applications translation being one of
them
we had another framework called cafe 2
which is embedded really deeply into our
infrastructure
it runs most of our applications today
at a pretty large scale and really onyx
was the bridge to try and get high torch
models which were largely research-based
into cafe 2 for large scale inference
and so you know onyx was was nice and it
really helped us but we actually wanted
to move faster and that's why we
actually started doing this work around
PI torch 1 so how do we prototype avoid
the transfer but then deploy all in the
same framework that basically became
what we call high torch and that's
that's really what pi torch one is and
so to sum up what pi torch one is in one
sentence it's for us a seamless path
from AI research to production so all
the teams that across Facebook and now
we're starting to see external teams
different research groups and different
companies they're starting to to look at
PI torch as a production framework not
just something that they can explore and
hack on this is a real frame
that is gonna skill as well as any but
give you that flexibility so I'll have a
little bit more into what production
actually means so when we say
transitioning from research to
productions it means in the same
environment so that's the seedless part
so I'm not having to export a model
import a model change environments it's
all the same code base and that means
things are fully loaded so there's
basically we expose things like just
torch distributed for distributed
training things like you know mobile
which we're working on for the future
here you know all these API is basically
anything for performance and related in
scaling is exposed to the user on the
front end through Python opt in so
flexibility is is basically maintained
and you only need to trade basically
opt-in in two different things when you
need them in other words you pay only
for what you really want to use so I can
have all the flexibility of Python but
when I start to transition into a
production mode I start to trade-off
that flexibility because I want to be
able to scale that model and say some
type of C++ serving environment like
sage maker and again same environment so
it allows me really to refactor my model
continuously without having to change
environments object basically go and
serialize my model imported into another
framework do anything like that I
actually can just refactor in the same
environment so what does production
concretely mean so for us at Facebook it
actually means Hardware efficiency so
you know I'm I'm judged on things like
how you know how well we're utilizing
our infrastructure so whether at CPUs
GPUs or custom accelerators you know
that either were rebuilding our cells or
through other vendors scalability so you
know power is expensive so we need to
utilize our GPUs and our infrastructure
as much as possible so distributed
training efficiency becomes really
important I'll talk about that with
respect to translation in a minute and
then cross-platform so how do I serve
models in an environment where there's
no Python interpreter that's a challenge
and you know mobile is a great
environment for that so mobile devices
in our infrastructure serving Python is
pretty much a no no unless we we make an
exception for it and it's a
time-to-market type of situation so we
have this thing called torched yet that
I talked about a little bit earlier so
you know things like experimenting so
yeah kind of have your full flexibility
of PI torch so think pythonic think and
dump I but with GPUs and and high
performance
you know tensor ops I can then take
basically parts of my graph I can
extract parts that I care about so
things like if I'm going to serve the
model I can extract a part of that graph
and go and serve that and I can identify
that using torch JIT and then of course
how do i optimize and deploy so Dan
talked about deploying at scale with
sage maker how do I take those whole
program optimizations and deploy without
Python in the environment so one thing
that you know when you look at
frameworks you know historically had
these these eager and static frameworks
so when we talk about production you
know eager mode has never been something
that has been production like you've
never been able to take eager mode
production models directly into a
scaling environment to run those a
large-scale difference basically the
difference between eager and Static is
when I actually defined my model I'm
actually defining the graph as I'm
defining it so it's a kind of a defined
by run environment in the case of a
static graph so something like cafe 2 or
say a tensorflow
type of model I'm actually writing I'm
doing kind of a meta programming where
I'm actually defining my graph I have to
compile that graph and then from there I
actually have this kind of static graph
which I can do new predictions with and
that's what I use for frame friends so
they both have a lot of you know pluses
and minuses from the eager front end
perspective this is actually what
researchers love research is a dynamic
environment I don't know if my idea is
going to work or not so I might iterate
hundreds
thousands of times and I try hyper
parameters I might try different layers
so they like the idea of having
something that allows them to express
any idea possible it gets some really
innovating thinking about different
ideas thinking about different
approaches but it's hard to optimize a
hard to deploy so that's been always the
knock on on eager it's always fun to
debug Python but getting that into
large-scale production typically not
going to happen from a static graph the
exact opposite is true so it is easy to
optimize I can take a graph it's static
rack I can optimize that very very
easily I can deploy it but man once that
static graph is compiled having to
rewrite that and recompile that every
single time and debug it I can't use
print statements for example or Python
debugger it becomes very very hard as a
researcher looking to iterate fast and
do real-time research so that's
something that we've we've thought a lot
about and something that we've brought
with PI torch and and some of the things
we're doing with this new PI torch warm
platform so again hi torch is your
models part of Python program with
autographs for generating derivatives
it's simple it's debuggable so print
statements yeah I can use those it's
hackable so I can plug in with other
Python libraries so anyone who loves
Python should love PI torch but of
course it needs Python to run and it's
difficult to optimize and paralyze but
we've added this thing called script
mode and so when you start to think
about okay this is a model that is
functioning and and meeting the
requirements of something I want to
actually take into production this is
when I start to look at how do I take
that how do I script that model or how
to trace that model and get that into
something that can be deployed well into
a production environment so where I
basically have no per ton know Python
dependency I can optimize it I can start
to use other libraries that are more
hard-coded and and optimize for serious
performance in serious infrastructure
like we have and that's really what we
we have as far as high torch JIT so this
is what we're betting big time on we're
basically this tool allowing you to have
the same environment same research
development environment would allow you
to gradually refactor your code using a
couple of rappers or Ritalin called
decorators and there's a bunch of
tutorials online if you go to Pytor chat
or you can check those out but really
there's there's two that matter and
that's really the the script decorator
and the trace decorator and they're both
handy and it can be used interoperable
so you can use them both together but at
a high level what trace does is actually
runs dummy data through your network it
actually generates a graph and so it'll
actually trace a graph out of something
that is purely imperative as I'm writing
it you actually generate the graph for
me in real time it's very cool it's this
is very handy and very good for
feed-forward networks computer vision
CNN's works really really well once you
starting to get into things like you
know language models that use control
flow so I'll talk about in a second the
translation work so control flow is
nasty it's data dependent in some cases
and so basically what we've done is
created also a script decorator and what
this does is actually constrains you
into a subset of a Python looking
language it's really a domain-specific
language a DSL that does not depend on a
Python interpreter and so when you kind
of give a get an idea of what you want
to do you can start to use the the
script decorator control flow will be
preserved and it allows you then to save
and load that model into a C++
environment so all of a sudden you're
free of Python and you can actually run
a really large scale very efficiently so
I'll talk briefly about translation and
stage maker so this is a collaboration
between Amazon and Facebook so very
briefly translation at Facebook has a
history we started out using Bing
Translate way back in the day
in 2013 we started bringing the all that
in-house in 2015 and started using your
own machine translation in 2016 and from
then on it's been basically all nmt and
it's been
very good so you know our progress has
been good I think we've we're now at
almost six billion translations a day
this is slow graphic and it's used sorry
it's used in things like social
recommendations engagement bade suicide
prevention and a lot of other
applications and you can see you know
kind of in in action here so this is one
one of the research scientists in our
translation team and his kids talking
about the fact that they love chocolate
ice cream and that's translated in
real-time and one of the things we
talked about previously was performance
and distribute training I think the one
of the gentlemen over here I'm asked me
about that so as part of patchworks 1 we
support distributed training and then of
course working with Amazon we've enabled
that through sage maker but we actually
brought this down on really large data
sets that we have internally down to 32
minutes so we can train these really
large data sets with pharisee which is a
sequence to sequence translation library
that we've open-sourced
and that has been optimized for a sage
maker and so doing large-scale training
on say 128 GPUs and that can be a p3
instance for example takes us for
example to 32 minutes
so if new data comes in we can train
very very quickly someone can go and get
a coffee and they're come back and and
they have production models trained so
now I'm gonna head back over to Dan to
talk about about the cycle gann work on
stage maker using pi Jorge and one quick
note here so the fair seek work is
actually in this bitly here so I think
this is actually Dan's github account
for now but it'll be in the in the
examples folder in Sage Maker soon but
this is again jointly optimized work
this is actually what we use in
production today at Facebook these you
know these models these architectures
and this is optimized any sage maker so
grab those check them out we do six
billion translations a day with it it's
it's pretty awesome very high
performance so we'll jump into the DC
KNX
yeah okay thank you so much Joe so it's
for a quick recap
so just spoke about the details of fight
forge and how it really drives the
momentum from research to production for
different data scientists and developers
I mean I got hooked on PI torch a year
and a half ago when working with a
customer and I walked in and I was
talking about different deep learning
frameworks because again at AWS we
support all of them and the customer
actually convinced me to to get to play
around with PI torch and see how easy it
makes things and what I thought I would
do is to walk you through a practical
example we don't have all the time we
need to go through what exactly is a
generative adversarial Network but I
found it to be cool enough to show you
exactly how to put the things that Joe
mentioned in practice and again the code
is available on github so you can pick
it up today and play around with it so
when I started learning Python again
about a year and a half ago it was
really easy for me to to pick that up
and then and then follow through the
execution as opposed to the way deep
learning looked like before where you
have to build a computation or a graph
and optimize that graph and eventually
that graph runs in a very optimal manner
but if you have a problem is really hard
to debug in right so with PI port it's
really easy to just walk through exactly
what you're trying to build and then
follow the execution step-by-step so
that you can troubleshoot that through
time that's one of the key benefits that
I picked up and it's great to see the
movement now with own x-square you can
develop in pike forge in a very
sequential manner and then after you're
done developing you can convert your
model in a very optimized binary that
you can run here or there so just to
look at so this is who here is familiar
with generated by their serial networks
and guns okay so the the idea here is to
train two networks newer networks that
work together and then one is
designed to basically discriminate
between real faces and thick faces in
the case of this example and the
discriminator in this case is trained
towards to understand whenever it sees a
new picture whether that picture is a
real picture or not and a generator is
designed to to generate net new pictures
from basically nonsense data aka noise
and it's trained against the the goal of
being able to fool the discriminator
into making sure the discriminator is
not able to tell whether a picture was
taken up and to the equilibrium state
that you get here is where you have the
generator getting better and better at
generating fake features pictures in
this case that look real and a
discriminator basically landing at a
coin flip of 50/50 whether it thinks
that the picture is is real not and then
if you if you want to extrapolate that
use case into many other scenarios like
fraud detection anomaly detection or
privacy and different other use cases
like that generated by the serial
networks to us seem to be very very
practical in those domains so this is
the structure of my code I'm executing
things on Sage maker but at the same
time I'm using my own laptop to develop
things and thanks to the sage Maker SDK
I can actually have that relationship
where I leveraged the the hardware from
from Sage Maker as for yet using my my
own laptop for development
so here the normal is the this is the
okay let me start with the neural
networking and it there's a lot to cover
so I'll advise you to just go read the
code and on github but the key call-outs
here are well we're creating a generator
using a generator class leveraging the
neural network tenon model library that
is provided by Python and not to me of
an NN module is is very simple in the
English in an init method of the class
basically what you have to do is to
construct your neuro network if you've
ever seen a neural network before it's
essentially a set of linear combination
that you potentially do
some convolutions on in case you want to
do image analysis or do other types of
transformations on and then use you put
that in sequences and then you decide
whether you want to keep the gradients
from these different calculations or the
different parameters associated with
these calculations or not and then if
you keep these gradients then the
backward propagation process is
basically designed to compute the the
rate of change of the last function with
regards to the rate of change of these
weights so that's the whole architecture
mental model of building a deep learning
model so in the mid class what you have
to do is to describe your neural network
in the case of a generator that's what
we do here and this small part is
basically initialization of the weights
or the parameters of that generator and
the the other important thing that I
want to call out here is this forward
function so every time you see a Python
code where a neural network is being
constructed you always see the module
created and the forward pass you never
see the backward function unless the
problem becomes very very complex
because by torch essentially makes the
backward propagation the calculation of
the gradients and then the tracking of
all of these rates of changes automatic
for you that's the essence of autocrat
so when you construct the neural
networking fighters you always have the
the design of the neural network and
then the forward function which
basically passes the data through the
neural network so we're doing that here
for the generator and then we're doing
the same thing for the discriminator and
essentially what the generator does is
that it takes any random vector of noise
and then come those transpose
convolutions to it in order to come up
with the structure of an image and then
it's trained against making that image
look as real as possible and it is this
community does the opposite it takes an
image looking something and then it does
convolution some type of object
detection stuff on that and then it has
a sigmoid activation or logistic look
yeah logistic activate activation
function at the end basically compare
computing the probability of that image
the binary probability of that image
being a real or fake so that's really
what it looks like in this single piece
of
good in my neural net file I have
described the generator and in the
discriminator and then I can make them
both work together now the next thing
that is important as far as part because
in this example we're using a PI torch
to train to build and train the neural
network but we're also using tensor
board in order to visualize different
things like the last function like some
images and like everything else that is
related now I've had someone asking me a
couple of days ago if I use H maker and
then I want to add additional libraries
like open a I dream if I'm into
enforcement learning or anything
anything else what can I do on sage
maker well the good news is if you if
you add in these requirements the text
file within your source directory of
your code and you deploy that to Sage
Maker
it's designed to pick that up and then
understand that do you need these extra
libraries and then go and install them
automatically automatically for you so
that's basically what I've done here and
again not no time to cover everything
but I wanted to make sure I hit this
point your requirements or text file is
important put that in there sage maker
would find it and then it's going to
install everything that is in that
requirements or text file and that's
what I've done here to have a 10-2 board
capabilities now the next thing is my
main function the function that is
actually going to leverage that neural
network description and then do the
iteration and do the training and do the
logging of the the the variables and
everything else to to to tend to board
so in this main function again I'll call
the pythons library which are very easy
and easy to remember Joe described them
just now where you have torch torch
distributed gives you distributed
capabilities torch ññ is what gives you
the module and in parallel and
optimization are basically capabilities
for building you know distributed
networks and optimization is for the
last function and in the backward proper
propagation and then you have some
utilities functions as well for for
handling the data and handling some
computer vision use cases so the API is
actually not complex and again it's very
easy to track and follow so the next
thing that I've done
here is that from my neural net Wow I
import my generator in my discriminator
I have some utility functions here to do
different things like loading the data
and have some loggers that are available
and then from tencel board X which is a
library to basically log ten to log PI
porch of a different type of data points
onto 10 to board I do import the class
called summary writer and it's a summary
writer that gives me access to these
writer object to which I point to what
what I call it tensile board log
directory now this log directory is
essentially an environment variable that
CHT maker provides and the environment
variable is called output data directory
so again I'm building my own neural
network and my own algorithm but I'm
aware that station maker would provide
that output data directory for me so
that if I put something there sage maker
it's gonna pick that up at the end of a
training job and put that over to Amazon
s3 that's the relationship between PI
toward stage maker and me as a developer
again ai driven development now I start
my summary writer and these are
basically some small helper libraries to
load the models I create I load my
generators the next important thing here
is my training function so in my
training function I do I lay down the
steps here so that you could read
through because we don't have a time to
cover everything but in in the training
function here
I essentially go through some of the
details about my environment I try to
find out if I have GPUs available and if
I have GPUs available I just say hey I
want to do distributed training and then
sage maker also would provide me with
environment variables that would tell me
the number of hosts that are involved in
these training exercise and as soon as I
have that I know that I can do
distributed training on these on these
different machines so the rest of the
code it's really about loading the
generator loading the discriminator the
optimizer and everything and then adding
some mechanisms to to logging the data
over to tensile board there's a lot
going on there but it's very easy to
read hopefully and
and I try to come and everything so that
you can track that through execution the
idea there is all these extra classes
and extra libraries like the average
meter and other stuff is a mechanism to
be capturing data as you training your
model and then pushing that over to 10 -
board as the model training started and
say shoemaker knows how to grab that put
that over for you on Amazon a string and
then I start my training loop and then I
go through the epochs and for each
epochs I I do some tracking how much
time did it take for me to to load in
the data and going through the code a
little faster here because I want a hit
important point after the whole
execution of a training everything is
commented so you can go through that I
basically write the the I basically
write the information that I have back
to back to a file or back to 2/10 abort
so the idea here is I've created my
generator Emma discriminator I pass that
to the model I stop my training loop and
I do my feed forward and backward
propagation what titles helps with but I
also have tensor board enable and I have
the summary writer that helps me write
things back to to tend to board and I
have my and I have a mechanism to write
the real files to a local folder for me
again in that output data directory and
because this is about generating images
I also want to see these files at the
end of the execution stage maker is very
useful in that sense because it would
capture all of that data that I saved to
that output data directory and then push
that back to the cloud storage at the
end of execution so two more minutes
guys we're almost done so the what what
the experience looks like at the end of
the day is I can go back to my Jupiter
notebook now and then start the process
of you know playing around with the data
a little bit doing some data exploration
that's what I was doing here I got into
the data and then looked at some of
those faces that I wanted to play with
just to just to make sure that I had the
right data set but this is where the
training really begins so if you
remember the anatomy of the you know
training job kick start with with sage
maker I still use the same PI torch
estimator
I used the main function that we just
walked through I pointed to the source
directory which has the requirements of
text file and then I pointed to the
framework version of hi torch that I
want and sage maker knows to go give me
pyatters 1.0 in case that's what I
pointed to and I say I want two
instances and I want the P 3 instance
which is a CPU base volta instance and I
say ok then I want to go through 438
parks and I could I could have added
extra hyper parameters here and the
framework would know how to kick that
off now this is the interesting part
based on the estimator if I say fit it
to the input and the input here are
basically pointing to to the data set on
my s3 bucket what's a shoe maker would
do is that it will kick up if we kick
off the job and the red the green color
and the red color logs here are
basically the lots that are coming from
different nodes that are involved in the
inner competition so I could have
decided to kick off the job and not wait
for the logs that's what that weight
variable that you saw earlier is about
but if I wait for the logs what's H
maker is going to do is that it's going
to go back to all these machines that
are contributing fetch the logs and
bring them back to me on my client
wherever my client is and then I see
different colored logs that I can track
for different execution now the few
things I wanted to show you here are the
things like CH maker installing these
libraries that are in the requirement of
text file for me that's basically what
is happening here the logs are long but
I'll catch that and we will be done yeah
so our points in a curse starts it says
okay you want these libraries I'm gonna
go fetch them for you and I had image IO
for example in my requirements a text
file so it went and and then search that
as long as it's peep installable it can
download and install that for me and so
on and so forth all the way up until it
has all the required libraries that you
need to basically start start the
training job and then whatever you print
out of your whatever you print to
standard out by print whatever stage
maker is gonna pick that up and then
push that out to the logs as well for
you and so this
this is basically the training
experience so in this case I've been
using two GPUs on the cloud to GPU
machines on the cloud to train my my
workload
the other interesting thing is that at
the end of the execution sage maker
would give me all these logs that are
metrics from the GPU CPU utilization and
everything that I need to to visualize
the execution so far so that's what you
get from the cloud watch logs and I can
I can also get the logs actually that
came out that I was looking at in the
case I was waiting for for the logs or I
can have that stored somewhere on the
cloud on AWS cloud watch logs and never
look at it if I'm not interested but
this is what you guys were all looking
for so at the end of the execution of a
job in in the specific example that I
showed you
I've been generating pictures at the end
of every epochs just to be able to see
what what is the output of my fake what
is the output of my real and then if
there is anything there that is
interesting so all of these output stage
maker put in the output folder that I
can now come and look at and say ok this
is really for each epoch what a fake
image looks like right so this is really
cool and in the case you you want to do
the kind of deep learning that really
requires you to do some analysis at the
end the relationship between th maker
Pike torch and tensor board gives you
that capability so there I was looking
at some some of the images but all the
cool thing that have that that is
possible to do here is to basically look
at look at the logs for example you can
create a Jif or gif depending on how you
came to you want to pronounce that you
can create a jiff too to start
visualizing your your execution and your
logs afterwards and all of these things
that you can create within the container
in your main script stage maker can pick
that up and then and then save that for
you afterwards so here I can see how my
generator is getting better and better
at at basically creating fake images
that means its own loss function is
going down and my discriminator is still
very good because it knows you know what
is it good and what is a bad image but
over time you will see that the
generators loss or error is going down
and this community of error is going up
and they're going to meet each other
somewhere
50% of around the point of stability so
that was for the last and then you can
also visualize some of the epochs you
know and what the images are looking
like through time now the last thing is
the tensor board experience so because
of because I've created all of these
logs and I stored them using again
tensor board X and PI porch I can now
kick off a tensor board environment
where I can visualize all of these
things intense board is powerful in the
sense that you can save scalars which
are real values and really anything that
you have on your mind so in this case I
try to track the average batch time the
time it takes for me to just like run
through a single batch or the time it
takes for me to load data whatever you
have in mind you can put that on on top
of that and that's why I had these
average meters and intensive board is
going to present those for you so if you
go be creative about the type of error
you want to lock but the example that I
have there can show you a few different
types of things to love and then so I
can plot the average loss for the
generator and then some of the values
that came out of the discriminators
through time and I could also plug plug
the batch time and then plot the data
time the time it took to load the data
and so on and so forth so all of these
things are stored and again station
maker picks that up and then sends the
log back to you so that you can
visualize we tend to board the last
thing is also the ability to visualize
the images themselves side by side right
so using tensor board X as you going
through training it doesn't matter if
you're using multiple machines if you
store these images then sage maker can
pick that up that up and then send it to
tend to board for you to visualize later
on so this really brings again the whole
concept of AI driven development
together where you can be writing code
for building and training deep learning
models but at the same time you are also
speeding up infrastructure
tearing it down and at the same time
you're building the cap of the metadata
that you would need in order to explain
and have a conversation about the deep
learning model that you train which your
team and which are management and things
like that so with that I know we're a
little bit over time and thanks again
for for for sticking around
we will be standing right out there if
you have any question and if you want to
follow up on any of these the
information for the yeah so this is the
CTA where you can get the the Python
environment and then the the DC gun
example code that that will have it's on
github thank you so much
[Applause]