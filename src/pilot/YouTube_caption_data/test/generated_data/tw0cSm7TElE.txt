I'm going to start with the sort of
high-level introduction to probabilistic
programming and then most of the rest of
the talk will be focused around three
sets of examples instead of making it
you know the talk instead of being very
theoretical or very driven by a bunch of
equations is going to be relatively high
level and we're going to sort of give an
introduction to probabilistic
programming through these three
different sets of examples so let me
just start with the introduction right
away okay so you know the first you know
we're just in probablistic programming
and so let's first talk about the first
part of that the probabilistic modeling
aspect that we're interested in so a
very useful paradigm in science and
elsewhere is model-based reasoning right
so we have some data that we want to
explain and so we have some hypothesis
about that data and we encode that
hypothesis and some math or in some
other basic relationships that we expect
to find in the data right so here's a
very basic example the universe is
expanding and here's some very simple
data that early astronomers use to
establish that fact right so what we
have plotted here is the distance given
galaxy and how quickly it's receding and
so you can see that this relationship is
approximately linear and so we can a
code we can try to gather information
about this data by fitting a linear
model to this data and so by model based
reasoning all I mean is you know models
are basically just things that connect
hypotheses to data in a sort of
quantitative way and they allow us to
make quantitative inferences about data
and so this is already a very powerful
you know linear regression is already a
very powerful set of models what we'd
like to look at more complex models than
this one thing that is already
problematic with this model or at least
this fit that I've shown here is that
this estimate here the you know we have
we fit some line to this data and
there's some slope that we fit but we
have a point estimate right so one of
the failings of this plot is that
there's no notion of uncertainty here
right so for different scientists using
a different data set try to measure
Hubble's constant and got a slightly
different and
we would it be hard for us to say our
priori does that mean that the theory is
wrong does that mean that one measure
you know one of the experimentalist made
a mistake we don't really know how to
reason about whose measurement is better
or worse how do we reconcile different
sets of measurements right so if we want
to do better than that
then a natural thing to do is to
introduce probabilistic models so for
probabilistic models we're going to
incorporate a notion of uncertainty and
the way we're going to do that is by
taking unknown quantities and promoting
them to random variables and so things
will now be governed by probability
distributions and in the Bayesian
paradigm there's a number of useful
terms that we need to understand to
understand any talk that revolves around
Bayes and inference and one of them is a
prior distribution right so we had this
and the previous slide we were doing
this linear regression the parameter of
interest is Hubble's constant and we're
gonna first of all begin by putting a
prior distribution that represents our
prior beliefs for what kind of values of
Hubble's constant we think are
reasonable before we've actually
observed the data so here is plotted a
Gaussian distributions that would say
represent which values of eight zero we
think are plausible so that's the first
component of our probabilistic model is
putting a prior distribution over the
different latent variables in the model
and then the second major component of
any such probabilistic model will be a
likelihood so this is the thing that
connects the actual latent variables the
things that are unobserved and unknown
to the things that will be observed
which are the observed data right so in
this case the observed data are the
velocities at which these different
galaxies are receding and for each
velocity observed velocity we know also
know the distance to that galaxy then
the quantity is Hubble's constant and
the way we connect the observation to
the latent variable h0 is through this
normal likelihood this Gaussian
distribution that I have shown right
here right so the
this sort of likelihood is equivalent to
the following kind of data generative
process where the velocity at a for a
given input distance D is given by this
linear relationship plus a certain
amount of Gaussian noise that represents
anything you know measurement error or
some other some other quantity that
isn't directly observed them in our
model but that we need to account for it
to be able to reliably model stochastic
and complex data so those are the two
basic components that we're going to
have for any probabilistic model we have
latent variables they're governed by a
prior distribution there's a likelihood
the likelihood connects the latent
variables to the data and once we have
the likelihood and the prior
distribution then we can compute the
basic quantity and Bayesian inference
which is the posterior distribution so
the posterior distribution is the
distribution that we get that will tell
us what our new beliefs are about this
quantity 8 0 this Hubble's constant
after we've observed the data right so
before we've observed data our beliefs
were encoded in the prior and after we
observed the data our beliefs will be
encoded in this posterior distribution
and computing this quantity the
definition this quantity is given on
this equation here oops so in particular
this numerator is the product of the
prior distribution and the likelihood
for all of our observed data and then
this distribution at the this product of
distributions on the in the numerator is
not normalized and so we have to
normalize it to make this a proper
probability distribution and that's what
this term at the bottom is doing and so
now this is a distribution that tells us
everything that you know I according to
our model this is our now our new set of
beliefs about what Hoppus constants
should be and sin particular if we were
to plot if we were going back to our
original plot you know now do we not
only have a single notion of the best
fit what was the best slope but we also
have a whole distribution of which
slopes we think are appropriate for this
data set and so if a different
experimentalist came along with
different data then we could instead of
comparing two point estimates we could
compare two distributions and we could
answer basic questions like
are these two experiments consistent
with one another or are they not
consistent with one another so that's
the sort of basic paradigm of
model-based reasoning and Bayesian
inference so now let me say something
similar but in slightly different
language right so previously everything
I'm describing was in this sort of
probabilistic and statistical language
but we can sort of in a analogy we can
instead talk about modeling a simulation
right so here's this black box which is
some sort of simulator we give it some
input it has some parameters that govern
the simulator and then it spits out some
output in the simplest case this is
maybe a deterministic simulator but it
could also be non-deterministic and have
some stochasticity but the general point
is there's some black box we feed in
inputs and parameters and outcomes and
output so that's just a general sort of
picture of what a simulator looks like
so to make that more concrete here's an
example let's let's say we're interested
in climate modeling right so maybe that
we we know that over the next 10 years
the co2 emissions will have a certain
rate we also understand something about
the physics of climatology for example
maybe we know the albedo parameter so
the albedo is something that tells you
about how much different parts of the
earth's surface how much light they will
reflect back into outer space right so
this is an important parameter in terms
of modeling global temperatures so let's
say we have some simulator under the
hood maybe it's you know solving some
numerical partial differential equations
and if we give it our inputs and we give
it our parameters and that will come
some output which let's say in this case
is the mean global temperature so that's
an example of a particular simulator and
in that language of simulators the
Bayesian inference problem we can think
of as basically reversing some of the
arrows on this basic diagram that I have
right so we're going to still have the
inputs going to the simulator but
instead of the question that we're going
to now answer is for a given set of
inputs and for a given set of outputs
what values of simulator parameters
would have been consistent with what
we're observing so if we're given
input-output pairs what simulator
parameter
could have generated that data so that's
basically what Basin inference is doing
we start with a model or a simulator
that has this form where the arrows
point in the directions as shown where
we give the inputs and we give in the
simulator parameters and outcomes output
and it said we want to reverse some of
those arrows and we want to infer what
appropriate simulator parameters are
right so that's the sort of basic basic
setup and we can sort of view base an
inference as doing this sort of
parameter inference in the context of a
simulator so that's same general concept
but slightly different language okay so
that was a very brief overview of
probabilistic reasoning and model-based
reasoning and then the other component
of probablistic programming will then be
programming languages so I don't have to
say very much about this because
everyone in here is familiar with
programming programming languages we
know that they're very powerful you can
do many things with them you can write
down arbitrarily complex neural network
so you can do whatever you want with
them and they have a lot of powerful
abstractions and paradigms that people
use that you know allows you know groups
of a hundred people can say build a web
browser together and these people work
on this aspect and they work on that
aspect and it's all modular and it's
beautiful and we can build these complex
systems because of 50 years of
programming language research and
compiler research and all these other
tools right so we know this is a very
powerful set of tools and we'd like to
borrow some of these tools to do
probabilistic modeling and probabilistic
reasoning so without further ado what is
probabilistic programming so it's at the
basic high level just a combination of
these two paradigms right so interested
in probabilistic modeling Bayesian
inference and we're also interested in
using programming languages to encode
our probabilistic models right so there
are different ways you could view what
publicity programming is so if you're a
programming languages person you might
really come at it at a very formal sort
of view and you might say oh well I know
how programming languages want to work
but I want to add some primitives that
are stochastic and then you could sit
down and prove theorems the way the
programming languages people like to
prove theorems
you could also say maybe you're a
psychologist or you're a philosopher and
you're interested in
human cognition how do people reason and
you want to build up a model of human
reasoning so you could also use
probabilistic programming as a paradigm
for thinking about and analyzing sort of
a high-level paradigm for human
cognition but the sort of viewpoint that
I'll be taking throughout this talk is a
more pragmatic and practical one which
is that of probabilistic programming is
basically a set of tools that we can use
to specify probabilistic models and also
to do inference and probabilistic models
right we want to invert our simulator
and figure out which which values of the
latent variables of interest are
consistent with the data that we have
observed and we want to use the language
we want to use tools from programming
languages to help us do that in an
efficient and powerful way and so the
analogy here is that programming
languages are the probabilistic modeling
as something like tensorflow and pi
torches to neural networks right so
believe it or not you know 15 years ago
when people are doing neural network
research these tools didn't exist and
people who are using terrible things
like MATLAB and doing Auto
differentiation by hand and if you made
a mistake and your auto in your hand
implemented derivative good luck right
there's a reason people weren't building
resonates in 2005 and part of that you
know the sort of systems and programming
languages and tooling aspect of what's
happened in the last five years the fact
that people have built things like
tensorflow and pi torch has you know
done a lot to catalyze research in
neural networks right you can't really
if every person at the sit sit down and
write their own auto differentiation
library we would have gotten very far
right so but we want to do in this view
at least is build a similar set of
powerful tools that enable and catalyze
probabilistic modeling and can make it
more accessible to more people build
more complex models and make Bayesian
inference easier so that's the high
level goal and so I'm in particular
gonna for the rest of this talk use pyro
which is as my example programming
publicity programming system so this is
a system that we developed at uber over
the course of the last two years
and it's built on top of Python so it's
embedded in Python and you know uses
familiar Python syntax and also you know
because it's in Python you can use
everything else in the Python ecosystem
it's also built on top of pi torch so pi
torch is this neural network library
with auto differentiation so we get
everything that pi torch provides for us
which is neural networks GPU
acceleration and so on and so forth and
so I'm going to use this program this
ppl or probabilistic programming
language as my example programming
language but there are very very many
probabilistic programming languages out
there I've listed a short small number
of them here there in fact even more
than what I've listed probably the most
well-known and most often used one
amongst the ones I've listed here is
Stan but there are also many others as
you can see here so now let me get back
to this analogy of a simulator right so
let's write down and just very simple
Python what a climate simulator might
look like look like so we have some
Python function we give as an input this
albedo which members this like surface
reflectance property of different parts
of the planet and then there's some
other functions simulate temp which led
me B goes and syrup solves some partial
differential equations it then returns a
mean temperature and then we return
let's say the mean temperature plus a
little bit of noise the reason we might
add a little bit of noise is if we want
the simulator to actually produce
realistic data we know that even if we
have two sensors who are nearby and
should be observing the same temperature
they're gonna you know in reality every
sensor will have a bit of sensor noise
so if we wanted to produce extra
realistic data we would probably need to
add a bit of noise so this is a very
simple climate simulator that's mostly
deterministic apart from this noise and
it's just using standard programming
language tools to build this simulator
and so now we want to upgrade this
simulator to a probabilistic simulator
and in particular we're going to do so
using PI robes so this is what a very
basic pyro model looks like so again
this is all in Python so this is again
just the Python function and now one of
the first things that we've done is we
upgraded albedo from being some
deterministic thing to being a
stochastic thing so now this albedo
parameter is is governed by a
probability distribution in particular a
log normal distribution and so this is
going to be our prior distribution over
this parameter given our previous
knowledge about this parameter and we
can still call our you know we're we're
still able to call our arbitrary Python
code within our models so this is say
again that simulator that we previously
had this is maybe solving some partial
differential equation and then we
upgrade our observed our you know the
temperature to a random variable because
we're gonna give it a normal likelihood
a normal distribution governs this
observed temperature and there's some
let's say standard noise here of a
certain magnitude and so what we've done
here high level is we've upgraded this
thing to a pyro model pyro models or
stochastic functions pyro involves a
certain primitive a sample primitive
which we use to declare random variables
and when random variables are observed
like this observed temperature then we
use this special keyword to denote that
that particular random variable has been
observed so we know what the value of
that thing is and that's so this is an
example of a very simple para model and
the syntax another program PPS is
different but all of them have sort of a
similar flavor there are something like
a sample statement that we have here
that you use to incorporate random
variables into your model so in pyro
there are other basic language
primitives that we have another main one
is a parameter primitive so this thing
that we used to introduce parameters
that we're going to train say using some
gradient descent or some other
algorithms so this is basically just
declaring a parameter we give it a name
theta so something I should have
emphasized on the previous slide is also
when we introduced latent variables we
give them a name so this will help pyro
sort of reason about which random
variables which random variable and
helps to keep track of what's what so we
do the same thing with parameters so
here I've just introduced some parameter
that's some hundred dimensional vector
and I can then pyro will later know that
this is something that I might want to
optimize for example in some the context
of some infant's algorithm so we've
briefly discussed two of the main
primitives in pyro one of the sample
primitive that we used to introduce
random variables another one is this
parameter there are some other
primitives that we'll see later in the
talk but instead of sort of describing
all these things in the abstract let's
just sort of see them as we build up
more complex examples so in the for the
next bit what I'm going to do is talk a
little bit about time series modelling
so we're going to look at a very simple
time series model so here's some
economic data so as most of you know
there was a financial crisis in 2008 and
there was a lot of unemployment and this
is unemployment data from the US and you
can see that infinitely unemployment
like initial claims for unemployment
insurance and you can see that they're
steadily going down over the course of
the last nine years but there's also
some other very complex behavior and
we'd like to model this time series so
first we're going to transform the data
and put it on a log scale so that the
scales are more reasonable so the same
data but we've transformed the weekly
claim numbers with a logarithm and now
we're going to build a very simple model
which is basically just a regression
model so we're gonna have two parameters
which is a slope parameter and an
intercept parameter and then we're gonna
have a likelihood which is a log normal
distribution and we're going to build
this simple power model so we introduced
using this params tape meant H of these
two parameters this slope and this
intercept then we loop over the data
that we've fed into our model and we
observe each of our observed weekly
claim numbers we give each of these
unique names since it's a unique random
variable and then we use the appropriate
mean here using this function using this
formula we compute this mean appropriate
to that data point and then we observe
the data so this is a very basic model
which is just encoding a basic linear
regression model with a log-log linking
function and so then we can use pyro
or to fit these
parameters we have these two parameters
we're going to just to begin with fine
point estimates ultimately we're
interested in being more basing and
getting distributions that are
consistent with the data like proper
posterior distributions but to begin
with let's just find let's find point
estimates so if you do that and you fit
those two parameters you can then fit
see what kind of fit we got and you can
see that this is what we got so we have
this we have a linear regression here
and the slope is clearly negative
because the you know the employment
unemployment numbers are slowly
decreasing over time but there's a lot
of behavior here that we're failing to
model and particularly there's a lot of
seasonality effects that we have felt
included in our model so let's build a
slightly more complex model so before we
do that let's actually look at the data
a bit more closely so what I've done
here is taken two years from that time
series 2010 and 2011 and plotted all 52
weeks in the year and you can see that
there are similar similar seasonal
effects amongst the two time series
right so there's something you know a
lot of at the beginning of the year
there's a big spike in unemployment
claims and in the middle of the summer
and at the end of the year there's also
tends to be a spike so we want to
include this sort of behavior in our
model and so the way we're going to do
that is by introducing new random
variables so we're going to introduce in
particular a seasonality random variable
and we're gonna have it's going to be of
length 52 so for each week we're gonna
learn we're going to introduce a single
random variable that will encode whether
you typically see an upward or downward
fluctuation for that week of the year
and we use this plate primitive which
basically just says that we this is a
group of random variables and we have 52
of them and they're governed by the same
prior and so here's here's our we're
going to put some normal prior on this
random variable we also once we sample
this random variable we will also
subtract the mean of that random
variable so the sum of all these
seasonality effects is AK is equal to
zero because we want the other the mean
behavior the sort of gross behavior
should be governed by the slope and
by these seasonality numbers so this
seasonality effects are normalized to
have some zero and then again we take
what previously used to be peridot Prem
statements and we now upgrade them to
sample statements because we want these
to have proper prior distributions so
now we put some prior distribution on
these two quantities and then we again
have an observed statement where we
observe our data we've now added in the
seasonality to our mean function that
describes the expected mean of this data
and once we've written down this more
complex model we can then use some
inference algorithm within pyro to do
inference so a basic a very common and
powerful inference arguin that's used in
Bayesian inference is Hamiltonian Monte
Carlo so we can't go into the details of
how this works but it's a Markov chain
Monte Carlo algorithm that uses
gradients to approximately sample from
the posterior distribution and the
important thing to note here is because
we use a probabilistic programming
language and we wrote down our model
using this sort of high-level abstract
syntax and because pyro knows how to use
this inference algorithm Hamiltonian
Monte Carlo it just handles all the
details for us we don't have to go in
and implement a custom Hamiltonian Monte
Carlo implementation that's specific to
our model pyro just knows how to do that
and so it will do that for us and so we
just with a few lines of code can gather
approximate samples from our posterior
distribution and then we can go and see
what kind of fit we got and one of the
major you know one of the basic tasks
and machine learning is forecasting
right so now we've using a certain chunk
of data up to let's say 2018 we've found
the pram you know the samples from our
posterior samples of these late these
latent variables that we've introduced
that are consistent with the data that
we've seen and now we can go and
forecast into the future using those
same parameters and so this is actually
where some of those other pyro
primitives that I briefed that I had on
the slide but didn't talk about come
into play so you know usually when you
run when you have a deterministic
program you just run it forward once you
you get an output you're done but if you
have a stochastic program you might
actually need to run the program for it
many many times and record different you
know each run will be different because
of the stochasticity and you'll do
things like record all the different
stochastic sites maybe you want to take
one program and replace this the
randomness that you got in a particular
run using some other procedure so we
need sort of high level primitives that
allow us to mix and move around
randomness and using those primitives we
can do things like forecasting so here's
what we now get using this more complex
model so two things to notice here
because we've introduced seasonality
specifically into our model it now does
a pretty good job of recovering
seasonality right so it has these weekly
it picks up these complex weekly trends
that we see throughout the year and also
because we now use the proper inference
algorithm and drew samples from an
approximate posterior we also now get
uncertainty estimates right so it's not
just that we just get a single point
estimate like we got here we have a
single curve that is approximately
fitting the data but we have a actual
distribution over what our forecasting
what we you know what our predicted
unemployment numbers are going into the
future
so that so another thing that we can do
now that we've computed a posterior is
we can actually look at the posterior of
the individual parameters right so one
of the parameters in our model is this
slope and here is what this in plot it
in blue is the approximate posterior
distribution that was discovered using
this HMC algorithm and you can see that
you know the slope is this distribution
is pretty tightly concentrated around
minus 0.18 which is consistent with what
we know it's very obvious looking at the
data that the slope should be negative
the unemployment numbers are decreasing
over time and so if you were an
economist say and you wanted to ask a
question you could now report an
uncertainty right you could say the
slope is minus 0.18 plus or minus
something right now you have some notion
of how certain you are about your
conclusion
conclusions about this data right so you
have more than a point estimate
you also have uncertainty estimates
which is very useful if you want to say
compared with some other economists
different model or a different set of
data right okay so now I'm going to move
on to the next two topics so that was
sort of a brief introduction to time
series modelling and so now but before I
get to the next two topics I need to
briefly discuss a inference algorithm
that's often used in Bayesian and for
instant it has become especially popular
over the course of the last five years
because it in particular because it is
scalable it's an algorithm that in many
cases you can run on very large data
sets so this algorithm that I used
before Hamiltonian Monte Carlo has the
property that it doesn't scale very well
to large data sets this algorithm or
this class of algorithms does scale
often to large data sets so the general
idea is that again we want to compute
this posterior distribution and in
general this is a very hard task and so
one thing this class of algorithms which
goes under the name of varial inference
proceeds by turning this post you know
this problem of computing this posterior
into an optimization problem right so
this posterior is some distribution that
we would like to approximate and so what
we do is we introduce a family of
distributions called the variational
family of distributions and this is some
parametric family of distributions so
for example in the simplest case maybe
this is some family of normal
distributions with unknown mean and
unknown standard deviation and now
amongst this large family of
distributions we're gonna find the one
distribution amongst that family that is
closest to the posterior in some sense
right so you can imagine this gray blob
as being some manifold filled with a
bunch of distributions there's one
distribution which is our particular
posterior distribution of interest and
we are now going to try to find the
variational distribution which is
closest to that target distribution
using some notion of distance and the
notion of distance that we use in burial
in variances some statistical divergence
that you can compute between two
divergences between two distributions
called the KL divergence so this is just
a general setup
we've done is turned a Bayesian
inference problem into an optimization
problem so we're going to find the best
fitting distribution to our posterior in
this green family of distributions and
so turning back to code this is sort of
what this might look like so at the top
we have a model and our model again has
some latent variable which we're going
to call theta it also has some observed
data Y and now we're going to introduce
a second probabilistic program which you
could call different things you could
call it a variational program you could
call it an inference network you could
call it many things we in pyro tend to
call this a guide because guide is a
nice short word and so this is a new
program that we're introducing and this
program also critically has the same
random variable as our model right so we
had at the top we have on the second
line a random variable called theta and
here we're also going to have a random
variable called data so this guide
program encodes a distribution over
theta space so it's precisely this
family of guides is a family of
variational distributions and this guide
is going to have some parameters in
particular we might introduce a neural
network into our into our guide so this
neural network will let's say consume
the data Y that we've seen it will then
produce some parameters we'll use those
parameters to define some distribution
and that distribution will then be our
variational distribution so this is some
general procedure for creating a family
of distributions and once we've found
once we've decided such a family we can
use variational inference to find the
best member of this family that's
closest to our target posterior
distribution and the way you do that
without getting into details is using a
objective function called the elbow so
this is you know sort of roughly
analogous to the sort of objective
functions you see in neural network
training where you have some loss that
you want to minimize here this is some
quantity that we want to maximize and
the main thing to notice about this
objective is that it's an expectation
with respect to this variational
distribution that we've defined over
theta space and so in general we cannot
compute this quantity because there
might be arbitrarily complex
nonlinearities inside of this integrand
and so we don't know how to compute this
but we can often compute a Monte Carlo
estimate of it and so the sort of
general procedure for doing this sort of
rational inference procedure is once
we've defined our guide program we can
run it forward and that'll produce
samples from this guide program and then
we can go back and run the model program
but replace the random sites in that
model in particular in this case theta
with the random value that we got from
the guide program and that will allow us
to then compute these quantities that we
have here this log density of the model
and the log density of the spatial
distribution now we can compute a Monte
Carlo estimate of this objective and
then we can use stochastic gradient
methods to try to optimize this
objective function and so we it's an
iterative method just like training a
neural network we take a bunch of
gradient steps at the end it's hopefully
converged and now we've found a guide
program that approximately encodes our
approximate posterior distribution so
this is a way of doing Bayesian
inference that is very useful and that
what will be that will be sort of
leaning on throughout the rest of the
talk so now let me now get to the next
topic so I want it the next topic is
something called Bayesian optimal
experimental design which is thing
that's been around for a few decades
so I'm gonna in particular be talking a
little bit about a paper that we wrote
earlier this year but mostly I'm going
to be just talking about how this sort
of experimental design setup works what
it what's it about and why probabilistic
programming languages are a very natural
fit for this sort of procedure
okay so let's again go over the terms
that we use when we do Bayesian data
analysis so again we have some
observations some observed data which
I'm going to refer to as Y we have some
unknown parameter which is going to be a
laden random
variable denoted by theta theta is
governed by some prior distribution P of
theta we have some likelihood P of Y
given theta that connects our random
variable theta to our observed data Y
and we again have this posterior
distribution which is the sort of main
quantity of interest when you're doing
Bayesian inference so let's say you're
doing an experiment in your lab so how
do you and you're using Bayes and data
analysis then how does the procedure
look so first you do your experiment you
observe some new data Y you compute the
posterior and now you update your
beliefs like the posterior now
represents everything you know about the
parameter theta including what you just
learned from observing data point Y and
so in an if you do experiments
iteratively then the posterior will
become your new prior and you just keep
going right so you if we do this at you
know after every new data point that
we've collected we can update our
posterior by computing the posterior and
that now contains everything that we
need to know about our data given the
model that we've specified and as we
learn as we collect more and more data
what we'll see is that our posterior
distribution will get more and more
narrow right we have more and more
certainty about which values of theta
are consistent with our data that's our
general expectation is to see the width
of this distribution get narrower and
narrower as we've learned more and more
things about the data that we observe in
the world so that's the third the
general procedure of doing Bayesian data
analysis and in this sort of iterative
experimental procedure and so the thing
that we'd like to focus on is part one
right so in particular which experiment
should we do so in general when you run
an experiment you have a certain amount
of control over what experiment you run
and certain experiments might teach you
more than other experiments and you want
to choose your experiment wisely given
the experimental resources are often
expensive so to introduce some
terminology the term design refers to
the part of an experiment that you the
experimentalist control so any some
examples are listed here right so if
you're let's say you're conducting a
survey
you chose you choose which questions
appear on the survey that's something
you don't choose how the respondents
respond to the questions that's out of
your control but you choose what the
questions are right so you need to
choose those questions wisely if you
want to learn a given thing about your
survey participants or let's say you're
giving patients a drug you don't know if
that drug you can't control if that drug
will cure that patient of a given
disease but you can at least control
what dosage you give them right or which
drug to begin with so these are the sort
of things that define a experimental
design it's something that is non-random
and it's in the control of the
experimentalist
and then the way it enters our
probabilistic model is it enters through
the likelihood so it controls how are in
this abstract sense abstract setting the
experimental design will entrant or
likelihood somehow and we now want to
ask the question how should we if we
have a given set of designs available to
us like let's say we could give the
patient either a dosage of 10
milliliters or 20 milliliters we want to
ask which design should we choose that
will give a that you know which design
do we expect to teach us the most about
our problem of interest and a natural
way to formulate that question in the
context of Bayesian inference is to use
an information theoretic criterion so as
we collect more and more data the
posterior gets narrower and narrower and
narrower and in particular the entropy
of that distribution is increasing
decreasing so if we've learned a lot the
entropy of our posterior should have
decreased a lot so what we want to do at
high level is choose experiments the ink
that decrease our posterior entropy as
rapidly as possible that's sort of the
high level goal that will help us this
choose clever experiments so to
formalize that notion we introduced
something called the information gain so
for a given observation Y it is just the
difference in entropy z' between the
prior distribution and the posterior
distribution right so this can be some
positive quantity that measures in bits
how much we've learned from doing a
given experiment
and the problem with this objective is
that before we've done an experiment we
don't know what Y is right why is this
random outcome that we'll only know once
we've done the experiments so we don't
know what this is before we've done the
experiment so this on its own is not
useful as an objective function but to
make it useful what we can do is
basically integrate out Y so given our
current model P of Y we basically sample
all possible values of Y that we could
imagine our experiment producing and
average out the expected information
gained right so basically what you can
imagine in your head you're simulating
possible experimental outcomes for each
different simulated experimental outcome
there's a different posterior
distribution that you would get and for
each such simulated outcome you've
learned a certain amount because the
entropy is decreased by a certain amount
and you want to choose experience
designs that on average decrease the
entropy the most so we want experimental
designs D that maximize this expected
information gain so this is sort of a
natural information theoretic criteria
and for choosing clever experiments okay
so then the optimal design will just be
the design amongst the set of designs
available to us that maximizes this
quantity that's what we want to do to
guide our experiments the problem with
this is that this is in general a very
difficult quantity to compute or
estimate because even even forgetting
about this outer expectation this
integrand itself involves the posterior
distribution and in general computing
the posterior distribution for a model
is a very difficult problem right so
this this process abating this quantity
involves this very difficult problem in
an inner loop so this is a very
expensive thing to compute in general
and it's very difficult to compute and
so to make any progress we inevitably
have to do some sort of approximations
to try to get a handle on this difficult
to compute quantity so one thing that we
can do is use variational methods right
so without getting into the details
there are different variation abounds
that you can write down for this
quantity so I've written down one of the
so it turns out that for any
distribution Q over Y that if I compute
this quantity here this expectation
where the expectations with respect to
our model and we also include the spur a
tional distribution this is always an
upper bound to this quantity so if we
try to find the Q that minimizes this
quantity on the right that will give us
some sort of approximation of the
quantity on the left in general there's
going to be some amount of bias because
our discipline said it where you know
we're going to only look at some family
limited family of variational
distributions but this still gives us a
procedure for trying to approximate the
quantity on the left and it will turn
this difficult intractable problem with
this thing that's more tractable and we
can use all the advances that people
have made in variational inference in
the last five years to try to estimate
this quantity in a reliable and
reasonably fast way so to make all that
abstract stuff a little bit more
concrete let's work out a simple example
so imagine you're psychologists and you
are interested in people's working
memory capacity how good they are
memorizing things and in particular
we're going to ask them to memorize D
digit numbers so we can ask you to
remember that two digit number or 50
digit number we want to see do you do
that correctly or not correctly and we
want to learn we want to choose good
designs so and in our case design will
be the length of that the number of
digits in that number so I'm going to
it's up to me if I choose to ask you to
memorize a seven digit number or a 12
digit number and we want to choose D
cleverly so that we learn as much from
you as possible as quickly as possible
and that's what this experimental design
framework allows us to do okay so in
order to do that I've now introduced the
sort of experimental setup I'm going to
ask people to memorize a number and then
they tell me say 10 seconds later what
that number was and they either get it
right or wrong right so now our model
our outcome is whether or not they
guessed the number correctly or
incorrectly so let's say y equals 1
encodes correctly and y equals 0 encodes
they didn't remember it correctly and so
that this we need to define a
probability distribution
this outcomes and they're only two
possible outcomes so we just use a
Bernoulli distribution and we're our
modeling assumption is going to be that
the very simple modeling assumption that
longer numbers are harder to remember
which is a very natural assumption and
so this is going to be our probability
distribution we basically our parameter
of interest is theta and we're going to
multiply that by the length of the digit
and there's a minus sign here so what
this because we use this sigmoid linking
function this probability will go to
zero for a sufficiently large D and
it'll you know it'll be larger and
larger for smaller D so this just
encodes the idea that hard longer longer
numbers are harder to remember and this
theta parameter basically controls the
slope of how quickly numbers get hard to
remember and so we've now specified the
likelihood of our model the parameter
again is this slope parameter theta and
then we just put some prior distribution
on our parameter which maybe we just
guessed or maybe this is based on
previous experiments that we've done and
this now fully specify is our bayesian
model and so we can now ask ourselves
given this bayesian model what design
should I choose to learn as much as
possible in doing the next experiment so
the first thing that we do is just
define our model so again this is a very
simple model we have a latent variable
theta governed by a normal distribution
we compute this using the sigmoid
linking function we just compute for a
given design which is again the number
of the length the number of digits in
our number we compute this quantity
which defines our Bernoulli probability
between 0 and 1 and then we just observe
we sample our outcome Y given that
probability so if we were to run this
model forward many many times we would
just you know we would generate a
distribution of Thetas and Y's
consistent with that model so that's
what this this model is sort of this is
our data generative process it generates
data according to this simple modeling
assumption and now again we want to
compute this Eid expected information
quantity and we're going to use these
variational methods to do so and so we
have to introduce a variation of this
tribution and again remember if we look
at this formula the virtual distribution
that we need to define is defined over
outcome space so we need we need a
distribution on Y space and remember Y
is a zero or one binary variable so we
just are gonna have to define some
binary just some Bernoulli distribution
and then we're going to find the
Bernoulli distribution which minimizes
this quantity on the right and that will
give us a biased estimate of this
expected information gain so once we so
the next after we would define our model
we now need to write down this guide
program and so this guide program will
need to have a stochastic site
corresponding to this outcome Y since
it's defining a very distribution over
Y's and it's just going to be some
Bernoulli distribution and it has some
parameters which govern the Bernoulli
distributed this Bernoulli distribution
and then the point is that in pyro we
have some under the hood it knows about
this objective function and it can go
and fit using stochastic gradient
methods the amongst this large family of
guides it will fit which one is best and
which one best approximates this
quantity and so once we've learned an
appropriate guided program using
gradient descent we can then use Monte
Carlo to get an estimate of this
quantity and that will now give us a
biased estimate but nevertheless usable
estimate of this expected information
gain and so if we do that we get
something like this right so here I've
plotted three different methods the one
that we described previously is was
obtained with this blue line so there's
using a marginal variational
distribution and so the point is that
this is now our estimate of the expected
information gain on the vertical axis we
have the expected information gain on
the horizontal axis we have our design D
which is again the length of the you
know the number of digits in our number
and you get a very natural behavior
where given our prior it turns out that
this expected information gain peaks at
nine digits right so this makes a lot of
sense right if I ask someone to memorize
a one digit
given our prior they are with very very
high probability gonna guess that number
correctly so if I if you come into my
lab and I ask you to memorize a one
digit number and you get it correctly
I really haven't learned anything
because I already knew with very high
probability that you were capable of
doing that similarly if I asked you to
memorize a hundred digit number you will
almost certainly get it wrong and so I
really didn't learn anything right and
there's some simples there's some sweet
spot in the middle where we were sort of
unsure of the outcome and that's where
it turns out we'll learn the most right
so this is a very simple experiment in a
very simple model and you know for this
very simple model we didn't need to
maybe go through this very complex
procedure to to come up with the idea
that the very intuitive idea that we
should be using digits of moderate
length if we want to learn something but
you can imagine in a much more complex
scenario where the model is much more
complex it's very hard to reason about
what a good design would be so this yeah
right so this is we simulate data
according to our model and we then
compute this expectation right so this
this procedure only really makes sense
or works if our model is actually a good
model of the data generative process but
like if if our model is reasonably good
then we can sort of simulate what would
happen for a given experiment and then
we can estimate what would we learn for
different you know how much would we
learn from different experiments and we
can choose the experiment that we expect
on average to provide us with the most
information and in this case we get this
kind of very intuitive answer that we
should be asking the person to memorize
digits of moderate length but in a more
complex scenario it might be very hard
to come up with a reasonable design and
so this gives us a sort of quantitative
methodology to do that so you can
imagine that for very complex
experiments where you know you have a
fixed budget of $100,000 you want to get
as much information as you can with a
hundred thousand dollars so you should
choose each of your experiments as
wisely as possible in a particular the
power of this experimental design
procedure becomes most powerful when we
do it in a iterative setting right
so let's say someone comes into my lab I
do a short experiment them I then
compute a new posterior and then I
compute the expected I find the design
that given all the information I have up
to that point is the best experiment to
do next then I do that right so I do it
adaptively after each person comes into
my lab I find the best design
appropriate to what I've learned what
I've seen so far so adaptively as my
experiment continues I continually
refine which experiment I'm doing so I
can learn as much as I can given the
finite amount of experimental resources
I have so you can well imagine that you
know if I'm let's say a biologist I
don't maybe care about this math but I
do care about being able to do this sort
of procedure in a probabilistic
programming language is very natural for
this because the user doesn't need to
necessarily understand under the hood
how this quantity is being estimated
they just need to be able to write down
their probabilistic model in some
high-level syntax and then the
probabilistic programming language can
sort of worry about the details so you
know these are still sort of early days
for these sorts of tools but you can
imagine as these tools get more and more
mature you could real scientists could
use this sort of procedure to help
optimize their experiments and use their
experimental resources more efficiently
ok so now I'm going to finally move on
to the third and final topic I mean yeah
I guess the question you're saying is
what I've described is sort of a greedy
procedure where I decide one step at a
time what the next para mak'st
experiment is in general I could expect
to do better if I design multiple
experiments ahead at one time but doing
that it you know makes a problem that
much harder and probably so this is sort
of a greedy compromise where you just
greedily one step at a time choose the
next best experiment
but this is already so hard to estimate
that you know it's not clear if the
Betty who would actually gain a lot of
benefits from doing the harder thing
because it would be just that much
harder to estimate in any case but
certainly in some cases it might be
worth trying to do that ok so I'm
finally going to move on to the last
topic so this is a paper that was
written sent by these people I had
absolutely nothing to do with this
project but I just thought it was cool
and in particular they used pyro under
the hoods this is sort of like an early
example of people in science trying to
use these sorts of tools to do something
that empowers their data analysis
so these are some astrophysicists they
wrote this paper and I haven't seen
their code but I'm going to sort of
pretend I have and reconstruct maybe a
little bit of what they did given their
description in the paper okay so what's
the phenomenon they're interested in
they're interested in something called
gravitational lensing right so you know
Einstein's theory of general relativity
tells us that space-time is curved and
light rays are curved
you know they move on geodesics in this
complex geometry and so in particular if
I observe a light ray from some distant
galaxy it along the way encountered a
bunch of mass perhaps it encountered
dark matter or black holes or something
and it's paths will be deflected by the
action of gravity and so you and you see
these sort of weird effects where here
you see what look like what look like
for different light sources but these
are actually all one light source it's
just that some galaxies or some other
source of mass between us and the source
is deflecting those laser rays of light
and so you get this phenomenon called
gravitational lensing okay so this is
sort of what this set up looks like high
level we have some say source galaxy and
we have some lensing galaxy and the
light rays that we observe here
basically get distorted by the
gravitational action of this lens galaxy
and this is cool because it allows us to
infer something about the mass
distribution of this galaxy without
actually directly observing the mass
which is particularly useful because
things like door
matter by deafness or by definition
don't emit light right so this is a very
powerful tool for learning about the
early universe and you know universe and
galaxies dynamics and so on and so forth
so the general setup is that we want to
learn about this lensing galaxy by
observing this sort of deflected right
the deflected laser rays of light here
on planet earth so these people wrote
down this model and in particular this
model has two components one is that
they have a source model so this is a
model of the different galaxies that are
that are emitting light and they have a
lensing model which in particular is a
model of how how a given lensing galaxy
distorts light and then they have some
observation likelihood this you know
they're say distortions caused by you
know you know telescopes always see
noisy data so you have to take that into
account but the point is that they have
this sort of pipeline with different
modeling components that sort of
describes end to end this data or
generative process and so let's look a
little bit into some of the components
so without actually getting into the
physics it turns out this displacement
angle alpha which is shown here which is
basically saying how much it given
photon is deflected is governed by an
equation that looks like this
where in particular it depends on some
integral over some surface mass density
over the lens right so different lenses
with different mass structures will
produce different kinds of bending of
light and so we want to infer some of
the parameters of this mass density so
we can learn about for example dark
matter and so now I was a very very
brief described description of the lens
model that's just pure physics we know
how that works and so you can just sort
of encode that in math and in PI torch
and in particular because you know if
you do things correctly this is
differentiable and so we can then use
auto differentiation frameworks like PI
towards to you know differentiate
through this numerical integration say
in a way that allows us to use powerful
inference algorithms that use
under the hood so now let's briefly talk
about the source model so the idea of
the source model at high level is that
we let's say we observe images of
galaxies and they're 64 by 64 pixels so
that's a large number of pixels but we
know that sort of the actual effective
dimension of the manifold of galaxy
images is going to be much lower
dimensional right with the there's some
much smaller manifold of images that is
then projected onto 64 by 64 dimensions
so to like encode all the important
information about a given galaxy
we don't need 64 by 64 bits of
information we often need a much smaller
number of bits of information so the
sort of picture is that we have some
high Dement you know we have some high
dimensional space of images but the
actual images all live on some lower
dimensional manifold in that space as
pictured here on this small low
dimensional representation so this is
part of like a general framework where
we build a latent variable model so in
particular the idea of high levels that
we have some low dimensional latent
codes II covered by some prior
distribution and then we have some
likelihood distribution which for a
given low dimensional Zee produces a
high dimensional image which we are
denoting as X and this is just basically
a way of building a complex distribution
in X space using a lower dimensional Z
so this is a way to basically construct
a model in image space using a lower
dimensional representation and so this
sort of general framework goes under the
name of variational auto-encoders as and
has seen a lot of interest in the last 5
years and I believe there is or there
will be a talk about this tomorrow as
well about some of the related machine
learning here and so let's just briefly
see how this sort of pyrrha model would
look like so in particular the model
looks as false so first of all we sample
this low dimensional z we've also
previously defined some neural network
called a decoder Network and we take
this Z and we shove it through our
complex neural network which is in
general some complex nonlinear thing and
that will now produce parameters of much
higher dementia
that we can then feed into some other
likelihood distribution in this case
normal distribution that is now an image
space so we basically took something
noisy and low dimensions passed it
through a neural network to produce some
image in a much higher dimensional space
that now encodes in our case galaxy
images and so then we since we're using
very old friends we also have to define
one of these guide distributions and so
again it needs to encode a distribution
in z space so we're going to again have
a z space distribution which again is
going to be a normal distribution but
and in this case we're also going to
make use of a neural network this neural
network is called an encoder so for a
given observed image which is again high
dimensional the encoder Network will
basically project that high dimensional
image onto something low dimensional and
using and that will then define a low
dimensional distribution in z space that
becomes our variational distribution
here so this those a bit fast but this
is basically just to show that using
neural networks in the loop and using
this sort of basic variational inference
paradigm we can define complex models of
images and in this case images of
galaxies yeah
yeah so but this mean and Sigma is an
image space which is high dimensional so
this will be a high dimensional Mew and
a high dimensional Sigma and this
returns a we here we're defining a
distribution in z space so this will be
a low dimensional Mew and a low
dimensional Sigma so they're basically
doing the same thing but going in
opposite directions ones going from
latent space to image space when is
going from image space to latent space
which is why it's called an autoencoder
because you're encoding in one direction
and then back again
okay so here's sort of a graphical
illustration of what this encoding and
decoding looks like right we start with
some high dimensional image we encode it
into some lower dimensional
representation and then a decoder is
able to then decode it into a high
dimensional image sorry
no so the decoder encodes our posterior
for a given X it get decided no so yeah
the encoder is a basically is the
posterior distribution or it encodes the
parameters of the posterior distribution
in this case okay so we don't have much
time left so let me just show you some
pretty images in their paper so they did
this they had a big data set of galaxy
images they learn such a model using
that data set and you can see that
they're able to at the top we have sort
of the observed data and then we here we
have the reconstructed data where they
basically take the image shove it
through the encoder and back through the
decoder and you can see that you get a
faithful representation so this low
dimensional representation actually is
able to capture the roll band details of
these images and then once they've done
that they can put everything together
into this pipeline and they can do
things like for a given you know they
generate for example fake data and so
they the in their fake data they
generate some source and let's say it
looks like this image
and then it produces an observation
which will have this weird halo effect
because of lensing and then they're able
to say for example so here's the true
source that generated this true
observation but then they can say use
for example maximum likelihood
estimation to find the most likely value
of the source distribution that produced
the observed image and you can see that
they got pretty good fit and there's a
little bit of bias here they didn't
recover the source distribution source
galaxy exactly but it does a pretty good
job and then there because everything's
differentiable because everything's
built on top of an auto differentiable
framework they can also use fancier
infants algorithms like Hamiltonian
Monte Carlo
so they for example do an experiment
where they have a lensing galaxy with a
given mass distribution and then they
produce an observation and then they ask
what values of the parameters that
define that lensing the mass
distribution of that length lensing
galaxy are consistent with the data and
then they get because they're doing
Basin inference there are posterior
distributions so you see here as they
got some distributions that are roughly
so here this box corresponds to the
value that was actually used in the
experiment the test data right so this
was the true value used in the lensing
distribution and they were able to
recover you know not perfectly but
pretty well a posterior estimate of the
value of that lensing distribution right
so they never they never observed the
lensing galaxy directly the only observe
images at planet Earth but they can
still infer in a probabilistic way what
what they observe what the lensing
galaxy mass distribution must have
looked like to be consistent with the
data and so that then learns that would
then in principle allow them to learn
things about dark matter and so on and
so forth so that's the end of that third
and final example so let me just very
quickly conclude what I've tried to
convince you of is that probablistic
programming languages are a powerful
tool for specifying complex models it
still sort of early days in the sense
that you know things like pi torch and
tensor flow are very mature frameworks
with thousands of users
and probablistic programming languages
there's still a bit of work that needs
to be done to bring them to that level
of maturity but we're making progress
and we might expect them to play or at
least I expect that in the future we'll
see that these sorts of tools become
used more widely in scientific analysis
and data analysis in general because
it's you know it's a powerful set of
tools of course you can also imagine
using this in industry or in other
applications but here I've really just
focused on the scientific set of
applications and then third and finally
something I didn't I wasn't really able
to discuss but which maybe sort of
emerged a bit from the talk is that this
is an interesting area of research
because it kind of combines multiple
threads so in particular it combines
aspects of programming languages it
combines aspects of Bayesian modeling
and basing inference and approximate
inference algorithms but it also you
know it's become increasingly the case
in recent years the approximate Bayesian
inference algorithms can make use of
neural networks so for example these
decoder and encoder networks that we
used made use of deep learning so all
these three different areas sort of have
a nice and interesting synthesis in this
area so if you're interested in this in
these things I said I recommend that you
consider instead of just doing deep
learning by itself which is boring why
don't you instead do deep learning in
the context of Bayesian modeling or do
deep learning in the context of
probablistic programming because that is
also an interesting set of problems that
is worth your time and attention so
thank you
[Applause]