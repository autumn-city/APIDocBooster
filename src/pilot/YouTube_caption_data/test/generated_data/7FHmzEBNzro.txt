trace paper is mlp mixer
an all mlp architecture for vision
in this proposed model no cnns or
attention modules are used the idea is
similar to the vision transformer model
but
it uses only multi-layer perceptrons
also known as mlps two types of mlp
layers are mainly used
the first mlp layer is in charge of
mixing per location features
and another mlp layer is in charge of
mixing spatial information
because the model only uses multi-layer
perceptions
the computation complexity becomes
linear
the model achieves competitive scores on
image classification benchmarks when
compared to other state-of-the-art
models
now this figure shows the architecture
of the mlp mixer
the model consists of per patch linear
embedding
mixer layers and a classification head
first the pear patch fully connected
layer takes non-overlapping image
patches as an input
like in the vision transformer and
projects each patch to a desired hidden
dimension
once all patches are linearly projected
with the same projection matrix
they are now fed into n layers of mixer
layer
this mixer layer is shown in the figure
at the top
each mixer layer consists of two mlp
blocks let's denote this matrix
after the layer norm as an input feature
called
x each row in different color
are the embedded patch tokens from the
per patch fully connected layer
this input is now then fed into the
first mlp block which
is in charge of mixing the tokens in
column direction
this block acts on the columns of x by
transposing the matrix
the second mlp block is the channel
mixing
mlp block the transpose matrix x
is now transposed back to its original
shape
and the matrix is fed into the layer
norm and another mlp module to mix the
features in channel dimension
note that each mlp block consists of two
fully connected layers with
jello non-linearity in between
now the final output tokens from the
mixer layers are
the applied and global average pooling
as shown in the figure
the pulled features is then applied a
final
fully connected layer to output a class
score
the authors evaluate the performance of
mlp mixer models
pre-trained with medium to large scale
data sets
on a range of small and mid-sized
downstream classification tasks
this table shows transfer performance
inference throughput and training cost
of different models
the models marked in blue are
attention-based models
ones marked in yellow are
convolution-based models
and the ones marked in pink are the
mlp-based mixer models
the mixer model has comparable transfer
accuracy to
other state-of-the-art models with
similar cost
looking at the mixer model pre-trained
on imagenet 21k
the model achieves 84.15 top one
accuracy on
imagenet dataset also on cleaned up
real labels the gap isn't that big
the top one accuracy of the mixer l
model achieves 87.86 percent
and the vitl model achieves 88.62
the graph on the left shows image net
accuracy and the training cost of
various
state-of-the-art models the models are
pre-trained on
imagenet 21k or jft or web
image text pairs with noise in it the
mixer models
marked in pink achieve they achieve
competitive results compared with other
models like
resnets vits and other hybrid models
on the right it shows the effective
growing
training data set size on imagenet top 1
accuracy
the authors state that the mixer model
pre-trained on a small subset of jft
strongly overfits but as the dataset
size increases the performance of the
mixer models grow faster than
models like bit
and also as the training dataset
size grows the mixer model and solid
line
exceeds the performance of other models
like bit
and vit
link to the paper and some useful
resources will be provided in the
description
that's all for today and i'll see you
next time with a new paper