thank you all for coming to the tutorial
if you noticed in the SyFy program there
are three tutorials doing Bay's stuff
this is I think unprecedented in the
SyFy tutorial program list what happened
was the three lead instructors that is
Alan Downey for his tutorial myself and
Hugo for this tutorial and Raveena and
Colin for the next tutorial on Bayesian
modeling we all decided to informally
coordinate together to put this informal
A's track thing and I know that there
are a few people who have actually
registered for all three of them so
props to you guys the handful of you
anyone want to take an estimate on how
what fraction of this crowd did that
twenty percent which would mean about
let's see sixteen people wish fifteen
people wish I think they're about sixty
to eighty people registered in this room
yeah so okay yeah I actually got the
actual number from Jill it's fifteen of
you so it's it's it's a good close
estimate there alright so for those for
the fifteen of you who were in Ellen's
tutorial some of this material will be
familiar to you some will be and for the
rest of you will go through enough
background to accomplish today's goals
which is to show you how you can write
arbitrary statistical models and perform
use them to perform inference okay
that's the end goal of today's tutorial
and there will be a tool of choice which
is PI MC 3 will also show some things
with numpy hopefully that will give you
enough grounding in probability in Bayes
rule to then be able to talk about how
our data were generated from a
statistical standpoint and build those
models and use them all right use them
productively okay some administrative
matters before we move on for those of
you who are still not set up I know that
there's a high probability that you are
set up set up being defined as you
either have binder working or you have
everything cloned locally and working on
your laptop or on a remote server that
you control
which is what I'm doing by the way so
yes I know what the feeling is like so
if you are set up and you're using
binder remember every 15 minutes on the
clock so the make sure you execute
something your notebooks because binder
sessions have a 20 minute timeout okay
if you are not already set up go to the
repeat page go to the readme on the
github repository look for the launch
binder button and hit it because your
gonna not want to waste time with DevOps
at this point okay cool
let's see we also have ravine ravine
Kumar who is one of the instructors for
the next base tutorial he is helping out
today and he'll be back in a few minutes
we can guess the time that it would like
we can take a guess at a time that he'll
be back you'll notice I'm using a lot of
these guess the time estimate that sort
of terminology it's very important that
we're we've got a both both a
theoretical view and a practical view on
that so hopefully today's tutorial will
will clarify that okay before we go on
do people have any questions any issues
that they want to clarify any things
that are you know blocking them from
doing today's tutorial okay if not if
not we're gonna deep we're gonna do the
following it we're gonna follow the
agenda that's on the whiteboard first
off we're gonna do a recap on
probability use some hands-on exercises
to make sure that you've got a grasp on
it and then we'll recap what Bayes rule
is so for the 15 of you who are in
Ellen's tutorial this morning this
should be familiar material well we
won't do a full rigorous proof of Bayes
rule and everything will we'll just make
sure that everybody has a grasp on that
and it builds on top of the rules of
conditional probability and joint
probability okay so once we're done with
that then we'll move on to what we would
call the core active core activities of
statistical inference which is
estimation estimation is the core of all
statistical inference because we're in a
Bayes tutorial I'm gonna go on limb and
saying calculating p-values is not right
calculating p-values is not the core
activities not even the point of
statistical inference so let's get that
out of our heads and then the final
thing is we're going to show how the
core activity of core activity of
statistical inference Bayesian
statistical inference which is Bayesian
estimation how that relates to things
that we might have now learnt already
learned in our undergrad statistics or
grad statistics classes which is you
know different forms of regression and
the legs and we'll do it we'll do a
short teaser on that it's not the main
point because the main thing is really
understanding estimation and how we
build models that help us perform that
ok so with that I'd like to invite
everybody to open up your notebooks fire
up Jupiter and open up notebook 1a and
you'll notice that there are always they
always come in pairs so they there's a
student version and an instructor
version right and what I've decided
unlike my previous workshops I'm gonna
live code with you not from memory
thankfully I have a second computer here
that tells me what the right answers are
you all by the way if you ever feel
stuck on anything the instructor
versions of the notebooks are there for
you ok and this is the first time I'm
going to say it also the last I'm going
to say it so if you're ever stuck just
open the instructor notebook don't
hesitate to copy and paste the answers
over the point is understanding doing
stuff helps with understanding but if
you get stuck doing stuff it can lead to
more frustration for learning than is
productive alright so at the end of the
day make sure you just have a good
conceptual view and that's good enough
all right let's run the first cell and
talk about what probability is I had a
few pop quiz questions just now right
what's the probability that someone
would have someone in this room if you
asked would have a satisfying lunch
today
someone said 75% of this room that's
another question what is the probability
that someone in this room has taken all
three tutorials right
all three Bayesian tutorials and that's
another question relating to probability
but I'd like to start by first asking
the question a little bit more
meta-level
what is probability do you have a
volunteer and I know no one's gonna
answer so I would like you to do is talk
with your partner for about 32 min 30
seconds to a minute not 30 minutes 30
seconds to a minute introduce yourself
and network a little bit and ask what is
probability okay so hopefully you all
know each other a little better those of
you who've been in the network analysis
tutorials that I've done know this trick
this is the networking trick lets you
build your network while you're in a
tutorial room so do we have volunteers
to share maybe a stab at the at the
question what would you define
probability as we know some properties
of probability right someone said it
must be bound from 0 to 1 what what else
what other things about probability do
you know volunteers back there a
symbolic expression of uncertainty would
you like to unpack that for us a little
bit no all right so then I'm supposed to
do a stab at that right ok let me ponder
that do we have another something ok
yeah yeah ok cool
degree of belief yep yep okay mark
yeah yep okay right yeah okay anything
else anybody else has a definition you'd
like to contribute part of me ah okay
yeah all right all right cool right so
what I'm hearing are essentially the
classical and the Bayesian I refuse to
say frequentist the classical and the
Bayesian ways of thinking about
probability Hugo who made the first part
of this tutorial the material for this
for the first part of this tutorial
found a really great quote and it's
inside your notebooks and you'll see it
right by Dan analysis by Sevilla and
skilling and there were essentially
historical historically there was a
shift in what how probability was viewed
there was the perhaps that what we might
call the first version that was formally
recorded in European scientific history
which would be a degree of belief
assigned to an outcome right and then
there was then a shift because of you
know views worldviews foundational
worldviews shifted and so people thought
of probability as more of the relative
frequency with which things occurred
which then gave rise to the name that I
refused to mention and because these
frequencies can be measured
they then seemed to form an objective
view of reality right I on the other
hand don't subscribe to this view right
I see probability defined so there are
formal there are formal definitions of
probability you
getting into these very technical
mathematical terms that involves spaces
and measure theory and the and the likes
the way that I prefer to for for
day-to-day use of you know Bayesian
methods and probability I tend to think
of probability as just being money
assigned on a number line now if I gave
you $100
how would you assign that hundred
dollars two points on the number line if
you were doing discrete things or how
would you draw a curve that would say
assign how you would distribute money to
the number line essentially it's a
measure of how much you're willing to
bet that this thing will take on a
particular value this thing that you're
interested in right so you can use money
or if you prefer not to gamble like
myself then I would just say credibility
points assigned to the number line right
credibility is credibility points
assigned to the number line gives us a
view of probability that is a good
enough working definition for how we can
view probability in you know applied
problems all right so we're where do we
want to then use probability as a tool
once again
spend 30 seconds with your neighbors and
talk about this let's gather back let's
gather back I'd like to hear from those
who haven't already raised their hands
and contributed something to the
discussion where would you use
probability in an applied setting right
at the back okay could you elaborate on
that please right yes yeah okay right so
probability is a tool for that next-door
neighbor okay
did you give an example
right so we so some key questions we
might be interested in would be like
what's the probability of a catastrophic
catastrophic failure of that piece of
infrastructure for example or something
like that mm-hmm
right so a calculation of again the
probability of a catastrophic failure
would be really important and the
probability that the locks are that the
safety mechanism would fail as well
would be also important right right
right so you need to know the
probability that the rate is equal to
something plus the uncertainty on it I'm
very I'm very happy that we're going in
a bayesian direction here okay cool
great so if if say we were example for
example in a marketing firm or a new
tech firm and we wanted to think about
click-through rates right that's another
place that probability comes into play
so another applied setting and
click-through rates are essentially
measured as the probability that a user
that arrives at your page will click on
something right so that's another
example and that's the the first example
that we're will use here to get a
practical handle on how we can use the
existing tools in our toolkit we
probably know numpy we probably know bit
of pandas and the rest numpy matplotlib
will will use numpy to help us get a
grasp on what exactly is probability and
how does it relate to proportions and
the like okay so let's say let's try
this example together okay it's a code
along activity let's say we've got like
a website and we've measured the
click-through rate accurately to the
ninth decimal place right so it's 50%
accurately to the ninth decimal place
so what does that mean of the the
visitors that come by so if we had a
thousand visitors how many people would
we expect to click on that button any
volunteers 500 right like that's the
expectation right we'll try to simulate
that okay so if we have one way to
simulate this kind of problem is to use
numpy and to start we'll take a uniform
distribution right well we'll start by
saying we've got a whole bunch of people
we don't know where they came from and
we'll we'll simulate this process of
clicking by taking a thousand random
numbers of bounded between 0 and 1 and
then cutting a threshold somewhere at
50% right so we'll do something like NP
dot random grand 1000 and what this will
give us then is if we plot the histogram
of that my kernel just died on me cool
all right let me try that again
we plot the histogram of that you should
get something that looks like this right
so this is what we would call a thousand
draws with equal credibility assigned
across the number line from 0 to 1 and
to simulate how these how people click
on the website we can do a few things we
can do first ask how many of those
values are below 0.5 and by definition
the rest are going to be above and
finally simply sum up the total number
of clicks that we did right so we define
clicks as a value being drawn from less
than from this distribution being less
than 0.5 and then we simply sum it up
how many do people get I get 481 what
what are the others for sorry 505 478
etc okay cool so we got a lot of we've
got a variety of answers here and this
is one of the core ideas behind
statistical inference that is there is
always some form of randomness involved
and when there's some form of randomness
involved what we will expect the answers
that so we can calculate this thing
called expectations and we expect that
on average from say a lot of us pooling
our results together the number of
people who click will be centered around
500 right at point five out of a
thousand however because of random
effects and whatever else that goes on
in the in the data generating process we
won't always get exactly 500 where
there's always some randomness involved
right so then we can calculate the
proportion who that clicked as basically
and clicks over Len of clicks and I will
get 0.48 1 and you will get your point
505 s and whatever right so this is so
this is this is one of the so one one
idea that we want to convey here is that
in in in statistical inference we're
often talking about random processes
right things that are not deterministic
there's always a component that we don't
know how to exactly write an equation to
model exactly right so and and and so
what we do is we use a statistical model
to help us get around this fact right
and there's randomness built in an
inherently inside there so all right so
what we did was we we said we had a
model so-called model of clicking in
which we said half of the people who
come will click and then we drew sort of
samples from that model right where we
had our own on our own computers we had
our own instantiation our own
realizations of that model and there
some random effects that are inside
there okay so what we'd like to do now
is have you all do this on your own for
the next two to three minutes try to
simulate what the results will look like
when you have a quick through rate of
0.7 instead of point five
all right so spend a minute or two
handling this if you are ever stuck you
have lots of resources you have your
neighbors so continue networking if not
you also have the instructor notebook
and you have the screen which I'm going
to be typing on as well
cool so it'll look something like that
not once again not everybody will have
the same results right right okay so
some numbers just popcorn style this 703
694 I have 708 what else 687 ok cool so
we have what this forms is a
distribution of numbers right
realizations cool this model that we've
just simulated by hand right he is known
as the biased coin flip
we know that the coin flip is the
classic example that every probability
tutorial has to deal with the biased
coin flip is just of nothing more than a
variant on that where else do you see
biased coin flips happening ok talk with
your neighbor 30 seconds nobody will
answer the first time like on my first
thing so do have volunteers where do we
see the biased coin flip
apart from click-through rates they have
a volunteer mark
oh okay so if you looked at a junction
you can calculate the probability that
the lights are favoring the main artery
rather than the side arteries okay cool
a friend widget manufacturing can we
talk about that okay so you have a
binary outcome accepted or rejected
where else back there yeah biological
sex at birth yes exactly
so it's it's a binary situation for the
vast majority of the population so under
that approximation then there's a what
is it the canonical is like 51 or 52
percent male female sorry and slightly
lower for male right okay
where else all right so I think the
point is well described by these
examples right there's a binary outcome
that we're seeking to model and the
binary outcome sometimes is merely an
approximation the binary outcome is a
useful if it's a useful approximation
then we use that we use we can use what
we call the biased coin flip to model
the process of generating the individual
outcomes that we actually observe right
okay cool
let's try a different example so this
example here was us basically hand
coding the generative process in a very
explicit fashion for what we would call
the Bernoulli trials or binomial
binomial trials okay we're gonna try a
different way which is to actually look
at real data and treat data as if they
were the population so to speak okay as
if the data that we measured were
infinitely large enough which is never
true but as an approximation we'll start
with that and ask how can we calculate
the probability of certain things it
under this assumption of the data being
a really good approximation of ground
truth
so what I'd like you to do is run this
next cell what this data set that we
have here is Finch Peaks measured on
Galapagos Islands right so how many of
you have learned biology and have heard
of the Finch finches before right so
there we go we have another binary
outcome right there
so in this someone went to the Galapagos
research team went to the Galapagos
Islands and measured Finch beaks
both their length and their depth and
asked and just recorded what they were
and let's assume that this is a
realistic sample a realistic
approximation of the population of
finches that were observed so let's grab
out just the beak length as a panda
series so follow along with that cell
and so what we'd like to ask is what is
the probability of a bird having a beak
length greater than 10 right so how
would we calculate that well one way we
might calculate that is under the
assumption that the data are the
population we could just ask what
proportion of what proportion of birds
have big length greater than 10 so we
can do something like P is equal to the
sum of lengths ths greater than 10
divided by the length of that and you
should get something close to you should
actually get this exact number in this
case because there's no randomness in
the data right so we're not simulating
the random process we're treating the
data as if they were part the the true
thing so everybody should get 0.85 one
right cool so what this these two
examples they're really here to show you
that proportions are somewhat a prop a
proxy for probability right under
certain circumstances if you make
explicit certain assumptions your
proportions can be a good estimator for
the actual probability that we're
interested in
all right so oh right so over here we
can actually try we can actually try
simulating the Finch beak lengths as
well right so we can draw random samples
from the data and use that as another
way of seeing estimating what the
uncertainty around that probability
would be so there's this this procedure
is what we would call resampling with
replacement
I think it's bootstrapping my memory is
my memory is blanking at this point but
resampling with replacement is another
way to do it if we're interested so now
if we break the assumption we break the
assumption that the data are exactly
what the population are then we're faced
with a problem we're faced with the
problem that is the proportion that
we've calculated may not actually be the
true probability of of Finch beaks being
greater than length 10 so how do we
estimate what that uncertainty might be
will do this hacker statistics sort of
method where we computationally try to
simulate random draws from the
population using the data so the way we
would do this here is to do we would
first do random pics from the data
distribution so NP random dot choice
lengths so we're choosing from the the
length panda series we want a thousand
we want to sorry we want 10,000 samples
from there and we want to do it with
replacement and finally we'll take the
sum of that
some over all of those that are greater
than 10 and divided by n samples and so
you will notice we won't get an exact
exactly the same number question yes it
is the latter it is choosing from
existing elements all right cool
question we're resampling we're sampling
with replacement ten thousand times yes
yes exactly okay okay so these are
computational methods one of them sort
of is a explicit simulation for the coin
flips right the other is treating data
as ground truth and then breaking that
assumption now saying data are not
ground truth what is the uncertainty in
this parameter we're trying to ask me so
they're hacker stats ways of handling
this there's another way that we can
deal with probability and talk about
coin flips and the likes right so coin
flips are essentially by Bernoulli
trials right every coin flip that I make
has a single outcome that can take on a
one of two possible values we'll call
them 0 & 1 or true and false a binary
outcome of some sort and we can actually
take advantage of we can take advantage
of num PI's random random number
generators or the probability
distributions that are inside there to
try to simulate this okay so there's
Bernoulli trials and then if you sum up
Bernoulli trials you get binomial by
normally distributed data right so so
we're gonna try this out over here okay
so
let's set a numpy random seed you can
use 42 you can use 1 million six hundred
and seven thousand 1912 sixteen million
sorry as they're numbered your choice
and we can flip we can we can do a few
things so I'm gonna we're gonna try this
we're we're gonna try simulating a
single flip right let's try simulating a
single flip MP dot random binomial one
right N equals one and it being a biased
flip will do zero point seven and so if
you rerun that cell many times you
should get a lot of ones but some zeros
right oh the seed my bad thank you there
we go yes so you should get and we'll
get a bunch of ones and a bunch of zeros
if you summed up all of those Bernoulli
trials say we we did ten of those trials
together and treated them as a single
experimental run so we'll now do empty a
random by that binomial of ten you'll
get sometimes ten sometimes four
sometimes six etc etc okay so now what
we're really interested in is how in in
a biased coin flip what we're really
interested in then is how what the
probability is of getting sorry in sorry
let me backtrack a little bit so in a in
a binomial trial in a binomial draw or a
binomial run well we're interested in is
calculating the probability knowing the
probability of getting up to that number
of successes inside up to that number of
successes for that run right so the
binomial trial is basically defined as
and being the number of successes sorry
n being the number of total Bernoulli
runs that we've done and P being the
probability of successes and it gives
back a number which is the number of
successes out of that n number of trials
so what values so pop quiz what values
can this take on the result of a
binomial dry trial zero to ten or n in
the general case where we know what n is
right okay cool so we can actually
simulate this we can simulate say 10,000
experiments of us flipping coins 10
times and counting the number of times
that it we got a head in each particular
experiment right so we can do that by
doing MP dot random dot binomial 10 zero
point seven and 10,000 if you want some
clarity you can actually use the
underscore between your numbers to make
sure you know what you're typing so and
that will give us something that looks
like this right so what I'd like to ask
you then is what do you see in the chart
what values are probable what values are
not probable I'm not going to ask you to
talk with your neighbor this time around
any volunteers so the most probable
value is 7 right and we know that from
the fact that our P which we set was 0.7
so the expectation are the most likely
value the thing we expect to see the
most is is 0.7 what else do we see the
upper bound found it at 10 right we
don't see any values at zero one or two
what's happening there yeah we've got
very we've done the experiment 10,000
times and that's still not enough right
it's still not enough for us to be able
to see whether there's a probability see
the the proportion of trials that we
will get one head only out of ten all
right okay so I'd like you to try the
following exercises spend about two to
three minutes on them the first one is
calculating the probability of five or
more heads for a value of P is zero
point three then four point five lot the
histograms for both of them yeah so
spend a minute or two handling that just
really quickly go over you should get
something like this for the first
exercise something around the value of
0.7 six five five something like that
for the second exercise right the
probability of seeing a heads is higher
so therefore the probability of seeing
five or more heads out of 20 is also
going to be higher right so 99% of those
have those have that result and if you
plot the histogram you should see
something like this now for those of you
who had that bit of time to think about
the question looking at the histogram
can you tell me how what is the
probability of seeing four or more heads
are me it's yes but a lot is not a
number from zero to one right so what is
that number from zero to one prior to me
85% okay maybe it's not very easy to
tell right not very easy to tell this is
where this is the sort of question where
histograms are kind of not the right
thing to look at it's the thing that
we're used to looking at but it's not
the kind of thing that would give us
rich it's not the kind of plot that
would give us rich
information on the data that we have on
hand okay question danke
yes exactly so we want what we want
instead is the cumulative distribution
of our data and that I argue in a blog
post gives us much richer information
than a histogram would a histogram is
nice and convenient to look at because
it sort of tells us where the central
tendency is but that's all it really can
tell us what the central tendency is
right maybe the balance but even the
bounds aren't going to be shown very
accurately as we would see from this
histogram up there right so this is
where the cumulative distribution of our
data comes into play and we can plot
what we would call the empirical
cumulative distribution function the e
CDF of our data and the e CDF is a great
way to visualize this okay so if we take
our data and we arrange them well
actually let's code along let's code
along and you'll see what the e CDF will
look like for this for by normally
distributed data so X flips and Y flips
are going to be e CDF of our data which
our data is X what it will return is
it'll give us indices on the x axis
which are where our data points fall and
then it'll give us an index on the y
axis which is a number from 0 to 1 that
tells us how much of our data falls
below that particular data points value
okay so let's do that and we'll do a PLT
dot plot X flips Y flips marker is a dot
[Music]
oops
line style is none forgot to run that
now things look a little clearer right
so for those of you who have the
histogram on your screen keep the
histogram on your screen and compare it
to the e CDF that's on the project
screens now let me ask what's the
probability of getting four or higher in
our data something percent yes okay so
here's how you look at it you go to four
you go up to the bottom of that pseudo
that bar like looking and thing and then
you draw a line across to there and it
falls roughly at point two right so the
probability of getting four or higher is
approximately 0.2 and the thing about a
CD f's is our enemy up point eight sorry
thank you one - we're doing the higher
thank you so the the thing about a CD
f's though is that it gives you much
richer statistical information you can
tell things like the central tendency so
what's the central tendency well we go
to 0.5 draw a line across you see where
it hits the data and look on the x-axis
and that value is 5 exactly what are the
bounds of the data exactly and take a
look at that we couldn't see the zero in
the histogram because it's kind of like
obscured by the height of the rest one
thing that's cool about the e CDF is
that it uses all of the data
there's no binning bias that can obscure
the values that your data can take on
that is a serious problem you can lie
with histograms you do not want to lie
with histograms I will come after you
ok ok cool so I made my case and I rest
my case with EC DFS don't ever use
histograms again always use e CDF's they
give you a much richer view onto your
data you can look at all of the
percentiles of interest you can
visualize sorry all of the percentiles
that are relevant in your modeling
problems using EC DFS ok
okay so we did a little recap on
probability just to make sure we're all
clear and we're on the same page
probability is credibility points
assigned to the number line
okay when we plot something like this
we're saying there's lots of credibility
points assigned to this value there's
very little credibility points assigned
to the tale values okay that's all
probability is a working definition for
our purposes we can simulate draws from
a probability distribution we did that
multiple ways we can simulate it from
data right by doing replacement sampling
with replacement now what we're going to
do sorry and finally we can actually
take advantage of the exact analytical
form analytically implemented
probability distributions in our in
numpy and psy PI stats and use that to
help us simulate what our data might
look like under a set of fixed
parameters when I was learning Bayesian
stats this activity was really really
helpful for getting familiar with the
shapes of probability distributions and
as you'll see later the shapes of your
probability distributions particularly
the bounds of the central tendency and
how they're skewed can be really useful
pieces of trivia that you might pieces
of knowledge that no longer are just
trivia in your head but actually can be
useful tools for modeling what we're
going to do now is do a very very very
very quick run-through of the different
probability distributions and what
they're so called stories are write
probability distribution oh well let me
backtrack a little bit how many of you
have heard of the term generative models
of beta yeah those who've been in the
deep learning world will know that
there's like generative this term called
generative models and frankly at some
companies like the one that I worked at
it's been overhyped
quite a bit everyone wants to talk about
generative models at its core generative
models are just how we can generate
things
look like data right and if you think
hard and long about it
probability distributions are generative
models of data probability distributions
are generative models of data because we
can construct a model that is composed
of purely just probability distributions
and use it to simulate data that looks
like actual data that we might collect
okay so let's think about what actual
data we might collect say starting with
the poisson distribution rights or
poisson processes and the Poisson
distributions that's a that's a
generative model and it's got a story
that's behind it in this concept of what
is the story behind each and every
probability distribution is something
that we need to make sure we're familiar
with so Poisson distributed data
basically can be thought of as something
like this I have a this is borrowed from
David McKay's book I have a town and
it's called Poisson Ville and the buses
they kind of have an odd they're kind of
like MBTA trains right in Boston so they
come and then sometimes you have to wait
a heck of a long time before the next
one comes and sometimes the next one
comes right after them right they're the
one that just came by yeah and then
sometimes they get derailed and well too
bad for Bostonians so the amount of time
that you wait for one train or one bus
is independent of the amount of time
that you waited for the previous bus
right okay so that's the story of a
Poisson process and that means and so
the timing of the next event is
completely independent of when the
previous event happened and so it's not
just you know faulty trains on on the
the red line and in Boston there's other
things too right there's like births in
a hospital
there's meteor strikes god help us if we
have one aviation incidents the rate of
things happening right a number of
events happening per unit time okay so
that's what a Poisson process is
and the number of arrivals that occur in
a given amount of time takes on a
Poisson distribution so all that the
Poisson distribution is modeling is just
how many things happen within a given
unit of time so we can actually simulate
that so if we want we can what we can do
here is we can simulate Poisson draws MP
dot random Ghassan with per unit time
six events happening so six natural
births per day or you know six
collisions per at Brigham circle near
near near Harvard Medical and we'll do
like 10 to the power of six let's do a
million draws right and then plot what
this distribution looks like
PLT dot hist of samples we'll just set
the bin size so that it's a convenient
thing for us to visualize and you'll get
something that looks like that right
does the shape look kind of familiar
not that you've seen it before but like
some other distribution the binomial did
I hear ya what's what's up with the
binomial like yeah so this this one
doesn't isn't exactly symmetric but
close enough yes
yeah so there's a neat relationship
between the Poisson distribution and the
binomial distribution that is for low
probability of successes that is for
really really rare events that happen
the Poisson distribution is you know the
the approximation or the limit as we go
to low probability of success and large
number of trials right so there's a
relationship between these distributions
if we we can actually let's plot the e
CDF of this as well
okay so e CDF of our samples that we
drew and then we plot this guy again
and you should get something that looks
like that
once again this tells us a lot of
information what is the central tendency
in this case six yes exactly so the
central tendency I'm using this term as
the more generalized thing right so the
central tendency can be either the mean
the median or the mode depending on
which one we're talking about the
expectation is the mean and the
expectation of the Poisson distribution
is the parameter that we passed into it
right so it's if we say that things are
Poisson distributed with you know six
events per unit time then we expect to
see six events the central tendency will
be six the median will be six the mean
will be six okay all right so something
that's also Poisson distributed would be
field gold field goal attempts per game
yesterday over over lunch a bunch of us
we're talking about how football and I'm
sorry to the American football fans
that's handegg football real football
that is played with your feet that sort
of at a rate of field goal rate that's
not that good whereas like hockey is at
the right field goal rate whereas bass
compose at some absurdly high field goal
rate so we're gonna do data we're gonna
do an example taken from Professor an
instructor at Cal Tech Justin voice who
I've met at the SyFy conference and it's
about field goal attempts by LeBron
James right and he was he did that's his
that's his data to his stats per game
right number of things that he number of
things that number of times that he shot
in one game so the unit time is one game
and we're asking how many attempts did
he make okay I need to reconnect my VPN
part of me okay so let's move on
so we've got the field goal attempts
here right that's that's okay it'll come
back when it comes back and we'll do the
EC DF of this data all right cool and
finally what we're going to do is we're
going to do many random draws from a
Poisson simulation from a Poisson
distribution and plot all of those
random draws the e CDF's of those random
draws to see whether it follows the same
distribution as what LeBron's you know
field goal attempts would would be like
so let's code along so for underscore in
range of that samples is NP dot random
possum
MP dot mean of field goal attempts and
the size is the length of field goal
attempts and in this case we'll plot the
e CDF of the theoretical samples and
let's see my kernel is disconnected so
I'm going to have to reconnect and rerun
cells
since we're at it I'll just bring up the
constructor version so that we have it
there you should get a plot that looks
something like that right okay and so
with that guy over there
that's one probability distribution
story that we can talk about the Poisson
distribution
now the Poisson distribution is a
discrete probability distribution what's
the other class of distributions the
continuous family right okay so the
discretes when we plot that they
technically follow a probability mass
function because there's a bulk of mass
that's associated with each particular
value that is how credibility is
assigned with continuous function
continuous distributions we have the
probability distribution function which
technically have zero mass at any point
on the x-axis right but they take on
they have a density of mass so called
from within a range of continuous values
okay okay we're gonna skip one or two
we're gonna skip the exponential
distribution I'll just leave it out
there for you that the exponential
distribution is the waiting time between
puts on events and so that's that's that
and so you can take a look at you can
take a look at how the CDF see CDF of
the post exponential distribution will
look like on your own time will go
through the normal distribution okay the
normal distribution everyone's familiar
with this right things are normally
distributed a lot of things are normally
distributed okay we've got measurements
and in this case we'll just take a look
at what the story is the story is in
this quote here when doing repeated
measurements we expect them to be
normally distributed
owing to the fact that many subprocesses
lots of individual data generating
processes contribute to this final thing
that we measure okay so things like
human height
a combination of many genes so it's kind
of reasonable it's a combination of lots
of genes plus environmental effects put
together and so it's reasonable to
expect that human height would be
normally distributed or approximately so
right so yeah so the the formulation one
formulation of the CLT the central limit
theorem is that any quantity that
emerges as the sum of a large number of
sub processes tends to be normally
distributed provided that none of the
subprocesses is very broadly distributed
itself that is it doesn't overwhelm the
final data distribution that we're
looking at okay so just to have you all
take a look at this these are what we
would call these are measurements of the
speed of light and there's an in in an
experiment lots of factors contribute to
what measure what value we eventually
measure ok so there's some data on the
speed of light and the estimate of the
speed of light tends to be normally
distributed because of this you know
data generative process right lots of
things contributing to this final thing
there's error in the instrument there's
error in there's the instrument itself
and it's got some error
there's the measurement technique and
it's got some error there's you know the
conditions of the day that we're
measuring and that can affect you know
that's the that's the reason most
biologists tend to use something happen
that day for a failed experiment and I
know it myself because I was one before
so yes you get you get the point all
right so there's a combination of
factors that lead to this ok now I want
to leave before we go on a break I want
to leave you with this little tidbit
which is which comes from Ellen Downey's
blogpost are your data normally
distributed I encourage you to look at
that blog post because though something
though we impose models on our data
our data may not necessarily be exactly
following that model that we've imposed
on so when we make an assumption or when
we impose this idea that our data might
be normal when we do a fit we check how
deviant our data are from normal
distributions we use that normal
distribution and later downstream things
for like estimating the mean and the
likes when we do things like that we're
saying we're imposing a model and that
model might be wrong but it can be
useful right this is a classic George
box quote I'm gonna mash that with
George Orwell saying that some models
are wrong but some are more wrong than
others okay
and we might be find some of them useful
right so take a look at Ellen's post
ponder about that point we'll come back
at 2:45 p.m.
there should be snacks inside the Tejas
room we'll come back to in notebook 1b
and very quickly go through joint and
conditional probability okay this is a
last-minute decision that I made in that
in the interest of time there's some
ideas I want to cover we'll cut down a
little bit on the foundational exercises
but then we'll have the concepts given
to you also you have the tools to do
your modeling later on okay so as I
mentioned in the interest of time we're
going to skip a few exercises the next
notebook is on conditional and joint
probability and really I want to give
you all mostly just the tools that you
need to be able to think through the
problems at hand okay so one of the
tools in the toolkit is the
distributions and the stories that are
associated with them I drew this out as
a table but by the way there's a really
great resource by Justin Boyce who put
all of the distribution stories that are
relevant on on on his Caltech website so
that's a really really great resource to
go and check you know make sure that's
also linked on our github repository so
you can always check that out from there
okay
so just to recap we've got probability
they follow we've defined as credibility
assigned on the number line we have ways
of simulating probability distributions
both analytically and by brute force
computation really the core thing that
we want to have in our toolkit is
actually the probability distributions
as well as their shapes and their
stories okay so this is a very
incomplete highly partial table of the
many probability distributions that
exist out there and I wanted to you know
hint it you should be building such a
matrix for yourself and it it is really
helpful like you can maybe have a cheat
sheet written by someone else but I
always find it like if I do the hard
work and draw it out and write it out
for myself
it's also really useful nonetheless I'll
be building a resource that basically
100% plagiarizes justin's resource but
gives added nice visuals for for some
for learning benefit okay but really so
the toolkits are you you want to know
the names of these distributions because
they aid in communication with other
people who do statistics when you're
writing things out you'll want to know
what their abbreviations are right so
that guy highlighted over there because
that gives a nice and compact way of
telling people what what distribution
you're using when you're writing stuff
in your mental model you want something
like the shape of a distribution but a
uniform is just equal credibility
between two bounds so that picture
should come into your head you should
know whether the bounds are relevant to
your problem or not and also whether
equal credibility makes sense or not
right just so that you have something
live there's a variant or the
generalization of the uniform
distribution which is the beta
distribution it takes in two parameters
success and six number of successes
number of failures it's actually bound
between zero and one explicit
and takes on values that can look like
that or can look like this or can look
like that right so but it the key point
is that it takes on values that are
bounded there's this term which took me
a little while to remember but it's
called the support of a distribution
this is something that you'll see inside
the statistics literature the support of
a distribution is nothing more than the
values that it can take on the values
that it can take on all right that's all
it is so the support for the beta
distribution is something like a zero to
one it's bounded between 0 to 1 right
and there's a story for the beta
distribution which is number of
successes expected fraction of successes
out of n success plus and failure
ok and it can take on the alpha and beta
parameters can take on not just integer
but also floating point decimal numbers
ok so you'll want to have this kind of
picture in your head when you're
thinking about that where can you learn
again I'm saying
Justin Boyce's website is a great place
I learned a lot of these probability
distributions and their their shapes and
their possible values by looking at the
pine seed docks PI MC 3 has a great
thing for that where you'll notice you
know in PI MC 3 basically all they're
doing is just all we're doing in the
docks is just simulating those
distributions and plotting them out so
that really is the best way to do it all
of that hands-on simulation that we just
did that is a great great way for you to
learn what the probability what
probability distributions are what their
shapes what their data generating
processes are all about ok so I want to
start with that that's one thing you
want to have in your tool kit the next
thing you'll want to have in your tool
kit
is this idea of joint and conditional
probability okay and so once again in
the interest of time by the way for
those of you who are who just got back
make sure and using binder execute
something so you don't lose your session
the way I think about joint conditional
and marginal probability is by a visual
that looks something like this yeah if I
have data that are jointly distributed
they might look something like that say
this is a bivariate Gaussian this put
together is what we would call the Joint
Distribution of the two things that
we're interested in X 1 and X 2 okay
then there's a thing called conditional
distribution that is what is the
distribution of one of the two axes
given that we know something about the
other okay so if we use red to do the
conditional distribution this is a known
value of x1 oh sorry this joint
distribution is denoted as P of x1 and
x2 okay
so in red we're going to we're going to
show you what the conditional
distribution looks like
so the known distribution looks like
that
oh shucks I hit the off button if we
take that joint distribution and project
it back onto the X 2 axis it itself will
follow a distribution okay okay with
this right this is the distribution of
x2 and have given that we know x1 this
is what we would call the conditional
distribution okay and this is denoted
probability of x2 given x1 where that
little thing over there is given okay
that pipe tells the statistician that
you're taking that you're computing the
distribution of x2 having known that
particular value of x1 okay are we okay
with this so far okay there's a final
idea which I'm hoping you'll keep which
is known as the marginal distribution
the marginal distribution looks like
this there are two distributions over
here okay this in blue are what we would
call the marginal distribution why why
is it called the distribute marginal
well first off it's on the margins of
this to this two axis thing okay it is
the value of x1 ignoring whatever values
it ignoring whatever value that x2 is
okay so it is the value of x1 completely
ignoring the other variants that are of
interest okay so this is denoted as P of
x1 this is denoted as P of x2 and those
are marginal distributions okay so prior
to drawing this out for myself this was
something that I didn't wasn't really
able to keep straight in my head but
this served as a very like what I would
call an anchoring example for what these
three terms mean joint conditional and
marginal probability now why are these
three terms really important it's
because it's from joint and conditional
probability that we make our way joint
additional and marginal probability that
we make our way to what we call Bayes
rule okay Bayes rule if you've seen the
famous neon light photo photo is written
as P of a given B is P of B given a
times P of a over P of B where does this
come from well this comes from the
definition of joint joint probability so
P of a comma B is equal to P of a given
B times P of B which is equal to P of B
given a times P of a and if you simply
isolate out this portion and move this
guy over there then suddenly you have
Bayes rule right and this is a this is a
neat thing because it gives us a way to
move between probability of data given
model or in probability of model given
data if we take the an alternative view
of what we're doing okay and to
illustrate this example we're actually
going to use the drug-testing example in
notebook 1b so I'd like to invite you to
navigate to there okay I'm gonna plug
this back it's my laptop so the drug
testing example is one of those classic
things where we can come up with you
know a solution to a problem but if we
don't do the stats right we'll be kind
of off all right so I hope you'll see
this from this point so let's look
the question right so we have a test
it's 99% true positive for drug users
and 99% true negative for non drug users
what that means is if I give you a drug
user and then I ask you to do the test
on that drug user then 99% of the time
it will be correct and if I give you a
non drug user and I ask you to do the
tests then 99 percent of the time it
will be correct as well so it sounds
like a great device right so what I'd
like to then ask you to do is before
doing any of this computation write down
what you think the probability that will
be that a drug user is I sorry a
positive testing user will be a drug
user just put down a number it's got to
be from zero to one because it's a
probability they put out a number
mentally in your head yeah it's in the
notebooks so 99% true positive results
for drug users and 99% true negative
results for non drug users okay it's in
your notebook so scroll down on notebook
1b write out a number and when you're
done give me a thumbs up don't think too
hard on this one you're meant to be
surprised okay so we can actually
simulate this whole process right
because we know switch back to here we
know a few things about the data
generating process
and we can actually represent this as a
tree that looks ugly that tree might
look something like this
someone's a user and someone's a non
user and then we do the test and they
turn out to be positive and negative
positive and negative now help me fill
in the blanks what is the probability
that the user tests positive given that
they are a user 99% and so by I'm sorry
I'm gonna use 0.99 here and so what
should the value be on the negative arm
0.01 very good and then what about the
non user 0.01 right so if is if the user
is an odd not a drug user one person at
a time they will test positive as a drug
user and 99% of the time oops I'm being
inconsistent they will test negative so
this sounds like a really good test
right well we're interested in however
is P of drug user given positive because
this guy here is P of positive given
drug user so how do we calculate this
well we can simulate it let's come back
to the notebook
okay so if we take 10,000 subjects and
we simulate up sorry let me backtrack a
little bit in order to solve that
problem we're missing one piece of
information that is what is the
probability that a user is a drug user a
person is a drug user we have the people
persons group and then we divide them
into drug user non drug user so let's so
some some may say that you know if we're
in Central West Virginia then you know
opioid crisis is like ravaging there
right and it's a black stain on the
pharmaceutical industry for that and so
we might put an estimate that five to
ten percent of the population are drug
users now if we are in clean clean
Massachusetts then what might we believe
about this fraction it might be ten
times smaller say point oh five or New
York City right like Hugo's from New
York or Sydney actually that's his
hometown so 0.05 right let's uh sorry
not point oh five 0.005 right so let's
let's simulate how many drug users we
will get under that particular regime so
we'll do mpg and my know me all will
have ten thousand users okay we have a
probability of them being the
probability of them being a drug user is
0.05 and we only want one trial and so
the non-users is going to be n minus the
number of users right number of non
users in our population is just the
compliment okay we run that cell oh yeah
all right I need
so of the users how many of them will
test positive said 99% but it's a you
know probability so will will explicitly
simulate it MP dot random dot binomial
again we have the the users number of
the number of users in the population
and 99% of them will test positive and
then we'll have the number of non users
but we also want to know how many of
them will test positive right so how
would we simulate that MP dot random dot
binomial non users and what's the
probability value inside here 0.01 right
because we're only interested we're
interested in given that you're positive
what is the probability that you are you
true user or not a user right of this
drug so what fraction of those tests
will be positive for users what do we
need to divide by we need the sum of non
users and users sorry non positive and
positives or rather non users that did
test positive gosh the the naming is is
tough that's the hardest thing in
computer science right if we calculate
that you'll get something like what
point three ish you all get that is this
surprising part of me it's a big number
yeah yeah like we thought we could get
away with like a 99% sensitive and 99% a
specific test but it turns out because
the thing that we're really interested
in is inferring the thing that isn't
shown from the data
the thing that isn't shown from the data
is the latent thing that is argue a drug
user or not and then that shows up in
the drug test but the drug test has some
probability of error as well so if we go
back and think about that tree that we
drew that's the full data generating
process that is the full data generating
process for the things that we observed
and now we can use that to back infer
the probability of the thing that we're
interested in rather than the
probability so the probability of data
given the underlying condition or the
model rather than the sorry probability
of the model underlying condition or
model given the data rather than the
thing that is easy to simulate the thing
that's really easy to simulate is
probability of the data given my model
of the world what if my model is wrong
right what if my model needs updating
and and so that's that's where this
Bayes rule thing comes into comes in
handy okay all right
now if you look at how this is solved
with Bayes theorem the equations are in
the notebook take a look at that what I
encourage you to keep in mind though is
we're we're interested in the
probability of our probability
distribution of our parameters of
interest given the data right when we're
talking about modeling our data
generating process we we're gonna stick
in parameters like you know P write
probability of success or lambda or mu
you know lambda for the rate of a
Poisson process or mu the central
tendency for a normally distributed
thing but we might be wrong and we need
to update our model having seen your
data and that's where Bayes rule comes
in okay so we're gonna look what we're
gonna do next over the next two and a
half hours is to look very sorry two
hours is to look very in-depth into two
particular data generating stories that
can be applied across multiple places so
in some senses these are fairly generic
models that you can take home
and using your modeling work but we're
gonna go really deep into each and every
one of those and so pardon me if you are
already quite familiar with this but I
think it's it's handy for a few reasons
one you'll have the mechanics of Pi mc3
which is the tool that we're going to
use under your tool kit you'll also have
the process of telling a data generating
story in your mind as well okay so and
we'll have practice with that okay so
we're gonna skip notebook number two and
instead move to notebook number three
directly okay so I'd like to invite you
to open up notebook number three and
this is where we jump right into what we
would call probabilistic programming and
Bayesian estimation these are
probabilistic programming sorry go ahead
yeah let me put that up yeah yeah yeah
definitely definitely definitely so
let's write this out okay Bayes rule
states that P of X 1 comma X 2 the Joint
Distribution sorry Bayes rule starts
from this formulation as P of X 1 given
X 2 times P of X 2 which is equal to P
of X 2 given X 1 times the probability
of X 1 okay all right so if we say
probability so if we if we we if we do
the rearrangement of terms then we get X
1 well let's let's make this fit the
example that we're looking at probably
bit of X 2 given X 1
is therefore equal to P of x1 given x2
times P of x2 over P of x1 are we ok
here so far woof ok it'll it'll be up
there
I'm unfortunately restrained by the size
of the screen here as well right in
order to fit enough in inside here so
let me use the pencil to illustrate what
this is this is the marginals okay
these are the marginals this is the
conditional okay this is what we're
interested in irie care that and what
we're interested in is also a
conditional just to be clear we're
interested in the red distribution
however we also need in order to know
the red distribution that's up here okay
in order to know that distribution we
actually need to know this distribution
that I'm highlighting the probability of
x1 given x2 but integrated or summed
over all possible values of x2 which
therefore we switch mics which therefore
means we're doing some form of summing
over or integration over all of all
slice or all horizontal slices of our
data okay so just to make this a little
clearer we're saying this this top term
up here is basically this slice plus
this slice plus this slice plus this
slice all summed up all multiplied
together okay
that's what we're doing and that's how
that formulation Bayes rule relates to
this picture that we've we've drawn for
conditional marginal and joint
probability right okay so I'd like to
have you all open up notebook number
three so I'm gonna do probabilistic
programming and we're gonna start with a
coin flip story we the coin flip story
is way too classic but it's it's it's
really one of those anchoring examples
in my mind
all right if you master the complexities
that we can build on top of it for the
coin flip story
then you'll grasp a lot of concepts that
will be used are usable across multiple
different types of models okay so we're
gonna do estimation like I mentioned one
of the core activities of statistical
inference is estimation of the parameter
given data notice the given right
there's a conditional is that we're
jointly modeling our data and our
parameters together and we're seeing
given that we've observed data now
what's the distribution of parameters
that we've got okay
so go ahead run that first cell the
first thing that we're going to do is
we're going to actually look at some
we're gonna look at click-through rates
again the classic binomial thing rather
than coin flips coin flips are a little
too boring so let's look at
click-through rates by loading this cell
I lost my kernel again you know what if
you all don't mind I'm gonna switch over
to the instructor notebook and not code
along but I'll make sure that you all
have enough time because I have all the
right outputs in the in the notebooks in
the instructor notes
yes notebook three so last night I
posted on our slack do a new get pull so
make sure you if you haven't done that
do it get pull so that's that should
hopefully clear up the confusion by the
way for the tutorials you get up on the
slack because it's useful for it's a
useful channel for the instructors to
one-way communicate information and you
know the other way get back
questions right okay so let's look at
the click-through rates data okay you
all have that loaded oh good my things
working now you should get data that
looks something like this right so we
have we've done this test we have a case
in a control and we've measured for
every single every single visitor to our
website whether they clicked on the
whether they clicked on the button
within a fixed period of time or whether
they just decided to leave okay so how
then do we use PMC syntax to build a
model that helps us estimate the true
value of P with its uncertainty for this
data so if you didn't have pi mc3 what
might you do you might do a D data frame
dot group by right so you might do R
group by group dot mean something like
that well okay yeah I need to run the
cell you might do something like that
and if someone runs that code what would
you what number do you get it while
mine's running it has to connect to the
kernel did someone do that
Oh CTR my bad click-through rate here
you'll get something that looks like
this you'll get like 0.1 400 500 for one
and 0.19 one two five click-through rate
for the other group right and how much
would you believe that data may be true
maybe not depends on like how how our
data were split between the control and
the test group right usually we do a
random random splitting but in this case
you know some malfunction happens so we
only had really 200 data points out of a
thousand for the for one of the groups
and 800 for the other so we don't have
50/50 splits
we have 8020 splits so one of the groups
is going to be smaller oh it's actually
two thousand and eight hundred so it's
even worse it's it's not even it's not
even one of those nice round numbers
that we can think about so cool so we've
got like this skewed about data which
one which number do you believe in more
given that you know that the test group
only has 800 you believe the control
number more right and because that's
because we've got more more measurements
for them all right well what we're gonna
do is we're gonna spend a bit of time
thinking about what the data generating
process looks like for this kind of
model so let that come up what does the
data generating process look like for a
Bernoulli or binomially distributed data
this by the way is the exact workflow by
which I go about every single problem so
we've got the question what is the data
generating process well we start with
the data that we have on hand we have
Bernoulli distributed data so we'll say
the likelihood
follows a Bernoulli distribution
okay we're gonna just estimate for the
control group so one of the two groups
we're not going to do with two groups
just yet we'll build it for one so this
parameter P however how is this
distributed
okay so we got the value P how would we
model that well if you've never seen
click-through rate data before
how would you assign credibility points
to the number line to correctly model P
talk with your neighbor for a minute or
two this time I mean it like really talk
with your neighbor
I hear something's crystallizing do we
have volunteers what might you believe
about how would you assign credibility
points to the value the values that P
can take on having never seen
click-through rate data uniform what
zero to one so let's try that
we'll say then that P is distributed
zero to one uniformly okay this being
the likelihood down it's this being the
likelihood means we're actually going to
this sorry sorry when I'm backtrack a
little bit what we've just drawn here on
the screen is one generative model for
our data one generative model out of
many possible models that we might want
to build the this thing that I've
highlighted being the likelihood is the
thing that we have observed okay
so I wish Ravine was here I would have
him copy this model onto the whiteboard
give me one moment
okay so we got the model on the
whiteboard I'm going to switch back and
we're gonna see how we can actually
build that model with PI MC code really
easily okay
so code along I believe this is code
along since we're only doing the
estimation for the control group we're
not going to worry about the test group
just yet how do we write this model well
we can write it this way PM dot uniform
right kind of looks very very close to
the picture we drew we'll call this
variable P it needs to have an explicit
name so rule of thumb is whatever you
name it in your variable as the Python
variable just give it the exact same
name that such that PI MC can recognize
this lower is zero uppers is one okay
and then the likelihood would be p.m.
Bernoulli will name it likelihood like P
is equal to P the Bernoulli distribution
only has one parameter it takes only one
parameter in it's called P what is it
it's distributed uniformly having not
seen the data okay
how we doing okay so far syntax
mechanics and then what we do next is we
say observed is inside our data frame
have it inside our data frame control DF
clicks okay so that is two step means
we've observed one's a sequence of ones
and zeroes
whether the user has clicked or not that
is the data that we have observed for
the Bernoulli distribution okay so this
is one way of writing that model I'm
gonna give you all how are we doing with
this syntax so far this is the mechanics
part of building a model that said if
you think back to what we drew on the
whiteboard just now the syntax looks
very similar right the syntax here looks
very similar to the syntax oh sorry the
syntax in code looks very similar to the
syntax the thing that we draw on the
whiteboard the pictures are much easier
to reason about right we can draw our
data generative process here and
directly translate it into code so
that's one way of observing it there is
another way another formulation right
and if you remember what I said just now
the distribution sorry if you remember
what I said just now a sequence of
bernoullis is binomially distributed
which what that means then is we can
actually write the model as a binomial
likelihood but we'll have to change a
little bit about how we do how we
structure the data to be input okay but
I'm gonna just give you the binomial
model as is and you can try to break the
model or break break how we structure
the data to get a better feel on how
things should work let's run that cell
the next thing that we do having written
our model and telling PI MC what we're
conditioning on is we say within this
model context sorry model one Bernoulli
please sample from the posterior 2000
times
take me give me 2000 draws that describe
how we expect the parameter P to look
like it's a simulation MC based Monte
Carlo based simulation what the
posterior will look like now this is
kind of unnecessary for for a simple
coin flip model but when
slightly more complicated hierarchical
versions you'll see that this comes in
really handy okay
run that cell and do the same for the
binomial
it's literally PM dot sample literally
that's all you need to do what's
happening here fancy math is happening
for lazy programmers that's been the
motto of Pi MC 3 we abstract away the
math the fancy math that is MCMC
sampling or variational inference and
allow users to focus on building
generative stories so really the place
in when doing PI MC modeling the place
that we need to keep our focus on is in
the place that I've highlighted up there
in this in the cell that is what is the
model definition is it Bernoulli is it
binomial okay
so I'd like you to run those two cells
and if you run those two cells and you
plot the plot the posterior distribution
you'll get something that looks like
this and how do you do the posterior
plotting you do RV's for which for which
ravine is one of the lead developers
it's a visualization tool for taking a
look at the results of MC sampling all
right and to be kind to those who still
haven't broken out of the histogram land
oops yeah I need to run all cells above
so I want you to invite you to write run
that you should get something that looks
like this that's happening on the left
hand side of the screen and you'll then
get a posterior distribution a view on
what we believe about the value of P
having seen the data and having seen the
having explicitly stated what we believe
prior to seeing the data
yeah for now I'm not going to go into
that that is something slightly more
advanced and that's the contents of our
evidence tutorial so if you're not
attending MIT then catch the tutorial
online because that's where they'll go
into a little bit more of the theory
behind the Monte Carlo sampling that
goes on all right for now we're gonna
ignore those errors what I want to give
you is the mechanics of writing the
model and the mechanics of sampling and
the mechanics of visualizing and
interpreting okay so do the same for the
Bernoulli mob for the binomial model
okay and you should get something that
looks like that the values range from
0.1 point one two five to a point about
0.15 five right
the 94% highest posterior density what
all that says is that there's 94 percent
of the credibility is assigned within
this black bar region all right that's
what we believe is true question yep yep
yep yep yeah exactly that exactly that
okay all right so what I'd like you to
do then is you've basically got the
template that you need to now replicate
this for doing two groups within a new
model right so I'd like you to do is the
hands-on activity below in which you
build a model for that that does both
the at both estimations one for the
control group and one for the test group
and spend about five to ten minutes such
that you get up to
this point where you get this plot okay
so go ahead do that question oh okay
I'll come and address it
it happens when you're doing the
sampling okay so oh okay okay okay um
give me a moment I'm blanking right now
all right all right so I'm gonna give it
I'm gonna give this to you that it is
the math is being executed when you hit
PM dot sample but really what happens
underneath the hood is we've computed a
joint likelihood of all the parameters
with the data and then we use MC
sampling to figure out what the typical
set of that of the posterior is and we
own that the goal here is to sample
around a typical set that is the typical
range of values for the for the likely
for each of the parameters that are
involved all right
okay so how many of you are done thumbs
up okay we got a bunch of people who are
still working at it
so if you're stuck you should have
something that looks like this you can
do it with the binomial so I'm
encouraging you to try it with a
binomial rather than Bernoulli but if
you want the Bernoulli I'll code it live
for you all
oh I see I hear fans running someone's
sampling real hard thank you so if
you're interested in what the Bernoulli
formulation will look like it'll look
like this the binomial formulation looks
like that on the right hand side all
right okay
and then if we're going to sample from
the posterior once again fancy math
happens for lazy programmers all right
if you do this final thing which is a
deterministic transform of your random
variables you can actually explicitly
compute the posterior distribution of
the difference of the two PS and this
would be akin to what you are trying to
do if you were to do a t-test of sorts
all right where basically you're just
comparing two means and asking how
overlapping are they are they
overlapping or not a binary decision are
they completely overlapping or partially
overlapping or completely non
overlapping is one greater than the
other right we're asking questions like
this basically so I'll show you how to
do that you can do P diff is p.m.
deterministic and it's nothing more than
math on probability distributions so
we'll say P test minus P control will
define test - control as the diff
difference between the two and rerun
that and what we'll get is a posterior
distribution on the difference of the
two parameters so I'm really tempted to
ask this but in a t-test would this be
significant so yeah okay sure now next
question with the t-test be appropriate
why
well there is an N is if we're doing if
we're doing so that it's not correct is
the not the right thing to do is the
correct answer but I'm I disagree
slightly with the reasoning we do have
multiple ends we have multiple
observations here so that's accounted
for let me ask you about the ship back
there maybe power calculations are one
thing that I had a long Twitter thread
on thankfully it's not a rant so you can
read it I'll post that yeah so so let's
see can a probability be normally
distributed no why right right exactly
exactly now we can approximate it with a
normal distribution but we have to be
extremely clear that's an approximation
of what already is an approximation a
model is an approximation of the world
we're putting another approximation on
top whoa okay all right so this is where
knowledge of the shapes and the support
of the distributions comes in handy P is
a probability it can never take any
value below zero or above one so why
would you impose a normal distribution
on on the probability parameter P right
especially now you we don't need to do
any math right well we don't need to
write equations out and solve these
equations that tell us what the
posterior will look like having seen
data under a normal approximation we
don't have to we can explicitly sample
from the posterior using MC simulation
so why not go whole hog and just use
probability distributions that are
bounded from 0 to 1 right okay cool
great so you all just had my I did a
talk at PyCon it was my 25 minute rant
on why we don't always do the t-test why
we should go away from cam to
statistical procedures this is one
example of it P is not normally
distributed
he might look normally distributed but
it is definitely not normally
distributed
it's got to be bound so you can't take
on a probability distribution that is
found from negative infinity to positive
infinity if you do that you're wrong
okay cool great and and on this
hypothesis testing thing think about
there's there's a great blog post by
Alan Downey again that talks about the
fact that every single classical
statistical test boils down to one
framework and if you go Bayesian there's
really no reason to calculate p-values
and the likes
you just look at posterior x' and look
at the posterior distributions of
statistics single valued statistics
single distributed sorry single variate
statistics that you're interested in you
don't you don't have to worry about
p-values here okay are we all right with
that cool cool cool
great now knowing that the probability
difference is knowing this probability
difference is is all good good and
useful but it's it's still not tied to
something that is real world and
interpretable in the minds of our
executive friends right so how do we
take this metric that we've calculated
pdiff and turn that into something that
matters well one thing that you might
learn from ravines tutorial if you're
going or watching it later is that there
is this idea of a loss function or a
cost function that we can attach to
these things of interest and I'm gonna
just show you a very simple example okay
let's say we know one thing we've
computed with other data and said that
our customers on average spend 25 US
dollars if they click and 0 US dollars
if they don't click right like this
there's money attached to this this
process how that money is attached to
this process we can always write into
equation that describes hours well will
write one arbitrary one so we don't have
to go too deep into what it is but if
you take this
if you look at what's in the trace for
pdiff you'll notice it's it's nothing
more than a sequence of 2,000 draws from
that posterior okay so run run that run
that on your note on your own notebooks
as well just open up and view what PDF
is it's an umpire array it's got two
thousand numbers okay so a difference in
probability can translate into a
difference in amount of money being
spent and if we attach a single you know
a single dollar value to each and every
one of those rather than a distribution
that's not complicated for the moment we
can do something like dollar
distribution is trace of P diff times 25
times 1 million right over a million
customers there's an increase in the
probability that they spend money so
under the test group and so how much
increase do we expect over a million
customers let's translate into some real
numbers we'll do that and finally you
should get something like that you
should get something like that so what
does this say well on average we expect
a lot of revenue increase but there is
always this very very small tail
probability that we still might lose
money and that's just the nature of our
computation okay that's so I just wanted
to put that out there you can always
tack on this is highly custom or problem
but you can always tack on a loss
function or a cost function that
describes that ties the metric of
interest you have to think about it to
some dollar amount or some hours spent
by people on a particular problem okay
and that's a way of communicating to non
so-called non-technical folks who want
to be maybe a little bit more spoon-fed
on what we expect to see
okay are we okay with this so far so far
so good
any questions okay if there are no
questions I want you to talk with your
neighbor for one minute one new thing
you learned anybody want to share
something new they learned thus far yep
yep that's right that's the point of
this first exercise get you familiar
with that mechanics of doing so anything
else any and that's the default sampler
yep yep yep yep yep absolutely
absolutely yep
how so yep oh gosh
so yeah yep
oh gosh yeah and that's right right
absolutely and and so that actually
brings up a very highly related point
we've built statistical models of the
world we build statistical models of the
world when I built the cost function it
was both
somewhat of a hybrid of a statistical
model and a mechanistic model of the
world that is people who click will now
spend money on average $25 per
click and so we do some multiplication
and that expresses some mechanism but
ultimately they're models and we have to
validate them and so that problem you
brought up is I think one of the
problems that we don't know how to
validate properly I think because we
don't have the negative data to build a
good model we have all the positives the
successful molecules we don't know what
the opportunity costs and how to model
the opportunity cost of those failed you
know incorrectly modeled molecules would
be yeah that's that's like that's a very
good point anything else okay if there
are no other points you'll notice all
right we have the case in the control
group if you listen to many of my rants
casein control isn't the only thing you
can do you really can do like arbitrary
number of groups without having to worry
much about multiple hypothesis
correction and the likes and like you
know having this guillotine of P values
which you chop off as you go down oh
gosh that's like I don't know how people
came up with that but it's it's a it's
it's complicated right whereas like just
looking at posteriors is so much more
clean much easier to interpret as well
so we're gonna do another example that's
still the binomial story still the
Bernoulli binomial story but it's gonna
show you how like we can't just copy
pasta test control player one player two
player three player four player 857
write or write a for loop with functions
and that's just like way too much we can
actually take advantage of some
syntactic things that allow us to write
in a very concise fashion these models
that model multiple you know more than
two groups okay
so I'd like you all to scroll down we're
gonna we're gonna look at baseball data
and this is one of the classic classic
this is one of those classic data sets
that one would pick up right so we we
want to we want to model the probability
that a player who is a batter will hit
up
right so they have the stat called
at-bats and number of hits at-bats is
the n the total number of times that
they've come up for batting and H hits
is the total number of times that
they've actually hit the bat so we've
got some data from the baseball database
all credit to them I'd like you to run
this cell that loads the data little
pitch for a tool that I've been
developing alongside colleagues here so
Zach in the back he also works on this
it's called Pi janitor and it's
basically there to help you make data
pre-processing easy to read so that you
you have a clean API for cleaning data
all right so we've got the data
I'd like you to load that you should you
should see something that looks like
this okay we've got at-bats hits the
salary of the player and this extra
column which we will use its called
player ID underscore encoded okay it's
basically just a an integer encoding of
the player ID nothing more than that
you'll find out why that becomes handy
later so once again it's the same old
Bernoulli binomial sorry now because we
have the data structured as at-bats and
hits which is the what should we what is
the likelihood that we want then is it
Bernoulli or binomial for me binomial
and why is it binomial yes because we
know exactly how many times we're not
recording every single bat we're
summarizing we're taking the summary
statistics which is the total number of
at-bats in the total number of hits and
this then forms the binomial story in
which we know the number of times
something has come up for trial
something has come up for a test of
whether they succeed or not okay this is
distinct from the Bernoulli where we
only know that they're coming up and we
just we only know that we know that
they're coming up and for for a success
failure trial and
you know the probability but we don't
have the number of times that they've
got that okay
so we'll be using a binomial
distribution as a we'll be using a
binomial distribution has the likelihood
so let's let's build this model and what
I want you to do is to notice some
syntactic changes here okay
so first we'll have the pitch model
we're gonna build that I'm gonna switch
over now to a beta distribution okay a
beta distribution is also bounded from 0
to 1 it has the correct support a beta
distribution also allows has this
parameter where two parameters that let
us control the shape where it's skewed
is it skewed left or is it does it have
more mass on the left or density on the
right it lets us do all of it lets us do
that right lets us control the shape of
the distribution so it's not just a
simple flattened uniform prior so code
along with that p is the name of the
thing alpha is 1 beta is 1 and then the
shape of this this is a new thing the
shape of this beta distribution is the
number of players that we have so all
we're expressing now is that we've got
instead of a single beta distribution
that lives in memory we've got a vector
of beta distributions one beta
distribution per cell right and that
beta distribution maps onto one
particular player player okay that's
that's what we're doing here the
likelihood is binomially distributed so
p.m. binomial it's name is like it's p
is distributed according to the beta
distribution but it's a vector of
distributions so what this effectively
does is it creates a vector of binomials
okay that's the key syntactic difference
and conceptual difference that you need
to take away from this example
you can get away with doing no you can
get away from for loops by simply
vectorizing everything okay n is data at
bat right this is the number of times
they've come up and it's also a vector
that is the same length of P and finally
observed is data hits okay my my line
wrapping is coming into play maybe I
shouldn't set this in the tripping or
notebook okay now we got that but since
we have salary information and we've
been talking about like we've been
talking about tying things to real-world
numbers right let's compute a metric
that says the probability of batting per
unit of salary right so how how we want
that number to be as high as possible
right so we want for the smallest salary
someone we want to figure out for the
smallest salary someone who bats has the
highest battering batting the percentage
that would play right into the
sabermetrics kind of thing right you
know you're looking for value for money
on different metrics
so we'll do a deterministic transform
we'll call this peeper salary PPS okay
and simply take P and divide it by the
salary that we observe in the data how
we doing any questions so far so far so
good
following long story we it's by the way
the exact same data generating process
the exact same story as we drew on the
whiteboard go ahead do sampling then and
look at look at the posterior
distributions
so notice how we're sampling it's fast
I'm taking advantage of the GPU that I
have at home this one in this particular
case I don't think it'll run any faster
on the GPU then on on a CPU because
there's no complex matrix multiplies
that are going on but if you were to do
like more complex things complicated
things like Bayesian neural net which
you actually can write in theano and i
MC 3 there's totally no problem with
that then moving stuff on to the GPU can
get you up to four to four to eight
times faster sampling and fitting okay
so we have built here oh I named it
trace batting let me resample again so
what what I've done then below is create
this custom visualization that relies on
hi widgets and the likes to get it
working the intent here is that if you
were to visualize posterior
distributions for 805 players that's 805
matplotlib axes they are drawing to
screen it's not the prettiest thing okay
so we'll take advantage of some
interactivity that we can build to oh
it's not working on my screens but it
should be working on some of yours right
if you've got ipi widgets working you'll
get a select that you can actually look
at and select multiple players and
compare their wear peeper salary and
their cumulative cumulative
distributions for each of those okay
actually while this is running I'm just
gonna very quickly get up and running
and so while you have that visualization
up I'd like you to try to hunt for the
player that's got the highest PPS
distribution and while I install some
while I reinstall some things once once
I'm finished with the commands we'll
come back and talk about that
okay so anybody found interesting
players yes that's right so one thing
that we have as an idea in Bayesian
statistics is that in the limit of lots
of data what your priors are really
don't matter unless unless you chose
priors that can never change
which means you're just hard-coding your
great uncle's your uncle's viewpoints on
politics which will never change right
so yeah I just need to reload
my mind my mind it's very slow so do we
have players that are that look
interesting part of me okay can you help
me out a little bit what does their CDF
look like sure sorry exponential being
what this this this this line that looks
like that okay some look like this right
you'll have noticed some of that any
other patterns yeah
squiggly but more or less straight line
okay let's talk about what each of these
CDF's Express what's this one expressing
yeah essentially it's expressing that
the credibility points are assigned
anywhere from the lowest to the highest
in pretty much a uniform distribution
okay what about something that looks
like this how tight is the distribution
compared to the first one very tight and
it's also very shifted to the right
right because this the central tendency
is way out over there okay and then what
else do we have we have this guy over
here what's this guy like
something in the middle something in the
middle still takes on lots of values but
the distribution is kind of skewed as
well right because there's a there's a
long tail on the left lots of lots of
probability or lots of credibility
assigned to the middle over here and
then it Peters off at the top it'll
always beat her off at the top once
again it's it's the richness of
statistical information so PDFs look a
lot like CDF's
I'm sorry PDFs look a lot like
histograms I take that back PDF to look
a lot like histograms and so you can't
tell more than the bounds in central
tendency from a PDF whereas you can tell
quartiles and percent percentiles and
roughly estimate where they are from a
CDF so most of the time I would just
default to using a CDF rather than a
histogram or PDF okay let's see if this
works my widget doesn't display never
mind you guys have you all have the
widget on your laptop so you'll be able
to view that okay so there are some
interesting players one thing that's
kind of interesting though is that we've
got players like the uniform
distribution one right super
uninformative and that raises a problem
the problem sounds something like this
now I'm gonna pose this as a question to
you all so think about it
having having seen players professional
players play and do their thing
do you really expect that having a few
at-bats those that comes from the
situation where we have like one or two
at-bats and zero or one hits do you
really expect that having seen that one
or two at that we should still believe
that their batting capability is at this
near uniform from zero to one talk about
that with your neighbor and then tell me
why yes so do you really should we
really believe having seen one or two
at-bats that performance would still
range so wildly from zero to one
basically what are your answers should
we really believe what we saw in the
posterior results that performance gets
basically is still uniformly or close to
uniformly distributed no I see a head
shaking tell me why both of you I have
both of you were a team
right right yeah some so what we're
encoding here is this notion so the the
response is pretty much how we would
think about it right the professional
baseball players don't tend to have
performances that range so wildly that
is a piece of prior information that we
can impose on the modeling problem so
how do we impose that there are two ways
one we can have a stronger prior a
stronger prior that is imposed on every
single player so we might do a beta so
if batting averages tend to fall within
the range of 0.2 to 0.3 we might put a
beta distribution with so 2.2 is about
approximately one success and four
failures so it would be a beta
distribution of one and four that is a
slightly stronger prior or if we wanted
it to be even stronger we would do a
beta distribution of 10 and 40 which is
much narrower compared to the beta of 1
and 4 if you don't believe me go
simulated numpy you're there the beta
distribution is right there and if you
really wanted to be a really strong
prior then it would be beta 100 400 okay
that's the beta that's the point two
batting average kind of prior we could
put on here so the advantage of using
the beta over the uniform is that I can
now tweak the alpha and beta per ml the
a and B parameters of the beta
distribution to change where we Center
the distribution and also change how
wide or thin that distribution is right
so as I was mentioning beta 1-4 looks
something like this
skewed but kind of wide beta 1040 looks
something like this and beta
100-400 looks something like that okay
are we okay with that yeah so ADA
distributions give us a little bit more
control over the over the the shape of
the distribution okay so putting tighter
priors is one way another way to
approach this is actually to impose what
we would call a hyper prior one that
governs the population of players so I'm
going to switch over to drawing again we
have the binomial likelihood and we know
this already right this is a very
familiar story for us by now in the
interest of in the interest of saving
space I'm not going to draw out the
distributions but picture them in your
head okay n is known he comes from
another distribution the way I want you
to think about this though is because we
vectorized everything we've got a vector
of binomial likelihoods and a
corresponding vector of beta
distributions for priors on the peep
parameter if we were to do this
hierarchically what we are effectively
doing is asking what is the population
Alpha and Beta look like that's kind of
ugly
so and we only have one of these each we
don't have a vector of them because
they're governing the entire population
okay so when we do this hierarchical
thing we're saying we're effectively
expressing this idea players themselves
are drawn from a parental distribution
professional players all generally
follow some general distribution that is
governing the performance of the
individual players so the population
distribution which is imposed on a and B
that governs the individual players
performance parameter which is the beta
distributed thing which then influences
the outcomes that we're interested in
okay so let me just annotate that
relation parameters we have the integer
individual parameters for T and then we
have the likelihood which governs how
our outcomes the data that we observe
are generated okay
are we okay so far now I'm going to go
on a limb and tell you that a and B
these two parameters they also have
distributions because then otherwise it
wouldn't be probabilistic but now the
question is what is an appropriate
distribution for a and B what might be
an appropriate distribution for a and B
I'll tell you if you I'll tell you a few
snippets a and B govern the number of
at-bats
well sorry govern the number of
successes and failures effectively for
the population of players this happens
over a single year okay
it can only be positive so it being over
a single year means there's generally a
finite number of positives and successes
and failures A's and B's that every
player has so given that you know that
what might be a suitable distribution
let's not worry about the shape of the
distribution just yet what is a suitable
distribution well saadon maybe it being
I forgot to say NB can actually be
continuous and that's sort of a giveaway
that Hasan's not so ideal but we could
try it
what's a positive bound continuous
distribution that you might have heard
of exponential is one there's another
one that we can define which is the half
normal right that is the normal
distribution but chopped up in half such
that now the support is defined only on
the positive half that's one way to do
it gamma distributions and other demos
actually I think a generalization of a
number of child distributions it's got
more parameters it's a little bit more
complicated but yeah we can
we do that look normal I think is as
well yep half coffee as well like you
know
yeah half student to you etc yes
definitely so we're all on the right
track
we're thinking of distributions the key
point is these distributions have to be
positive bound because a and B can only
take positive values if we were to do
anything with the full normal
distribution we'd be doing it the wrong
way okay so for simplicity sake let's
assign a and B to take Exponential's and
the only reason I would start with this
but maybe not end with it is that
Exponential's are easy they have a
single parameter so there's not much
hyper hyper prime parameters that we
have to worry about these this a and B
thing that we're assigning distributions
on these are what we would call hyper
priors hyper being an added dimension
right and add a dimension of modeling
that we're doing here so for the sake of
simplicity we'll start with a simple
exponential what did I do
in the real thing in the actual thing we
use 1 over 29 which is just an arbitrary
number which we can always debate about
but we're not going to today we're going
to do that in a long modeling critique
session that my colleague Zack and I
have done umpteen times now where we
debate their priors debate the model
structure for now for learning purposes
we'll just stick with that okay
so let's take this model and code it up
I'm gonna switch back to the notebook
like to encourage you all to also switch
over inside the notebook you will have
you will see that we've already got the
binomial likelihood and the PPS metrics
the deterministic transforms defined for
you so now I'd like you to code along
and let's fill in the rest for the beta
distribution and the a prior and B prior
okay so we
have alpha is a prior beta is B prior
the shape is still Lana of data and then
we'll have exponential 1 over 29 oops
make sure it's all floating points and
let's call it hierarchical baseball
so once you've coated up the model go
ahead and sample from it and tell me
what you see is kind of different
you'll notice also this model is a
little slower to sample from
okay so while you all are waiting for
models sample and finish up questions
yes no problem
well well well things are sampling it's
actually a great time to talk with your
neighbor about something new you've
learned okay before we go on into
something new you've learned I want to
ask a few questions about what you're
observing about these baseball player
posterior z-- having seen this having
been fit under this hierarchical model
what's different from what you saw
before okay so you see more sigmoid doll
type of distributions rather than
uniform or exponential types
that's one good within one good one
what's another property that you're
observing as well what are the bounds
and ranges artemiy they're more tight
they're more tight okay and this is the
result of this type of of switching over
to a hierarchical model rather than
using an independent model right so the
the first model that we wrote where
every player is modeled as a beta
distribution on its own that is what I
might call an independent model and then
the hierarchical model actually sort of
pools these player properties as being
drawn from one parental distribution so
they're sort of constrained by the
parental distribution this is a property
and I'm not going of hierarchical models
I'm not going to assign a value judgment
on whether this is always good or always
bad it depends on the problem but if it
is justifiable by your modeling domain
expertise then a hierarchical model is
actually a really powerful way to borrow
information from players that have had
lots of
from the population of players to do
entrance on the players that we have not
had much information about so for those
players that had one at bat and one
success what you will notice is that
their posterior distribution is still is
going to be kind of wide not as crazy
wide as it was before it's gonna be kind
of wide centered roughly around what the
population mean is and follow roughly
what the population mean is as well if
you have something that's a little bit
more extreme like seven at-bats seven
hits then you'll get something that's
shifted to the right because there's a
little bit of information saying that
this player is kind of good or maybe
lucky we don't know there's a bit of
it'll be shifted to the right it'll be
narrower but it does express that you
know it's not gonna be wildly like 90 90
something centered on ninety two hundred
percent to be centred off shifted off so
at least in this setting it correctly
encodes our intuition that players
generally fall within this like
population distributed they follow the
population distribution much more than
we would expect from just looking at
them independently okay so this is a
very powerful thing that phenomena where
you have these wild estimates being
shrunk towards the population mean is
called shrinkage shrinkage is a term
you'll want to look out for in the
literature okay so you have this
vocabulary that you won't be confused by
question
yep okay okay cool so the beta that
we've put in there
expresses a prior that is unconnected to
any other player so the the prior for
the beta distribution the a and the B
the priors that we put they are even
though they were point estimates they're
just saying this is this is the shape
we're putting an identical shape of
distribution on every single player now
when we connect the players by saying
they all draw from a population
distribution it's not that we're putting
uncertainty I would be hesitant to say
that we're putting uncertainty on a and
B rather we're expressing that their
shapes are now controlled by a
population shape right that that's where
that's where this shrinkage comes in
it's not that we've we were really sure
and so we assigned a single point value
it's more that that the two point values
can give one shape but that shape for
the beta distribution was unconnected to
the population at first that's all it
was okay and and now when we have a
connected set of shapes right so we have
we connected beta distributions by the
hyper priors that we've put on now we're
saying is what we're saying is that
there is a population shape for the beta
and and they're influencing and
governing the individual player shapes
the data distribution shapes does that
make sense by ironically putting a
distribution on it not putting a ring on
it yes yes yes
so if in doubt don't put a ring on it
put a distribution on it okay cool
right so that's that's that phenomena of
shrinkage that I wanted everybody to
have some intuition about okay so let's
now go into something new you've learned
we've actually come to the end of the
binomial story and we've gone really
really deep we've come from like the
simple naive 1 1 group to 2 groups
- now vectorizing over multiple groups -
then now adding on a hierarchical model
on top I'm hoping it's not yet
information overload because there's
more so what's something new you've
learned and let's get that like itched
in your head you have volunteers sure
yeah cool great that was part of the
point anything else
something new that you didn't up expect
or something that like has been
resonating with you anyone on this side
this side has been really quiet I'm
gonna point at someone a second last row
middle guy anything new you learned
no no no okay still processing that's
that's completely valid that's that's
totally cool and the fact that you're
still not sure means it's there's
there's processing going on and I fully
appreciate that how about in the middle
anybody else any volunteers sorry can
you say louder
okay so reinforcing the value of the
cumulative distribution plots right
that's that's super important the fact
that we get richer information from that
is very useful okay oh the beta
distribution yes the fact that we're
able to constrain in shape yes yes
exactly exactly it's very it's a very
useful tool to have in your toolkit yep
cool cool awesome and back there last
one yeah oh yeah yep yep yep yep
absolutely there are lots of good
connections there so a lot of a lot of
the classical stats are connected than
this way yes yeah I was hoping that this
question would come up and trust me I
did not plant her in the crowd so when
we think about which distribution to use
there are a few rules of thumb okay the
first rule of thumb is find something
that has the correct support that is
absolutely crucial if you use something
that's got the wrong support that
you've got data that showed up negative
so you've got data that are showed up
negative can take on negative values but
you put a positive only distribution
inside there you're gonna get not a
number of errors inside sampling right
and and that also means that you've not
you've missed something in the modeling
process so getting the support correct
is the first step then the next step is
to think about a likelihood right and
that's where knowing so that's that's
for any arbitrary distribution getting
the support correct is absolutely
crucial the next thing is to think about
the likelihood function how are the data
that you are interested in the thing
you've actually measured how is that how
is that distributed so that's where
knowing the probability distribution
stories comes into place essentially
rules rules of thumb are if you've got
something that's got a amount of stuff
happening per unit time it's Poisson if
you've got trials that are positive
negative its Bernoulli binomial right
these are very generalizable stories if
you're really unsure you might start
with a normal distribution if you've got
some other types of processes so for
example the negative binomial
distribution counts the number of
failures until a success right so
knowing this generative story helps as
well okay so and then also there's this
family of distributions called the zero
inflated distributions so you can have
the zero inflated Poisson distribution
what it expresses is that there's there
are two processes at play there's a
process that generates lots of zeros and
then there's a process that generates
the Poisson side of that and there's
there's this is essentially a mixture
model so you're now having to infer both
the probability that it is in the zero
versus not zero P and 1 minus P as well
as the Poisson parameter the rate
parameter of ink
and it it's really important to think
through the that part of the problem so
that's the second part then the third
rule of thumb is to think about what the
shape of the distribution should look
like so this is where I would then look
at the PDF rather than the CDF because
this is all analytical because then it
gives me a sense of the skew and the
central moments of the distribution and
it can help me express quantitatively
what I'm thinking about so the beta
distribution is that classic anchoring
example that I always come back to it's
bound from 0 to 1 and it's suit
therefore suitable for a probability
parameter I can tweak whether it's
centered on 0.5 0.2 0.9 by simply
tweaking the a and B parameters and I
can tweak how Shh how tight that
distribution is by doing you know beta 9
1 versus beta 90 10 versus beta 900 100
right so there are ways to control the
shape of the distribution that way those
are the three rules of thumb what I in
practice find myself doing is thinking
about the problem going like oh yeah I
need something that's positive here
because that can only take on positive
values so now I'll go hunting in the
distribution library for something
that's positive and most of the time
we're sort of expressing you know say
for a standard deviation parameter we're
expressing the fact that things
generally are not going to be wildly
standard deviation parameters are
generally like tight but then sometimes
can take on high values so I might take
like 1/2 Koshi because standard
deviations can only be positive but I'm
allowing for really high tails or 1/2
student-t for example okay so that's a
few examples he'll back there
yeah um so the Exponential's you can you
sorry so choosing an exponential you can
think of it as this is the first model
alright and then if you go to ravines
tutorial tomorrow there's this whole
business of model comparison that's
sometimes you'll find it doesn't really
matter what the hyper prior is and
sometimes it does matter when we check
things like the information criteria
metric that uh
the information that's contained in
inside the model sometimes you'll find
whether it's half Koshi or exponential
just quantitatively it doesn't really
matter so in some senses start with
something and then run with it and then
be ready to change the model yep yep and
I emphasize that we we don't want to
really get into debating that choice but
we can actually offline debate that
choice if we want we can look at how the
lambda parameter controls the shape of
the exponential distribution and whether
that expresses qualitatively what we are
intending to express right so some the
exponential distribution generally
starts high and then goes low right if
you increase I think if you increase the
lambda parameter in quantity it'll it'll
become more and more flat if you
decrease it it'll become more and more
closer to two to zero centered on zero
right so that's that sort of how and
then we'll have to ask is that what we
want to express in the in the model okay
yeah yeah yeah yeah
yeah both from a mechanical standpoint
that is like I have fewer things to
worry about and from I guess parsimony
standpoint is like a simpler it's it's a
simpler model right like we don't have
that many knobs to turn right yeah cool
all right let's see it's 4:40 right now
and we end at 5:30 so I'm debating what
we should worry about next so what's
what we would have done what we sorry so
what I originally planned was to go
through one more example of how we do
Bayesian estimation this time not with
binomial stories but with like student T
distributions and normal distributions
that's one thing we could work on or we
can first jump to arbitrary curve
regression so I'm intentionally setting
the this up is like you can fit any
curve with PI and c3 not just a line
like linear regression is what we're
used to that's kind of boring so let's
go in and like fit a different type of
model what would you prefer so let's do
a vote and I will estimate the
probabilities how many if you want to do
the curve regression raise your hand how
many want to do a second estimation okay
so we'll do we'll do the curve
regression if we have time I'll come
back and show you a few things live demo
rather than interactive coding okay on
the second estimation thing so with that
I'd like you to open up notebook number
five notebook number five is all about
arbitrary curve regression let me see if
I can connect in here cool so curve
regression I'm gonna put this out here
curve regression is nothing more than
estimation with it with
equations okay so we're gonna use a
radioactive decay data set to sort of
reinforce this point okay so we know
linear regression you have you have
something Y is modeled as a function of
a linear combination of your X's right
and so you can have the thing we're
really interested in is like the W is
the weights or you know the M's if
you're from physics y equals MX plus C
the m's if you're in physics or the
weights of your in stats and the bias
term is wealth and really nothing should
stop us from just thinking about linear
regression is the only form of
regression that we're interested in
right there's there's you can do all
sorts of regression you do Poisson
regression whatever you can do neural
net regression if you know how to write
neural nets you can do in this case
exponential decay curve regression right
so we're gonna see whether we can from
noisy radioactive decay measurements
back infer the correct parameters that
help us identify a radioactive material
okay so like you to run that first cell
where we load data okay you'll have
something that looks data that looks
something like this what's on this data
well it's got time on one axis and then
it's got activity or radioactive you
know Geiger count things on the y-axis
okay I've sort of well this is synthetic
data noised out for educational purposes
alright so if you plot the data you
should look you should get something
that looks like this right right
everybody got that okay so given that
we're in this like radioactive decay
sort of scenario I'd like to ask you to
think about what equations can be used
to model this data
we'll get into what the statistical
model is in a moment but I want to first
think about what equations we can use to
govern this model there's an exponential
decay equation what are the parameters
of that equation you have time which
we've observed part of what we've
observed any what else half-life the
decay constant right and anything else
ah sure we're ignoring that for the time
being but yes if we were to be fully
mechanistic we would do that what else
is there
offset so is that the first offset or
the baseline offset baseline offset and
then there's one more which is which
governs the original the starting point
right so there what a C and tau we have
three parameters to estimate for this
curve and you'll notice the the readings
are actually kind of noisy as well right
and that's because there's you know
measurements are not always perfect
there's going to be some amount of noise
that we've got to deal with so how do we
do that well we've got to build a model
I'm gonna switch back to drawing we've
got to build a model that lets us link
the x-axis component which is the time
component to the y-axis thing we've
already said that the equation is y is
equal to a times e to the negative T
over tau plus C right the C term we can
interpret it's sort of like systemic
bias in our measurement the a term we
can interpret right the a term starts is
the starting radioactivity
and the tell term we can also interpret
it is the characteristic half-life of
this radioactive element part of me ah
yes so we're gonna we're gonna talk
about noises oops
Siri goodness yes that is very good and
let's add in this plus epsilon but
epsilon is not one of the mechanistic
components is a statistical component
and that's why I've drawn it in reddit
or a different color right it's not part
of the mechanistic part that we're
really interested in so let's see what
are we're now going to take this
equation and we're gonna build a
statistical model around it what is a
good prior for a what is it sorry what
is a good distribution for a something
positive yes all right and what's the
simplest positive distribution that we
can think about part of me yeah it's
like the first first thing we measure
right yeah so it's a bit like an impulse
that way it's the so we might say a
follows some exponential distribution
let's just start with that because it's
a simple one
what about towel
what values can tell take on it must be
positive yes yes okay so let's say
exponential and what about C this is
systematic bias not the noise in the
data Gaussian systematic bias could be
positive could be negative yes so it
could be normal
could be ah great thank you so instead
of normal what will we do then and we
might do exponential sure what about the
likelihood though
how do me why would it be Poisson we're
measuring counts yes however at least in
the data we've got it as continuous
right now because of the noise in the
machine that reports back a continuous
value so what might we do
cheat a little bit it's an approximation
on our approximation we use a normal
distribution here because the range of
values for which we've got data are
tight enough and far enough from zero
that essentially at the tails of our
normal we don't have any much really
credible credibility points assigned
there so these are like the struggles
that we wrestle with if with every new
modeling problem that comes in is a
normal distribution likelihood
reasonable is it correct probably not is
it useful
maybe all right so I want to get that in
your head yeah likelihood is the thing
that you're observing about the data
right so what do you mean by unit then
right right right right so what we're
gonna so if you look at I'm gonna detour
a little little bit and talk about
linear regression okay so you have y
equals MX plus C okay we might write a
model that says M is normally
distributed for whatever distribution
parameters C is also normally
distributed why is the likelihood of the
data
it's got noise and if we assume that the
noise is Gaussian noise then we can
impose a modeling assumption that says
that this is normally distributed where
the MU is equal to MX plus C and the
Sigma is equal to something else
the Sigma is our epsilon and we can ask
what is the epsilon how is the how is
that going to be distributed does that
make sense yeah so I'm glad you asked
that question because if you look at the
parallels here we'll need a sigma of
prior and just for just for convenience
I'm just gonna put the standard half
Koshi okay inside there so we got that
so then we might impose the same or a
similar set of modeling assumptions on
the likelihood which is our Y right or
rather than calling it likelihood
because that's overloading terms let's
just do y
how is y distributed Y we might impose
that this is normally distributed where
the MU is equal to a times e to the
negative T over tau plus C and then we
have some noise which is our epsilon and
our epsilon just for convenience we'll
also make it half Koshi
let's let that sink in for a moment or
maybe I should say something I'm just
gonna leave this up on there no so we
have questions things that are not clear
yeah if the if the errors in this
particular case if my what we've assumed
is that our errors are not dependent on
the values on the x-axis if now we
suddenly found that the values the the
error varies with the value on the
x-axis suddenly we have to write a
function that models Sigma as a function
of in this case T right so that's one
place where this model would fail and
have actually encountered that at work
before how do we get around that we get
around that by either explicitly stating
upfront that this assumption does not
hold in our data and but we're willing
to work with the consequences of that or
we go hunting for the function and
sometimes that's kind of hard when you
have like limited X values to work with
whoops
Syria keeps coming out cool any other
questions on this this is like the key
key point this is the key point here
like you can write the parameters of
your likelihood distributions as a
function or a transformation on the
other things that you've you're
interested in okay okay so if you go
ahead and let's let's go ahead and code
the model together what's inside the
instructor notebook might be different
from what we just wrote out what I
wanted to give you all just now was this
live experience of like well I don't
know so what are we what modeling
assumptions am I willing to stand with
right and and then we can go back in and
re critique the model one more time so
let's let's put in in this case just
copy and paste what's inside the
instructor notebook all right and let's
not worry too much about the others
again you'll notice that thing that
equation I I eluded it eluded to this
point it's named a few times it's called
a link function right so y equals MX
plus C is a link function y is equal to
a times a times e to the negative T over
tau plus C that's just another link
function you can have your four
parameter dose-response curves as a link
function you can put the standard
logistic regression curve as a link
function like any math function that you
can think of can be a link function all
right and then that goes and that what
that does is it can it controls the the
mean curve parameter it controls the
mean of our data as a function of you
know this thing on the x axis all right
so let's copy and paste what's inside
here
here are the modeling choices are half
normal exponential I think I chose C to
be normal under the assumption that
sometimes the machine might go faulty
and give us like a completely negative
baseline sure and if that never happens
then I would change change that to a
half Koshi or exponential
okay so then we sample oh I hear the jet
engines running again
ah so this is a this is the thing that
I'm wondering ravine will you be
covering pollen will be covering it
tomorrow in the RV's or the Asian model
evaluation tutorial so ignore that for
the time being and you should get
something that looks like this guy the
do we all have that thumbs up if you do
yeah okay
so you get like traces this one's been
simple right because we've got only a
single alpha a single capital a a single
capital C a single towel right but you
all saw just now how we can actually
have a vector of a is a vector of towels
a vector of C's our likelihood normal
distribution can also be expanded to be
a vector of likelihoods there's just
some little intricacies that we have to
worry about with respect to the you know
y equals y is the mu the link function
right so you have to play around with
that but this is totally doable and then
once you have that multiple groups thing
once again you can do your hierarchical
player hierarchical thing if it's an
appropriate modeling decision right so
if you think about it think about it
that this arbitrary curve regression
thing is once again nothing more than
estimation at its heart and instead of
estimating like a instead of estimating
distribution parameters directly like in
this case in previous cases we were
estimating a P hierarchically right now
all we've done is we've said there's an
equation that governs that key parameter
and now we want to estimate the
parameters of that equations in a of
that equation in a statistical fashion
rather than just treat it as some fixed
point okay how are we with that okay if
you want go back and like figure out
what the element is I'm not gonna reveal
the answer right now but I want to point
you to the table at the bottom
your notebooks the table at the bottom
of your notebooks says in compact form
everything that I just said that is you
can put any arbitrary curve as a link
function and you don't have to be
restrained to modeling just linear
models you can model these decay curves
you can model logistic regressions you
can do you can write a neural net like
if you've got some weird function that
is you know
non non monotonically linear then go
ahead write a neural net and estimate
the parameters you might not want to do
like MCMC sampling that's a little too
much you might want to bust out the
variational inference tools that we have
but in primacy three but you know it's
all possible it's all totally possible
okay so all right that's it for the
arbitrary curve regression notebook I'm
going to do we have any questions before
we go back into doing estimation one
more time what you have to do let me see
if I can pull this off here you want to
see not just the single regression line
but the full family of them right yeah
all right this is gonna test my live
coding abilities trace dot bar names
cool one way to do this is to plot what
T would look like first so T is n P dot
linspace from 0 to 800 okay
and then you'll want to write the
equation out so the equation is this guy
I'm gonna put this down here so you
turned that okay then the trace will
have if we inspect trace of a it's a
vector it's 2,000 long 8,000 long I hope
I've done this before but I'm just have
to do this correctly
Oh save time I'm gonna have you we'll
talk afterwards and I'll put that I'll
be sure to put this on the notebook so
that everybody benefits from this
question I think it's a great question
because I've done this before I just
like I'm blanking on life coding but
I'll get that up there for you guys okay
yeah but there's some form of
broadcasting that we need to do right
there's like x over tau needs to be
broadcasted into a matrix and then we
plot each of those rows of the matrix
but I'm not sure how to do this right
now so we'll work that out later let's
come back to estimation before we wrap
up
so with estimation we're going back to
notebook number four so I'd like to
invite you to open up notebook number
four and all I'm going to do is rather
than code with you I'm going to show you
another show you this example is
basically another case study where we've
got information from two groups but now
we have this third group for which we
don't have information of information we
want to be able to make reasonable
inferences on it okay so I'm going to
use the instructor version oops
rather than the student version and I'm
just gonna run run down to about here
first okay so we've got data you always
love to have data and for educational
reasons what I did I took the liberty of
adding in an extra species that was
unknown we know it's a Finch but we've
never we've never really measured it but
and it's so rare we've only got one
measurement now we want to make
inferences on well what's its what's
what's the what do we expect to know
about this new finches beak depth right
we've been measuring the beaks depth and
their length
what do we expect to know so under this
case is like damn we have like one
measurement there's no way we can even
compute a standard deviation on this one
independent measurement right so how do
we even estimate uncertainty in this
case and this is the sort of scenario
where a hierarchical model can be
helpful in exactly the same way that it
was helpful
for those baseball players who had only
one at-bat and know no other data and
then that one at that okay so if we
think about the data generative process
we'll say something like oh yeah our
beaks maybe they are student-t
distributed why student-t it's because
student e is the generalization of the
normal and the Koshi
the Cauchy distribution the student T
distribution has this degree of freedom
parameter his degree of freedom
parameter controls how high or how fat
the tails are normal distribution has
really really low low tails low
probability density on the tails the
Cauchy distribution has really high
probability density on the tails
relatively speaking so the student T
distribution says that when degree of
freedom is 1 it's the Koshi and when
it's infinite it's the normal and
everything else in between is controlled
by this degree of freedom parameter so
we might define a student T likelihood
if we do an independent model we'll get
these we'll get these posterior
distributions on on the beeg beeg death
right and it's on the mean weak depth
and this is kind of like where this
independent model is really not not the
right place to be okay so think about it
we've got like values that can range
from 0 to 15 where we know that finches
generally are constrained maybe more to
where it's more to where it's 4 to 40
or four to 11 or something like that
right in figure out the centimeters and
millimeters but you get the point right
this this independent model doesn't
really have that borrowing of
information from the known species to
help us constrain our estimates so I'm
gonna throw this on the right hand side
here keep this one in mind this is the
independent model now if we fit a
hierarchical model and I look something
like this guy right similar syntax
nothing fancy we have our priors and
everything and we have the Broadcasting
that's happening going on just like in
the independent model except now we have
prior distributions on the parameters of
our distributions for the like on the
distributions for the parameters of our
likelihood function okay it's a bit of a
mouthful but I hope you get the point
there are hyper priors that exist we do
sampling now oh sorry I'm going to
instead throw this up on the right and
there we go this is the one we want oh
okay maybe it's better on the bottom and
if you look at this guy over here throw
this one up here okay
so down on the bottom is our posterior
distribution estimates for the
independent model okay and up at the top
is the posterior distribution estimate
for the hierarchical model which one
looks more reasonable for this unknown
species that we're interested in
this might take a bit of prior knowledge
but then you think about the 95%
posterior density values these are
pretty extreme these are quite extreme
for the problem at hand right Finch
Peaks that are like zero centimeters
like are really close to zero not not so
believable in this case because we have
only a single measurement the math works
out such that our smallest beak size
will you know and the 94% density will
be at 0.9 still might be unreasonable
but if you look at where most of the
credibility is associated it's out here
whereas on this side well
yeah most of the credibility is
associated here but there's still lots
of credibility signed like at really low
values nonetheless so qualitatively
speaking it still doesn't really make
sense right given the background prior
knowledge that we've had okay okay so
that's that's all that I really wanted
to say about this particular model it
was it's intended so you can do this at
home it's intended this exercise is
intended as you know can I build a model
for the data that is now not following
the binomial story because we really
really harp on the binomial story to
illustrate these other things
hierarchical models vectorization of
probability distributions and the likes
okay so we really harped on that but
here here this case yet you can get some
practice with you know something that's
T distributed or normally distributed
and try out other problems for yourself
as well try out the other probability
distribution this case so that's that's
also really helpful all right so a final
thing that we have to do is I'm going to
have ravine and find a few helpers I've
got these little cards that congratulate
you for taking this tutorial and
sticking all the way through to the end
so this is my little way of saying thank
you at the same time I have a little ask
as well there is a survey that we have
on the readme of the github repository
or if you prefer to use your phone to do
it there's a QR code on the back of the
congratulations card I'd like you to
fill out that form to tell us where
where we did well on this tutorial where
we could improve it
every generation of tutorials gets
better and better and it's all thanks to
your feedback that we're able to do it
so while that's happening I'm also happy
to take questions and then I have one
final final final thing for everybody
right so while that's going around while
you all are doing the surveys I hope you
all can open up the readme if you want
to do it on your computer or scan the QR
code if you're on your phone you have
questions on the material today
okay if not then we'll continue I'll
just wait until I've got some form of
quorum on like everybody being done yes
sure okay oh cool all right uh that's
that's good for me to know because
really tight okay yeah yeah oh cool
thanks a lot I learned something today
anything else
[Music]
yeah
so let's see the simplest the simplest
way to do this is actually to use the
SyFy stats module and and use that to
calculate the likelihood of data under
your distribute calculate sorry
backtrack a little bit we've always in
in our examples had data being basically
like a data frame or like multiple rows
of stuff when we calculate the
likelihood it's really the sum of
likelihoods over every single data point
and what happens when we're sampling I
think ravine and pollen will know this
better than I would but it the mental
model that I have is we're sort of we
draw a number from it from our prior
distributions we assume that to be true
and now fit it put that into the
likelihood then compute the sum of
likelihoods over all of our data and
then there's like this the step that
says well okay
given given this thing that we've pulled
out the likelihood is this particular
value now there's you know there's
there's great an information that tells
us which way to go and in in the right
place to sample that will now help us
increase likelihood now I want to be
clear this is not gradient ascent okay
this is not great in descent and I've in
talking with Colin multiple times I've
actually made that mistake of thinking
of it as gradient descent so I don't
want you to think of it as great at this
gradient ascent at all
it's much more complicated than that
alright but that's basically a glimpse
into what's happening underneath the
hood
that is the MCMC step that we're doing
we're like randomly sampling values from
our from our calculated posterior
distribution sorry I didn't I didn't
catch that we can do it with simpler
math so the who depends on how we've
defined simple and complicated integrals
are kind of complicated and if we were
not to use MCE methods we would be doing
integration and that'd be a bit of a
pain I think Ravine had something to say
for that question specifically like the
toilet I'm leaving tomorrow which is on
github has an entire notebook for that
question of how MCMC works and
diagnostics for it so if you're in it
that's good otherwise you can come talk
to me I'll give you the link to the
github and we can talk through MCMC in
well hopefully a meeting about a detail
so I've done it before where we cheat
and look at the data first and then try
to see what distribution might be
suitable well this is mostly for the
likelihood cheating evasions are called
empirical Asians so that's one way of
approaching the problem in practice what
happens is this will build a model and
then it's all got its got all the
simplest things it's it's normals its
Exponential's and like we're not
thinking too hard about the mechanics of
the problem we're not thinking too hard
about the details of the problem we're
not we're sort of ignoring ahead of time
what potential problems might show up in
MC sampling and just running with it
first and then we'll encounter a problem
with MC sampling which often is an
indication that like the model is kind
of bad as well then we'll go back and
think a little bit more carefully about
it
so I've done these sessions at work
where I start working on a model at just
after lunch and I don't go home until
7:00 p.m. because at 7:00 that's when
like something that might look correct
starts to show up right and even then
I'm still not hundred percent sure that
that model is the best model however I
have a pragmatist in my head restraining
me from going til 9:00 p.m. right and
and it says well okay you know you can
you've got the key parameters of
interest and you know their uncertainty
to some degree go home rest over it and
maybe present it and someone else might
be able the peer review process at work
then shows up right like if no one else
can critique the model any further we
sort of all have to just agree that
let's just run with it yeah
yes yes yes yes yes exactly and and if
we're not sure about that we we change
that change the distribution there are
positive only distributions that can be
centered you know way out further out
right I'm blanking right now on exactly
which one's but if you look at the PI n
c3 distribution gallery and you'll see
those pictures and it becomes clear yeah
so there there are these so called
improper priors the flat distribution
assigns forgot of what likelihood it
assigns but it just assigns a single
constant number from negative infinity
to positive infinity and then pine see
there is the there is machinery that
lets you bound a distribution by setting
its lower bound upper bound or both and
that's available and yes you can do that
though I think the pros say don't do it
avoid the improper priors where you can
like it's not it's not the best thing
the reasons why I'll have to dig but the
the rule of thumb I've remembered is
don't like just avoid it yeah
weekly informative priors that say
things like yeah it's probably more
close to zero but I'm really not sure or
it's probably centered around here but
I'm willing to give lots of uncertainty
at the at first those are the general
rules of thumb for selecting priors cool
anything else ok if not let's do a very
quick recap of what we went through
today there's a lot of material but the
the core thing that I hope you take away
are the following firstly that
probability itself you've seen it so
many times here it's
nothing more than assigning credibility
points to the number line where there's
higher credibility points we believe it
more and where there's lower credibility
points we believe that that parameter
takes on that value less times right
that's all it is we saw how we can go
from joint and conditional probability
to Bayes rule and how that map's on okay
marginal probability joint and joint and
marginal and conditional probability are
all really important for this one thing
I really hope you all take back is know
your probability distribution stories
super duper important if you know what
their stories are then picking them for
your modeling work becomes much easier
all right picking them becomes much
easier and so knowing what the
continuous azar and what the discrete
SAR that's really important knowing
their shape their support what story
they tell that will help you in your
modeling work and finally there's we
went through one really simple example
but built it up in depth the binomial
distribution story and went along and
and and showed how you can take a
seemingly simple model and complicated
enough to fit the problem that you have
at hand right you have do you have more
than one group well vectorize the thingy
do you have some groups with lots of
info and some groups that don't have
lots of info well use a hierarchical
model and the mechanics of how we build
these models we reinforced over and over
and over okay and from the discussion
notice a lot of light bulbs going off so
that always makes me very happy to see
all right cool and with that I'm gonna
say we're gonna end here thank you all
for coming
if you want office hours I'll put them
on the slack they will always be in the
Tejas room in the afternoons exact times
see the slack Channel all right thanks a
lot
you