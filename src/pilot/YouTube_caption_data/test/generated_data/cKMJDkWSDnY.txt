what is going on guys hope you're doing
freaking awesome
so in this video i want to show you how
to create
custom layers
so far we've seen how to build very
flexible models
using subclassing now we want to go one
level
deeper and even create the layers by
ourself
so i'll show you what i mean by that but
first just to explain the code we have
in front of us right now we just have
the imports that we've seen pretty much
every video
and then we have these two lines to
avoid any gpu errors and then lastly
we're just loading the mnist data set
so this is just to save some time and so
what we're going to start with
is creating our own uh custom model so
sort of like we did in the last video so
it's going to be very simple we're going
to do class
my model we're going to inherit from
keras.model
we're going to start with finding the
init method so we're going to do self
and then we could specify the number of
classes for m
stats 10 and then we're just going to
call the super
super my model self dot init
and then we're gonna do self-dense one
so all we're gonna do here is just
create two dense layers all right
we're gonna do uh layers dot dense
let's do 64 nodes and then
self dens two is layers
dense of num classes we're just gonna do
the call so we're gonna
call self and then x so the input or
let's call it input
tensor and then we're gonna do uh
self.dens 1 of input tensor
and then let's run a tf.nn.relu
on top of that so this is just what
we've seen in the last video this is why
i'm going through it pretty quickly
and then we want to return self.dense 2
of x
all right so now let's quickly build a
and just a model compile a model fit on
that so we'll do model
equals my model we'll do model.compile
loss equals keras losses sparse
categorical cross entropy
from logit equals true
and then let's do our model that fit so
you've seen all of this before
so i'm just gonna write it out pretty
quickly
all right so now we have a custom model
uh using subclassing and then we're just
defining our compile and our fit and our
evaluate
so just let's let's just make sure that
this actually works
so if we run this we see that it's
actually training and this should go
relatively quickly because
we just have yeah so we have a very very
small network like 64 nodes and then
10 output nodes let's actually talk
about what i want to show you
in this video so we now want to actually
create these layers by ourself
right now we're using the layers uh from
keras
and that has the dance layers in it and
then we're using
tf.n.relu and that's
that's all right and you can build very
flexible models using that
um that's actually in most use cases
that's fine
uh but some uh sometimes and just for
understanding you want to actually
be able to build those layers by
yourself so that you really understand
what's going on
and more under the hood so i'm going to
show you how to do that
and let's do class and let's do dense
and we're going to
inherit from layers dot layer uh
we're gonna create our init function so
we're gonna do init
and then self uh units and then we're
gonna specify the input dimension
so all we got to do then is uh we gotta
run the super method
so superdenseself.init
and we're going to do self.w is
self.add weight
so there are actually multiple ways of
doing this this is the more
easy way you could also do initialize it
by yourself
with a tf.variable but
this is the easy way i'm going to show
you uh in this way
now the first thing we got to do is we
got to set it to a name
let's just call it w and actually this
is quite important
you'll see in the next video how we can
save and load models
and i found out that if you don't
actually specify a name
you can't save the model so this is
very important and then we're just going
to do a shape
we're going to specify shape as input
dimension and then to unit
all right so input dimension is just
what we have in the beginning it's going
to be 784
which is 28 times 28 and then units is
just
what we're going to map it to so uh when
we're using layers 364 the unit is
64. then we can also specif
specify the initializer so we're going
to do random
normal and you could check out what what
other initialization methods
you can do and then we're going to
specify trainable
equals true and so
trainable equals true this is for layers
like batch norm and so on where some of
the parameters are not actually
trainable
but so all of our parameters in this
dense layer are going to be trainable
then we're going to do self.b is
self.add weight
we're going to call it uh b and then
shape
is just going to be units right
so we don't have so we're doing the
matrix multiply with with
w and that's why it has to have the
input dimension
but then it's just going to be unit
nodes so we're going to add one
for each of them that's why we just have
units right here
then we can do initializer and we're
just going to initialize it as zeros
and then this is also a trainable
parameter
and lastly we just have to do the call
method so call
of some inputs and we're going to return
tf.matrix multiply
with the input and then self.w
and lastly we just got to add b
so now we can actually replace
this right here so we can do
let's do self.dense one is dense
and then let's do 64 and then 784
and then let's do self.dense 2 is dens
of
10 and then 64 as input
so let's i'll comment that one right
there and let's see if this works
all right so we seem to get pretty much
equal
results as we did on the last one and
most importantly it actually runs
so one thing you can notice here first
of all is that on these ones
we didn't have to specify the input
dimension
and this is what we're going to call it
making the the layers uh
lazy in that you don't have to actually
even say what the input dimension is
it's just going to
work that out so that's what we want to
do now we want to
actually remove this part and make it
work
regardless of the input dimension so how
we can do that
is that we're going to create a build
method
all right so we have our init right now
and we're going to remove the input
dimension right here
and then we're going to do a build
method
so we're going to define build we're
going to have self and then we're going
to have an
input shape and then we're actually
going to create
the the w's right here so we're going to
paste that in in the build method
instead
and now what's so great is that uh
instead of using input dimen
input dim we can do input shape and then
we're gonna do
uh sort of the last of those so in
in this case we have the training
examples
on the first dimension and then we have
784
because of the way we've reshaped it
right here so that's why we do minus 1
here and then we're going to do
unit although what we're going to do in
the init method
is do self.units equals unit and then we
gotta
do uh replace these units right here
with
self.unit for both of the
uh for the w and then self.b
so what's amazing now is that if we
would run this we wouldn't have to
specify the input dimension this
uh hopefully will work so we can
actually we can do that num classes
so let's run this now and let's see what
we get
and it seems to work uh and now we see
that this
like these two are pretty similar right
the functionality of them
are pretty much identical and then you
might be saying well we're still using
tfn.relu
it would be nice to actually create this
ourselves as well and
uh so that's our next step you can do
this in two ways you can create a
function
or you can create a class like we're
doing so far
i think the most common way is actually
defining a function
but let's just create a class and you
can try out making a function
it's going to be pretty much the same
but we're going to do class
myrelu and then we're going to do
layers.layer
and we're going to do define init
of just self we're going to call super
of my relu
self and then dot init and then for our
actual
call we're just going to return
tf.math.maximum
of x and 0. this is just going to return
the maximum of x
or zero which is exactly the the
relative right
so at this point you might be saying
well how would we actually create
this tf math maximum function uh
you might be feeling that this is a way
of cheating and
and so what i would say is that this
would be even more low level and this is
something you can
try out and you could read the
documentation in
the source code for how they've actually
implemented this function and so on
but there will always be times where you
can go even deeper and explore
uh the details and uh so this is where i
would draw the line and that we can use
these
these mathematical operations on on
these
tensors so when we have this myrelu we
can do
self.relu is my relu so we gotta
instantiate the class although if you
use a function this wouldn't be the case
uh so what we got to do then is we got
to replace this tfn.relu
and then we're going to do ma self.relu
on top of that so uh
we can run this first of all so
now you've seen how to build these
models by yourself with keras
subclassing and then
also how to actually build these layers
like
dense layers and relative functions so
these are pretty
simple ones but you can imagine building
more complex ones as well
and i also suggest you to try that out
alright
so that's it for this video thank you so
much for watching and i hope to see you
in the next one
[Music]