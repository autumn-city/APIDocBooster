so in this video let's learn about
recurrent neural networks how they work
how they learn and also some of the
improvements that were made on them over
time so let's get started a nice thing
to do before start talking about rnns is
to remember how neural networks look
like so if you remember in a normal
neural network or a deep neural network
you can call it
we have an input layer a hidden layer
and an output layer and information runs
sequentially so first the information
comes to the input layer then it is
passed to the hidden layer and then
finally the output is produced so all of
the data points that you have you give
them at the same time all of the
features of your specific data point
that you're training on the difference
in neural networks of recurrent neural
networks is that they are recurrent so
that means that the same thing happens
over and over again in a way
um so this is what a recurrent neural
network looks like when it's unrolled
and i will also show you the rolled firm
but basically what you need to know is
that each of these little sections are
actually
one of the same they're the same network
or they are the same neurons so what
happens is these are all the different
input features and you give them one
after another so you first give the
first feature of your input
and output is calculated and also pass
to the second time step we call them
time steps
and then the second feature of your
input data input or a data point is
given
including the output of the first one
another output is calculated and so on
and so forth
so when you put it when you show it as
it is actually and how it is is that it
is just one
unit
so what happens is you give it an input
output is calculated but the same time
output is passed again the second
feature of the input is given so when
you compare it to neural networks what
happens is in neural networks you give
all of the features at the same time
whereas in the current neural networks
you give all of the features one step at
a time so it is basically things happen
in a time step manner so we would call
this the first time step the second time
step third fourth and fifth time step so
basically in each of these time steps
except the first one you have two inputs
one of them is the output from the
previous time step and one of them is
the input of this time step so how do we
calculate the output of recurrent neural
networks well we do it with this formula
it's actually quite simple even though
it looks like a complicated math formula
thing
you have weights for your input so for
each time step you have the weights
and for all of the outputs from the
previous step you have another set of
weights
and you also have the biases all of this
is of course passed through an
activation function and at the end you
have the output of your recurrent neural
network one thing to understand here is
that even though we have it looks like
we have many steps as i said it's
actually just one unit so
w x and w y which are the weights of the
previous the output of the previous
layer and the input of this layer or
time step is the same because they're
actually the same here we're just
showing them in an unfold way because
it's easier to understand so there's
only one set of
weights for the input and one set of
ways for the output so there is a very
common way of showing cells of
rnns because these cells here how the
output is calculated can be different
and we will talk about different types
of cells too but let's talk about how to
depict a simple rnn cell
so what we normally have of course as we
talked about the input at the output
from the previous step time step and the
input of this time step so these t's
basically depict this time step t this
time step t minus 1 is the previous time
step
so to show this formula on a diagram we
can say okay there is a multiplication
happening here this is multiplied with w
y and here another multiplication in
this is multiply with wx
and then we also add the bias here so
that's why you know the crosses and the
pluses here and this all goes through an
activation function of course and very
commonly what's used is the hyperbolic
tangent function
um but of course you can use other
things too but generally hyperbolic
tangent function is the one that is
being used with simple rns and at the
end we pass the output of this time step
to the next time step and also we output
it
but
sometimes this is not the case so for
simplest of rnn cells this is the case
where you pass the output to the outside
world and also to your next time step
but sometimes you
do an extra step
on before you output something so you
maybe pass it through a soft mag
function if you want it to be between
zero and one instead of minus one and
one because that's what the hyperbolic
tangent function where it will produce
uh then basically what you pass to the
next state will not go through the
softmax function that extra step and
then it will still pass it to the next
function next time step so
it still is the same thing but
has seen less processing so then what
happens is we call this the hidden state
of this time step so the hidden state of
this time step is passed to the next
time step so this is just a
interesting difference to keep in your
mind that you do not always just pass
the output that you get to the next time
step but sometimes you call it the
hidden hidden state there are of course
a bunch of ways how you can use this
architecture because you do not always
have to pass all of your or output all
of the outputs that you calculate in
your cells so let's look at a couple of
different options the first type is
sequence to sequence rnn model so
basically for every input that you give
in each time step you get an output for
that time step and for these kind of
things you can use for things that where
you're forecasting things so for example
price price forecasting or stock
exchange forecasting sort of things
uh the second one that you can use is
called sequence to vector sometimes it's
called sequence to single because you
only get one output at the end of the
whole rnn uh network that you have um
and these things that you use for let's
say you have a sentence or you have an
email and at the end of the
or as an output of the model what you
want to know is is it scam or not or for
example sentiment analysis like what
kind of sentiment does it have is it
negative or positive that kind of things
so for that in each time step you give
your network a input and then you just
ignore the outputs even if it's
producing the outputs you do not look at
them the only output that you look at is
the one at the end because
before seeing the whole sentence your
model cannot come to a conclusion of
course another one that you have is
vector to sequence or as i said single
to sequence uh by the way the reason
that they are not called single but
vector here is that because the output
that you have is in the form of a vector
most of the time and it is not just a
single number that's why we just call it
vector instead of single but
both ways are fine so in vector the
sequence sort of architectures what
you're doing is you're giving the
network one input and you are letting it
output a sequence of things so this
could be for example you give your
network an image and then you're having
it output a explanation of that image
word by word so let's say you give a
photo of a dog running on a beach then
for each of these time steps the network
will output
a dog running on the
beach for example so those are the kind
of things that you would use a vector to
sequence uh architecture for
and lastly we have encoded decoder sort
of architecture so in these kind of
architectures rn architectures at first
you are only giving your network inputs
so input input input for a couple of
time steps or how many uh ever that you
need and then you get outputs
so and then in the second part you only
get outputs you do not give any inputs
and these kind of architectures are good
for translation because to translate a
sentence your network needs to see the
whole sentence words because
meaning of some words might change the
translation of some words might change
if you
see the whole sentence based on the
context so that's why you first give it
the whole sentence and then you give the
trend get the translation word by word
on the decoder part all right but how
does rnns learn so how does the training
work it's actually
quite simple it's very similar to
normal neural networks that do not have
any interesting architecture
but we just call it back propagation
through time so what happens is the
output of the network is calculated of
course and we do it in you can think of
it as like it's unrolled for a form so
at first you give it input as
zero and then uh or the hidden states
from the previous time step because it
doesn't exist as zero you give the first
input you get a first output you give
the second input you get a second output
and then you just calculate which
whatever you want to calculate and then
we calculate the cost of this uh network
but as we said in the previous slide
sometimes the network how you use the
net network might change so you might
want to ignore the first couple of
outputs or you maybe you're just
interested in the last two outputs or
maybe even you're just interested in the
last one so based on that the cost is
calculated and then based on that as we
did with normal neural networks we
calculate the gradient and then the
gradient is passed back uh through the
network and the weights are updated but
as we said all the weights of all of
these time steps are actually the same
because they're actually one time step
and then the gradients are calculated as
we do with normal neural networks and
then these gradients are passed back in
the network to update the weights but as
i said you might ignore some of the
outputs while you're calculating cost so
the gradients are passed back only
through the ones that you
used in the cost calculation so rnns are
actually really good for analyzing
sequential data so this could be time
series data text or audio files for
example but of course they have some
shortcomings so the first one is that
they have unstable gradients
and you can imagine that right because
it's a very long sequential sort of
architecture the further back you go in
this architecture
the smaller your gradients are going to
get so you might not be able to update
the weights on the previous timestamps
in a way or to previous time steps in a
way that will be helpful for the whole
network
what you can do for this problem in iron
ends is use other techniques that we use
for normal neural networks too or just
deep neural networks too to deal with
unstable gradients or you can use layer
normalization instead of batch
normalization because batch
normalization is not as effective about
your current neural network so they're
kind of tricky to apply to recurrent
neural networks so instead you can use
layer normalization and another problem
with simpler noun cells is that they
forget if you give it a very long
sentence
it tends to forget what was being said
at the beginning of the sentence so the
applications that are created with it
are not really
effective or they don't work as well so
instead we have the lstm or dru cells
that we can use to make sure that we
still remember the beginning of a
sentence at the end of it so let's see
what a lstm cell looks like if you
remember how i showed you in that
diagram the rnn architecture the simple
rnan cell looks like it will be easier
to understand this one here
so what we have is a couple of sigmoid
activation functions
and then we have another hyperbolic
activation function and you know we here
see that there are two hidden
states that are passed from the previous
time step step to us and then we again
pass those two different hidden states
to the next time step
and then we again have an input and then
we have an output so let's look closely
what all of these things mean so the
first things that we need to understand
here is that as i said we have two
hidden states coming and going to the
previous and next time steps the first
one is the previous one as we talked
about is just a hidden state from the
previous step but the other one c
is a long-term memory hidden state so
basically as you say there are less
things happening to this hidden state
and we either forget or add some things
to this long-term hidden state and it
passes to the next time step without
much
coming out of it or going into it
um why do i know or how do i know things
are being added or extracted from these
time steps well because
we have the forget gate here we have the
input gate here and we have the output
gate here so basically these are the
gates that
create the information workflow or the
how this information is used in this
specific time step so let's talk a
little bit more in detail about that so
what we have here are called gate
controllers so anything other than the
hyperbolic tangent function we call the
gate controls and what the gate
controllers do is they either input zero
or one and in this way they determine if
something is going to be forgotten if
something is going to be input or
something is going to be output from the
long term state and how this works is
basically in the forget gate we decide
which part of the long-term memory
should be removed from the long-term
memory so you know we're saying okay
this can be forgotten now we don't need
to unders we don't need to remember this
information anymore in the input gate we
decide which part of the hidden state or
the information that we just added to
the hidden state needs to also be passed
on to the long-term memory so we're
deciding okay actually this piece of
information is important to remember
later and in the output gate we decide
which part of the long-term state we
need to extract from the long-term state
right now and use as an output either as
in the hidden state to pass on the next
timestamp or as an output in this or as
part of the output that we generate in
this uh time step so it sounds kind of
complicated even though it's sort of
intuitive that there is one long-term
memory one short term shorter term
memory and then either forget things
from the long term one either use it or
add new things and it's kind of
confusing to understand okay but like
how does this thing hold work why
why do we forget input or how does the
forgetting and inputing happen
um but you don't have to understand
everything behind it you don't have to
understand how it's all working
basically what you need to know is that
it is working and this is the intuition
and i think that will get you
where you need to go so you don't have
to obsess over how this hole works
so another different kind of cell that
we have is called a gru cell dru cell is
basically like a simplified version of
the lstm cell you know you have less
things happening here
you do not have a separate output as you
can see the hidden state that is passed
to the next time step and the output is
exactly the same and here the gate
controller r decides which part of the
previous state will be shown in the main
layer so main layer being
the one where we pass through the
hyperbolic tangent and basically the
main state that is being passed to the
next time step the r decides which part
of the previous state that we got from
the previous timestamp is added to the
timestamp and the output of this or the
state and the
output of this time step and that's it
that's kind of like a first step at
rnn's kind of beginner level information
that you need to know if you want to
learn more about rnns and deep learning
in general go check out my course deep
learning 101. i will leave the link in
the description below before you leave
don't forget to give me a like and maybe
even subscribe to show your support i
would also love to hear your opinions
about this video or any questions that
you have in the comment section below
but for now thanks for watching and i
will see you in the next video