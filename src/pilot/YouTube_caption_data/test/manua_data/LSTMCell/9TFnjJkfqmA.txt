So, with that motivation let us go to the
next module, where we will talk about Long
Short Term Memory and Gated Recurrent Units
ok
So now, all this was fine in terms of ok.
I gave you a derivation on the board and say
that, this is not required, but can I give
you a concrete example, where RNNs also need
to selectively read, write and forget right
only then you will be convinced that this
kind of morphing is bad in the case of RNNs.
So, I will start with that example and then
once we agree that, we need selective read,
write and forget. How do we convert this into
some mathematical equations? Right, because
conceptually it is fine, but you have to write
some equations. So that the RNN can do some
computations, where you have selective read
write and forget right. So, that is what we
are going to do over the rest of the lecture.
So, first let me start with the concrete example,
where you want to predict the sentiment of
a review using an RNN. So, this is the RNN
structure, we have done this in the past that
you have a sentence, 1 word at a time is your
every time step, you will feed this to the
RNN and at the last time step, you will make
a prediction. And as I said the RNN needs
a document from left to right and by the time
it reaches the end, the information obtained
from the first few words is completely lost
right because, it is a long document and you
are continuously writing to the same cell
state.
So, you will lose the information that, you
had gained at the previous time step. But,
ideally we want to do the following, we want
to forget the information added by stop words
like a, an, the these do not contribute to
the sentiment of the I can ignore these words
and still figure out the sentiment of the
document.
I want to selectively read the information
added by previous sentiment bearing words.
So, when I have reach the last time step,
I should be able to read everything else,
which had some sentiments before it and focus
on those words just I want to selectively
read from these sentiment bearing words and
also I want to selectively write the new information.
So, I have read the word performance, now
I want to selectively write it to the memory,
whether I should write it completely or should
I only write parts of it or not that is what
I need to decide. So, that is fair, this is
a typical example, where RNN also when it
is dealing with long documents, it needs to
understand what is the important information
in the document that needs to be retained
and then selectively read, write and forget
ok.
So, I am spending a lot of time on this analogy,
because you need to really understand that
this is important and this is where RNN suffer
right, if you are using them for very very
long documents, if we have document of the
size 1000 words, which is not comm, which
is not uncommon right because Wikipedia pages
have much more than that per document. So,
it is going to be very hard to encode the
entire document using an RNN not that it is
going to become significantly easier with
LSTM or GRUS, but to certain extent it will
become easier ok
Now the next part is how do we convert this
intuition into some mathematical equations?
Right so, let us look at that. So, in this
diagram recall that the blue colored vector
is called the state of the RNN, it has a finite
size, so now I will just call it as s t belongs
to some R n and the state is analogous to
the whiteboard and sooner or later it will
get overloaded with information and we need
to take care of this right. So now, our wish
list is selectively read write and forget
ok. So, let us start with that.
So, what we want to do is that and this is
the problem definition now, that we have computed
the state of the RNN, this is a blue colored
vector although, it is not blue, but this
the blue colored vector from the previous
diagram, where the state of the RNN was computed.
I know what the state is at time step s t
minus 1. Now I want from here to here go from
here to here; that means, from s t minus 1.
I want to compute the new state of the RNN
right. So, I had something written on the
whiteboard, I want to write something new,
I want to change the state of the whiteboard
and this is the new information that has coming
to me right the x t is the new information
at time step t.
And while doing this I want to make sure that
I use selectively write, read and forget.
So, these 3 operations have to come in somewhere
in the between. So, that I am true or faithful
to the analogy, which I have been giving right
that is the this is the our problem definition,
now going from s t minus 1 to s t and introducing
these 3 operations along the way that is what
we are interested in doing.
I will go one by one; we will implement each
of these 3 items right. So, we will start
with selective write. So, recall that in RNNs
this is what happens at every time step t,
you take the previous time, step previous
cell state, you take the current input. Do
you recognize the operation here? How many
of you recognize the operation here? Raise
your hands ok. So, this is nothing, but the
following operation and as usual I have ignored
the bias. Ok? Is that fine. So, that is what
I am representing it as.
But now so, one way of looking at it is that,
when I am computing s t, I am reading or I
am taking the whole of s t minus 1. So, once
I have computed s t minus 1, I am writing
this to my whiteboard and then whole of it
would be used to compute the next time step
ok, but instead of doing this what I want
is, I want to read only selective portions
of s t minus 1 or rather I want to write only
selective portions of s t minus 1. Once I
have computed s t minus 1, I do not want to
write the whole of it, because then the whole
of it will be used to compute the next cell
state, I do not want that I just want to selectively
write some portions of it ok.
Now, in the strictest case since, I know that
s t minus 1 belongs to R n. It is an n dimensional
vector in the strictest case, what I could
have done is I could have used a binary decision
that of all these n entries, I am going to
read some entries and ignore the others. So,
all the other entries I am going to set to
0 fine, that is the strictest thing, that
you could have done. Now for any of these
strictest things, what is the soft solution?
So, for binary what is the soft solution?
Binary 0 to 1. So, what is the soft solution
for that? Between?
0 to 1 so, read some fraction of each of these
dimensions right. So, let us try to understand
what I am trying to do here ok. So, and the
third bullet some of these entries should
have gone to 0 right ok.
So, instead of doing this, what we want to
do is we have this vector, which has n entries
this is the cell state at t minus 1. I do
not want to write the entire vector onto the
final cell state, what I want to do is, I
will take some fractions of it is say 0.2
of this 0.3 of this 0.4 of these and then
write only that, do you see the operation
that I am trying to do right. I want to take
some fractions and write only those to the
cell and as I said this is the softer version
of the hard decision, which would have been
0 for this 1, for this again 0 for this and
so on right.
How to do this? Why to do this? All that is
not clear, I am just telling you the intuition,
how and why will become clear later is that
fine? Ok. So, we want to be able to take s
t minus 1 and write only selective portions
of it or pass only selective portions of it
to s t.
So, whenever we compute s t, we do not want
to write the whole of s t minus 1, just want
to use selective portions of that. So, what
we do is we introduce something known as a
gate and. So, this gate is o t minus 1 ok,
we take the original cell state s t minus
1 do an element wise product with a gate,
which is known as the output gate and then
write that product to a new vector, which
is h t minus 1 ok. So, initially this will
look confusing, but it will become clear,
by the end of this lecture ok. So, is that
fine, this is what I am trying to do. Again
how to do this is not clear, but this still
matches the intuition, which I have been trying
to build that I want to write only selective
portion of the data, which I already have,
is that fine ok. So, each element of o t minus
1 gets multiplied by the corresponding element
of s t minus 1 and it decides what fraction
is going to be copied. And this o t minus
1 is going to be, between 0 to 1.
But how do I compute o t minus 1, how does
the RNN know what fraction of the cell state
to get to the next state? How will it do it?
We need to learn something whenever you want
to learn something, what do we introduce?
Everyone,
Student
Parameter sorry, what did you guys say, back
propagation. Back propagation will do what?
It will work in the air or propagate to what?
.
Whenever you want to do some kind of a learning,
I want to learn some function, what do I introduce?
parameter.
Parameter right so, that is what we are going
to do, we are now going to introduce a parametric
form for o t minus 1 right. And, remember
this throughout in machine learning, whenever
you want to learn something always introduce
a parametric form of that quantity and then
learn the parameters of that function, do
you get this how many of you get the statement?
Ok this is what we have been saying day from
right from class 2 or class 3 right.
Always introduce a parametric function for
your input and output and learn the parameters
of this function. So, that is exactly, what
I am going to do, I am going to say that o
t minus 1, is actually this function. I am
just giving some time to digest this. So,
this is at time step t minus 1. So, it depends
on the input at time step t minus 1. It also
depends on the output at output means whatever
comes out of this right. So, the same operation,
what have happened at time step t minus 2.
So, whatever was the output at that state
it will also depend on this.
You just take a while to digest, this equation
you will see at least 6 more equations of
this form in this lecture. So, if you are
comfortable with one all of them, would be
clear. So, try to connect the whole story,
I have s t minus 1, I do not want to pass
it as on pass it on as it is to s t. So, I
am computing some intermediate value, where
I will only selectively write some portions
of s t minus 1 and selectively write in the
strictest case, it should be binary, but that
is not, what we are interested in. We introduce
fractions if the fraction has to learn binary
let it learn, but we will make it fractional;
that means, we will make it between 0 to 1.
Hence, the sigmoid function right, remember
in one of these lectures, we had said that
sigmoids are still used because in RNNs and
LSTMs; remember in we had said that sigmoids
are bad use stanage or use ReLU, but we had
ended with sigmoids are still used in the
case of recurrent neural networks and LSTMs.
So, this is where they are used ok, how many
should get that connection? Ok good fine.
So, we use sigmoids, because we want the fraction
to be between 0 to 1 and we also want some
parametrization right and this is the particular
form that, we have chosen. There are various
equations possible various things, you could
have done here. In fact, there are 10 to 15
different variants of LSTMs, I am covering
the most popular one, which uses the following
equation right. So, it is says that this is
how you will compute the output gate and that
gate will regulate, how much of the cell state
should be passed on from t minus 1 to the
next state? Ok. Everyone clear with this ok.
So now, if you are clear with this give me
an equation for H t minus 1.
.
Loudly, everyone. s t minus 1 is that fine
right. So, this is the equation that we will
have. So, we have done selective writing and
these parameters are no special, they will
be learned along with the other parameters
of the network ok. So, let us spare some thought
on that. You got a certain loss at the output
ok. Earlier, you just had these parameters
W, U, V which were the parameters of RNN,
which you are adjusting to learn this loss.
Now in addition, you also have the flexibility
to adjust these parameters.
So, that if the lost could improve by selectively
writing something then, these parameters should
be updated accordingly right, may be you are
being over aggressive and making o t minus
1 to be all ones; that means, you are passing
everything to the next state right. Now it
has the chance because, they have introduce
parameters, if it helps the overall loss,
it better make these fractions more appropriate.
So, that only selective information is passed
to the next state, how many you get this intuition?
So, that is why anytime, you introduce parameters,
you have more flexibility in learning, whatever
you intend to learn.
There is remember, one clear difference here
right and that is where I said that, while
I was giving the analogy, I was really setting
up things, but here there is one distinction,
what is the distinction that is there? Ideally
what would, I have wanted. Suppose I take
the example of the review ok and the review
was say, the movie was long, but really amazing
ok. Now which is the word here, which is actually
trying to mislead? So, overall sentiment is
positive right, everyone agrees with that,
but which is the word which is misleading?
Long.
Long right; that means, I need to do what
with that word?
.
Forget that would, right. Now ideally, I would
have wanted someone telling me retain, retain,
retain, forget, retain, retain, retain. I
would have a label for each of these words
and then I could have a loss function which
tells me, whether my gates were actually athering
to this decisions or not? So, remember my
gates are learning some distribution o t minus
1, which tells me what fraction to retain?
And at this particular time step, I would
have wanted o t minus 1 to be all 0's ok.
I would have wanted to forget, but this kind
of not just o t minus 1, this will become
more clear, when I do all the other gates
also. So, what I am trying to say, is that
you should have had some supervision, which
tells you which information to retain and
which information to forget, but you do not
have this supervision right no one is telling,
whether these are the important words these
are not the important words?
So, that is the difference between the whiteboard
analogy, there you knew exactly which step
is important and which step is not important,
here you do not know that. All you know is,
that you have a final loss function, which
depends on plus or minus, whether the this
prediction is close to positive or close to
negative and what is the loss and that loss
is what is being back propagated. But the
difference, now is that you have introduced
a model, which can learn to forget some things
right. Earlier you did not have a model, which
could learn to write or read or forget selectively,
now you have introduced a model, this is a
better modelling choice right. So, the same
as we have had arguments that you could do
y is equal to W transpose x or you could do
y is equal to deep neural network of x right,
you are making different modelling choices
here and with the hope that one modelling
choice is better than the other choice.
So, just as RNN was one modelling choice now
you are using a different modelling choice
where again with the help of these gates and
all you can definitely write a function of
y is a function of the input and that function
is going to be LSTM function, which we will
see in detail. So this, one part of that function
and while doing this you are just making a
better modelling choice, which allows you
to learn more parameters and along the way,
if important do selective write, read and
forget is that clear? Right so, you would
see the difference, what would have been the
ideal case and what is it that you have? The
ideal case would have been explicit supervision
for what to forget, read and write, you will
never have that, but you are still making
a modelling choice, which allow you to do
that. So, if it required to model while back
propagation should be able to learn these
parameters. So, get you are able to do that.
I know I am repeating myself, but it is very
important that you understand this situation,
how many of you get this now? And as I said
these parameters will be learned along with
other parameters and o t is called the output
gate, because it decides what to output to
the next cell state, ok. Still you see that
there is a lot of gap here, we have not reached
s t yet we are still at s t minus 1, we have
computed some intermediate value. But we have
not reached s t yet and along the way we had
3 things selective write, read and forget,
we have only taking care of selective write
so, far ok.
Now let us look at selective read. So, what
this selective read, do you are going to get
new information at time step t, which is x
t right and now instead of this original cell
state, you have used the selectively written
cell state because that is what you have written
now. So, that is what you should use.
Now, using a combination of these 2, I am
going to compute, some intermediate value,
ok. And just stare at this equation. This
equation form is very similar to the RNN equation
form, right. Only thing is that instead of
s t minus 1, I am using h t minus 1 and for
good reason because, I know that h t minus
1 contains only selectively written values
from h t minus 1. Is that fine and x t is
the new input. Still there is some gap here,
I have not reached s t yet I am still at an
intermediate value. So, this is the new input,
which I have received. Now what should I do
with this new input? Selectively read this
input I do not want to take all of this input
because, may be the input which I have got
now is a stop word and I do not want to read
all of it right, do you get that?
So, now it captures all the information from
the previous state as well as the current
input and we want to selectively read this.
So now, what would you do to selectively read?
Again the same situation that, you have a
s tilde the answer is already here. You have
s tilde and you do not want to pass it on
as it is to s t, this is s tilde, s t is somewhere
here, which you do not know how to get to,
but you know that you do not want to pass
on all the input that, you have read, you
want to selectively pass it on. So, what will
you do now? Again introduce a.
Gate.
Gate and this gate will be called?
Read gate.
Input gate or the read gate right ok.
So now, what can you give me an equation for
the gate i t is equal to sigma of that is
good because, sigmoid is what we need it is
going to be a fractional thing. Let me add
the easy part. W into.
H t minus 1.
H t minus 1, that is telling you what has
happened so far and U times x t. You see the
same equation, same form the parameters have
changed. So, these we will call as W i U i
and V i and they are depending on the input
as well as the previous state, previous temporary
stay that we had computed ok. So, that is
exactly, what your input gate is going to
be and now this operation is the selectively
reading operation. How many you are fine at
this point? Ok and then this product is going
to use to be it is will help us to read selectively
from this temporary value that, we have constructed
or the input that we have taken ok.
So, far what do we have, we have the following,
we have the previous state, which was s t
minus 1, then we have an output gate, which
was o t minus 1 using these 2, we have done
selective write right, we have taken the previous
state and the gate and then a selective write.
Is that fine? ok. We need to check, if the
sigmoid should come here because, sigmoid
is already there in the computation of s t
minus 1 right. Oh it is not there. So, this
already has 1 sigmoid right yeah. So then,
again a sigmoid on that is it there? Ok we
will figure it out, just check the equation
right.
So, there may or may not be the sigmoid, the
sigmoid already applied to s t minus 1, but
we can figure that out ok. So, this is the
selective write portion then, you compute
the current temporary state ok, and just look
at the similarity between these equations.
Then you have an input gate and using these
2, we have done a selective read ok. So, you
have taken care of selective write and selective
read, but you are still not reached s t, I
still do not have an arrow here, I still need
to figure out, how to compute the s t finally,
ok. So, what is the operation which is remaining
now? Selective.
Forget.
Forget ok. So, what do you think should we
forget? We want to find new s t. So, let us
see what we will forget right.
So, the question now is that you had this
s t minus 1 and now you have a temporary state
s tilde t which is here. How do we combine
these 2 to get the new cell state? Ok. So,
the simplest way would be, that you say that
s t is equal to whatever was there in s t
minus 1 plus selectively reading from the
current input. Is that fine, this is the one
way of doing it ok, but now what am I doing
here, what is the problem here? I am reading,
I am taking s t minus 1 as it is right. So,
what should I do? I should forget some parts
of s t minus 1. So, what should I do for that?
Introduce a, what gate?
Forget gate.
Forget gate right. So, we may not want to
use s t minus 1 as it is, but we are want,
to forget. So, there is at this point all
of you should get some confusion, if you do
not then, I would be worried, if you are getting
some confusion good right. You should all
get confused at this point. Why are you confused?
Because you already did selective write and
now again you are doing a selective forget
also right, but there is a difference because,
the selective write was then used to compute,
how to read the information right, but now
once you have read the new information, you
want to see how to assimilate it back with
the old information that you had right. So,
that is why you introduce a separate gate.
So, think of it as this way that you are keeping
these functions separate input, output and
forget. So, they can separately learn things
ok.
So, whatever you want to selective write let
it be a separate function. These h t minus
1 is not going back to s t right. Let us just
by use so that, you can compute these temporary
states. So, that is what is being passed to
the next temporary state. Let i t only decide
how much of this input should be read? Ok
and then when you want to combine these 2,
just use a separate gate and this exact idea,
which is confusing all of you, why have a
separate write gate and a separate forget
gate? Led to something known as gated recurrent
units, where they merged these to gates we
will get back to that ok.
So, at this point it is fine. I am just telling
you the original equations for LSTM and this
was the motivation that they had. So, as I
said there are at least 15 to 20 different
variants of LSTM, which use different equations,
they tie some of these weights. So, one thing
could be that forget is the same as 1 minus
remember. Right? Or output could be same as
1 minus input right, you could have tied these
gates instead of learning separate parameters
for that.
So, in the most parameterized form you have
a separate parameter for all of these ok.
So, we introduce the forget gate again, can
you tell me the form for this forget gate
f t is equal to first term.
W f.
W f, second term U f, what will be there in
the second term x t and the first term ok.
So, this is what it will look like ok. So,
if you remember one of these equations, you
will be able to write all of these, not that
I am going to ask you to write them in quiz
or something, but why take a chance. So, and
then once you have completed the forget gate,
instead of this equation, can you tell me
what is the equation and you are going to
use? What is the first term going to be it
is s t minus 1 here? What is it going to be
now?
F t into.
F t into?
S t minus 1.
S t minus 1 that fine ok.
So now, we have the full set of equations
for LSTM, we have certain gates and certain
states, what are the gates? Output gate.
Input gate.
Input gate.
Forget gate.
Forget gate, why do you guys has this momentary
amnesia like suddenly you forget everything
ok. So, output gate, input gate and forget
gate all of these are the same form with different
parameters ok. What about the states, which
are the states that we have completed? One
was s t the other was h t and the third one
was s tilde t ok. S tilde t from s tilde t,
we get s t and from s t, we compute h t ok.
So in the diagram, that you see here at the
top. Tell me which are the computations, which
are happening at one time step. At time step
t, which are the computations, which are happening?
Is it I will give you the options right, is
it this or is it this let us call this 1,
let us call this 2 or this 3 or this 4. Which
are the computations happening at one time
step? And you see the order also here this
should be straight forward right why?
.
How many of you say 4? That is the one right
because, you start with selective reading
right and you can just go by this right these
are all indexed by t right is that fine ok.
So, these are the computations, which happen
at time step t and these are exactly the computations
which were written right. So, we have the
3 gates, which you need to compute at every
time step and you have the 3 states, which
you need to compute at every time step. Is
that fine? And this s t minus 1 is not being
computed. It is just taken from the previous
time step is that fine. So, you have these
6 computations, which happened at every time
step and the output final output of an LSTM.
So, when you use tensor flow or something
the output of an LSTM, would give you 2 things.
It will give you h t comma s t because, these
both the states that are being computed, one
is the running state and another one is the
current state, which is being computed ok.
And I choose the notation S because, that
is what we have been using for RNNs, but in
LSTM in all the literature instead of S, you
will find it to be ct because it is called
the cell state. So, that is why s t ok. So
all these equations wherever, you see an s
when you are reading some standard blogs or
things like that you will see C instead of
S. So, you just do this mapping in your head
ok.
So, LSTM actually has many variants with include
different number of gates and also different
arrangement of the gates. So, as I was saying
that you could say that input is 1 minus output
or input is 1 minus forget or things like
that and also why this particular parametric
form right why not make W 0 into s t minus
1 instead of h t minus 1 and so on. So, the
all points, of things that you could do or
all of these are valid these are all valid
variants of LSTMs.
So, there is this paper called LSTM, a search
space odyssey. So, you can go and look at
I think we link it in the in the reading material
right ah. So, you can see that there are actually
many many variants of LSTMs, but this is the
most standard and default variant, which you
will find in most platforms on tensor flow
or pytorch form.
And there is another very popular variant
of LSTMs, which is called gated recurrent
units. So, we will just see gated recurrent
unit. So, I will just give you the full set
of equations for GRUS. So, you have gates,
but unlike LSTMs, you have only 2 gates, you
have an output gate and you have an input
gate, you do not have the forget gate ok.
So, what am I going to do for the forget gate?
So, this is what I am going to do, you see
the last equation. So, instead of forget gate
I am just saying that this is what you are
going to selectively input from the current
temporary state. So, the rest of you rest
of it you take from the previous state right.
So, I have just tied the input gate and the
forget gate any other changes do you see in
this. So, earlier we had s t minus 1, everywhere
right now we have s t minus 1 itself is that
fine ok. So, the basic idea these equations
are many many and you could think of your
own equations, you could say that I will not
really use this input information at all or
I will choose to use it differently or what
not right there are several things that you
could do. At a very abstract level this is
what you need to, what is this?
.
So, these parameters could then make a difference
right, they could adjust it accordingly and
so on right. So, that is what I was coming.
So, the there are various ways of realizing
this right at the abstract level, you need
to understand that the original problem was
trying to store all the information from time
step 1 to time step T capital T right, which
is not feasible, because of this finite size
that you have. So, along the way we built
this intuition that it should be good to have
these operations, which allow you to selectively
read write and forget right. How do you mathematically
realize these operations? There are various
various choices for doing that and we saw
a few choices for doing that right, there
are many others you could have done. But this
is largely, what whenever you say that I have
used an LSTM, most likely you are using the
set of equations which I saw, which we saw
on the previous slide and whenever you are
using a GRU, these are the set of equations
that you will be using ok.
And again remember this, that there is no
explicit supervision here, it is just that
we have a better modelling choice we are just
introduce more parameters. So, that if required
these parameters could be adjusted to do a
selectively read, write and forget right.
So, it is often, it is often valuable, if
you are doing some task with RNNs or LSTMs.
You should visualize these gates right, you
should see that at time step t, if you thought
that it should have forgotten everything that
it has learnt. So far, because suppose you
had this the movie was long, but I really
loved it because, the direction was superb
and so on.
Now this word, but actually changes everything
right because, it whatever was written before
it does not matter anymore right. So, is it
really learning those kind of gates, where
everything before, but was forgotten right.
So, it would be helpful to visualize these
output gates and see what kind of values,
they are learning. What kind of things they
are remembering forgetting and selectively
reading and so on right. So, as I said I will
just again summarize the key thing here, is
the intuition and then the realization in
the form of equations, there are multiple
choices, we have seen a few of those right
that is what I will end with. And, in particular
in GRUS, there is no explicit forget gate
and instead of h t minus 1 you use s t minus
1 everywhere.