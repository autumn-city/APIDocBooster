Hello, good evening everyone
Sorry to keep you waiting
I was having dinner just now
So it's a bit late
Okay, let's talk about what I've been up to recently
I've been working on my Unity environment recently
And yesterday, I had a live stream
So I wanted to show you guys what my current environment looks like
I mentioned earlier that after I updated to Windows 11, it kept downgrading
It kept flashing
But yesterday's live stream didn't seem to have any problems
So I'll add a few more special effects for the live stream
Oh yeah, and I might have to change the character's image later
I'll edit all of them and use Unity for the live stream
That way, it'll look cooler
And I'll add some chat rooms
I'll add some special effects for the characters
After using Unity, I'll be able to do a lot of fun things in the city
I've been working on this recently
Okay, I'll talk about the schedule for this and next week's live stream
Tomorrow, there will be a viewer, Huang Jianxin
I don't know if I pronounced her name correctly
She's a viewer
She said she used PyTorch Lightning to make the Mipnerf
It's the method we introduced in our previous video
She told me that the code she wrote didn't work as well as the script
The code will be on the GitHub tomorrow
We'll try to find out which part of the code is different from the script
Or which part of the code is wrong
We'll look at the code tomorrow to find out which part of the code is better
Next week, I haven't thought about the full schedule yet
First of all, on Friday, I'll be talking to NoLi
She wrote that she didn't finish the adventure game last time
Even though we were chatting
If you're not too interested in the city, you can listen to our chat
She's very good at chatting
She's a chat master
We'll talk about the research she did with Python
About astronomy
On Saturday, there will be a city puzzle
The secret room escape
That should be on Saturday
The schedule for next week will be these two
I was thinking...
Taiwan also has a holiday next week
It's a three-day holiday
From May 1st, Labor Day, to Monday
In Japan, we call it Golden Week
There are a lot of national holidays
So I have a holiday next week
I don't have to go to work
So I have a lot of time
Other than being my Unity
I'll probably do a live stream introducing my thesis
The theme should still be Nerf
I'll start with something I'm familiar with
Something you might be interested in
I'll probably talk about...
Neuro scene flow fields
Nerf can be extended to a scene with moving objects
I'll probably talk about this thesis
You just need to input a single eye...
It's lagging
A single eye video
You can cut out the time and space
You can set the time and space
It's a pretty cool method
That's all for today
Today, I'll be writing a thesis
This is the content
Coordinate MLP
The background is...
When I was introducing Nerf
Let's review Nerf
Nerf is a 3D position
X, Y, Z
You can see the angle of the point
Input these
After a MLP
The output point...
This part
A 3D point has X, Y, Z
You can see the angle of the point
Theta Phi
It's an angle
After inputting these to a neural network
The neural network will output the RGB color
Sigma and its density
If you just simply
The angle of the point
For example, X is 0.1
You input 0.1
The result you get will be very blurry
Nerf thesis is in the following part
Let me search
It's not written here
It should be
Search
Here
If you just simply
Input X, Y, Z
Directly input to the neural network
The result you get will be like
The picture on the right is very blurry
In the nerf
They proposed a method called
Positional encoding
They just
Input X, Y, Z to the neural network
Before inputting to the neural network
Make it a sine and cosine
These linear combinations
After inputting like this
You can get the middle
The second picture on the left
Very high density
High density picture
This
When I introduced it before
Everyone thought it was black magic
You just need to add a sine and cosine
These linear combinations
Can make a blurry thing
Become so clear
Today, I will do this
Just compare
We didn't add positional encoding
And the difference between them
This is the first one
Then
In addition to this positional encoding
There are actually other researchers
Also studied a lot of different methods
Can make this
The picture that was originally very blurry
Become very detailed like this
As far as I know
The one I introduced before
Fourier feature network
I will introduce this to you later
Is this
Fourier feature network
This is one of them
Then there is one before
Also caused
The big topic of this siren
This is the case
It is the activation of the neural network
The ones we are familiar with
Relu or tangent edge
Or sigmoid
They turned it into this sine
Then today
Should do these two parts
This is definitely
In addition to this
I am in this
This is the folder
Today
The code will be uploaded here
You can also help me start
In addition to these
I also listed other
Some papers about this
Then this is
If you have time, you will do them one by one
Most of the words are to put that
Siren is more special
No, it's not siren
It should be said that most of the words are the same as siren
Is to put that activation
Help us from the original familiar Relu
Change to other different types of activation
Then they experiment
Don't use Relu, but use their kind
Newly studied activation
The effect they get will be very good
Ok, then we today
Part of the actual work
Will refer to this
Fourier feature network
Originally, we are Nerve
Nerve is like this place
It is
A 3D point
Then there is its view
Then the color of the output point
But this
It will be a bit complicated to install
So we take one today
A relatively simple problem setting
Then the setting of this problem
Is the task setting
What is this task?
The task is that we want to put a picture
For example, today I will use this
The picture without
Then we want to put this picture
Use a nerve network
To restore it
What does that mean?
I can draw it here for everyone to see
For example, we have a picture
Can be a little thicker
A picture is a lot of pixels
Then I just draw it here
For example, it is like this
3 times 4 times 4 pixels
Then every point
Every pixel has its coordinates
What about this coordinate?
Originally in the whole number
For example, the upper left corner is 00
Then the lower right corner is this
33
Then in this
Our problem today
What about this task?
We will put it
Just normalize
We will put these coordinates
A formalization
That is, we will put 00
Become
1-1-1
The upper left corner becomes 1-1-1
Then the lower right corner becomes 1-1-1
So our problem today is
We want to define a coordinate
UV
Every pixel has a coordinate
UV
Then we have to train a nerve network
For example, this is our nerve network
A multi-layer
MLP
Then we have to output that point
Color
RGB
This is our task today
Setting
Compared to the 3-dimensional XYZ of Nerf
Then Theta Phi
Simplified a lot
We just need to know one
A pixel coordinate
Then we go
Push the color there
This is our simple one today
A problem setting
it is good
Then let's do it today
This part
If you do it, you can refer to
I introduced that last time
Pytorch lightning
And when I introduced this nerve last time
I said that the nerve network training
Is
Write this
The three major steps of deep learning
The first is that information
Theta, the second is model
What is this nerve network model?
Then the last part is loss
OK
Then we follow this order
Let's do it slowly
Then the first step is this
Theta part
Then I have this here
My that
I haven't
Now there is nothing on it
Then I am
First in my
Local environment
I wrote a part first
Then today
Actually, I didn't write too much
Most of them are still empty
Then today, I will write slowly
Correct
I am also the first time
The first time in the live broadcast
Write this deep learning
If you write too slowly, you can
Look at it patiently
Like this
Or if you have any questions
You are welcome to ask in the message area
Now everyone seems to have no comments
Correct
If you have any questions
You are welcome to ask in the message area
it is good
Then we
First write this part of the data
Ok, then what about us today?
The picture used
Is the same as him
Is this little fox picture
Then we also follow his this
How to cut the picture
Ok, but
If it is input, I will refer to this place first
Then
Because the original picture is bigger
Then he is
First cut out the middle
512 x 512 pixels
Then here we will refer to him
Here
Image pass
After we read in
Then we do one here
Ok, then don't do this
We will fix the size
512 x 512
it is good
Then here
According to his writing
Let's put
Read this picture first
Come in
This means the first three
Originally there may be alpha
Then he removed the last alpha channel
Then only RGB
Then divided by 255 is
Oh yes
The original picture RGB is
Between 0 to 255
Originally between 0 to 255
An unsigned integer
8
Then we will
Usually in this task
We will normalize it
Become 0 to 1
Like this
So the benefit of doing this
Is that we can use a sigmoid
Just get the result
So here is divided by 255
Then
Here is the middle part of the crop
Then just copy it here
Here is the center of the picture first
Then
Then write 256 directly
You can also write 256
Then r is its radius
It is 256 left and right
it is good
Then we have
There is this picture of this RGB
it is good
Then next
We want to get this picture
Every pixel of its UV coordinates
Then the UV coordinates
It is written like this
It is using lean space
This can also be
Or you can do it directly with np.meshgrid
Then my words are
Know one
Know one
That library is called cornea
This is also
When learning depth
It should be used by many people
One is
One about image processing
The library for deep learning
It is
The advantage is that
For example, we resize an image
Just resize a picture
You may use OpenCV or something
SideKit image
Those can be done
But their disadvantage is that they can't be calculated
Their operation
You can only
After the operation and the operation
Between these two
There is no way to calculate it
But this cornea is
Its advantage is that
When it is writing
All this kind of
Image processing
It is written in a state that can be divided
For example, you have one
A picture
Here
You can introduce this thing by the way
It's not difficult
For example, you have a picture
You have four pixels
For example, you have to
Resize to one pixel
Resize to one-tenth of one-tenth
In fact, this pixel
This pixel RGB color will be
For example, you use
Linear Interpolation
Linear
Linear interference
The color of this pixel
In fact, it will be
The original four pixels
Add up to four
For example, here C1, C2, C3, C4
That's it
And the last C
It will be
These add up to four
So this thing
In fact, it can also be divided
This thing is actually
Can be divided
But it's usually those
What OpenCV
They don't have this kind of demand
No need to divide this kind of demand
So they
Can't be divided
But this cornea
Its writing
The written data bag
Is to help you
All these operations
Can be divided
Then we
What we use is
There is a function called
Create Mesh Grid
This thing
Let me take a look
Cornea
Create Mesh Grid
It will
Is to generate a
It's a picture
Each of its pixel coordinates
That's it
The shape of the last tensor
Is E times H times W times 2
For example, the one on the left
The top left corner is 00
The bottom right corner is
This is 00
And then
The bottom right corner will become
So it will become
H minus 1
Then W minus 1
So ugly
H minus 1
W minus 1
Recently
The epidemic in Taiwan seems to be very serious
Everyone take care
In Japan
We can take a look
In Japan
A few months ago
And then
At the beginning of January
It was very serious
Or March
Or January
Oh yes
I think
I was in Tokyo at that time
More than 20,000 people
And then what else
Is it an emergency declaration?
Anyway, one
Is to restrict everyone
After eight o'clock in the evening
There will be no food
The restaurant will close
This kind of thing
And then
It seems to be
Slowly this is
Keep going down
And then it seems to be at the end of March
It was closed
So we are now
More suspicious
It seems to be more suspicious
And then
It seems to be more suspicious
And then it seems to be more suspicious
And then it seems to be more suspicious
We are now more free
How many thousand people are now
it is good
Just talking about this
Mesh Grid
If
Just give him an HW
He is a output
Each pixel of the coordinates
Then you can
Provide him this
Normalize coordinate
This parameter tells us if we want to correct all the coordinates of this pixel
If we correct it, the upper left will become negative 1 and the lower right will become 1
This is exactly what we need
So today we will directly use this function
The first parameter of this function is the height of the image
The radius of the image is 256, so the height and width are 2 times R
Normalize is set to true
This will be 1x1
Is it too dark?
The reason why I set it to dark is because
In Unity, if you have watched my test video, you will know that I plan to add a fire effect
It will move according to the sound of my keyboard and music
That thing is brighter, so I want to set it to black
Then there is a cool feeling that the flame burns in the dark
The reason why I set it to black now is because
This is multiplied by 2
Then we have UV
Next
Next
This is called selfR
Next, we need to get all the colors of the pixel
The color is directly obtained from this picture
We can use this to take out the first one
Just become like this
Ok
Then our rgb is equal to
Wait a minute
The image is originally 512x512x3
Then we can turn self.rgb into
It becomes the image of torch.tensor
Originally this is a non-py array
Or image.io's unique format
If you use torch.tensor, you will turn this image into a tensor format
I didn't know how to write this before, I used torch.
For example, if I use float, the green is float
If I want to turn it into a tensor with a vertex, I use this way of writing
But I found that it would be better to write like this
If you use torch.tensor, it will automatically help you find out what your current preset information is
The preset is of course float32
32 vertex
This will also be different according to your actual training
For example, if you want to train fp16
Only the number of 16 members
Then there will be different information formats
If you use torch.tensor, it will automatically help you find out what your information format is
The result will be 512x512x3
This tensor
Ok
Let's define init
The place of init is
Oh yes
This may not be mentioned in the previous iTorchLining tutorial
Before, it was directly written by Amnest
If you want to create a data set
The method is to inherit the data set in the torch
Then create a class
This class contains at least three parameters
The first is init, initialization
The second is land
How many data are there in this data set
And the last is getItem
You can get every data
This class contains at least these three parameters
The place of init is usually to read all the information
We have finished reading
The coordinates of all the pixels and the color of each pixel
The length of the data set is very simple
The square of selfR
No, it's 2 times selfR
This is the number of pixels
So it's 256x2, which is 512x512
Finally, we have to define getItem
We have to get every data
This is not difficult
Wait a minute
Wrong
I said the wrong place
This thing is all the pixels
But in the paper, it doesn't train all the pixels
I will show you the coordinates
It trains every four pixels to train one pixel
How to say
Let's use this picture
For example, this is 4x4
The training pixels it uses will be these
Take one out of every four
These will be the training pixels
The test will be the rest
These are the test
So ugly
Let's use the color to distinguish
This is train
This is test
Why is it like this
You can't train all of them
If you train all of them
The training will be overfit
You don't know if this model is good
So we have to see the result
Although it is in the same picture
If you train, don't take all the data
Don't train all the pixels
Only take one part
In the paper, it takes one out of every four
The rest is the test data
So we actually
This size is not the square
It's the square of self-r
Just one-fourth of the value
For my getitem
Think about how to write
Or I directly
I directly create a trainUV and testUV
It's also possible
For example, I add a split in this parameter
This split is train or validation
These two
I can write here
Train or validation
Then our uv and rgb
It will be different according to this split
For example, when this train
If we split
Is this train
We take one-fourth of this
So the writing method is very simple
Is the symbol symbol 2
The symbol symbol 2 is to take one out of every two
It takes one out of every two
It seems to take one out of every two
The center is also taken out of every two
So in the end, one-fourth of the data will be obtained
Rgb is also the same writing method
So this will be 256 times 256 times 2
Then the following will be 256 times 256 times 3
If it is validation, it will not change
Then we don't have to write else
Because our split only has these two
Then lens
Here we can write
Return self.uv lens
Then this is
After taking it, we can flatten it directly
We can directly reshape it into N
N symbol
We can also use what we learned before
The INOPS introduced before
I won't say more
This thing
I said a lot of times before
When changing this shape
It is very easy to use
And it won't make mistakes
For example, this uv is hw times c
If you want to pay attention to this number
You also have to give it this
Give it an English letter
If it is e, you can use e directly
Then we use the first two together
So this is the writing method
Ok
After this comes out
No need to comment
Because this is easier to understand
Then the rgb is the same
Ok
Then the first
This uvrgb
The first shape of this tensor
Will be the number of all pixels
So we can use this lens
This method to get all
All the information
Or you can also write
self.uv.shape0
I personally like the writing method of land
I don't know why
But these two are the same
You can also write like this
This is the 0th place of the shape
The 0th place
Ok, this getitem inside
Not difficult
Then just return the things inside
You can use a dictionary
For example, uv is the index of self.uv
Then rgb is the index of self.rgb
That's it
One line is over
This is to get every piece of information
Get every piece of information
So the method of getting is directly labeled index
Ok, then
We have finished writing the part of getting information
Today's task is relatively simple
So it can be written faster
Ok, then let's define this model
For the model, we will first
The simplest is a uv input
Nothing
No positional encoding
Then there is no siren
There is directly an MLP
Ok, then we
Think about it
How to write here will be better
Because
Call a class called identity
I use a class called identity
Just call it MLP
Use a class called MLP
Then it's very simple
Is a
The dictionary uses a four-fold MLP
Then we can use it the same
Then we can add some parameters
For example, this number of layers is four
Then this hidden unit of each layer
Hidden units are expected to be 256 in the dictionary
Then we use the same
Then this one
Don't forget to be super init every time
Then the nn in PyTorch
This is the neural network.module
It needs at least these two
One is initialization
That's what everyone needs
Then the other one is forward
Is to define
When calling this module
What kind of action is it
Ok, let's define this MLP first
Then this thing is not difficult
We will use an NP
Sequential
What is the writing method of sequential
Haven't written for a long time
Because it was copied before
Let's take a look
Take a look at what I wrote before
What is written in the nerf
I should also use sequential
Just add it together
That is, each layer is
The first layer input is 2
Or I just
I kind of forgot
Should be able to pass a list
Let me check how to pass a list in this module
module list
nn.module list
If it's sequential
You have to define everything from the beginning
This module list is
You can give it a list
You first add all the layers
Into a list
Then use this module list
Turn it into a module
It should be like this
So let's write this outside
It's called a module list
At the beginning
Input is 2
Because it's UV
UV has two things
The first layer is like this
Then in for in range
nlayers.1
The first layer is already there
Then we are in the rest of these layers
The last layer is also special
Take a look
How many layers is it
If you write it like this
It's okay
Then we have to judge
Is it on the last layer
nlayers.2
Or I can just do this
If it's in the middle, then add
Add nlayers.1
Add hidden units
Then add values
like this
If it's the middle layer
Add nlayers.n
If it's the last layer
Add nlayers.3
Because the last output is RGB
So the output is 3
Then the input is 2
Then this network will be
module list
This thing
Remember to separate this list
Is it written like this?
I remember this
Or can
Or just write
Wait, I'm not sure about this
We can test it
We can use this
For example, use Jupyter Notebook
To test it
Hey
Just write it here
Wait, where did this go
I usually debug with this
Jupyter Notebook
Because it's interactive
So you can see the results right away
I want to make sure now
Is this writing method right?
I rarely use module list
Import torch
Let's take a look
MLP
Then you don't have to separate
One more time
Is this right?
Is the number correct?
I'm curious
Anyway, it doesn't matter if the number is wrong
The structure is like this
The input is a UV, so it's 2
Then the output is in the middle
After many layers of 256
The middle layer of 256
Then the last output is 3
Is RGB
But there is one last place to write wrong
Is the last output is not Renew
Because the last output is RGB
So RGB is between 0 and 1
So we will use Sigmoid
The middle layer is all Renewed
The middle layer is all Renewed
Then write the forward part
The X we enter in the forward part
It will be a badge times 2
That's the UV of the pixel
This is after the formalization
Then return here is simple
Then it will output a B times 3 RGB calculation result
Then our model is very simple
Just finished writing like this
Hello
Sequential
So using sequential is also possible
But you have to add a signal
understand
Are these two different?
We can try
I just used this one
Then we use
It should be the same
This is nn.sequential
Thank you Huang Jianxin
Let's see
This result should be the same
Now it becomes a sequential
Then the back is exactly the same
It should be either module list or sequential
So what's the difference between these two?
Let's check what's the difference between this module list and sequential
Just now the problem is asking this
I didn't care what he was asking
Module list is
He said that it cannot be exchanged
There are some situations that you can only use one
Wait
Hello Noli
Now write deep learning
Here he said module list does not forward
He did not define any neural network
So module list can't forward directly
He wrote like this
So we have to use sequential
We can try
For example, we are like this
Then use the one just now
See if it will be wrong
It should be wrong according to his statement
If it is module list
And I'm not familiar with this
So I feel like I'm doing some beginner
But you can also learn by the way
How to learn beginners
Deep learning
This is the relu
Then let's define this one
Then I give him an input
X is torch.tensor
For example, a thing of 10 times 2
Then zeros
10 times 2, then we pass to this X
Okay, no
Like he said, he did not define forward
What if it's sequential
Sequential is fine
Then we should not use this module list
We should use sequential
If it's sequential
Add a signal in front of it
The signal is in front of a list
It means to spread it out
We can also show it to you
Can't just use it outside
Can't just use it like this
You have to define it in a function
For example, return
Then like this
It means that the signal is in front of a list
It means to cut it
But you can't use it alone
You have to use it in a function
Okay, then our model
The simplest input is identity
This is already written
This is not difficult
The place of return is also written forward
Okay, then next
Let's go back to this place
We have written the model
OK, give him a check
The module list needs to be taken out of each layer with FORGE
Then I know
If you want to use the module list
In this FORWARD
You have to put it
For example, this NLAYERS
Then you have to put it
For example, you have to put it
Net 0 is equal to this
For example, the second one in self.net
Then you have to X is equal to self.net 0
Pass it one layer by one layer
You can write it like this
It should be written like this
If you use the module list, it should be called FORGE
Then take out each layer one by one
Then pass it one by one
If you write it like this
You might as well
The top place
Every layer
Don't write it like this
For example, you define this
self.layer 0 is equal to this
Then self.layer 1 is equal to this
Then define four like this
Then when you are here
X is equal to self.layer 0
Pass X
X is equal to self.layer 1
Pass X one layer by one layer
I might like to write it like this
Of course it won't be like this
Layer 0
Layer 1
Layer
Then you have 10 layers
Then you have to write 10
For Python
Let me teach you some tips
For Python
There is something called set attribute
Get attribute
For example, you have 10 layers
Let's say every layer is the same
That is to say
You can set attribute
self.layer
Then I like this
Then set it as something
For example, set it like this
That means
The writing method of this sentence is equal to the writing method of the following sentence
That is to say, if you originally variable
You can't add any variables here
But if you use this set attribute
You can include variables in the variable name
That means self.layer 0
Then self.layer 1
One variable like this
Then the I in it can be freely changed
If you want to take it out
If you want to take it out
Is to use this
Same method
That is
For example, layer can be equal to get attribute
That is getattr
Then you put the same name on it
You can get that layer
The structure of this neural network
Then you say x.layer x
For example, if you have ten layers
You don't need to use self.layer 1, layer 2, and layer 9
You just use this set attribute
One row is over
That's not one row
If the structure of the middle layer is different
You still have to
This is also wrong
You have to add a sequential
The idea is like this
If you define each layer
You can use this set attribute method
Then we just use this
Sequential
A list
Then one row is over
Then finally we have to go back to this
Come to the train
We said we want to define this
The place of loss is in this train
Then we put the ones we need
The models we need to use
Input in
I have other models
But I haven't written the content yet
We only have one MLP now
That input is an identity
Then what is left?
For example, we add
Add an arch in this option
This is the type we want to train the neural network
Default is directly identity
Network structure
Wait a minute, we will add that
Then we default first
Only give him an identity
If you want to limit the choice
You can use this choice
The code is too long
If you are all layer 0 and layer 1
That's super long
I know maybe someone will
Here is the introduction
If you want to limit the parameters you can add
A parameter called choices
This is a list
Now you can only choose identity
If you choose others, you will make mistakes
This means
Then
Then this image pass
Do you still need it?
Still give him
Then here
self.net is
Look at this hyperparameter
What is the arch inside?
If it is identity
We treat it as an MLP
Then we set the hidden dim
Fixed to 512
Anyway, today
The structure of the neural network is fixed
4x256
Then test different
For example, siren or positional encoding
Or other activation
The performance of the neural network
Let's compare them
That's it
Then forward is self.net
This is OK
Then prepare data
The original MNIST is to download information
Download information is written in this prepare data
But we have downloaded the information today
Is this
That image
But you can also test other different images
Today is because we
The main thing is to see the reconstruction of this method
For example, these hairs
One by one
The distance between each one is very close
Then how do you put these hairs
Is
Restore it well
We mainly look at this
So we use this picture
Just have this
High-frequency feature
OK
Then we just said that we don't need prepare data
Because we have downloaded the information
That setup is
Put that train data set
And validation data set
All the places ready
Then we
Very simple
Our train data set is
That thing just written
So we have to remember to import it
Is in this data set
Import name is
What is the name of import
The name is image data set
Then
What about train
Is
The first parameter is
Pass
Passing path
Then the second parameter is split
Then train is train
Then next
Validation is the same
That
The name of the data set used is the same
We just need to change this validation
Just change split
OK
Then this setup is also over
Then train loader
Validation loader
In fact, we don't need to change
This worker seems to be taken away by me
The worker parameter
Then I see
I said before
Some people experimented the result is
Probably this thing affects
The speed of reading data
Although we have very little data now
Just some pixels
How much to set
Should not affect
That is to say
If you are a big data set
Then someone experimented
About
Four or eight
The average effect is the best
So usually this parameter is fixed
Because I'm too lazy to modify it
Because it has nothing to do
With the performance of the model
Then batch size
The rest are the same
Then we don't need to care about it
Then optimizer
Let's refer to the original paper
See what he is using
Optimizer
Is it bleeding?
Optimizer
Adam
He is still using Adam
This is the most common
Adam
Then learning rate
He didn't use
That one
So we
You don't need a schedule here
We just use an optimizer
Just
Maintain the same as his thesis
The fashion is the same
Then you don't have to gather
Seed everything
Should not be needed now
The seed everything just now
Is to ensure that every result is the same
Then we
I don't need it today
Then the definition of the optimizer
Next is the training step
Is every step
What to do
How to calculate loss
Then what to log
I will change this name first
What are we called today?
Coordinate MLP
Coord MLP
Before it was called MNIST
Change it first
Then we
Batch
What is in the batch?
In the batch, if I give it
Let's go back to the data set
Where
If we give it a dictionary
In the getitem
Then in this batch
It will also return in a dictionary
So
In this batch, it contains
This UV key
RGB key
So what we have to do
Then we have to put this
RGB predicted
Equals
UV in the self key batch
That is, we have to put UV
Pass to the neural network
Then get out
The predicted RGB value
How much
Then let's calculate loss
What about loss?
Usually this kind of calculation
Loss between colors
We will use the mean square error
That is, between each pixel
Its
The color difference between pixels
Then we cut it down
Then square
Then take the average
Then this
We can actually define one more
I usually
Will define one more
Define a file called loss
Although we are today
Today
May not use too much
But I am used to
Define a file called loss
Then I am inside
Is to define all possible
Will need to use loss
For example, what we will use today is mean squared
Then there may be
What will we use
It's just absolute difference
Also possible
Define a file
More convenient
For example, we call
MSE loss
But if this is the case
Then it seems to be useless
Because this is also a row
Then forget it, let's not define it
Because this loss calculation is actually
A row is over
Hey
I forgot the mean squared error in PyTorch
Then directly a MSE
Then we are in this
Initialization
Define a self loss
Equal to MSE loss
You can also use functional inside
I am used to using a class
Yes, I just said
Define another file
Called loss.py
The advantage is
You can choose here
You can use this
Hyperparameters
For example, you are here
Define a parameter called loss
For example, this is
When it's called MSE
You can do this
Then if
If it's something
It can be self.loss
For example, it is
What is it called?
There seems to be no such thing
Ok, it seems to be not in PyTorch
Is there?
Just directly
That
The absolute value difference
L1
L1 loss
For example, that thing is equal to L1
Then use L1 loss
You can use this thing to put it
Is
Separate
Then we only have MSE loss today
it is good
Then
Self is equal to
Loss is these two things
Is there a relationship between the front and the back?
I forgot, I remember
Still have
Usually the input is placed in front
Then the ground truth behind us
Is the RGB in the batch
Ok, let's calculate the loss
Just write it like this
but
I would like to calculate another thing
Called PSNR
Then this PSNR
Is to judge
How similar are the two pictures
The higher the value
The more similar
That is actually with this MSE
MinSquare Loss
MinSquare Error is
The calculation method is the same
Just after calculating the MinSquare Error
It takes the log 10
This
Turn it into a small number
Between about 20-30
Is a good result
Usually between 20-30
Then this
I wrote it before, so I copied it directly
In the nerf, we also compare
The similar degree of this picture
That is also using that MSE
PSNR to calculate
Ok, I just wrote it here
I just copied it all
Where should I paste it?
Then according to the writing method just now
I called a matrix.py
Ah, wait, wait, wait
There is also in the PyTorch Lightning
This is his own
PSNR
Seems to have
I haven't used it yet
Just check how to use it
PyTorch Lightning
PyTorch Matrix
I still need to install a new one
I still need to install a new one
Then forget it, I still
Use one
Then check how to use this thing
Here I also put MSE in by the way
Can import again
PSNR
PSNR is to take two
Take two pictures and then
Calculate his PSNR
We can add a torch no grid here
This means that
We don't need to calculate this
Gradient
Because this thing just let us
The score of his training
We don't have to do it
A reverse transmission
So we add this decorator
After adding this repair service
The next calculation will not have
Calculate gradient
Will reduce this
Memory
Is the use of the GPU memory
Or you can use this
The same way
With torch no grid
Then remember to mark
Then you write here
Don't calculate gradient
Or like just now
Directly add
Add torch no grid
The same
Then our PSNR
Will be equal to
PSNR
Let me see
The first one is prediction
The second one is ground truth
Ok, this thing
We can also log it out
Called
Train slash PSNR
Equal to PSNR
Then
Prog bar
We log it in
Progress bar progress bar
It's easier to monitor
It's easier
Ok
Ok
Ok
Then it's gone
Next is validation step
Validation step
Validation step
Do the same thing
Then we don't need loss
Use loss
Wait
Suddenly found
When I was taking data
Is
The test data
Is all
Ok, so just now
There is something wrong
This test is not only
Not included in the train
It was originally in the train
Also in the test
Yes, it is all
This whole picture is the whole test
So actually
Resetting effect will be better
Then the rest may be worse
But he
But he doesn't care about these
All pixels and then average
Ok, then we are
Just like his writing method
Then we are here
Just calculate the same thing
Actually
It's exactly the same here
Then
Loss is loss
Then
PSNR is
PSNR
Ok
Then finally validation epoch
End time we put all these
Conquered
Mean PSNR
Then we put
This thing
Export
That's it
According to the previous one
Amnest
Just add some things
Ok
Then we
Finally, it's the place to train
We save everything here
Save
The rest should not
Need to be changed
We don't have to do something special
So the rest of these parameters
We just
Don't move
Ok, then we are like this
Then
The part of the city
It's almost the same
At least this identity part
Oh yes, not yet
There is one more part
Before that PyTorch Lightning
I didn't introduce it
Yes
But I want to take a look at this before
We
When we introduced PyTorch Lightning last time
I said we
Calculate this
Post or something
We can write it in this
Inside the TensorBoard
Then we can open it later
Look at the training curve
Ok
At that time, I only talked about these
Just
Subtitles
The method of recording is self.log
But there is
TensorBoard
In addition to recording some
Other uses
For example, it can record pictures
Even what
All the weight, even what
For example, a Confusion
Matrix, there are many different
You can record the information
Then we are here
In addition to recording these loss PSNR
We can still
When it comes to validation
Record this picture
What does it look like
We can also put this
Every epoch after training
Its results show it
Correct
Today, by the way
How to teach everyone in this
PyTorch Lightning
Then in the TensorBoard
Add pictures
This is also copied
Written before
There is a self.logger.experiment
Then add image
This thing
I will copy it directly
Should be in
Take a look
Copy
There is also a copy
Wait a minute
Wait, this is in
Validation epoch
Validation step
I think about it
That is to say, now we are
In every step
In every step
There is only a part of the pixel
For example, a picture is
512 x 512
But if my batch size is 1024
Then this will only contain
1024 pixel value
Or I put it
In the log
Then at the epoch end
Take it out and draw
Can also
Or if I can
Guarantee that
512 x 512
Pass all at once
Memory will not explode
Then I can do it here
Actually
Forget it
I put this
I put this rgb predicted
also
Write it in this log
But will it be recorded like this?
Should not
I put this midway result
In a parameter
Then
I will stack it
rgb predicted
So this thing will come out at the end
512 x 512 x 3
Wait a minute
Think about it
This thing is
How much?
Every one is
Every thing is
B x 3
Forge.stack
Then I should use cat
Stack should be in the 0th
Place of the source
Stack it up
Then we should use concatenate
If this is the case
It will be all x 3
Should be like this
Then
After taking it out
This thing
Reshape it
Reshape it into
The format of the picture
This thing
Rearrange
Then rgb predicted
Then
HW
C
Then turn it into HW x C
If this is the case
Wait a minute
Here we have to pay attention
Is that we
Write on the TensorBoard's method
It needs
Is the format of the picture
Which is CHW
Then we reshape it
To CHW
Then the way to write a picture
Is very simple
This
Logger.experiment.addimages
Then
This can be written directly
I wrote before
Write the ground truth
Also draw the ground truth
Also can
Also can
Also can
This is our ground truth
With
Ground truth and
The predicted results
Then we can compare
Then wait a minute
Just write the prediction
Just write the prediction
Prediction
Good
Then this
We just write this thing
Good
Then it should be fine
The last parameter is
Is the current training
Steps
Then you can use self.globalstep
This is what PyTorchLining contains
There are two that are often used
This is the global step
Here you can get
The current training steps
For example, you can train
For example
if
self.globalstep
For example, maybe more than
After 1000
After 1000
Your loss may become
Another loss
Anyway, you can do some operations
Use this
The current training steps
To do some strange operations
Hello Luoxin Yong
Or is
In addition to this global step
There is another called current epoch
This can get
The current training epoch
The number
For example, it was 0 at the beginning
For example
After training more than 5 epochs
You change to L1 loss
This operation is also possible
Yes
You can use global step
Or current epoch
To get some training steps
Epoch and so on
Ok
Ok
This step
Is to teach everyone how
To add a picture in the TensorBoard
This is how to write
Wait a minute, is it add images?
Let me see
Because I only have one picture
Then is it
Just add image
It should be the same as this writing method
Ok, just add image
Add images
Should be multiple
That is, multiple pictures are used to add images
Only one picture today
Just add image
Then we might have to add
A ground tree next to it
Just add images
Ok, that's it
Ok, that should be
Ok
Next is to witness
Is there anything wrong
It's not the time for miracles
Now this is the simplest part
Ok, it seems
There is no problem
Ok, let's
Take a look at the training
Let's open another one
Ok, for example
How many GPUs are there today
It's already used quite a lot
I'm going to
Miracle now
Miracle
We have now used 4GB
Although I have nothing to use
Just use OBS
But the rest should still be enough training
We put that
Badge size is adjusted
It should be fine
Ok, let's take a look
It's basically
Badge size and learning rate
Then we need to train a few EPUBs
Let's refer to the original
This original paper
Take a look at its training
Ok, it's 11-4
10-4 is learning rate
There is no problem with this
Then
What is the batch size
Is it this, mapping size
I don't think so
What is the batch size
B, is it this?
No
I can't find the batch size
OPT
B, is it this?
No
Is it mapping size
No
It's weird, I can't find its batch size
Frame model
Or it's all at once
Maybe
Yes, it's all at once
It's all at once
Wait
It looks like
It's all at once
The training data
As a batch
It's not bad
What is the batch size
If it's like this
What is the batch size
256x256
Do we have to do this
It sounds so big
Let me think
You can try
Do you want to try
A whole batch
It's too strong
No, it's a whole image
Let's try it according to it
Wait, then
It's training 2,000 epochs
It's equal to the number of epochs
Because it's a whole picture at once
If it's like this
Let me see
Because I save every step
I save every step
A checkpoint
After every training
There will be 2,000 models
That's too much
Or should I
Get rid of this checkpoint
Don't save anything
Don't save anything
The scale suddenly got bigger
It's just a picture
Let's try it like this
Let's try it like this
It's batch size is a whole picture
Then
Epoch is
Every step has several steps
It's 2,000 steps
Then we are the same as it
Let's start training
Then we set this
The name of the experiment is good
Then we have identity here
Ok, let's try it like this
Workspace
It needs takes one
Matrix is wrong here
What do you mean
It needs takes one positional argument
I don't understand what it means
It needs takes one positional argument
The two were given
Why did I make a mistake when I imported
Or did I write wrong
No grads
How is it possible
I need to add an exclamation mark
Sorry
I wrote the wrong place
This place also needs to add an exclamation mark
I just didn't add an exclamation mark
Ok, it should be ok
Then
Then I need to give it an image path
Or I just set it
I will default here
Default is the Fox.jpg below the image
This picture
Here
This is the picture of the fox
Then we crop it in the middle
And train it
Ok, it should be ok
The error is in this HW
Because it doesn't know how many H and W are there
So it will make a mistake
We will give it how many H and W are there
Then
It is
The width is
The half of the picture multiplied by 2
We can also use train data set here
To get
The self.r defined here
Ok
Again
Ok, wait
Eh
It complains that
The size is different
But validation should be 512
Then I look at this
Validation
Oh, it is validation data set
This is the same
Start debug
Wait a minute
Let's look at the validation data set
Inside
UV
This should be right
It should be 512 multiplied by 512
262144
This value
This value is correct
This value is correct
Then why is this side wrong
Ah, it is down
I don't know why it is down
Oh no
Open one more
The size of this one
The size of this one is the size of this one
Let's print the size of this one
The size of this one is the size of this one
I got it
I got it
There is a function called NUMSANITY VALIDATION STEF
The function of this parameter is
The one I mentioned last time
You will do validation after training every time
So if the validation code is wrong
If you don't confirm if it's wrong
You will waste your training time after training
So this means you should do a test before training
But because we only test a part of a picture
It's only 256x256
So if you do validation for the first time
You won't have all the pictures
Then the size will not be enough
Because there is only one quarter of the picture
The solution is very simple
We just use this thing to zero
Because we also know that today's program is relatively simple
So the validation code should not be wrong
Let's see if the default is 2
Let's make it zero
OK
This should be fine
Training
I think we can start this time
FOUND D TYPE DOUBLE
There is a double
Why is there a double
Is it double when I'm in the data set?
Let's print it
But I am
Yes, RGB is double
So RGB is double
So RGB is double
Yes, RGB is double
Why?
Shouldn't it be a float in this way?
It should be a float in trainer
This is the precision thing
Then I will change it directly
Now
I'm too lazy to find which one is wrong
Change it to float tensor here
It will be forced to become 32-megapixel
This is the precision thing
This is the precision thing
This is the precision thing
This is the precision thing
OK
Wait
Now the training is a bit slow
Let's stop for now
Hello, hello MiyabiTV
I'm searching for Coordinate MLP Deep Learning
Let's look at the results
The advantage of Windows Sub-System Linux is that you can see the TensorBoard directly from here
Wait, why isn't any of the trains coming out?
Wait a second
Haha, I found something weird
Why isn't any of the trains coming out?
Still Validation
And the color of the train coming out is also weird
Wait a second
Why isn't any of the train logs coming out?
TrainLogs
Yes
Will it come out if I add this?
Delete this and run again
Wait a second
It's not right
It's TrainSamples
It's Logged
Hello, Ms. Kokoro
Wait, I found something wrong
It has nothing to do with that
It has nothing to do with Checkpoint
We don't have to save it
But it says Train...
Oh
It says Train logs every 50 steps
But we only have one EPUB every step
So nothing will come out
Let's think about how to do this
Personally, I want to say don't...
Hello, what cool things did you bring me to see?
It's still in debug
Let's see how to rebuild a picture
This is the patch size
Let's think about it
This...
Let's say I don't want a picture
Let's say I don't have 1024
Let's say I have 4096
Let's change the learning rate
How many times is 256 x 256?
16 times
That means my EPUB will be 32,000
Wait, that's a bit too much
If that's the case
This...
256 x 256 divided by 4096
There are only 16 steps in total
We still need to add log every n steps
This is equal to 1
Log every step
And...
Validation...
Let's not do it too often
Let's go back and see
Let's take a look at the training loss
If it's simple...
Simple is like this
This is the non part
The training error is...
Starting from more than 12
It goes up to 16 at the end
It goes up to 16 at the end
It goes up to 16 at the end
It goes up to 16 at the end
If the result is similar to this
There should be no problem
Wait, I'll still...
I'll still use the same result
It's a bit long
Because 2,000 steps is a long time
One more time
Because I have to wait for him for a long time
One second at a time
His PSNR is going up
Now is the identity part
The loss is also going down
Let's monitor the TensorBoard again
This time everything should be logged out
Everyone saw that we succeeded
At least the logged out
Let's take a look at the images
There is still nothing
A blank
This train's PSNR should be
It's almost the same as his
I feel
It's almost the same as his
It's close to 12.5
It's close to 12.5
But he has to train for a long time
But at the same time
We can proceed to the next step
We can add this positional encoding
This positional encoding is very simple
We said that just now
The symbol of UV is directly inputting this MLP
Now we add a positional encoding
What this thing is doing is
Put these UV, for example, U
Our positional encoding is called gamma
When we finished last time, it was called gamma
For example, our gamma of U is
Sine U, then cosine U
Then sine 2U, cosine 2U
Then until
For example, if there are 10
If there are 10 frequencies, the last is the 9th square U of 2
Then cosine 9th square U of 2
This thing is a positional encoding
Then V is doing the same thing
Ok, let's move on to the actual work
For the actual work
You can write it like this
Write all the frequencies first
Then put it up
Then throw it into sine and cosine
But there is actually a better one
Is a better way to write
What is a better way to write
Is that we actually create a
Playing Epoch and
What is Wang Jianxin saying
There is actually a better way to write
Is that we create a very big positional encoding
We create a very big positional encoding
For example, this positional encoding is on the left side
1001
Then it is next to it is 2002
Then until
Until the 9th square of 2
0, 0, 9th square of 2
Let's create a positional encoding called P
Equal to this thing
Then let's plug this in first
We just said gamma of U is like this
Gamma of E is the same
Then if we create a P that looks like this
If we now put P into this
What will happen if uv
If you put P into uv
You will find that all the frequencies are out
This thing will be equal to
This is u, then v
Then
Then 2u, 0, 0, 2v
You will find that all things are actually
The frequency is out
Then finally you have to put this thing
The cosine and the sine are added together
All of them
All of those
All the things we want in this gamma
All things
So today we also use this way of writing
Is to create a positional P first
Then put P and uv together
Then sine and cosine
Then stack together
This way of writing
At the same time, let's take a look at the result of this training
It's only 200 steps
One tenth
Take a look
Why is it the same as train and validation?
Maybe different
Are they the same?
I think it's the same
The fox's wheels have come out
But it's very bad
It's like
The result of training here is the same
It's still blurry
Wait a minute, I'm more worried about
Why train and validation
The results are the same
For example, 144 is 13.23
It's the same
Then why train and validation are the same?
Let's find out why
This is a whole picture
Normally it should be lower
But it looks the same
It looks the same
Train and validation
So it's almost normal
The actual value
It's still a little different
It feels the same
It's okay, let's write this first
Let's write the positional encoding part
Then
This one
I have created a class called P1
This class is to do positional encoding
This class is to do positional encoding
Then we have a parameter
Is the encoding matrix just mentioned
Called P
Then what we have to do is
Every X
This is 2
Every X is UV
Yes, badge size times 2
UV coordinates
Then we do it with P
P is called F2
F times 2
Okay, put it with P
Then add cosine and sine
Then stack it up
This is our final result
This thing is called X bar
Okay, then
This shape is
B times 2 times 2 times F
So this thing will become B times F
You can imagine this thing
We put it
Imagine that we put it
Every one originally only has
The size of two
The quantity is only
U and V
This quantity makes it called
Featureization
If you have a
Data engineer, you will know this thing
Is to put it
The original information has more features
Then positional encoding
What it is actually doing is almost the same
Is to put a
Very low-resolution
Very low-resolution information
Increase its resolution
So you can see 2 becomes F
F is actually
See how many frequencies you have
For example, if there are 10 frequencies, this is 10 times 2
Hello,
Then we finally
This thing is just putting its frequency
Just put it on
So after this, it will become
U, 2U
Until the 9th square of 2U
Then we have to add
Sine and Cosine separately
Then finally
All these features are added together
Remember concatenation
Its dimension is 1
Because every one
It's this thing
Its size will be the same as X bar
B times F
Then we put it in
The first dimension
After concatenation
It will become B times 2F
Like this
Then this thing is
Positional encoding
Hello, recently
The problem of Python has not been solved
I'm starting to study numbers and algorithms
That's right
It's true that Mr. Miyabi
B problem was solved
That is
If it is above B rank
Algorithm and data structure
I think
I will need knowledge
I think it's good to start studying
I think it's good to start studying
I think it's good to start studying
They are simple
The operation of strings
Not only
Ok
Then here
It will become B times 2F
What we are going to do now is
How to build this P
Where to build P
Ok
First of all
We define
We define
We define
We define P1 in there
If we use P1
We have to do
Personal encoding
Hello, good evening
Coordinate MLP
Now the simplest part is done
The new audience
Fan Youming
I didn't follow up before
Let me tell you what I'm doing now
I'm doing positional encoding now
I'm doing positional encoding now
But with a simpler problem setting
But with a simpler problem setting
The problem now is a picture
Then you want to train a neural network
Then you want to train a neural network
To rebuild it
Rebuild means that every element
There is a coordinate
After you input this coordinate
After you input this coordinate into the neural network
You can calculate
You can calculate
The color of the coordinate
And let this thing use the neural network
And let this thing use the neural network
Ok, we have said in NERF
If you just simply
Put UV in
The result will be very bad
Like this
Is a blur
In NERF, we talk about
Add a thing called positional encoding
Add a thing called positional encoding
A linear combination of cosine and sine
A linear combination of cosine and sine
If you add this thing
Then your image's
Resolution will suddenly become very good
For example, like this in the middle
It should be here
The image's reconstruction effect will become very good
The image's reconstruction effect will become very good
Let's compare today
Let's compare today
Is there really so much difference?
Put this thing
Put this thing in pie-torch
Put this thing in pie-torch
It means the line is astrology
It means the line is
Ascension
Ascension
Ascension
If one has a big A
If one has a small A
That two 
They may be able to
If two small A
That invisible
That invisible
Great
Let's define P
P is...
P is...
P looks like this
1001 2002
Until 290029
So what is it?
It is...
You can separate it
It is...
The front is a vertex
This is also a vertex
Until the end, it is a vertex
That is...
Every vertex, you multiply it by 2
So how can we write it here?
We can create a list first
It is a torch.i
i means vertex
Only the vertex line is 1
2 means 2x2
So this thing is 1001
This is the vertex
Ok, let's multiply this thing by 2i
for i in range
For example, 10...
10...
10...
The frequency is 10
What will it create?
It will create all these...
All the vertices
But they are separated
Finally, let's concat these things
That's it
The dimension of concat is 1
So this thing will become 2x10
2x20
Yes, it will become like this
This should be our...
No, not what we want
What we want is Fx2
Let's transpose it
Or...
Let's...
Let's...
Let's do this
Let's not transpose it
This is what I found
It's a faster way to write
It will create the array of P
Put P and U together
Add sine and cosine
It will be the positional encoding
After this, we add self.net
equal to mlp
The part of mlp is the same
But...
It's not exactly the same
In mlp, we need to add...
We need to add an input dimension
Here...
Is it ok to enter?
Just enter
It's hard to think
nint
We need to add an input here
Ok
So...
The original nint is 2
There are only 2 UV inputs
If it's positional encoding
If it's input
We can add a property here
We can add a property
Decorator
And...
It's called out-dim
It will return the output shape
What will it be?
The output dimension will be
self.p's first shape
F
Then multiply by 2
2 is sine and cosine
Then...
The advantage of this is
We can...
In train
Take out the size of p's output
We can turn nint into p.out-dim
Otherwise, how do you write?
Otherwise, you have to write...
You have 10 here
You can write 10 times 2
It's ok
I'm used to
Using some properties
After I change here
I don't need to change here
The
I might have written it wrong
Don't show it to everyone
I just wrote it wrong
Why can't I erase it?
Sorry, I wrote it wrong
Anyway, this thing
Then multiply by u and v
Ok
Then...
After this, the rest should not be changed
Then here
The possible option is to add a p1
I understand
So CSB also wants to say that this group should...
Yes, yes, yes
That's right
Linearity
It should be said that you only need to know the linearity
You don't need to know the separation
Or crisis
Crisis score
Crisis score may...
But that will be more...
More theoretical
Crisis score part
Yes
The theoretical part
You will need to use the crisis score
Then if it's this
If it's real
You only need to know the linearity
You only need to know the proof
That is, the weight of the neural network
Proof of its shape
How do you hold it up?
It's ok
The theoretical part still needs the crisis score
Let's take a look
Let's see where we trained just now
800 steps
Not bad
The validation is still a bit higher than the training
Ok
Yes, yes, yes
It's coming out
Did you see it?
We are getting closer to it
Getting closer to it
This one
The picture of this example
It's a very vague training
It's a very vague training
It's a very vague training
It's a very vague training
A vague fox
Let's turn off the
Let's turn off the Jupiter first
Because it shouldn't be used anymore
It's just that the training is a bit slow
Wait a minute
The training is a bit slow
It's been less than half a minute
I can't finish the training today
I thought it would be very fast
One epoch in a second
One epoch in a second is quite slow
It's going to be 2,000
Or we cut it in half
Cut it in half to 1,000
But 1,000 is going to take a long time
Let's take a look
Ok
But it will be higher and higher
The rest
So if it ends too early
Maybe it's not right
Or we just validation
Don't let him do it every time
Just every five steps
Just do a validation again
Or every ten steps
Because most of his time
It's all about validation
Because you see here
Every epoch has five steps
In fact, he only has one step
Is training
Because we say
One training
A whole picture to train
He has four left
Because our batch size is
One-fourth of the whole picture
So validation will have four steps
Every time
Validate one-fourth of the picture
So there are four steps
So it's one plus four equals five
So there are four
Four-fifths of the time is in validation
We don't need so many validations
Where is validation set?
It should be here
Check about every end epoch
It should be this one
We still call out
Documentation of PyTorch Lightning
I might still be wrong if I look over there
This training result
I'm stuck
Is it like this?
Very abstract
Let's take a look
It should be this one
It should be this parameter
Wrong validation loop
Let's change it to 10
Let's wait for it to 100
Wait for it to 1000
We'll close it
Then start training that
Positional encoding
OK
Let's train 1000 epochs for the first one
Let's change it to every 10 steps
Validate every 10 steps
Or every 20 steps
Every 20 steps
OK
Let's take a look
The result of training 1000 steps
Without adding anything to UV
Just throw it into MLP
You can only train
A very abstract loop
Then this one
DSNR is about 17
It's a very bad result
When a picture looks OK
It should be 25 or more
17 is a very low number
OK
Next
Let's look at the result of positional encoding
Add pe10
It's positional encoding
Then my arch
Use it as positional encoding
Let's take a look
OK
TensorFlow
Ah
Wrong
I wrote it wrong
I haven't
I haven't used positional encoding
Positional encoding is also a module
So this is right
Then import
Import pe
Then import
Then this thing will be self.pe
Yes
I wrote it wrong just now
And this self.pe is equal to pe
Because pe doesn't change
So actually
You can have a writing method called register buffer
This means you register
The meaning is the same as self.pe equal to pe
Then register buffer is
It will fix this pe
Won't do reverse transmission to pe
OK, that should be fine
Delete the outdem here
OK
Go go go
Wait a minute
Take a look
Here it says the shape is different
Why
Why
In is 40
That's right
Ah
I have to change forward
Forward
That is to say, if it is identity
It is directly input to the net
But if it is pe
Output to pe first
Then input to net
So I have to change it here
Or maybe a better writing method is
Directly in models
Define something called mlp
Then with pe
Then call that
Then this forward doesn't have to be changed
It doesn't matter if we change it here
It's the same
If it is pe
Then
To pass first
pe
Then throw it into the net
So it is like this
OK
Start training
Wait a minute
Why doesn't it look better
It doesn't look better
Why
Looks worse than the original
Wait a minute
Normally it should be better
Basic should be this basic
Oh, wait a minute
No mapping
Basic mapping
That's right
It should be at least as good as basic
Let's take a look again
Oh yes
It has become a little better
A little faster
Oh yes
Speed is fast because
I turn validation into every 20 steps
Only do one validation
Training 20 steps
So validation is fast
But why
Now this progress bar has become weird
Oh yes
We can see
Now this positional encoding is better than the identity just now
Good
Quite a lot
We can
But
200 steps is not enough
If you look at his picture
It should be this orange line
OK
Let him run for a while
Let's do something else first
Just
In this thesis
He mentioned
In addition to the simple P just now
Every one of them is done
Simple
Other than the second one
He can also do some special things
That's what we just said
This thing
It's all sign
What is it?
In this Gaussian Embedding
What is that called?
Fourier Feature
In this thesis
Which one is this?
This one
Fourier Feature Let Networks Learn High Frequency Functions in Low Dimensional Domains
In this thesis
He mentioned
We don't need to manually design such a P
We can specify randomly
That is to say, our P
We can turn it into a mess
It's called B in there
For example, every number here
It's a mess
It's from a Gaussian branch
Sample out
A mess that comes out
What will be the last gamma U?
Because there will be a mess
So it's actually like
For example, it will become sine 2U plus 3V
Then cosine 4U plus
For example, 6.6V
This is
In addition to this
The frequency is no longer
There is only one square of 2
He also made U and V
A linear combination
Then add to the inside
Yes, these are two very important points
Yes, one is this
The frequency part is no longer
Just some square of 2
The second is
Add the linear combination of U and V to the inside
This U and V add to the linear combination
What about its benefits?
In his thesis, he mentioned
This U and V originally
These two of some square
U on top
This thing is
It can be reproduced very well
For example, on one axis
It may change very much
It's like this
High frequency change
It can always be used
Original positional encoding
But if today's U and V
I'll draw it here
For example, it's a picture
If the high frequency part today
It doesn't coincide with this axis
For example, it is in this place
High frequency changes like this
If you add one today
For example, sine U plus V
The high frequency change in this direction
You can use these
These parameters
These benefits
To reproduce it very well
In his thesis, it's called
Axis-aligned
Axis-aligned frequency
I kind of forgot the name behind
The adjective is this Axis-aligned
Is the feature of these high frequency
Does it coincide with the axis?
Okay, then
Let's do this next
High CN feature
That's actually
It's actually the same
In fact, the writing method is also using
The one we just used
Traditional encoding
So
I still call it P1
This is called high
PA
That's to say, this P
We are no longer only some of the two axes
But there are
But there are ten, for example
Scale on the top
Some of Gauss's variables
Okay, here's the writing method
It's actually the same
This P will be
Here we are
Use random numbers to get it out
These frequency parts
Then Gauss
See how to get
Torch.Normal
I can use torch.Normal
Okay, wait
Torch.Normal
Okay
Torch.Normal
Then is Torch.zeros
Some things
STD is Torch.Once
Then we have to give it the shape
The shape is
For example, it is 256
Oh, wait
256
Wait
Oh, wait, wait, wait
We're here
His experiment seems to be
There are still some unfair places
That is to say
Our positional encoding
With the basic mapping here
There are still some differences
It's his basic mapping here
Its frequency is only two
Or just one frequency
We just said
Our positional encoding
There are ten frequencies
The number of frequencies
Can be adjusted by yourself
The total length will be
The number of frequencies multiplied by two
So for example, if there are ten frequencies
There will be 20
Multiply by 20
The basic here is only one frequency
So it is only
The length is 2
It just added cosine and sine
But this Gaussian
Its length is 256
It is 256 times 2
Because there are both cosine and sine
So the featureization he did in Gaussian
The characterized length
Is longer than the basic
So here I feel more
It can't be considered unfair
But he should compare
Positional encoding
If there are 256 parameters
What will be the total result?
But he doesn't seem to have it here
How many points did I talk about today?
Today I should
Maybe finish this
Finish this at least
Maybe in 30 minutes
But it doesn't matter
If it is unfair
Let's write it as it is
A
256
OK, then
We have to add a scale
The scale is
This is a hyperparameter
How much do you decide by yourself?
We can also add one in this option
A little higher
Otherwise
GA sounds like Ga
We add one more
The scale of Gaussian Embedding
If the scale
Just predict it to be 10
It looks like
10 is the best in his training result
Then 1 and 100 will be a bit different
We can try it later
OK, let's take a look at the training result
I have reached 1000
Let's just compare it around 1000
1000 steps here
OK, let's see
Some better results came out
Let's change it to 1000
Change it to option
OK, A
OK
Let's take a look at the training
This positional encoding
There are a total of 10
10 frequency results
That is this blue line
You can already see that
Training and validation
It can't be displayed
The training and validation
PSNR
Are much better than the identity
It was around 17
When 1000 steps
Now it's close to 22
Looking at the picture
It's also much better
It was originally just a very abstract painting
Now at least you can already see
It's a fox
But the details
The hair
We just said
We want to make the hair
See very clearly
But this positional encoding
It doesn't seem to be able to achieve such a goal
OK, then the next one is
The Gaussian Embedding mentioned in this paper
In addition to some of our manually set 2
He used all these frequencies
Use Gauss's distribution to generate
And then
In addition to this
Also added this
The linear combination between U and V
This is also random
OK, let's try it first
For example, when scale equals 1
This is called high
And then called high 1
We can just name it
Go go go
Wait a minute
None type as no
Here
Because I just wanted to change it, I forgot to change it
When Gaussian
Also add positional encoding
And then pass
OK, then we
Relative results
Understand distribution composition
High 1, yes, high 1
There is also high 10, high 100
No high 2, high 3
High 2, high 3 is a painful period
So we skipped
Then we can also take a look
Hey
Did you write it correctly just now?
Actually, I'm more worried that I didn't write it right
But should have
Let me see
Is 1 times 256
256
Yes yes
He is better than the previous one
He is better than positional encoding
10 frequencies better
That should be
Wait a minute, high 1 is here
I can't see the first few epochs
Here you can actually slowly
Compare step by step
That is to say, identity
When nothing is added
207 steps is still a lake
If he is Gaussian
140 steps have already been expanded
This is too early
I can't see it
His training is still longer than I imagined
But we Gaussians
We have to run three
One is 1, one is 10, one is 100
Then we still have to wait
We may not be able to talk today
Siren
Otherwise, we will finish Gaussian today
Just finish the experiment
That siren and other activation
We can talk about it next time
That is
Siren is different
Siren is that
The neural network is different
Or we can talk about it when we are waiting
OK
Let's talk about it when we are waiting
This siren
This thing is
Activate the neural network
Is the activation function
Change from ReLU to Psy
But
This study
Change the activation to Psy
It seems to be there before
But the result before
Not good
Maybe
It was there a few decades ago
It's not more than a few decades
Maybe more than ten years ago
Then
But this paper
It mentioned that
In addition to changing the activation to Psy
You still have to
Do one more thing
Is to put
The initialization of the neural network
The initialization of Psy
Make a adjustment
You have to use a more special
Initialization action
You can take a look inside
It provides a colab here
If you are interested, you can open it yourself
Then run it yourself
Then
Here
Here
Here
Here
At this time
Yes
It is in a special range
This industry is doing
In the initialization part
This is in
It is in a negative root
6 divided by
The hidden sirens
And then divide by one
Then divide by one
the scaling factor of the frequency
to make a very special method
then this way, the Siren will be able to train normally
the result of the training is compared to the traditional encoding in the thesis
the result of the comparison is that the Siren is better
on the right is the Siren, and on the right is the traditional encoding
that is, 10 frequency
the result of the comparison is
if you only look at these two
one is the yellow line
the traditional encoding is the yellow line
the Siren is the green line
the green line is always on the yellow line
the result of the Siren is always better than the traditional encoding
in addition to being able to reproduce a picture
it also listed other better benefits
in addition to the high accuracy of the reproduction of the picture
it is also very accurate in the area of dividing
that is to say, the original picture looks like this
if you do a gradation for the picture
the gradation is actually
you put the continuous left and right of the picture
for example, the two pixels on the left and right to do the comparison
you can get the difference
the greater the difference
the more obvious the color is
is expressed in this place
then the Siren, if you reproduce it
if you do a gradation for it
you can get the same result as the Ground Truth
this result is not possible to get in other ways
for example, the result of the traditional encoding is very good
but if you do a gradation for it
you will find that the result is very good
in addition to the gradation, you can also take the Laplace variable
the result is the same
the traditional encoding is the best way to reproduce the original correct result
if you want to know more about it, you can take a look at this paper
let's just copy it
which one should I copy?
let me think
I should just copy the Siren part
there is also a thing called SineLayer
and a thing called Siren
let's copy this part
let's not make any changes
wait a minute
my computer is lagging
the training is almost over
I don't need the MLP
let's paste the Siren part
but there are some things that we might not need
let's take a look
forward with intermediate
forward
this is the visualization that we need
forward with activations
some things that we don't need, we can delete them
it's just for visualization
these two are not needed
that should be it
let's take a look
there are still 100 steps left
then Siren
let's see if there are any changes
we need to import the non-py
I don't think we need this part
let's just return it
let's call it X
that should be it
let's add Siren in this option
Siren only needs one parameter
Omega 0
there are two parameters
I think 30 is not bad
let's try it
let's take a look at the experiment
it's over
let's run X
wait
wait
I forgot to specify the value
wait
I didn't paste it
I didn't paste it
I wrote it here but I forgot to paste it
it doesn't matter, it was 1 just now
so it's correct
I forgot to paste it
I'm glad I found it at this time
otherwise the result would be the same
let's paste the scaling
the rest is the same
let's run one more time
I think it's a bit too long
why is it a bit too long this time?
why is it a bit too long this time?
it's stuck again
let's run one more time
what was the value just now?
high
scaling is 10
let's take a look at the result
it's higher
the result is weaker than the positional encoding
the result is like this
it's a bit worse than this
but I trained 2000 epochs
the result is a bit worse than this
if it's 10, it should be much better
let's take a look at the result
it's a straight line at the beginning
let's take a look at the result
it's better than the result just now
you need to control the frequency
not the frequency, but the range
although it's high scaling, it's still big
let's take a look at the result
let's take a look at the result
it's better than the positional encoding
it's underfitting
it's underfitting
it's overfitting
let's take a look at the result
it's only 1 in 4
let's take a look at the result
it's better than the result just now
let's take a look at the result
it's better than the result just now
it's higher than 100
let's take a look at the result
it's better than the result just now
the result sends out an 1982 copy
it's an old-fashioned version
There are a lot of noise, and the PSNR is also very low
So if you use Gauss distribution as your feature
The only thing you have to adjust is the scale
You have to find the best scale for your information
But this 10 is not necessarily every picture is 10
You have to try it yourself
What is the best effect for your picture
But it seems to be saturated soon
It looks like it will soon reach the peak
But the effect is better than the previous ones
As you can see, the picture will soon reach the peak
But I'm still curious
Because we still want to reset the hair
But it seems that there is no way
Let's see
For 1000 Epochs, it can reach 25
Let's see if our training results can reach PSNR equal to 25
But I don't think it looks good
Why
Why
Why
But the operation I do should be the same as it
Input mapping
It has the upper layer
It should be the same
As you can see, it has the upper layer
I don't have the upper layer
This is the only difference
But this difference should not be too big
Then why does my model feel more
Otherwise, the ability is lower
But it doesn't matter
It's just to show you that
This high-CN Embedding is better than other effects
Then you have to manually adjust that
Adjust the value of Sigma
Then we will do a high-CN 100 later
That's it for today's live broadcast
The rest of the Siren and some other activations
You can also talk about it here
There is also a thesis in my third thesis
Beyond Periodicity
He mentioned that Siren changed the activation to Sine
But actually not only Sine can
He did a lot of different activation experiments here
He made a table
There are a lot here
You can see that
The above may be more familiar to us
Relu or some variations of Relu
Then Tangent H, Sigma
Sine is mentioned by Siren
He also experimented in this thesis
A lot of different
For example, Gaussian
Quadratic, Multi-Quadratic, Laplacian
A lot of different activations
Everyone experimented and said that
These seem to perform better than the above
It seems that there is no
According to different pictures
Different tasks will have some differences
But that is to say
The activation of the new species just mentioned
At least it can be compared with Relu and Siren
It's almost like this
And if you use these activations
You don't need to be like Siren
To do the initialization of the weight on each layer
You just need to use the default
Standard initialization
Then you add these activations
Your performance will be very good
Today, we don't have enough time to do this
In fact, it is not difficult
Just put the activation
Just put this
Not here
Just change the Relu of MLP to the previous one
For example, change it to Gaussian
Just change it to write like this
Then we will do it today
This Burier feature is good
Next time, do this Siren and other
These activation experiments
Ok, so our last task today is
Look at the rest of these results
It seems that this result is not particularly good
It's only 22
Why?
But it looks like it's getting more and more familiar
The only difference is to layer 2
Is it better to layer 2?
That is to change this Gaussian 10 to 10 times 2
Or is it 10 times 2?
10 times 2
It's about 60
It may be better to wait for 100
It doesn't matter
After we finish this
We run the last experiment
Is the scale equal to 100
Ok, the last 30 steps
What do you think of the live broadcast today?
Although today is a relatively simple program
If it is more difficult, I may be stuck for a long time
I think it's not bad today
But at least I can write what I want to write
Of course, there is also a debug part
Do you think the rhythm is okay?
Too slow, too fast, etc.
Or just right
There are fewer comments today
Everyone should have understood what I am talking about
I'm afraid you didn't understand
Let me ask you what you think
I'm late today
Oh good
Go to watch the live broadcast tomorrow
Usually every day
I feel better when I actually see the results on the way in
That is indeed
I am also afraid that the whole thing is debugging
People who come in can't understand what I'm doing
Can you turn Gauss into a learning parameter?
Oh, can
It can be
Jianxin said that the method is
Turn Gauss into a learning parameter
Or even you
I don't think it's okay to change the learning parameter
It feels like it's going to explode
Turn this scale into a learning parameter
Should be
Should be possible
Oh
Should be possible
Is to put the parameter here
This standard deviation place
Use a
NN parameter
Then make it possible to train
In this case, I have to change it here
Don't register buffer
Then self.p equal to p
Then I still write it like this
OK
But the live broadcast usually needs to wait for execution time
Yes
I thought it was fast
Just one epoch in a second
Run
But this doesn't seem to run so fast
Or
Ah
You can also use a smaller picture
256 times 256
The speed becomes four times
It can also
I didn't think of this at first
I found it after the live broadcast
Where did we go with the high 100 just now
Oh, high 100 is here
Hey
High 100 is here high 100 is here
Snow encoding is still very magical
I didn't see it just now
But he is in this picture
The result is not very good
It may be better to call Gaussian
But you see
When Gaussian 100 is with this
The result in the paper
Here is very similar
It's a bunch of this photo
Then it looks like that
What photo yellow
Originally a very cute
Very colorful picture
Then here is the color yellow
Disappear
Yes, he is over there
He has multiplied by 2 here
So the overfitting situation
May be more serious than me
I don't have multiplied by 2 here
So it doesn't look so bad
And it seems to be the best
Disappear
But his color is not right
Overfitting
Wait a minute
Why is his color not right
PSN is still so high
His color is not right
Still the first place
Weird
I may have to reset
Still have to multiply by 2
Let's try
Try the result of multiplying by 2
Let's stop this first
We know it's overfitting
Let's try the result of multiplying by 2
What will it be
Just multiply by 2 here
Here also import the numpy
Just in this scale
Multiply by 2
Multiply by np.py
Then multiply by the rest
Let's try this
This should be the result
With his
That is 20 pys
20 pys
The result of this should be the same as his best one
Ok
Then I will change this first
What he said just now is not on the same device
Because I am training on GPU
But if you write it like this
This will be on the CPU
So CPU and GPU cannot be calculated
If it is PyTorch Lightning
It will automatically help you
Automatically help you put them on the same device
For example, the GPU is put on the GPU at the same time
But the premise is that you have to
Write this register buffer to help you put
So I will change the register buffer first
Ok, we just did the action of multiplying by 2
This should be the same as his best result
Please
We just compare these
If it is 10
It should be very high at the beginning
But it seems not
Why is he still like this?
2 times 10 times PyTorch
2 times 10 times PyTorch
2 times 10 times PyTorch
2 times 5 times PyTorch
2 times 5 times PyTorch
Why do you still feel weird?
Feel really high 100
The result is almost the same
It's okay
Wait a minute
Let's find it, for example
20 maybe
20
20 look
Start doing that
Hyperparameter tuning
Now I want to find the best one
I want to at least let you see
He really can
Is
Very good to reproduce
The original picture
10 times scale
It should be right
Torch zero
It should be right here
Each is a Gauss distribution
Then multiply by a scale
Is it better now?
It's a little better than just now
It's a little better than 10 when it's 20
Let's take a look
When it's 1000, it should be normal to reach 25
When it's 250, it should reach 22.5
It's still related to Initialization
It may still have a little relationship with
The result of the initialization
Because he used
This author uses Jax
There may still be some differences
2225
2225
2225
2225
2225
We are now testing the last one
The last one is good
But it may take a long time to train to the final result
It's a little better than 10
The last one is good
The rest should be
Take a long time to train
For example, like here, 2000 training
Then his PSNR can reach nearly 30
Nearly 30 should be
Those hairs will be very
Detailed
Then I will go back to this thing
Go back and train again
The time will be longer
Today, I introduced to you this Coordinate MLP
How to do an implementation on PyTorch Lightning
Then some simple implementation
Identity
Positional encoding and the Gaussian feature
Today's live broadcast is about this
The program code later
Today's program code will be uploaded to
KITOP
You can match this live broadcast to see what I am writing
Then the next live broadcast may be next week
Then put the rest of the siren that I haven't finished yet
Then there are other activations
These
Gaussian blah blah blah
Next time the live broadcast will do it
Do it again and compare the two of them
They and today's
These methods
His
What is the performance like
It feels like you have to run a little longer to see the complete trend
Yes
But super long
Then next time we run half of it
How is it
It's 512 times 512 now, but we can also
Turn it into 256 times 256
If it is like this, the time will become one-fourth
We can try it next time, don't run such a big picture
Just one-fourth
Resize it to one-twelfth
Well, Kondo has worked hard, you have worked hard, Yang Zhonghan, good night, good night
Thank you for your hard work
Everyone has worked hard
Today
I feel that there are fewer comments
I still hope that everyone can leave more comments
Let me know if you understand
Or feel that what place is so powerful
For example, this method
I didn't think about it before
You can write like this
Or why Gao Xian wants to
Gao Shi branch and then
What does the linear combination mean
One-fourth of the fox is not one-fourth of the fox
Turn the picture into one-fourth of the size
Just
Resize
It still looks like a fox, but it's a fox with a lower resolution
Not one-fourth of the fox
A fox with a lower resolution
Well, Fan Youming, good night, good night
Thank you for your hard work
Everyone has worked hard
I hope that today's live broadcast can let everyone learn some things
Then we will
See you next time, next time I will
Find a good parameter and then run this experiment longer
See how many points it can reach
Then our live broadcast is over
Good night, everyone
See you next week, bye bye