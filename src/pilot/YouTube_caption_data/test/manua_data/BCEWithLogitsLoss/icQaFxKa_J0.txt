 Yeah, since we talked so much about logistic regression now, I
 thought that might be a good opportunity now to introduce two
 terms logits and cross entropy, because that's what I will also
 use quite often later in this class. And that's because also
 it's a very common. These are two common terms in the deep
 learning literature. I sometimes refer to it as the deep learning
 jargon, because, yeah, we sometimes use these terms a
 little bit differently in statistics, compared to deep
 learning. So I just wanted to briefly clarify how they relate
 to the concepts we just discussed. If this doesn't make
 complete sense, I mean, we will be using these terms later. And
 you will then have something to refer back to. Okay, so in deep
 learning, when we have multi layer neural networks, let's
 say, I will just draw a very simple one here, we will
 actually talk about them on Thursday. So if we have multi
 layer perceptron like this, everything is connected to
 everything here, it's a fully connected layer. And then we
 have this one output node. And in this network, we also have
 net inputs similar to add a line or logistic regression. But now
 we have two net inputs, one is here, let's call that z one. And
 one is here, it's z two, we will talk more about that on
 Thursday. But yeah, in deep learning, it's very common to
 call these here, the net inputs that come before the output to
 call them the
 the logits.
 In statistics, the logits have a specific meaning. This is the
 logarithm of the odds or log odds. And in the context of
 logistic regression, which we just covered, these logits are
 naturally the net inputs.
 So where does that come from? This is the inverse of our
 logistic sigmoid function.
 That's our logistic sigmoid function here, just, I just
 abbreviated it as P. So the log of P over one minus P, this
 function is the inverse of this function. And yeah, so in deep
 learning, though, it does not necessarily any relationship
 between the log odds and the net inputs of the last layer. It's
 just like, I think it's derived, the term is derived from
 logistic regression, people call that logit logits in the context
 of logistic regression, and then just generalized to this wording
 to arbitrary multilayer perceptrons, where we may not
 even have a sigmoid function, as we will see later. So you can
 just think of it as the net inputs of the odd of the last
 layer, basically. So here, just for reference, so here on the
 left hand side, the logistic sigmoid function and here on the
 right hand side, the logit. So the logit function is the
 inverse of the logistic sigmoid function. So basically how it
 looks like, I don't think I have to go into do that into the
 to that in much detail, because we have seen that before. And
 this is essentially just flipped. So there is another
 concept that is maybe a little bit confusing, but it's exactly
 what we've covered before. Actually, we had a seminar at UW
 last week, where we also, yeah, it was briefly mentioned,
 coincidentally, there was like a question whether it's the same,
 the negative log likelihood and cross entropy. So yeah, the
 negative log likelihood and the binary cross entropy are
 equivalent. And in practice in deep learning, people just say
 cross entropy, multi category cross entropy, which would be a
 multi class version of the negative log likelihood, which
 we will cover later in this lecture when we talk about the
 softmax function. So just to keep it brief, the negative log
 likelihood that we just covered a few videos ago, is the same as
 what people call the binary cross entropy, they were just
 formulated in different contexts. So negative log
 likelihood comes more like from, I think it's like, it's probably
 from a statistics context, I don't know the first paper, or
 reference that mentioned that. But this is something usually I
 see in statistics papers, and the binary cross entropy thing
 has originated from the field of information theory, or computer
 science. So we have actually seen that, or not, the cross
 entropy, where we have seen the self entropy, or just entropy,
 and statistics 451. For those who took this class, in fall
 semester, where we had used the entropy function in the context
 of the information theory and decision trees, but we used a
 lock to instead of the natural algorithm, but yeah, it's kind
 of somewhat related, if you have taken any class where you
 talked, for example, about the KL divergence, or callback
 Leibler divergence, which measures the difference between
 two distributions, the KL divergence is essentially the
 cross entropy minus the self entropy. Of course, you don't
 have to know that it's just like a fun tidbit here. The only
 thing you have to know is or should know, because it's useful
 to know, is that the negative log likelihood is the same as
 the binary cross entropy, this is like a useful thing to know.
 And this is what we've discussed in the previous videos. And
 there's also a multi category version is the multi category
 cross entropy, which is just a generalization of the binary
 cross entropy to multiple classes. So in order to make
 that negative log likelihood or binary cross entropy work for
 multiple classes, we assume a so called one hot encoding, where
 the class labels are either zero or one for some reason, it was
 cut off here. But this is something we of course, haven't
 discussed yet. And if this doesn't make sense yet, we will
 actually discuss that after the logistic regression code
 example, when I will introduce the multinomial logistic
 regression model. So again, all I wanted to say here is the
 logits in deep learning, usually refer to the net inputs of the
 layer that just comes before the output. And the term binary
 cross entropy and negative log likelihood are essentially the
 same. Alright, so in the next video, I will show you a
 logistic regression code example. And then we will take
 a look at this multi category cross entropy.