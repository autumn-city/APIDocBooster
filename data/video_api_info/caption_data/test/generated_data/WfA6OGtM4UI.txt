okay uh thank you everyone uh today
this this presentation will cover the
homework for part two
it's called uh listen attendance berlin
encoder and decoder neural network and
also we will cover something
some prerequisites for language modeling
which is related to homework for part
one
and also some recap over the last
restation
the attention models and to be honest
this
is so a little bit out of schedule
because
uh i know you have already you just
started working on homework for three
but now we are
giving next giving registration for
homework for
part two uh well please check later okay
so let's get started
[Music]
oh why can't
okay oh so first let's talk about
language modeling we should cover this
way before but let me introduce this
briefly
what is language modeling uh language
language modeling is very simple it's
just a model
that is used to assign a sentence a
likelihood
so it's giving a sentence a probability
and
the intuition is that sometimes some
sentence is more natural to humans or is
more
human readable for us like for example i
saw a cat
it's very natural common we can similar
[Music]
but this is more common than cats or
cats or cat eye because we rarely see
words c sentences like this so
uh naturally the probability of ice or
cat
it should be higher than the probability
of a cancel cat i
and what's the application for luigi
modeling and
when you when you google some questions
when you type the questions you can see
some uh
auto complete sentences in the drop down
menus but they are actually generated
from the
language modeling and also when you
heard about something
very fancy models nlp models like bird
or gbt3 they're very good at generation
method
generation task actually they are
somehow
a form of language modeling they are
modeling sentences
and what's the mathematic formulation
it's just model the joint distribution
for a sentence
so uh the joint distribution of a
sentence is just the product
of the individual for each individual
words
the pro probability of the h in which
words
is actually determined by the previous
words so for example here
uh the probability of word five is
dependent on probability of
of word one to four but somehow we can
make
simpler assumptions to uh simplify this
equation
for example we if we are using unigram
model we will assume
the probability of the current word only
only depends on
the previous word we only look one step
ahead
the the formula will be simplified and
also
is the case for bigram models
how is this relate your recurrent neural
network
so record that for our we have
iron is capable of several tasks the
this task the manage q1 task is actually
used for language modeling
and intuitively the input is a is a
sentence
it's uh previous words and output is
only one probability distribution
over the over your vocabulary
and let's do some recap on the attention
i think in the last restitution
jingle and barons has already covered
lots of
content for attention
and here i will just do some recap
okay why do we introduce attention here
uh so in a very normal encoder and
decoder
including decoder and iron
architecture we have one encoder and
decoder and
the encoder will take a sequence of
inputs and output a hidden state the
hidden set could somehow be modeled
could be viewed as a condensed
accountancy feature
representation of this satan sentence so
consider
we have word to back which represents
the words in a vector
but also we have some sentence to back
or a sequence to effect
we we condense the sentence or
or a sequence inputs into a
and you are like 128 numbers
vectors so encoder this is what this
what include does and after you get this
embedding you feel this you fit this
condensed feature representation into
the decoder
decoder is responsible for generating
some meaningful
meaningful sequences of output that's
the general architecture for encoder and
equal neural network
however this is pro problematic case
um because for the decoder the
the the feature representation generated
from the
encoder is not informative enough
because it's only 128 for example 128
numbers it's not it's not informative if
not
and also the hidden state will only fit
into the decoder
in the first time step and afterwards
the kinder states is only dependent on
your previous timestamp
this is a problem and people just
introduce the attention to solve this
problem
and attention is introduced here to
get somehow like a context information
over your or the input encoder inputs
so if you have a sentence in fact if
this is a sequence to sequence model
like motion translation you can get a
context
from your input sentences the the way it
works is that
for each output timestamp for each out
for each decoder timestamp you have some
hidden states which is generated from
your previous time instead
and this hidden state is used to
calculate your attention
weights over all the encoder inputs you
get some weights
and then you use this weights attention
weights to aggregate all the hidden
states from
the encoder hidden states and in this
way you have some
global view global context information
for
each time step of your decoders so the
the input of your decoder will be
concatenated with the
uh context inform information aggregated
from
your encoder and these two these two
concatenate
together will generate very good
performance
that's the information formation network
and one instance for a attention
it's not product attention so it's very
similar
it's very simple it's just similarity
like what you did for cosine
similarities or you
product each other to calculate some
similar similarities
and in a very general attention
framework we have
q key and value so this is three
the three concepts are normally
introduced i
i normally introduce in a transformer
architecture and then
extended to form a general architecture
for
attention network there are many
variants for
attentions but generally this arc the
overall architecture is the same you
always have
query key and value for the query
in the encoder decoder framework the
query is the hidden state
generated from the decoder output the
key
is the hidden states generated from each
timestamp of your
encoder the value also is hindered from
your encoder
part and i need the sum dimensions for
all the
for the quiz queries and values and
please check later i have i have a lot
of words
later in the slides i try to make this
recitation
self content i know some of you um
don't don't don't watch lecture but just
resize it
and i hope that you get as much
information as
watching this slide okay
uh so uh what is the listener tensor
spell spell architecture uh this
is you this is a encoder decoder
framework that is designed especially
for
speech recognition it comes it composed
of three
modules listener which is that which
acts like an
encoder speller which is a decoder also
it's
it's a language model and also we have
attention
uh this most of this is the simplest
attention dot product
so for the listener uh this is a little
bit different from what you
what you you are pre you are
implementing now
you are implementing in homework 3 is
just normal lstm or bi-directional
stm but for the listener we're using
a pyramid by stm this is a little bit
different
so what does pyramid lstm it means that
the shape is not so previously in normal
stm you have one input and
corresponds to one output however in
pyramid
biostem you have t input
and only t divided by two outputs for
each
plstm layer layer the way it works is
that it will concatenate pairs of inputs
over your
frequency domain frequency dimensions
for example
if your input data is batch time
animation
btb and then your output after
concatenation your output should be b
sorry should be b t
divided by two and d multiply two
and in the original paper they use three
stacked
uh p bl stem so this means that
your time resolution will be decreased
by eight times
and also we have they have a normal
blstm at the first layer why do we
introduce this
uh the intuition is that for humans the
audience is very long
as it's like 1300 or 1700
timestamps it's too long for ion rn to
generate the condensed
accountant version also you know
recurring the network is
suffering from some some problems
saturating and
managing so they try to decrease the
timestamps
without losing much information so in uh
in machine translation you cannot do
that because one word should correspond
to one output
but for audio or audio recognition
you can do that because the signals
somehow they have noise and also somehow
in the
time steps they overlap with each other
when you know
when you generate the spectrograms you
are using some window sizes
they will overlap each other and that's
why they decrease the time
time resolutions uh there are some
variations for this model
recommended in the form of paper uh so
it's a regarding the attention
so previous i said the key value p
value and queries they are all just the
raw
hidden states but they're suggesting you
you can you you could uh you could use
some you use some mlp
layers to project your hidden sets into
different
uh into different dimensions to make
sure to distance
the key and queries so each each concept
will have their own representation he
has a
their dimensions and has a unique value
well we'll talk this talk this later
what is a speller speller i think it's
very
it's just language modeling so with even
even without encoder your speller
should work just work as language models
your input
is just your uh
it's just a current word and you are
trying to predict the next word
but the difference is that here the
decoder
uh is not predicting your next word over
on it given the information of your
previous word
but also giving the information of the
context it's the
phonology information so it's like
syntax information plus
phenology information to generate your
next word oh sorry it's not next
word you know original paper they are
modeling this using character level
representation so it's not a word level
wait could you could you take that again
oh which part again
uh you spoke about something about
phonologies
but i don't yeah don't fully understand
that
okay okay so um so in
in signal processing i in in the
audio recognition or speech recognition
your encoder is
is modeling the the phonemes
right the phone signals so
somehow using attention your decoder
will receive this global context
for all your encoder inputs and they are
concatenated into a small vector
the context vector you can view this
context
after as some information about
phonology
so it's a it's about how you should
so uh the pronunciation for this part
for this word
is something like this okay and also you
have another
input which is the your current input
character
it's recovered yeah and that that is the
syntax information you're trying to use
language modeling to generate meaningful
sentences so this meaningful synthesis
is not only meaningful for in syntax
uh way but also in phonology way okay
okay okay i understand thank you okay
uh also this language modeling is
training use cross entropy
you're just predicting your next
vocabulary
over all the character uh vocabularies
and there's no alignment issue like
like you have in homework three which
you are using cities
ctc loss to uh to
to deal with the alignment alignment
issue but now it's not there's no
alignment issues one
one two one alignment
so attention um
i think you covered i have already
covered this but i
can introduce some so previous i said
instead of using the raw hidden states
you can add
some mlp projections to change the
dimensions
so the way it works is that so the
original
key and value and queries there are raw
hidden states
for example if they are 128 and then you
can
for example you want the key dimensions
to be six and
four you want a more condensed version
of keys
so you add one a 108 108x64
matrix a linear transformation to trans
to to project the key and queries into a
lower dimension
vectors and also you keep the hidden
states
at the values or you can just anot add
another projection to change the
to change your values and this way you
can disentangle
this tango or decouple the key and the
values
key and query they should correspond
with each other but key and values
they are not they could not they
they don't need to be the same they
could
be different and this is similar to i
think in the last resolution we talked
about scaled
scale the dots product attention
i think it's something related
so putting it all together this is the
whole
architecture for las uh the decoder
is using two layer stm so it's a
language modeling use
two layer stm it's not bi-directional
because
you are this is language modeling you
are modern in the language in one
sequence
not the other way around uh
also uh decoder input
and every time step is concatenate
character embedding
annotation context vector so this is
what i previously said
the syntax information plus the
phonology information
you can also use the initial context of
zero for your first time step
and also you can use parameterized or
notable ways for your first
uh for your first hidden states but it's
not recommended
because there is not already good enough
um the paper suggests that concatenating
attention contacts with the final hidden
value for character distribution so this
means that
you have a contacts and also you have
character embedding
output character embedding one method to
pull them together is to add or
other things but they're they're
suggesting you can't get them
so you have a longer representation
representation for each time step
and training training this whole
architecture is difficult because
this encoder decoder framework is um
it's naturally
hard but you can
trim them in in a separate way you can
firstly train your decoder
without the contact information so just
assume that
you have no phonology information that
you just zero padding all your phenology
information
and you are training a soloy language
modeling
over the transcripts you have so in this
way
so say you got a very good language
modeling it could it could generate
very meaningful and human understandable
sentences
even though those sentences are not
meaningful in your phonology way that
does not have some it's not charisma
does not correspond to some
um speech signal input
and then you initialize the whole
architecture
and then you need to initialize the
decoder
the decoder modules with your language
module you
like initialize your weights using your
trained language modeling
and this way you that and then you train
the end to end
so you have already got a very good
language modeling so this way
your model could convince converge
faster
than if you train it end to end you can
train end to end
you can uh pray to god that
even though you are not using mongolian
modern into
training training you are not training a
language model
sololy but the model the whole actual
architecture could still
uh be powerful enough to model somehow
like a language modeling but this is
very hard
i is easy to implement because you just
to implement
the whole architecture and you click the
trim button
and everything you leave it to the
to the a dark dark art dark
arctic technology but your risk
you're taking the risk of debugging the
a very hard and
monolithic model because
um based on my own experience
i spent like more than one week on
debugging this
i tried doing the training and i spent
over one week
on debugging and also i rewrite the
whole architecture for two times
it's very hard to identify isolate the
box
so usually you train it for like three
epochs
it doesn't work there's no arrows
you don't know why but you have to
so you will have harassed many pas
like us this semester and they don't
know what you
so the only thing you can do is to
follow the instructions
rewrite the whole architecture maybe
you got correct these times
so that's some let's talk about some
implementation details
the first one is the handling of various
lens
i think you got some ideas from your
homework three part two
you have to pad and use some pack unpack
things
but this is a little bit different for l
a and
l a s because you're using language
modeling
so in the language modeling part you are
generally you're you're generating
different lengths of sentences
so you cannot uh you cannot backward
use you cannot back do the backup
propagation
over all the sentences because remember
you pad all
of them you cannot so for some for
example
i can exit here screen and then do some
annotation
so make sure can you say oh okay we'll
see my annotations
yeah yeah we can see it okay so so in
the batch
because we are always processing all the
sentences in the patch
in the batch some centers have different
lenses
so now we have three sentences they have
differentness so we have
so the first thing we will patent them
to form a a tensor
but when we do the back propagation we
have to make sure
those things this part does not
propagate
back propagate so what we do is to
is that we will uh firstly use cross
entropy loss
reduction method is now so the this part
is mean by default
so what you get is a concrete value but
if you're using now you get a matrix
over is i think it should be batch
t matrix batch t matrix okay accurate
so here you have to generate a mask
also it's also batch t which indicates
which part you want to do back
propagation now this
mask you will zero out all the padded
values
so some sometimes some batch some
sentences
it's not long enough you will you have
to zero out
all the loss function or all the lost
values after
their length so again you get my point
okay
uh and another thing is that
because we have variations in the
training time
you know you know the sentence less you
have cs1 but in the inference time
you don't know you don't know how long
your synthesis is
so you have to add some special tokens
this is the end of sentence token also
you have a startup sentence token
so you can only rely on you have to
firstly process your sentences to add
some special tokens before and after
your sentence
and then you only rely on this this
this token to stop your generation
at influence time okay
and also uh is
yeah i think that's that's what
let me okay
okay lstm cell so in homophobic part 2
you're using lstm as a whole
architecture the
pathwage modules already does the for
loop
in inner follow for you but in the
language modeling part in the decode
part you cannot
use that you have to manually for use
the for loop
to generate each character
so that's why you are using using lstm
cell
which is the unit for the lstm
architecture uh i will i
have some words here but you can check
later i think
um it's quite stable it's quite simple
for you
so other two important part
the first thing is teacher forcing so
the training of the
decoder part is very hard because you're
uh you are using your predictive value
as your input value i better draw some
do something
yeah so this is the decoder part you're
predicting something
a distribution over all the characters
and then
you go to the next time step you put you
put forward the hidden state
and then you do the arg max
to try to find the predict value and
then you feed as your input
this is your default how your decoder
works normally
but this is problematic because
if you if you predict one character
wrong
it's not it's impossible for your
decoder network
to generate a good sentences it will
diverge from your boundaries
very much and the way we encounter this
problem is introduce teacher forcing so
the idea is that instead of instead of
fading the
your predict value into your next time
step
you are you're fading then the ground
shoes
you're feeding the ground shoes so so
regardless what you are predicting here
it's used for your back propagation
for your your next time step i will not
use your practice values i will just use
the
gonna choose this way uh this
this acts like a teacher who always gave
you the best answer
not the best answer could always give
you the true
input right this will is your
burden of training the decoder part and
also you have to do this in a problem in
a probabilistic way
you cannot you can also use t2 percent
every time
otherwise your model will totally
overfit so you have
to do it with the probability with the
probability
p p i think so if you're using you have
if you're so the intuition is that if
you are increasing the teacher forcing
rate
so the the higher the teacher position
rate the more likely you are feeding the
ground to choose
so it's that your model will focus more
on the synthetic information
because you don't need your model your
model does not need to address
the you address the the tactic
generating a very meaningful sentences
because you're always giving
given the you are always provided with
the wrong choice
so your model will instead focus on the
context part
and model very sent
all right i think i i
increased proportion rates more focus on
synthetic information
uh i think it's vice versa
but i think you have to change this i
will modify the
slides yeah swap this
yeah more higher teacher forcing rates
more likely you replace
the critical character with the ground
choose and more focus on the phonology
information because
it's like my tea i'm a teacher
i take the burn for you to give you to
pretty
good value you don't need to worry about
predicting the correct value
you just need to worry about um
take advantage of the context
information and
try to predict some phenomenon
phonologically
meaningful sentences that's the
intuition
and also we have sampling so in
in this time step uh so you are using
i i was saying using arg max this is
actually greedy method to
sample the correct answer however this
is about
optimal because it's too deterministic
so
the best the better way is to add some
noise is to add some
redness during the prediction so that's
why we introduce gumball noise
so in in implementation is very simple
you just use
torch and functional direction
combo soft max but for more mathematical
proof please check this
html it's very help helpful
okay
i think drawing is great okay uh
inference so at in first time
basically you have three methods the
first one is greedy
so greedy is just you
[Music]
really said just every time step
you predicting the most probable
characters
it's greedy and beam search i think you
cover uh you know this
in for homework 3 part 2 but it's a
little bit different from
in in homework 4 so what it how it works
is that
you start from the start of sentence
token you predict k
values and then
you predict the best k values at
type t one this is time t one
t1 here w1w1
and then if after you finish in the
first layer
you like to do a bfs search you project
it to the second layer
predict for your previous keyword
and predict more the secondary words
and then you only keep the first k
i'm sorry bad drawing but you get i
think you get my idea
this is very tricky too this is very
tricky to implement but
it is guaranteed to have the best
performance
okay and another method is random search
this is trivial it's very easy to
implement but it's as powerful as the
bim search
the only drawback the only drawback is
that
it's much slower it's much much slower
unless you
implement in a very very parallel
effective way
the idea is that you uh you just
you render sampling so instead of
instead of here
greedy method instead of using arg max
you just random drawing
using random to draw a
character out of this distribution
random sampling
and for each sample use like sample
100 times for each time
you aggregate the likelihood so for each
time you're generating a sentence
the sentences has some has a likelihood
like the likelihood is just the product
of all the individual
uh probabilities and you get a
likelihood you normalize it by the lens
so that your prediction will not bias
towards
the longer the shorter sentences you
have you normalize it by the
lens and you you pick the lock
you pick the largest one that with
the you pick the sentences with the
largest likelihood
that's how it works and it's trivial to
implement you just do them
forward forward loop and keep track of
all the likelihood
and choose the best best one
so utilities are these some useful tools
which will be used for you for you to
debug
the let me clear the joints
so the first one is plotting the
attention weights
so here this is sample plot
uh this domain y-axis corresponds to
the um i think it should be
output the decoder
decoder time the this set dimension is
the encoder time
so the white the white dot you can say
they are one
in the most ex in the streamless way
they're one
and all the rest of their zeros so this
means that uh
so it's like uh it's optimal the
the what you are expected to see
is something like this it's a tight
diagonal
it's diagonal this means that at this
decoder output they are
putting more focus on your corresponding
input this is meaningful this is
reasonable right
you are you have a sequence of signals
and when you are predicting the
character level
characters for this part you only focus
on this subpart and that's that's the
intuition that's why you are expected to
see a
diagonal for your attention with feature
map
and another method is plotting the
gradient flow
so you're using rna right it's
highly likely that you will got
exploding
uh exploding the gradients or vanishing
gradients
so you have to make sure you are using
some correct method
uh make sure you don't have bugs on this
so that's why for and i always recommend
to use gradient
clipping to make sure your gradients are
within a range
and that's uh
yeah i think uh binary
okay this one yeah this is tricky
because
so previous previously i said something
about masking your loss function
and also this is also true for your
attention map
because your attention matrix uh
they are also padded they are also
padded their dimension is
b t encoder
and t decoder so these are all max
values
that padding you have to make sure your
value for those
uh padding part should be zero it's
guaranteed to be there
you have to zero out those parts as for
your attention weights
yeah i think that's all for uh all for
my part
the stereo serial part i think sammy
will
introduce the code for you
are you taking over yes yes i'm taking
over thank you for
introducing
this part let me share my screen
so that you can see my code
can you see my screen
anyone is seeing my screen yeah
[Music]
okay so today um
we are going to go through the code
we are not going to use like
notebook like you we used to see in
recitation
we and we are not going to learn to run
these codes because
actually we are we are releasing this
code we are
just giving you these codes uh and you
will
you will you will have to implement
uh the rest of of the code
so it's not it's not like
it's not final and you will have to
understand
what the codes are doing and you will
have
to implement your parts according to
your implementation
so it has different implementation this
is why we are
we are going through the code today
so we have uh different files i mean
module
we we have data loader and you
or we have models actual in model it's
where we
implement our attention implement
our encoder and the decoder
our listen and despair
and we also have our
bp ls team
and attention so let's start with
data roader or data loader is just
loading the data returning
and we will be given training
you're given will be given data
and transcripts so
you will have to implement this uh
transformer letter to index actually
it's a
it's a it will be taking
the transcript and the it will be taking
also delete
the letter list uh the letter lists
uh is here in main you can see here here
so it's a list of letters that you will
be
using
we have we have also a function to
to to create dictionary so we have
to implement this one as well
we have also speech to
text data set class which is
transforming speech into the text
so you will you will have to probably we
have to
implement to modify this
get item according to uh
the way you implement the collateral and
correct test
functions uh correct training
return you know it's a
it will it will return uh a padded like
like any homework free uh pilot
or homework free part one the way you
return the padded speech
uh i mean they parted the padded
square sequences so it's the same way
that you will
implement this one
or let's go straight to our models
so basically let's start with encoder
encoder is uh is taking our
attention as input and it will map
to the project uh it to
the key value the key and value that we
will
uh put into the decoder
and the attention as well so
as adam said we are not going to use
lstm in this network
because of the issue mentioned
he mentioned before
so we
here
we use this our our
pyramidal bl stm
so implement it we have to implement it
first
so that will use it in the encoder
so this one if you use uh
you can use as recommended
free pyramid
by uh lstm
or you can feel free to
to to do to implement your own
version so there is
no rule here
to to follow so you can use
you can implement your own version of
this
bp pblc
stm so after implementing this one
you will you will you will have to use
it here okay so in your encoder
and the encoder will return
the key and the value as mentioned
uh okay so i think here you
we have also you we have to add some
codes here and here
oh before we go to decoder
let's see the attention class
so the attention class is calculated
using the key
value from the encoder
and also the query from
the query i mean the input
so in in this intention you have
to basically implement uh
this function uh you have to get the
image
uh and you have to get the attention and
the
the context uh so you have to get the
the context
as the output and here
basically we do the bmm
uh most of i think you did the
bmm universe the recitation
if you you didn't understand it you can
see the documentation
say it is so straightforward
all right uh yes here we we we
will be using the query from the decoder
i will be talking about it
uh remember in the decoder we are not
going to use
the normal lstm
we'll be using the lstm cell
so the query is is from this
decoder uh we have also the
the key and the value from
the encoder pro like every time step
and also on the or here also
we have to we have to
add the attention i i mean the mask you
have to do the mask
on the attention uh we you have to be
careful here
in implementing uh this mask because
of the padding uh mentioned
so you you make sure
you you zero out the padded
departed part
okay then you return the output which is
attended context with the context
and also the
[Music]
yeah the attention mask
okay that you can approach
so let's look at the decoder
okay so the recorder remember the output
from
the encoder are fit
into the
[Music]
the output of encoder are feed into the
decoder
so first of all
yeah we have to
to look at this embedding embedding
layer
which taking vocab size and the hidden
layer dimension uh make sure the padding
index is zero this is the key so all
vocab size is this but also we can
uh we can use the vocab size from uh
[Music]
yes from these letter lists for
the character
modeling but if we are using if we are
looking at
words or sentences we can we can lose we
can use a different one
so we have uh ls lstm
cells as mentioned so the output
from the first cell is fit to the
second cell or
we check if
it is attended if it is attended then we
create
the attention if not where
we use we will be using values
here those values where will be
inputted
and also we have a linear arena layer
which is uh which is giving us the
probability distribution of characters
forward pass
yeah we look at so in training in
training we are given
sentences we know the shape of the
sentences that we are having
we can use we can loop over
the sentence length
but if we are not if for
if we are not in training
we can use the max length of 250
collectors
as a adam recommended
that we can use from 200 to
300 so we choose 250
predictions we start with for an empty
prediction
but we we are going to
load the prediction into this array
this list
at the beginning hidden state we have no
hidden state hidden state is
none
yes here is where we look through the
the the sentence and to implement the
gamble noise or the cheat
the teacher forcing techniques
uh note that if the attention is true
here if we we it is attended
so we replace the value uh
today uh today
to the context uh to the attention that
we get
from the above
okay so if you haven't implemented
implemented the attention yet uh you can
check the index and the break
out of the loop so be careful while
implementing this
[Music]
gambo noise and the teacher forcing
techniques
so basically we get the
embeddings and we concatenate
we have the input and the input from our
previous head approved previous
lstm is
fitted to the next uh to the second
i mean sorry to the second uh
lstm we get the hidden stats
we compute with the attention
from the output of second
lstm cell and then we populate
we append the prediction we get the
prediction
and we return the prediction
uh i think that
this decoder you will have to
modify this section
and you have to implement depending on
how you have implementing the
the encoder you have to uh
to modify this this section of codes to
implement
your decoder
so this sequence to sequence is like end
to end
where we're having we put together
everything
uh it's like a lapa moda
so we we get our encoder we get our
decoder
and then we have our predictions
oh let me
show you our main function
so this is where we get our character
characteristics
we import our we load our data
from data loader correct training
correct
tests transform data and everything
we have our letter list uh note that we
have for our start off for sentences and
the end of
sentence special characters and they
also
pad so we use pad
uh to for for the first for the starting
because usually
part do we we put we encode it
as zero
so we have our model uh remember that
we have uh we have to use uh uh
this is what adam was or was saying
that we have to use the reduction
to be none for our cross-entropy
loss
all right then we okay
then we we train our model
and then we get we get the results so
basically we have to implement this this
is not
uh everything you have to implement this
section
from the training and the testing
and also this one here is here you can
also approach
or there are some utility function that
we we didn't
give you we didn't uh put in this
citation but we will provide them
for protein weight
and a
yeah for protein uh
yeah for plotting the weight and the and
the and the dutch graph that
adam showed you so
and let's go to trend tests trend tests
also
yeah it's it's challenging because
there is no chord provided but we gave
you the
pseudo the pseudo chord here step by
step
line by line uh
here you will have to
to import some necessary
function or model
you iterate through your order
use autograd
and everything so the i think all those
steps are
are are straightforward uh
it's well documented and uh i think you
can
you you implement you implement it and i
think
in the next race the citation when the
harmonic four we
will be released i think we go back to
uh to those steps and they and the clear
out uh if there are
things that are missing so uh
i think uh
yep that is it uh i think
i think uh my recommendation is to
even if you are not yet done with free
if for this hard work for is released
maybe you can start as early as
possible be because it's the most
challenging
homework that that you have is there any
question
yes uh i recommend start as rdf as
possible so
yeah recall that i rewrite the whole
architecture for two times
yeah yeah
okay if this i will stop recording
all right