 And this is  essentially it's in that way, step one of batch norm is  similar to the input standardization. 
For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.  However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].
 And yeah, one little fun memory aid to remember that  is, if you consider this case, so you have batch norm, then  you have the activation and then you have dropout, you may call  it bad, it might be better to have batch norm after the  activation, that's typically a little bit more common. 
 So let's say you have the Google  search engine, and there's just one user running a query, and  you have a network that has batch norm.  So you have to  normalize, but you don't have a batch of users.  So there are two  ways to deal with that scenario, the easy one would be to use a  global training set mean and variance.  So you would compute  these means for the features and the variances for the features  for the whole training set. 
 The same with batch  norms, instead of using batch norm, one D, which we used  earlier, when we talked about multi layer perceptrons of fully  connected layers, for the convolution layers, we use batch  norm 2d shown here. 
