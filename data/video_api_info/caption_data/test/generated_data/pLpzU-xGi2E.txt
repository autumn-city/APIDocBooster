so today's uh session
is about well it's not a comprehensive
uh kind of presentation or introduction
to the theory of the model so we have
introduced most of that in our previous
meetups
but just to give you an idea i will
still give a brief overview of
what is the recurring neural network uh
secrets to sequence model and
what is the tension right so i will
still uh
introduce these basic concepts
and then the bulk part of today's talk
will be
focused on how to build a machine
translation model
in tensorflow so
it's almost from scratch uh well what i
mean by that
is i know because it's a very popular
model
right so people have kind of wrapped it
up to pretty high
apis that can that you can directly use
but i want to actually avoid that
and really to see the building blocks
uh how to put these building blocks
together
uh and organize the model and from from
defining the layer
uh from defining the attention function
and loss function and in the end to put
everything together
so the references are based on the
stanford nlp class
so actually the whole nlp series is
based on the stanford classes
you can actually find all the slides and
videos on their
course websites so i think it's a very
good
materials for you to dive deeper
into nlp and also of course the codes
are built upon the tensorflow
core tutorials they also have
machine translation with attention this
tutorial
so the calls are built on that and i
have extended the calls to
say accommodate more attention uh
or different attention mechanisms
so i have submitted my calls already to
github so i have
i have already posted in a comment you
can find it
and then check out the course yourself
now
okay so the first is what is the
recurring neural network actually
to me right now i will think of this
type of structure as
still is learning a function learning a
function
about how to encode sequential data
right so all of this is about sequential
data well you have
a sequence say x1
x2 x3 x4 well we usually give them
different time steps
well it can be time series or it can be
languages
right or maybe even video frames right
so all of these
are in sequences and we want the neural
network to encode
the sequences into a state right so here
is the um you know each unit is kind of
encode
states that represent um current
observed sequence so for example the
states over here
kind of represents the sequence of x1
and x2
right and then there is also the outputs
so you will hear the concept like states
and outputs that we will also
talk a lot in the codes right so there
will be also the output
well the output sometimes can be the
same as the states
or it can be different right based on
what
r unit you are using say here is uh
is the the outputs and usually well
depending on different tasks whether you
want to utilize
all the outputs at each each
cell or you just use the last
output like for example if you are doing
a
sentiment analysis maybe you just care
about the last
output however if
you are doing machine machine
translation
well at each time step well this output
can serve as
the input at the next time point
right so it's pretty it's very flexible
of how you are going to
utilize the states and the outputs
right so in summary the recurring new
network is basically
you can think of think of it as a
function that
encodes the sequence
sequential data
and also a special point about that you
can you will see the weights
it's kind of a transformer uh like
a weight matrix over here and the weight
matrix will be the same at
different time steps so that's the
special
part in the iron
and speaking about the popular
rn unit there's the lstm
the long short-term memory cell unit
or the gated recurrent unit the giu
basically i think the giu is kind of a
simplified version
of stm and it's usually trained faster
but people usually say lstm performs
better when you have a larger data sets
basically you can see actually if you
just first
look at this structure right it looks a
little bit
it it looks like a circle design right
and uh and it looks a little bit
complicated
well but if you just look for the
sigmoid
function there it's pretty much just
designing gates
right so each sigmoid because sigmoid
function maps any value between zero and
one
so it it's kind of like design the
the gate right to to to determine
whether or not
to pass down the information so the
material
motivation behind the lstm and giu they
are both
solving the issue of the vanishing
gradient descent
problem because when the rn when you
basically like unroll and roll in it if
you have a
really long sequence you will find that
the the gradients
mostly you will still learn from the
shorter from the nearby
kind of nearby information rather than
the information that
many many time steps ago right so how
how can we pass on the information that
long stem time steps back to current
time point
so that's why people design stm value
well
they design these skates to directly
pass on the information
right so if the gate is say date it
means pass
so this is the previous sales date and
the date is passed
and then you don't include any new
information so it's basically just a
straight
connection from past to the current time
point
right and then the gru is basically a
simplified version you can count right
so the sigmoid
actually sigma layer is one less
uh and it basically combines the
the input and uh for get gate they also
call it update
gate right to to just
by using one sigmoid rather than to
separate sigmoid
forget get the same as drop out
uh well yeah it's an interesting idea
i have never thought of that way but
yeah so it's
it's uh yeah so you're skipping
connections right
yeah i think it served as a similar idea
over there that's an interesting thought
okay and then let's introduce the
sequence to sequence model right so
basically it's putting
two are and together
right so one is the encoding rns that
encodes your source sentence or source
sequence
and then here you just you know encode
the the sequence by the last
state hidden state and then you pass
the last hidden state to the to another
rn right so basically you get your last
state and the cert and make it as the
initial input
to initialize the state for the next
rn and then it's going to generate
the you know the numbers for the for the
for the
next sequence or your target sequence
and then once you generate
one out that will serve as the input to
the next
and then you basically just un and
rolling it right till you hit the end
which marks the end of the sequence
now so basically this is just one way of
how you
how you kind of transform
from your source sequence to the
target sequence right and then they
connected by the the hidden states right
so the hidden states here kind of
represent
all the information while we assume
the hidden state represents all the
source information
and then the decoder basically just
utilizes that one
information to to enroll itself into
another sequence
all right so this is uh initially uh the
sequence to sequence model
and then people later realized well
it may be too strict if you just
use the if you just use the
the hidden states in the last unit right
so
so then people um find out okay why not
just
you know utilize more outputs from the
encoder
such as like each unit is also going to
generate the outputs
right so we we maybe can find a better
representation
from all these outputs from the source
sequence
and that comes into place the tension
uh mechanism right so
that that that is what behind the
tension uh
so basically here it's still the same
right the states
the the states of the encoder
the last state of the encoder service
the initial state for the decoder
what he difference is say we reach this
point right the state
is going to uh compare with each of the
output of the of the encoder
right just to realize well it finds out
okay this state
the output state is actually very
similar
say to this to this output right
and then it basically finds a
distribution
uh that tells you which output is more
important
to current decoding task and it finds
out okay this output is more important
and then they take a weighted average
that will become your
context vector right so this this
context vector is basically
a weighted average of all the outputs
from the encoder
and then that context vector together
with your input uh basically
concatenate them together that will
serve as the input to your decoder
right so yeah so the i think is very
cool idea
because uh i think it first
solves the vanishing gradient problem as
well because if the if your
sequence is very long right so the last
state
may lose the information of
of the of the word say in the beginning
right but however if you are if you
basically utilize all the outputs from
the encoder you kind of
kind of youtube you you can you can get
a better representation
for the encoder no matter for the for
the source
sequence no matter how long it is
right so we we have the information of
both the
the the final um hidden states and
also all the outputs from the encoder
so that well actually gives the boost in
performance as you can see as you all
see in the in a tutorial
so any idea on the or any questions so
far
um um what is sequence to sequence
what is the tension and
and why we are doing this
yes i have recorded this session and
will post it later on
um okay so if no question i will just uh
move on so basically the attention
is is a very general framework right so
you have the values
well the values in the
in the decoder encoder states the values
basically are the outputs
from our encoder right and then you have
a state basically is the hidden state
in the decoder and then the tension
always involves
first computing the attention scores
right and then basically there are many
ways to do to to do that
and i think the goal is to
compute the similarities well you can
think of it as
computing the similarities between your
values
and query i will call it a query
like how similar these vectors are and
of course the most direct
similarity is the dot product
right and then people well that product
does not involve any ways to learn
and it requires the values and your
hidden state or query has the same
dimension
right but what if there are you know the
dimensions are different and then people
let design the multiplicative
which is use the matrix to transform
them
so that they can you can still do the
product
of of vectors of two different
dimensions
and people also design adjectives
attention right so basically the w1
matrix and w2 matrix are going to
transform this into the same dimensions
so that they can
kind of sum up together
and then through a transformation and
then soothe
another v is also going to be learned
kind of another dot product to make it
into a scalar
right so that describes the similarity
between the values values h
and the query s and once you d
right once you find out the scores you
are going to apply a soft max function
basically that map array of any values
into a distribution right so you can
think of an array of any values and then
the softmax function is going to convert
that
into a distribution so that's the array
sum up to 1.
and then from the distribution we are
going to do a weighted sum
of all the uh of all the values
right so the values here say is the
output of our encoder
and that gives us our context factor or
tension output
now i say some values get really high
attention
uh really high value in tension
distribution and then the context vector
is going to like that
value vector a lot right so in this
tutorial we are going to implement dot
product
to serve as a baseline and also
implement the adjective attention
so here is the steps for the tutorial
first we are going to process the data
and then we are going to build the
basics basic sequence to sequence model
without
any attention and then we are going to
add
uh two attention mechanisms a dot
product
and additive uh and compare their
performance
uh and of course in order to do that we
need to define our loss function
and optimizer we are just training like
putting
all this together and training in
batches
and finally we are going to just apply
it to translate some sentence
yeah so i'm going to send this link
again
to the chat
okay any questions so far before we
start looking at the codes
if not so let's uh look at the codes
okay uh
anyway i don't think i'll go yeah so if
you
are able to download the calls here
right so this is the github link
so you can
yeah so codes so you can click or you
can just download
as a zip and then you can
if you want to run uh or maybe you have
already tensorflow installed in your
laptop or if not you can directly go to
the
the collab research so it provides you
so for example over here so it provides
your gpu access
yeah gpu access
yeah by body ul kind of uh
disconnect itself after i think
maybe five minutes also of silent time
you will just disconnect yourself from
the worker
um okay so here actually we are
in this uh the collab
uh it has the two-point tensorflow 2.2.0
installed so that's the version of
tensorflow
in this example
so the first is to load the data set
so we are using uh we are using a
spanish to english training tag
text so this is already hosted on the
google
storage so we can directly
kind of get the file on this link
and extract it directly extract it
into the collab disk so you can see the
disk is already
kind of um there will be some it
shows you how much ram and disk
is utilized and then we are going to get
this file
right so this uh spanish to english
file over here uh and and
so the first step of course is to do the
cleaning
uh how do we ensure gpu is used you just
check the runtime
right so uh change runtime type
i think uh by default is none but will
be very
slow to run so better change it to gpu
over here in the runtime
okay uh and then so after we download
the file to the disk
we need to do the pre-so pre-process the
sentence
all right so uh i think the first is
to just keep the charts we want
in summary that is what it's doing to do
going to do and
keep the words and charts in more
structured format
for example we want to say
okay uh like he's uh he's a boy
right and then the punctuation but we
want to make sure that there will be a
space
around a uh there will be a space
around around it so that when we
tokenize it we will get uh
we will get uh like
uh you know each individually right so
that the boy
and the dot is not putting together you
know all that
so to do this processing uh one quickest
way to do is through the regular
expression right so it's using this
re regular package
it comes with the python you don't need
to install any
separate python library but it comes
with python so you can directly import
three
to do all this processing
and for example here it is doing that
well i want to replace
every other chart except
these letters and this common
punctuations
right so you can just run one regular
expression to replace all the other
charts you don't want to process to
filter those out
and then you can use the strip function
strip function to remove the actual
space
right and of course the final step
is to add the start and the end
to each sentence so that marks the
beginning and the end
and this is the requirement to train
encoder decoder model so i think i have
shared it on the yes i'm i have
posted the link again in the chat in
case you missed that link
uh okay so i should post it to public i
think that's
why maybe someone yeah
okay uh and here just sees an example
right so say the the sentence is this
uh i specifically put
a weird char here just to see whether it
can be filtered out
right so after you're calling this
preprocess function you can call that
well
the space is correctly put around the
charts
and the edge has been filtered out
and and here i want to show like why we
want to encode it in
ascii right so if we
encoding ascii this chart is correctly
printed out
but if we print it as the utf-8 a
unicode it's going to be this uh
kind of the code coding for that rather
than this
special chart there so that's why we
need it as a
sci encoding
and then so here is how can
we can process process each sentence
by putting the space around charts
by removing the actual spaces
filter out all the charts that we don't
want and
add a start and end mark at the end
right and then the next is how we are
going to deal with the whole data sets
because we want to create
say we want to create the training and
test data set
right so the create data set here
obviously well the each sentence
is uh in the document is
by this dash t so we can split
split it and into word pairs
so from here we can create the peers
right so the english and spanish
i think the spanish is our source
sequence and english is our target
sequence
and you can print it out what he looks
like right so
yeah so this is both after the
splitting in the file like we read one
line
read one line and split it into pairs of
sentences
which one is english one spanish
okay and then after doing that the next
step
is we want to tokenize it because well
usually the neural network can only
understand numbers
right so we need to index it
basically just label uh each word
or each punctuations
with uh with the integer number
and there is already a function that can
that is able to do that
such as kira's pre-process
text tokenizer and then you can directly
just fit it on the language
and apply it to the sequence
right so the text to sequence is
basically just
uh from that text
you get array of integers from that so
it's an array
of numbers and then we want to do the
path sequence because the rn
is kind of expect the same length of
input
so we are going to add the
spaces or zero behind to match
to make sure that all the input sequence
are at the same length so this is what
is tokenizing is doing
so here basically just load the data and
tokenize it
yeah so this is calling that law data
function
and you can see that the market the max
length of target nand
targeting so is the english length is 11
and then the input length is encoded uh
into 16. so you can print out
what the tensor final times look like
right so you can see that the training
this is the input for training
so each i think one indicates the start
the start work so it's indicating this
start
and all the other so if you if it ends
early
two means end if it's n's early and then
we are going to patch the sequence
with zero over there
okay so that is the whole preprocessing
process
any questions so far in terms of
this pre-processing
[Music]
are you all able to run it is any of
you running this right now
cool yeah let me know if there's an
issue actual place
okay and then the next step is once we
have this
uh token right so basically or in
tensorflow we usually call it tensors
it basically just factors these vectors
of integers
and then the next we are going to create
a
tensorflow data set sorry it should be
data set
um so the tensorflow dataset so
well i think it can still work without
creating it but just
by utilizing the tensorflow data set
it will make the process so much more
efficient
um and easier to build a pipeline
from the data set so here i also
listed all the configurations
for the encoder decoder
well batch size you can ignore it right
now so batch
batch size is the batch in the
stochastic gradient descent right how
many
um how many samples you are going to
train together right so
of course this setting will affect the
performance
i mean the running performance so if you
increase the batch size to larger number
and then running each batch will be
slower
uh so this this is basically like how
many batches we are going to run
right so so the impedance of
the train is and then divided by the
best set this is how many batches
in the training and how many batches in
in the validation data
uh and since we are dealing with a word
we are going to add an embedding layer
so basically it's kind of like the word
to back
embedding but here
there's we we haven't used any word to
whack we just
automatically derive embedding layer
from that
and the dimension for the embedding
layer is to 256
and then we define the dimensions
for the outputs for the recurrent neural
network
uh and we just started at about around
1000
and then this is the vocabulary size
of the input and the target and because
in the end we are going to map the
output from the rn
into a vector of of length of the
vocabulary size
basically it tells you a distribution of
what is the probability of that word
right so this is required um to define
the final
output from the neural network so you
will see how
it's going to be used and here we create
the data set
right so we basically put our train
um the data and then the y so
this is the x basically and this is the
y
we're trying to predict we are making a
pair of them
together and then we randomly shuffle it
and here basically just batch it right
so we set
our batch set to be 64.
and once we batch it you can see that
every time we iterate through the data
set
you will automatically you know just
give us the batch
of that data a batch of
64. right so that well makes it really
convenient instead of we
writing the you know you know iterating
through the rows by ourselves it will
make it
much more efficient okay so
right now we have created our tensorflow
data set and we are able to
iterate through the batches
and next is to define our sequence to
sequence model
um so i think this is a just a different
taste
right so in the code here um
so this is used right so how we are
going to define a tensorflow model
is by subclassing the
tf keras model there are two different
ways to
define a tensorflow model so one is
through a functional api
right so you can just define the layer
and then output of your layer is serve
as the input to the next layer
as such and then
who is someone enjoying over here
ah
let me uh
i noticed
yeah okay
so mouse yeah
so this is one way to define that right
so uh
uh a different way to define is through
the
subclass in it right so you can um
define your init function so this is
when the class object is initialized
it will automatically create
the definition of all the layers you
need
and then the coil is when you call that
object
right so just like this right so when
you call this object for example model
that you give the inputs to the model
it's kind of kind of like calling a
function right and then it's going to
direct directly invoke this call
function
so don't change the names of these
functions
it's kind of uh predefined right you
just need to fill in
uh what layers you want to define in the
init and how you are going to call
these layers in your core function
and then here basically we can directly
get our
tensorflow model by calling this class
okay so here is what we are going to
implement
so we implement the encoder we are going
to implement the decoder
so right now is without attention
so here the encoder we start classing
from the clearass model
we are going to store all the config
settings
right so what is the batch size what is
the
units the the dimensions of the
encoder output and we are
initializing our embedding layer for the
word
so and we have uh specified the what is
the embedding dimension so it's 256
so that each word or each integer is
going to be transformed into a vector of
256.
so this is our embedding layer and then
this is we define our giu so it's just
one layer of giu
and then we are going to use the keras
layers giu
right so in layers actually there are
many types of different
layers defined already so we're going to
use the giu
so in the gio here phrase we pass in the
dimensions which is 1024 i think
and then here this is important
because we are going to use uh
all the outputs from the encoder for
attention later on so we set it to be
true
so here is whether or not we want to
return the last state of course we need
that
because this is served as the input for
the decoder
so this is also set to be true and this
is how we are going to initialize the
weights
i think just some uniform distribution
and then this is how we're going to put
the layers together
in the core function first we are doing
an embedding layer
and then the embedding is serving as the
input to the giu
and then there's the initial states of
the hidden so that is the first
hidden states so what is that initial
hidden states
so this hidden state is going to be all
zero
in the beginning okay so we can we can
just
test out whether all the dimension
matches and whether it can be called
and then we can just run this sample
input
to see whether the dimension output is
what we expect
right so all the output well the first
dimension will be the batch size which
is 64.
uh and also the sequence length so in
the input output shape
will be the the sequence length of
16 and then the output dimensions for
the rn is 1024
okay and that's the encoder and here is
the decoder
i think it follows very similar
structure
right so we have embedding layers
we have the giu units
uh the only difference is in the end we
are going to add
a dense layer that maps the outputs from
the giu
to the dimensional vocabulary size
right so that we can from that vector we
can infer
um what is the most probable
next word to output the only difference
that it has additional dance layer
there and then in a core function
basically this is how to put all these
layers together
so we have the embedding layers we have
the giu
uh that takes in the input
um the only difference is this
right so is that this uh
encoding um encoding output
yeah so i think this encoding output
should be
served as the input for the gio initial
states it's not
yeah so so later on we are we are going
to give
uh uh like uh the the state from the
encoder
we'll give it to the encoder for the
yeah i think it may be missed one step
over here
right so that this giu the initial
states should be initialized with this
uh yeah i think it maybe runs
later on that we initialize it
outside and then
the output of the iron is going to be
give
given to the the fully connected layer
yeah so again and we can um
generate some uh kind of
sample data and just to see whether this
decoder can be called
and and get us the correct dimension
output
okay so that's the decoder uh
yeah so basically it's all pretty much
the same with the encoder right so it's
uh you're you define your
layers in the image and in core you are
going to
i think maybe it means something over
here
the initial state should be this
or maybe later on it already initialized
outside
anyway um
okay so any questions on the basic
encoder decoder
okay uh the next
let's implement the one basic
attention which is the dot product
uh and here we are going to implement a
new layer
it's a tension layer right we have our
query
and the values basically here right so
here is
uh values and this is our query
and then our tension scores especially
uh dot product
and we are going to implement the sub
max function a weighted sum of the
vectors
right so here the unit product is
through this
right so the query multiplied with the
values and then we
reduce some so it basically tells you
along which
dimension you want to submit the cost
across
uh yeah so you will see lots of expand
dimensions over here basically that is
just to make sure that
this multiplication
the dimension matches with each other
uh in the beginning without the expand
dimension you will see lots of kind of
like dimensional matching
issues over there so so this basically
just to guarantee that all the
dimensions matches
uh with each other and then the soft max
already exists in the tensorflow so
you're passing
a score that gives you the distribution
of the weights
and then attention weights um
multi-multiplier with the values h right
so you get the
uh contact you get your context factor
so you sum it up
sum it all together so this is doing the
multiplication
and then you need to sum it all together
and so this is all sum up together
so that gives you your final context
factor
and again you can test this uh layer
so you can think of this right so the
even the the the layers
the models they are all kind of like a
function
or transformation right so
uh you can directly once you initialize
it you can call it
all right so the uh just by giving
give give it the input and the output
right so you can directly call that
so this is one way to implement the dot
product and then
this is the for the additive attention
right so
well the difference is in the dot
product we don't have any additional
weights to be learned
well in the additive attention there's
uh
all the w uh one weight matrix two
and v is all going to be learned from
the data
or trained so that's why
we need to define there's a init
function
that defines the you know how we are
going to initialize
especially they are a dance layer or
you can think about just a vector right
so it's a
dance layer for w1 that's layer for w2
and another v so the wii if the output
of v
is actually a scalar so that's why it's
a density of
just a unit one so that after applying v
uh you will get a scale out of it
okay so here is the same right so you
expand dimensions so that you add one
dimensions
over here uh and then
this is the how we calculate the scores
right so the w1 is uh
is the lay is a layer it's a dance layer
and the inputs we give the values to the
input so it represents this
and then we pass on the query to w2 so
it
represents this we sum it up together
and then we apply the tan tange function
and then finally we apply the v v layer
so this serve as the input to the v
layer that gives us
the final score out of that
and after we get the final score we take
the soft max
to map them into a distribution of
weights
and yeah so the context factor
yeah and then we again it's the same so
this step is the same with the dot
product
all right so here is basically just
define
uh attention layers so why is dot
product tension and the other is
this attention and then the next is how
we are going to implement this structure
uh earlier we have introduced uh the
basic
encoder decoder right so here is
how can we add this attention layer
to the decoder so
basically i wrote another decoder
function
um this
is still the same with the basic decoder
the the difference is there will be a
tension layer
so i i put it here is
well you need to pass in the attention
layer
right you initialize your attention
layer outside
and then if it's none then basically
it's the basic
uh decoder without attention
or you can initialize it with the dot
product attention
and then here the tension layer is
initialized as the dot product
attention layer and then in the call
here so if the attention is surpassing
uh from the tension layer you are going
to get your context vector
and the tension weights right so that is
defined by your
attention layer over here so you are
going to get your context vector and
attention weights
and then we are going to put concatenate
the context vector with your input
so basically it's this step right so the
context vector is concatenated with your
input
uh that is your new input and then you
pass on the new
input to the to your gi u
right and then yeah
so and then the the this step is the
same with the basic encoder
so the only difference is this is you
passing the attention
and you get your
current hidden layer to the tension
and then the then you get your context
vector from an attention layer
and concatenate it with your input as
your new input
to your gi u layer
that's the modification on the basic
ones
so that that that defines it so that is
all for the
um decoder
definition and the next is how we're
going to train it
right so we are using
first we need a a loss function
over here so we are using uh
basically a log loss so if the true
label is one
right so if we predict probability for
that is 0.6
and then you see the loss will be very
small but
if we if the true level is well and we
predict a very small probability
it will incur a very large loss so this
is what the loss function is doing
right so here we directly use the sparse
categorical cross entropy
function from tensorflow clearance
losses
yeah it's pretty straightforward and and
actually returning from the
we use the reduced mean so that is the
mean
loss for that batch
and in the training here
we are we define a train step
and in the train step it's going to do a
gradient descent
right so with the tensorflow gradient
tape
um it can do this automatic
differentiation
so basically we get our encoder
and then the input first we still need
to add this start
kind of starting signal in each of our
sentence right and then we pass it in
to the decoder um
that is that's served as the first input
to the decoder
right and then um
while it gives our predictions and then
we pass on the predictions and the true
labels to the last function
that gives us a loss
uh and over here i think this
is the last um basically
so because we were doing this for loop
right for the
length of i think 11 so for the target
sentence the maximum length is the ec11
all right so we are doing this and then
we uh divide it by
11 so that's kind of the great the loss
like average loss over time
um this is our batch loss over time that
like sum
across the sequence uh
and then here is our variable so from
here we can
directly get all the train trainable
variables just by
you know encoder you get your trainable
variables and decoder you get a not
over available all right so this is the
uh kind of iterative way uh
to like to train one batch how we are
going to train one batch
well you will notice that there is this
first this is this
a tf.function basically if you add this
tf.function
over here around
a big function it will actually speed up
the process
in the training
and then we also defines how we are
going to calculate the validation loss
i think it's pretty much the same uh
with above
the only difference is that we only got
the loss we don't apply the gradients
it's only on the training data we apply
we we apply the gradients but in the
last week
you know validation we just calculate
the validation of that's it
and then here is we define our training
how to train a sequence to sequence
model
all right so here we initialize our
encoder
we initialize our decoder over here
and then uh at each
epoch right um
uh and then we train it for example for
10 impacts
and within each epochs we take
a batch of data and then we pass it the
input and target to the train step
function
that well that we we have
applied over here right so that that
calculates the
gradients and doing the stochastic
stochastic gradient descent
and then it returns this batch loss
over here and
uh and basically after
training for for the epoch over here
and then we are also going to uh take
the batches from the validation data
and calculate the validation loss
uh yeah they should pretty much get the
same results
because we assume each sentence is
independent but you will
notice some difference in the
performance because well is
this the classic reading design you
cannot
assume them to be the same uh every in
every run
right so you are observed sometimes the
the performance will vary from rent to
run
but definitely definitely the uh is not
water dependent
okay so this is what the training
process is doing is basically
loops of the earpods loops of
the badges
uh and here
once we define what this universal
function
it will become easy for us to compare
different models
uh for example over here the attention
we first set it to none basically it
means we just train a basic sequence to
sequence
right so we pass on the attention layer
to the this function and then
and then yeah so it will start training
and we'll report on the
training loss and validation loss so we
just run it
for 10 epochs
so it's kind of becomes flat in the end
for the basic model and then for the dot
product we can directly just
initialize our dot product attention
layer
uh pass it down to this training
functions
and also training for 10 epochs you will
notice that
the value the loss is so much better
for that and the same for the additive
attention right so the additive
again is the same function we just
applied
we just input the attention layer to
that
and then in the end we i basically put
all the validation laws together and
then we can compare different models
right you can see that without
no no attention basically i don't think
it's performing
you know well it's just stays pretty
high loss
and for that product is actually
actually it's pretty good job for that
product because that product does not
um have any additional
weights uh to the no attention right so
it's just doing a simple dot product
and it's already so much better than the
encoder decoder without any attention
and for the additive
tension because it introduced more
weights
it it kind of more flexible to calculate
the scores
so you can see it's ma it's also much
better than the dot product
there's another leap of
improvement over there right so it's
pretty amazing to see this
differences
so i see your question in the chat so
how do we calculate
uh the size of the sample
sequence
uh right now there's just one layer i
haven't tuned
any models is that what you mean the
number of lstm layers
right now just just just one
giu layer so it's a giu layer
and then the units yes so the units is
preset it's basically the dimensions
we want to encode the sequence is
preset to be 1024 yes
and this this this is definitely the
parameter can be tuned on
and the size of the sample sequence
what do we mean by this so the size of
that you mean the 16 and 11
like like what's the sequence length so
that is inferred from the data
right so uh for example in the training
data
uh we connect all the sentences we just
realized okay the longest
uh maximum length of the sentence is
freedom is 11
and then it's set to be 11 and then we
are going to
pat every other shortened sentences with
zero
so in this tutorial there's no
uh like model tuning or anything so
there's no
uh model tuning step but it yeah it
should be definitely especially
the embeddings for the um
the dimensions for embedding layers the
dimension for the giu
you can definitely chew on that
so after the model has been trained
we can apply this to translate the
sequence
right so this is the translate function
and we can pass in
any trained encoder decoder
uh
yeah so
and and again for for each input uh
sentence we are still
going to pass them if
first we are going to set an image of
the max lens because that is
the lens that the giu can handle and
if it's shorter than the max lens then
we are going to
uh pad pad it with zero
right and then
from here uh the only difference in the
training
is we are we are uh the up we are we're
basically
uh putting the predictions
um i think into the next input
right yeah so the predicted id so
there's no teacher enforcing
or here so whatever we predicted over
here is going to serve as the input
uh to the next unit right so
next decoding unit over here
right so we are we are going to keep
predicting the word until
the output corresponds to the end which
marks the end of the sentence
so this is how we are going to uh
translate
and then you can apply the translate
over here
right so this is basically the results
from the additive attention
additive attention uh encoder and
decoder so we pass it in and we want
this in
order decoder translate this sentence
and it just
looks well over here but if we use the
dot product
encoder decoder uh you translate this is
my office
and i'm feeling like okay life equal to
office in some sense
yeah uh and then yeah you can apply it
to
any other uh sentence
as well but you note that this this
data set is a pretty small so the limit
is said to be
then i think the name is set to be 30
000 so it's not using the whole data
so that at least the training time is
um you know
yeah so the number of example you set to
be
a smaller set so it's not the full set
of the data
so we are not taking advantage of the
whole set
just to reduce the training time but
yeah if you want to do that you can
you can trade on a much larger data set
and also to the questions in the
comments we are going to do
there will be more model tuning required
if you want to improve the performance
uh and also we can implement other
attention layers such as the
multiplicative
attentionator and and other
attention functions to calculate the
similarity scores
you can also apply the similar framework
for other sequence to sequence task
yeah so that's pretty much it so i
basically uh
pretty much work through
all the calls and i think i find one
back there
uh for the decoder
yeah so just let me know whether it runs
well on your
actual place and if you observe or find
any issues
feel free to comment it on the
on the issues over here so
cool uh
yeah so i hope that this can serve as a
template
for any sequence to sequence model right
because i kind of try to make the
function
uh universal right so you can you can
pass in uh for example the
the decoder with the attention right so
you can pass in whatever attention layer
you have depop defined
and still can do the job
yeah so that's it for the course
and uh and of course uh you can also add
more complexity to the model by adding
for example more um
giu layer right now it's one layer right
so you can add multiple layers to that
and also you can add
bi-directional as a bi-directional gr
or you can change the giu to our lstm
i heard that lstm usually performs
better
so you can you can do that right so this
is just serve as the beginning or serve
as a template
for any you know flexible flexible
modification you want