" After this, we will update the alpha term unlike AdaGrad there is one more term in this equation known as 'rho' this is used to avoid infinitely increasing alpha value.  As we saw in AdaGrad, the alpha value was increasing with each iteration but in the case of AdaDelta, the alpha first increased till 50 iterations and then continuously decreased so now there is no problem of learning rate decay. "
" Now, let's visit our Adam again... Adam's main motive was to add the concept of momentum into the previous optimizer like AdaDelta. "
" So,   you find that this RMSProp which takes  expose exponentially decaying average,   the AdaDelta takes a moving window average of  the squared gradients.  So, that is the only   difference between RMSProp and AdaDelta. "
Adadelta has a very slow start. 
"I think Adadelta performs better with bigger networks than yours, and after some iterations it should equal the performance of RMSProp or Adam. "
 As you can see from these plots the alpha is increasing with every iteration and the learning rate is decreasing so this is quite problematic. 
" This is just like AdaDelta... instead of rho, we are writing beta 1 and beta 2. "
" So, in today’s class we will  talk about two more algorithms,   one of them is RMSProp and the other  one is Adam and we will also see a   very closely related algorithm which  is very closely related to RMSProp. "
" And we said   that there is a very closely related algorithm  very closely related to this RMSProp; so,   that closely related algorithm  is what is known as AdaDelta.  So, we have a closely related algorithm known as  AdaDelta. "
" And  in fact, both these algorithms were proposed   almost simultaneously, but independently, and  both of them gives almost similar performance. "
" Then you  have NAG Nesterov accelerated gradient operation,   then you have Adagrad, then you have a  Adadelta, then you have RMSProp. "
"If you are working with something like neural machine translation, this sparsity is an issue. "
"Very few people use it today, you should instead stick to: "
"This algorithm is very similar to Adadelta, but performs better in my opinion. "
