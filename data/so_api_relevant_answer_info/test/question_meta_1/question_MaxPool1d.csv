,Keras load sound instances in batches,<python><keras>,"I need help trying to figure out how to achieve batch loading with Keras.
So far I'm trying to make a song classifier with Keras CNN. I've built the model below for 10 genre classification.
[code snippet]
It's working when I load the instances and labels myself but my computer can't handle 1000 songs at once. I tried using ImageDataGenerator to load them in batches with flow_from_directory. The code is as below:
[code snippet]
I ran into the problem of audio files not being images so I added .wav to whitelisted file formats in 
[code snippet]
This let Keras to find the audio images but it can't really open them. I changed where it is opening them using Pillow to Librosa but it gives more errors. I don't think I can change all of them, so I was wondering if there's a way to achieve batch loading?
Edit: I came by to [this question (hyper-link)] which pointed me to Keras [sequences (hyper-link)] I implemented one as seen below.
[code snippet]
This time the train times got ridiculously long. Previously, 10 epochs were completed in 3 hours but now it takes 14 hours for one epoch. Is there anything I can do to reduce train times?
Edit 2: Changed steps_per_epoch parameter in fit_generator function and it is down to acceptable levels.
",www.stackoverflow.com/questions/49349522
,Conv1D with kernel_size=1 vs Linear layer,<python><conv-neural-network><pytorch>,"I'm working on very sparse vectors as input. I started working with simple Linear (dense/fully connected layers) and my network yielded pretty good results (let's take accuracy as my metric here, 95.8%).
I later tried to use a Conv1d with a kernel_size=1 and a MaxPool1d, and this network works slightly better (96.4% accuracy).
Question: How are these two implementation different ? Shouldn't a Conv1d with a unit kernel_size do the same as a Linear layer?
I've tried multiple runs, the CNN always yields slightly better results.
",www.stackoverflow.com/questions/55576314
,"Getting : ValueError: Expected ndim=3, found ndim=2. Full shape received: [100, 1000] on running model.fit() in tensorflow's keras module",<python><deep-learning><tf.keras>,"So I am trying to make a CNN for doing sentiment analysis on COVID19 tweets. I have made a basic CNN which is supposed to be trained from the top 1000 word count vector of each tweet which are labelled as positive or negative.
[code snippet]
[code snippet]
Now, the model compiles just fine and I think that the input shape is as per the documentation too i.e (Number of elements, Length of each element).
The problem arises when I try to run the .fit() function.
[code snippet]
Here I get...
[code snippet]
I have tried to make different variations which I thought could potentially be the issue but no luck. I don't face any issue in Conv2D which simply takes the shape of a single element as the input_size and the whole features and labels array in the .fit() method. So, what more needs to be done here with Conv1D other than the different input_size format which I have already given.
",www.stackoverflow.com/questions/63074769
,"ValueError: Negative dimension size caused by subtracting 2 from 1 for MaxPool1D with input shapes: [?,1,1,128]. Full code,output & error in the post:",<python><tensorflow><machine-learning><deep-learning><conv-neural-network>,"I have an error while making the following CNN model:
[code snippet]
The output and the error while executing:
[code snippet]
[code snippet]
The data I'm trying to input is features_train which has the shape (2363,2,1). I believe this is some issue with input_shape and dimensions. I'm new to neural networks, so any help would be appreciated. Thanks
",www.stackoverflow.com/questions/68372530
,"ValueError: Input 0 is incompatible with layer model_16: expected shape=(None, 335, 48), found shape=(None, 48)",<tensorflow><keras><conv-neural-network><layer><max-pooling>,"I'm training my model in order to classify sleep stages , after extracting features from my signal I collected the features(X) in a DataFrame with shape(335,48) , and y (labels) in shape of (335,)
this is my code :
[code snippet]
I got the error :
Input 0 is incompatible with layer model_16: expected shape=(None, 335, 48), found shape=(None, 48)
",www.stackoverflow.com/questions/65461201
,Keras Transfer learning with own model,<python><tensorflow><keras><transfer-learning>,"EDIT:
Apparently some issues with keras vs tensorflow.keras caused my problem. I followed this to change my imports:
[The added layer must be an instance of class Layer. Found: <tensorflow.python.keras.engine.input_layer.InputLayer> (hyper-link)]
I'm trying to load a model and exchange the last dense layer with another one of different output dimension.
This is what I've got so far:
The model as it is saved:
[code snippet]
and the attempt to retrain:
[code snippet]
I get the error output:
[code snippet]
I've only found information on models like VGG16 but cannot apply that to my own network. Is what I've done in the retrain() function an appropriate approach? How can I make it work?
",www.stackoverflow.com/questions/62575639
,CNN1D learns only one class in a binary classification problem (Keras),<tensorflow><keras><conv-neural-network>,"I try to find in a graph the nodes which belong to a particular structure (for example a clique). I must therefore have at the output a vector [0,0,1,0,1,1,0 .......] where the 1 represents the nodes belonging to the cliques.
My inputs are graphs where each node of the graph is represented by an embedding vector, the input is in this form:
[code snippet]
the problem is that my model only learns one of the two classes, either (1) or (0) depending on the greater presence in the dataset. after doing a data rebalancing the results are around 0.5 accuracies.
I tried the data rebalancing, change of embedding method but the result remains the same.
Does anyone have any idea what is causing the problem?
here is the code:
[code snippet]
I am not sure of my (activation function, loss, metric) even if the best results given are with this.
[code snippet]
thank you in advance for your feedback;).
",www.stackoverflow.com/questions/66768051
,"Error when checking target: expected 3 dimensions, but got array with shape (32, 6)",<python><arrays><tensorflow><keras><valueerror>,"I'm working on re-purposing a signal classifier.
My train and validation generators feed batch arrays of [32, 3000, 1] into the model (32 signals, 3000 time steps, flattened). 
basic_model encodes each signal to a vector 1D vector before seq_model maps this vector to a class.
To get an idea of architecture, the code to build the model can be seen below.
[code snippet]
Running this code gives the error:
ValueError: Error when checking input: expected input_1 to have 4 dimensions, but got array with shape (32, 3000, 1)
Checking model.summary() - input_23(InputLayer) Output shape: (None, None, 3000, 1). Trying to change seq_input to Input(shape=(3000,1)), I get the error ""Input tensor must be of rank 3, 4 or 5 but was 2"". So 1 None is used to fulfill the tensor rank requirement and from documentation the other is essentially a placeholder for batch size. 
Learning this I next tried reshaping my training inputs from the generators. I did this by using X_train = np.expand_dims(X_train, axis=0) to give an input array of [1,32,3000,1]. 
This got me almost through the model but I get the error message Error when checking target: expected conv1d_161 to have 3 dimensions, but got array with shape (32, 6). (conv1d_161 is the output layer of seq_model.)
My question is how can I format my batch arrays to work with this setup? Following on, what is the dimension I'm missing?
Thanks for your time:)
",www.stackoverflow.com/questions/60425309
,"ValueError: logits and labels must have the same shape ((None, 328, 328, 3) vs (None, 1)) with autoencoder",<python><tensorflow><machine-learning><keras><deep-learning>,"I am trying to build an autoencoder with the following code
[code snippet]
For my training data i use:
[code snippet]
When I try to run it with autoencoder.fit(
train_ds,
validation_data=val_ds,
epochs=50
) I get the ValueError: logits and labels must have the same shape ((None, 328, 328, 3) vs (None, 1))
Does somebody know how to fix this?
",www.stackoverflow.com/questions/63841519
,Issues converting Keras code into PyTorch code (shaping),<machine-learning><keras><deep-learning><pytorch>,"I have some keras code that I need to convert to Pytorch. I am new to pytorch and I am having trouble wrapping my head around how to take in input the same way that I did in keras. I have spent many hours on this any tips or help is very appreciated. 
Here is the keras code I am dealing with. The input shape is (5000,1)
[code snippet]
Here are the results of model.summary() from the keras code
[code snippet]
Here is what I have made in pytorch
[code snippet]
Lastly here is the model summary for what my pytorch model creates
[code snippet]
",www.stackoverflow.com/questions/55636138
