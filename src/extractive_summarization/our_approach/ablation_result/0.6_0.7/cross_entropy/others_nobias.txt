In your example you are treating output [0, 0, 0, 1] as probabilities as required by the mathematical definition of cross entropy.  But PyTorch treats them as outputs, that don’t need to sum to 1, and need to be first converted into probabilities for which it uses the softmax function.
You can interpret the above true distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C. 
 So this particular function, which is identical to MNIST loss plus �.log� jhas a specific name and it's called binary cross entropy, and we used it for the threes vs. sevens problem, to, to decide whether that column is it a three or not, but because we can use broadcasting in PyTorch and element-wise arithmetic, this function when we pass it a whole matrix is going to be applied to every column. 
 So here's  this special case of binary cross entropy,   and now our rows represent rows of data, okay,  so each one of these is a different image,   a different prediction, and so for each  one I'm just predicting are-you-a-cat,   and this is the actual, and so the actual  are-you-not-a-cat is just one minus that. 
 If we use negative  log likelihood loss or cross entropy in pytorch.  But  numerically, like stability wise on the computer, the cross  entropy one is more stable.  So and also for this one, really pay  attention to this one, it's taking the logits as input. 
