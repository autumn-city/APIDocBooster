[Music]
hello and welcome to our new video
in today's lecture we'll be discussing
batch normalization and we will learn
how to apply to train a deep neural
network
after finishing the theoretical part we
will explain how to implement batch
normalization in python using pytorch
so let's begin with our video
in order to understand batch
normalization first we need to
understand what data normalization is in
general
when training a neural network usually
our goal is to normalize or standardize
our data as part of the preprocessing
step
this is the step where we prepare our
data to get it ready for training
normalization and standardization both
have the same objective of transforming
the original values
of a data set to new values
and in order to put all the data points
on the same scale
a normalization process consists of
scaling
the numerical data down to be on scale
from 0 to 1.
now let's explain this a little bit in a
little bit more detail
so suppose
suppose that we have a set of
positive numbers from 1
to 100
to normalize this set of numbers
we can just divide each number with a
largest number
in the data set in this case
it is a number 100
in that way our data set has been
rescaled to the interval from
0 to 1.
a typical standardization process
consists of subtracting the mean of the
data set
from each data point
and then dividing the difference by
standard deviation of the data set
so basically we take every value x
inside the data set and transform it to
its corresponding
z value using this formula
after performing this
computation on every x value inside
inside our data set we have a new
normalized data set of z values
the mean of the data set is calculated
using the following equation
it is a sum of all values divided by the
number of values
to calculate the standard deviation we
use this equation
we take the difference between each
element
and mean square those differences and
then average the result
the standard deviation is just the
square root of the result
so we already explained that
normalizing by dividing by the largest
value here the effect of transm
transforming the values of the data set
to the inter interval from zero to one
and as you can as you can see
looking at this formulas the effect of
the standardization process
is that the mean value of the data set
is transformed to 0 and standard
deviation is transformed to 1.
so why
do we do this
well in general the purpose of both
normalization and standardization is to
put our data on the standardized scale
if we didn't normalize our data we may
end up with some values in our data set
that might be very high and other values
that might be very low
for example let's say that
we want to learn how much some company
pays its employees
we have a data set that consists of the
company's salaries
that ranges from highest to lowest so in
this data set we may have someone with a
salary of let's say million dollars and
someone else with a salary of only one
thousand dollars
this data has a relative relatively wide
range and it's necessary uh
and isn't necessarily uh on the same
scale
additionally each one of the features
uh
for each of our samples could
vary widely as well
if we have for example one feature which
corresponds to the age of the employee
and then another feature corresponding
to the gender of an
employee we can also see that these two
pieces of data will not be on the same
scale
so this is a huge problem because this
imbalance can cause instability in
neural networks also
non-normalized data can significantly
decrease our training speed
however there is another problem that
can arise
even when normal even with normalized
data
when we normalize a data set we are
normalizing the input data that will be
passed to the network
then the weights in our model
become uploaded over each epoch during
during training
so
what will happen if during turning one
of the weights
ends up being drastically larger than
the other weights
[Music]
well this large weight will then cause
the output from its corresponding neuron
to be extremely large
and this imbalance will again continue
to cascade to the network uh causing
instability
and this is why we need to apply batch
normalization to layers within our
network
when applying batch norm to a layer
the
first thing that batman does is that
it's normal it normalizes the output
from the activation function
after
normalizing the output from the
activation
function
batch norm
then multiplies
this normalized output by some arbitrary
parameter and then adds another
arbitrary parameter to this resulting
product
this calculation
with the two arbitrary parameters sets a
new standard deviation and mean for the
data
these four parameters
are
uh
trainable
meaning that they
they also will be become optimized
during the training process
and this addition to the addition of the
batch norm can
greatly increase the speed and accuracy
of our model
so when we apply standard normalization
the mean and standard deviation values
are calculated with respect to the
entire data set
on the other hand with batch norm the
mean and standard deviation values are
calculated with respect to the batch
now we learn the basic theory behind
batch normalization
so let's see now how we can apply a
batch norm in python
for our experiment we are going to build
the length 5 model
the main goal of planet 5 was to
recognize hand-written digits
it was invented way back in
1998
and it was the first convolutional
neural network
this network
takes 30 32 by 32 grayscale images as an
input
uh so to train the model we are going to
use the fashion mnist dataset which
consists of 10 classes
uh and for our experiment we are going
to create
two networks
first we are going to train the first
model
without batch normalization and for the
second model we are going to apply the
batch normalization
and our goal is to train these two
models and then compare their accuracy
in order to see which one
performed better
so let's begin without
with our experiment
first we need to import
necessary libraries
and then
we will define
variable devices
which will store cpu or gpu depending on
what we are training on
and the next step
is to download the fashion and list for
training and validation
so after we downloaded our data
set uh we need to apply function
transform.normalize
and this function will
transform
our data
so
let's type
transforms dot
normalize
and here we will specify
our mean
and our standard deviation
okay
uh so as we already explained this
function transform dot normalize
uh
will
normalize our data
so
inside the function we first we
normalize
the data by dividing
maximum
pixel value
in our
image
dividing each pixel with the
maximum value of the pixel and that and
in that way uh we will
uh
normalize our
data set to be in range from zero to one
and then
uh we can apply standardization process
uh
and we already explained uh
we are standardizing uh each pixel value
by
calculating z value
and z is calculated uh
by
subtracting from each pixel we subtract
the mean
and then
dividing that with
the standard deviation
and in this case we specify that mean
and standard deviation are equal to 0.5
so
let's say that maximum value is one
one minus zero
point five is zero point five divided by
zero point five is one
and
let's say the minimum minimum value is
zero
zero minus
zero point five is minus zero point five
divided by zero point five
five it will be minus one so in that way
we standardize our data set
to be in range from
-1 to 1
and
we already explained that
our goal here is to
show you
difference
between training with
data normalization and without data
normalization so
we'll create two models
first model
is model where
normalization and batch normalization
is applied and the second model is a
model where
neither of the leader of them
is applied
uh so
the next step is to extract
one batch from the tennis set just to
check the shape
of our images
and here here we can see that we have
images of 32 by 32
uh one channel channels that mean that
it is a grayscale image and we also have
64
labels
okay and now here we can see our two
models first model is called planet five
and this is standard model where batch
numbers
where we didn't apply batch
normalization
so this is our landed five
norm
model where we
where we're going to apply the batch
normalization
note that
our data are already normalized
so input data are already normalized
and
this is the same model as the previous
one without batch normalization so we
have a convolutional layer value and max
pulling
and here
we going going to apply
batch normalization using this function
batch norm 2d we're using 2d
because
here we are dealing with
images
and inside the brackets we need to
put
the input features to our
batch normalization and
this is the same number as the output
feature in the convolutional layer so
output feature in a convolutional layer
will be the same
the same number of features we put in
the
batch normalization
and know that note that you can use
batch norm
uh
multiple types
but here for our experiment uh we will
use just one batch normalization in the
convolution layer and other one here
in the linear layer
only now
we use batch norm 1d
because this is the flattened this is
the vector and
we just need one dimension and also
number of output features in the linear
layer will be
number of input features in our
batch
layer
and now when we
define our two models
we just need to
uh
call the model and
set it to work
on device that we defined in the
beginning
and here we're defining our optimizer
and criterion and also
we define the
schedule object
uh
in our previous post we already
explained uh
what is uh learning late uh
scheduling
uh
this is so we here we we specified the
learning rate the initial learning rate
to be equal to 0.001
and uh
we're using schedule to change the
learning during the training process
here we're using method step
learning rate
and we
as a parameter we specify
step size
and gamma factor
here the step size is equal to 5 and
gamma is equal to 0.1 so
this means that
every 5 epoch
we are going to change our initial
learning rate
for uh this gamma factor so our initial
learning rate is
0.001 and after five epochs it will be
0.0001
okay
and after that we just need to
train our model here we specify the the
number of people is dirty so we will go
and train
with 30 epochs
and okay let's run this
and see
what result we will get
as you can see we achieved the
validation accuracy of almost 86.5
with the model where batch normalization
was not applied
this is pretty good uh result especially
because uh training
accuracy tends uh to increase if we
uh continue training for more ebooks
however on the other hand uh the model
where batch normalization was was
applied
achieved the validation accuracy of
almost ninety percent
and that is three percent improvement
compared with the model without batch
number
also
the bachelor model has much faster
convergence
that's mean that it reaches minimum much
faster than the model without batch norm
here we can see that at 10th epoch
here
uh
the model without battering machine 85
let's say close to 85
percent accuracy while the batch number
model at the same epoch
achieved accuracy of 89 percent
so applying the uh
normalization and batch normalization
can be very useful and it can and can
help you to improve performance of your
model
so that is it for this tutorial if you
like this video drop a like and
subscribe to our channel
bye