Hi everyone,
today we'll talk about
how to accelerate autonomous vehicle
development using Amazon SageMaker.
I'm Nitin Wagh,
ML specialist at AWS
and with me, I have Alex Bain,
ML engineer from Lyft Level 5.
So, we'll start off
with typical challenges in machine
learning in AV development,
then we'll deep dive
into Amazon SageMaker
capabilities for machine
learning development in AV,
and then, Alex
will walk you through
how Lyft Level 5
is using Amazon SageMaker
for hundreds
of their scientists.
So, let's get started.
So, autonomous vehicle,
also known as self-driving vehicles
are designed to travel
between destinations
without a human operator.
It takes an incredible
amount of data processing
and compute
to make this happen.
Autonomous vehicle
development lifecycle
consists of data acquisition
by driving cars on the road,
then massively ingesting the data
to Cloud at regular interval,
then data pre-processing,
data-labeling,
mapping, model training,
and simulation.
Today we will only focus
on machine
learning for autonomous
vehicle development.
So, there are three primary stages
in machine learning lifecycle for AV.
First, you label
terabytes of LiDAR 3D point Cloud
and image data,
then set up and manage GPU clusters
for machine learning training,
and then, deploy the trained model
into cars for testing.
Typically,
matching learning users
at AV
includes researchers and engineers.
AV testing vehicles can generate up
to 20 to 60 petabyte data per year.
And model training
for fully autonomous vehicle
requires hundreds
of thousands of GPU
based servers for machine
learning training.
So, there are several challenges
in ML development for AV.
For first, you know,
data labeling at the scale
with terabytes for data
gets complex and costly.
Then, setting up
self-managed infrastructure
requires significant engineering
so multiple teams
can experiment
in a matter of hours
and, at the same time,
you have to manage cost of GPUs.
And once, models are ready,
they need to be turned
into a deployable model,
so they can run into
hardware installed in the card.
And in absence of
a standard platform,
there is often a divide
between developer environment
across the teams
leading to knowledge silos.
So, let's see
how Amazon SageMaker
can help you overcome
these challenges.
Amazon SageMaker is
a managed machine
learning service adopted by many
AV ADAS and automotive customers.
It eliminates
undifferentiated heavy lifting
by providing
on-demand capabilities
to manage the machine-learning
lifecycle for AV development.
To begin with, it provides
SageMaker ground truth
for data labeling for 2D images
and 3D point Cloud at scale,
then it uses, then users can leverage
managed data
preparation capability
for data pre-processing.
For example,
image to TF record conversion
then they can launch
managed one-click model training
on single GOU
or multi-GPU training clusters
and once a model is trained
then optionally customers
can leverage SageMaker Neo
to compile model
for target
hardware architecture.
It is a standard platform
that can be leveraged
by research
and engineering teams
and supports industry-leading
frameworks like PyTorch, Tensorflow,
and MX Net.
Users can manage
and compare track experiments
and also profile or monitor
long-running training
jobs automatically.
Developers can continue
to use their existing
IDEs like PyCharm,
VScode, you know, Command-Line,
or Jupiter Notebooks
using Boto 3 or Python SDK.
And customers
who are using Kubernetes
can also leverage open-source
projects Kubernetes components
for SageMaker or SageMaker
operators for Kubernetes to leverage
SageMaker capabilities
right from within Kubernetes.
So, let's discuss
these capabilities in depth.
Based on our discussion
with many AV customers,
what they need is scalability,
visibility,
and control in their data
labeling pipelines.
So, SageMaker Ground Truth
is a fully managed
labeling capability within SageMaker
that provides 30 plus
computer vision
workflows for 2D image labeling,
a task like semantic segmentation,
barning box, instant segmentation.
And it also provides production
gate lider 3d point cloud
for object detection
tracking and segmentation.
It supports assistive tooling
to increase the speed of,
you know, annotation using machine
learning capabilities like Dexter,
Interpolation,
and many more.
It also provides
auto-labeling capability
to reduce the cost of labeling
by up to 70 percent.
Customers can continue
to use their internal teams
or use 8000 managed annotators
available via AWS marketplace.
And all capabilities are
pay-as-you-go
with no lock-in and customers
can qualify for volume discounts.
Now, once the data is labeled,
next step is data pre-processing.
It's a common practice
in the world of machine
learning is to shuffle and process
the data before training.
So, SageMaker processing
allows you to launch data
pre-processing
for large volumes of data
on a distributed cluster
using on-demand CPU or GPU clusters.
You store the data
in Amazon S3 which acts as a source
and then author processing logic
using Python or Spark trip.
You can also, bring your own
custom container package
with logic and dependencies.
Once you have your script ready,
using one-click API,
developers can then quickly launch
a single or distributed
processing job
and SageMaker takes care of
launching on-demand in cluster.
Now, processed data
is then stored back
into your Amazon S3 bucket
and instances are terminated.
And you only pay for the time
the processing job ran
and is billable by seconds.
So, here is a sample example
of a one-click API
where you can bring
your own script or container,
select the type of instance and
provide input and output location,
and then, SageMaker takes care
of launching instances
using single-node
or distributed processing.
For distributed processing,
data sharding is a common technique
and it is also supported
and based on S3 prefixes or keys.
Once the data is processed,
the next step in the machine-learning
lifecycle is training.
One of the most
powerful capabilities in AV
is SageMaker managed training.
You don't have to invest
engineering time to set up
and manage training clusters
and use on-demand GPUs
when needed for training.
For using one-click API,
you can use Amazon S3,
Elastic File System,
or high throughput FSx for Lustre
as source for SageMaker
training and SageMaker
provides you built-in containers
for TensorFlow,
PyTorch, MXNet where you can
bring your own script,
or you can just use
the custom container.
Using one-click API
via Python or Boto3 SDK,
you can then launch
SageMaker training job.
SageMaker handles
launching the container,
using on-demand or Spot instances,
streaming your data,
and it'll optionally monitor the job
for conditions
like vanishing gradients,
or exploding gradients
at no extra cost.
Once the training is completed,
model is saved back
into your Amazon S3 bucket
and instances are terminated
and you only pay for what you use
during the training billable
by seconds.
Now, you can also use
managed-spot instances
for training to save cost by up to
70 percent over on-demand instances.
So, here is a simple example
of SageMaker one-click API
using Python SDK.
Optionally, you can define
monitoring rule
such as vanishing gradient here.
Then, using estimator API,
you provide your script
or custom container location.
Then, select number of instances.
Then, choice of GPU
like P3.16xlarge with 8 GPUs
or other suitable instances
in P3 P2 family.
Then, you can optionally use
the managed spot
training by specifying
this simple flag
and SageMaker will use spot
instances for training.
Then, you call estimator.fit method
with the source location
which is in S3.
And finally, provide experiment
tracking configuration
like trial names,
stage, so,
SageMaker can attach training
to a trial
that can be tracked
via API or virtually.
Modular SageMaker training API
is consistent across all frameworks
like TensorFlow, PyTorch,
and MXNet, bringing standardization
for machine-learning training.
Now with this, I'd like to invite
Alex Bain from Lyft Level 5
to share their experiences
about SageMaker journey,
used by hundreds of researchers
and machine-learning engineers
of Lyft Level 5.
Alex, over to you.
- Thank you, Nitin.
I'm Alex and first,
I'd like to tell you
some about Lyft Level 5.
So, Level 5
is the division of Lyft
that's building an autonomous
ridesharing service.
We started in 2018.
So, relatively new
but we were working fast.
Right now, we are actually
testing our AVS
in the San Francisco Bay area.
Our service is not open to the public
but you might see
some of our vehicles.
If you do, they are being operated
by our fleet of safety test drivers
and we are working on them,
we are working on this service
as fast as we can and iterating
as quickly as possible.
So, to start off with,
I'd like to tell you some about
how we use machine-learning
with our vehicles.
Looking at the screenshots
you can see,
some images of the AVS
that we are building.
On the lower left,
you can see the kind of sensors
that are outfitted
on the vehicle.
So, we have, you can see
these sensors below
and on top of the car.
Zooming in on the screenshot
of the photo
that's on the right for the sensor
that's on the top of the vehicle,
it has a number
of high-definition cameras.
It has radar units.
And the thing on top is
a high-powered LiDAR sensor.
So, these sensors
are what feed
into our machine-learning models
for perception, prediction,
and planning that I'll tell you
about in a second,
and ultimately feed into
our machine-learning models
and help the car decide
how to drive.
So, since you're interested
in this talk,
I'm sure that you must
be interested in AVS
and you've probably seen some videos
online about our company's AVS
or perhaps Tesla's Full
self-driving beta.
But it's always fun
to look at AVS in operation
and look at how they use
machine-learning
to drive on the road.
So, let's take a look at some
screenshots of our vehicles.
So, this first image
is about how the car collects
and processes the LiDAR sensor data
that it uses to,
it's one of the signals that it uses
to get information about the world.
So, if you remember the LiDAR
is the sensor
that's on top of the vehicle
and the way it operates
is that it sends out beams of light
in every direction and so,
those are represented by
the concentric circles in the image.
Those reflect out the elements
on the road
and are collected back
in the sensor
and feed into our
machine-learning models.
And what you see
in the screenshot
is kind of a visualization
of the reflections of things
that the LiDAR perceives
in the world.
So, now let's take a look
at our actual AVS in operation.
So, looking at the screenshot
on the left,
this is a view, the driver's view
looking it out through one of our AVs
which is operating
in an autonomous mode.
You can the Bay Bridge
in the back, in the background.
So, this is driving
in San Francisco.
Now, we are driving along,
and we see cars in the world,
and the other lane,
a couple of buses pass
and now looking on the right,
this is the view of how
the AVS perceiving the world.
We may call these models
our perception stack.
So, these are machine-learning models
that are about detecting the cars,
pedestrians, cyclists,
traffic signs, stop signs,
and probably surface areas
like the line, the lanes,
all the elements in the world
that the AV needs to really perceive.
You can see in the screenshot
that the blue boxes
represent the vehicles,
and you'll see, you can see
the vehicles in front of us,
you can see a bus pass by
on the left if you're quick enough.
A stoplight will come up and there
are no pedestrians in the screenshot
but you'd see them when they are
highlighted in the different color.
Now, let's turn to our different
class of models beyond perception.
The next one is about prediction,
specifically motion prediction.
This is a very different
screenshot
and focus your eyes
on the screenshot to left.
So, what you see in the middle
is the red square
is our autonomous vehicle.
Now, you can see
it's driving on the road,
and the lines obviously
represent lanes.
The yellow squares are the motion
of other vehicles on the road.
We'd also capture pedestrians
and other elements
but this is just, it's vehicles.
If this information,
it feeds into our machine-learning
models for prediction.
So, what's going on here
is that as the car is driving,
it's recording the motion
of the paths
taken by all the other cars
on the road
or other pedestrians or cyclists
or other elements on the road
and we're gonna feed that information
into our prediction models.
As the car is driving,
it's constantly making predictions
about where all the other elements
on the road
would be over
the next few seconds
and using that to understand
where their motion will be.
Taking these two models together,
they actually feed into our third
class of models which is planning.
So, planning really is where
everything comes together
and the river meets the road,
so to speak.
Planning is all about how the car
decides where to drive,
decides what lanes to drive in,
how fast to go,
whether to stop,
accelerate and so on.
What's going on here, as you can see
this AV just passed the stop car
that was ahead of it
which is kind of the red car
and you'll see it again in a second.
Now, as you see in these perception
models to understand
where all the cars on the road
or other elements on the road
such as stop signs, traffic lights,
people, the lanes, and other things.
Then, it uses the prediction models
to understand
that this red car in front of us
is likely to stay stopped
and finally,
the AV realizes through planning
that it needs
to do something about it.
So, what it decides to do
is nudge around it to the left
and it makes a decision,
it has just enough information
to pass safely
between around the red car
to the left and without interrupting,
without any risk of hitting
any other cars around it
and continue to go on its way.
So, between perception, prediction,
and planning, this is really how,
the core of how our AVs decide
to operate on the road.
And this is all pretty exciting
to do, pretty innovative,
especially
for a machine-learning engineer.
But honestly, I want to turn
to the life of our engineers
from day to day
and that actually is no different
from any other machine-learning
organization or team.
This is how we spend if we do things.
We gather data that comes off of
our AVs which is basically images,
video,
and motion of other vehicles
collected from the time
that AVs spend on the road.
We send some of that information
to a third-party annotation service.
So, for example, you and an iterator
will look at a video,
draw bounding boxes,
where all the cars are.
And that becomes ground truth
in probation
that our machine-learning models
will use to understand
which parts of the video
or images are cars.
Then, we train models which today
we use PyTorch
to tell you some more about that
in the next slides.
But we are always, we are iterating
our models to improve them.
Sometimes,
we are starting new models.
We'll take those models and next,
we will deploy them in simulation.
So, we wanna understand
in a simulated environment
how low our models might perform
as they drive throughout the world.
And we take our best models,
we deploy them to a car,
we test them through
the San Francisco Bay area
and we will repeat this process day
after day, week after week.
So, now, I want to turn to
from the actual system side,
what we are doing to run this process
and train our models.
So, at first, not surprisingly
what we did
is we started
with workstations for GPUs
that we bought at the store,
put in our workstations,
started training TensorFlow models.
Obviously, this only would
scale us to a certain size,
just as much data and complexity as
could be fitted into a workstation.
So, what we did next?
Well, we built
a Cloud Kubernetes Cluster
consisting of EC2 instances
running in the Cloud
and we enabled our users
to start using it to train models.
Kubernetes is definitely complicated.
It's complicated to operate.
It's complicated to use.
But our users were able
to start training models.
What they did is they picked
a powerful GPU instance type,
started using it to train.
They added more data to it.
They increased the complexity
of the models and eventually,
training begins to slow down.
So, we went to the next most
powerful GPU instance type.
The training was faster again
and they started outing more data
and increased the complexity
of their models
and we have the same problem.
And we kept, very quickly, we arrived
at the most powerful GPU instance
type offered by AWS at the time.
And the same thing happened.
And the model training slowed down
from hours, actually to day,
and then, actually to multiple days
to train our computer vision models.
Users became overall frustrated
with the complexity of operating/
working with Kubernetes
and the slowness
in training the models and so,
I got involved in deaf,
I looked at one of our models,
and actually from
the first couple of hours,
all the models did was spend time
downloading data from the Cloud
to use for training the model.
I spent a lot of time
to improve the data pipeline
and to improve
the efficiency of the job.
And that worked but we have dozens
of models and the approach was,
it wasn't just going to scale to all
the training that we were doing.
So, I began to realize that we needed
to make a major change
to how we were operating at Level 5.
At about the same time,
my team and I
were doing a prototype
with Amazon SageMaker.
So, we were attracted by the fact
that SageMaker is a service
that's completely managed by AWS.
So, we don't have to run
any additional
Cloud infrastructure to use it.
All we do is,
we train our containers.
We upload them to ECR and we run
a SageMaker custom training job
and that runs all of our training
with our containers for us.
So, it's super easy to use.
It's super easy to get started with.
It's super clear for users
how to use it.
As part of this process,
I realized that it would be
really easy with SageMaker
to start using
distributed training.
And so, this just means
that I can just,
I can train with a single node,
I can train with multiple nodes.
I could train with like 8, or 32,
or even with 62 nodes with SageMaker.
Super easy is just
a parameter to specify.
We started converting
some of our models for the PyTorch
and we realized that we could
use PyTorch with SageMaker
together to start using distributed
training is super easy.
Distributed training is actually
built into PyTorch through,
it's DistributedDataParallel class.
That's a class built into PyTorch
to orchestrate all the
distributed training for you.
It's come parallel
with something else
in PyTorch
called DistributedSampler.
So, what DistributedSampler
does is that
when you run
a distributed trainee job,
you've all these nodes
participating in the job.
Each node would just take
a small random piece of the data set
and use that for training.
So, your entire data
set is being chopped up
and split up amongst all the nodes
that are participating in your job
and that actually solves
our data scaling issues,
so, now that we are training
with the DataParallel job
and each node is just using
part of the data set.
So, overall, we were able to use this
to achieve a newer linear scaling.
So, for example,
if we run a job with four nodes,
our job is run roughly four times
as fast as they did before.
You can't achieve an exact one for
one speed up if there's some overhead
but we were definitely back to
training jobs in minutes and hours,
instead of hours and days.
I have a couple of more notes on
the slide about the instance types,
and minutes by training
that we are also using,
I'll come back to that
in a little bit.
But this is the actual impact
of what happened
when we started using SageMaker.
So, at the beginning of this year,
we decided to go all-in
on SageMaker and PyTorch
together and with
distributed training.
And started doing it for all
of our machine-learning models.
So, my team is just myself
and a couple of people
and we have at least
hundreds of engineers
every day training
all kinds of models
using SageMaker and essentially,
from the support perspective,
since it's a completely
managed service,
we don't spend very much time
on the support aspect.
Since there's not additional
Cloud infrastructure
to operate SageMaker,
we have no additional
infrastructural overhead
or cost associated
with operating SageMaker.
It's all handled by AWS.
For users, since they
can scale to their data
and train on multiple nodes,
they are back training models
quickly in minutes and hours.
And the surprising aspect
is that since our training efficiency
is just so much better
than it was
when we were using Kubernetes
in the previous year,
that we are actually
spending less money
much less money for all of our
machine-learning with SageMaker,
even though we are now training
three times many models
as we were with Kubernetes.
So, finally,
I wanna give you,
I want to share some of
our technical conclusions
about distributed training
for deep learning.
So, this is kind of like
some hardcore knowledge
for machine-learning
system engineers.
This first one is actually
just my opinion.
It's about if you're starting
a new deep-learning project,
I suggest that you use PyTorch
rather than TensorFlow.
So, when PyTorch
really first came out,
it was actually much easier
to use than TensorFlow.
Although recently
in the TensorFlow 2.x series,
they've adopted
a library called Carlos
as one of the primary
APIs for TensorFlow.
And they have a mode called
Eager execution.
Between the settings, you can
basically do anything with TensorFlow
that you could do in PyTorch.
However, it's still true
that from mentally,
from the mental overhead,
it's still just a little bit easier
to do things with PyTorch
than TensorFlow.
So, if you're starting a new project,
I would start it with PyTorch.
If you already have experience
with TensorFlow, then,
it doesn't matter so much.
And you could use either
and accomplish the same thing.
You might have heard
of a project called Horovod
which is about distributed training
and it came out a few years ago.
It introduced an algorithm
called the ring all-reduce
which is an efficient algorithm
for distributed training.
And it would use actually
an old library called open MPI
which is a highly scalable
network communication back up.
What I wanna tell you
is that these innovations
have now largely been incorporated
directly in the PyTorch
and also in TensorFlow.
So, if you wanna get started
with distributed training,
you don't need to use Horovod.
Just get started with PyTorch or
TensorFlow directly on your side.
It's still true that
if you want to train
the largest distributed
training models,
like if you want to use
64-nodes or 128-nodes,
that Horovod would open MPI
has a bit of better,
it still has a bit
of better scaling factor
than PyTorch distributed training
but most of us won't be
training models that are large.
So, just use TensorFlow,
PyTorch to start with.
And if you want to get started,
I suggest you download
an open-source project
called PyTorchLightning
which just makes it really
convenient to work with PyTorch
and it's not too difficult
to hook up to SageMaker.
So, now we are gonna
go into a short demo
about how we use SageMaker
custom training jobs
to train models every day
at Lyft Level 5.
Now, let's review a quick demo
of how we use SageMaker at Level 5.
As you're doing
the presentation,
we use SageMaker custom training jobs
to run our Docker containers.
So, for this demo, I wrote
this short Docker container.
You can see that
it's a PyTorch container
and it's gonna train by the custom
image classifier that I wrote.
We actually have a lot
of PyTorch tools at Level 5
and if you'd like
to learn more about them,
we have some great
open-source blogs.
But for this demo, I'm just gonna
use this example container.
If I build it,
I can run it easily in SageMaker
by using the AWS
into run a command like this one.
This will work just fine
and if you try SageMaker Home,
you'll do something like this.
And one of the things that you'll see
is that this command actually takes
lots and lots of arguments.
Some of which are related
to infrastructure settings,
like networking configurations,
security groups, and so on.
Not all of our
machine-learning engineers
would know exactly how to specify
these settings correctly.
So, what we've done to make it
a little bit easier to use SageMaker
is we have actually written
our own command-line interface
that just wraps
the AWS command-line interface,
and applies a standard configuration
for a lot of these parameters,
just makes it a little bit easier
to use SageMaker.
So, what we have our users do
is we have them write
a short EML file
with some of the configuration
parameters for the job,
like the GPU instance type
that they want to use,
how many instances
they want to use,
so one here it means
a single node training job.
Run-time parameters that are relevant
to their machine-learning model,
like the number of training epochs
in our learning way.
And then, when they want
to run the job,
they run our command-line interface
with a command like this one.
What it will do is build the docker
container, push it to ECR,
apply the standard configuration
parameters to the job,
submit the SageMaker training job,
and give you back a link to the job.
So, now you can see those things
go by and as you can see,
first it's building
the docker container.
Here it is, pushing it to ECR.
Submits the job to SageMaker
and at the end,
you get a link back to
the SageMaker job like this one.
And if I open it,
I get back a pages SageMaker job.
So, here is the version
of the job that is completed already
that I run just
a little bit before.
You can see it back
all the configuration parameters
that we applied for the job.
It's the run-time parameters.
It's also helps with system graphs
where things like all
the CPU memory, GPU utilization.
So, basically, what happened
is the job started and then,
these things
all went straight up.
And since, it's just an example job
which ran for a few epochs,
and so, they all ended
after a short period of time.
And then, I've got the links
to SageMaker
logs to CloudWatch
logs from the job.
And quick through, I entered
into CloudWatch and boom!,
I get the logs
for the jobs right away.
And the logs basically,
show up almost in real-time
as the job executes.
It's easy to pull through them,
all these things together
just making it really convenient
for users to work with SageMaker.
We'll see some third-party
experiment tracking software
that I'm not gonna show here today.
But this really captures the basics
of what our users do
on kind of like a daily basis.
Thanks for letting me
show you this demo.
Thanks for watching the demo.
So, I wanna end
with some of the lessons
that we had from the past year.
The first one is that
we just stayed super focused
on making it as easy
and fast as possible
for our machine-learning users
to train models.
We didn't worry too much
about get too
with a particular
Cloud technology.
We just stayed focused
on the models and for us,
that meant adopting SageMaker.
For you, it might mean
something else.
But the overall effect
on our organization
has just been a massive increase
at the efficiency of the productivity
of what our machine-learning
users are doing.
The next is that we have
benefitted massively from the fact
that SageMaker is
a completely managed service.
So, there is very
little support cost.
There's still cluster overhead
and it's simple for users
to understand and use.
And the last one is that we've
benefitted in unexpected ways.
So, for example, the SageMaker
is always new instance types,
new feature is being added
every few months,
and we benefit from that
because we use SageMaker.
And finally, I'll mention
that we use spot instances
with SageMaker
to reduce our cost.
So, this is an instance type
where you accept
that there might be
some delays in the job
starting or that a few jobs
might be interrupted
and restarted later by AWS.
But in exchange, you get a discount
between 40 and 70 percent
on your job cost
and we just had to really reduce
our machine-learning cost.
So, on behalf of Nitin, I wanna
thank you for joining us today.
I hope you enjoyed our talk
and that you got something from it.