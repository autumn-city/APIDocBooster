.
So, welcome ah in the last lecture we had
studied about two interesting architectures
and they were much deeper and then you could
go up to 100s or 200s ah of layers over there.
One of them was residual network the other
one is what is called as a densenet or a densely
connected dense ah residually connected networks
over there. So, today we are going to do ah
hands on for the first perient of it which
is called as densenet or ah very deep network
with the residual connection over there.
Now ah its its not much of a change as far
as the first part of it is concerned and and
these are also some things which take good
amount of time. So, I am just going to show
down a notebook, which has most of the things
which are re compiled over there and we do
a simple walkthrough of it.
Now, over here the first part is pretty simple,
which is your just the header files or or
the initial ones which you need to take in,
and then ah next comes down your data over
there.
Now, this particular network quite different
from your unlike your googlenet which would
have needed you to take down 200 and then
ah 229 cross 229 sized images over there.
So, here we take down smaller ones which are
224 cross 224 and this is a network, which
has been trained to stick down exactly two
my image that ah kind of models and imagine
it kind of sizes ok.
Now here fortunately we could take down a
larger batch size and that is that is all
dependent on how much of intermediate data
my model is handling. So, based on and then
that is a pretty easy calculation based on
what we had done in the earlier lecture itself.
Now ah you need to work it out around over
there based on how much your ah available
ram is there. So, while in googlenet we had
to stick down to a much smaller batch size
of 32 ah based on the particular card I was
using over here.
So, for me ah with this given with the same
card and the same kind of a system architecture,
I am able to get down more number of images.
And here I am pretty convenient taking down
128 ah images into one single batch. Now the
rest part is quite simple. So, instead of
doing it with image net, we are doing with
a smaller dataset called a c 5 and this is
just a 10 class classification problem. Now
based on this 10 class classification problem,
accordingly we will also be modifying the
network architecture which we ah download
ah from from our model zoo available with
us. Now that is this for this first part of
it which is quite straightforward and clear
there is nothing much to do.
Now, the next is to look into whether the
data has been loaded down and yes the same
way that we had divided 50,000 for our ah
training set. And then 10,000 for our testing
said the same thing comes out. So, so that
is ah pretty straightforward. Now, here ah
what we do after that is we just get down
our models and this is my resnet 18.
So, this residual network is just 18 layers
deep and I am choosing down only a very smaller
one. You can go down 251 layers as well pretty
much, but then you need to keep one thing
in mind that your batch size will reduce and
we will take much longer time to a finish
off each of these epochs. Now in order to
ah avoid any of those problems over there,
I am just using a smaller model over here
and this is just residual network 18.
Now there is also another fact that I am not
trying to discriminate between multiple ah
number of classes. So, an earlier residual
network which was really deep there is something
which was done to actually classify between
ah 1000 classes of objects over there. Now
I am choosing down a very ah smaller one which
is just 10 classes of objects and now just
because of using 10 classes of objects I can
have this liberty they are actually good and
choose do not know much smaller size network.
Now, that is what I am doing with ah choosing
down resnet 18 and this is what you can look
down on the whole thing. Now ah over here
we do not make actually much of a change to
the whole network and you do not have any
issues of an auxilary arm or something coming
down over there. Now what does definitely
come to play is that ah all of your basic
blocks and everything being put down together.
The next ah interesting part is that your
final one is something which gets down 512
as a linearized ah feature vectors and the
terminal lane. And then this 512 are what
are connected down as a fully connected network
on 2000 years. You now this is the part which
we need to modify because we did choose that
the we would be going down with just a 10
class classification problem and not a 1000
class classification problem. So, this is
the only change which comes down and quite
unlike the ah googlenet which we had done
in the earlier ah session is where you had
these auxiliary arm.
So, you also had to modify the auxilary arm,
you do not have this issue over here because
there is nothing of an auxilary arm. But given
that the the main point which comes down is
that you have these residual connections as
well now and and just by virtue of these residual
connections which are present over there.
So, your gradient is actually back propagating
much easily and because of this easy back
propagation of the gradient, we we do not
need to worry about whether there would be
a vanishing gradient at some point of time
as we are traversing down the depth over there.
Now, ah that is that is pretty ah much simple
and easily done, what you need to keep in
mind is that because ah of these residual
connections, its its again made down in the
same kind of a tabular form of connecting.
So, you first build up the small blocks over
there for each of them, and then you drop
down residual connection and keep on doing
a parallel connection and build it up. So,
this is quite similar to how you were doing
it down for your googlenets now ah since we
are not doing much on the data structure of
these neural networks as such to do. So, and
then that is pretty much well documented within
pie torch communities documentation. So, I
will not be spending much of time over there,
but my most important factors to discuss are
more of related to what happens within the
learning dynamics, what you need to make a
change from adapting a model from an existing
one to your particular one and and further
on as we keep on going over there, and what
are the properties which does come to impact.
Now, once that is done, we look into the total
number of parameters and then if I go through
them you would see that ah as a total calculation,
this particular model is something which has
eleven million parameters over there.
Now, this is definitely a model which is almost
the same size as in a googlenet, but it does
have much lesser number of parameters and
by virtue of a lesser number of parameters
you have a model which takes less space to
load. ah By virtue of lesser number of parameters
you are intermediate operating points also
consume lesser amount of memory that is that
is something which we had done for one of
our networks earlier. Now just because of
this sure ah interesting aspect and attribute
present down within the model, you can now
actually feed down much larger batch sizes.
Now one thing you know is that if you are
feeding on much larger batch sizes and the
total number of back propagations over there
is; obviously, decreasing. And once you keep
on decreasing the number of back propagation.
So, your compute is roughly definitely going
to become faster and faster over there.
So, one is the compute which is steady which
is for the forward pass and that is dependent
on the order of the sample size within your
training set over there. Well then your back
prop is always dependent on your batch size.
So, larger the batch size is the lesser number
of ah larger is the batch size the lesser
is the number of batches which fix in over
there. So, the lesser is the number of back
propagation you are going to do and that is
something which does the definitely come to
play a very important role now here ah this
is what we have gone down and looked down.
So, this this particular network has about
11.7 million parameters. So, when we had ah
and in the earlier theory, when we were comparing
down our resnet versus densenet and there
was one of these plots which was plotting
down the total number of parameters versus
what is the accuracy saturation accuracy achieved
over there. So, you did see that ah densenets
ah for the same kind of a depth have almost
half the number of parameters to come down
to the same kind of an accuracy over there.
So, this this is something which we will do
in the next class ah when we compare down
with densenet and do a practical hands on
with densenet is when we get down the parametric
calculation over there as well. Now the thing
which you need to modify for this network
is your last layer, which is ah the f c. Now
if we go over here this is my f c layer and
this f c layer has to be changed from this
kind of a linear structure to something which
connects 512 to just 10 nodes over there and
nothing more than that. So, that is the change
which we bring over here. So, its a linear
connection from 512 to just 10 nodes and that
is ah the modification, which we do for a
residual network to be used for a c power
10 class classification problem.
Now once having done that the next part is
just to copy down all of my weights and keep
it. So, these are my initial weights now I
will look down what happens after my whole
training.
The next is ah pretty straightforward and
simple as we had done in the earlier case
is to check down whether I have a g p u available
with me and whether cuda libraries and resources
are all set and running.
Now, if I have that then I can just keep on
using using my cuda for acceleration.
Now, once that is done, next is to look into
my criteria it stays the same as in for classification,
we choose to stick down with negative log
likelihood ah classification criteria and
then with adam as my optimizer. Now in the
earlier classes we had discussed about different
kinds of optimizers and their features and
the different attributes, which they play
down and what we did figure out that adaptive
momentum and and also by a experiment, we
have seen down that adaptive momentum and
a general operating conditions is something
which would take a tad bit longer to actually
optimize and give me the results. But then
its something which would ah definitely guarantee
convergence at much lesser number of epochs
as well as if you take the total time consumed.
So, your pour epoch time is ah increasing
because your per batch time is increasing
on view of the adam optimizer over there.
However, this net product of ah total number
of epochs into the time taken per epoch that
is much lesser when it comes down to adam
in order to come down to a saturation point.
So, that is that is really interesting because
in terms of how many minutes or hours or normal
human time and c p u time it consumes that
is being brought down significantly by use
of adam. Now once this part is done next is
to look into the network and we start training
the network.
So, here what we do is ah go down with the
same plain old rule of training it down just
for 10 epochs, I do not drink it more than
10 epochs it it does not play a role over
here and then although this does not take
much amount of time, this network does train
much faster than googlenet which otherwise
would have taken down about 8 minutes close
to 8 minutes to do it. So, here it it goes
wrong about belly between something between
1 and 2 minutes over there. Now within each
of my epoch ah now the difference which comes
in that here this is just one single tapped
out network which means that the classification
and the losses, which you get down is only
at the end part of the network its no more
in between.
So, in googlenet you had these auxiliary ones
auxiliary arms coming down and for that reason
you also had to ah get down the losses, computed
out of your auxiliary arm. And you also had
needed to a back propagation with the losses
being fed down over there.
Now, we do not need to do any of those ah
for our case over here, now here what you
are just going to do is the plain simple calculate
out the final loss over there and then then
do the ah back pope.
Now, within each of these batches which comes
down over there, you for zero down your optimizes
gradient and everything.
And then you convert your if there is a g
p u or available then you convert your variables
into g p u and then type casted as a variable.
So, that you can you can do your ah back propagation
operations over there. Next is you find your
feed forward and get your output then you
find out what is your predicted and whether
your predicted is correct or not, and then
ah this would help you in getting down your
error over here ok.
Now, once I have all of this done and my errors
computed, and then done then what I do is
I do a back propagation of my loss over there
and the optimizer comes into play in steps.
So, whenever I do a step it means that the
optimizer is being solved over there and my
weights are getting updated. So, this is where
my weight update happens and then I have my
running losses which I just calculated and
stored.
Now, within each ah like each each epoch over
there I am passing down all my training samples,
and I have in total ah 50,000 training samples
over there. So, if I need to find out my accuracy
or or my loss, then I will have to take some
sort of a average over there and that is what
I am actually doing over there. Now once that
is done, then train a c c and train loss these
are the two different arrays which I just
being created dynamically. So, one array has
an entry for each epoch over there.
Now, once that is done the next part is ah
where we need to get down ah my my next part
of it is running, which is my validation over
there.
Now, in validation what we need to keep down
in mind is that one point which we said down
over here is false, ah we just give an identifier
that we are no more training the network and
this is for the reason. So, is that your batch
normalization does not come into play. So,
typically when you are training a network
you will always be trying to normalize in
batch.
So, whenever you have a batch norm coming
to place. So, there is a batch normalization
taking place, but whenever you are doing a
feed forward and just an inference in over
there ah your batch like this may be different
and based on that it will have a different
dynamics coming out. So, the same sample if
it is located at different batches based on
what all other samples are located in the
batches; if if the other samples in the batch
are changing, then then the whole thing gets
normalized along a batch and the response
has a very skewed or behavior. So, in in typically
during infarencing we do not use batch normalization
and that is just switched off. So, this is
the very simple way of actually ah switching
off all of my batch normalization ah issues
ok.
Next what I do is just find out if my g p
u is available. Then I typecast my ah data
on to a cuda array, such that it resides on
my g p u memory and my network can also work
on my g p u and then I find out my outputs
do a prediction over there and then find out
whether its correct or not.
Now, that is what you would be doing down
ah for your feed forward on the validation
side of it, and then since there are 10 thousand
samples present in my validation which get
evaluated every epoch.
So, I take an average over all the 10,000
samples the next part is pretty simple to
plot it down and there is nothing much of
a change.
Now, if you look down at ah how the model
was working out. So, ah we trained it over
at any epochs it takes about ah 1 minute 51
seconds. So, that is about 8 to 9 seconds
short of ah 2 minutes is what it takes down
to train it down.
Now, it starts with an ah average accuracy
during training ah ah for testing somewhere
at 50 percent and then my test accuracy goes
up to something around 70 percent. Now in
between it was at 75 percent 73 percent as
well.
Now, you would see that ah my training loss
is what is starting at point 1 and then it
keeps on going down and down. Now while ah
my accuracy had grown gone high on my ah testing
side. So, you see that it goes out goes up
to 71 percent then falls down to 55 percent,
then again goes to 71 percent, then to 67
percent. Then again to 75 percent, but on
the other side of it you would see my training
loss is constantly decreasing. Now one thing
is ah there are few people who would argue
over here that possibly I am over fitting
on my training loss and that is my testing
one is going down, but nonetheless you need
to keep one thing in mind that this kind of
a jitter behavior over here is something which
is happening, because its possibly hopping
from one minimum point to the other minimum
point.
Now, ah that is a beauty of adam which comes
to play over here, that you are not getting
locked into local minimum points, but then
you can hop and you are going down towards
the global minimum and that is that is the
dynamic change which you are observing over
here. So, 10 ah epochs get in total of ah
18 minutes and 38 seconds. So, that is roughly
about 1.8 ah minute poor epoch and ah that
is not so bad actually given the fact that
when we were trying to do it with googlenet.
It was actually taking me about 8 epochs ah
8 minutes per epoch and here and something
about 2 minutes per epoch. Now I have a model
which is roughly one third of the model size,
but much deeper and ah it takes me about one
fourth of the time to run it over here, now
that is that is a total cumulative thing which
comes down. So, if you can really play around
with your architectures you do save a lot
of your resources over there.
Now, we have our train and test losses over
this you would see that your training losses
is something which starts lower and keeps
on going and your test loss is still going
lower, but it does have a jittery behaviour.
On the accuracy side of it you see this this
whole jittery behavior between 51 percent
and 71 percent and keeps on going. So, somewhere
around ah at the end of tenth epoch is where
it sticks down to 70 percent and that is more
or less where it will be seeking down because
you can see that its almost over there ah
its its roughly at 70 percent where it keeps
on going. So, you can keep on training over
a longer number of iterations, which I would
typically suggest you to do and and find out
where it keeps on saturating. The next part
after training is to ah copy down my weights
and then try to look into the visualization
of the weights over there that is what we
do over here.
Now, if you ah recall from the discussions
which we had on the architecture. So, the
first layer over there is something which
has a ah special kernels which are off span
of 7 cross 7 and that is that is what we plot
down over here and there are ah 1, 2, 3, 4,
5, 6, 7, 8 and 1, 2, 3, 4, 5, 6, 7, 8. So,
its a 64 cross 64 and then if you remember
from your earlier lecture you can you can
actually flip back onto that earlier one.
So, you had this connections from 3 cross
2, 2 4 cross 224 and then you had 7 cross
7 kernels and you had 64 of those. So, each
is one of these kernels of ah 3 cross 7 cross
7. 3 is for the input number of channels and
then you have 64 such channels which are visualized
on a 8 cross 8 grid of ah these kernels coming
down. Now these are the weights at the initial
which is at a randomized start over there
and then these are the weights which come
down after 10 iterations of training.
Now, if you look down at the difference yes
there is a significant amount of difference
you see and then these are mole of like gradient
based changes, which come from although look
looking down at the kernel its really hard
to understand like what is changing over there
ah because they are they still appear as random
ah pixels of colour.
Now, here you would be seeing that there are
certain kind of directional gradients which
start setting in over there, and then these
are more of colour based detection of gradients.
Now that is an interesting factor which which
does come into play and ah, but then given
the fact that we are not putting down any
imposition on what is the nature of change
which should take place and how it should
be taking place. The only thing which we are
doing is back propagating my gradient across
the network over there. So, that is ah what
happens and and it keeps on changing. ah Nonetheless
to say that since this is a residual network
which ah with a much wider ah ah receptive
field on the first layer over there.
So, you are able to encapsulate much wider
aspects of the images and the objects present
in those images and that is what does play
a significant role. Now ah see far which was
a much smaller 32 plus 32 or 64 64 things
which were scaled up to almost 8 times of
that to 224 cross 224. Now instead of that
if you are taking down very detailed images
of 224 cross 224 and on which I have a level
on your image straight. So, that is something
which we will leave down to all of you guys
to do its a large data set you need to need
time to download you would also ah incur more
time to actually train down per epoch as well.
Now if you are able to download that and do
over that, you would see that there would
be much finer granules or changes which would
come down despite it it would also be kneading
down more number of epochs. So, that is something
which I leave up to you you you are free to
do so.
You can in fact, look into any of our other
datasets which we are done with auto encoders
and stuff as well, you would be getting on
a very different behavior for each of them
and and that is something interesting to really
keep on looking down over there. So, that
is ah what we have for the first convolution
kernel over there.
Next week we look into the next kernel which
the next layers are now 3 cross 3 the first
first layer over there and then this is one
of these kernels and now since it connects
all the 64 over there. So, you still have
one layer which corresponds to one of these
ah convolution. So, for one kernel you have
64 such channels which are over there and
each has a spatial span of 3 cross 3 which
connects on the previous layer and since the
previous layer is giving you an output which
has 64 channels.
So, here also you need to have those many
64 channels and that is what you get down.
So, these are the initial weights before training
these are the ones after 10 epochs of training
and this is the kind of changes which happens.
And now these changes are really massive these
are not zero changes, these are all nonzero
changes. Now this is another thing you need
to keep in mind. Now a very higher difference
comes down when trying to compare it with
googlenet over there there was not much of
a change in majority of them and the second
layer coming down, but here you do see a change
coming down over there now that is that is
interesting to explore out as you keep on
going down.
Now that ah makes me come to an end for residual
networks and then then stay tuned for the
next lecture, where we would be ah doing and
discussing about what happens with densely
connected residual networks or densenets until
then.
Thanks .