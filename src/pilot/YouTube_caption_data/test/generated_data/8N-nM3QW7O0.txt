hey guys I hope that you're doing great
and in this video we are going to do
some sentiment analysis using birth
finally so we have the Google place
reviews we have the text be processed
and now we are going to use the hugging
face library and the birth model from
there to do the sentiment analysis
itself but first we are going to have a
look at what birth actually is so let's
get started so birth was introduced in
2018 in this paper which is available on
archive and the paper is called birth
returning of deep bi-directional
transformers for language understanding
and this is a mouthful and if you don't
know what any of this means
that is quite all right but if we open
the paper itself you see that there is a
comparison between the open iya AI GPT
which is the first version of the very
popular model GPT 2 and we have Elmo
here so compared to the elmo you can see
that the bird is using this thing called
transformer which is an architecture
which is introduced in another paper
called attention is all you need also by
the Google team of course and compared
to the ELMO we are not using our streams
and Elmo has introduced contextual
embeddings
so what this means is that the word in a
sentence might have different meaning
depending on the words that surrounded
so for example the word nails might have
two different meanings for example
finger nails or metal nails so the next
thing that is such a big idea in the NLP
community is the transformer act
architecture itself and this one is used
in the original version of the DBT model
and the transformer is very different
compared to the HTM the main thing about
it is that it feeds the input in at once
compared to the sequential reading that
the
htm' models required and it turns out
that in the real world at least the
transformer is much better at handling
long term that dependencies between
words so you might have the task of
question answering or reading
comprehension in general and when you do
that you have to basically remember some
of the context of the stuff that you've
read before or even after sometimes
depending on the text that you are
reading and in those cases the
transformer models are actually doing
better job compared to the other streams
when looking at information vastly in
the previous words or the next words so
the next thing that actually bert
introduced was the by directionality so
if we have a look over here as well the
GPT model is doing left to right
understanding of the text and sometimes
when you're answering questions it might
be the case that the answer is actually
after the question was posed into the
text so the bird model is actually
bi-directional actually it's non
directional but that's a bit well it's a
bit more advanced topic and we will not
go into that so the big thing about the
bird it's bi-directional or non
directional model it uses transformers
and it does have contextual word
embeddings so different so the same word
might have a different meaning depending
on the words surrounding it and the most
most important thing about the bird
model is that it can be actually very
easily fine-tuned on the tasks that you
might be interested in so you might do
tech specification with it you might do
question answering you might do
sentiment analysis as we are doing in
our case entity recognition etc etc but
how was the bird model actually trained
well the thing about the board model is
that it had to
in task one was the masked language
modeling and the other one was the next
sentence predictions but what this means
if we go into the roughly the beginning
of the paper you can see that we have
let me just scroll up we have the masked
LM task in the next sentence prediction
so roughly speaking about 15 percent of
the text that we had in the corpus and
the corpus was comprised of the Toronto
Book corpus and the Wikipedia English
corpus so had a large large data set and
in that data set some of the words at
least fifteen percent of the words was
were masked and the task of the bird
model was to uncover or guess the words
that were masked the next task was
called next sentence prediction and
given a pair of sentences the task was
to say whether the second sentence
should or follows the first one so I
know that this might be a bit confusing
so let's have a look at some examples
and we are going to start with the mask
language modeling let me open up a text
editor for you and in here we might have
the input which is masked
that's mask she masks
so given this input the task of the bird
model was to uncover or guess the words
that were behind the mask so in this
case this should be that's what she said
alright so this is the masked language
modeling task alright for the next task
we have the next sentence prediction so
this is basically yeah the next sentence
prediction we might have the following
input
this should include the classification
special token that we found that we
talked about in the previous video so we
are asking birth to do some
classification and we have basically the
same thing next we have a separator
which is again a special token that
we've talked into the previous video and
then we might have something like ahaha
nice and we have another separator here
so the label for this one should be yes
this is next sentence okay in the other
case we might have something like the
same input but this time we might have
something like Dwight you ignorant and
we might have a mask right here
of course the label for this should be
not next and basically those are the two
tasks that were used for training the
bird model of course we are not going to
use the bird model in the original paper
for which the source code was provided
by the Google team they also provided
some pre-trained models which is just
amazing but we are going to use of
course the holding face library and the
face library has the sorry the hugging
face library code to the Transformers
and this library has a whole section on
both models of course you can do that
with pay torch or 10:02 and we are going
to use the Python version in this one so
there is a very good summary in here you
can go ahead and read when you start
working with birth using the huggy face
library you might find out that there
are a couple of different different
flavors of birth of course you might be
confused at first but let's get right
into understanding what those types
might be so the first thing about birth
was that I am showing you the industry
birth by Jay our mark here which is a
great article and there will be a
reference into the source code in the
tutorial of this video to this article
if you have if you wanna have a look at
it
and here we have the bird watch and the
bird base another thing that you might
notice in here is that the bird model is
basically a stack of transformer
encoders and the number of those
transformer encoders is labeled as bird
base for which we have twelve encoders
and for the bird watch we have 24
encoders of course the watch model is
more accurate it has more parameters but
it can be a bit slow to Train even on P
100 password or whatever you might want
to train with the or the word one you
might want to fine tune the word one
when you have multiple GPUs at least
that's my experience of it of course we
have a bit more attention layers in the
attention thing
refers to the idea that when for example
you are saying something let me open up
the text editor right here of course so
you might have something like jim is
pretty cool his ideas are amazing or his
let's say pranks are amazing okay so in
this case the attention mechanism of
course the attention mechanism and the
sail rotation mechanism is very let's
say complex think into the NLP community
and you might wanna give a deeper dive
into the transformer architecture in the
attention mechanisms from the papers or
on other tutorials but in our case I'm
going to give you a simple intuition
right here so let's say that we have
this example text or these two sentences
and you might have a look at this thing
this word his
so the attention mechanism is basically
going to show you that there is a high
relationship between his and Jim so his
is referring to Jim at least in this
case so this is what the attention
mechanism does for you okay so we have
the attention mechanism we have the bird
base and the bird watch models and we
have the hugging face library so the
next thing that I'm going to do is to
what the bird model itself from the
hugging face library I'm going to open
up the notebook from the previous video
and I'm going to what the bird model
from the human face library and this is
the notebook with a bit of changes
actually so in our case I'm using the
version of bird which is the base model
so with the 12 sorry with the 12
transformer in colors here and I'm using
the cased version so in the last video
I've shown you how to work the encased
version but in our case at least what I
found by experimenting or empirically is
that the casing of the words matter when
we are doing some sentiment analysis for
reviews of course this might be the case
because intuitively when you write
something like that in all uppercase
might convey more sentiment or that
something is really bad compared to when
you use the bat into work is letters so
I'm basically pre-loading the
pre-training model with the bird base
cased version so if we go ahead and look
at the tokens of our tokenizer you can
see that the casing was preserved so
this is a bit different from what I've
shown you into the previous video
another thing that I've changed in here
is the fact that now our GP reviewed
data set is also returning the review
text and this will be helpful when we
are trying to evaluate
the model itself the last thing that
I've changed was to actually increase
the seek the maximum sequence length of
our reviews and I'm using 160 tokens
right now and I'm going to train with a
part size of 16 and I'm going to Train
450 box okay so this is pretty much the
new picture of course in the worst video
I've also had extra dimension right here
which was one so I've removed that as
well so now we have the batch size times
the sequence length and all this is now
into the correct shape so given that I'm
going to use the hugging face library
and initialize a bird model from that
alright so I'm going to create the model
and I'm going to call bird model from
pre-trained and I'm going to pass in the
pre trained model name and this will go
ahead and download the model weights
from the report in this may take a bit
so during this time I'm going to
basically call the resulting model and
I'm going to call it using the input IDs
from the example that we had of the
encoding and I'm going to pass in the
attention mask as well
all right so this is great
and this will return two things the last
hidden state from the topmost
in color which is this one in our case
and it's going to return a put output so
we are going to have a look at both of
these and note that we are still running
on the CPU for this one and now that we
have the outputs of the Bert model we
can have a look at those and the last
hidden state is the sequence of hidden
states of the worst layer of the model
so it's basically a sequence from the
worst encoder right here from the break
base model and we can have a look at the
shape of it and we have one example
seven six eight hidden units we are
going to have a look at this so this is
okay but the put output is the one that
we are going to use and it's basically a
pulling procedure over the 32 elements
into this dimension and this is coming
from the birth pore which is defined
into the github repo of the whole
unified library so this is taking the
hidden size of the seven six eight right
here and it's basically applying a
tonnage function over it so this is the
pooling that the hanging face library is
doing for you
so you're basically getting a summary of
this hidden state and you might go ahead
and try something better but in our case
did this will suffice
all right so where you can see
this information about the hidden size
we have the birth model it has a config
and you can have a look at the hidden
size right here hidden size
alright so yeah this is the number of
hidden units right here okay so with
this in mind we are going to build a
sentiment classifier building sentiment
classifier using birth so we are going
to create a new class and this class is
going to extend from the torch and in
module and we are going to accept the
number of classes that we have we are
going to feed the data to our birth
model we are going to apply some dropout
for regularization and then we are going
to append an output layer which will be
of course fully connected layer and call
a soft max function of the output of
that one so this will basically give us
the classification that we are going to
need in order to classify a review as
neutral positive or negative one so
let's start with the cross for the
sentiment classifier
and we are going to have a constructor
which is going to accept the number of
classes that we have and I'm going to
call the super constructor and I'm going
to pass in the self right here next I'm
going to initialize the birth model the
same way that we did into the demo above
next I'm going to create a drop out
layer and I'm going to specify the
probability of drop out as 0.3 and I'm
going to specify the out put layer this
will be the number of hidden size units
that we have into our birth model and
I'm going to use the config that I've
shown you and this way you can basically
do some pretty generic stuff and
implement the birth model using the
watch or the base model and I'm going to
output the number of classes right here
finally I'm going to apply a soft max
function so I'm going to create instance
of that and we are going to do I want
the first dimension of course because we
want to classify each example next I'm
going to define a forward function which
will implement the base forward function
and here we have the self as the first
parameter we have the input IDs of our
tokens in the attention mask so right
here I'm going to skip the worst hidden
States and get just the boot output from
our birth model
I'm going to pass in the input IDs in
the attention mask next I'm going to get
the output from the dropout and I'm
going to apply the output layer
crossfire and finally I'm going to
return the result of the softmax
function so this should pretty much give
us a classifier based on the birth model
right here and I'm going to create an
instance of that so I'm going to take
the length of the class names that we
have and I'm going to move the model to
the GPU device which in at least in this
notebook I'm going to use let me see
test what t4 so this is the device that
I'm using to this quad notebook at least
and next I'm going to have a look at the
data or a sample data from our data
Waller and this data is going to contain
the input IDs I'm going to take that
from the data or sample data I'm going
to put that onto the GPU and I'm going
to basically do the same thing for the
attention mask
let's have a look at the shapes of these
and this should correspond to the bud
size which is 16 and the number of
tokens that we have in each example
which is 160 all right so this is work
this works at least as I expected and
let's basically run our sentiment
classifier model through this of course
our model is still untrained so we are
not going to have a deeper look at the
accuracy of this model and I'm going to
pass in the attention mask so this
should be the output and we have 16
examples right here and for each of the
three classes that we have we have the
probabilities of how much our model
things about each class and you can see
that this tensor is basically on the GPU
device and we have we can apply backward
propagation on the result of this model
next we are going to train our model and
hopefully it's going to be good at
predicting sentiment so let's start with
the training process and this one is
going to be a bit different compared to
the standard stuff that you might
already know how to do using pi torch so
I'm going to go ahead and create a
training section right here and the
first interesting bit about the training
at least using the holding face library
and the birth model is that we are going
to use a bit of a different optimizer
and this one is called Adam W so this
one is basically implementation of the
Adam algorithm with weight K fix and
this one is just the same thing that the
original birth paper did and I'm going
to pass him the model parameters
I'm going to pass in a warning grade
which is going to be this number right
here because basically the birth paper
is using or recommending some warning
rates which are doing good on
fine-tuning the birth model and I'm
going to pass him that I don't want to
correct the bias of the algorithm
because this is what the original paper
also did next we are going to use a
linear schedule which is going to decay
the warning rate and for that I'm going
to need to calculate the steps that we
have and this will be the number of
batches that we have in to the training
data water times the number of epochs
which is 50 in our case so using that
I'm going to create schedule get linear
schedule with warm-up which is provided
by the hanging face library I'm going to
pass in an optimizer and I don't want to
have any warm-up steps just as the bird
paper recommends and I'm going to pass
in the total steps that we have finally
I'm going to define hours function and
this is going to be cross entropy was
because we are doing classification
cross-entropy was I guess the out
complete is not very good today and I'm
going to put that onto the GPU okay so
now that we have all of this already we
are going to define a helper function
which is going to be called train epoch
and as the name of the function suggests
this will go ahead and
go over all the training data at least
for one ebook and use that to apply
backpropagation to our sentiment
classifier so for this one I'm going to
let me just move this away from our
screen I'm going to go ahead and define
the function and this one will take a
lot of parameters I know that's a bit of
a bad design but yeah let's go with it
we'll accept the model the data water
the worst function the optimizer the
device on which we are going to do the
training the schedule and the number of
examples because we will calculate the
was the training goals that is and the
training accuracy okay so the first
thing that I'm going to do here is to
put the model into training mode so that
for example our dropout is actually
applied during this training process I'm
going to store the losses and the
correct predictions okay so now for the
meat of this function and this is going
to be a bit of a length one I'm going to
iterate over each example into our data
water and I'm going to take the input
IDs the attention masks in the targets
or the labels water which we are
predicting the sentiment I'm going to
take that from the input IDs and I'm
going to move that over to the GPU I'm
going to do the same thing for the
attention mask and for the targets of
the sentiment I'm going to take the
outputs of the model
and attention masks alright now that we
have this I'm going to take the
predictions which are going to be
basically the classes that have the
highest probability and I'm going to do
that using torch max I want the first
dimension and I'm going to apply a loss
function over the model outputs and I'm
going to use the targets to actually
calculate the cross entropy rows and the
number of correct predictions right here
will be this one using torch dot some
I'm going to get all the predictions
that are basically the same as the
targets so this will tell us the number
of correct predictions and I'm going to
append the current Wars next I'm going
to do the back propagation steps but we
have something a bit different right
here I'm going to apply the backward
propagation then I'm going to basically
do some gradient clipping on our model
and we are going to use the
normalization of 1.0 so this is
basically duplicating the steps that
original birth paper has described
during the final recommendations for the
training core fine-tuning the birth
model and to do that I'm going to use a
name dot u2's doc clip dropped norm and
I'm going to do this in place so I'm
going to use the underscore version of
it I'm going to pass in the model
parameters and maximum equal to 1.0 so
if you're not familiar with the
gradient clipping Huck if you will this
is basically a way to combat the
exploding gradients so if the gradients
are to watch for example your model will
or your training procedure will go
unstable and you will basically have no
not a good model so the gradient
clipping is basically doing a hack to
not allow the gradients to become so
much the next thing I'm going to call
here is to do a step of the optimization
I'm going to do the same for the
schedule not that this schedule is
called every time we are going through
an example but in our data water and
finally I'm going to zero the gradients
right here so the last thing in this
function that we are going to do is to
return the number of correct predictions
and the was for this training key book
and I'm going to take the correct
predictions the double tensor from that
and I'm going to divide that by the
number of examples and I'm going to take
the losses and calculate the mean was
from that one so this is pretty much the
training training epoch helper method or
function okay so we have that and I'm
going to write another one for
evaluating the current model this one
will take a model data water was
function device and the number of
examples as well so during a vow mode to
basically evaluate our model we are
going to put it into a vowel mode so now
that the stuff like button normal is
and drop out are not enabled I'm going
to also store the losses and the correct
predictions right here and the first
thing that I'm going to do before
looping or going over the examples from
the data folder are going to be to
disable the gradient function so torches
a bit faster and next I'm going to
iterate over our examples into the data
water and I'm going to basically do the
same thing that we did over here so I'm
going to go ahead and copy and paste
that at me alright so we have pretty
much the input IDs the attention must
targets we have the outputs of the model
we have the predictions the Wars and we
are storing the correct predictions and
the losses so we're not doing any
training right here so nothing else
pretty much we are done and I'm going to
copy and paste the return statement
which is going to be the same thing
right here okay so the vowel model
function is ready and mixed I'm going to
execute this cell and I'm going to
basically create a training group with
the help of both of those functions of
course I want something to store the
training and validation losses and
accuracies so we define our default
dictionary which contains lists and
we'll call that history
and this is very similar to the way that
basically the Charis library is storing
the history and I want to store the best
accuracy so I'll be able to basically
discard or save the models which are the
best at doing the valley basically has
the best accuracy on the validation set
and I'm going to iterate over the number
of epochs I'm going to start with
printing the current epoch out of all
the a box and I'm going to print some
characters so this is a bit nicer to
look at and now we are going to
basically call ball to call the train a
balk helper function in this will
determine the training accuracy and the
training course for this epoch and I'm
going to pass in the parameters which
are model the train data water the worst
function the optimizer the device
schedule and length of the number of
training examples which is stored into
the training data frame and now that we
have this I'm going to print out train
was which will be the train was in the
accuracy it's going to be the Train
accuracy all right I'm going to do
pretty much the same thing for the
evaluation modes
sorry on evaluating the model model
validation data water the worst function
the device and the number of examples
into the validation dataset so I'm going
to paste to get this paste it down here
and I'm going to form up this so we have
pretty much the same visual
representation of this and here I'm
going to put in the validation what's in
the validation occurs and finally I'm
going to print a new wine right here
okay so now that we have the training
can validation accuracies and what's
this we are going to basically store
these into the history that we've
created I'm going to do that for the
worse
and next I'm going to do the same thing
for the validation accuracy in the
relational alright so the final thing is
that we are going to save the model with
the best accuracy so I'm going to check
the validation accuracy and if this is
better than the best accuracy for which
were starting with zero so we are
storing the first-ever model that we
have I'm going to save the model to
model dot but and I'm going to save the
best accuracy into the validation occurs
I'm going to set the best accuracy to
the validation occurs so the worst thing
that I'm going to do is to basically
time this using the magic command
percent percent time so I'm going to run
this
and Brett's is not defined so I have an
error okay so I have this arrow into the
this name so this is pets and this is
bred for predictions so if I execute
this again and wait for the training to
start okay I guess that this should work
out and we will basically go ahead and
wait for the model to try so we train
the model for a little bit over three
and a half hours and I'm going to show
you the walk of the training process and
it's a different notebook that I'm
preparing for the tutorial to this video
and here is the result you can see I've
trained for 50 a box and at the final
epoch we have a validation accuracy of
84% and training accuracy of roughly 92%
and I've actually did better in the
previous epochs you can see that we have
validation accuracy of 84.6% but I'm
currently just saving the worst model
because I've been saving the model using
the model itself and I found that you
have to watch this thing with the state
dict so I'll save it with the static and
I'm going to what the static otherwise
the ringing face Larry is not loading
the model properly I guess so a little
note on how I found the parameters that
are work for our case I did lots of
experiments with the base in the watch
model I've read through the
recommendations of the bird paper model
and basically I had to
run roughly let's say 15 experiments and
I did that for a different amount of
epochs after started with five epochs
and after that after I've filtered most
of the parameters and just let the model
train for 50 box I guess that if you
continue training this model you might
get even better results but I'm going to
basically give you the Google Drive
check point to the model that I found
that works best so let's continue with
this and this is our original notebook
and now that I've changed that I am
saving the state
instant of the model right here so I'm
going to go ahead and download the pre
trained model and I'm going to take the
ID to my Google Drive so this should be
available for you as well and this
should go ahead and save this model
based case state and I'm saying that the
validation accuracy for this one is
eighty four point one percent and I'm
going to create a new instance of our
model I'm going to specify the length of
the class names and I'm going to what
the state dict using torch dot Watt and
I'm going to pass in the model base case
state dict 88 for one and I'm going to
move the model to the GPO and while this
is running I'm going to open up the file
browser and you can see that this model
is 413 megabytes which is quite a large
model but keep in mind that this is just
the base birth model included right here
so if you are using the word run it
might be a lot bigger
okay so we'll have
to do some evaluation and I'm going to
write the function called get
predictions so this function will
basically written the text reviews the
predictions of our models for model the
prediction probabilities for each
prediction and the real values or the
real sentiments for each review and this
is going to accept a model in a date
water right here I'm going to start with
going to evaluation mode and I'm going
to create four lists which will store
the data that we need so you have a list
for the review texts the predictions the
prediction probabilities and the real
values or the sentiments that are
assigned to each review so I'm going to
go ahead let me just save us some time
and go ahead and copy
most of the above model stuff that we
have right here I'm going to fix the
indentation and we are not interested
into the walls right here at least and
I'm going to take the review text from
the water
and I'm going to viewing the lists that
we've created I'm going to extend that
those lists because the texts and input
ideas etc all of it are just lists I'm
going to do the same thing for the
predictions and the prediction
probabilities and I'm going to just take
the route outputs because those are the
outputs of the softmax function from our
model and I'm going to save the targets
okay so now that we have all the data in
our lists I'm going to convert those
lists into tensors and I'm going to
return their values so the predictions
will be torch
stuck so this will convert the list of
pencils to tensors to a single tensor
and we'll basically add a new dimension
to our tensor I'm going to move that to
the CPU I'm going to do the same thing
for the prediction props or
probabilities and I'm going to do
exactly the same thing for the targets
or the sentiments
finally I'm going to return the review
texts and we didn't do any
pre-processing on those because they are
just a list of texts or strings and now
to have this rather watch helper
function I'm going to call it and I'm
going to pass in the model in the test
data water so this should take a while
and while this is running we might be
let me just show you how you can do
evaluation on the test data water
actually so let's get the test accuracy
and the test was I believe and I'm going
to call a vowel model and I'm going to
pass in the model the test data water
the reverse function the device and the
number of examples is going to be e F
dot test I believe and let me just go
ahead and check the eval model so it's
the accuracy and the Wars okay
so if I run this I should get the test
accuracy for a model and you should be
this number right here
okay so we have 82 percent 82.2%
accuracy on the test set which is quite
good because the reviews themself are
quite hard to sometimes classify even as
a human I mean like neutral a reviews
can be really challenging and we will
see that ever are pretty much state of
the art sentiment crossfire model is
having hard times with some of the
neutral reviews at least and let me
start our evaluation or continue our
evaluation using the classification
report from the psychic warned library
I'm going to pass him the test values
the predicted values and the target
names which will be the class names that
we have and for each class we have a
precision recall f1 score so right here
you can see that the positive and the
negative reviews have relatively high
precision and recall and the neutral
reviews at least in our case the ball is
having a hard time with those which is
expected because yeah those even as a
human are hard I can tell you that I've
read through many many many reviews on
Google Play or the App Store on Apple
and guessing reviews with three stars
it's really hot stuff okay so let's see
let's have a look at the confusion
matrix for our model and I'm going to go
ahead and copy and paste a helper
function that I've shown multiple times
right now so this show confusion matrix
function accepts a confusion matrix in
the format of data frame so I'm going to
prepare the confusion matrix by calling
that confusion matrix from socket worm
then I'm going to convert that into
updated frame and the index is going to
contain the class names and the columns
are going to be again the class names
and finally I'm going to call show
confusion matrix so you can see that
once again our model is having
relatively hard time with the
classifying the neutral reviews
otherwise it's going to drink pretty
okay job on the positive and negative
reviews which is once again expected so
if you continue training this you might
get even better results but I wouldn't
expect that anytime soon a model is
going to be perfect at this task you can
have a look at the training accuracy
walk and you can see that even on the
training data our model is not able to
achieve close to 100% accuracy next I'm
going to basically show you what unmod
our model thinks about a single review
and I'm going to choose an index for
that review and we'll start with index
of two I'm going to take the review text
for that so this is where the review
tags are really useful I'm going to take
the truth sentiment and I'm going to
create a prediction data frame which is
going to tell us the probabilities for
each class
and the values for those probabilities
at least the predicted ones
alright so next I'm going to print out
the text review and I'm going to use the
app function provided by the text wrap
module in Python 3 and if I bring this
you are going to see this is the review
text I'm going to print another wine and
I'm going to print out the true
sentiment for this review and you can
spend some time and read through this
and guess the sentiment of that one I
guess you might say that it's pretty
hard thing to do so if I run this again
you can see that true sentiment for this
is neutral so I used to use habitable
and I must say this is a great step up
I'd like to see more social features
such as sharing tasks only one person
has to perform setbacks for it to be
checked off otherwise etc etc I could
easily justify roughly $1 per month or
eternal subscription if that place could
be met as well as fine tuning this would
be easily worth 5 stars so this is
pretty much you might say that this user
is kind of a bully because he wants some
features and after some price reductions
or something like that then he or she
might eventually give 5 stars to this up
but if your developer don't succumb to
this kind of social bullying or user
bullying so next I'm going to show you
what our model things about this review
and does it actually predict correctly
that this is this is really a neutral
sentiment
so I'm going to put the predictions or
the probabilities for each class name
I'm going to pass in the prediction data
frame and I'm going to orient the chart
horizontally so it's a bit easier to
understand what's happening I'm going to
label the y-axis a sentiment and the
x-axis as probability and I'm going to
do a limit or normalization on the
x-axis so it's always 0 / 1 in in our
case our model thinks that this indeed
is a neutral sentiment let's go ahead
and try another example so let's see
example 42 why not and this is once
again neutral sentiment let's try
something else 33 we have some negative
sentiment what the heck I used to wonder
this happily for seven years complete BS
they make you sign in on account to use
this if no deleting this up immediately
so this is pretty much pretty negative
stuff right here and if we rerun the
prediction of our model it is highly
certain that this is really a negative
sentiment so our model is quite good
actually at least from what I've
expected so next I'm going to predict
sentiment on route texts so let's give
it some review texts that we are going
to think of I love completing my to dos
best app ever
let's do uppercase here so we have this
review and obviously this is like
positive review or five-star review and
if we go ahead and encode dust this
I'm going to use the tokenizer and cold
plus so this is pretty much the same
thing that we did into the data frame in
the previous video for how you can
encode your data and I'm going to pass
in the max length of the sequence which
is 160 I'm going to speche up the
special tokens a return token type IDs I
don't want those and I'm going to pass
the sequence I want the attention mask
and I'm going to convert those to
dancers or fighters dancers so after the
encoding is complete I have the input
IDs in the attention mask so I'm going
to move those to the GPU the input IDs
to the device in the same thing for the
attention mask okay
so finally I'm going to run the input to
our model and get output or the
prediction and I'm going to use torch
max to get the most certain prediction
and next I'm going to print the review
text and I'm going to print out the
sentiment that our model has predicted
at least for this one and I'm going to
print the quest name of the sentiment
and if we run this you can see that our
model is predicting that this is a
positive sentiment which is great let's
try another example horrible no let's
see very bad so expensive developers are
trash let's try this one of course those
are some easy examples and you can see
that the sentiment is negative right
here so this is pretty much how you can
train your model I will create or finish
up the tutorial and this one will be
linked into the description of this
video I'm going to continue with this
series and in the next video I'm going
to show you how you can deploy this
model behind REST API and we are going
to build a simple web app in which
you're inputting the text that you're
interested in instead of using this text
right here into the group eternal book
so it will be a more user friendly stuff
and the web app that we are going to
build probably in JavaScript is going to
make a post or it's going to make a rest
request or HTTP request to the server
and we are going to predict the
sentiment using the model that we've
already pre trained we are going to
return the result and the web app is
going to present some sort of
well I hope beautiful design or UX so
the user can predict the sentiment for
whatever text he is interested in thanks
for watching guys I hope you enjoyed
this series and if you liked this please
like subscribe to this channel turn the
notification bell on so you know when
you videos are landing thank you bye
stay safe