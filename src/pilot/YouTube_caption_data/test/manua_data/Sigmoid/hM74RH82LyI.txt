Here we introduced the important 
back propagation algorithm  
for training a neural network 
with gradient descent.
We need the relevant gradients for each weight,  
the derivative of the loss with respect to 
each weight in every layer of the network.
But the loss is computed only 
at the very end of the network.
How do we find these gradients 
for weights in the early layers?
The solution is a method called error  
back propagation or back prop 
in the neural network world.
It turns out that backdrop is a special 
case of backward differentiation,  
a method that relies on computation graphs.
A computation graph is a representation 
of the process of computing
any mathematical expression in 
which we break down the computation  
into separate operations, each of 
which is modeled as a node in a graph.
Consider computing the simple 
function L. of (a,b,c) equals c(a+2b)
If we make each of the component 
addition and multiplication operations  
explicit and give names to 
the intermediate outputs,
the resulting series of 
computations is D equals 2 * b.
E, which we've also created, 
is A plus D and then L is C
* E. We can now represent this as a graph 
with nodes for each operation and directed
edges showing the outputs from each 
operation as the inputs to the next.
The simplest use of computation graphs 
is to compute the value of a function  
with some given inputs.
So let's assume we have inputs. A equals 
three, B equals one, and C equals minus two.
We can then walk forward and 
compute a value for D as 2B
So that's two and then a value for E as A plus D.
So that's five. And then a value 
for L as five times minus two.
Here it is without my messy handwriting. The 
importance of the computation graph, however,
comes from the all important backward pass  
in the backward pass is used to compute the 
derivatives that we'll need for the weight update.
Now, in this example, our goal is to compute 
the derivative of the output function well  
with respect to each of the input variables.
So that's the derivative of L with respect to A  
the derivative, with respect to B and 
the derivative about with respect to C.
The derivative L with respect to A is telling us  
how much a small change in A 
affects the final output L,
while holding all the other variables constant.  
Backward differentiation relies fundamentally 
on lots of application of the chain rule.
Let's remind ourselves of the chain rule.  
Suppose we have a composite 
function, F of X equals U of V of X.
The chain rule tells us that 
the derivative of F of X  
is the derivative of view with respect to V 
times the derivative of V with respect to X.
And the chain rule extends to 
more than two functions. If we're  
computing the derivative of a composite 
function, F of X equal U, V, W of X,
the derivative of F of X can 
be computed by the chain rule  
as the derivative of view with respect to V times,
the derivative of V, with respect to W 
times the derivative of W with respect to X.
So let's now use the chain rule to 
compute these derivatives that we need.  
Since in the computation graph
L equals C times E, we can directly compute 
the derivative, the partial derivative of L.
With respect to C is simply E.
For the other two, we'll need to use 
the chain rule. The derivative of  
L with respect to A is the 
derivative of L with respect to E.
times the derivative of E with respect to a.
And similarly, the derivative 
of L with respect to B  
is the derivative L with respect to E, E, 
with respect to D and D with respect to B.
I put those equations up here on 
the top. And you can see that they  
require five intermediate derivatives 
through of L with respect to E e,
with respect to A E, with respect to D and 
D, with respect to B and L with respect to C.
And we can compute them as follows. The derovatove 
of L with respect to E, we already saw with C  
and the derivative with respect to C is just E.
Making use of the fact that the derivative 
of a sum is the sum of the derivatives,
the derivative of E with respect to A is just one,  
and the derivative of E 
with respect to D is just 1.
And the derivative of D 
with respect to B is just 2.
In the backward pass, we 
compute each of these partials  
along each edge of the graph from right to left,
multiplying the necessary partials to 
result in the final derivative we need.
Thus, we begin by annotating the 
final node with a partial of L.
With respect to L. Which is one. 
And then we move to the left.
We can compute the partial of L with respect to C.
Which we can look up here and we see 
that's E and the forward pass conveniently
will have computed the values of these 
intermediate variables we need like D and E.
So we know that E is five. So 
we know that the partial of L  
with respect to C is five and now we 
can also go left to the partial of L
with respect to E, we can look that up in 
that's C which we know from here is -2.
Partial of L with respect to E is -2.
And now the partial of E with respect to D.
We can look that up, that's 
1. And now the partial of L
with respect to D. We get that by multiplying 
the partial of L with respect to E and E.
With respect to D. Just like here. These two.
And so that's gonna be -2. 
We can go one step further.
The partial of D. With respect 
to B we can look that up.
That's 2. And now the partial of L.
With respect to B: we multiply 
our -2 times 2 and we get -4.
Well, we can do similar things 
for the rest of the graph.  
Here is the entire backward 
pass written out neatly.
Of course, computation graphs for real 
neural networks are much more complex.
Here's a sample computation graph for a two 
layer neural network with two input units,  
two hidden units and one output unit.
And we'll have a ReLU in the middle and a 
sigmoid at the end. Here's the equations.
So we'll have our W, X plus B, our ReLU, and then 
the next layer of W, X plus B and now a sigmoid.
And the final answer Y hat is 
the activation after the sigmoid.
In order to do the backward 
pass on this computation graph,  
we'll need to know the derivatives 
of all the functions in the graph.
So we've seen the derivative of the 
sigmoid, the derivative of sigma of z  
is sigma of z times one minus sigma of Z.
And the derivative of the ReLU. Here's a 
picture of the entire computation graph.
The weights that need updating, so 
those for which we need to know the  
partial derivative of the loss 
function, are shown in orange.
So for a particular example, 
a particular observation,  
X one, X two, we would run the forward pass.
Assign variables to all of our nodes 
and then starting with these last nodes.
Run the backward pass. And for the backward pass
we'll need to know the derivative 
of all the relevant functions.
Let's show how to start off the backward 
pass just by computing the first couple of  
steps to compute the derivative,
of the loss unction with respect to 
Z. And here I mean Z2, the last Z.
So, I'll write. A for a^2 and Z for z^2.
So we have the loss. And we 
have our two layer network.
Here's the standard cross entropy 
loss function. And for y-hat
I'll write a, meaning a^[2], 
just replace the Y hats with A's.
And now if we want to compute the partial 
of L with respect to Z, this Z here.
That's the partial of L with respect to A 
times the partial of A with respect to Z.
The partial of L with respect to A we can 
take the derivative of this with respect to a.
So that's y times the derivative of 
log(a) with respect to a, plus one minus Y  
times the derivative of the log of one minus a.
With respect to A.  
And conveniently the derivative of log of 
X is one over X times the derivative of X.
So we have Y times one minus A.  
And then here we have one for one minus eight 
times the derivative of one minus eight.
So that's minus one. And that simplifies neatly. 
And then the derivative with respect to Z.
The beautiful sigmoid derivative is just a times 
one minus a. And we can multiply these together.
And simplify even more beautifully. To a minus Y.
So we've seen a couple of pieces of the 
backward pass for a simple two layer neural net.
So in summary, for training a neural 
network, we need the derivative,  
the loss with respect to weights and 
earlene the layers of the network.
But the loss is computed only at the end. And 
the solution is backwards differentiation.
We take a computation graph and given the 
derivatives of all the functions in it,
we can automatically compute the derivative, 
the loss with respect to these early weights.
We've seen the important 
back propagation algorithm,  
the idea of computation graphs 
and backward differentiation.