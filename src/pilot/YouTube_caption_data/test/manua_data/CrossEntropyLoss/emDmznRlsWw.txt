 Yeah, so this was a very long lecture on sequence modeling
 with recurrent neural networks and transformers. So we talked
 about RNNs with attention, then we talked about self attention
 and multi hit attention, which are concepts found in the
 original transformer model. And then we talked about some
 popular transformer models, such as BERT, GPT and BART. So in
 this last video, I wanted to show you a code implementation
 of BERT. I know that, yeah, this lecture has been about
 generating sequences. So, however, as we also have
 learned, the BERT model is more or better for discriminative
 modeling, like prediction tasks, whereas the GPT would be a
 better model for generating text. So BERT, because I just
 find this model also interesting, I wanted to show
 you a simple example using our familiar movie review
 classifier data set. And then we can compare it to the
 performance of the LSTM model that we trained in lecture 15.
 But of course, please feel free to experiment more with this.
 There is essentially a company who is developing an open source
 library that everyone uses when working with transformers at the
 company is called hugging face. And yeah, they have a lot of
 tutorials. So I was just focusing on classification, but
 they have many other tutorials available. So if you are
 interested, so you can find also sequence generation tasks, and
 so forth, like question answering, and yeah, different
 different types of things, language modeling. So please
 feel free also to explore more. I wanted to just because it was
 a long video or lecture already, I wanted to just finish this
 with a shorter example, showing you how to use this resource,
 the transformer library from hugging face. So here we are
 going to train a BERT model. And I was actually implementing BERT
 models on this movie review data set or fine tuning it
 essentially, I couldn't get really good performance with
 that. So I changed that to using distilled BERT. So distilled
 BERT is a smaller, faster and computationally cheaper version of
 BERT. So they essentially took BERT, the original BERT model
 and distilled it down to a smaller size. So it has 40% less
 parameters. So I can make this a little larger here. So it has
 40% less parameters, and run 60% faster, while achieving
 approximately 95% of the original performance of BERT
 measured on a language understanding task. So again, I
 just wanted to show you in general how we can import these
 models from the transformer library, which we can install
 here via pip install transformers. So in for this
 particular example, I was using the version one 4.6. Alright, so
 and I also structured this whole notebook in a similar way that
 I structured previously, for instance, the LSTM notebook that
 we had for movie review classification. So I'm
 importing a couple of things here as a tokenizer and
 classifier model. So this is just for pre processing the
 text. And then this is the main for loading the main distilled
 BERT model, the tokenizer here, but I understood correctly based
 on the documentation is actually the same tokenizer that they use
 for the regular BERT model, they call it distilled BERT, but you
 could technically also just use BERT tokenizer. Just a few
 settings here. I'm only training for three epochs, because it's
 it's a slow, it's a big model, right? It's what was it 300
 million parameters. So it takes a while to train. So that's why
 I only train for three epochs, because I didn't want to wait
 forever. Um, yeah. So actually, 300 million parameters, I think
 is the original BERT model. So that is 40%. So it's more around
 170 ish. So first, I'm loading the data set. So remember, I had
 like a pre processed version of that data set that we used also
 in our LSTM code before just for convenience, I'm using that it's
 a CSV file. So here, this is just downloading and unzipping
 the CSV file from my book directory where I have this
 pre processed version. And here's how it looks like the
 reviews as text and then the sentiment here as as the class
 label, whether it's positive or negative. We have 50,000 movie
 reviews, and I'm splitting this into a training validation and
 test set. So I use the first 35,000 for training, then I use
 5000 for validation, and the remaining 10,000 for testing.
 Then I'm loading the tokenizer from the pre trained model here.
 So why using a pre trained tokenizer, I think this includes
 the generation of the word embeddings. So the vocabulary,
 and everything. Now, this is very convenient. So we can just
 apply that. So now it would then process those other words, and
 our reviews the same way it processed the ones when it was
 doing the unlabeled training. So just to recap, let me go to the
 lecture slides. So there are two steps I should have maybe said
 that earlier, we are in this notebook only focusing on the
 pre training. So if I go back to the main concepts. So yeah,
 there are two main approaches, the two training approaches for
 the transformer. So first, there's this pre training on the
 large unlabeled data set. And then there's the training for the
 downstream tasks on smaller label data set. So in our case,
 we are loading the pre trained one. So the people at hugging
 face on prepared this pre trained model, which we are
 loading. And then in our particular case, we are just
 doing the training for the downstream tasks on this small
 labeled movie review data set. And we are using the fine
 tuning approach where we update the whole bird model. And then
 we are doing the pre training. Okay, um, so here we are
 creating then our encodings. So encodings for the text, I think
 this includes also the word embeddings. So the context size
 is 512. So that's also one thing to keep in mind, what it will do
 is it will truncate. So if we have a movie review that has
 112 words, it will do a padding. If it's longer than 512 words,
 it will truncate. So then everything will be the same
 size 512. And we set up our data set here. So this is just a
 pytorch data set from the main pytorch library. So it's
 something kind of familiar. So now we work with encodings and
 labels. Yes, a specific way to pre process or arrange the
 encodings. So it's essentially from the key, like the ID, the
 word ID to the encoding. Sorry, yeah. Sorry, this includes the
 labels, right, right. Okay. Yeah. And then we construct our
 data set from the encodings and the labels. So this gets our
 encodings and the labels. Let's see the encodings. And this is
 the labels. labels are just the zeros and ones, then I'm setting
 up my data loaders. Notice I'm only using batch size of 16,
 because the bird model is quite large. And I got memory problems
 on my GPU when I was increasing that. And now we are loading the
 pre trained model here. So we are using the distill bird for
 sequence classification from pre from pre trained. So we are
 using an uncased model. So it's case insensitive. That was
 something I tried. It's just a simple model. I think they also
 have case sensitive ones. But since we have a small data set,
 that might be the better option here, using the atom optimizer.
 And then we're doing the training. Okay, so training is
 very unspectacular. So this is by the way, that's just the
 accuracy function. I have carried this over from our
 previous code examples, I made a few modifications, just about
 this part here. This is for loading the data. But except
 that it's this is from our STM notebook earlier, where we also
 should have a accuracy function here. Just took it from here
 from our previous code, modified a little bit for our encoder,
 transformer. And yeah, that is then computing the accuracy in
 the same way. Now not also that the outputs when we run the
 forward pass of our model, so the bird model gets the input
 IDs, and then the attention mask. And the outputs are two
 things, it's a tuple, one is the loss, and one are the logits. So
 it's computing already the loss inside. So we don't have to
 worry about, let's say using a cross entropy loss during
 training also. And then we get the predicted labels by looking
 them at the maximum logic here. So this is similar to our LSTM
 classifier, or any other classifier we trained before.
 Okay. So yeah, we are training again. So this is for preparing
 the input data set. So getting the input IDs, the attention
 mask, and then the labels, and these all go together as input
 to the bird model. So if it's the input ideas, sorry, I said
 earlier, that the tokenizer includes the word embedding,
 that's not true. This is then happening in the model itself,
 if it's just in getting the ID. So the conversion to the
 embeddings must have happened in the bird model itself. So if we
 go back to our slides, so I don't have a specific bird
 slide here, I think. But if we look at the general transformer
 here, so these input embeddings are probably happening inside
 the bird model, together with the positional encoding. So in
 bird, this is GPT. In bird, this looked like this, if you recall,
 so these embeddings probably happen in inside the model, the
 easier. Okay, because this is just the IDs. Yeah, then the
 backward pass, we set the gradients from the previous round
 to zero, perform the backward pass on the loss. Again, the
 loss is returned by the bird model. So the model when we call
 it with a forward pass, we return a tuple, which consists
 of loss and logits. And we use the loss for the backward pass.
 Here, we don't really use that logits, we only use the logits
 later, here for computing the predicted labels in our accuracy
 function. Yeah, and then it's training. This is just our
 boilerplate for keeping track of the training. And yeah, it's
 take some time, it's 20 minutes per epoch. It's much, much
 slower than our LSTM, but it's also much, much larger than our
 LSTM in terms of the number of parameters. And in the end, it
 gets pretty good performance. So three epochs took about one
 hour gets 99% training accuracy and about 92% test accuracy. Now
 let's compare that to our LSTM model that we trained before in
 lecture 15. So that was the with the packed sequences where we
 had a more efficient way of packing sequences. So when I
 compare it with this model, we got around 89% accuracy. So let
 me scroll down to the bottom. So it trains much faster, you can
 see it's only a few half a minute, approximately less than
 half a minute for one epoch. And 15 epochs was just in five
 minutes or four minutes, it's impressive. So 89% accuracy,
 it's actually not too bad. So, but yeah, using our pre trained
 board model, we get even better performance out of this, which
 is also interesting. So yeah, in that case, that's how the bird
 model works is how we can use a pre trained model and fine tune
 it. So the key idea is essentially using the tokenizer
 to get the encodings, the so called encoding. And then we can
 use this is also getting our attention masks. And then we can
 use the pre trained model here distilled word for sequence
 classification, and just train this further for our target
 data set. So did some more experiments. So to further
 improve the performance, I looked into what they recommended
 in the tutorial. So I made a few changes. So one of the changes
 was using Adam w, which is an atom that supports decoupled
 weight decay that's to regularization. So let me use
 this. So here's from the paper. So there's a paper corresponding
 to that Adam with weight decay here if you're interested. And
 this is just from the screenshot explaining how they perform the
 Adam weight decay, but let's not get sidetracked here. So the
 difference is I'm now using Adam with weight decay here, before
 I just used the regular Adam. And also what I'm doing is I'm
 using a learning rate scheduler with a linear learning rate
 schedule. And this is something I got from the hugging face
 repository. So let me Yeah, so you can see that. So I got that
 from the transformer library. That's also something they
 recommended with these settings. And when I train that then, with
 these two changes using Adam w and the linear schedule, which I
 update after each iteration, I got even better performance, I
 got 93% point three, it's about 1% better test accuracy. Another
 thing I found really cool is what they also have in the
 hiking face repository is they have a so called trainer class.
 So this is essentially the same as before. So I'm using again,
 distill bird for classification with us. They also have an Adam
 w I use just the one I think in pytorch. Maybe there's this even
 better. I'm not exactly sure what the differences are between
 their Adam w and the one in pytorch. But my guess is either
 they implemented it before it was implemented in pytorch, or
 they have like a small change to it that makes it maybe more
 specific or better for transformers in any case. So this
 is all the same setup, also the same tokenizer. But now the
 difference is after loading the model, so maybe I should have
 left it up here. Sorry. Okay, because I'm not using the one
 from pytorch. I'm using theirs now. So before I use the one in
 pytorch, I'm using theirs now doesn't really matter. And here
 I'm using this trainer class from the repository. So this is
 actually very convenient, they pack a lot of functionality in
 there. They have a per device training batch size. So when I
 was running this on my machine, which had multiple GPUs, it was
 utilizing the different GPUs for efficiency. And you can also
 specify directly the scheduler and everything. So these are
 the arguments for the trainer. And then you can specify the
 trainer with these arguments can provide a training set and
 validation set. So you can just use that one, instead of having
 our oops, I'm not sure why it's what swaps. I want to show both
 at the same time, somehow, it doesn't like it right now. But
 instead of defining our training loop here ourselves, we could
 also use use their trainer here. And then it trains for a couple
 of times gives you some output for the different steps or
 different losses here, can see the loss goes down. And yeah,
 it's my accuracy and evaluation and get even slightly better
 performance than with the one I trained with my own scheduler.
 Okay, so this was just a very quick video on using distilled
 bird from hugging face, they have way more models here, you
 can see there's a huge range of models. They also have GPT
 models here if you're interested in the more generative modeling
 parts, this is usually a model that is better for sequence
 generation. But yeah, there's lots, lots of stuff here, if
 you're interested in language models. So with that, let me end
 this lecture, which was already probably a little bit too long.
 But yeah, I hope that was useful.