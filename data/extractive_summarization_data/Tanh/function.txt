 This Tanh is a hyperbolic tangent itself, but in the world of Neural Network, Tanh converts the input value into a non-linear one and keeps it in the range of -1~1.  It is used to serve as an activation function. 
 As you can see, when a large positive value is input, the output value is stuck to 1, and when a large negative value is input, the output value is stuck to -1.  And when the value in this area is input, the output changes almost linearly. 
In this way, it can be shown that a combination of such functions can approximate any non-linear function. 
On the other hand, to overcome the vanishing gradient problem, we need a function whose second derivative can sustain for a long range before going to zero.  Tanh is a good function with the above property.
Tanh and the logistic function, however, both have very simple and efficient calculations for their derivatives that can be calculated from the output of the functions; i.e.  if the node's weighted sum of inputs is v and its output is u, we need to know du/dv which can be calculated from u rather than the more traditional v: for tanh it is 1 - u^2 and for the logistic function it is u * (1 - u). This fact makes these two functions more efficient to use in a back propagation network than most alternatives, so a compelling reason would usually be required to deviate from them.
