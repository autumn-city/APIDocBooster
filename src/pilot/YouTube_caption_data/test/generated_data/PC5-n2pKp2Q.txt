and completed his mtech in the
intelligent systems from uh triple i t
alphabet in the year 2004 he received
his phd from j into ananthapur
under the supervision of dr c h process
chdv subaru
currently he is working as head research
and professor in it at blue crest
university liberia he has 19 years of
teaching and industrial experience he
served as the head of the department of
i.t in sv college of engineering
therapeutic during 2009
he has seven ibm professional
certifications
and two microsoft certifications three
edit certifications and so on he has
completed nine nptel certifications
four spoken tutorial projects from iit
bombay he has guided one phd and five
students pursuing phd under his guidance
so he has more than 110
publications in his name in several
journals and conferences
and he has written three text books and
he has been sanctioned to dht seminar
grants
and he has received several awards like
bharti vidyarathan
and
international business council delhi has
given the award on december 14 he has
received state itap 2020 award best id
teacher of the year award
and best mentorship award from igm tgmc
challenge and best spoc
from iit bombay hsb spvac from nbtl swim
etc
and thank you sir thank you for
accepting our request and coming as a
resource person for this workshop
over to you sir
thank you sir for nice introduction
so before go
going to
a data science applications
using machine learning and deep learning
techniques
first let me check
what is happening in the worldwide
where the data is going
how the data is coming
and how do
we store the data
how many servers are required
please check this
url
uh now see this uh
url
www.internetlivestats.com
how many internet users are connected
right now
total number of websites opened
email sends today
and tweets
google searches today
how many videos viewed right now in this
second say the number now the number is
increasing rapidly
you cannot read this a huge number
that means
every second
thousands of users lacks of users are
connected to this internet
so where this huge volume of data is
stored
how many servers are required
how many data centers are required
so this is a very big question
in order to solve this question
we use
cloud computing concept in the behind
cloud is nothing but
internet internet-based technology
the backbone of
cloud computing is
virtualization and data center
i think everybody knows data center
collection of high integrated servers at
one place is called the data center
virtualization
is the ability to perform multiple tasks
in a single system
with help of
popular virtualization
technologies are
hypervisor
zen
virtual iron
so with this
we are creating a number of virtual
machines
so that's why
gmail is a best consumer of cloud
computing 24x7 we are getting the
service
how many servers are required to store
this this data
see it retweets
websites open
skype calls
tablets sold
smartphones sold
computer sold
this is the statistics we can see in
this present scenario
so
so today
i would like to explain the
contents introduction to data science
statistics components
artificial intelligence concepts
data mining concepts
machine learning concepts
and deep learning concepts then we will
see the practicals
on kovite 19 virus prediction and
detection
and kovid 19 straight
virus production and detection
ebola virus prediction detection
prostate breast and lung cancer
prediction detection
these are the applications recently i
have executed six database health
database applications i have
ex executed
so the pre-requisition of data science
starts with data
data is raw material
fact or image
data can be anything audio video text
graphics animation
process data is called information
collection of interrelated data is
called database
to manage the database we are using
relational databases
such as sql mysql teradata
db2
ms access
these are the relational databases
nowadays
we are using
relational less databases
such as
 database
hive
hbase
cassandra
etc
combination of data plus dbms
is called
database system
storing of
subset of data warehousing is called
data map
collection of data mods is called data
warehouse
data warehouse is also called as online
analytical processing
once we are storing huge volume of data
we are extracting
meaningful interested patterns from the
huge volume of data
that is called data mining
with help of
classification clustering association
rule mining outlier detection
recommender engine etc
then comes to the big data analytics
it deals with the
three v's majorly
like facebook database
traffic database
weather forecasting database health
databases
etc
there are problems like
volume velocity
variety
veracity
viscosity
value
validity etc
so in order to solve big data sets
we are using
hadoop
spark
matlab
python
advanced java concepts etc
then finally
data science
data science is the process of examining
and extracting
hidden patterns
insights from the
large volume of data
with the help of
scientific methods
processes
algorithms
and
computer programs
that is called data science
so can anyone give the answer for this
logo
can anyone technical name of this logo
everything we have incorporated in this
one
analysis process system structures
knowledge programming
everything incorporated in this image
what is the technical name of this
logo
the technical name of this logo is
called
data science word cloud
everything incorporated in this one
then how do we measure the computer
information
computer understands only ones and zeros
one bit is equal to zero or one
four bits is equal to one nibble
eight bits is equal to one byte
four bytes is equal to one word
four votes is equal to thousand twenty
four bird
then kilobytes megabytes gigabytes
terabytes petabytes hexabytes octabites
jetta bites
your turbines
hi i am robo
my speed is one terabytes
my memory is one zettabyte
before that robo movie we we don't know
the terminology of how do we measure the
computer information after that movie
we got to know
uh jetta bite yottabytes so thanks to
sankar director
and thanks to rajnikanth
after that
i have searched yesterday night there
are two more terminology
is presented now
1024
yottabytes is equal
one brontobyte
1024 brontobyte is equal to
one zero
byte so
it is not jiojio
that jio ruined students life
they are not studying these days
they are playing games because of
internet facility
so
if you ask me
1024 geo byte is equal to
infinity
if you compare normal range big data
range
kilobyte is equal to 10 power 3
and gigabyte is 10 power 9
terabyte is 10 power 12
then big data range
if you see
10 power 15 is called one petabyte
10 power 18 hexabytes
10 power 21 jetta byte
10 power 24 your terabyte
so 10 power 27 one brontobyte
now 10 power 30
1 geobin
that is a maximum measurement of
mathematics range
so data is central
to data science without data there is no
database application
so if wherever data
is presented
we are going to solve
very complex applications in data
science
with help of machine learning
ai techniques and deep learning
techniques
so data is categorized into two types
categorical data numerical data
categorical data examples
marital status
now political parties
eye color
these are the categories of data
numerical data again we are classifying
into two types
discrete
continuous data
discrete data is nothing but
say number of children
defects per hour
like
counted items
continuous data
see as a human being we are growing
every everyday weight
weight voltage measured in
characteristics
then how do we collect the data
there are data collection tools such as
interviews
through observation
service
usage data
focus groups we are collecting the data
like online forums
in-depth interviews online communities
web survey chart etc
so where this data is coming
what are the big databases
big data sources
like facebook database
legacy documents world databases
media database cloud database web
database facebook database social
influencers active generated data data
warehousing applications
network related iot databases
so
erp data is a big database for data
science applications
may be a transaction data public data
social media data data in health data
and marketing
and mobile devices cameras data
microphones data
data is everywhere and it can help
organizations
any industry in many different ways
data has become too complex
too dynamic to be able to process the
data store the data analyze the data to
manage the data with traditional data
tools
those are sql mysql teradata db2 ms
access etc
we are unable to process traditional
data databases that's why we are going
to unstructure data to handle
unstructured data we are using hadoop
spark matlab etc
so the beautiful definition of data
science
it is the it is also called as kdd
process
knowledge discovery database
it is a data driven science
it is an interdisciplinary field
about scientific methods processes
and systems to extract our insights from
data in various forms
either it is structured format and
structured format
if you take
data science is a kdd process
as a programmer we have starts with
quality data
how we are getting quality data before
quality data we are doing pre-processing
eliminating redundant data noisy data
dirty data inconsistent data etc
then data becomes data quality
then descriptive data diagnostics data
predictive data and semantic data and
finally we are getting knowledgeable
data
that is called a
kdd process
data science is a combination of
visualization
statistical modeling
statistical computing data technology
data research data consulting
real world application scientific
methods
there are three important
characteristics of data science data
volume
what is the size of the data we are
handling in the real-time environment
data variety
how many types of files we are handling
the real-time environment audio video
text graphics animation etc
data velocity
what type of databases we are handling
batch processing periodic data near time
data real-time data legacy data old data
what type of database we are handling in
the real time in
environments
why do we want to study data science in
your mind you are getting doubts
that's why
if you are collecting more data it leads
to the more knowledge
more knowledge leads to the more money
all business magnets are increasing
their businesses because of this
more knowledge how you got more
knowledge you are collecting more data
then we are doing pre-processing with
help of free pre-processing techniques
likes
binning box plot etc
data science is the process of examining
large amounts of data
of variety of types to uncover hidden
patterns
interested analytics
unknown correlations under the other
user information
that is called
data science
so why data science what is the
responsibility of data science now
we are collecting data from various
sources like cloud computing internet of
things big databases that is called data
fication
once we got the data then we are doing
pre-processing
then we are applying the machine
learning algorithm then we are
developing a model then finally we are
getting the results
we are analyzing the results
so why learn data science my dear
participants my dear students
so it is a fuel for 21st century
problem of demand and supply a lucrative
carrier
data science is changing the world
data science is a future
because
we are solving
complex applications
so facebook alone generating thousand
terabytes of data daily whereas many
other organizations like jet airways
stockx
stock exchange market generates
terabytes of data every hour
so data science is a collection of data
sets
which is so large and complex that is
difficult to handle using dbms tools
there are three types of data we are
handling in the real-time environment
structured data unstructured data
semi-structured data
so with help of relational databases sql
mysql teradata db2 we are creating the
tables and databases
this is called a
structure data
examples tables transactions legacy data
etc unstructured data is nothing but
there is no schema there is no procedure
but we are creating a databases
like a facebook database health database
stock market exchange database
audio video text graphics animation
youtube videos that comes under
instruction data
so in order to process unstructured data
we are using hadoop spark matlab or
python advanced java programming
languages
tools
then semi-structured data
it is a 50 percent
structure data and 50 percent
unstructured data
that is called a semi section data
so who uses uh
data science
big change in the market
disney parks google amazon nasa
booking.com discovery facebook uber
linkedin these are the very big chains
are using
this data science
applications
what is the structure of data science
data processing
it is consists of three tires tire one
data science
data sources tire to big data analytics
and finally tier 3 analytics
data sources consists of structured data
and structured data
we are giving as input the system
and we are choosing the tool and
technique and algorithms
hadoop hive mapreduce matlab spark or
python etc
final analytics we are generating
dashboards reports and scorecards
through pie chart bar chart line chart
etc
and there are various data file formats
such as audio video image text graphics
animation
sql files db2 files public record books
transaction documents
etc
data science is a interdisciplinary
subject
we are using statistical methods
like
linear regression nonlinear regression
logistic regression
statistics operations like mean mode
standard deviation
operations
we are using advanced computing cloud
computing data centers we are using then
visualization purpose we are using r
matlab
tableau etc
data science is a multi-disciplinary
subject
see how many subjects are incorporated
in this one statistics pattern
recognition machine learning
data mining databases and data
processing
visualization
so this is the
all subjects integrated see now data
science integrated software development
because we are using python java or
matlab etc
and then business analytics we are
generating
and statistical mathematics we are using
like linear regression non-linear
regression
and logistic regression machine learning
algorithms we are using
such as classification clustering
association rule mining
outlier detection recommend our engine
in machine learning three types we are
using supervised learning unsupervised
learning reinforcement learning
data science roadmap
if you see the first we need to frame
the problem
then unstructured data
and then we need to understand the data
then extract the features
then develop a model
that model predicts the
results and code
so this is the data science institute
there are five steps
capturing the data with help of data
acquisition data entry signal reception
data extraction
then we are maintaining the database
with help of data warehousing tools like
informatica hadoop hive hbase cassandra
etc
then data cleaning
pre-processing is nothing but data
cleaning
eliminating the redundant data noisy
data inconsistent data dirty data
data staging
we are converting one form to another
format that is called data staging data
processing data architecture etc
process
we are using data mining clustering
classification data modeling data
summarization
finally we are analyzing the data
text mining regression
predictive analytics qualitative
analysis explorative analysis etc
and communication finally we are
connecting with the end users
then this is the life cycle of data
science
understand the problem business
data collection
data preparation exploratory data then
we are developing the model
model evaluation
model deployment
these are the major components of
data science
data
there are three types of data i have
already explained structure data and
structure data satisfaction data
data is a collection of factual
information
information
big data is nothing but
deals with the large data sets which are
traditional systems are unable to
process the data
big data is enormously big data sets
various v's
volume velocity variety value
veracity vision
validity
etc
machine learning is nothing but learning
from the observational data
there are three types of data we are
handling in the real-time environment
okay
uh
any doubts if you are getting please ask
me
supervisor learning unsupervised
learning reinforcement learning these
these are the
three types of
machine learning techniques i am going
to show you practicals
then statistics and probability
the numerical foundation of data
sciences insights and likelihood
there we are applying three types of
techniques linear regression non-linear
regression logistic regression
then finally we are applying programming
languages
like python java
or matlab
python i mean
julia programming etc
why python is so popular rich number of
packages we are using in this python
programming
recently i have used
keras tensorflow pytorch sklearn numpy
pandas
various packages i have used in order to
solve the deep learning under machine
learning applications
then what is the difference between data
analyst and data scientist
data analyst is concentrating only two
parts like business administration
domain specific responsibility for
example marketing analyst
campaign management financial analyst
equity research etc
data exploration analysis and insights
data scientists
is concentrating on data exploration
analysis and insights advanced
algorithms machine learning deep
learning algorithms is a applying for
various applications
finally data product engineering
applications is going to developing
so
the process of data science final step
acquiring the data
preparation of the data reformat and
clean the data then analyze the data
and applying technique
and getting the results and getting the
results and analyze the results finally
we are communicating with the customers
end users stakeholders
these are the data science tools
sas
sas is nothing but
so statistic analysis software
we are using spss apache spark big ml d3
matlab excel tableau jupiter matlab
live
natural language toolkit psychic learn
tensorflow keras
various
tools we are using in order to solve the
data data science applications
if you see the tableau for visualization
purpose and excel also we are using to
generate the statistics rapidminer it is
a data mining tool data robot
and click view and h2o tensorflow big ml
snowflake
trifecta these are the data science
tools
usually there are three or five kinds of
five kinds of
data science tools we are using data
store purpose we are using mongodb
database mysql
hadoop
like that
then transformation we are using spark
python sql etc
then for modeling purpose we are using
panda spark sql
etc for visualization purpose
we are using
our r program tableau
d3
etc
other tools like a kafka for messaging
tool rabbit and queue
various tools we are using
then data science if you want to settle
as a data scientist in the market you
must have knowledge on mathematics and
statistics ethical skills
team player skills life long lifelong
learning skills
communication skills
real world machine learning data
visualization skills date of wrangling
and
data wrangling is nothing but
the lower wrangling is nothing but
hello
reprompting the database
processing the database
we are eliminating noisy data
then skills as a data scientist
then skills as a data scientist have
data mining
foreign
hello sir my voice is audible
sir some echo is coming sir i think you
have
it on to ah now it is ok sir
hello sir
hello sir my voice is audible yes sir
now my voice is audible yes sir
okay okay
hello
[Music]
hello sir
hello sir my voice is audible
yes sir yes sir
but uh screen share has stopped
yes yes yes i'm giving
so
70 to 80 percent statistics we are using
in order to solve the real time database
applications data science applications
then machine learning algorithms we are
applying and finally visualization five
percent which we are using like our
tableau
various tools we are using
sir screen share is not there sir
one second sir
okay sir
um
screen share
so my dear participants any doubts if
you are having please post in the chat
box so that i am going to
clarify your doubts
don't be hesitation mode
you must ask queries
so that you can update knowledge in this
set data science domain
here i am going to cover
five subjects like
statistics
ai techniques
sir please uh allow me sir
okay
i'm i'm going to share my screen please
allow me
so now you can check sir
yeah it is coming now
so now my screen is visible
yes sir it is visible
okay so
this is the way we are solving uh
data science application statistics 70
to 80 percent machine learning 10 to 15
percent
predictive modeling 70 to 80 percent
30 text mining
visualization 5 percent
so my journey to become a data scientist
and head research
i have applied like this step-by-step
procedure
understand the customer's customers
business
define the metrics that matter
translate for non-technical audience
statistical packages
advanced mathematics experimental design
model fitting scripting languages
predictive analytics
and hacking and coding tool tools like
sql
mysql
teradata hive hedge-based data
governance data preparation get the
right
and ask good questions make it
actionable
so five subjects i am applying in this
data science
what are the data science advantages and
disadvantages
so it's in demand right now
abundance of positions highly paid
career highly prestigious jobs are
available versatile
disadvantages it is blurry term
mastering data science is near to
impossible
large amount of data knowledge is
required arbitrary data may yield an
unexpected results etc what are the
applications of data science image
recognition
speech recognition
internet of science
digital advertisements
recommend our systems
price comparison websites
gaming delivery logistics fraud and risk
these are the various applications we
are solving with help of machine
learning and deep learning
applications so data science use cases
amazon is the big change in the market
improving e-commerce experience they are
using
machine learning deep learning
application algorithms
optimizing rights uber bank of america
increasing customer experience
and airbnb improving searches they are
using machine learning and deep learning
techniques
and data science use cases i have
already explained what are the
major use cases in the real-time
environment
health care targeted advertisement image
recognition
fraud and risk detection
price comparison websites internet
searches speech recognition predictive
predictive systems online dating product
delivery
various use cases are available right
now in order to solve
real time applications
data science use cases
the major use cases in healthcare right
now in this pandemic period we are using
uh recently i have collected 1960
x-ray image databases from the hospitals
and
then after collection of uh
real-time x-ray images from the hospital
from uci web page
yes sir
screen
yeah
and
after that i have collected a few more
synthetic data sets from kaggle database
uci database
then i have applied
machine learning and deep learning
algorithms
hello
sir my screen is visible no sir
yes sir now it is my screen is visible
sir yes sir
yes sir it is visible oh
yes yes
and
data science case studies
if you see the data science in
pharmaceutical industries
predictive modeling for maintaining oil
and gas supply
data science in biotechnology data
science and education everywhere
everywhere we are using
machine learning deep learning
techniques
so the future of data science
algorithms especially we are using
massive scale graphing algorithms
geospatial temporal predictive analytics
and hyper fast analytics embedded deep
learning cognitive machine learning
natural language algorithms
and ai techniques
applications in future of data science
cyber security applications health care
data analytics iot internet of things
applications
smart
everything customer engagement expertise
and data for a
societal good purpose we are using
and the summary
all of them have a tech background
mission learning and data mining are the
common skills
programming
statistics database and visualization
are the top areas they specialize
so you must have knowledge on hadoop
stack mathematical modeling nlp machine
learning behavioral economics predictive
analytics data algorithm python splr
julia programming etc
so
so what is the mathematics you require
to solve data science applications
what type of mathematics we are using in
math maths linear algebra and calculus
we are using
in statistics
we are using a descriptive analytics and
uh
inferential statistics we are using
uh sorry uh
here are a lot of disturbances through
internet sorry for inconvenience
again i'm connecting my screen
so
what type of mathematics we are going to
use in data science applications
especially descriptive statistics
inferential statistics
i am going to show you practicals
linear regression algorithm in python
how we are using for predictive database
applications
and linear regression non-linear
regression logistic regression
how i am using linear algebra calculus
etc
i am going to
explain these practicals
one second
a lot of disturbances there
so now my screen is visible
hello my friends
so statistics i think everybody knows
statistics there are two types of
statistics we are studying one
quantitative statistics and qualitative
statistics
so statistics can be used to derive
meaningful insights from data by
performing mathematical computations
on it okay so statistical methods
descriptive methods and inferential
methods we are using in descriptive
methods we are using three types
univariate methods bivariate methods and
multivariate methods
inferential methods we are using central
limit theorem binomial theorem and
hypothesis testing normal distribution
and applied to means like
t test anova site test sci square test
etc we are performing i have explained
already data maybe a two types
qualitative data quantitative data in
qualitative data
we are using like a categorical or
attribute like marital status political
parties eye color etc quantitative data
we are discussing data that represents
counts or measurements there are two
types discrete and continuous
number of children defects per hour
continuous weight and voltage etc
so in basic probability in statistics we
are using central tendency like mean
median mode and outliers
measures of spread we are using range
standard deviation variance and
quantiles
in percentiles we are using position
position of data percentile rank
percentage
percentile range
probability distribution we are using
uniform distribution normal distribution
that is called gaussian distribution
poisson distribution dimensionality
reduction we are using pruning
techniques especially in image
processing phase recognition application
we are using principle component
analysis
and sampling we are using a
software uh
i mean a requirement
software we are using statistics
requirement software reservoir under
sampling over sampling bayesian
statistics we are using like p of a by b
is equal to p of b by a into p of a by p
of b that is called bayesian statistics
measuring belief for
confidence problem we are using
covariance and correlation how data is
related
and these are the statistics formulas we
are using in data science mean variance
median mode etc i think everybody knows
then statistics for data science we are
generating a graph like a
sine wave bar chart line chart okay and
arrow mark chart pie chart etc
so it is used to process the complex
problems in the real world so that data
scientist analyst can look for
meaningful trends and changes in data
science
so it is used to derive
meaningful insights from data by
performing mathematical computations on
its so these are the basic statistics
formulas we are using
population measures like mean variance
standard deviation
sampling we are using sample mean sample
variance standard deviation just
score
correlation
in linear regression we are using
formula
probability
like uh binomial distribution one sample
jet statistics
like confidence interval formula margin
of error formula minimum sampling
one sample t
t statistic two sample t statistics
sample proportions two sample
proportions i square statistic etc
so there are eight basic statistics you
are using concepts in order to become a
data scientist
understand the types of analytics
probability central tendency variability
relationship between variables and
probability distribution
hypothesis testing and regression
i am going to show you
our types of analytics like descriptive
diagnostic predictive prescriptive
analytics i am going to show you
practicals
then probability i think everybody knows
complement intersection union
then conditional probability independent
events how we are using mutually
exclusive events
base theorem just now i have explained
central tendency there we are
calculating mean median mode
skewness
kutosis
these are the things we are applying in
central tendency
variability we are using
range percentile quartile that is called
iqr again we are calculating percentiles
quartiles interquartile range
etc
then variance standard deviation these
are the popular formulas especially we
are using in a population sample etc
standard error we are using population
method sample
an estimate of the standard deviation of
the sampling distribution
relationship between variables like a
causality covariance correlation
so that is called relationship between
variables
and the probability distribution we are
using three methods like pmf pdf cdf
probability mass function
probability density function
cumulative density function etc
then a continuous probability
distribution we are using uniform
distribution
normal gaussian distribution
and poisson distribution
okay exponential distribution psi square
distribution why we are using psi square
distribution
the distribution of the sum of squared
standard normal deviates
for that purpose we are using psi square
distribution
discrete probability distribution
bernoulli distribution binomial
distribution poisson distribution
to increase the predict prediction
levels of the application these are the
formulas
and uh why we are why we are doing
testing in order to eliminate the bugs
in the runtime duration those are called
exceptions in order to solve exceptions
we are using hypothesis testing
like null hypothesis alternative
hypothesis etc we are using
interpretation purpose p value critical
value significance levels and rejection
region we are using
z test i think everybody knows why we
are using desktops whether it is working
for one sample z test or a two
proportion z test
okay
and t test also we are using in
statistical test if the population
variance is unknown and the sample size
is not large
n less than 30 then we are using t test
okay
and anova test i think everybody knows
anova
analysis of variance
anova is the way to find out if
experiment results are significant
one-way anova two-way anova we are using
very interesting uh technique in spss
tool
statistical package for social science
and sas tools we are using it in order
to perform anova testing
size square test
goodness of fit test sci square test for
independence etc
so psi square test checks whether or not
a model follows approximately
normality when we have a discrete set of
data points then we are using size by
test
okay regression there are three kinds of
regression techniques we are using
first one is a linear regression
linear regression is a approach to model
the relationship between dependent
variable and one independent variable
that is called y is equal to mx plus c
m is the slope and c is the constant and
there is a y is a dependent and x is the
exploratory variable
now see this is the linear regression
formula y is equal to
the dependent variables and independent
variables are prediction
and why interpret intercept
see now i am going to execute this
linear regression algorithm
we are using understand the model
description causality under
directionality
in step two we are using check the data
categorical data missing data and
outliers
step three we are using simple analysis
and step four multiple linear regression
check the model and the correct
variables
step 5 residual analysis step 6 in
interpretation of regression output
this is the way i am going to show you
right now practical
simple linear regression
if you
see the step by step by step how i have
executed in python programming
in import packages and classes
data to work with eventually do
appropriate transformations
create a regression model and fit it
with existing data
check the results of model fitting to
one to know the weather model is
satisfactory or not
so this is algorithm this is the python
code linear regression python code
now i am going to show you practicals
linear regression non-linear regression
and logistic regression please see my
screen
my screen is visible stuff please check
now all of you
yes sir
my screen is visible
yes i am
okay
so now i am going to execute
python3 in the command prompt i am
executing python3
see first linear
regression demo i am going to execute
now
if you want to
if you want to see the code
see i am going to show you
so in this python
is the command
and linear regression demo is the
dot py
now it is executing
it will generate graph
and mean intercept value
so what i have shown in the
[Music]
it is processing now
see now it is generated i have taken x
array this is the graph it is generated
now
now see all of you
this is the x value i have taken numpy
array i have taken 5 10
25 that see if you see the
weight weight i am showing
my screen is visible
hello sir my screen is visible
yeah now see i have imported numpy so
import numpy is a package as np
and from
sklearn.linear underscore model import
linear regression what i have taken
import matplotlib in order to generate
the graphs
so i am taking a without reshape is a
variable
numpy array i have taken f 5 15 25 35 45
55 print
before reshaping numpy array
then
x is equal to np means a numpy package
variable np dot array reshape minus 1
comma 1 means columns becomes rows rows
becomes columns with this function
after this reshaping numpy array then
i am using on printing those values then
applying the model
so in this model i am using a linear
regression
inbuilt
inbuilt
package code available in a
numpy packet from scalar from sklearn
the linear regression model is available
i am applying that function model.fit of
x comma y
then i am calculating mean square error
then printf quotient of determination
then intercept value then slope value
then testing by prediction value i'm
predicting the
values
then show this is the values i have
generated now see once again
this is the way i am executed
okay
now my screen is visible all of you if
you want to see my code notepad
okay
this is the code i have written see this
is the in python code
okay same code i have written and
executed
now if you see
nonlinear regression
so this is a
linear regression
again i am executing a non-linear
regression
see
non-linear dot py
this is the graph i have generated
okay
so this is the graph
and these are the values
so now if you see in this one
non-linear regression multiple i mean y
is equal to
b naught plus b 1 f b 1 x x 1 b 2 x
b 2 x 2 etc we are calculating y
intercept values slope coefficient error
term
etc non-linear regression
regression trees and random forest which
are three based nonlinear algorithms we
are using under a non-linear regression
this is the code i have used nonlinear
regression in python code
okay these are the results i got it
i am showing through my presentation
again i am going to execute in a command
prompt one more logistic regression
logistic regression is a statistical
model that in its basic form uses a
logistic function to model a binary
dependent variable
although many more complex extension
exists
okay so this is the formula we have we
have used in logistic
the difference between linear regression
logistic regression
okay
and logistic regression algorithm it
derives 0 r1 finally this is the code i
have written
this is the values i am getting this is
the graph i have generated now see
again i am going to execute
logistic regression python
python
and
this is a logistic dot py
so i am going to share with you
all sort of uh
programs
and code applications everything i am
going to share with you
okay so these are the results i have
generated
now uh now i am moving to
artificial intelligence algorithms i am
going to execute three more algorithms
then data mining algorithms i am going
to show you
okay
so now
now see
what are the top 10 statistical
techniques
linear regression resampling methods
shrinkage dimension reduction
classification
non-linear models unsupervised learning
free based methods support vector
machine what is the difference between
data scientist data engineer
statistician so data scientist is using
python spss julia spark r matlab etc
tableau data engineer is using mapreduce
power cloud error database hbs
cassandra sql database pentaho
then statistician especially is using
our matlab excel wolf from whole from
alpha etc
the difference between a statistician
data scientist data is given in text
files need to get data for databases
focus on modeling focus on results bring
data to model bring model to data
further away
from production system embedded in
production system
so these are the various features
between a data scientist and data
analyst background a data scientist
deals with various data operations
data analyst
his role is related to data cleaning
transforming generating etc
scope
involved with several underlying data
procedures involvement is limited to
small data and statistic inferences
type of data it handles structure data
and structured data data analysis deals
with the structured data only skills
processes knowledge of mathematics
statistics and machine learning
algorithms
as problem solving skills knowledge of
basic basic statistics
tools like proficient in sas python or
tensorflow hadoop spark
sql or tableau etc
so this is the work each
uh
each person responsibilities like data
scientist is performing on data analysis
statistics machine learning data mining
statistical modeling research algorithms
analytics programming data engineer
works on data warehousing etl tool i
think everybody knows extra action
transformation loading tool informatica
hive
databases business intelligence
and data scientist uses matlab sas
software analysis
statistics
data engineers use oracle hadoop
microsoft sql server mysql high
languages like
r python latex type c plus plus data
engineers java java linux etc
data analyst versus data scientist
less pay less experience
data scientists more pay more experience
and statistic modeling predictive
modeling basic programming advanced
programming we are using in matlab you
know
keras
tensorflow sk learn pytorch these are
the
various frameworks i have used for a
prediction detection of health database
applications through recently six
database applications i have executed
so then support vector machine
i am going to explain this svm and uh
principal component analysis
and decision tree algorithms in a
in data mining
so okay
now the second concept is artificial
intelligence
intelligence behavior in a machine is
called artificial intelligence
the science and engineering to make
intelligent machines
it is proposed by john mccarthy
in the year 1969
intelligence is nothing but the ability
to reason the ability to solve novel
problems the ability to act rationally
and efficiently
the ability to act like a humans
okay in artificial intelligence mimic
like human
okay think like humans
so intelligence is demonstrated by
machines unlike the natural intelligence
displayed by humans and animals which
involves
consciousness and emotionality
see a is a popular branch of computer
science that concerns with the building
intelligent smart machines capable of
performing intelligent tasks with rapid
advancements in deep learning and
machine learning the tech industry is
transforming radically
so this is a difference between deep
learning machine learning artificial
intelligence
technique to perform machine learning
inspired by a human brain's
network of reasons that's why deep
learning is a sub branch of machine
learning
machine learning is nothing but learning
from the observational data
and with help of supervised learning and
supervised learning under
reinforcement learning artificial
intelligence means
so imitating the intelligence or
behavioral patterns of humans or other
living entities
so where
ai is used
predictive analytics real-time
operations management customer services
risk management and analytics customer
insights pricing and promotion customer
experience supply chain human resources
fraud detection
knowledge creation research and
development etc
the five most important concepts of ai
machine learning learning from the
observational data is called machine
learning
learning from the actionable data with a
deep manner we are using deep learning
we are using uh
neural networks like artificial neural
networks convolutional neural networks
deep
convolution neural network current
neural networks collaborative filtering
especially we are using the commercial
websites like amazon flipkart etc there
we are using recommender engine then
finally predictive intelligence
artificial intelligence playing a vital
role in this data science we are
applying top 10 data science algorithms
in this data science so what are the
features of ai we are using deep
learning facial recognition automatos
data ingestion cloud computing
quantum computing chart boards etc
okay so data mining deals with the raw
data collection data processing data
cleansing exploratory data analysis
model novel
modeling algorithms communicate visually
reports if you see the green green
rectangle box we are applying model and
algorithms and
neural networks data production
intelligent agents if you see the yellow
box
it includes machine learning if you see
the only blue rectangle box neural
networks data production intelligence
agents that is called deep learning
so
these these are the
uh ai and ml components like
development tools they are using ah
python jupiter etc languages like
shisha power python julia java etc data
platforms we are using spark hadoop sql
server postgresql etc machine learning
ai tools like h2o dot ai
and
tm lc xz boost
and the data exploration visualization
tools excel sql server
weka is a data mining tool vicar to
environment knowledge for analysis two
faculty members invented weka tool from
icar to university which is located at
new zealand
and the data ingestion tools like sql
server
okay
and
so
so again
one second
so these are the ai ml components
so what are the pros and cons of
artificial intelligence
increased efficiency in
artificial intelligence reducing human
risk aid in decision making availability
and
disadvantages like cause of unemployment
high initial investment dependency on
machines lack of creativity etc
these are the top 10 most popular ai
models linear regression logistic
regression linear discriminant analysis
stationary name base k nearest neighbor
linear vector quantization support
vector machine i am going to execute svm
and descentry
and few more algorithms tagging and
boosting deep learning etc
so these are the factors
differences between data science and
artificial intelligence
scope types of data tools applications
especially applications we are using
advertisement in data science marketing
internet search engines etc
and manufacturing
automation robotics transport healthcare
etc
scope it involves various underlying
data operations in data science limited
to the implementation of machine
learning algorithms types of data
structure data and section data can
handle in data science standardized in
the form of embedding and vectors
tools data science are python science
spss tensorflow keras psychic learn etc
in artificial intelligence we are using
scikit-learn cafe pytas
next third subject is data mining data
mining is the process of extracting
meaningful interested patterns from the
large amount of data
so
there are four stages data sources data
exploration data modeling model
deployment
see nasty is the mother of invention han
and cumber if you read it data mining
and data warehousing test book two
authors they have done 10 years research
on a
frequent pattern mining concept they
invented a very surprising items 10
years legacy data they have collected in
the u.s walmarts
and they kept in the data warehouse
then they applied the prediction
algorithms on the database
surprisingly they got two interest items
usually u.s employees there they are
outing every saturday and sunday with
families
so this hannah and cumber done very good
research on that
10 years database of supermarket and
walmart's data sets
they have taken as input and applied
prediction algorithm surprisingly they
got two items those are the
diaper and beer
so these two items are heavily sold out
in their markets
so they have done
very good research
in this
okay
so in this
nesting is the mother of invention they
have given the this dialogue
okay
and
surprisingly they got
two items heavily sold out those are the
diaper render beer
because all employees of usa they are
outing every saturday and sunday
so 10 years data they have collected
they have applied prediction algorithms
surprisingly they got two items are
heavily sold out those are the diaper
and beer
so that is that's why we can call it as
a hidden patterns we are extracting from
the huge volume of data what is the
difference between data science and data
mining so data mining is a sub branch of
data science you can call it as
qualitative analysis multidisciplinary
unstructured data data products data
mining we are using extracting data
discovering hidden patterns developing
predictive models etc
then these are the top 10 algorithms we
are using c 4.5 that means
classification 4.5 name based
classification caught what is the cost
expansion class
and the classification and regression
tree page rank algorithm support vector
machine adapter ir algorithm fp growth
especially in frequent pattern mining
k-means algorithm etcetera these are the
top 10 uh top 10 data mining algorithms
and data mining softwares wake up rapid
miner 9 mahout rattle our programming rn
general
adap boost page rank expectation and
maximization of priority support vector
machines k means c 4.5 class
classification algorithms etc this is
the comparison between data collection
data mining data compression
experimentation if you see the
process parameters
so microstructure properties performance
in data collection data mining process
modeling micro scale simulation micro
six cell simulation multiscale analysis
performance prediction data
communication reduce order modeling
stochastics pattern recognition pca
principle component analysis
uncertainty
so these are the data analysis analyst
skills like data visualization data
cleansing matlab or python
skills machine learning linear algebra
and calculus strong microsoft excel
skills critical thinking communication
etc and this is data mining data science
analytics
i have explained already all the terms
jargon my dear participants
please
post your queries in the chat box at
last 15 minutes i am going to clarify
your doubts
so in data mining is the
process of extracting meaningful
interested analytics
these are the terms we are using data
warehousing data engineering data
profiling data translation data mining
data discovery text mining machine
learning computer science etc data
science
science are scientists information
science statisticality
advanced analytics etc in analytics
machine translation speech recognition
robotics search engines etc
so the fourth subject is machine
learning learning from the observational
data is called machine learning machine
learning is a sub branch of artificial
intelligence
so it provides more
alternatives to analyze fast volumes of
data
by developing
fast and efficient algorithms and
data-driven models real-time processing
of data
we are using machine learning algorithms
so ordinary system how introduction to
machine learning ordinary system with a
test cutoff system with ai we are using
some algorithms machine learning we are
applying supervised unsupervised
reinforcement algorithms in order to
develop a model that model will predict
the results and
it improves the accuracy
then machine learning there are three
types i have already explained
supervised learning
so supervised learning
class we are categorizing the data that
is mean task driven system
and unsupervised learning
in unsupervised learning we are using a
data driven system
so
in this unsupervised learning we are
using k-means clustering
some clustering algorithms we are using
in this
unsupervised learning in classification
supervised learning we are using
decision tree support vector machine
and
patientry neural networks etc
under reinforcement learning
based on the punishment and reward we
are solving the
various gaming application especially
nowadays our students are playing pubg
free fire
where is chess games
various games they are playing
because of reinforcement learning
punishment and reward concepts
recovering from the mistakes
error handling between reinforcement
learning algorithm
the screen is you're solving various
applications
yes sir one second i am reconnecting
again
so my dear participants if you are
having any queries please feel free to
interact with me
post your queries any doubt
related to statistics artificial
intelligence machine learning deep
learning
data mining please ask me any doubt with
practicals especially in advanced java
programming
and python programming any library any
framework you can ask me i am going to
clarify your doubts
ok
sir please
participants in sharing for me
you allow me
one minute
i have joined with the doctor yes
one more email ladies and gentlemen
disabled
no sir it is already enabled only
from my side
so can you re-login again sir with that
yeah yeah
please allow me sir
itself
[Music]
main learning algorithm we are using
so supervised learning based on the
label we are classifying the data
unsupervised learning there is no label
based on the similarity behavior we are
classifying the data
based on euclidean distance manhattan
distance minkowski distance we are using
unsupervised learning with help of
k-means algorithm
k-means clustering algorithm etc
reinforcement learning is nothing but
based on the punishment and reward we
are solving
that is called
learning from errors
works on interacting with the
environment whereas the supervised
learning methods
works on given sample data
experimentation
then types of machine learning just now
has planned supervised learning task
driven unsupervised learning data data
driven reinforcement learning from
errors learning from the errors
so if you see the big data sets we are
learning and training the model big data
models we are using
and inferencing we are asking the query
it has to give the answer
and if it is not giving then again we
are training the model and expecting the
very good results
this is the machine learning life cycle
data gathering
data preparation date of wrangling i
think everybody knows data wrangling is
nothing but reprocessing the data
eliminating the noisy data redundant
data inconsistent data dirty data
then we are applying the data analysis
then we are developing a model training
model testing eliminate the exceptions
in the runtime duration
then deployment etc
then how does this machine learning
works
input we are giving input three kinds of
data structure data instruction data
semi-structured data to the system
analyze data find patterns prediction
store the feedback
then machine learning example
self-driving cars these days we are
using self-driving cars especially
google started this r d
research and development
okay and predicting an illness credit
worthiness ranking on social media
computer vision agriculture targeted
emails and the quality and assurance
based platforms fashion industry etc
uh i am going to explain how
reinforcement learning the algorithm it
works please check now all of you
left side fire is their right side
please analyze this scenario my dear
students
my dear faculty members please analyze
this scenario you come to know how
reinforcement learning is working
so one side fire is there and another
side bucket is there my chat bot at robo
is observing two sides
so wrongly it stepped into fire
so
that means there is a fire it is a heart
so i am detecting 50 points in that one
then again
that chatbot or robo analyzing oh there
is a fire and there is a right side
bucket with full of water next time
is taking a full of bucket of water and
pouring into the fire that means if if
it does a bad step
we are giving punishment to the robot
that is called punishing and if it does
right step we are giving the reward
that's why reinforcement learning is
called a punishment and reward concept
my dear friends learning from the errors
learning from the mistakes is called
reinforcement learning
understood all of you
types of machine learning we are
discussing supervised learning what are
the techniques
classification regression in
classification we are using fraud
detection email spam detection
especially in gmail
diagnostic image classification
regression logistic regression linear
regression and non-linear regression and
risk assessment score prediction etc in
unsupervised learning dimensionality
reduction text mining face recognition
big data visualization image recognition
etc
in clustering we are using biology city
planning targeted marketing
reinforcement learning especially gaming
applications we are developing with help
of
reinforcement learning my dear
students
concentrate concentrate on reinforcement
learning because you are playing so many
games these days in pandomic period you
are playing so many games
playing game is not important
how to develop a game is important as
engineering students
you must learn reinforcement learning if
you are having any doubts please ask me
how to develop a game
how to develop a chatbot application etc
okay we are using in reinforcement
learning gaming finance or manufacturing
inventory management robot navigation
etc
these are the various algorithms we are
using regression non-linear polynomial
decision random forest classification
k n trees logistic regression name base
svm
clustering svd pca k-means
okay
fp growth
various algorithms we are using i have
explained
in unsupervised learning recommender
engine targeted marketing customer
segmentation etc in reinforcement
learning
we are using real time decision game
artificial intelligence skill
acquisition learning task robot
navigation etc in supervisor learning
again two types classification
regression in regression i am using a
popular population growth prediction
estimating life expect expectancy market
forecasting
and weather forecasting advertisement
popularity prediction classification
identity identify fraud detection
diagnostic etc so these are the machine
learning tools we are using
statistical platforms pandas exploratory
data analysis predictive model markup
language machine learning model builders
portable format for analytics
database special unstructured data
scriptable databases hadoop map reduce
for programming on java
spark especially in scala python or java
it supports
storm real-time analytics
fpga
and
asics we are using gpus and crew
password we are using
in machine learning softwares we are
using veka
apache spark rapid miner
hto shogun 9
and apache mahogany
advantages and disadvantages
easily identify strengths and patterns
in machine learning no human
intervention is needed in machine
learning
continuous improvement in machine
learning
wide applications we are using handling
multi-dimensional and multi-variety data
disadvantages
high error rate and susceptibility data
acquisition time and resources
interpretation of results etc
applications of machine learning
especially check my dear friends
we are using image and speech
recognition application medical
diagnosis statistical arbitrage learning
associations classification prediction
extraction regression various
applications we are solving under
machine learning
these are the applications of machine
learning
like
automatic language trans translation
medical diagnosis stock market trading
online fraud detection virtual personal
assistance email spamming and malware
filtering self-driving costs product
traffic prediction speech recognition
image recognition automatic language
translation etc
these are the formulas i'm using the
main formulas for machine learning
especially naive bayes k nearest
neighbor
perceptron neural networks back
propagation guidelines
and linear regression principle
component analysis logistic regression
etc
see now i'm going to show you practicals
on
okay
i am going to show you practicals
decision
svm sir my screen is visible
sorry especially my screen is visible
hello sir
my screen is visible please respect
so i am going to explain
now i am going to explain support vector
machine svm okay
and
see
so this is a python support vector
machine algorithm it is going to
show you
analysis kernel of rbf and kernel of rbm
and
so again it is going to show you
so these are the kernel dot py linear
dot py solver these are the we have
developed already in this
sk learn algorithm
and
these are the
support vector machine
algorithm decision tree now i am going
to execute decision tree
now i am going to execute decision tree
[Music]
so decision tree algorithm now i am
executing please observe all of you
decision tree dot py my file name is
decision tree dot
python
decision tree
so here i am calculating guinea index
gain ratio and how the node is at
deciding the
value and i'm generating confusion
matrix and i'm generating the age fair
and this seven seven one four rows and
three columns i have taken i am
calculating gain impurity
and especially
so this is about decision tree algorithm
execution i am going to share with you
these algorithms i have executed nearly
15 algorithms i have executed
those
practical algorithms and code i am going
to share with with the coordinator and
coordinator going to share with you my
dear friends
see now
deep learning next concept the last
concept is deep learning
what is mean by deep learning learning
from the action-enabled data
with the deep observation
n number of
hidden layers and
input layers we are using deep learning
gets named from the fact that it
involves going deep into several layers
of
network which also includes hidden
layers it is a sub branch of machine
learning machine learning is a sub
branch of artificial intelligence
okay i think everybody knows
what is deep learning learning from the
observational data
learning from the actionable data
with a deep manner and see example what
is deep learning here i'm giving input
to the system car it has to identify car
or not
so i'm doing feature extraction means
what is that type of the model
model of the car car color price
various features
right side driving left side driving
what are the special features in this
car first i am extracting the features
based on that feature so i am doing the
classification algorithm finally it will
detect whether it is car or not
so deep learning is a subset of machine
learning machine learning is a subset of
ai
ai is nothing but learning from the
action learning from the i mean
intelligent behavior in a machine that
is called ai
in the year 1950s artificial
intelligence came
john mccarthy invented the term
ai in the year 1969
then machine learning ability to learn
without being explicitly programmed that
is called machine learning learning from
the observational data
ability then
deeply in the year 1990s and up to 2006
still it is leading deep learning 2010
until date learning from the
actionable data with the deep manner
learning based on deep neural networks
who invented deep learning
alexi van ken who is a russia faculty
member
he created small functional news which
is considered to the first serious deep
learning back
uh learning
breakthrough
so
deep learning is an
ai function that mimics the working of
the human brain in processing data for
use in detecting objects recognizing
speech translating languages and making
decisions
that's why using multi-layered networks
for machine learning we are using
dna deep neural networks convolutional
neural networks recurrent neural
networks
in machine learning learning from the
observational data training missions to
get better at a task without explicit
programming
artificial intelligence enabling
machines to think like humans
and why deep learning deep learning
requires larger training data sets
amount of data i am taking x-axis and
performance
wherever huge volume of data is there
please apply deep learning techniques
then only it gives very good accuracy
okay
so this is the reinforcement learning
concept i have already explained how
it is that the child is learning from
the mistakes
so a lot reinforcement learning is
nothing but learning from the errors
learning from the mistakes
okay
this example i have already explained
so deep learning modeling like
data and labels and reference models
create update models train
test models evaluate model server model
etc
so deep learning pipeline data streaming
purpose we are using various
data tools like hdfs
cassandra
etc
even peak programming also we are using
under data streaming programming
language youtube
audio video text graphics
unstructured data it can handle very
easily pick programming language it is a
data flow language
then users data preparation analytics we
are using jupiter
apache japanese spark we are using
for especially frameworks and clustering
we are using pi torch tensorflow sklearn
and keras these are the deep learning
and the frameworks we are using models
for building machine learning models
very deep learning
high number of hidden layers and input
layers we are using model serving we are
using
predicted models
then these are the
tools we are using
in this
programming machine learning programming
language especially nowadays popular
python and r python is so popular
because of rich number of libraries are
presented machine learning tools like
weka sk learn tensorflow etc deep
learning tools especially keras pi touch
analytical tools like spark and loop
addition tools
see human intervention is presented in
machine learning there is no human
intervention in deep learning my dear
participants faculty members please
observe this diagram it gives more
knowledge more inference you can extract
from this image see the difference
between machine learning and deep
learning
input i am giving same but final output
also i am getting same whether it is car
or not but uh where the intervention is
presented mission learning there is no
human intervention is presented in the
deep learning feature extraction then
click final output here
no see here i'm using hidden neurons
feature n number of there is no
restriction you can use enough layers
and input layers
this is the difference between deep
learning and machine learning data if
you take data is a
data parameter needs big data set for
deep learning performs well with a small
data to medium data set hardware
requirements it requires
this with gpu in deep learning
graphical processing units machine
learning you can use cpu
desktop laptop etc
very big application go for high
computing servers
then engineer
particularities needs to understand the
basic functionality of the data
understands the features
and how they represent the data training
data it takes long time in deep learning
and machine learning takes short time
processing time a few
weeks a few seconds or hours
number of algorithms few
difficult
some machine learning algorithms are
easy to interpret
so this is deep learning process i am
taking input layers hidden layers
l2 l3 l4 so many hidden layers we are
using in order to increase the
performance levels deep learning process
i'm using in the year 1986 came actually
john mccarthy and ivanka ivankomenko
introduced a learning deep learning
concept
then it identifies pixel values identify
edges identify combination in order to
find out the face recognition
application
identify features then finally we are
getting the output
these are the popular algorithms
dnm cnn rnn
dnn stands for deep neural networks
providing lift for classification and a
forecasting models
then cnn especially for image
classification we are using cnn
feature extraction and classification of
images
and rn and recurrent neural networks
for sequence of events language models
time series etc we are using rnn deep
learning models we are classifying
supervised and unsupervised models
cnn classic neural networks secret
neural networks unsupervised
self-organizing maps boltzmann machines
auto encoders especially we are using
so dnn cnn rnn can perform
all these tasks like prediction time
series analytics
and video analytics audio analytics and
traffic data analytics stock market data
analytics bombay stock market analytics
sensex data analytics weather
forecasting data analytics
computer
recognition applications classification
we are using these three techniques deep
learning
methods classified into four types
in cnn we are using
vgg mod i have used vcc model
and
resnet google net i have used for my
research work
and rbm based methods we are using these
belief networks the boltzmann machines
deep energy models
auto encoder based methods we are using
sparse encoder d denoise
these are the formulas
[Music]
etc
these are the top 10 algorithms i have
used for my research myself and my
friend turkish were barbara we have
executed six database applications in
order to predict and deduction of corona
virus image chest image data sets corona
strength chest image datasets we have
taken 960 images we have collected from
the various hospitals and we have
applied a tensorflow
and
microsoft cognico toolkit cafe if you
are perfect in java programming go for
deep learning 4j chainer method keras
these are the various open source tools
okay and python is integrated with so
many frameworks if you see this diagram
it is integrated with the keras
spark flow tensorflow
theano torch lua pytard fast dot ai
and shisha paddle
and d lib c plus plus c maxnet is
connected to perl java or julia closure
and various
tools are integrated with python
python is so popular because of rich
number of libraries are
incorporated thanks to
rossum he invented the python
programming language
and what are the major applications of
python i mean deep learning especially
instant visual translation applications
predict predicting the future chatbot
medical self-driving cars
read
read lip movements deep learning
and photo description colorizing the
images medical care
predicting the future advertising so
many
applications are there
five application areas
are presented fraud and risk detection
image speech recognition health care
analytics augmented reality airline
route planning
these are the these are the top 10
fascinating applications of deep
learning self-driving cars especially
deep dreaming automatic mission
translation
news aggregation sound addition to
silent films
demographic prediction pixel restoration
automatic handwritten generation
detection of growth delays in children
these are the top 10 fascinating
applications of deep learning my dear
friends keep in mind and ask doubts at
the last
okay
and these are the data science
application especially e-commerce like
amazon flipkart these are the
e-commerce websites are
getting so much benefit with help of
deep learning
algorithms e-commerce manufacturing
banking
health care transport finance etc these
are the real-time applications what
sells most ice creams learning to speak
recognizing defects using suggestions to
make better decisions
various fields wherever data is
presented wherever data complexity is
there we are applying deep learning
algorithms
so
data science is a combination of all
sort of things you require ide you can
use pycharm jupiter calibratory spider r
studio etc web scraping we can use
beautiful soup scrapey url deploy aws
cloud we are using amazon amazon is so
popular under infrastructure as a
service cloud
jeff bezos is the ceo of amazon company
there are two big chains clouds ec2 and
s3 under the amazon web services
then we are using mathematics statistics
linear algebra differential calculus
data analysis
and machine learning programming
language like python or java etc
so these are the tools we i have
explained many times deployment tools
data ingestion tools languages data
platforms deep learning virtual machines
ml and ai tools etc
so python or
sql says
sas is nothing but software analysis
software i mean statistical analysis
software sas very popular
sas tool spss first in order to generate
the statistics we are using spss excel
aws power bi spark ml perl bash
various tools
changes sentiment analysis
elections
my dear friends can anybody knows
how joe biden won the elections
behind there are data scientist fighters
especially they sit together e-voting
system sms messages every channel they
have utilized and collected the database
and stored in the system
and the
action algorithms which are joe biden
lagging in that state
free of current rice
everything all the amenities facilities
he has been provided to the all the
public where he is lagging so
definitely
those states is getting
some votes that's why joe biden won
behind the data analytics played by the
vital role
has done tremendous work in order to
wonder
so that's why that much importance is
there for deep learning data
ridiculing sentimental mining behavioral
mining you can do fantastic application
how we can capture and deliver etc these
are the challenges of
data science according to the gartner
survey he is going to reveal every week
every month
hardware is the number one in the market
so dirty data he found that it is a
major challenge in data science
especially okay
and
these are the skills you required a
special id skills business skills
analytical skills data science i have
explained already
according to gartner survey he has been
given beautiful data flow diagram in
order to solve the real-time
applications we are following gartner
survey
so
as a beginner my dear students and
faculty members you may get doubt which
is a better to start ai ml and dl you
may get the doubt for that doubt i am
giving the answer if you are a beginner
just to use artificial intelligence any
technique which enables computers to
mimic human behavior so better to start
as a beginner artificial intelligence
learn the expert system fuzzy logic soft
computing techniques etc then you learn
machine learning it is a subset of ai
techniques
which use a statistical method to enable
machines to improve the experience
then you go for deep learning
subset of machine learning which make
the compute computation of multi-layer
neural networks feasible etc
so these are the six database
applications myself and my friend
tarques for barbara in data science
research laboratory we have executed
tremendous applications kovit 19 virus
detection application
we have executed kovit 19 co stands for
corona why stands for virus i think my
dear students virus can anybody expand
virus vital information resource under
seize
that is called virus there are many
viruses are coming these days
d for disease
so
so it came from source convolutional
visualize
covariance 2 i mean
covet virus severe acute respiratory
syndrome these are the symptoms we have
extracted from the various databases
fever cough chills etc these are the
x-ray images i have collected
and i have applied deep learning
algorithms like recurrent neural
networks convolutional neural networks
these are the steps i have followed
like a convolution operation it refers
to the combination of two functions to
produce a third function
then relu rectified linear function in
order to improve the performance i have
used pooling function
where i am integrating there are two
types of max pooling and minimum pulling
then flattening full connection these
are the algorithms cnn i have used
then
this is the
example i have taken of pneumonia normal
kovid19 image as an input system i have
applied cnn and resnet frameworks i have
used and 10 by 10 by 2048 image pixels
i've extracted then
concatenated those features
then i'm applying a flatter features
then i'm dropout function in order to
increase the performance i'm using
dropout function in a convolutional
neural networks and recurrent neural
networks this is the final output i am
getting normal image covet 19 pneumonia
this is the training data set i have
taken and this is algorithm i have
applied in python algorithm then this is
the code i am going to share with you
my dear friends
i'm going to share with you this code i
have used
i'm using keras library and
that is a very fantastic
deep learning
framework in especially in python and
then sequential function i'm using
dropout activation function flat and
dense
then a training augmentation
configuration then testing augmentation
then
training augmentation i mean generates
uh
batch batches of augmented image data
this is the corona test data set
then this is the chest x-ray of covet 19
virus and severity levels of the image
this is the results i got it
86.7
then this is the execution flow this is
the data loss versus accuracy execution
time between covid19 virus data set
versus number of
processors i have used four processors
see my system it stuck one time
this is the execution flow while running
my application deep learning my system
got dead slow by the time
while executing then covet 19 data size
versus accuracy this is code 19 strain
so this is one more database application
i have executed same as covid19 but a
few symptoms are differ like sore throat
and
and congestivity is a rash on the skin
and diarrhea discoloration of fingers or
toes headache etc
and this is a train input strain input
data set to the cnn model this is a
tight training this is the code rnn i
have used rnn especially training
parameters network parameters graph
input define weights
these are the functions i have applied
in this finally i got the x-ray images
x-ray covid19 strain virus and severity
levels then prostate cancer have
executed one more database application
these are the symptoms of prostate
cancer i have collected various
x-ray images from the kaggle uci google
database microsoft database i have
collected the various x-ray images this
is a training data set and this is a
testing data set i have tested with
external image it gives the prostate
cancer versus normal image then breast
cancer same as i have collected
breast x-ray images these are the
symptoms of breast
these are the
x-ray images i have taken and this is a
training data set this is the final
output i got
x-ray of breast cancer versus normal
image then lung cancer i have collected
again 960 x-ray chest images these are
these are the symptoms
and these are the x-ray images i have
collected and i have applied cnn model
and a training data set and finally i
got the output x-ray of lung cancer and
normal image and the last database
application is ebola virus prediction
detection here also i am you i have used
cnn and rnn model these are the symptoms
and 90 percent of infected people die if
one's ebola virus attack and these are
the symptoms like headache
and vomiting abdominal pain diarrhea
muscle pain etc
these are the training data set this is
the code i have used training parameters
network parameters and define weights
this is the code i have executed final
output i got it
so this is this is the conclusion of my
today's international webinar on machine
learning i mean data science application
i have executed 960 images
we have found 149
images are positive 234 phenomena i mean
pneumonia positive 250 normal under
diagnotic we got uh fantastic results
89.62 and 91.54
for sensitive sensitivity for 19 classes
thank you sir for giving me this
opportunity
these are my
webinars and future
i'm going to conduct a few more
countries webinars this is at
thailand
and
this is at malaysia
this is at singapore and this is a as a
keynote speaker at github
and this is a resource person that meets
madanapali and this is resource person
at sri vijayana ketan engineering
college this is jury member
at the madana parliaments
and this is keynote speaker
and this is the doctorate from jain to
ananta poor bharata vidya
from delhi in the year 2018
and this is a best it teacher of the
award itap best it teacher award at
hyderabad 2020
and
this is best local chapter award from
iit
madras
in the 2019
this is international seminar at salem
this is sangam university at rajasthan
i have given the talk this is a abs
college at gazia bar
this is at jaya engineering college
tamil nadu
and this is a webinar on maharshi
markandeya engineering college punjab
and data science
talks at blue crest university this is
big data analytics using spark at woman
saudi arabia i have given
and this is a data science application
hands-on
and and this is at russia i have
delivered a talk
with 19 strain virus detection
and this is a today's big data symposium
at
blue crest university this is from
russia
i got the
young scientist award from that russia
university
and this is a today's
symposium on data science application
this is at bhartiya university tamil
nadu
and this is lord lords institute of
technology hyderabad i.t career and
opportunities
this is a data science using our
programming
and this is at riva university bangalore
i have acted as an international panel
member and i delivered a talk this is a
karpaga vinayaka college of engineering
at tamil nadu
and this is at viktor bharati
hyderabad
topic is implementation of data science
application using machine learning
algorithms this is a research conclave i
have conducted and i have invited 49
universities and
we have a conducted research conclave
this is a krishna university
data science application using machine
learning deep learning techniques
and this is at kl university
i have conducted a two days on robotic
process automation tools ui path and
blue prism these are the popular tools
under
automation robotic process automation i
have given a two days with practical
sessions
then