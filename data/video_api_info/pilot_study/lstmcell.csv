Block Label,Cluster Label,sentences to be labeled,video title,video link,video description
,,10 : And I wanted to explain this again for  everyone.,L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"11 : And spoiler, there's also an LSTM cell class that I  also wanted to explain, especially the difference  between the LSTM cell and the LSTM class in pytorch.",L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"12 : But yeah,  one thing at a time.",L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,16 : So there are three  values.,L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"17 : If we look up the values here in the usage, it's the  input size, the number of expected features, essentially  the hidden size, and the number of layers.",L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"18 : So usually, well,  not usually, but previously, we used only one hidden layer.",L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,52 : And this  is our input here.,L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"53 : Now, the LSTM cell class is, it's kind of like  part of the LSTM, it's a smaller unit, like LSTM cell is, is  only a small unit.",L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,54 : And we can actually use both on either the  LSTM or the LSTM cell for implementing the character RNN.,L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"62 : And using the figure that I showed  you before here in the red box, this is essentially what the  essentially what represents the LSTM cell.",L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"63 : So it only receives  one character as the input, then for the one layer of the hidden,  the initial hidden and cell state, and it only outputs one  output here, and then the hidden and cell set for the next state.",L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"64 : So why that is more useful is in a way for computing the loss  essentially that you can get one thing at a time, essentially  instead of running the whole thing.",L19.2.1 Implementing a Character RNN in PyTorch (Concepts),https://www.youtube.com/watch?v=PFcWQkGP4lU,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/tL5puCeDr-o

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,new video
,,"0 :  Alright, let's now take a look at a code implementation  regarding the character RNN that we talked about in the previous  video.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"1 : So in the previous video, I gave you some overview of the  LSTM and the LSTM cell.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,2 : And I prepared two notebooks.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"10 : Then we will go on here again, it will receive receive the  hidden state and the cell state from the previous time step, the  current time step input, output something and so forth.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,11 : So  that's how the LSTM cell class works.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"12 : All right, but before we  get to this part, the LSTM cell class, let's start at the top.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"18 : So here, I didn't use any helper files, I  tried to keep everything in the notebook, because the code here  is relatively short and simple.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"19 : So here, I have some hyper  parameters like the text portion size.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,20 : So how long a typical text  portion is the number of iteration for iterations for  training.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"21 : So here, we don't use epochs, we just use iterations.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"22 : Learning rate, the size of the embedding and the size of the  hidden layer.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"23 : Okay, execute that.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"49 : So here, I have a function for getting a  random portion of the text of size, text length.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"50 : So we have  this text portion, sorry, my text portion size 200.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,51 : So it  gets text of the portion size 200 from the whole text here  randomly.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,52 : So this will be our training batch.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"80 : Okay, so here's now our RNN  implementation.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,81 : So I just have something to keep track of the  hidden size.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"82 : This is our embedding layer that goes from  the integer, the character integer to the embedding vector,  a real value vector of size, let me see of size 100.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,83 : And the  hidden dimension is 128.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"84 : Okay, so we have the embedding size,  and then the LSDM cell takes vectors of size 128, and has a  hidden size of one, sorry, of 100, and has a hidden size of  128.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"85 : So if I go back to my slides, maybe using this  representation here, so our text, we had here a one hot  encoding.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"90 : So in the forward pass, we first put the character  through the embedding.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"91 : So this will be accepting batch size  embedding dimensionality, we use only one character at a time.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"92 : So  it will be one times embedding dimensionality, which is in our  case, 100.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,114 : I think this should be on  definitely not this one should be the number of characters.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"115 : Oh,  but here I said hidden state output size would be the output  size fixes.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,116 : Okay.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"124 : Okay, and all right, then let's get started.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"125 : So actually, the output size, I should mention this is the same  size as the input size.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"126 : Yeah, so let's initialize the RNN.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"127 : So as  input size is the length of the number of characters, it's 100.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,128 : So the output size would be also 100.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"129 : In between, we have  the embedding and hidden dimensions.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"150 : So here, this is just  for making the dimensionalality right, because this is just one  single value.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"151 : But as you recall, we want one times sorry, batch  size times one.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,152 : So it should be a 2d tensor.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,153 : So we are doing  unsqueezed adding empty dimension.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"154 : We provide the hidden  from here, the cell state from here, these initial cell states.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"155 : So in the first round, these will be the initial ones.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,161 : So we  are computing the loss between the outputs and the targets one  at a time.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,162 : And then we just normalize by the text portion  size.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"163 : So we have, it's just the average the mean, mean over the  batch size, if you will, because we add, let's say 200 losses,  and then we divide by 200, just averaging that's just so that  it works better with a learning rate.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"185 : So it's just, it's just essentially  running in this case, through two of these, right, one, two,  and then we get to this part.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"186 : And for each, in the prediction  length, we are generating text of size 100.",L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,187 : We are just running  it as before.,L19.2.2 Implementing a Character RNN in PyTorch --Code Example,https://www.youtube.com/watch?v=tL5puCeDr-o,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L19_seq2seq_rnn-transformers__slides.pdf

Code: https://github.com/rasbt/stat453-deep-learning-ss21/tree/main/L19/character-rnn

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/mDZil99CtSU

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,new video
,,49 : And in a way this can  be achieved by keeping a hidden state for each of   our features.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,50 : Then we have an LSTM cell that  takes in the previous hidden state and the new   observation and action and spits out a new hidden  state.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,51 : And that is exactly what you see on the   screen.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,52 : Let me just stress that the LSTM cell has  learnable parameters which are shared among all   the features.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,"53 : Anyway, the K matrix is nothing else  than the output hidden state of our LSTM cell.",The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,54 : Here you can see the attention computation  together with the shapes of the matrices.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,61 : This attention computation will work out  for any number of features and the final latent   code size is dictated by the number of embeddings.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,62 : This means that our model will be able to accept   feature vectors of arbitrary size.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,"63 : Second of all,  if you disregard the fact that each feature could   have a different hidden state this entire pipeline  of going from the observation to the latent code   is permutation invariant.",The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,95 : The first one is   the number of embeddings which is  just the number of rows of our table.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,96 : Then the hidden size which is just the number  of columns.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,97 : What we return is a numpy array   that represents the positional encodings.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,112 : So the forward pass will accept two inputs.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,113 : The  first one is going to be the query tensor that has   the shape of number of embeddings times hidden  size and the second input is going to be the   key tensor of shape number of features times the  hidden size.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,114 : We then return the attention weights   that are of shape number of embeddings  times number of features and what's a   little unusual is that they won't sum up  to one in general because we are going to   use the hyperbolic tangent activation  rather than the softmax.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,124 : Then we provide more hyperparameters.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,"125 : Namely,   the projection dimension and the hidden size.",The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,"126 : Internally, we will have this hidden state   and note that it is going to be specific to the  LSTM which means that it's actually two tensors.",The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,131 : Here we instantiate the LSTMcell.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,132 : The reason why  the input size is 2 is because we will concatenate   one element of our observation vector (e.g.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,133 : the  position of the cart) with the previous action.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,143 : In case the hidden state was None or in  other words if this is the first time   we run the forward pass we populate  both of the hidden states by zeros.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,144 : Here we call our LSTM cell.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,145 : We create a variable for our query which  is nothing else than the internal buffer.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,325 : Also we   will implement a multi-processing logic and here  one can specify the number of jobs.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,326 : Here another   optimization related parameter which is the  population size and by default it is set to 256.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,327 : And basically dictates how big a possible solution  set is going to be at each iteration.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,334 : I  know that here we need to specifically   mention the number of features at training time.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,335 : Here we create a multi-layer perceptron with  a single hidden layer size with 16 nodes.,The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,"336 : Finally, the star of the show - the  PermutationInvariantSolution solution   with these hard-coded hyperparameters.",The Sensory Neuron as a Transformer in PyTorch,https://www.youtube.com/watch?v=mi_mzlhBGAU,"In this video, we implement a paper called ""The Sensory Neuron as a Transformer: Permutation-Invariant Neural Networks for Reinforcement Learning"" in PyTorch. It proposes a permutation invariant module called the Attention Neuron. Its goal is to independently process local information from the features and then combine the local knowledge into a global picture.

Paper: https://arxiv.org/abs/2109.02869
Official website with visualizations: https://attentionneuron.github.io/
Official code: https://github.com/google/brain-tokyo-workshop/tree/master/AttentionNeuron
Code from this video: https://github.com/jankrepl/mildlyoverfitted/tree/master/github_adventures/neuron

00:00 Intro + credits + disclaimer
01:10 Paper overview: Goals [slides]
02:10 Paper overview: Inputs and outputs [slides]
02:54 Paper overview: Pipeline [slides]
03:42 Paper overview: Attention inputs [slides]
05:22 Paper overview: Attention [slides]
06:16 Paper overview: Linear module [slides]
06:44 Multi-layer perceptron [code]
09:50 Positional encoding [code]
11:12 AttentionMatrix module [code]
13:23 AttentionNeuron module [code]
18:02 AttentionNeuron testing it out [code]
20:31 PermutationInvariantNetwork module [code]
22:10 Solution abstract class [code]
26:22 MLPSolution [code]
27:36 PermutationInvariantSolution [code]
29:33 Custom CartPoleSwing up [no code]
30:19 Task constructor [code]
32:44 Task helper methods [code]
34:06 Task rollout [code]
36:17 Covariance matrix adaptation [no code]
37:02 Training script helpers [code]
38:14 Training script CLI [code]
39:43 Training script main logic [code]
44:02 Launch multiple experiments [code]
45:10 Tensorboard results [plot]
46:26 Feature shuffling evaluation [plot]
48:34 Noise feature injection [plot]
49:25 Final thoughts + outro


If you have any video suggestions or you just wanna chat feel free to join the discord server: https://discord.gg/a8Va9tZsG5

Twitter: https://twitter.com/moverfitted

Credits logo animation
Title:  Conjungation · Author: Uncle Milk · Source: https://soundcloud.com/unclemilk · License: https://creativecommons.org/licenses/... · Download (9MB): https://auboutdufil.com/?id=600"
,,new video
,,27 : they actually fail to actually scale or try to reproduce these kinds of results because LSTM networks tries to map each word to each word.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,28 : So they will tend to have an equal sized input and an equal size output for mapping each of the input words to the output words.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,29 : But what will happen if we want to.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,35 : This can be GRU cells as well.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"36 : So in basic sense, these are generally comprised of bi-directional LSTM cells or bi-directional So generally we have the X as the input features and we have the hidden state from the previous timestamp of the previous cell.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"37 : So these can be any variants of far, just like I mentioned, either LSTM or GRUs, if we're considering LSTM, then each of the intermediate hidden layers, each of the intermediate hidden cells will have two outputs.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,38 : That is the H and the C that is the hidden cell state and the current cell state.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"39 : H & C, but if we were using  then as we know, In GRUs, the two hidden States that is H & C are merged together.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,42 : GRU cells.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"43 : The input gets passed one after the other, for each particular LSTM cell.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,44 : This is denoted by X one X two and X three.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"49 : And this is particularly adhering to one language, particularly one language, let's say English, what happens after this is the last LSTM cell or the last RNN sell it outputs certain values traditionally and LSTM outputs, three values.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,50 : So we have the hidden cell state.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,51 : We have the cell state that is C and we have the output value.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"60 : Now, this particular thing is known as the encoder factor.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,61 : That is the hidden cell output of the stack of LSTM cells.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,62 : And encoder cells is known as the input vector.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"69 : And also pass, there will be another input which is corresponding to the inputs of the different language that is Chinese in our case.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"70 : So the inputs of the Chinese language and the input vector, a hidden cell together that is output of the important cell together forms the input of the decoders and in decoders.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,71 : We have a similar architecture where we pass the hidden States for each of the individual LSTM or the GRU units or any RNN variant units of the decoder model.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,87 : weight vector for the input.,Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"88 : HD minus one is the previous timestamp hidden cell and WHS says corresponding wait, and F is generally 10 H if we have seen the previous videos LSTM, we generally use the teenage version and we get the hidden output.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,"89 : Now, this is all good.",Encoder-Decoder Sequence to Sequence(Seq2Seq) model explained by Abhilash | RNN | LSTM | Transformer,https://www.youtube.com/watch?v=kNhGWjjWQFk,"Connect and follow the speaker:
Abhilash Majumder - https://linktr.ee/abhilashmajumder 

A blog used in the video:
https://towardsdatascience.com/understanding-encoder-decoder-sequence-to-sequence-model-679e04af4346

Join and follow us on social media to get notified about our online and offline activities,
Website: https://colearninglounge.com
LinkedIn: https://www.linkedin.com/company/colearninglounge
Facebook: https://www.facebook.com/groups/colearninglounge
Github: https://github.com/colearninglounge/co-learning-lounge
Telegram: https://t.me/ColearningLounge_AIRoom
Instagram: https://www.instagram.com/colearninglounge
Twitter: https://twitter.com/ColearninLounge
Medium: https://medium.com/co-learning-lounge"
,,new video
,,"110 : We're gonna make two modifications, in fact.",NeurIPS 2020 Tutorial: Deep Implicit Layers,https://www.youtube.com/watch?v=MX1RJELWONc,"This is a video recording of our NeurIPS 2020 Tutorial - Deep Implicit Layers: Neural ODEs, Deep Equilibrium Models, and Beyond - by David Duvenaud, Zico Kolter, and Matt Johnson.  Full information about the tutorial, including extensive notes in Colab notebook form, are available on our website: https://implicit-layers-tutorial.org"
,,"111 : We're going to, first, at every step, rather than just adding a bias we're going to re inject the input, and actually you could add a bias too, you can always add an extra bias to the input, but for now, we'll think about just always adding the input again.",NeurIPS 2020 Tutorial: Deep Implicit Layers,https://www.youtube.com/watch?v=MX1RJELWONc,"This is a video recording of our NeurIPS 2020 Tutorial - Deep Implicit Layers: Neural ODEs, Deep Equilibrium Models, and Beyond - by David Duvenaud, Zico Kolter, and Matt Johnson.  Full information about the tutorial, including extensive notes in Colab notebook form, are available on our website: https://implicit-layers-tutorial.org"
,,"112 : In addition, we're also going to use the same weight at each layer of the network.",NeurIPS 2020 Tutorial: Deep Implicit Layers,https://www.youtube.com/watch?v=MX1RJELWONc,"This is a video recording of our NeurIPS 2020 Tutorial - Deep Implicit Layers: Neural ODEs, Deep Equilibrium Models, and Beyond - by David Duvenaud, Zico Kolter, and Matt Johnson.  Full information about the tutorial, including extensive notes in Colab notebook form, are available on our website: https://implicit-layers-tutorial.org"
,,new video
,,40 : So our computation graph will look like this.,How to Generate Music - Intro to Deep Learning #9,https://www.youtube.com/watch?v=4DMm5Lhey1U,"We're going to build a music generating neural network trained on jazz songs in Keras. I'll go over the history of algorithmic generation, then we'll walk step by step through the process of how LSTM networks help us generate music.

Coding Challenge for this video:
https://github.com/llSourcell/How-to-Generate-Music-Demo

Vishal's Winning Code:
https://github.com/erilyth/DeepLearning-SirajologyChallenges/tree/master/Art_Generation

Michael's Runner up code:
https://github.com/michalpelka/How-to-Generate-Art-Demo/blob/master/demo.ipynb

More Learning Resources:
https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714#.swstv6z61
http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html
https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/
http://deeplearning.net/tutorial/rnnrbm.html
https://maraoz.com/2016/02/02/abc-rnn/
http://www.cs.cmu.edu/~music//cmsip/slides/05-algo-comp.pdf
http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
https://www.reddit.com/r/algorithmicmusic/

Please Subscribe! And like. And comment. That's what keeps me going.

Join us in the Wizards Slack channel:
http://wizards.herokuapp.com/

And please support me on Patreon:
https://www.patreon.com/user?u=3191693

Thanks Ji-Sung Kim for the example code:
https://deepjazz.io
Follow me:
Twitter: https://twitter.com/sirajraval
Facebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/ 
Signup for my newsletter for exciting updates in the field of AI:
https://goo.gl/FZzJ5w
Hit the Join button above to sign up to become a member of my channel for access to exclusive content!"
,,41 : The vectorize sequence of notes will be input into the first LSTM cell.,How to Generate Music - Intro to Deep Learning #9,https://www.youtube.com/watch?v=4DMm5Lhey1U,"We're going to build a music generating neural network trained on jazz songs in Keras. I'll go over the history of algorithmic generation, then we'll walk step by step through the process of how LSTM networks help us generate music.

Coding Challenge for this video:
https://github.com/llSourcell/How-to-Generate-Music-Demo

Vishal's Winning Code:
https://github.com/erilyth/DeepLearning-SirajologyChallenges/tree/master/Art_Generation

Michael's Runner up code:
https://github.com/michalpelka/How-to-Generate-Art-Demo/blob/master/demo.ipynb

More Learning Resources:
https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714#.swstv6z61
http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html
https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/
http://deeplearning.net/tutorial/rnnrbm.html
https://maraoz.com/2016/02/02/abc-rnn/
http://www.cs.cmu.edu/~music//cmsip/slides/05-algo-comp.pdf
http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
https://www.reddit.com/r/algorithmicmusic/

Please Subscribe! And like. And comment. That's what keeps me going.

Join us in the Wizards Slack channel:
http://wizards.herokuapp.com/

And please support me on Patreon:
https://www.patreon.com/user?u=3191693

Thanks Ji-Sung Kim for the example code:
https://deepjazz.io
Follow me:
Twitter: https://twitter.com/sirajraval
Facebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/ 
Signup for my newsletter for exciting updates in the field of AI:
https://goo.gl/FZzJ5w
Hit the Join button above to sign up to become a member of my channel for access to exclusive content!"
,,42 : Then we'll apply a dropout to help ensure that the model generalized well.,How to Generate Music - Intro to Deep Learning #9,https://www.youtube.com/watch?v=4DMm5Lhey1U,"We're going to build a music generating neural network trained on jazz songs in Keras. I'll go over the history of algorithmic generation, then we'll walk step by step through the process of how LSTM networks help us generate music.

Coding Challenge for this video:
https://github.com/llSourcell/How-to-Generate-Music-Demo

Vishal's Winning Code:
https://github.com/erilyth/DeepLearning-SirajologyChallenges/tree/master/Art_Generation

Michael's Runner up code:
https://github.com/michalpelka/How-to-Generate-Art-Demo/blob/master/demo.ipynb

More Learning Resources:
https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714#.swstv6z61
http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html
https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/
http://deeplearning.net/tutorial/rnnrbm.html
https://maraoz.com/2016/02/02/abc-rnn/
http://www.cs.cmu.edu/~music//cmsip/slides/05-algo-comp.pdf
http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
https://www.reddit.com/r/algorithmicmusic/

Please Subscribe! And like. And comment. That's what keeps me going.

Join us in the Wizards Slack channel:
http://wizards.herokuapp.com/

And please support me on Patreon:
https://www.patreon.com/user?u=3191693

Thanks Ji-Sung Kim for the example code:
https://deepjazz.io
Follow me:
Twitter: https://twitter.com/sirajraval
Facebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/ 
Signup for my newsletter for exciting updates in the field of AI:
https://goo.gl/FZzJ5w
Hit the Join button above to sign up to become a member of my channel for access to exclusive content!"
,,59 : L-S-T-M. Say it again.,How to Generate Music - Intro to Deep Learning #9,https://www.youtube.com/watch?v=4DMm5Lhey1U,"We're going to build a music generating neural network trained on jazz songs in Keras. I'll go over the history of algorithmic generation, then we'll walk step by step through the process of how LSTM networks help us generate music.

Coding Challenge for this video:
https://github.com/llSourcell/How-to-Generate-Music-Demo

Vishal's Winning Code:
https://github.com/erilyth/DeepLearning-SirajologyChallenges/tree/master/Art_Generation

Michael's Runner up code:
https://github.com/michalpelka/How-to-Generate-Art-Demo/blob/master/demo.ipynb

More Learning Resources:
https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714#.swstv6z61
http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html
https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/
http://deeplearning.net/tutorial/rnnrbm.html
https://maraoz.com/2016/02/02/abc-rnn/
http://www.cs.cmu.edu/~music//cmsip/slides/05-algo-comp.pdf
http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
https://www.reddit.com/r/algorithmicmusic/

Please Subscribe! And like. And comment. That's what keeps me going.

Join us in the Wizards Slack channel:
http://wizards.herokuapp.com/

And please support me on Patreon:
https://www.patreon.com/user?u=3191693

Thanks Ji-Sung Kim for the example code:
https://deepjazz.io
Follow me:
Twitter: https://twitter.com/sirajraval
Facebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/ 
Signup for my newsletter for exciting updates in the field of AI:
https://goo.gl/FZzJ5w
Hit the Join button above to sign up to become a member of my channel for access to exclusive content!"
,,"60 : An LSTM cell consists of three gates-- the input, forget, and output, as well as a cell state.",How to Generate Music - Intro to Deep Learning #9,https://www.youtube.com/watch?v=4DMm5Lhey1U,"We're going to build a music generating neural network trained on jazz songs in Keras. I'll go over the history of algorithmic generation, then we'll walk step by step through the process of how LSTM networks help us generate music.

Coding Challenge for this video:
https://github.com/llSourcell/How-to-Generate-Music-Demo

Vishal's Winning Code:
https://github.com/erilyth/DeepLearning-SirajologyChallenges/tree/master/Art_Generation

Michael's Runner up code:
https://github.com/michalpelka/How-to-Generate-Art-Demo/blob/master/demo.ipynb

More Learning Resources:
https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714#.swstv6z61
http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html
https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/
http://deeplearning.net/tutorial/rnnrbm.html
https://maraoz.com/2016/02/02/abc-rnn/
http://www.cs.cmu.edu/~music//cmsip/slides/05-algo-comp.pdf
http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
https://www.reddit.com/r/algorithmicmusic/

Please Subscribe! And like. And comment. That's what keeps me going.

Join us in the Wizards Slack channel:
http://wizards.herokuapp.com/

And please support me on Patreon:
https://www.patreon.com/user?u=3191693

Thanks Ji-Sung Kim for the example code:
https://deepjazz.io
Follow me:
Twitter: https://twitter.com/sirajraval
Facebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/ 
Signup for my newsletter for exciting updates in the field of AI:
https://goo.gl/FZzJ5w
Hit the Join button above to sign up to become a member of my channel for access to exclusive content!"
,,61 : The cell state is like a conveyor belt.,How to Generate Music - Intro to Deep Learning #9,https://www.youtube.com/watch?v=4DMm5Lhey1U,"We're going to build a music generating neural network trained on jazz songs in Keras. I'll go over the history of algorithmic generation, then we'll walk step by step through the process of how LSTM networks help us generate music.

Coding Challenge for this video:
https://github.com/llSourcell/How-to-Generate-Music-Demo

Vishal's Winning Code:
https://github.com/erilyth/DeepLearning-SirajologyChallenges/tree/master/Art_Generation

Michael's Runner up code:
https://github.com/michalpelka/How-to-Generate-Art-Demo/blob/master/demo.ipynb

More Learning Resources:
https://medium.com/@shiyan/understanding-lstm-and-its-diagrams-37e2f46f1714#.swstv6z61
http://mourafiq.com/2016/05/15/predicting-sequences-using-rnn-in-tensorflow.html
https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/
http://deeplearning.net/tutorial/rnnrbm.html
https://maraoz.com/2016/02/02/abc-rnn/
http://www.cs.cmu.edu/~music//cmsip/slides/05-algo-comp.pdf
http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/
https://www.reddit.com/r/algorithmicmusic/

Please Subscribe! And like. And comment. That's what keeps me going.

Join us in the Wizards Slack channel:
http://wizards.herokuapp.com/

And please support me on Patreon:
https://www.patreon.com/user?u=3191693

Thanks Ji-Sung Kim for the example code:
https://deepjazz.io
Follow me:
Twitter: https://twitter.com/sirajraval
Facebook: https://www.facebook.com/sirajology Instagram: https://www.instagram.com/sirajraval/ Instagram: https://www.instagram.com/sirajraval/ 
Signup for my newsletter for exciting updates in the field of AI:
https://goo.gl/FZzJ5w
Hit the Join button above to sign up to become a member of my channel for access to exclusive content!"
,,new video
,,113 : So this model is relatively simple and you can imagine this working reasonably well assuming that you tuned all the hyperparameters right but it's kind of a problem right.,Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,"114 : So in this setup, since we're applying a bunch of convolutions that are all keeping the same spatial size of the input image, this would be super super expensive right.",Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,"115 : If you wanted to do convolutions that maybe have 64 or 128 or 256 channels for those convolutional filters which is pretty common in a lot of these networks, then running those convolutions on this high resolution input image over a sequence of layers would be extremely computationally expensive and would take a ton of memory.",Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,120 : We've seen that you can do strided convolutions or various types of pooling to reduce the spatial size of the image inside a network but we haven't really talked about upsampling and the question you might be wondering is what are these upsampling layers actually look like inside the network?,Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,121 : And what are our strategies for increasing the size of a feature map inside the network?,Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,"122 : Sorry, was there a question in the back?",Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,"312 : So given our input image in this case we'll run some region proposal network to get our proposals, these are also sometimes called regions of interest or ROI's so again Selective Search gives you something like 2000 regions of interest.",Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,"313 : Now one of the problems here is that these input, these regions in the input image could have different sizes but if we're going to run them all through a convolutional network our classification, our convolutional networks for classification all want images of the same input size typically due to the fully connected net layers and whatnot so we need to take each of these region proposals and warp them to that fixed square size that is expected as input to our downstream network.",Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,"314 : So we'll crop out those region proposal, those regions corresponding to the region proposals, we'll warp them to that fixed size, and then we'll run each of them through a convolutional network which will then use in this case an SVM to make a classification decision for each of those, to predict categories for each of those crops.",Lecture 11 | Detection and Segmentation,https://www.youtube.com/watch?v=nDPWywWRIRo,"In Lecture 11 we move beyond image classification, and show how convolutional networks can be applied to other core computer vision tasks. We show how fully convolutional networks equipped with downsampling and upsampling layers can be used for semantic segmentation, and how multitask losses can be used for localization and pose estimation. We discuss a number of methods for object detection, including the region-based R-CNN family of methods and single-shot methods like SSD and YOLO. Finally we show how ideas from semantic segmentation and object detection can be combined to perform instance segmentation.

Keywords: Semantic segmentation, fully convolutional networks, unpooling, transpose convolution, localization, multitask losses, pose estimation, object detection, sliding window, region proposals, R-CNN, Fast R-CNN, Faster R-CNN, YOLO, SSD, DenseCap, instance segmentation, Mask R-CNN

Slides: http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf

--------------------------------------------------------------------------------------

Convolutional Neural Networks for Visual Recognition

Instructors:
Fei-Fei Li: http://vision.stanford.edu/feifeili/
Justin Johnson: http://cs.stanford.edu/people/jcjohns/
Serena Yeung: http://ai.stanford.edu/~syyeung/

Computer Vision has become ubiquitous in our society, with applications in search, image understanding, apps, mapping, medicine, drones, and self-driving cars. Core to many of these applications are visual recognition tasks such as image classification, localization and detection. Recent developments in neural network (aka “deep learning”) approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. This lecture collection is a deep dive into details of the deep learning architectures with a focus on learning end-to-end models for these tasks, particularly image classification. From this lecture collection, students will learn to implement, train and debug their own neural networks and gain a detailed understanding of cutting-edge research in computer vision.

Website:
http://cs231n.stanford.edu/

For additional learning opportunities please visit:
http://online.stanford.edu/"
,,new video
,,53 : It's a complicated topic.,L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,54 : So here is how the LSTM cell looks like.,L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"55 : So there are many,  many things going on.",L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,57 : So where is this before I  explain all these letters and notations?,L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,58 : Where's this LSTM  cell located?,L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,59 : So you would actually put this LSTM cell here  in the center.,L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"72 : So you can see, this red  one really would fit here into this RNN.",L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"73 : So instead of having  just a regular hidden layer, we would now have this, yeah, LSTM  memory cell.",L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"74 : So yeah, going through things step by step, we  have this cell state at a previous time step.",L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,"89 : So we have before we had this matrix H, H,  right, and w, h x, here, we are calling it for forget gate  instead of h h, let's call it f h and f x.",L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,90 : And then we have  also bias unit BF.,L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
,,91 : So I just see I would actually here.,L15.5 Long Short-Term Memory,https://www.youtube.com/watch?v=k6fSgUaWUF8,"Slides: https://sebastianraschka.com/pdf/lecture-notes/stat453ss21/L15_intro-rnn__slides.pdf

-------

This video is part of my Introduction of Deep Learning course.

Next video: https://youtu.be/TI4HRR3Hd9A

The complete playlist: https://www.youtube.com/playlist?list=PLTKMiZHVd_2KJtIXOW0zFhFfBaJJilH51

A handy overview page with links to the materials: https://sebastianraschka.com/blog/2021/dl-course.html

-------

If you want to be notified about future videos, please consider subscribing to my channel: https://youtube.com/c/SebastianRaschka"
