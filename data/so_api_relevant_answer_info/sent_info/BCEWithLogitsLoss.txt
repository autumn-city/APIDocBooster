The pytorch implementation computes BCEWithLogitsLoss as
nn.BCEWithLogitsLoss is actually just cross entropy loss that comes inside a sigmoid function.
However, you don't need to do anything like that (i.e take sigmoid) when you use nn.BCEWithLogitsLoss.
As reduction parameter default value is 'mean' in BCEWithLogitsLoss.
When using [BCEWithLogitsLoss (hyper-link)] you make a 1D prediction per output binary label.
Using BCEWithLogitsLoss you implicitly apply Sigmoid to your outputs:
The PyTorch [documentation for BCEWithLogitsLoss (hyper-link)] recommends the pos_weight to be a ratio between the negative counts and the positive counts for each class.
BCEWithLogitsLoss works directly on the outputs and labels in the sense that it expects the logits as one input and the class (0 or 1) as the second input.
You can find more details regarding this loss and the expected label tensor here:
[BCEWithLogitsLoss (hyper-link)]
nn.BCELoss expects that we have already applied sigmoid activation over logits, while nn.BCEWithLogitsLoss expects logits as inputs and internally applies sigmoid activation over logits before calculating binary cross entropy loss.
Option 2 is implemented with the pos_weight parameter for BCEWithLogitsLoss
[nn.BCEWithLogitsLoss() (hyper-link)] stands for Binary Cross-Entropy loss: that is a loss for Binary labels.
After that the choice of Loss function is loss_fn=BCEWithLogitsLoss() (which is numerically stable than using the softmax first and then calculating loss) which will apply Softmax function to the output of last layer to give us a probability.
BCEWithLogitsLoss applies Sigmoid not Softmax, there is no Softmax involved at all.
From the [nn.BCEWithLogitsLoss documentation (hyper-link)]:
