today select chip it's called jax can it
beat by torch and tensorflow
what is it all about
we probably all know that nowadays there
are two main
deep learning frameworks torch and
tensorflow but it was not always like
this historically there have been other
frameworks which is
dead by now or never really succeeded
and of course there is no reason to
believe that this dopamine of python
tensorflow is going to persist in the
future of all times until the end of our
universe it's quite possible that in the
future new frameworks will appear and
compete successfully with those two and
of course even nowadays many people and
companies are trying to create their new
deep learning frameworks do you all hear
me this is a presentation
yes
okay good then i continue and today i'm
going to tell you about google gex one
of the new frameworks which we just
trying to compete with the men too
nowadays
why they why did they create gx i don't
really know ask google people but
probably the ai fork at google got tired
of tensorflow and wanted a new toy so
they created jx
jx is not one library nowadays there is
a whole jax ecosystem jax itself is
lower level api sync by torch without an
n or in the flow without keras
and there is also high level layer api
flex and another one called haiku y2
because you might know google is a big
corporation and there is a thing called
deep mind which is formerly part of
google but in real life they don't want
to
have anything to do with the rest of
google and always do things differently
so of course if
if
the google created flags for gx
deepmind had to create their own
framework instead so they created hey
which is similar to
net a tensorflow library which is used
by deepmind and nobody knows outside of
deepmind but in deepmind they used this
library sonnet instead of keras and they
did the same with gx created their own
library fake inspired by sonnet
and there is also optics which contains
optimizers and loss function for jx and
many many other libraries based on jx so
you can see the jax ecosystem article
known that currently there is no data
set or data loaders for classes for gx
people have to create their own or most
often you use tensorflow by
data sets
let's start at with the projects which
is a lower level api
what is jx really it it's a python
libraries which didn't appear out of the
thin air it had its own predecessors
autograde was a python library which
implemented new pi like functions but
with differentiations like back
propagation style
and completely different project is
google xla or accelerated linear algebra
it's a
fast matrix operations for cpu gpu and
tpu the same quadrants and different
devices inside it's it compiles stuff
into efficient machine code it was
optional in tensorflow for jax it's no
longer optional ejects is an auto grad
like library
based entirely on xla and it includes
selects a python api for excel
which is really really low level and all
which is slightly higher level you can
simple chances of numpy with back probe
gt just in time compilation and gpu and
tpu support
there is a
link to github documentation electri
epiphany and this actually first lecture
there are four lectures there you can
look at all the stuff and of course
today i'll give very brief overview of
gx and tell you just the main idea as
you are not going to learn how to use
checks from this lecture alone you have
to do tutorials official tutorials or
other lectures if you really want to
launch your excel just give a very brief
overview of what this monster is
so let's continue
in checks you write code similar to
numpy but using jnp instead of np
but
jx is entirely
based on the functional programming
paradigm and because of that all tens is
simulatable
both and more on that later you can
calculate gradients with the function
grid you can compile python code to xla
with function sheet you can add batch
dimension to a function using v map and
p map
and basically by i covered the main
functions of low level jx same code can
run on cpu nvidia gpu or tpu
but gpu version requires strict code and
qdn versions much like tensorflow so you
probably want to use the current zero z
unlike for tensorflow there is no
official docker images so you have to
create them yourself or find some
unofficial ones unfortunately
next
jax uses functional programming paradigm
but what is functional program very
brief reminder in mathematics there was
a thing called mathematical functions in
programming programming functions were
largely based on mathematical functions
but but not really because in
programming functions you could put
print you can put global variables and
so on so-called side effects and
functional programming means using only
pure function pure function means
it has no side effect in particular it
receives input data only through the
arguments x y z it outputs result only
through the return value and nothing
else it does not modify arguments in any
way it doesn't access any global
variables and doesn't have any other
side effects like printing or writing
files or accessing website anything like
that
that's called pure function which
behaves much like mathematical functions
and this paradigm is very popular and
one i don't know academia theories of
programming languages and maybe not so
popular in real software development but
anyway injects try to
follow this paradigm
and what about objects and classes
originally in pure functional progress
means there were supposed to be no
objects and classes but of course in
python you cannot do things like race
without classes so which checks
compromises
it allows objects like array or even
like neural networks as long as they are
immutable but they for example they can
they can contain some parameters like
size of the ray or something but they
should be immutable
how do you work with immutable tensors
in june pi you can modify arrays
like that you you create an array and
then you modify two elements of the
array injects you cannot do like that
because arrays are strictly immutable
but what you can do you can do add and
set operation which works the same way
except that it copies the date and
create a new object which is now
assigned to a
this should be remembered when writing
the code
and for and what does functional
programming means for deep learning
in deep union you typically have a model
which has model architecture
and also model parameters like the
ovates like numerical parameters of each
layer and of course this
model parameters are mutable so in our
paradigm the model parameters must be
separated from the model definition of
model code like in python you have model
class which had model definition and
model parameters and everything here you
cannot have like this you have to
separate model different model
definition and model para meters i'll in
a few slides you will see how it works
in practice
also
if you are using bottom you will have
also state which is similar to para
meters but we but variables which are
not optimized like like averages
from batch norm operation like moving
averages and these are they're typically
called state and they're like parameters
but not optimized by the optimizer
you cannot have model class like and by
torch already told you and exactly the
same goes for example for optimizer you
have to separate code from parameters
and even not random number generator
a question is there connection are you
still here in me
yes
very good let us continue because
internet could be sometimes unstable
here
next function is cheat make a python
function run much faster basically it's
like this you'll define a function
python function and as you might know
python language is a bit slow compared
to real programming languages like c or
c plus plus or fortran but
but here you can use jit function
process my function to create a new fast
function which is supposed to be fast so
you think wow magically you can make any
python function run fast
much faster sometimes ten times so more
faster
magic but unfortunately it doesn't
really work like that to some extent yes
but there are limitations what does it
do it compiles python code to the google
xla code which as we know can run on
different devices it uses tracing
similar to tor by torch trays
but there are limitations the functions
the function must be pure any side
effects prints using globals are not
reproduced
second
things like
the code of the function is very limited
if and four constructions are not
supported unless unless their reliance
only static arguments which are known
and and the time of compiling the
functions it's possible to declare some
of the arguments static the actual
compiling happens not when you call
cheat but when you call fast function
for the first time so so it only makes
sense if you run the same function many
times
moreover
the compilation requires there exactly
known types and shapes in case of arrays
of the input arguments
if if there's types of shapes of input
arguments change or static values of
static arguments change then the
function is automatically recompiled
which which you can take pretty long
time so so this cheat can make the
function run faster but it only makes
sense if you run the same code many
times and exactly with the same you know
size and shape and types of input
arguments
otherwise it will be slowly because of
recompiling every time
let's then next function great great
this is a gradient of skylar function i
mean in our company you probably all
know about deep learning and things like
book propagation and
john jack's jax graduate just implements
back probe for python code using jnpgx
version of new prime
example is like this there is a function
of
of
some of the vector components squared
you take
gradient
and it writes you the gradient
gradient also uses tracing by tracing
under different conditions so now things
like if and four are allowed basically
it's a back props for python code you
can also recall function value and
greatly which returns that double of
function itself and this gradient there
are even more complicated things like
jacobian look at the documentation by
default grade differentiates with
respect to the thoughts argument of a
function typically in deep learning you
have a model and you always want to know
the gradient of the model with respect
to model parameters which we have to
optimize
we'll see in a next interesting slide or
too
next function is v map
some it's not always needed but
sometimes it's useful to vectorize a
function along the base dimension for
example here we have a function f
mix with the class sum matrix three by
three and multiplies
w times argument x
it works only in shape three suppose you
want to base dimension instead of shape
three shape two three this function
wouldn't work but
if you if you do apply operation we map
to the function there and then it now
supports the batch dimension and it can
be executed on data like this sometimes
it's useful if you have some
non-standard highly experimental neural
network layers but the way
and you don't want to bother with b
should i mention you can
write a version with batch one and then
edge back dimension later similar
function is p map it's like vmap but
works on multiple devices and gpu use
different gpus or several tpu cores no
gpu core
several gpus or several tpu cores
limitations like this number of devices
must be greater or equal to base damage
and you cannot have batch larger than
number of the devices if you use pmap
by default you cannot run it in multiple
cpu cores of course you want to try it
without any tp use so you want multiple
cpu cores by default there is only one
cpu course used
by
xla but if you specify xla flex
environment variable you can make xla
run on eight cores then you can try pmap
on a cpu there is also very rudimentary
communication between devices like psa
and so pmx looks at the documentation
do you still hear me
okay
yes okay good
the next topic is jax patrice what is a
pie tree gx by three is a nested
structure of collections like dick
the list and so on
the end nodes called the leaves are
typically but not always jack's arrays
and also custom collections can be
registered for batteries like frozen
diction flex for example the immutable
version of dictionary why does checks
use python by trees typically contain
parameters or states of a neural network
optimizer and so on as i told you model
code is separate that from parameters
and you need some data structure for
parameters and this typically uh to a
partly basically nested dictionaries
because
and nested because our network
can
contain many layers which layers its own
parameters thus nested
dictionaries
and yes has special functions to work on
by trees for example tree map applies a
farm
to
to each leaf of the by three and returns
by three of exactly the same structure
but with function applied to each leaf
and you cannot also combine two trees of
the same structure and size with stream
multi map
common jx great function support by
three so basically if the shortest
argument of a function is a pi 3 network
parameters jx great
differentiates with respect to this by 3
parameters and returned by 3 of the same
shape
with the same dictionary keys
next topic random numbers
okay
python packages like new pine pie torch
they have a global random native genre
number generator with a global state
glo well is not very nice and especially
it's not acceptable for functional
programming paradigm
in injects you cannot have globals and
cannot even have mutable objects like
random number generator t plus plus for
example can be not global but it's
immutable object you need to separate
the random generation code and the state
you need an explicit random generator
state which is called key in gx you can
generate a key from this seed then you
can use the key to generate random
numbers okay but
but then for example i do it again and
they get exactly the same result why of
course of course this normal is not
really normal it depends on the key and
if you supply the same kit wise you will
get obviously the same result it's not
good random number generation so what do
you do
what we do is we use function split
every time you use a random number
generation you generate a new key for
example you have key then you split k1
from from
from key and then you key
then t1 is used only once and and key is
updated next time you can again
do split and generate q2 and update key
so typically you have one key where
variable key which is updated all the
time and every every time you need a new
random number operation you can split q1
from that and use keyboard only once
that's how random number generation
works in functional programming paradigm
where you have to separate random state
with a random code
and
you can also create split two keys if
you specified three keys so you can
spread a split list of 10 keys if you
want to all this possible
now i'm almost finished with phx
let's let's see a minimum neural network
example in phx let's generate some
linear day
plus noise this is linear function with
some noise and we generate 101 point
using the random number generation as i
explained
then we do a linear model which
with two parameters which tries to
interpolate this data with a linear
function this is our model then
then we define a loss function as
mse loss
so your
typical loss function gets parameters x
and y you call model with parameters
index know that how you call model you
have to specify parameters explicitly
you calculate prediction and you
calculate loss between ground truth and
prediction then you calculate value and
gradient of this function and also it
goes through cheat for speed
and value and gradient and gradient are
taken with respect to parameters here
it's only a list of two numbers but in
general it could be by three
i define the two parameters learning
rate
and then training loop for hundred array
i calculate medians dependent parameters
x and y and then
and then i add gradients to parameters
how in gradients
it is here i use three mounting map
which combines two pie trees one by
three
parameters are the path regret and the
function is a p minus learning rate
times gradient so we implemented the
simplest form of sgdo or here is rather
gradient the same knowledge test td
because you don't have pages
and such thing works in real life you
use optimizers
from optics instead of simple sgd
next chapter don't worry it's going to
be very short compared to chapter one
only one slide about objects let's
change our chord to it objects
we
the declare optimizer optics adam
specifying learning rate but you already
know that that optimizer cannot be an
object with a state so stay it should
exist somehow separately and that's how
you do it you initialize optimizer
called init specifying params parameters
of neural networks and based on
parameters is create an optimizer state
and then at each iteration your data
optimizes state here you call optimizer
updates applying gradients and all
optimizer state
and the result is updated optimizer
state and gradient updates that's how
functional programming works and then
you call objects apply updates to
parameters basically it just numerically
adds update quite a little parameters by
three
and here is the same example but they
use adam to optimize also objects
contain
the various loss functions schedules and
so one you can read yourself
the next chapter three is flex
do we still have connection people can
you hear me yes yes we have yeah
very good what is flex flexible jax is a
layer api for gx
seen
my sync with something like a key of
keras or torch and then
and as we remember it's used by the
whole google except for deepmind which
have the different frameworks and
actually there was old flags flex and
then which was deprecated and eventually
removed nowadays
there is a new flex with api flex lining
which is usually imported as an end but
it's not exactly the same as the
alternate so forget the old flags i'm
going to tell you about the new flex
let's create a model of c galilea
and then dance v output features three
but of course it's functional
programming so we know that this model
is not complete it's only model code but
it so should also have model parameters
somewhere how to get model parameters to
get model parameters you call the init
function and then it function and gets a
random generator key which is used only
once you need and also sample input x as
a sample input so that init function
knows the model input size and shape
and then that initializes the model
basically it's again some sort of
tracing through the python code
and create model
a frozen dict for this simple mode of
the frozen dict has a simple single key
param so i'll explain soon why
the parameters are always under the key
params and here you only have one layer
with bias and human kernel it's actually
a bad name but it's actually w of the
linear layer and c is a typical use of
jx3 map lambda p p shape it's it's
creates the part 3 with the shape of
each leaf instead of the leaf itself
this trick is standardly used injects to
print
the size of that of the parameters
so once again first you have model code
but to get model parameters which is
separate because of functional
programming you call in it function
remember focus my there you had
also you need it's a standard pattern
used everywhere
in jax
universe
how do you run the model again you
cannot just call the model object like
in python you have to call apply
function and supply
this explicitly supply model parameters
and also argument x
or if your model have randomness for
example if you have dropout layers you
have to specify
the number keys for every layer which
have randomness it's a bit
inconvenient i would say
also you can what about more complicated
models than one layers you can use
sequential models
or you can create your own model class
model class is inherited from an end
module which is a python data class if
you don't know what a spice and data
class learn it it's interesting you
declare the fields with a type in the
beginning of class and it automatically
creates constructor to initialize all
these fields and because constructor is
already busy in data class setup happens
with the setup method which is called
note when object is created but when it
is called and here we declare two dance
layer and finally the call
method is
actually used
to run the neural networks we you apply
first layers and release and second
layer
and you
you create this class with two arguments
fit one and fit two that's that's how
you create a custom model in flex
or there is a simple way which works in
many cases a without setup method you
can just
use call method and rapids in compact
decorator there because of compact
decorator you can declare
layers in
inside just inside of the call method
anyway they're correctly registered and
correctly added to the dictionary of
parameters because of magic of compact
decorator here either clear the layers
and immediately call them and call
methods that's how people usually write
models and flags
the convolutional model is not much more
complicated you put convolutional layers
and and relu and average pool note
notice how can a colon with an object
which is created and registered it has
parameter and then called but relu and
average pool are just functions no
objects no parameters no registration
and compact as you always
can we write a custom linear layer in
flex
yes you can again you need compact for
some flex magic and you declare
parameters so just
just in call function i guess if you
didn't use compact you have to declare
them in in setup you that this is a
custom linear layer you have to declare
initializer so which reasonable default
and you declare parameters with self
para method parameter name initializer
and shape and again
because of compact magic is not
recreated every time you call call it's
created once registered on
initialization and then this parameter
from the dictionary supplied is used
when you run neural network inference
that's a code for custom linear layer
which is similar to predefined linear
layer
next is a problem of state as i told you
already what if we have a model with
state or non-parameter variables for
example which batch norm has such state
variables which are not parameters which
are not optimized by the optimizer
how do we do it in flex
it's a bit complicated a bit
inconvenient and flex first you have to
initialize wraps model indeed it
initializes all variables parameters and
not parameters you remember the
parameters conditionality params and
other variables which are not parameters
the state are using different keys and
then you have to separate stay
transparent
into state and parameters it's important
that only parameters must be optimized
and not state after you separate you
supply parameters to the optimizer but
not state
at inference time you have to recombine
in parameters and state again into a
single dictionary like that and you
supply this this full variables
parameters and state to apply moreover
you have to specify the keys everything
except parameters which are mutable and
the mutable keys are changed in the
model and returned into the
as a use updated state source state is
updated in this line for example batch
known states are updated at every
inference while parameters are updated
by optimizer steps that's how it works
not very convenient but that's how you
do it in flex actually flex has a higher
level abstraction called train state
which combines model parameters and dope
tax optimizer together so i don't want
to overload you with information if you
are interested you can learn from the
tutorial how to use trend state
and finally leave the final chapter
today well almost final chapter of
today's lectures hike this google deep
mind alternative to flex let's look at
it
in flex flex it's even more haiku sorry
is even more functional programming
styles and flex here in flex you always
have to define your network module is a
class here model doesn't matter there is
something called module but it's not
very important what you must specify a
function for example for linear layer
you specify a function like this neural
network is a function
but then you cannot use this function
directly you must first transform it
otherwise haiku models wouldn't work
your entrance hypotransform function
on this forward and
create a model function and then it's
similar to flex you run in it on this
model object now some kind of object is
created but you don't create this
class explicitly you initialize them all
the
and random number keys and
sample input and then you apply the
model note that by default trico always
needs a single key and z and it
automatically applies as if to generate
splits new keys and gives each during
each dropout layer for example if you
have drop out layers
and
compare flex and haiku in haiku all
magic happens in haiku transform while
flex
has a module class with some magic and
especially compact function so basically
what happens here in transform function
and flex happens in module and compact
function
there are different versions of hk
transform if you if you want to use
apply without random key if you don't
have any dropouts in your model you can
do additional transformation without
applying g so now you call it live
without random number t
if you have a state what do you do you
actually do transform this state and now
indian generates parameters and state
you see hypo automatically separates
parameters and state you don't have to
do it by hand like in flex and apply
takes parameters and state you don't
have to combine them by hate so in my
opinion
and of course only parameters are sent
to the optimize so in myopia your new
state is involved parameter supply state
are managed cleaner in haiku than in
flex
you can have models if you want but in
haiku it's only a lightweight container
basically the real magic happens in
transform you can declare a model of two
linear layers and relu like this create
the two modules then
then call them in call
but actually why do you need models you
can just declare function and in the
function you can declare two linear
layers and the result will be the same
if you don't real you have to use models
i mean
more additional master models can make
your network a bit clearer but you don't
have to use them you can define a neural
network as a function
and once again i already told you you
see how you declare the layers right
inside the function and hk transform
works similar to compact of flex
um and
and let's finally let's write a custom
linear layer in haiku
it's not very difficult but you have to
use that parameter to register
parameters and immediately use them
and of course when networks initialized
the parameter is created and
when
when you call call afterwards that a
parameter is not recreated by just used
and again you have name of parameters
shape type and initializer
and the magic again happens in haiku
transform
and also haiku has predefined networks
we such as mlp with given number of
layers and number of features per layer
and relu in a non-linearity by default
you can change it and other networks
like mobile net and resnet but
unfortunately no trained models
currently
we're finished with haiku and finally
the final chapter conclusion
remember we had a question can checks
succeed can it beat in the flow and i
told you at least become a serious
competitor i don't know the answer but
now i can see the good and bad things
about checks
the good things
jx is very tpu friendly it's also has a
reasonably good multi-process and
multi-device support without any
additional libraries
functional progress
make things a bit cleaner if you are a
professional well beginners probably
wouldn't agree and finally there is a
rate of google corporations behind it
which probably means something
what are the bad things about checks the
same functional programmings which
professional love can be annoying for
beginners because you need to instead of
one object two objects state and quote
apart from tpu very few real advantages
over by torch or tensorflow for that
matter
currently there are few deploy options
the only deploy options is experimental
checks students of law can we water
against you cannot live without in the
flow
and there are relatively fewer existing
and pre-trained more dulls compared to
say pythog but seals repository reflex
models derive for you
it's still a mature aviation zero of
everything api could change in the
future like the flex got completely
rebooted from an end to line
and since the end of my lecture thank
you for your attention
as