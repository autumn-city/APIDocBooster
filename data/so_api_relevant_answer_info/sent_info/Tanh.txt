Tanh is a good function with the above property.
If you consider these qualities, then I believe you can use ReLU in place of the tanh function since they are very good alternatives of each other.
Tanh
Mathematical expression: tanh(z) = [exp(z) - exp(-z)] / [exp(z) + exp(-z)]
First-order derivative: tanh'(z) = 1 - ([exp(z) - exp(-z)] / [exp(z) + exp(-z)])^2 = 1 - tanh^2(z)
Hard Tanh
Mathematical expression: hardtanh(z) = -1 if z < -1; z if -1 <= z <= 1; 1 if z > 1
First-order derivative: hardtanh'(z) = 1 if -1 <= z <= 1; 0 otherwise
The output from tanh can be positive or negative, allowing for increases and decreases in the state.
That's why tanh is used to determine candidate values to get added to the internal state.
The GRU cousin of the LSTM doesn't have a second tanh, so in a sense the second one is not necessary.
There aren't really meaningful differences between the derivatives of sigmoid and tanh; tanh is just a rescaled and shifted sigmoid: see Richard Socher's [Neural Tips and Tricks (hyper-link)].
3: There are a number of approximations that can be used to calculate tanh().
To perform a tanh, you meant vvtanh, not vvatanh (or vatanh as I originally wrote).
vvatanh is the vector base inverse hyperbolic tangent.)
First, the point that should be obvious: If vvtanh were universally faster/better than tanh, then tanh would simply be implemented as vvtanh.
So, if you have a large group of numbers arranged in a C-array and need to compute tanh on them all at once, then vvtanh is likely a good tool and you should profile it.
If nothing else, you may save the function-call overhead of iterating over tanh (provided it's not inlined).
If you can't structure you data that way, and you would be forced to call vvtanh many times, then it's almost certainly a lose, and the simpler tanh is going to be better.
In this case, maybe they want activations to fall between -1 and 1, so they use tanh.
[This page (hyper-link)] says to use tanh, but they don't give an explanation.
"The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output
layer which uses the Tanh function.
It could be that the symmetry of tanh is an advantage here, since the network should be treating darker colours and lighter colours in a symmetric way.
The formula is:
tanh s' = 0.5[tanh(0.01(s-μ)/σ) + 1]
np.tanh() for the tanh function
Note this code implement a modified tanh-estimators proposed in Efficient approach to Normalization of Multimodal Biometric Scores, 2011
In truth both tanh and logistic functions can be used.
The idea is that you can map any real number ( [-Inf, Inf] ) to a number between [-1 1] or [0 1] for the tanh and logistic respectively.
Now regarding the preference for the tanh over the logistic function is that the first is symmetric regarding the 0 while the second is not.
In my experience, some problems have a preference for sigmoid rather than tanh, probably due to the nature of these problems (since there are non-linear effects, is difficult understand why).
The activation function of each element of the population is choosen randonm between a set of possibilities (sigmoid, tanh, linear, ...).
Most of time tanh is quickly converge than sigmoid and logistic function, and performs better accuracy [[1] (hyper-link)].
However, recently rectified linear unit (ReLU) is proposed by Hinton [[2] (hyper-link)] which shows ReLU train six times fast than tanh [[3] (hyper-link)] to reach same training error.
Update in attempt to appease commenters: based purely on observation, rather than the theory that is covered above, Tanh and ReLU activation functions are more performant than sigmoid.
For example, try limiting the number of features to force logic into network nodes in XOR and [sigmoid rarely succeeds (hyper-link)] whereas [Tanh (hyper-link)] and [ReLU (hyper-link)] have more success.
Tanh seems maybe slower than ReLU for many of the given examples, but produces more natural looking fits for the data using only linear inputs, as you describe.
Many of the answers here describe why tanh (i.e.
Tanh and the logistic function, however, both have very simple and efficient calculations for their derivatives that can be calculated from the output of the functions; i.e.
if the node's weighted sum of inputs is v and its output is u, we need to know du/dv which can be calculated from u rather than the more traditional v: for tanh it is 1 - u^2 and for the logistic function it is u * (1 - u).
In deep learning the ReLU has become the activation function of choice because the math is much simpler from sigmoid activation functions such as tanh or logit, especially if you have many layers.
And tanh has a zero mean.
Ref: [https://medium.com/analytics-vidhya/activation-functions-why-tanh-outperforms-logistic-sigmoid-3f26469ac0d1 (hyper-link)]
If gradient is increasing then tanh activation function.
You will also need to replace that with the derivative of the tanh function, which is 1 - (tanh(x))^2.
Which is in line with the tanh plot:
Are you sure you get -1 for tanh(0)?
Your model includes a line + 3 tanh functions.
It's not entirely clear that the data support that many different tanh functions.
Either way, it would probably be helpful to be able to easily test if there really are 1, 2, 3, or more tanh functions.
You may also want to give not only initial values for your parameters, but also realistic boundaries on them so that the tanh functions are clearly separated and don't wander too far off from where they should be.
To clean up your code and to better allow you to change the number of tanh functions used and place boundary constraints, I would suggest making individual models and adding them as with:
You just run the risk of any fixes or changes to the real V8 Math.tanh that may occur in the future.
I've confirmed using an HP 42S emulator that tanh(1/(sqrt(3.0))) is approximately 0.520737 (I get 0.520736883716).
tanh-1(i ⁄ √3) = πi ⁄ 6 (where i is the imaginary unit, √-1)
Tanh: (ex-e-x)/(ex + e-x)
Sigmoid usually refers to the shape (and limits), so yes, tanh is a sigmoid function.
(( 2/ (1 + Exp(-2 * x))) - 1) is equivalent to tanh(x).
Generally the most important differences are
a. smooth continuously differentiable like tanh and logistic vs step or truncated
b. competitive vs transfer
c. sigmoid vs radial
d. symmetric (-1,+1) vs asymmetric (0,1)
Generally the differentiable requirement is needed for hidden layers and tanh is often recommended as being more balanced.
The 0 for tanh is at the fastest point (highest gradient or gain) and not a trap, while for logistic 0 is the lowest point and a trap for anything pushing deeper into negative territory.
Generally, the recommendation is for tanh on the intermediate layers for +/- balance, and suit the output layer to the task (boolean/dichotomous class decision with threshold, logistic or competitive outputs (e.g.
So I recommend  unbiased instance normalization or biased pattern standardization or both on the input layer (possibly with data reduction with SVD), tanh on the hidden layers, and a threshold function, logistic function or competitive function on the output for classification, but linear with unnormalized targets or perhaps logsig with normalized targets for regression.
The word is (and I've tested) that in some cases it might be better to use the tanh than the logistic since
The derivative of tanh (1 - y^2) yields values greater than the logistic (y (1 -y) = y - y^2).
For example, when z = 0, the logistic function yields y = 0.5 and y' = 0.25, for tanh y = 0 but y' = 1 (you can see this in general just by looking at the graph).
MEANING that a tanh layer might learn faster than a logistic layer because of the magnitude of the gradient.
I believe you need to use a raw-string to ignore the \t, so label=r'$\tanh(x)$'.
The methods which require fdlibm semantics are sin, cos, tan, asin, acos, atan, exp, log, log10, cbrt, atan2, pow, sinh, cosh, tanh, hypot, expm1, and log1p.
When you don't use parallel_for_each syntax the code will be run on the CPU, which only implements one "precise" tanh function, and hence gives the correct answer.
If this is too slow and the precision for fast_math::tanh is otherwise sufficient, you could try something like
Some of them (like tanh) can even return NaN values.
On my HD 6870 fast tanh of all numbers more than 90 returns NaN.
You can "bound" Tanh argument to 10
This will not cause any precision loss because float has only 7-digits precision while [difference between Tanh(10) and 1 (hyper-link)] is 4*10-9

Alternatively, you can implement your own Tanh function which wont have such limits.
Found this tanh approximation somewhere a long ago.
However, if you need very accurate tanh u can replace concurrency::fast_math with concurrency::precise_math.
So, tanh u evaluate there is evaluated on CPU with no GPU specfic bugs.
you can see result of first tanh before syncronization while getting result of the parallel_for_each block requires it.
If not what diviation is best for tanh squashing function?
But tanh is a scaled and shifted sigmoid:
So if the activations are normally distributed for the sigmoid activation, they will still be normally distributed for the tanh.
So the same input distribution works ok for tanh.
Tanh, or hyperbolic tangent is a logistic function that maps the outputs to the range of (-1,1).
Tanh can be used in binary classification between two classes.
When using tanh, remember to label the data accordingly with [-1,1].
Sigmoid function is another logistic function like tanh.
The value range is between -1 and 1, but that's not necessarily a problem as far as the Tanh is concerned.
By learning suitable weights, the Tanh can fit to the value range [0,1] using the bias.
There is no such thing as tanh is better when labels are {-1,1} and sigmoid is better when they are {0,1}.
tanh(x) maps the input to the interval [-1, 1] and sigmoid(x) maps the input to the interval [0, 1].
If you change the sigmoid to tanh, that regression would no longer be a "logistic regression" because you are not modelling a probability.
A loss function like mse should be able to take the ouput of tanh, but it won't make much sense.
If the average activation of tanh is 0 (which is what we would want for a sparse autoencoder) then the KL div given on that page is unhappy.
is there a form of KL div which has an appropriate range for the tanh activation?
In my code I have used the tanh activation function.
One final note: In the MNIST example architectures I have seen, hidden layers with RELU activations are typically followed by Dropout layers, whereas hidden layers with sigmoid or tanh activations are not.
tanh is expensive to compute on most architectures.
Looks like you're using tanh in your output layer, where tanh has the range -1, +1 and the expected outputs are in the range 0, +1.
The Tanh method transforms the input to values in the range -1 to 1 which cross entropy can't handle.
Some possible fixes would be to rescale the input in the final layer in the input is tanh and the cost cross-entropy.
If you want to use a tanh activation function, instead of using a cross-entropy cost function, you can modify it to give outputs between -1 and 1.
Using this as the cost function will let you use the tanh activation.
So I guess you have already read in the other question: sigmoid/tanh functions have a fixed output range.
For sigmoid, this is (0,1), while for tanh, it's (-1,1).
For possible Sigmoid functions, check here (tanh is not the only possibility):
another solution is to store a table with the tanh function values in an array, and define a JavaScript function which interpolates the tanh values for x based on the tanh values stored in the array
Once you have an implementation for sinh/cosh is easy to get tanh.
For those led here by Google... it would be safer to use tanh(x) instead of cosh(x) to avoid overflow.
There are a bunch of pitfalls when it comes to tanh-sinh quadrature, one being that the integrand needs to be evaluated very closely to the interval boundaries, at distances less than machine precision, e.g., 1.0 - 1.0e-20 in the original example.
With [tanh_sinh (hyper-link)] (a project of mine), one then gets
What I understood is, instead of using sigmoid, you have to use tanh and so you want the output data in format of +1s and -1s instead of 0s and 1s.
Note: This answer is not python specific, but I don't think that something like tanh is fundamentally different across languages.
Tanh is usually implemented by defining an upper and lower bound, for which 1 and -1 is returned, respectively.
If we look at the glibc-implementation of tanh(x), we see:
for  x values greater 22.0 and double precision, tanh(x) can be safely assumed to be 1.0, so there are almost no costs.
for very small x, (let's say x<2^(-55)) another cheap approximation is possible: tanh(x)=x(1+x), so only two floating point operations are needed.
for the values in beetween, one can rewrite tanh(x)=(1-exp(-2x))/(1+exp(-2x)).
However, one must be accurate, because 1-exp(t) is very problematic for small t-values due to loss of significance, so one uses expm(x)=exp(x)-1 and calculates tanh(x)=-expm1(-2x)/(expm1(-2x)+2).
The best way is probably just to measure the time needed to calculate tanh(x) compared with a time needed for a simple multiplication of two doubles.
So for very small and numbers >22 there are almost no costs, for numbers up to 0.1 we pay  6 FLOPS, then the costs rise to about 20 FLOPS per tanh-caclulation.
The key takeaway: the costs of calculating tanh(x) are dependent on the parameter x and maximal costs are somewhere between 10 and 100 FLOPs.
There is an Intel-instruction called [F2XM1 (hyper-link)] which computes 2^x-1 for -1.0<x<1.0, which could be used for computing tanh, at least for some range.
So if your program uses vectorization and has to use an unvectorized tanh implementation it will slowdown the program even more.
For this, the intel compiler has the mkl-library which [vectorizes tanh (hyper-link)] among the others.
Starting with the Turing architecture (compute capability 7.5), GPUs include an instruction MUFU.TANH to compute a single-precision hyperbolic tangent with about 16 bits of accuracy.
Best I can tell, MUFU.TANH is exposed at the level of the virtual assembly language PTX, but not (as of CUDA 11.2) as a device function intrinsic.
For arguments small in magnitude we can use tanh(x) = x and experimentally determine a good switchover point between the two approximations.
According to the backend function _tanh in gen_math_ops.py:
Since quantization is really new, perhaps the new version of _tanh is still in progress.
Here is a simple sequential model using tanh and sigmoid activation layers
LSTMs (the underlying RNN block) have been always defined in literature to use the tanh activation function.
If I recall correctly tanh works better than relu for recurrent networks, but I can't find the paper / resource of this memory.
In fact they can be worse than activation functions which are naturally limited when weights are large such as sigmoid or tanh.
On the contrary, though it does saturate when going to 1, tanh does not saturate (actually it's a maxima for its derivative) around 0 so learning in this region is not problematic.
The complex tanh method could be implemented like that:
Using the comment from Hans Passant another way to implement the tanh method would be:
If you want to use Math.tanh(), which is an ECMAScript 6 feature, you should target ES6 in your pom.xml and add a reference to core-lib 6.
Anyway, if you feel that way, you can also use this MDN equivalent using just simple Math.exp() calls: [https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Math/tanh#Polyfill (hyper-link)]
Later on, people started using the tanh function since it is zero centered, which gives somewhat better characteristics in some cases.
In these, using the unbounded actiation functions such as the ReLU would quickly lead to an explosion in results, and people still use the sigmoid and/or tanh in these cases.
The output Elemwise{tanh,no_inplace}.0 means, that you have an element wise operation of tanh, that is not done in place.
You can just simply change that from tf.tanh to whatever activation function you want.
You are using Xavier initialization for V variables in different layers, which indeed works fine for logistic sigmoid activation (see [the paper by Xavier Glorot and Yoshua Bengio (hyper-link)]), or, in other words, tanh.
But tanh is probably not an optimal activation function:
If max is unknown, you can normalize based on the data you do have, with the knowledge that tanh has good characteristics for values that exceed a magnitude of 1.
As the error message states, there is no fixed-point version of tanh and tansig functions in Matlab.
Good news is that tanh(x) only becomes +/- 1 when x is +/- infinity, so you do not need to worry too much about this.
For very small x, one could use [the Taylor expansion of 1/tanh(x) - 1/x around 0 (hyper-link)],
First of all the calculation of tanh does not rely on the standard definition of tanh instead one expresses it in terms of exp(-2*x) or expm1(2*x) which means one only have to calculate one exponential which is probably the heavy operation (in addition there's a division and some additions).
For these values tanh would be immediately decided to be 1 while exp must be calculated.
The only difference between tanh and sigmoid is scaling and offset.
This is a rational function to
  approximate a tanh-like soft clipper.
It is based on the pade-approximation
  of the tanh function with tweaked
  coefficients.
For an accurate answer using fewer Math.exp()s, you can use the relationship between tanh and the [logistic function (hyper-link)].
Tanh(x) is exactly 2 * logistic(2 * x) - 1, and expanding out the logistic function, you get:
Math.tanh – hyperbolic tangent of a number
Math.atanh – hyperbolic arc-tangent of a number
The error clearly says, Tanh only takes 1 argument, a tensor.
The problem is that there are many overloads of [std::tanh (hyper-link)].
You might want to explicitly use the float variant std::tanhf:
If you need to use tanh as your output, then you can use MSE with a one-hot encoded version of your labels + rescaling.
However, keep in mind that tanh reaches -1 or +1 at the infinities and the problem is purely theoretical.
For example, with tanh() activation function (my favorite), it is recommended to use the following activation function when the desired output is in {-1, 1}:
In the case of tanh() activation function, the error would be calculated as
Mathematically, it is known that d(tanh(x))/dx = 1 - (tanh(x))^2.
That means that, in general, we will reach the computation of the gradient of tanh(x) after reaching the step where we compute the gradient of an "outer" function g(tanh(x)).
g represents all the operations that are applied to the output of tanh to reach the value for which the gradient is computed.
The derivative of this function g, according to the [chain rule (hyper-link)], is d(g(tanh(x)))/dx = d(g(tanh(x))/d(tanh(x)) * d(tanh(x))/dx.
The first factor, d(g(tanh(x))/d(tanh(x)), is the reverse accumulated gradient up until tanh, that is, the derivate of all those later operations, and is the value of dy in the documentation of the function.
Therefore, you only need to compute d(tanh(x))/dx (which is (1 - y * y), because y = tanh(x)) and multiply it by the given dy.
The resulting value will then be propagated further back to the operation that produced the input x to tanh in the first place, and it will become the dy value in the computation of that gradient, and so on until the gradient sources are reached.
np.tanh is trying to delegate the task to the elements of array.
In this running code I don't see a tanh, so I don't know what kinds of arrays are using that.
There is also faster formula for computing tanh, requiring only one exp(), because tanh is related to logistic function:
tanh(x) = 2 / (1 + exp(-2 * x)) - 1
also
tanh(x) = 1 - 2 / (1 + exp(2 * x))
If you have to use tanh in this case, yes you would have make the image labels either -1 or 1.
(tanh is just sigmoid*2-1).
Since tanh(0) = 0, the network is unable to learn any transformation of zero to non-zero at any layer.
"Why is using tanh definition of logistic sigmoid faster than scipy's expit?"
Answer: It's not; there's some funny business going on with the tanh and exp C functions on my specific machine.
It's turns out that on my machine, the C function for tanh is faster than exp.
which matches the ~3x increase in the tanh function when called from Python.
The strange thing is that when I run the identical code on a separate machine that has the same OS, I get similar timing results for tanh and exp.
Running your script above I exp is faster in C++ than tanh.
But running the python snippet I get a 3x speedup from using tanh over expit.
Did you also change the function in the training, or you just used the same training method and then changed the sigmoid to tanh?
Have a look at the graphs of sigmoid and tanh:
sigmoid: [http://www.wolframalpha.com/input/?i=plot+sigmoid%28x%29+for+x%3D%28-1%2C+1%29 (hyper-link)]
tanh: [http://www.wolframalpha.com/input/?i=plot+tanh%28x%29+for+x%3D%28-1%2C+1%29 (hyper-link)]
We can see that in the tanh case, the value y = 0.5 is around x = 0.5.
where torch.tanh(output/10) is inlined in the forward function of your module.
Add it to your models with CustomTanh(1/10) instead of nn.Tanh().
sigmoid(1 * 1 + 1 * 0) = 0.73105857863, tanh(1 * 1 + 1 * 0) = 0.761594155956
H~t = tanh(1 * 1 + 0.769381871687 * 0.20482421 * 1) = 0.8202522791
To avoid using tanh(x)-(x/3) inside the for loop, which could get messy, let's define an anonymous function that takes x as an argument:
You can simplify the expression 1-tanh(x) to 2/(e^2x - 1).
It looks like your equation is a little more complicated than just log(1-tanh(x)), but the basic recipe is, simplify as much as possible, then leverage math functions such as Math.log1p() to avoid regions of instability.
theano.tensor.tanh((x * y).sum(2))
Just a hint that tanh-sinh quadrature is also available without mpmath, via [tanh_sinh (hyper-link)] (one of my packages).
And then the immediate Activation() will calculate tanh(z_norm[l]) to give a[l] i.e.
Now, to answer your question about using a ReLU function in place of a tanh function.
As far as I know, there shouldn't be much of a difference between the ReLU and tanh activation functions on their own for this particular gate.
Moreover there is a nice research of tanh-estimators by Hampel et al.
They considered the following x_norm = (1/2) * (tanh (0.01 * ((x - mean(x)) / std(x))) + 1)
Tanh-estimators normalization reference can be found at: [https://www.cs.ccu.edu.tw/~wylin/BA/Fusion_of_Biometrics_II.ppt (hyper-link)]
Just do: a = -2:2; plot(a,sinh(a),a,cosh(a),a,tanh(a));
You want to be out of the saturation regime of the tanh sigmoid.
Activation function simply makes some output values unreachable (sigmoid can output only values in [0,1], tanh in [-1,1]), while this is not true for the input (all activation functions are defined on the whole R domain).
So you could try switching from Rectifier to Tanh.

Regarding (2), I'm afraid you're right - the 'tanh' outputs won't be used; you'll need to pass activation='tanh' instead.
What I understand after reading about equation of mean field theory is that you want to solve s=tanh(zms/T) for different values of T then plot s versus T. Here is how you can do it in Scilab (no need to code Newton's method yourself you can use fsolve (see the help page of this function)
A final unit (tanh or sigmoid) - should receive in it's input a huge negative number in case of negative class and a huge positive number in case of positive class.
Try to use relu instead of tanh, and if you are using the deep q learning, you might dont need any activate function or take care about the optimizer which reset the weights.
The Math.tanh() method and others have been implemented in the JVM as native code.
In C, you only know it comes something like tanh(x) out.
For most C++ compilers, tanh is an [intrinsic function (hyper-link)] (a [built-in function (hyper-link)]), meaning there is no function call overhead because the compiler adds specific assembler instructions instead of a library call.
However, in Java, Math.tanh doesn't seem to be intrinsic.
For the logistic and tanh seems to be right.
Every squashing function sigmoid, tanh and softmax (in the output layer) 
means different cost functions.
Typically, ReLu or TanH are used between layers.
In Appendix A of this paper, under "Network architecture", it is also described that the actor output layer uses tanh as activation function.
If you do want to have both negative and positive outputs, are you limited to just tanh and linear?
If you do want to have both negative and positive outputs, are you limited to just tanh and linear?
The remaining model is full of relus, but here there is a tanh.
tanh sometimes vanishes or explodes (saturates at -1 and 1), which might lead to your 2-class overimportance.
[keras-example cifar 10 (hyper-link)] basically uses the same architecture (dense-layer sizes might be different), but also uses a relu there (no tanh at all).
Make all activations relus, unless you have a specific reason to have a single tanh.
Using tanh from cmath fixes the issue.
Also, you might try to remove the tanh function in the output layer.
I do not think that tanh is a problem since the output is between -1 and 1 but generally, for regression problems, we use linear activations and we keep squashing functions such as sigmoid and tanh for classification problems.
math.tanh does not know about decision variables.
Additionally, since you are transforming your labels in range 0 and 1, you may need to change your last layer activation function to sigmoid instead of tanh, or even may better to use an activation function like relu in your first layer if you are still getting poor results after above modifications.
With the activation functions tanh and [sigmoid (hyper-link)], the output of the neuron is limited to [-1,1] and [0, 1] respectively, so your output can't be more than one.
Just remove the tanh, and your output will be an unrestricted number.
Edit to add: You almost certainly want to keep the tanh (or some other non-linearity) between the recurrent connections, so remove it only for the output connection.
In most RNNs for classification, most people use a softmax layer on top of their LSTM or tanh layers so I think you can replace the softmax with just a linear output layer.
TanH: -1 < y < 1 (with center in 0)
For predicting negative outputs, tanh is the only of the three that is capable of doing that.
Tanh - The idea behind tanh is to not have sparsity which is enforced by ReLu and utilize complex network dynamics for learning similar to the sigmoid function.
Tanh in a simpler way, tries to use the entire network's capability to learn and addresses the vanishing gradient problem similar to ReLu.
Sigmoid and tanh is both saturated for positive and negative values.
Just divide each pixel with 255. the fact that LeakyRELU
performed better than sigmoid or tanh is because the values are too
large.
large in a sense that they get mistreated by tanh and sigmoid
and get rounded by the computer to integers.
Look carefully about how the neural network weights are
initialised if you intend to use tanh or sigmoid.
However, you can use a [sequential (hyper-link)] wrapper to combine a linear layer with tanh activation.
s='9.81*100*tanh(2*pi*3/x)-x=0';
Or for when tanh is used as the transfer function: sech^2(x) ... where x = weighted input?
Well, in this case its because the page doesn't mention the tanh as a potential activation function.
Your network is very small and glorot_uniform will work better in consonance with the tanh activations.
As above, I used all Tanh() transfer functions, is that correct?
the transition is very sharp such that neither erf, tanh, nor sigmoid work.
The modified tanh and the x/(1+x**p)**(1/p) fit equally well.
Specifically, the problem is that you are dealing with vanishing (or exploding) gradients, specifically when using loss functions that flatten in either direction for very small/large inputs, which is the case for both sigmoid and tanh (the only difference here is the range in which their output lies, which is [0, 1] and [-1, 1], respectively.
But functions like np.tanh don't have array methods.
OK so I think what was going on was that the Tanh function reaches 1 too soon.
Your problem might be the tanh's input range.
Note that sigmoid works a lot like tanh - it can be easily overloaded by a big number.
this is because of the nonlinearity, in case of tanh the type of nonlinearity in the interval [-1,+1] is different than in other intervals, i.e.
While [-1, 0, 1] is a valid target range for your tanh activation function, experience tells that Keras models don't work well with classification in a binary output.
sigmoid function to tanh function: Not necessary, many publications use sigmoid for non linear regressions.
I just implemented a simple non-monotonic activation function in Torch, and it not only works, but it slightly outperforms Tanh on the [MNIST demo (hyper-link)] with the default settings (which uses SGD by the way.)
According to the activation a linear layer and a tanh would both make an SVM the tanh will just be smoothed.
I would suggest making it binary and use a tanh layer, but try both things to see what works.
The problem was solved with changing the activation functions to "tanh".
The sech^2 part is always positive, so the sign of second_deriv depends on the sign of tanh, which can vary depending on the values you supply as x and w2.
): 'sigmoid' and 'tanh' are not 'output layers' but functions, usually referred to as "activation functions".
Additionally, your question recites a choice between two "alternatives" ("sigmoid and tanh"), but they are not actually alternatives, rather the term 'sigmoidal function' is a generic/informal term for a class of functions, which includes the hyperbolic tangent ('tanh') that you refer to.
Though, that seems unlikely because I could not get it to work with tanh and the results varied widely based on the number of hidden neurons.
Apart from the note on random initialization of weights (which you have already addressed now, moved that point to the bottom of the answer), it is important to note that you are using tanh as activation, but you expect outputs close to 0 or 1. tanh is much more suitable for cases where you expect outputs in [-1, 1], rather than [0, 1].
Consider this plot of the tanh function:
Our output is always going to be y = tanh(w0 + w1 x1 + w2 x2).
In other words, we want to have tanh(w0 + 0 + 0) ~= 0.
tanh(w1 x1) = 0
tanh(w2 x2) = 0
So, to conclude, with the tanh() activation function and the specific architecture you chose for the network, it's impossible to get precisely the output values we want for the AND problem.
If you want outputs closer to precisely 0 and 1, you could consider using the [sigmoid function (hyper-link)] instead of tanh.
The tanh function is pretty standard in this basic setup, but others are possible Take a look at the list of [activations (hyper-link)] that Keras provides out of the box.
You could use a [ReLU (hyper-link)] (linear activation) activation function for the first few layers and do a tanh activation for the output layer.
I assume this has something to do with floating point variations introduced by the tanh function in LogisticV2
I think your problem lies with activation function - tanh(500)=1 and tanh(1)=0.76.
If so can I just use any other non-linear activation function such as sigmoid or tanh?
Basically, sigmoidal functions (those which saturate on both ends, like tanh) are smooth almost-piecewise-constant approximators.
Tanh, for example, has a domain of (-∞,∞) and a range of (-1,1).
To interpret in simpler words, tanh(W[s;h])(what paper mentions as neural net) is a feedforward layer that is trained along with the encoder and decoder together.
The weights α1, .. are obtained by a softmax on the outputs of the attention layer/net, in your case the tanh.
In this case, the weights of tanh are learned, ie., the backprop and gradient update of tanh is done along with the entire encoder-decoder network.
If inputs comes from a tanh, then -1 <= inputs <= 1, and hence
Just noticed - using tanh is completely wrong, this is hyperbolic tangent function, it has no relation to geometry.
**However* I've just checked (installed the R-devel binary from CRAN on our Windows server virtual machine) and I see that the problem is still present there:  In yesterday's version of R-devel, tanh(500+0i) still returns NaN+0i.
as it should for  tanh(500+0i)    --- but does not on Windows.
Think about what would happen if you replaced your tanh(x) threshold function with a linear function of x - call it a.x - and treat a as the sole learning parameter in each neuron.
That's effectively what your network will be optimising towards; it's an approximation of the zero-crossing of the tanh function.
for classification :
You can use sigmoid, tanh, Softmax etc.
For sigmoid/TanH this would be glorot initialization, stddev = sqrt(2 / (Nr.
If you for simplicity use logistic and not tanh as your activation, then return tf.add(tf.constant(<lower-bound>), tf.scalar_mul(tf.constant(<upper> - <lower>), action))
Haven't tested, but seems reasonable.
If you want the tanh it is straightforward to fix the calculation accordingly.
The first issue here is that nowadays we never use activation='tanh' for the intermediate network layers.
For hidden-layer neurons, tanh is a common activation function, and it has range [-1, 1].
In the case of tanh, I think there is a division of two values with the same scaling, so it would cancel out in that case.
When using CORDIC you calculate tan and tanh by taking the ratio of sin to cos or sinh to cosh.
While using tanh as activation function, here is the result after 800 episodes.
tanh converges consistently faster than logistic.
The right is the tanh function, also known as hyperbolic tangent.
It's easy to see that tanh is antisymmetric about the origin.
Symmetric sigmoids such as tanh often converge faster than standard logistic function.
See [elliott (hyper-link)] for an alternative of tanh with easier computations.
It's shown below as the black curve (the blue one is the original tanh).
First, TANH usually needed fewer iterations to train than Elliott.
Elliott completed its entire task, even with the extra iterations it had to do, in half the time of TANH.
In your case of z = a*tanh(-b*x/a)+c + (d*y), the modeling function is
This can be explained by considering the sigmoid and tanh activation functions, displayed below.
For this data and NN architecture, with tanh activation, I was not able to get better results.
For tanh activation, still not able to get good results
[ (hyper-link)]
Your last layer in the Dense-NN has no activation function (tf.keras.layers.Dense(1)) while your last layer in the Variational-NN has tanh as activation (tfp.layers.DenseVariational(
1, activation='tanh' ...).
I also observed that relu and especially leaky-relu are superior to tanh in this setting.
Try plotting tanh(x) or tanh(c*x) in [WolframAlpha (hyper-link)].
Also try plotting 1/tanh(x).
According to the plot, what happens to tanh(x) as x goes to zero from the left and from the right?
The first thing you can try is changing your activation to LeakyReLU instead of using Relu or Tanh.
In other words, vpa(tanh(1)) calculates the hyperbolic tangent in double precision and tanh(vpa(1)) does so using variable precision.
Additionally, nowadays we practically never use tanh activation for the hidden layers; you should try relu instead.
I would also consider replacing the tanh activations with relu, so you at least partially avoid the vanishing gradients.
S = w sech(s) w'
T = (w tanh(s) w')(wv') = w tanh(s) v'
U = wv'
you can apply a ai * tanh(xi) + bi transformation selecting ai and bi for each variable so that it is bounded to the desired interval
When using sigmoid or tanh, the value is still set to zero.
Eg the logistic and tanh functions are approximately linear over any small region.
CuDNNLSTM has a hardcoded tanh activation.
You can reverse the tanh activation, then apply whatever activation u want!
For example, on tanh, I look for tanh, or it seems the convention is Tanh, in the source code:
The result: tensorflow/core/ops/math_ops.cc:REGISTER_OP("Tanh").UNARY_COMPLEX(); looks like what we are looking for.
We know tanh is a coefficient-wise operation, and that, with a bit more search, tensorflow/core/kernels/cwise_op_tanh.cc and tensorflow/core/kernels/cwise_op_gpu_tanh.cu.cc are the only kernels registering tanh.
Looking through the code, the implementation is functor::tanh, which is a wrapper for the actual implementation, usually delegated to the Eigen linear algebra library.
The cause turned out to be entirely tanh.
I don't know why I didn't think of this last night, but this morning, I commented out the tanh, and here's what I get:
So the conclusion is that the Linux implementation of tanh is really lousy.
I'll have to implement my own or snag the tanh source from the BSD libc.
Is only used by some of the (trigonometric) functions (like sin, cos, but not tanh) - I'm just guessing here: based on how the function values are computed (e.g.
Switch to libm.a (for sin and tanh):

[code snippet]

As seen:

For sin, the linker complained (even without -fno-builtin)
For tanh, things went fine

From now on, I'm going to focus on the case that doesn't work.
At the output layer you should use tanh, as it matches the range of your output ([-1, 1]), but for hidden layers you should use ReLU or similar.
Do not use sigmoid or tanh for hidden layers (only in recurrent networks) as they will produce the vanishing gradient problem
My theory is that you have very large or small values going into your tanh function, this causes the  output to near 0 or 1 every time.
Using batch-normalisation will bring your values away from the extremities where the tanh output is only 0 or 1.
