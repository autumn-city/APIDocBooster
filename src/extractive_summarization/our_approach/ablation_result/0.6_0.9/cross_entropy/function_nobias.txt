torch.nn.functional.cross_entropy function combines log_softmax(softmax followed by a logarithm) and nll_loss(negative log likelihood loss) in a single function, i.e.  it is equivalent to F.nll_loss(F.log_softmax(x, 1), y).
Yes, the cross-entropy loss function can be used as part of gradient descent. 
In short, cross-entropy(CE) is the measure of how far is your predicted value from the true label. 
The cross here refers to calculating the entropy between two or more features / true labels (like 0, 1). 
And the term entropy itself refers to randomness, so large value of it means your prediction is far off from real labels. 
