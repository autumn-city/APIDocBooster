word to work is a technique in computer
science that allows you to do
mathematics with the word. For example
you can give this equation to a computer
which will be something like king minus
men plus woman
and computer will tell you the answer is
queen.
What? Isn't that mind-boggling?
This is super cool. And it works really
well
and I'm not making this up. So how can
computer do this
well think about this computers don't
understand text so
they understand numbers. So if there is a
way to represent a word
king in a number such that it can
accurately represent the meaning of the
word king.
Now that number cannot be one number so
you need to have
set of numbers and in mathematics set of
numbers are called vectors
so let's think about this how about we
represent working
into a vector which is just a bunch of
numbers
such that it can represent the meaning
of word
king accurately. Now think about king
king has a different property so there
are different ways of representing
the word king. For example king has
authority
king is rich usually. King
has a gender of male, okay.
Does king have a tail no
generally horse will have a tail right
so that answer will be
zero so what if we do this all right so
for authority
we'll give number one for tail we'll
give number zero because
king doesn't have tail for being rich
we'll give number one. One meaning super
rich
zero meaning very poor and for gender
let's say we give number minus one minus
one is
male and one is female now we came up
with this vector one zero one minus one
that represents the meaning of word
king. You can do similar thing with
another word for example horse for horse
the property tail will be one
but the other property such as authority
being rich etc
will be close to zero and if you do this
for
all type of different words in your
vocabulary you will be able to do
a math so let me just show you a very
simple example here.
Let's say I have a story of king and
queen and I want to represent
all the words in that story with
word vectors here i have different
properties such as authority event
hashtag and so on and let's say there is
a word called battle.
For battle battle is an event so the
that value is one remaining values are
zero
horse has a tail that's why it's one
horse might have little authority
0.01 or might be a little rich 0.1
if it is a horse of a king
so and gender is 1. here like
values might not be 0 because i'll tell
you the reason behind that
a little later but when you have king
we already saw it's 1 0 0 1 -1
and for different words you can come up
with these
different type of vectors and once you
have the vectors you can do the
mathematics so now when i do king minus
men plus
woman just do a simple math 1 minus 0.2
plus 0.2 is 1
0 0 I'm taking individual elements okay
one minus point three is point seven
plus point two point nine and that
result vector is similar to a vector of
queen.
It is not exactly the same but it's
quite similar right .9 and 1
that's only difference
so you already saw how that math works
when you give this equation to computer
computer will be able to tell you that
the answer
is queen and that is pretty
powerful. Now
you don't want to hand code all these
properties for all these words
let's say you're doing doing natural
language processing for
all the text on wikipedia there are so
many thousands of
words and to come up with these kind of
properties for each of these words will
be very very difficult
so you don't want to hand craft it in
computer programming
you can use basically neural networks
to learn these feature vectors so
these
these numbers are called feature vectors
okay
so authority event has still are called
features
in the language of machine learning
and using neural networks you can learn
these feature vectors.
You don't have to hand code it so let's
see how that is done
and by the way when you learn these
feature vectors one interesting thing
that will happen is you will not
know what these feature vectors are you
will not know that this one means
authority
but it will all work magically
so what you do is you take a fake
problem
and you try to solve it using neural
network and as a side effect
you get word embedding now what does
that mean
so what is a fake problem let's say the
fake problem is you want to
find a missing word in a sentence.
That's your fake problem well the
problem is real but
our goal is not to learn what is the
missing word in a sentence our goal is
to learn
word embedding as a side effect.
Say there is a story of great king
Ashoka
you know he was the king in India in
ancient times
and when you're reading this story you
can take a fake problem which is
complete this sentence so here when I
say order his minister see based on
this tax this my taking order his
minister the emperor order his minister
I can say
the missing word is king or emperor
and when I give this task of filling in
the missing word
to a computer as a side effect
this is a very important keyword side
effect
as a side effect it will learn the
vectors for king and emperor.
These are those feature vectors and once
you have vectors you can do math
you can say king is almost equal to
emperor.
So see you will be able to derive the
synonyms the antonyms. You can do
math such as king minus man plus woman
is equal to queen
and so on. So now
let's look into this problem a little
further for example you have this
sentence
eating something is not very is very
healthy
and if I ask you to fill in the missing
word.
well most likely you will say April and
walnut. Because that's
food and that's healthy pizza is also
food but it's not healthy so you won't
feel it and
forget truck eating talk is very healthy
are you crazy
similarly when you have the sentence the
likely keyword
will be rocket you are not going to say
NASA launched pizza
last month. So now
when you are in this process of
finding out the missing word you realize
one fact which is meaning of a word can
be inferred by a surrounding words.
If someone gives you surrounding words
so these surrounding words are also
called
context. So based on the context
you can figure out what that missing
word is.
So now let's take this paragraph
and we will
try to auto complete those missing words
and auto completing missing words is
really not the area of our interest it
is our fake problem
our area of interest is to learn the
word embeddings.
The vectors which can represent those
words.
So I will parse this paragraph and I
will take a window of
three words and here
I will say if I have a word lived
and a I can predict that there is a word
there
so I'm taking the second and third word
and trying to predict the first word
and these are my training samples so I
can move that window of three words
throughout the paragraph
generate all these training samples.
You see I generated all these training
samples.
And now this becomes a training set for
neural network.
So the words on the left hand side are
my x.
the word on the right hand side are my y.
and you feed x into neural network
and you want it to predict the word
worldwide.
Now if you have not seen my neural
network video please go watch it
you need to have some understanding of
neural network
in order to understand things which I am
going to explain in this video.
So if you don't know already what is
neural network pause the video right now.
I'm going to provide my neural network
video link in a video description
below. So just get some basic
understanding
assuming you have the basic
understanding now let's go back to our
problem which is you have training
samples.
And by the way this problem is called
self-supervised because
all you had was this paragraph you did
not have like X and Y
you had a paragraph from that paragraph.
You generated these training samples
now let's try to train our neural
network using each of these
training samples. So let's say my first
sample is order his so order his is an
input
based on that you want to predict
working which is an output
now you can build a neural network that
looks something like this
the input layer will have
a one hot encoded vector so let's say
there are 5000
words in my vocabulary
then there will be a vector of size 5000
and
only one of them will be one so if the
word is ordered
the value of order will be one and
remaining
numbers will be zero. And same thing is
for his see here's this one and
remaining numbers are zero and the size
of this vector is let's say five
thousand. 5000 is let's say
vocabulary. Vocabulary means
unique words in your text corpus
or in the you know text problem that
you're trying to solve
and in the hidden layer here I have put
4 neurons and these 4 neurons
are the size of my embedding vector.
Now size of embedding vector could be
anything like there is no golden rule I
just selected 4.
but it's a hyper parameter to your
neural network it could be
5 10 200 anything this is something you
learn
using trial and error in the output
layer.
I will have 5000
size vector
and when I feed this training sample
into my neural network what happens is
these weights or the edges will have
random weights
so using random weights it will predict
some output which will be wrong most
likely.
King is the right output so you all know
how neural network works how back error
propagation works
you compare your actual output which is
why
with your predicted output y hat you
take a loss
so loss is a difference between your
predicted output and actual output
and you back propagate again if you
don't know about back propagation
i have some videos you can check it out
but when you back propagate
essentially what you're doing is you are
adjusting all these weights
and then you take us and then you take a
second sample third sample
you take all 10 000 or 1 million samples
and your goal is to train a networks in
such a way that
when you input order is the network
accurately finds out
that it is a king so you expect
one to be here actually you expect in
the emperor here also you expect it to
be 1 because it could be anything
now you take the second sentence which
is
emperor ordered his and you're not
taking the whole sentence you're taking
a window of size three
it could be window of size four or five
depends on you how you want to
experiment.
But same thing happens here where you
feed
and input the neural network will find
out the output it will compare it with
the actual output
there is a loss and it will back
propagate.
And it takes the third sentence you know
from Kalinga and battle you're trying to
predict that the missing word is
after same thing there is predicted
output actual output loss
back propagation and eventually when you
have
done you are done feeding your
let's say 1 million elements and let's
say you run 10 or 15 or 50 epochs
and your neural network is strain at
that point
the word vector for king would be these
weights.
w1 w2 w3 w4 so those weights are nothing
but a trained word vector
and this vector will be very similar to
a vector of
emperor. So the vector for the emperor
will be w5 w6 w7 w8
just think about it it will be similar
because the input is same so
here order and his both for king and
emperor the input is same
so when the input is same you expect
that
or these weights will also be similar
and hence the vector for king and
emperor will be very similar
using this approach. This approach is
called continuous bag of words
so here you have a context which is
order his
and based on that context you are trying
to predict
target which is king. There is a
second methodology called skip gram
in script gram we do reverse we have a
target working
and based on that we try to predict
order his.
Again, predicting target from context in
context from target these are fake
problems you know we are not interested
in solving this problem
but while we solve those problems as a
side effect
we get word embedding so we are more
interested in learning word embeddings
just to summarize word to Vec
is not a single method but it it could
be using one of the 2
techniques which is either continuous
back of words or skip gram
to learn word embeddings see the word
word to vec means
convert word to a vector so word to vec
is a
revolutionary invention in the field of
computer science
which allows you to represent words in
an
in a vector in a very accurate way so
that you can do mathematics with it.
Let's talk about script gram so in skip
gram
I have inverted my neural network
diagram so here you can see it's exactly
reversed then
the c bar here you have the word
king based on that you're trying to
predict order his
and you will do the same thing. You will
feed one sentence
calculate the expected output. Compare it
with the
actual output do back propagation and so
on.
And in that process you learn the word
embeddings for each of these
words you know. There are 5000 words less
in our vocabulary
so the embedding for Ashoka will be w1
to w4
the embedding would be m for emperor
would be w6 to w9.
So when you're using skip gram the word
embedding
is a layer between the input layer and
the hidden layer
in the c bar it was the weights between
hidden layer and the output layer.
You can do wonderful things with word to
wack such as
usa minus Washington DC plus Delhi do
you have any guess
pause the video this is a quiz for you
well it is India okay any guess on this
one
wow you all are so smart yes apple
so computers can do this kind of
mathematics
this works really well.
I took this diagram from the towards
science
article you can represent these vectors
in a vector space.
So here I'm showing you three
dimensional vector space
that could be n dimensional vector space
and you know using a method called
Disney you can change that n-dimensional
vector space to two-dimensional vector
space
and you will find that
the word the relationship between
walking and walked will be similar to
swimming and swam.
So once you have learned this
relationship when you give a word
swimming to a computer
it will tell you the output is swam
It can also figure out a relationship
basically it will say okay
whatever is Madrid to Spain the
same thing is Rome to Italy
so it can draw it can learn
these kind of complex relationships
so that was all the theory behind word
to work
I hope you like this video if you did
please share it with your friends who
are confused about word embeddings
word to vector and if you have any
question post in a video comment below.
In the next video we will be looking at
the coding part
where using python we will see
how word to vec works and we'll run
some code
to see this magic in works. Thank you.