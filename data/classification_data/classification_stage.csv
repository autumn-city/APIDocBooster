text,text2,label
"In a ViewGroup the ViewGroup has the ability to steal the touch events in his dispatchTouchEvent()-method, before it would call dispatchTouchEvent() on the children.",dispatchTouchEvent,1
The ViewGroup would only stop the dispatching if the ViewGroup onInterceptTouchEvent()-method returns true.,dispatchTouchEvent,1
The difference is that dispatchTouchEvent()is dispatching MotionEvents and onInterceptTouchEvent tells if it should intercept (not dispatching the MotionEvent to children) or not (dispatching to children).,dispatchTouchEvent,1
The dispatchTouchEvent() method of a ViewGroup uses onInterceptTouchEvent() to choose whether it should immediately handle the touch event (with onTouchEvent()) or continue notifying the dispatchTouchEvent() methods of its children.,dispatchTouchEvent,1
"dispatchTouchEvent is actually defined on Activity, View and ViewGroup.",dispatchTouchEvent,1
Think of it as a controller which decides how to route the touch events.,dispatchTouchEvent,1
For ViewGroup.dispatchTouchEvent things are way more complicated.,dispatchTouchEvent,1
It needs to figure out which one of its child views should get the event (by calling child.dispatchTouchEvent).,dispatchTouchEvent,1
This is basically a hit testing algorithm where you figure out which child view's bounding rectangle contains the touch point coordinates.,dispatchTouchEvent,1
dispatchTouchEvent handles before onInterceptTouchEvent.,dispatchTouchEvent,1
•ViewGroup.onInterceptTouchEvent(MotionEvent) - This allows a   ViewGroup to watch events as they are dispatched to child Views.,dispatchTouchEvent,1
How a ViewGroup handles touch:,dispatchTouchEvent,1
"ViewGroup.dispatchTouchEvent()  onInterceptTouchEvent()  Check  if  it  should  supersede  children Passes  ACTION_CANCEL  to  active  child If it returns  true  once,  the ViewGroup consumes  all  subsequent  events  For  each  child  view  (in  reverse  order  they  were  added)       If  touch  is  relevant  (inside  view),  child.dispatchTouchEvent() If  it is not  handled  by  a previous,  dispatch  to  next  view  If  no  children  handles the  event, the listener  gets  a  chance       OnTouchListener.onTouch()  If there is no listener, or its not handled       onTouchEvent()",dispatchTouchEvent,1
Intercepted  events  jump  over the child step,dispatchTouchEvent,1
The event dispatch in Android starts from Activity->ViewGroup->View.,dispatchTouchEvent,1
"But before it can dispatch the event to the appropriate child view, the parent can spy and/or intercept the event all together.",dispatchTouchEvent,1
"When using dispatchTouchEvent , you take all touches in your activity, if you want only detect one touch, you have to filter the touch by its type, you can do this using the MotionEvent parameter.",dispatchTouchEvent,1
" onCreate is responsible for creating your tables inside your SQLite database, and onUpgrade will be responsible for upgrading your database if you do make any changes.",onCreate,1
This method will not be called if you’ve changed your code and relaunched in the emulator.,onCreate,1
"After executing in first time deployment, this method will not be called onwards.",onCreate,1
onCreate() is called when you call getWritableDatabase() or getReadableDatabase() on the helper and the database file does not exist.,onCreate,1
"If the file is already there and the version number is the requested one, no callback such as onCreate() is invoked.",onCreate,1
" Each time I say to the Android SDK I want to get a connection to my database, Android will determine whether the database exists or not.",onCreate,1
"If it doesn't exist yet, Android will call the onCreate method.",onCreate,1
" In  particular, the starter code gives us   one function or one method that is already being  overridden.",onCreate,1
"And that's called onCreate, to the   Android system will automatically invoke or call  this function when our application is starting up   when it's creating our screen.",onCreate,1
"However, this method will only be called if the SQLite file is missing in your app’s data directory (/data/data/your.apps.classpath/databases).",onCreate,1
1) onCreate(): This method invoked only once when the application is start at first time .,onCreate,1
So it called only once,onCreate,1
"As already explained in the older answer, if the database with the name doesn't exists, it triggers onCreate.",onCreate,1
If you want onCreate() to run you need to use adb to delete the SQLite database file.,onCreate,1
onCreate() is only run when the database file did not exist and was just created.,onCreate,1
Sqlite database override two methods,onCreate,1
"2)onUpgrade() This method called when we change the database version,then this methods gets invoked.It is used for the alter the table structure like adding new column after creating DB Schema",onCreate,1
Database version is stored within the SQLite database file.,onCreate,1
Another issue is that SQLiteOpenHelper lifecycle methods such as onCreate() are invoked inside an transaction.,onCreate,1
" It's a late  initialization because we're going to initialize   it inside of the onCreate method and not in the  constructor, which is why it's late initialization   is a variable and that we're calling it at base  mount the convention I follow that the name of   the variable is exactly equal to the name of the  ID.",onCreate,1
" After the set content   view, we are going to say at base mount find  view by ID et base amount.",onCreate,1
" And then because SQLiteOpenHelper class is an abstract class, you have to implement to abstract method here.",onCreate,1
Those are onCreate and onUpgrade.,onCreate,1
" And you can implement all the predefined methods, which are already provided by SQLiteOpenHelper class.",onCreate,1
 These methods will be called automatically by the Android SDK.,onCreate,1
 Next we have the onCreate method.,onCreate,1
This is where the SQL we've been talking about and our new contract class are used together.,onCreate,1
" The first time the database is used, SQLiteOpenHelper's onCreate will be called.",onCreate,1
 We need to write the correct SQL statement string so that we can create the table sunshine needs.,onCreate,1
 We then have the system execute this SQL by calling db dot execSQL.,onCreate,1
onCreate() method is creating the tables you’ve defined and executing any other code you’ve written.,onCreate,1
SQLiteOpenHelper should call the super constructor.,onCreate,1
The onUpgrade() method will only be called when the version integer is larger than the current version running in the app.,onCreate,1
"If you want the onUpgrade() method to be called, you need to increment the version number in your code.",onCreate,1
onCreate is called for the first time when creation of tables are needed.,onCreate,1
We need to override this method where we write the script for table creation which is executed by SQLiteDatabase.,onCreate,1
"[SQLiteOpenHelper (hyper-link)] [onCreate() (hyper-link)] and [onUpgrade() (hyper-link)] callbacks are invoked when the database is actually opened, for example by a call to [getWritableDatabase() (hyper-link)].",onCreate,1
The database is not opened when the database helper object itself is created.,onCreate,1
SQLiteOpenHelper versions the database files.,onCreate,1
"As an implication, you should not catch SQLExceptions in onCreate() yourself.",onCreate,1
onUpgrade() is only called when the database file exists but the stored version number is lower than requested in the constructor.,onCreate,1
The onUpgrade() should update the table schema to the requested version.,onCreate,1
"When changing the table schema in code (onCreate()), you should make sure the database is updated.",onCreate,1
Delete the old database file so that onCreate() is run again.,onCreate,1
This is often preferred at development time where you have control over the installed versions and data loss is not an issue.,onCreate,1
Some ways to delete the database file:  Uninstall the application.,onCreate,1
Use the application manager or adb uninstall your.package.name from the shell.,onCreate,1
Clear application data.,onCreate,1
Use the application manager.,onCreate,1
Increment the database version so that onUpgrade() is invoked.,onCreate,1
This is slightly more complicated as more code is needed.,onCreate,1
"For development time schema upgrades where data loss is not an issue, you can just use execSQL(""DROP TABLE IF EXISTS <tablename>"") in to remove your existing tables and call onCreate() to recreate the database.",onCreate,1
"For released versions, you should implement data migration in onUpgrade() so your users don't lose their data.",onCreate,1
(OnCreate() is not executed when the database already exists),onCreate,1
Points to remember when extending SQLiteOpenHelper,onCreate,1
override onCreate and onUpgrade (if needed),onCreate,1
onCreate will be invoked only when getWritableDatabase() or getReadableDatabase() is executed.,onCreate,1
And this will only invoked once when a DBName specified in the first step is not available.,onCreate,1
You can add create table query on onCreate method,onCreate,1
Whenever you want to add new table just change DBversion and do the queries in onUpgrade table or simply uninstall then install the app.,onCreate,1
Max pooling does not dilute the location of the maximum pixel - instead consider it as a way of downsizing.,MaxPool2d,1
Max pooling is just a way to reduce dimensionality of the problem such that your problem fits into device memory.,MaxPool2d,1
A nice side property is that it pools the strongest acitvations from your feature map.,MaxPool2d,1
"If max-pooling is done over a 2x2 region, 3 out of these 8 possible configurations will produce exactly the same output at the convolutional layer.",MaxPool2d,1
"For max-pooling over a 3x3 window, this jumps to 5/8.",MaxPool2d,1
Maximum pooling produces the same depth as it's input.,MaxPool2d,1
With that in mind we can focus on a single slice (along depth) of the input conv.,MaxPool2d,1
"Max pooling does nothing more than iterate over the input image and get the maximum over the current ""subimage"".",MaxPool2d,1
Max pooling decreases the dimension of your data simply by taking only the maximum input from a fixed region of your convolutional layer.,MaxPool2d,1
In the case of bbox prediction it also reduces the number of proposed regions for bboxes.,MaxPool2d,1
Which later in a non-maximum surpression step would kill all redundant proposed bbox locations.,MaxPool2d,1
" And if you are familiar with Max pooling, then you know this is going to cut our image dimensions in half.",MaxPool2d,1
There are 8 directions in which one can translate the input image by a single pixel.,MaxPool2d,1
"They are considering 2 horizontal, 2 vertical and 4 diagonal 1-pixel shifts.",MaxPool2d,1
That gives 8 in total.,MaxPool2d,1
" That means, it tries to make the learning rate inversely proportional to a sane cumulated history.",RMSprop,1
RMsprop keeps the exponentialy decaying average of squared gradients.,RMSprop,1
RMSprop uses a momentum-like exponential decay to the gradient history.,RMSprop,1
Gradients in extreme past have less influence.,RMSprop,1
It modiﬁes AdaGrad optimizer to perform better in the non-convex setting by changing the gradient accumulation into an exponentially weighted moving average.,RMSprop,1
" So, that is the problem with the Adagrad  algorithm and in case of RMSProp instead   of using this cumulative or acumulative sum of  squared gradients the RMSProp takes exponentially   decaying average of the squared gradient.",RMSprop,1
"And, it  does not consider the extreme past histories while   accumulating the sum of square gradient.",RMSprop,1
"And, as  a result of this the algorithm converges rapidly,   once it reaches once your vector reaches  locally convex ball short of surface that .",RMSprop,1
"And,   what you can do is we can assume this point once  it reaches that locally convex error surface,   that as if your Adagrad algorithm is initialized  at that point within that locally convex ball.",RMSprop,1
"However, RMSProp does not keep a moving average of the gradient.",RMSprop,1
"But it can maintain a momentum, like MomentumOptimizer.",RMSprop,1
" So, Adagrad got stuck when it was close to convergence because the learning rate was killed and it was no longer able to move in a direction of b, but for RMSProp, it overcomes this problem by not growing the denominator very aggressively, ok. Now, can you think of any further modifications?",RMSprop,1
" Actually it is taking a moving average of; there is the same as the momentum base role, right?",RMSprop,1
" This is the same as what RMSProp suggested that you divide the learning rate by a cumulated history of gradients, right?",RMSprop,1
" So, in today’s class we will  talk about two more algorithms,   one of them is RMSProp and the other  one is Adam and we will also see a   very closely related algorithm which  is very closely related to RMSProp.",RMSprop,1
" So, the algorithm  that we will talk about is what is RMSProp   which tries to address this problem  of Adagrad algorithm that is vanishing   learning rate as the time increases  as the number of iteration proceeds.",RMSprop,1
"So, what is this RMSProp algorithm does  is instead of taking the accumulative sum   of squares of the gradients of the sum of the  squares of the past and gradients or this past   gradient starts from time t equal to 0.",RMSprop,1
" So in   case of RMSProp, the scaling factor is not  the cumulative sum of gradient histories,   but it is the exponentially decaying  average of the squared gradients.",RMSprop,1
"So, if I go to the updation algorithm is in  RMSProb, the updation algorithm will be like this;   you will find that you will compute the  gradient in the same way as we have done in   case of Adagrad, right.",RMSprop,1
" So, in  case of RMSProp we did not have any concept   of momentum.",RMSprop,1
We are using gradient descent to calculate the gradient and then update the weights by backpropagation.,RMSprop,1
"There are plenty optimizers, like the ones you mention and many more.",RMSprop,1
The optimizers use an adaptive learning rate.,RMSprop,1
With an adaptive loss we have more DoF to increase my learning rate on y directions and decrease along the x direction.,RMSprop,1
They don't stuck on one direction and they are able to traverse more on one direction against the other.,RMSprop,1
Adam with�beta1=1�is equivalent to RMSProp with�momentum=0.,RMSprop,1
The argument�beta2�of Adam and the argument�decay�of RMSProp are the same.,RMSprop,1
A detailed description of rmsprop.,RMSprop,1
maintain a moving (discounted) average of the square of gradients  divide gradient by the root of this average  can maintain a momentum,RMSprop,1
" So, your  basically the operation that was done in Adagrad   algorithm is r t, the scaling factor which is  1 upon square root of epsilon plus r t i.",RMSprop,1
"So,   if you go for component wise this r t i is  nothing, but sum of g t i square of this   or let me put it as g tau square instead of  g t g tau square and you take the summation   of tau is equal to say 1 to t. So, you find  that this being a square term and which you   are going on adding.",RMSprop,1
"So, r t goes on increasing,  it monotonically increases , it does not reduce.",RMSprop,1
" We created our ADADELTA method to overcome the sensitivity to the hyperparameter selection as well as to avoid the continual decay of the learning rates."".",Adadelta,1
"The second good thing about AdaDelta is you don't have to choose the learning rate, it automatically computes it.",Adadelta,1
Adaptive gradient algorithms have learning rates for each parameter.,Adadelta,1
This is very helpful when you have models where some parameters might be more sparse (increase its learning rate) or not sparse (decrease its learning rate).,Adadelta,1
Adadelta is an adaptive learning rate method which uses exponentially decaying average of gradients.,Adadelta,1
"Adadelta optimizer has a way to adaptively change learning rate but still, it needs an initial value of learning rate.",Adadelta,1
"At step 0, the running average of these updates is zero, so the first update will be very small.",Adadelta,1
 If you have gone side-track just remember one thing we are going into so much hassle because normal gradient descent has its learning rate constant for the entire training phase... which is so LAME!,Adadelta,1
So the optimizers like AdaDelta have some techniques to vary the learning rate with every iteration that's it.,Adadelta,1
"As the first update is very small, the running average of the updates will be very small at the beginning, which is kind of a vicious circle at the beginning",Adadelta,1
"Looking at [Keras source code (hyper-link)], learning rate is recalculated based on decay like:",Adadelta,1
I suspect that decay is not intended to be used with Adadelta.,Adadelta,1
It just multiplies the variable updates (see [the update op implementation (hyper-link)]).,Adadelta,1
"For any ""automatic learning rate"" scheme, you can always scale the resulting updates by a constant (whether it's necessary to do so is a separate issue).",Adadelta,1
 To overcome this issue AdaDelta was born.,Adadelta,1
"In the AdaDelta paper by Matthew Zeiler, he talked about this drawback of AdaGrad saying ""due to the continual accumulation of squared gradients in the denominator, the learning rate will continue to decrease throughout training, eventually decreasing to zero and stopping training completely.",Adadelta,1
 This was made possible due to this rho hyperparameter.,Adadelta,1
It is also known as 'Decay Constant'.,Adadelta,1
"Instead of writing it with this equation, I have written it separately because we will need delta theta in this equation.",Adadelta,1
"So, this term over here is the learning rate calculated by AdaDelta.",Adadelta,1
 We are dividing square root of delta x with the square root of alpha... again this epsilon is a very small number to avoid zero division error.,Adadelta,1
Then we can sum it with our theta term... normal gradient descent stuff!,Adadelta,1
"At last, we will update our initialized delta x term by rho multiplied by delta x of the previous iteration plus one minus rho into update square.",Adadelta,1
" So, what AdaDelta does is in case of   RMSProp you are taking the exponentially decaying  average of the squared gradient; AdaDelta instead   of taking the exponentially decaying average of  squared gradient it computes the moving window   average.",Adadelta,1
"So, you can take a more window size of  say W. So, when you compute v t, v t is computed   over a past window size of W. So, if I take the  window size W is equal to say 5, in that case in   order to compute say v 10 it will take the first  5; that means, it will take v 10 v 9 v 8 v 7 and   v 6 or the say s 6 s 7 s 8 s 9 and s 10.",Adadelta,1
"So, you are computing average over past   samples which are within this window size of W.  So, this is what is moving window average.",Adadelta,1
Use an adaptive gradient algorithm like Adam or Adadelta or RMSProp.,Adadelta,1
The rule is related to updates with decay.,Adadelta,1
The full algorithm from the [paper (hyper-link)] is:,Adadelta,1
The issue is that they accumulate the square of the updates.,Adadelta,1
Here is my code to play a bit with the Adadelta optimizer:,Adadelta,1
The thing you need to know about AdaDelta is the general context of online machine learning.,Adadelta,1
"Online machine learning is where you get your data one-at-a-time (and thus have to update your model's parameters as the data comes in), as opposed to batch machine learning where you can generate your machine learning model with access to the entire dataset all at once.",Adadelta,1
" So where onCreate is called when the activity is first created, onPause it's going to be called when you leave the activity.",onPause,1
whenever a new ACTIVITY starts the previous activity's onPause will be defiantly called in any circumstances.,onPause,1
onPause is guaranteed to be called on Activity A (No matter whether Activity B is non-full-sized transparent or full-sized).,onPause,1
onPause() is always called on your Activity if it is in the foreground when Android wants to do something else.,onPause,1
It may start another Activity which may result in your Activity's onStop() getting called.,onPause,1
It may just call onResume() on your activity.,onPause,1
It may just kill your process without calling any more of your lifecycle methods.,onPause,1
"Since onStop() is not guaranteed to be called, you can't always do in onStop() what is done in onPause().",onPause,1
What is the good reason for always having onPause() before onStop().,onPause,1
We can do in onStop() what is done in onPause().,onPause,1
 onPause is called when your activity is about to resume any previous activity.,onPause,1
onPause() is called when your activity is no longer at the top of the activity stack.,onPause,1
"A Dialog by itself is not an Activity, so will not replace the current Activity at the top of the stack, so will not cause anything to pause.",onPause,1
"A dialog (lower-case) does not need to be implemented by a Dialog class, however.",onPause,1
"If you can still see any part of it (Activity coming to foreground either doesn't occupy the whole screen, or it is somewhat transparent), onPause() will be called.",onPause,1
"**I am not referring to an Android Dialog here, rather a conceptual idea of something that pops up and only obscures part of the user screen.",onPause,1
onPause() is always called.,onPause,1
This is guaranteed.,onPause,1
If you need to save any state in your activity you need to save it in onPause().,onPause,1
"onStop() may be called after onPause(), or it may not.",onPause,1
Depends on the situation.,onPause,1
 In onPause it could be possible when it is either about to resume a previous activity or it is partially visible but user is leaving the activity.,onPause,1
"This is typically considered as the best state to commit unsaved changes to persistent data, stop animations and anything that consumes resources.",onPause,1
 So use onPause or onStop to save the data or state.,onPause,1
" And onPause it will be onPause, onResume, onResume.",onPause,1
And start.,onPause,1
" [SOUND] onPause indicates that the activity has lost focus, followed by onStop when the app is no longer visible.",onPause,1
" Alternatively, through the static registration, you can also register and unregister a receiver at one time via the context class, Register receiver method either in onCreate, or in onResume activity methods.",onPause,1
 And unregister receiver methods either in onDestroy or in onPause activity methods.,onPause,1
"Practically, one should consider the difference between “onPause()” and “onPause() + onStop()”.",onPause,1
Whenever some new activity occurs and occupies some partial space of the Screen.,onPause,1
So your previously running activity is still visible to some extent.,onPause,1
"In this Case, the previously running activity is not pushed to Back Stack.",onPause,1
"So, here only onPause() method is called.",onPause,1
onPause()- Screen is partially covered by other new activity.,onPause,1
The Activity is not moved to Back Stack.,onPause,1
onPause() + onStop()- Screen is fully covered by other new activity.,onPause,1
The Activity is moved to Back Stack.,onPause,1
1- a part of previous activity is visible or the new activity is transparent: only onPause will be called.,onPause,1
"NOTE 2: if its an Activity whose theme is set to a dialog, the behavior will be just like a normal activity.",onPause,1
NOTE 3: apparently a system dialog like permission dialog since marshmallow will cause onPause.,onPause,1
"No, if some activity comes into foreground, that doesn't necessarily mean that the other activity is completely invisible.",onPause,1
onStop will only be called when Activity A is completely overridden by full-sized Activity B.,onPause,1
So in onPause you can save an Activity's state or some other useful info if required.,onPause,1
Being in the foreground means that the activity has input focus.,onPause,1
"It is simple: BatchNorm has two ""modes of operation"": one is for training where it estimates the current batch's mean and variance (this is why you must have batch_size>1 for training).",BatchNorm1d,1
"The other ""mode"" is for evaluation: it uses accumulated mean and variance to normalize new inputs without re-estimating the mean and variance.",BatchNorm1d,1
In this mode there is no problem processing samples one by one.,BatchNorm1d,1
" Alright, so batch normalization, or in short, batch norm goes  back to a paper published in 2015, called batch  normalization, accelerating deep network training by reducing  internal covariate shift.",BatchNorm1d,1
Pytorch does its batchnorms over axis=1.,BatchNorm1d,1
But it also has tensors with axis=1 as channels for convolutions.,BatchNorm1d,1
"BatchNorm1d can also handle Rank-2 tensors, thus it is possible to use BatchNorm1d for the normal fully-connected case.",BatchNorm1d,1
" In practice,  people nowadays, it's more common to actually recommend if  you use dropout to recommend having batch norm after the  activation.",BatchNorm1d,1
" Yeah, and also  note that now when we use batch norm, batch norm has learnable  parameters.",BatchNorm1d,1
"So if we use batch norm in a given layer, it has we  have an additional two vectors that have the same dimension as  the bias vector, right.",BatchNorm1d,1
"So if we have, we use batch norm here in  this layer, we will have two, four dimensional vectors, like  this bias vector here would also be four dimensional, right,  because there's one bias for each in layer activation.",BatchNorm1d,1
what does [BatchNorm1d (hyper-link)] do mathematically?,BatchNorm1d,1
" And also, we will talk briefly about how batch norm  behaves during inference, because yeah, you know that  during training, we have mini batches, but that's not  necessarily the case if we want to use our network for  prediction.",BatchNorm1d,1
" But again, this is again, highlighting  train and evil are important here that we during training set  our model into training mode, because that's where batch norm  will compute the running mean and the running variance, I will  talk about this in the next slide.",BatchNorm1d,1
"So here, batch norm will  actually compute some running statistics during training.",BatchNorm1d,1
" So usually, practice people keep a moving average of both the  mean and the variance during training.",BatchNorm1d,1
"So you can think of it  as also as the running mean, how it's computed is by having a  momentum term, it's usually a small value like point one.",BatchNorm1d,1
"And  this is multiplied by the running mean from the previous  on epoch, or sorry, previous mini batch.",BatchNorm1d,1
"And then what you so  you have this, this term, this is like the running mean times  momentum term, this is a point one value.",BatchNorm1d,1
"And then you have one  minus the momentum, this is like a point nine value, then plus  yet plus point nine times the current sample mean.",BatchNorm1d,1
"So that's  the mini batch mean, and you just do the same thing also for  the running variance.",BatchNorm1d,1
"So here, essentially, this is just like a  moving average or running mean.",BatchNorm1d,1
And you do the same thing for  the variance.,BatchNorm1d,1
 That's because there is a  slightly different version of batch norm for convolutional  networks.,BatchNorm1d,1
We will discuss this in the convolutional network  lecture where this would be called batch norm 2d for the  convolution networks.,BatchNorm1d,1
"So to keep them apart, this is called batch  norm 1d.",BatchNorm1d,1
 And how  you can think of it as an additional normalization layer.,BatchNorm1d,1
" So  yeah, here, that's the first step of batch norm, there are  two steps.",BatchNorm1d,1
So the first step is to normalize the net inputs.,BatchNorm1d,1
So the j is the  feature index again.,BatchNorm1d,1
So you can actually use batch norm for any  type of input.,BatchNorm1d,1
So we will also see there is a two dimensional  version for that for convolutional networks later on.,BatchNorm1d,1
" So let's  say we have, yeah, the J feature.",BatchNorm1d,1
"And if I go back, so if  you consider this activation here, what are the features, so  the features are all essentially all the previous layer  activations, right?",BatchNorm1d,1
So all these go into that activation.,BatchNorm1d,1
So all  of these here are the features of this activation here.,BatchNorm1d,1
So J  really is the index over the activations from the previous  layer.,BatchNorm1d,1
" But in the regular batch norm in the 1d version, we were  computing things for each feature.",BatchNorm1d,1
So we were computing  this gamma and beta for each feature over the batch  dimension.,BatchNorm1d,1
" There are places of computing myu and Sigma is running average, again you can do that as well as you know some exponentially varied average schemes are available but this is the way to, during testing you will calculate myu and Sigma for every layer, this myu and Sigma are for the activations of every layer, every neuron and every layer.",BatchNorm1d,1
 Calculated by running it through the entire forward pass dataset.,BatchNorm1d,1
 That will be added computation or or you can just maintain a running average during training.,BatchNorm1d,1
Which one to use depends on the dimensionality of input data.,BatchNorm1d,1
"BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice.",BatchNorm1d,1
Core ML does not have 1-dimensional batch norm.,BatchNorm1d,1
"If you want to convert this model, you should fold the batch norm weights into those of the preceding layer and remove the batch norm layer.",BatchNorm1d,1
The getBaseContext() is the method of ContextWrapper.,getBaseContext,1
"And ContextWrapper is, ""Proxying implementation of Context that simply delegates all of its calls to another Context.",getBaseContext,1
"Can be subclassed to modify behavior without changing the original Context.""",getBaseContext,1
(as per javadocs),getBaseContext,1
So this is used to delegate the calls to another context.,getBaseContext,1
The method getBaseContext() is only relevant when you have a ContextWrapper.,getBaseContext,1
The method getBaseContext() can be used to access the “base” Context that the ContextWrapper wraps around.,getBaseContext,1
"You might need to access the “base” context if you need to, for example, check whether it’s a Service, Activity or Application:",getBaseContext,1
getBaseContext() -  If you want to access Context from another context within application you can access.,getBaseContext,1
Or if you need to call the “unwrapped” version of a method:,getBaseContext,1
"The answer by Waqas is very clear and complete, however I'd like to further clarify the difference between using this vs. getBaseContext(), or getApplication() vs. getApplicationContext().",getBaseContext,1
"Both Activity and Application extend not Context itself, but ContextWrapper, which is a",getBaseContext,1
"""Proxying implementation of Context that simply delegates all of its calls to another Context"".",getBaseContext,1
"I doubt it makes any practical difference, though.",getBaseContext,1
The same logic applies to getApplication() vs. getApplicationContext().,getBaseContext,1
getApplicationContext(),getBaseContext,1
this is used for application level and refer to all activities.,getBaseContext,1
 I'm just going to create the table and its structure.,onCreate,2
"To do that, I'll remove the TODO comment, and I'll use the database argument that's being passed in.",onCreate,2
"It's named db, and I'll call a method called execute SQL or execSQL for short, and I'll pass in my constant that contains the SQL command that will create the table, that'll be TABLE_CREATE.",onCreate,2
The version number is the int argument passed to the [constructor (hyper-link)].,onCreate,2
"In the database file, the version number is stored in [PRAGMA user_version (hyper-link)].",onCreate,2
"If onCreate() returns successfully (doesn't throw an exception), the database is assumed to be created with the requested version number.",onCreate,2
"So when the database helper constructor is called with a name (2nd param), platform checks if the database exists or not and if the database exists, it gets the version information from the database file header and triggers the right call back",onCreate,2
catch is the constructor,onCreate,2
" I'll pass in the db argument, and now I've recreated the table structure.",onCreate,2
"super(context, DBName, null, DBversion); - This should be invoked first line of constructor",onCreate,2
"When we create DataBase at a first time (i.e Database is not exists) onCreate() create database with version which is passed in  SQLiteOpenHelper(Context context, String name, SQLiteDatabase.CursorFactory factory, int version)",onCreate,2
" So just to summarize, the hyperparameters for pooling are f, the filter size and s, the stride, and maybe common choices of parameters might be f equals two, s equals two.",MaxPool2d,2
"This is used quite often and this has the effect of roughly shrinking the height and width by a factor of above two, and a common chosen hyperparameters might be f equals two, s equals two, and this has the effect of shrinking the height and width of the representation by a factor of two.",MaxPool2d,2
"I've also seen f equals three, s equals two used, and then the other hyperparameter is just like a binary bit that says, are you using max pooling or are you using average pooling.",MaxPool2d,2
" If you want, you can add an extra hyperparameter for the padding although this is very, very rarely used.",MaxPool2d,2
 So the number of input channels is equal to the number of output channels because pooling applies to each of your channels independently.,MaxPool2d,2
"Adding padding is NOT an ""absolute must"".",MaxPool2d,2
"Sometimes it can be useful to control the size of the output so that it is not reduced by the convolution (it can also augment the output, depending on its size and kernel size).",MaxPool2d,2
" When you do max pooling, usually, you do not use any padding, although there is one exception that we'll see next week as well.",MaxPool2d,2
"But for the most parts of max pooling, usually, it does not use any padding.",MaxPool2d,2
You should try to put your learning rate at 0.001.,RMSprop,2
"The error clearly says, Tanh only takes 1 argument, a tensor.",Tanh,2
"Activation functions accept a single tensor, you are passing two random list elements.",Tanh,2
"So yes, lr is just starting learning rate.",Adadelta,2
"However, note that, by default, decay parameter for Adadelta is zero and is not part of the “standard” arguments, so your learning rate would not be changing its value when using default arguments.",Adadelta,2
learning_rate: A Tensor or a floating point value.,Adadelta,2
"If you really want to use Adadelta, use the parameters from the paper: learning_rate=1., rho=0.95, epsilon=1e-6.",Adadelta,2
"A bigger epsilon will help at the start, but be prepared to wait a bit longer than with other optimizers to see convergence.",Adadelta,2
To match the exact form in the original paper use 1.0.,Adadelta,2
"To print it after every epoch, as @orabis mentioned, you can make a callback class:",Adadelta,2
and then add its instance to the callbacks when calling model.fit() like:,Adadelta,2
"On the other hand, rho parameter, which is nonzero by default, doesn’t describe the decay of the learning rate, but corresponds to the fraction of gradient to keep at each time step (according to the [Keras documentation (hyper-link)]).",Adadelta,2
Although as you can see in tensorflow [source code (hyper-link)] to achieve the exact results of Adadelta paper you should set it to 1.0:,Adadelta,2
"Note that in the paper, they don't even use a learning rate, which is the same as keeping it equal to 1.",Adadelta,2
As per the documentation we need to specify num_features parameter which is the input size of tensor.,BatchNorm1d,2
"Batch normalization works when batch size is greater than 1, so an input of shape (1, 32) won't work.",BatchNorm1d,2
In most cases you should be safe with the default setting.,BatchNorm1d,2
"If you pass torch.Tensor(2,50,70) into nn.Linear(70,20), you get output of shape (2, 50, 20) and when you use BatchNorm1d it calculates running mean for first non-batch dimension, so it would be 50.",BatchNorm1d,2
" So yeah, I here insert batch norm after the  linear layer, notice that there's a one D, it may be  confusing.",BatchNorm1d,2
Why is there a one D?,BatchNorm1d,2
 But on by default bias is true if  you don't set anything and I found it was the same  performance.,BatchNorm1d,2
"For both functions, the d1 parameter is the number of features, and equals dim C of the input tensor.",BatchNorm1d,2
"Try a larger batch size, like 2.",BatchNorm1d,2
"It takes input of shape (N, *, I) and returns (N, *, O), where I stands for input dimension and O for output dim and * are any dimensions between.",BatchNorm1d,2
"PyTorch's CrossEntropyLoss expects unbounded scores (interpretable as logits / log-odds) as input, not probabilities (as the CE is traditionally defined).",cross_entropy,2
So the weights are changed to reduce CE and thus finally leads to reduced difference between the prediction and true labels and thus better accuracy.,cross_entropy,2
" So our loss, if we just care about disease  (we're going to be passed the three things)   we're just going to calculate cross_entropy  on our input versus disease.",cross_entropy,2
" So the way you read this colon means every row,   and then colon 10 means every column up to the  10th.",cross_entropy,2
"[ContextWrapper.getBaseContext() (hyper-link)]:  If you need access to a Context from within another context, you use a ContextWrapper.",getBaseContext,2
"That ""real"" context is what you get by using getBaseContext().",getBaseContext,2
getApplicationContext() - Returns the context for all activities running in application.,getBaseContext,2
getContext() - Returns the context view only current running activity.,getBaseContext,2
[View.getContext() (hyper-link)]:  Returns the context the view is currently running in.,getBaseContext,2
Usually the currently active Activity.,getBaseContext,2
[Activity.getApplicationContext() (hyper-link)]:  Returns the context for the entire application (the process all the Activities are running inside of).,getBaseContext,2
"Use this instead of the current Activity context if you need a context tied to the lifecycle of the entire application, not just the current Activity.",getBaseContext,2
Note that onCreate() is only invoked when the database file didn't exist so the DROP TABLE is really not needed.,onCreate,3
" And then that query you have to execute inside the callback method onCreate, which you have to override once you extend a class with the SQLiteOpenHelper class.",onCreate,3
So that query you have to execute inside the onCreate.,onCreate,3
Either you can execute it directly-- you can put the whole query directly inside a db.exeSQL.,onCreate,3
" And inside the SQLiteOpenHelper class, I will be putting a database-- I will be creating that whole structure.",onCreate,3
"There will be onCreate, there will be onUpgrade, insert query will be there, and update, delete-- all those queries will be there inside one class.",onCreate,3
That will be the subclass of SQLiteOpenHelper class.,onCreate,3
" If the database already exists, but I've indicated through the database version value that I'm changing the version, that is that I've incremented it, then the onUpgrade method will be called.",onCreate,3
" Then once I've dropped the table, in order to recreate it, I'll simply call my onCreate method.",onCreate,3
This is the one exception to what I just said that I wouldn't call the onCreate method directly.,onCreate,3
Below explanation explains onUpgrade case with an example.,onCreate,3
"Say, your first version of application had the DatabaseHelper (extending SQLiteOpenHelper) with constructor passing version as 1 and then you provided an upgraded application with the new source code having version passed as 2, then automatically when the DatabaseHelper is constructed, platform triggers onUpgrade by seeing the file already exists, but the version is lower than the current version which you have passed.",onCreate,3
Now say you are planing to give a third version of application with db version as 3 (db version is increased only when database schema is to be modified).,onCreate,3
"In such incremental upgrades, you have to write the upgrade logic from each version incrementally for a better maintainable code",onCreate,3
Example pseudo code below:,onCreate,3
Notice the missing break statement in case 1 and 2.,onCreate,3
This is what I mean by incremental upgrade.,onCreate,3
"Say if the old version is 2 and new version is 4, then the logic will upgrade the database from 2 to 3 and then to 4",onCreate,3
"If old version is 3 and new version is 4, it will just run the upgrade logic for 3 to 4",onCreate,3
" So  initially, the default tip is going to be 15%.",onCreate,3
"So with that diviner the constant before  we do anything in the listener, great in   the onCreate method, will they seek bar tip dot  progress is equal to initial tip percent.",onCreate,3
And we   also want to update the label appropriately.,onCreate,3
" Finally, we need to initialize the SDK.",onCreate,3
"So here, in the onCreate of our initial activity, we just type MobileAds.initialize, and you'll notice we can parse a context, and then there's a completion listener.",onCreate,3
"So if you're using mediation, you'll want to wait for the listener callback before loading an ad.",onCreate,3
 The first two steps are calling WindowCompat.set DecorFitsSystemWindows with false in your activities onCreate method and setting windowSoftInputMode to adjustResize in your activity manifest.,onCreate,3
"Together, these tell the platform that you're going to handle all insets yourself, including the soft keyboard insets.",onCreate,3
" As soon as you are ready, open the activity where you plan to show the Google Pay button and obtain a new instance of the paymentsClient inside of your onCreate method.",onCreate,3
 Now you see we've got it onCreate and an onUpgrade method.,onCreate,3
" In the onCreate method, we're going to start by creating a string to build the weather entry table using data defined within the weather entry contract.",onCreate,3
" Either you want to alter the table, or you want to drop the whole table which was previously used by the user.",onCreate,3
" And again, pass the control back to onCreate, because in onCreate, there will be a new table created for you.",onCreate,3
" There are some optional methods, also, which you can take a look at, like onDowngrade, onConfigure, or onOpen.",onCreate,3
 That will generate the Java class and add two methods named onCreate and onUpgrade.,onCreate,3
 Now let's go to the onCreate and onUpgrade methods.,onCreate,3
 I will never call these two methods directly.,onCreate,3
They'll only be called by the SDK.,onCreate,3
" In the onCreate method, you should add code that creates your database tables and if you like, you can also add code to add data.",onCreate,3
" So, that command is called and my table is created.",onCreate,3
" I'll pass in my LOGTAG constant and a literal string as a message, Table has been created, and that's all I need to do in the onCreate method.",onCreate,3
 So I'll move the cursor into the onCreate method.,onCreate,3
 I'll call it within this class but not from the rest of the application.,onCreate,3
 I'll call the onCreate method.,onCreate,3
" Now I'll go down to my onCreate method, and I'll place the cursor right here before I call the ArrayAdapter code and I'll instantiate the dbhelper object.",onCreate,3
"I'll say dbhelper = new, and then I'll use my new class constructor method and I'll pass in this as the context, and then I'll get a reference to the database.",onCreate,3
I'll call database = dbhelper.,onCreate,3
And then I'll call a method that this class has inherited from SQLite database.,onCreate,3
"This returns a reference to the connection to the database, and I'll be able to use that connection to do things like inserting data, retrieving data, updating, and deleting.",onCreate,3
"Simply by calling the method, that will trigger the onCreate method of my database open helper class, and in turn that will create the table structure.",onCreate,3
 Note how we're using all of the weather entry constants to write our SQL statements.,onCreate,3
"Suppose for the first time deployment , database version was 1 and in second deployment there was change in database structure like adding extra column in table.",onCreate,3
Suppose database version is 2 now.,onCreate,3
"For what it's worth, it's also a bad idea to catch exceptions in onCreate().",onCreate,3
"If the method returns successfully (doesn't throw), the framework thinks the database was created successfully.",onCreate,3
Pooling is of MUCH MORE IMPORTANCE in convnets.,MaxPool2d,3
" But you see, max pooling used much more in the neural network than average pooling.",MaxPool2d,3
"Pooling is not exactly ""down-sampling"", or ""losing spatial information"".",MaxPool2d,3
"Consider first that kernel calculations have been made previous to pooling, with full spatial information.",MaxPool2d,3
Pooling reduces dimension but keeps -hopefully- the information learnt by the kernels previously.,MaxPool2d,3
But if you look carefully at what's going on you may notice that the after first convolutional layer the dimension of your data might severely increase if you don't do the tricks like pooling.,MaxPool2d,3
"And, by doing so, achieves one of the most interesting things about convnets; robustness to displacement, rotation or distortion of the input.",MaxPool2d,3
"Invariance, if learnt, is located even if it appears in another location or with distortions.",MaxPool2d,3
"It also implies learning through increasing scale, discovering -again, hopefully- hierarchical patterns on different scales.",MaxPool2d,3
"And of course, and also necessary in convnets, pooling makes computation possible as number of layers grows.",MaxPool2d,3
" Here, I am going to use, sure you have a five by five input and we're going to apply max pooling with a filter size that's three by three.",MaxPool2d,3
So f is equal to three and let's use a stride of one.,MaxPool2d,3
"So in this case, the output size is going to be three by three.",MaxPool2d,3
"And the formulas we had developed in the previous videos for figuring out the output size for conv layer, those formulas also work for max pooling.",MaxPool2d,3
"So, that's n plus 2p minus f over s plus 1.",MaxPool2d,3
That formula also works for figuring out the output size of max pooling.,MaxPool2d,3
"But in this example, let's compute each of the elements of this three by three output.",MaxPool2d,3
"The upper left-hand elements, we're going to look over that region.",MaxPool2d,3
So notice this is a three by three region because the filter size is three and to the max there.,MaxPool2d,3
"So, that will be nine, and then we shifted over by one because which you can stride at one.",MaxPool2d,3
" So if you need to know more about max pooling, or you just need a refresher, same thing with padding here for zero padding, activation functions, anything like that, then be sure to check those episodes out and the corresponding deep learning fundamentals course on depot's or.com.",MaxPool2d,3
"Vanilla adaptive gradients (RMSProp, Adagrad, Adam, etc) do not match well with L2 regularization.",RMSprop,3
"Usually batch sizes around 40 gives better results, as for my experience training with 40 batch size for 3 epocs using default RMsprop gives around 89% accuracy.",RMSprop,3
Have you tried adjusting the learning rate of RMsprop optimizer ?,RMSprop,3
"try with a very small value first ( default is 0.001, in keras implementation) and try to increment it with factors of 10 or 100.",RMSprop,3
Adam is a recently proposed update that looks a bit like RMSProp with momentum.,RMSprop,3
"Just a question for you, why aren't you using Adam Optimizer which seem to be the best optimizer in a lot of cases ?",RMSprop,3
(It is even partially inspired from RMSProp that you use),RMSprop,3
"In   particular, when combined with adaptive gradients, L2   regularization leads to weights with large gradients   being regularized less than they would be when using   weight decay.",RMSprop,3
"Although the expression ""Adam is RMSProp with momentum"" is widely used indeed, it is just a very rough shorthand description, and it should not be taken at face value; already in the original [Adam paper (hyper-link)], it was explicitly clarified (p. 6):",RMSprop,3
"There are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running average of first and second moment of the gradient.",RMSprop,3
The best algorithm is the one that can traverse the loss for that problem pretty well.,RMSprop,3
"Tanh seems maybe slower than ReLU for many of the given examples, but produces more natural looking fits for the data using only linear inputs, as you describe.",Tanh,3
For [example a circle (hyper-link)] vs a [square/hexagon thing (hyper-link)].,Tanh,3
"Most of time tanh is quickly converge than sigmoid and logistic function, and performs better accuracy [[1] (hyper-link)].",Tanh,3
"A good neuron unit should be bounded, easily differentiable, monotonic (good for convex optimization) and easy to handle.",Tanh,3
"If you consider these qualities, then I believe you can use ReLU in place of the tanh function since they are very good alternatives of each other.",Tanh,3
"Update in attempt to appease commenters: based purely on observation, rather than the theory that is covered above, Tanh and ReLU activation functions are more performant than sigmoid.",Tanh,3
"For example, try limiting the number of features to force logic into network nodes in XOR and [sigmoid rarely succeeds (hyper-link)] whereas [Tanh (hyper-link)] and [ReLU (hyper-link)] have more success.",Tanh,3
" In the Deep Neural Network, which I mentioned earlier, this affine and tanh process is repeated three times.",Tanh,3
This combination of Affine and Tanh can be used once to function as a Neural Network for one layer.,Tanh,3
"From documentation, [https://pytorch.org/docs/stable/nn.html (hyper-link)]",Tanh,3
"Usually, you can use torch.cat to concatenate two tensors.",Tanh,3
"However, recently rectified linear unit (ReLU) is proposed by Hinton [[2] (hyper-link)] which shows ReLU train six times fast than tanh [[3] (hyper-link)] to reach same training error.",Tanh,3
" After this, we will update the alpha term unlike AdaGrad there is one more term in this equation known as 'rho' this is used to avoid infinitely increasing alpha value.",Adadelta,3
"As we saw in AdaGrad, the alpha value was increasing with each iteration but in the case of AdaDelta, the alpha first increased till 50 iterations and then continuously decreased so now there is no problem of learning rate decay.",Adadelta,3
" Now, let's visit our Adam again... Adam's main motive was to add the concept of momentum into the previous optimizer like AdaDelta.",Adadelta,3
" So,   you find that this RMSProp which takes  expose exponentially decaying average,   the AdaDelta takes a moving window average of  the squared gradients.",Adadelta,3
"So, that is the only   difference between RMSProp and AdaDelta.",Adadelta,3
Adadelta has a very slow start.,Adadelta,3
"I think Adadelta performs better with bigger networks than yours, and after some iterations it should equal the performance of RMSProp or Adam.",Adadelta,3
 As you can see from these plots the alpha is increasing with every iteration and the learning rate is decreasing so this is quite problematic.,Adadelta,3
" This is just like AdaDelta... instead of rho, we are writing beta 1 and beta 2.",Adadelta,3
" So, in today’s class we will  talk about two more algorithms,   one of them is RMSProp and the other  one is Adam and we will also see a   very closely related algorithm which  is very closely related to RMSProp.",Adadelta,3
" And we said   that there is a very closely related algorithm  very closely related to this RMSProp; so,   that closely related algorithm  is what is known as AdaDelta.",Adadelta,3
"So, we have a closely related algorithm known as  AdaDelta.",Adadelta,3
" And  in fact, both these algorithms were proposed   almost simultaneously, but independently, and  both of them gives almost similar performance.",Adadelta,3
" Then you  have NAG Nesterov accelerated gradient operation,   then you have Adagrad, then you have a  Adadelta, then you have RMSProp.",Adadelta,3
"If you are working with something like neural machine translation, this sparsity is an issue.",Adadelta,3
"Very few people use it today, you should instead stick to:",Adadelta,3
"This algorithm is very similar to Adadelta, but performs better in my opinion.",Adadelta,3
"A dialog**, for example, may not cover the entire previous Activity, and this would be a time for onPause() to be called.",onPause,3
"In most Activities, you will find that you will need to put code in onResume() and onPause().",onPause,3
" And let's see what happens when config changes, how it affects the activity.",onPause,3
"So, on configuration change, the Android first shuts down the activity by calling method in sequence, that is onPause, then onStop and then onDestroy.",onPause,3
"For instance, an activity can be visible but partially obscured by a dialog that has focus.",onPause,3
"In that case, onPause() will be called, but not onStop().",onPause,3
"When the dialog goes away, the activity's onResume() method will be called (but not onStart()).",onPause,3
 The implementation of onPause is very fast because the next activity is not resumed until this method returns.,onPause,3
"And is either followed by onResume, if the activity returns back to the front, or onStop, if the activity is becoming invisible to the user.",onPause,3
This is how we implement this onPause callback method.,onPause,3
" Firstly, knowing the OS can terminate your app's process without warning at any time you can't rely on having an onExit() handler that will be called when your app is closed.",onPause,3
"Instead, your Activities should listen for onPause handlers that indicate your app is no longer active.",onPause,3
"At this point is may be terminated at any time, so it should save any user data to prevent potential data loss.",onPause,3
"On other hands, if some new Activity occurs and occupies the full screen so that your previously running activity is disappeared.",onPause,3
"In this Case, your previously running activity is moved to Back Stack.",onPause,3
"Here, onPause() + onStop() are called.",onPause,3
NOTE 1: if a dialog starts on top of an activity NONE of onPause or onStop will be called.,onPause,3
"For example, it is not uncommon to implement one with an Activity whose theme is set to that of a dialog.",onPause,3
"In this case, displaying the dialog-as-an-Activity will cause the new Activity to be on the top of the stack, pausing what previously was there.",onPause,3
"If you cannot see any part of it, onStop() will be called.",onPause,3
"You usually don't have to do anything in onStop(), onStart() or onRestart().",onPause,3
" So, prior to being restarted again, it is called after your activity has been stopped.",onPause,3
" On onResume, called when the activity will start interacting with the user.",onPause,3
At this point your activity is at the top of the activity stack with user input going to it.,onPause,3
 onStop is called when your activity is no longer visible but still exists and all state information are preserved.,onPause,3
 It has now moved to the top of the activity stack and is always followed by onPause.,onPause,3
" Start by freeing resources, stopping services, disabling sensor listeners and turning off location requests, and otherwise disabling anything that consumes resources.",onPause,3
all within the onPause handler of your Activities.,onPause,3
Take this a step further by avoiding singletons and custom Application objects within your application whenever possible.,onPause,3
" So in this particular project I'm going to create my main activity and I'm going to put all the lifecycle methods: onCreate, onStart, onResume onPause, onStop, onRestart and onDestroy.",onPause,3
 So let me just override other methods also.,onPause,3
"OnStart and onResume... onPause... onStop... onRestart... and last, onDestroy.",onPause,3
" So, if I now try to click on Home button, what should be the lifecycle now?",onPause,3
It should go to onPause first.,onPause,3
Then onStop.,onPause,3
"So, onPause Finished and onStop Finished.",onPause,3
"In the same manner, it should show here, too.",onPause,3
"As you can see, onPause and onStop.",onPause,3
But this application is still not destroyed.,onPause,3
" Last but not the least, how we destroy our application, that is the destroying lifecycle.",onPause,3
" If I click my back button, it goes to onPause, onStop and all the way to onDestroy.",onPause,3
 Now my application is indeed destroyed.,onPause,3
" So you can see in Log also, onPause, onStop, onDestroy.",onPause,3
 But you must be thinking that I can still see my application in this task manager.,onPause,3
 And the other thing we want to do is to save the note.,onPause,3
"So to do that, we're going to use another method called onPause.",onPause,3
" Now in onPause, we can use that same database object.",onPause,3
 We've seen onCreate and we've seen onPause.,onPause,3
" If you forget this, the Android system reports a leaked broadcast receiver error.",onPause,3
"For instance, if you registered a receive in onResume method of an activity, you should unregister it inside onPause method.",onPause,3
2- previous activity is completely covered by new activity: both onPause and onStop will be called,onPause,3
"The first activity with the fields is obscured by another activity, and the user can no longer interact with it.",onPause,3
"However, it is still visible with all the resulting consequences.",onPause,3
That leaves a question which activity is considered fully opaque and covering the whole screen and which isn't.,onPause,3
This decision is based on the window containing the activity.,onPause,3
"If the window has a flag windowIsFloating or windowIsTranslucent, then it is considered that the activity doesn't make the underlying stuff invisible, otherwise it does and will cause onStop() to be called.",onPause,3
For instance: When a user traverses from Activity A to Activity B (FullScreen Non Transparent) following things happens,onPause,3
Activity A's state changes to paused state (which calls onPause on Activity A),onPause,3
Then Activity A's state changes from paused to stopped state (which calls  onStop on Activity A) .,onPause,3
onStop is called because Activity B was full screen  non transparent activity.,onPause,3
(If Activity B is non-full-sized or transparent then onStop is NOT called on Activity A),onPause,3
" And  having batch norm before the activation, that's usually how  yeah, that was originally how it was proposed in the paper.",BatchNorm1d,3
 That  means it may be that we need fewer epochs to get the same  loss that we would achieve if we don't use batch norm.,BatchNorm1d,3
"So you  usually with batch norm, the networks train faster.",BatchNorm1d,3
" And this is  essentially it's in that way, step one of batch norm is  similar to the input standardization.",BatchNorm1d,3
"For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.",BatchNorm1d,3
"However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].",BatchNorm1d,3
"BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice.",BatchNorm1d,3
"The BatchNorm1d normally comes before the ReLU, and the bias is redundant also",BatchNorm1d,3
When evaluating your model use [model.eval() (hyper-link)] before and [model.train() (hyper-link)] after.,BatchNorm1d,3
" But  sometimes people also, nowadays, it's even more common to have  it after the activation.",BatchNorm1d,3
" So  I know, instead of here having it before the activation, I now  have it after the activation in both cases.",BatchNorm1d,3
" And yeah, one little fun memory aid to remember that  is, if you consider this case, so you have batch norm, then  you have the activation and then you have dropout, you may call  it bad, it might be better to have batch norm after the  activation, that's typically a little bit more common.",BatchNorm1d,3
" So let's say you have the Google  search engine, and there's just one user running a query, and  you have a network that has batch norm.",BatchNorm1d,3
"So you have to  normalize, but you don't have a batch of users.",BatchNorm1d,3
"So there are two  ways to deal with that scenario, the easy one would be to use a  global training set mean and variance.",BatchNorm1d,3
So you would compute  these means for the features and the variances for the features  for the whole training set.,BatchNorm1d,3
 That's something you would also  usually do or could do when you compute the input standardization.,BatchNorm1d,3
" The same with batch  norms, instead of using batch norm, one D, which we used  earlier, when we talked about multi layer perceptrons of fully  connected layers, for the convolution layers, we use batch  norm 2d shown here.",BatchNorm1d,3
" So if n is my batch size here, we have an input that  is two dimensional, it is n times m, where let's say, m is  the number of features.",BatchNorm1d,3
" So we had if we  had three features, we had three gammas and three betas.",BatchNorm1d,3
"Now, we  extend this concept here to the two dimensional case where we  compute these four inputs that are four dimensional, right,  because we have now the batch size, we have the channels, we  have the height, and we have the width.",BatchNorm1d,3
"So we compute the batch  norm now, over the number of inputs height and width.",BatchNorm1d,3
"So in  that sense, we, we combine these.",BatchNorm1d,3
It depends on your ordering of dimensions.,BatchNorm1d,3
Add the model.eval() before you fill in your data.,BatchNorm1d,3
This can solve the problem.,BatchNorm1d,3
(I don't think PyTorch has a way to automatically do this for you.),BatchNorm1d,3
" In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.",cross_entropy,3
This is numerically more stable.,cross_entropy,3
" Yeah, conceptually this is a tensor of integers, they can only be 0 or 1, but we, we�re going to be using a cross entropy style loss function, so we're going to actually need to do floating-point calculations on them.",cross_entropy,3
"That's going to be faster to just store them as float in the first place rather than converting backwards and forwards, even though they're conceptually an �int� we're not going to be doing kind of �int style calculations� with them.",cross_entropy,3
"We say the loss is minimized because the lower the loss or cost of error, the better the model.",cross_entropy,3
The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss.,cross_entropy,3
"This terminology is a particularity of PyTorch, as the nn.NLLoss [sic] computes, in fact, the cross entropy but with log probability predictions as inputs where nn.CrossEntropyLoss takes scores (sometimes called logits).",cross_entropy,3
"For a special case of a binary classification, this loss is called binary CE (note that the formula does not change) and for non-binary or multi-class situations the same is called categorical CE (CCE).",cross_entropy,3
"Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs.",cross_entropy,3
"In your example you are treating output [0, 0, 0, 1] as probabilities as required by the mathematical definition of cross entropy.",cross_entropy,3
"But PyTorch treats them as outputs, that don’t need to sum to 1, and need to be first converted into probabilities for which it uses the softmax function.",cross_entropy,3
"So H(p, q) becomes:",cross_entropy,3
"Translating the output [0, 0, 0, 1] into probabilities:",cross_entropy,3
"For example, suppose for a specific training instance, the true label is B (out of the possible labels A, B, and C).",cross_entropy,3
The one-hot distribution for this training instance is therefore:,cross_entropy,3
"You can interpret the above true distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C.",cross_entropy,3
"Now, suppose your machine learning algorithm predicts the following probability distribution:",cross_entropy,3
How close is the predicted distribution to the true distribution?,cross_entropy,3
That is what the cross-entropy loss determines.,cross_entropy,3
Where p(x) is the true probability distribution (one-hot) and q(x) is the predicted probability distribution.,cross_entropy,3
"The sum is over the three classes A, B, and C. In this case the loss is 0.479 :",cross_entropy,3
" I'm just  saying that we will use them in practice, we will actually use  the cross entropy in practice.",cross_entropy,3
" But in practice, this is more stable, I  recommend using this one.",cross_entropy,3
" So this particular function, which is identical to MNIST loss plus �.log� jhas a specific name and it's called binary cross entropy, and we used it for the threes vs. sevens problem, to, to decide whether that column is it a three or not, but because we can use broadcasting in PyTorch and element-wise arithmetic, this function when we pass it a whole matrix is going to be applied to every column.",cross_entropy,3
" Random numbers and movies by 5, okay.",cross_entropy,3
"And so to calculate the result for some movie and some user we have to look up the index of the movie in our movie latent factors, the index of the user in our user latent factors and then do a cross product.",cross_entropy,3
"So in other words we would say, Like oh okay, for this particular combination we would have to look up that numbered user over here and that numbered movie over here to get the two appropriate sets of latent factors.",cross_entropy,3
Thanks everybody and I will see you next week or see you in the next lesson whenever you watch it,cross_entropy,3
" What happens   if we're not predicting which of five things  it is but we're just predicting “is it a cat?”   So in that case if you look at this approach  you end up with this formula, which it's   exactly… this is identical to this formula but  in for just two cases, which is you've either:   you either are a cat; or you're not a cat,  right, and so if you're not-a-cat, it's one minus   you-are-a-cat, and same with the probability  you've got the probability you-are-a-cat,   and then not-a-cat is one minus that.",cross_entropy,3
" So here's  this special case of binary cross entropy,   and now our rows represent rows of data, okay,  so each one of these is a different image,   a different prediction, and so for each  one I'm just predicting are-you-a-cat,   and this is the actual, and so the actual  are-you-not-a-cat is just one minus that.",cross_entropy,3
" And so then these are the predictions  that came out of the model,   again we can use soft max or it's  binary equivalent, and so that will   give you a prediction that you're-a-cat, and the  prediction that it's not-a-cat is one minus that.",cross_entropy,3
" So we're just going to decide, all right, we're  just going to decide the first 10 columns,   we're going to decide are the  prediction of what the disease is,   which is the probability of each disease.",cross_entropy,3
"So we can now pass to cross_entropy   the first 10 columns, and the disease target.",cross_entropy,3
"The cross-entropy loss that you give in your question corresponds to the particular case of cross-entropy where your labels are either 1 or 0, which I assume is the case if you're doing basic classification.",cross_entropy,3
"As to why this happens, let's start with the cross-entropy loss for a single training example x:",cross_entropy,3
"where P is the ""true"" distribution and ""Q"" is the distribution that your network has learned.",cross_entropy,3
"The ""true"" distribution P is given by your hard labels, that is, assuming that the true label is t, you'll have:",cross_entropy,3
which means that the loss above becomes,cross_entropy,3
"In your case, it seems that the distribution Q_s is computed from the logits, i.e.",cross_entropy,3
the last layer before a softmax or cost function which outputs a set of scores for each label:,cross_entropy,3
The traditional matrix multiplication is only used when calculating the model hypothesis as seen in the code to multiply x by W:,cross_entropy,3
"It is this measure (i.e., the cross-entropy loss) that is minimized by the optimization function of which Gradient Descent is a popular example to find the best set of parameters for W that will improve the performance of the classifier.",cross_entropy,3
 If we use negative  log likelihood loss or cross entropy in pytorch.,cross_entropy,3
"But  numerically, like stability wise on the computer, the cross  entropy one is more stable.",cross_entropy,3
"So and also for this one, really pay  attention to this one, it's taking the logits as input.",cross_entropy,3
 And the term binary  cross entropy and negative log likelihood are essentially the  same.,cross_entropy,3
So overall it seems preferable to use the global application context when possible.,getBaseContext,3
The benefit of using a ContextWrapper is that it lets you “modify behavior without changing the original Context”.,getBaseContext,3
"For example, if you have an activity called myActivity then can create a View with a different theme than myActivity:",getBaseContext,3
ContextWrapper is really useful to work around device/version specific problems or to apply one-off customizations to components such as Views that require a context.,getBaseContext,3
"So although this (for Activity) and getBaseContext() both give the activity context, they  (a) do not refer to the same object (this != getBaseContext()) and  (b) calling context through this is slightly less efficient, as the calls go through an extra level of indirection.",getBaseContext,3
getContext() and getBaseContext()  is most probably same .these are reffered only current activity which is live.,getBaseContext,3
"I couldn't find really anything about when to use getBaseContext() other than a post from Dianne Hackborn, one of the Google engineers working on the Android SDK:",getBaseContext,3
"Don't use getBaseContext(), just use   the Context you have.",getBaseContext,3
"That was from a post on the [android-developers newsgroup (hyper-link)], you may want to consider asking your question there as well, because a handful of the people working on Android actual monitor that newsgroup and answer questions.",getBaseContext,3
ContextWrapper is really powerful because it lets you override most functions provided by Context including code to access resources (e.g.,getBaseContext,3
"openFileInput(), getString()), interact with other components (e.g.",getBaseContext,3
"sendBroadcast(), registerReceiver()), requests permissions (e.g.",getBaseContext,3
checkCallingOrSelfPermission()) and resolving file system locations (e.g.,getBaseContext,3
getFilesDir()).,getBaseContext,3
Android provides a ContextWrapper class that is created around an existing Context using:,getBaseContext,3
"To further add missing points here, as per the request by Jaskey",onCreate,0
exeqSQL() executes just one SQL statement.,onCreate,0
"You need to split your SQL to individual calls, one for DROP TABLE and another for CREATE TABLE.",onCreate,0
You should not start a nested transaction yourself.,onCreate,0
"SQLite itself does not support nested transactions, and Android's SQLite wrapper only adds partial support for transaction nesting.",onCreate,0
" We simply create an Intent with Intent intent = new Intent() We pass the context, which is ""this"" And we pass the class we want to open Which is Activity2.class And in the next line we write: startActivity() and we pass the intent we just created And that's basically the whole process This part here will open the second activity.",onCreate,0
" Let's test it Okay, as you can see here we are in Activity 1 currently Now we click our button And we changed to Activity2 When we click the back button, we get back to Activity 1 And if you click again, we go back to Activity 2 I hope this was helpful.",onCreate,0
 Take care,onCreate,0
" So  I have in our studio running here, and I'm running   arctic fox, but any recent version should do tap  on new project.",onCreate,0
 And I'm going to pick the empty   activity template.,onCreate,0
" These other ones are useful  sometimes, but they bring in a lot of cruft into   our application, which could be confusing.",onCreate,0
" So  usually, I'll just go with empty activity.",onCreate,0
 Let's   call our application Tippie.,onCreate,0
" The package name,  usually what I'll do is I'll take my domain name   or my email address and flip it backwards.",onCreate,0
" So with the project that we have, right now, we  only have one screen.",onCreate,0
 And that's referred to as   the main activity or the main screen.,onCreate,0
 And that's  the business logic that we're defining right here   in this file called main activity.,onCreate,0
 And the important  line here is line nine setcontentview r dot layout   activity main.,onCreate,0
 So R stands for resources.,onCreate,0
 So now in the onCreate.,onCreate,0
" And   the logs that we care about are the ones  from only our application, which is calm,   calm, rk Pandey Tippi and we also only  care about info level logs because that   is what log.io represents info level  logs.",onCreate,0
" We also only care about logs,   which have this particular tag, which is main  activity.",onCreate,0
 So let's add main activity as a filter.,onCreate,0
" And here as we change the seat bar, you can  see how we get one line of logcat output,   and it represents exactly what is the current  indicator of the seat bar showing.",onCreate,0
" And then   on a string, we know it's going to be a currency  like a decimal, so we'll call to double on it,   in order to turn it into a number that we can  work with.",onCreate,0
 And we'll call this base amount.,onCreate,0
" And then similarly, for the 2%, we want to get  the value of the progress on the seat bar, this   attribute called progress, and that'll be saved  in a variable that I'm going to call tip percent.",onCreate,0
" Let's increase the margin here to be 56   Next, let's drag out one more text you.",onCreate,0
 And we'll call this TV tip description.,onCreate,0
" And then the text should be empty, because  that will depend on the initial tip percent,   which is set programmatically.",onCreate,0
" And then if we bring   it down to zero, we get poor and if we go all the  way up, we should get amazing which we do.",onCreate,0
 One   quick thing want to do is back in activity main.,onCreate,0
 I want to set the font weight of this to be bold.,onCreate,0
 Let's see how you can do that.,onCreate,0
 You can do that by adding the following tag inside of your application node.,onCreate,0
 Note that the getPaymentsClient method takes a walletOptions parameter.,onCreate,0
" Now, you'll note I have made  the database name public, and that's because we're going to use it in our tests in the future.",onCreate,0
 Now I'm just going to add this comment so you know where to go back and add the location entry stuff later.,onCreate,0
" For delete, we have delete, and update and count, too.",onCreate,0
" After you define your SQLiteOpenHelper class, then you have to get to the activity-- your main activity or in whichsoever activity you want to access your database.",onCreate,0
 You have to create the instance of your SQLiteOpenHelper class.,onCreate,0
 Your database is ready.,onCreate,0
 All you have to do is instantiate that OpenHelper class inside your activity and access your database.,onCreate,0
 How do we do that?,onCreate,0
" So inside your main activity, or any other activity, you will be initializing the object of that SQLiteOpenHelper class.",onCreate,0
" And if you remember, we pass the context object there, so you will be passing this there, because activity contains the context object.",onCreate,0
" If the SQL string is invalid, throws SQLException.",onCreate,0
" Inside activity, where you want to use database, create an object of DatabaseHelper (which extends from SQLiteOpenHelper) and  call the method getWritableDatabase() on that DatabaseHelper object.",onCreate,0
 And this is going to give you an object of  SQLiteDatabase.,onCreate,0
 I'll describe how to do this in this project named CreateDatabase.,onCreate,0
" In this version of my project, I moved my XML parser classes to a new package that ends with .xml, and I've created another new package that ends with .db.",onCreate,0
 This is where I'll place my java classes that manage my database.,onCreate,0
 I'll make sure I have the option to create method stubs for Inherited abstract methods and click Finish.,onCreate,0
 I'll come back to these methods in a moment.,onCreate,0
 You can name your database file anything you want.,onCreate,0
" It's common to use a file extension of .db, but it's not required.",onCreate,0
 The database version is required.,onCreate,0
 So I'll get rid of the TODO comment.,onCreate,0
 I'll use the I method.,onCreate,0
" When the onUpgrade method is called, I'll receive arguments named oldVersion and newVersion, and I might want to write some very finely-tuned code that examines those values and upgrades the database in some complex way, but again, I'm going to keep this simple.",onCreate,0
" I'm just going to drop the existing table, the tours table, and then I'll recreate it.",onCreate,0
" I'll call db.execSQL and I'll pass in this explicit SQL Command, DROP TABLE IF EXISTS, and then I'll append to that the name of the table, TABLE_TOURS.",onCreate,0
" So, that's what a basic open helper class looks like.",onCreate,0
" That database type will be SQLiteDatabase, and I'll name that database.",onCreate,0
" I'll save my changes, and now I'm ready to test.",onCreate,0
 These values get passed into the constructor to initialize the database helper.,onCreate,0
onCreate(),onCreate,0
onUpgrade(),onCreate,0
execSQL method.,onCreate,0
"onUpgrade
This method is called when database version is upgraded.",onCreate,0
Two main approaches:,onCreate,0
Uninstall your application from the emulator or device.,onCreate,0
Run the app again.,onCreate,0
"I found some relevant information on [this Github issue (hyper-link)], and by asking a [similar question (hyper-link)].",Adadelta,0
The problem here is that [tf.initialize_all_variables() (hyper-link)] is a misleading name.,Adadelta,0
"It really means ""return an operation that initializes all variables that have already been created (in the default graph)"".",Adadelta,0
"When you call tf.train.AdadeltaOptimizer(...).minimize(), TensorFlow creates additional variables, which are not covered by the init op that you created earlier.",Adadelta,0
Moving the line:,Adadelta,0
...after the construction of the tf.train.AdadeltaOptimizer should make your program work.,Adadelta,0
N.B.,Adadelta,0
"Your program rebuilds the entire network, apart from the variables, on each training step.",Adadelta,0
"This is likely to be very inefficient, and the Adadelta algorithm will not adapt as expected because its state is recreated on each step.",Adadelta,0
I would strongly recommend moving the code from the definition of batch_xs to the creation of the optimizer outside of the two nested for loops.,Adadelta,0
"You should define tf.placeholder() ops for the batch_xs and batch_ys inputs, and use the feed_dict argument to sess.run() to pass in the values returned by mnist.train.next_batch().",Adadelta,0
tf.train.MomentumOptimizer with 0.9 momentum is very standard and works well.,Adadelta,0
The drawback is that you have to find yourself the best learning rate.,Adadelta,0
tf.train.RMSPropOptimizer: the results are less dependent on a good learning rate.,Adadelta,0
[ (hyper-link)],Adadelta,0
The first 10 lines:,Adadelta,0
 Let's see how.,Adadelta,0
 DONE!,Adadelta,0
" In Adam, we will initialize two variables namely 'm' and 'v' as zero.",Adadelta,0
" Now, iteration starts... the first equation is to update initialized m variable and the second equation is to update initialized v variable the only difference between these equations is in the first one we are multiplying one minus beta1 with gradient but in the second equation we are multiplying one minus beta2 with gradient square.",Adadelta,0
" Typically, beta1 is set to 0.9, and beta2 is set to 0.999 as suggested by the author of adam's paper.",Adadelta,0
 Same with the second equation just instead of m_t here it is v_t and in the denominator instead of beta1 it is beta2.,Adadelta,0
" Now, it's time to write the final update rule of Adam... Our learning rate parameter is back because in Adam you have to set a learning rate, unlike AdaDelta in which it is automatically computed.",Adadelta,0
 And then we can compute gradient by dividing bias-corrected m_t or m_t hat with the square root of v_t hat.... again this epsilon is a small value to avoid zero division error.,Adadelta,0
" So, as we have seen in the previous class the  Adagrad algorithm is given something like this,   that at time t you compute the batch gradient;   gradient of the loss function with respect  to the weight vector or with respect to the   parameter vector.",Adadelta,0
" And, then what you do is,  you go on accumulating the squared gradient   or you take the sum of squared gradient of  all the historical gradient values and this   sum of squared gradient is used to scale  the gradient of all individual locations.",Adadelta,0
" So, as a result our upgradation or weight  upgradation algorithm that we have seen earlier   is given by this expression, where if W t is the  parameter vector or weight vector at time t. Then at time t plus 1 we get our updated weight  vector as W t minus eta upon square root of   epsilon I plus r t times g t where, these  operations are actually done element wise;   that means, whenever I rewrite 1 over squared  root of epsilon I plus r t. So, for individual   components this will actually be 1 over square  root of epsilon plus r t i.",Adadelta,0
" So, this is the sum   of the squared gradient; sum of the squares of  the ith component of the gradient vector and you   take the square root of this and times g t; that  means, this will be multiplied by corresponding   ith component of the gradient g t and that  will be added with the ith component of W t. So in fact, your expression will be that if I  go for component wise W t plus 1 ith component   will get W t ith component of this minus eta by  square root of epsilon plus r t i times g t i.",Adadelta,0
" So, this is sum of all the historical values.",Adadelta,0
" And, this is what is actually scaling the  ith component of the gradient vector which   effectively is updating the ith component of  the weight vector or the parameter vector.",Adadelta,0
" So,   what is the effect of the scaling is that if  for certain component say for W i you find   that del L del W i which is nothing, but our  g t i.",Adadelta,0
" And, if this is  small; r t i is small and then the corresponding   learning rate of the ith component of  the parameter vector will also be large.",Adadelta,0
" So, this is how Adagrad algorithm is  adaptively tuning the learning rate   of individual parameters or individual  weight components of the weight vector.",Adadelta,0
" So, we have also seen that what are the  positive points of this Adagrad algorithm.",Adadelta,0
" So, these are  the positive points of the Adagrad algorithm.",Adadelta,0
" And, the negative point is if the function is  non-convex which is quite possible given your   high dimensional space in which the error function  is defined, then while the algorithm proceeds the   trajectory of the weight vector or the parameter  vector may pass through many complex terrains   before coming to a locally convex region.",Adadelta,0
" So,  the moment it comes to locally convex region,   we want that the algorithm will quickly converse  to the minima of this locally convex region.",Adadelta,0
" So, only difference is this r  t is not the cumulative sum of squared gradients,   but this r t is the exponentially decaying  average of the squared gradients.",Adadelta,0
" So, you  use both this first order and second   order moment that becomes a variant of  RMSProp and this is what is known as Adam.",Adadelta,0
" So, in case of Adam you are including both  first and second moments for weight updation   or parameter updation and in addition  you Adam incorporates one more term,   that it tries to correct the bias  to account for initial to zero.",Adadelta,0
" So,   what he said is that when you are  taking exponentially decaying average,   you are initializing the average value  at zero at t equal to zero, right.",Adadelta,0
" So, this corrected  first moment is represented as s t hat,   similarly the corrected second moment which is  r t hat is nothing, but r t by 1 minus beta 2.",Adadelta,0
" So, once given this your weight updation of the  parameter updation rule simply becomes W t plus   1 is equal to W t minus eta times t hat, where  s t hat is the bias corrected first moment upon   square root of epsilon I plus r t hat, where r  t hat is the bias corrected second moment.",Adadelta,0
" So,   you find that this is nothing, but similar to  your RMSProp algorithm where you are incorporating   where, this Adam algorithm incorporates bias  correction operation and it also incorporates the   first moment in the update step.",Adadelta,0
" So, as you see over here this red curve,   the red curve is actually the pure SGD  algorithm or Stochastic Gradient Algorithm,   the blue one gives you the momentum.",Adadelta,0
" So, you   find over here that the SGD which you find that  it is still diver converging whereas, the other algorithms have already come first.",Adadelta,0
"I tend to use Adam, and always in combination with clipped gradients.",Adadelta,0
Adam is a bit more computationally expensive I suppose but gives good results.,Adadelta,0
The learning rate.,Adadelta,0
"As discussed in a relevant [Github thread (hyper-link)], the decay does not affect the variable lr itself, which is used only to store the initial value of the learning rate.",Adadelta,0
"In order to print the decayed value, you need to explicitly compute it yourself and store it in a separate variable lr_with_decay; you can do so by using the following callback:",Adadelta,0
as explained [here (hyper-link)] and [here (hyper-link)].,Adadelta,0
"In fact, the specific code snippet suggested there, i.e.",Adadelta,0
comes directly from the underlying Keras [source code for Adadelta (hyper-link)].,Adadelta,0
"As clear from the inspection of the linked source code, the parameter of interest here for decaying the learning rate is decay, and not rho; despite the term 'decay' used also for describing rho in the [documentation (hyper-link)], it is a different decay not having anything to do with the learning rate:",Adadelta,0
rho: float >= 0.,Adadelta,0
"Adadelta decay factor, corresponding to fraction of gradient to keep at each time step.",Adadelta,0
tf.train.MomentumOptimizer with 0.9 momentum is very standard and works well.,Adadelta,0
The drawback is that you have to find yourself the best learning rate.,Adadelta,0
tf.train.RMSPropOptimizer: the results are less dependent on a good learning rate.,Adadelta,0
[ (hyper-link)],Adadelta,0
The first 10 lines:,Adadelta,0
"The activation of a ReLU is unbounded, making its use in Auto Encoders difficult since your training vectors likely do not have arbitrarily large and unbounded responses!",Adadelta,0
ReLU simply isn't a good fit for that type of network.,Adadelta,0
"You can force a ReLU into an auto encoder by applying some transformation to the output layer, as is [done here (hyper-link)].",Adadelta,0
"However, hey don't discuss the quality of the results in terms of an auto-encoder, but instead only as a pre-training method for classification.",Adadelta,0
So its not clear that its a worth while endeavor for building an auto encoder either.,Adadelta,0
"Basically, you have a set of datapoints along with the targets you're trying to predict in the form",Adadelta,0
where A_k is the k-th piece of training data and b_k is the correct answer.,Adadelta,0
You generally want your machine learning model (e.g.,Adadelta,0
"a classifier or a regressor, for example a neural network or perhaps a linear model) to update its internal parameter",Adadelta,0
"as it reads in the datapoints (A_k,b_k) one at a time, i.e.",Adadelta,0
"you want the model to update x in ""realtime"" as the data comes in.",Adadelta,0
"This is as opposed to something like batch learning, where your model has access to the entire dataset D all at once.",Adadelta,0
"Now, we generally have a notion of ""cost"" --- in linear regression, the cost function that we're trying to minimize is the root mean square (RMS) of the difference between the predicted target value and the actual target value.",Adadelta,0
"Recalling the definition of online linear regression, you have a [stochastic gradient descent (hyper-link)] based update step where the parameters x get updated based on all the data the model has seen so far:",Adadelta,0
"What update rules like AdaGrad and AdaDelta do is to provide a ""nicer"" way to perform updates --- this can mean things like making sure the parameters converge to their optimal value faster, or in the case of AdaDelta, it means that the parameters x ""step closer"" to their optimal values in appropriately-sized steps, with the step size changing based on past performance.",Adadelta,0
"Let's go through your questions, one question at a time:",Adadelta,0
Question 1:,Adadelta,0
The gradient in higher dimensions (i.e.,Adadelta,0
when x is represented by an array) is defined as,Adadelta,0
"where f(x1,x2,...,x_n) is the function you're trying to minimize; in most cases it's the cost function at a single example as a function of x, the model parameters.",Adadelta,0
In other words: take the derivatives of the cost function and then evaluate it at the current parameters xt.,Adadelta,0
Question 2:,Adadelta,0
"Based on my understanding, the RMS of delta-x is defined as",Adadelta,0
where,Adadelta,0
initialized with,Adadelta,0
Question 3:,Adadelta,0
AdaDelta is purely an update rule.,Adadelta,0
The general structure is something like this:,Adadelta,0
where,Adadelta,0
because the point of AdaDelta is to make the learning rate into a dynamic value rather than forcing us to choose an arbitrary value for it at the start.,Adadelta,0
Question 4:,Adadelta,0
"The idea behind ""correcting units with Hessian approximation"" comes from physical intuition, in some sense; that is to say, if you give each variable some sort of unit (length/meters, time/seconds, energy/joules, etc.)",Adadelta,0
then the Hessian has the appropriate units to correct the update term so that the dimensional analysis works out.,Adadelta,0
Question 5:,Adadelta,0
Delta-x is the difference in x after updating.,Adadelta,0
"So if x_i is your parameter before an update and x_{i+1} is your parameter after updating, then Delta-x is (x_{i+1} - x_i).",Adadelta,0
"(∂f/∂x) is the derivative of the function you're trying to minimize (usually in ML, f is the cost function).",Adadelta,0
This note was added to clarify based on a comment from @GMsoF below,onPause,0
There are a lot of lifecycle methods.,onPause,0
You don't need to override all of them.,onPause,0
You only need to override the ones where you need (or want) to customize the behaviour for your activity.,onPause,0
There are a lot of lifecycle methods because different applications have different requirements.,onPause,0
The lifecycle of an Activity is well-documented and well-behaved.,onPause,0
"This allows programmers to put the code exactly where it is needed, based on the particular requirements of the application.",onPause,0
You have asked,onPause,0
 The first thing that we are going to do is we are going to talk about activity lifecycle and how activities transition from and into different states.,onPause,0
" And then, we will be going to learn about those callback methods which are called during the state change.",onPause,0
 We will also cover the saving and restoring of instance state when dealing with Android activities.,onPause,0
 Then it gets started where the app is visible but not its components.,onPause,0
" And then, on onResume state at this state the user will see the activity and it's in the foreground, and users can interact with it.",onPause,0
" Now, if the user gets a pop-up dialog box, the activity will go into the paused state where it is still partially visible to the user.",onPause,0
 Let's understand the activity states through callbacks graph.,onPause,0
" For example, if you've been through the onCreate, onStart, onResume, we have our activity running and then the activity becomes paused.",onPause,0
" For example, someone clicks a button and launches another activity on the top of your activity but, then they hit the back button and they're going back to your activity, onResume is going to be called and your activity is going to be running again.",onPause,0
" Also, if the application is stopped and a user has left the application, completely left that activity, onStop may get called.",onPause,0
" But when the user goes and relaunches your application, onRestart is likely to be called which will restart your application but not recreate your application because it's still in the memory, the Android hasn't actually killed it.",onPause,0
" So, in that case, we are going to go back through that cycle and onResume is going to be called and your activity is going to be running again.",onPause,0
" Now, if the application was actually killed, then the next time it gets launched it's going to restart at the onCreate.",onPause,0
 This is how it is implemented.,onPause,0
" Here in onStart the activity is becoming visible to user, which is followed by onResume if the activity comes to the foreground, or onStop if it becomes hidden.",onPause,0
 And this is how it is implemented.,onPause,0
 It is a transient state which occurs very fast and is always followed by onStart.,onPause,0
 This shows the implementation of onRestart callback.,onPause,0
 onResume where activity will start interacting with the user and has started accepting the user input.,onPause,0
 This is how we implement onResume method.,onPause,0
" onStop is the stopped state which is called when the activity is no longer visible to the user, when new activity is being started and is brought in front of the one, and this one is being destroyed.",onPause,0
" And remember, the system may destroy the activity without calling this method sometimes.",onPause,0
 This is how we implement onDestroy method.,onPause,0
" Now, the second step is where Android started all over again, by calling onCreate, onStart and finally onResume.",onPause,0
" Either using onCreate which is more preferred where it ensures that your user interface, including any saved state, is back up and running as quickly as possible.",onPause,0
" Or another way, by calling or implementing callback onRestoreInstanceState which is called after onStart.",onPause,0
" So, if savedInstanceState doesn't equal null, then we can go ahead.",onPause,0
 I'm going to call my main activity as MainActivity.,onPause,0
" I'm going to override all the methods, and while I'm doing that, inside each method I'm going to put a Toast and a Log.",onPause,0
 Okay.,onPause,0
 For onStop it will be onStop message in logcat and onStop Finished for Toast.,onPause,0
" So, here, now we will try to understand how exactly the application-- our application's main activity works.",onPause,0
" If I can see my activity right on the screen, that means it has already reached to onResume state.",onPause,0
" But to reach it to onResume state, it has to go through first onCreate and onStart.",onPause,0
 See here?,onPause,0
" OnCreate, onStart Finished and onResume Finished.",onPause,0
" So, you see the chronological order here.",onPause,0
" First, the onCreate Toast was called, then onStart, and then onResume.",onPause,0
" In the same manner, you can look at the Android monitor tool also, inside which there will be a logcat, and in the logcat you can see first the onCreate.",onPause,0
" So, Mainactivity onCreate is shown first.",onPause,0
" Next, onStart, then onResume.",onPause,0
" And you can see my application is still there, and if I click it, what will be the lifecycle?",onPause,0
" It will start from onRestart, then onStart, and then onResume.",onPause,0
" So, onRestart, onStart and then, onResume.",onPause,0
" If I can see my applications, that means it's already in onResume.",onPause,0
" In the back, onRestart and onStart  has already finished.",onPause,0
" Same goes with the Toast, so onRestart, onStart and onResume.",onPause,0
 See?,onPause,0
" onCreate, onStart and onResume.",onPause,0
" So, in the same way, you can see Logs, also, onCreate, after onDestroy.",onPause,0
" It never started from onRestart, onCreate, onStart and onResume.",onPause,0
 So that was all about activities lifecycle.,onPause,0
" So just as before, let's make sure we call super.",onPause,0
 And we'll call that id and we'll just give it a default value of 0.,onPause,0
 So we'll say MainActivity.database.noteDao().,onPause,0
" So to do that, we're going to use a third method.",onPause,0
 We're going to use one more called onResume.,onPause,0
 So onResume is going to be called every time an activity is brought to the foreground.,onPause,0
" So when the activity is first created, it's going to call onCreate.",onPause,0
" And then it's going to call onResume, because the activity just entered the foreground.",onPause,0
" Then we could jump off to some other activity and come back and then onResume is going to be called again, because we're back in the foreground.",onPause,0
 So this is a great method to reload our RecyclerView.,onPause,0
" That same sequence, then happens in reverse.",onPause,0
" Finally, there's an onDestroy method, indicating the end of the app lifecycle.",onPause,0
 You can see the code snippet on the bottom of the slide.,onPause,0
 Broadcast receiver is very important component of Android.,onPause,0
To Summaries-,onPause,0
Know more about- [Back Stack (hyper-link)].,onPause,0
actually there will be two circumstances:,onPause,0
----Good to state some notes:,onPause,0
Consider the following case:,onPause,0
[image],onPause,0
Here we see both activities at the same time.,onPause,0
The relevant code can be found in com.android.server.am.ActivityRecord:,onPause,0
"To keep it simple, here is the small info from the android [developers documentation (hyper-link)].",onPause,0
Please refer to [this (hyper-link)] life cycle demo application.,onPause,0
It's really helpful in understanding the Activity lifecycle.,onPause,0
"Also as Raghunandan suggested, you must read [this (hyper-link)].",onPause,0
Added:,onPause,0
Good Luck :),onPause,0
try and write down the equation for the case of batch_size=1 and you'll understand why pytorch is angry with you.,BatchNorm1d,0
How to solve it?,BatchNorm1d,0
Here is the official documentation for PyTorch's BatchNorm:,BatchNorm1d,0
[BatchNorm1D (hyper-link)] and [BatchNorm2D (hyper-link)],BatchNorm1d,0
Ok.,BatchNorm1d,0
I figured it out.,BatchNorm1d,0
So for example:,BatchNorm1d,0
" So Jack so that is we we wanted to do these 2 videos together, basically data normalisation as well as batch normalisation because I think it helps you understand this better, okay thank you.",BatchNorm1d,0
"  All right, let's now talk about how we can use batch norm in  practice.",BatchNorm1d,0
 So how  would that work?,BatchNorm1d,0
" So but yeah, first, let me show you how  batch norm works.",BatchNorm1d,0
 When we use pytorch.,BatchNorm1d,0
" And first, notice  that I'm using again, flatten, that's because I was working  with MNIST and MNIST is on n times one times 28, times 28  dimensional, and flatten will essentially flatten this to a  vector to n times 784 dimensional vector, where n is  the batch size, and then it will work with the fully connected  linear layer here.",BatchNorm1d,0
 This is essentially just the batch norm that we  discussed here in the previous video.,BatchNorm1d,0
 So the one is just to  keep them apart.,BatchNorm1d,0
" I ran it with and without it, I didn't notice any difference in  this case.",BatchNorm1d,0
" Okay, so this is how your batch norm looks like a  full code example can be found here.",BatchNorm1d,0
" But there's really  nothing, nothing really to talk about, because it's just two  lines of code here.",BatchNorm1d,0
" Um, yeah, so I was also just for fun running  some experiments without the bias that I just showed you.",BatchNorm1d,0
 I then also inserted batch norm after the  activation instead of before the activation like here or here.,BatchNorm1d,0
 But I could at least  see there was no overfitting anymore.,BatchNorm1d,0
" So when we look again at our training function  here, this is exactly the same training function that I used  last week in dropout.",BatchNorm1d,0
 And  these running statistics are then used in the evaluation mode  Asian mode when we evaluate our model on new data.,BatchNorm1d,0
" And in inference, you may only have a  single data point, right?",BatchNorm1d,0
" But yeah, here's just like the explanation what's  gonna happen under the hood.",BatchNorm1d,0
" Okay, so yeah, that is how batch  norm works in pytorch.",BatchNorm1d,0
" And in the next video, I want to  briefly go over some very brief, a brief rundown of all the types  of literature that try to explain how batch norm works.",BatchNorm1d,0
 Or at least we will discuss some of the  theories trying to explain why it works.,BatchNorm1d,0
" And just to for simplicity, I will  actually ignore the layer index in the next couple of slides  just so that the other notations are a bit simpler to read.",BatchNorm1d,0
 So this is like what we had in a  previous video as standardization.,BatchNorm1d,0
" Except now, we are  looking at a hidden layer.",BatchNorm1d,0
" So yeah, this is essentially it.",BatchNorm1d,0
" And yeah, this is how batch norm works.",BatchNorm1d,0
" Um, do I have anything  more?",BatchNorm1d,0
" So you can actually  apply an argument, say bias equals false.",BatchNorm1d,0
" Alright, so I think this is all I have about batch norm.",BatchNorm1d,0
 This is  how batch norm works.,BatchNorm1d,0
" In my slides, I actually had also  short or not short, it was actually like 10 slides or so  how we do backpropagation with batch norm, but I promised you  not to torture you with these nitty gritty details and math  mathematical details, because that's not super important  because we use auto grad and practice anyway.",BatchNorm1d,0
 I hope you don't mind.,BatchNorm1d,0
" So in that way, next  video, I will show you batch norm in pytorch.",BatchNorm1d,0
" Now, we will end with something known as Batch Normalization, which is again almost a defacto standard at least in convolutional neural networks.",BatchNorm1d,0
 And this is where we apply batch normalization.,BatchNorm1d,0
 So we specify type equals batch norm.,BatchNorm1d,0
" In the batch normalization layer, this is where we apply our activation function.",BatchNorm1d,0
" Um, the, the data, the patterns in the data is going to be less when you're using batch normalization.",BatchNorm1d,0
"  All right, let's now learn how we can translate familiar  concepts, such as dropout and batch norm to the convolutional  settings.",BatchNorm1d,0
 So these are also called spatial dropout and  batch norm.,BatchNorm1d,0
" And it sounds fancier than it really is,  because you will see it's pretty straightforward.",BatchNorm1d,0
" So usually in the  later stages of the network, these channels represent higher,  higher or larger concepts, as we've talked about in the last  last lecture, where we have a bigger picture concepts like a  channel represents the eye that was detected one the mouth one  the nose and so forth.",BatchNorm1d,0
" So here, the idea is really to drop these  higher order features in that way, dropping entire feature  maps instead of individual pixels.",BatchNorm1d,0
" So essentially, drop  out to D is drop out applied to the channels rather than the  pixel.",BatchNorm1d,0
 It's not very complicated.,BatchNorm1d,0
" So just to briefly recap, I don't want to  explain the batch norm again, because we have a video for  that.",BatchNorm1d,0
 So this is my input dimension.,BatchNorm1d,0
" So we  had usually, let's say a table where we have different  features, let's call them f1, f2, and f3.",BatchNorm1d,0
 So we have three  features here.,BatchNorm1d,0
" And here, this is our batch dimension.",BatchNorm1d,0
 Let's use yellow.,BatchNorm1d,0
 Okay.,BatchNorm1d,0
 So each of these are are fine.,BatchNorm1d,0
"Moreover, you're trying to use ReLU in the form x = nn.ReLU(x).",BatchNorm1d,0
"This is wrong, as nn.ReLU is a layer.",BatchNorm1d,0
This line of code returns you the ReLU layer itself rather than a tensor.,BatchNorm1d,0
"Either define nn.ReLU() layers in your init method, or use F.relu(x) or nn.ReLU()(x).",BatchNorm1d,0
Like so:,BatchNorm1d,0
Tensorflow has has channels in the last axis in convolution.,BatchNorm1d,0
So its batchnorm puts them in axis=-1.,BatchNorm1d,0
One can find the answer inside [torch.nn.Linear documentation (hyper-link)].,BatchNorm1d,0
That's the reason behind your error.,BatchNorm1d,0
I met this problem when I load the model and started to test.,BatchNorm1d,0
Here is the official documentation for PyTorch's BatchNorm:,BatchNorm1d,0
[BatchNorm1D (hyper-link)] and [BatchNorm2D (hyper-link)],BatchNorm1d,0
The tensor must have at least rank 3.,BatchNorm1d,0
whence:,cross_entropy,0
Code:,cross_entropy,0
output:,cross_entropy,0
Read more about torch.nn.functional.cross_entropy loss function from [here (hyper-link)].,cross_entropy,0
[Deep Learning with PyTorch (hyper-link)],cross_entropy,0
Use this formula:,cross_entropy,0
[ (hyper-link)],cross_entropy,0
Here is the above example expressed in Python using Numpy:,cross_entropy,0
"So that is how ""wrong"" or ""far away"" your prediction is from the true distribution.",cross_entropy,0
A machine learning optimizer will attempt to minimize the loss (i.e.,cross_entropy,0
it will try to reduce the loss from 0.479 to 0.0).,cross_entropy,0
We see in the above example that the loss is 0.4797.,cross_entropy,0
"Because we are using the natural log (log base e), the units are in [nats (hyper-link)], so we say that the loss is 0.4797 nats.",cross_entropy,0
"If the log were instead log base 2, then the units are in bits.",cross_entropy,0
See [this page (hyper-link)] for further explanation.,cross_entropy,0
"To gain more intuition on what these loss values reflect, let's look at some extreme examples.",cross_entropy,0
"Again, let's suppose the true (one-hot) distribution is:",cross_entropy,0
Now suppose your machine learning algorithm did a really great job and predicted class B with very high probability:,cross_entropy,0
"When we compute the cross entropy loss, we can see that the loss is tiny, only 0.002:",cross_entropy,0
"At the other extreme, suppose your ML algorithm did a terrible job and predicted class C with high probability instead.",cross_entropy,0
The resulting loss of 6.91 will reflect the larger error.,cross_entropy,0
"Now, what happens in the middle of these two extremes?",cross_entropy,0
Suppose your ML algorithm can't make up its mind and predicts the three classes with nearly equal probability.,cross_entropy,0
The resulting loss is 1.10.,cross_entropy,0
[ (hyper-link)],cross_entropy,0
So to answer your original questions directly:,cross_entropy,0
Is it only a method to describe the loss function?,cross_entropy,0
"Then we can use, for example, gradient descent algorithm to find the
minimum.",cross_entropy,0
Further reading: one of my [other answers (hyper-link)] related to TensorFlow.,cross_entropy,0
 thanks okay great alright thanks everybody  see you next time for our last lesson,cross_entropy,0
"  Yeah, as promised, let me now show you a brief code example  illustrating the concept of the cross entropy in code using  pytorch.",cross_entropy,0
" So I have prepared a notebook I will share it with  you, you can find the link as usual on canvas, or here just  on GitHub.",cross_entropy,0
" Because if we wouldn't make a  mistake, we would get a loss of zero, which would be kind of  boring, I think.",cross_entropy,0
 But I'm trying to explain how  these functions and pytorch work.,cross_entropy,0
" Like I explained in when we go up, I think I had a video  on that in logits and cross entropy.",cross_entropy,0
 And then the  networks are not training.,cross_entropy,0
" So here, again, this is  our net input matrix.",cross_entropy,0
" So here, we give it the net inputs.",cross_entropy,0
" Sorry, should be like this.",cross_entropy,0
" But in this case, we also have a mask that we must apply.",cross_entropy,0
" So what we will do is we will first compute the loss in every node, applying the usual cross_entropy with logits function and calling the appropriate inputs.",cross_entropy,0
 Now we need to use the mask to only use the parts of this loss function for the nodes that belong in that particular set.,cross_entropy,0
 We'll use a TensorFlow GradientTape to record all of the gradients as we go along.,cross_entropy,0
" So we apply our Cora GNN on the features and adjacency to compute the predictions at this step, and we can compute the loss using the masked_softmax_cross_entropy on the predicted logits and the ground truth labels.",cross_entropy,0
" And in particular, we want to compute the loss on the training set, so we pass on the training mask.",cross_entropy,0
 Why is it a tensor of floats?,cross_entropy,0
" We saw in the MNIST Notebook, we didn't need it but we're gonna train faster and more accurately, if we use it, because it's just more, it's going to be better behaved, as we've seen.",cross_entropy,0
" So is the first column, you know, what...",cross_entropy,0
" So this is where it's so cool in PyTorch, we can kind of run, write, one thing and then kind of have it expand to handle higher dimensional tensors, without doing any extra work.",cross_entropy,0
" We don't have to write this ourselves, of course, because PyTorch has one and it's called �F.binary_cross_entropy�.",cross_entropy,0
 We can just use PyTorch�s.,cross_entropy,0
" As we've talked about there's always a equivalent module version so this is exactly the same thing as a module �nn.BCELoss�, and these ones don't include the initial sigmoid, actually.",cross_entropy,0
 If you want to include this initial sigmoid you need F.binary_cross_entropy_with_logits or the equivalent nn.BCEWithLogitsLoss.,cross_entropy,0
 So BCE is binary cross-entropy.,cross_entropy,0
" And so those are two functions, plus two equivalent classes for multi-label or binary problems, and then the equivalent for single label like MNIST and pets is �nll_loss� and �cross_entropy�.",cross_entropy,0
 That's the equivalent of �binary_cross_entropy� and �binary_cross_entropy_with_logits�.,cross_entropy,0
" So these are pretty awful names, I think we can all agree, but it is what it is.",cross_entropy,0
 Sure.,cross_entropy,0
 �In traditional machine learning we perform cross validations and k-fold training to check for variance and bias trade-off.,cross_entropy,0
" Is this common in training deep learning models as well?� So cross-validation is a technique where you don't just let your data set into one training set and one validation set, but you basically do it five or so times, like five training sets, and like five validation sets representing different overlapping subsets, and basically this was this used to be done a lot because people often used to have not enough data to get a good result, and so this way, rather than, kind of, having 20% that you would leave out each time you could just leave out like, 10% each time.",cross_entropy,0
" �.head� will give us the first five rows, and we mentioned just before what it looks like.",cross_entropy,0
" It's not a particularly friendly way to look at it, so what I'm going to do is I'm going to, cross tab it, and so what I've done here is I've grabbed the top, I can�t remember how many it was.",cross_entropy,0
" Well, who cares?",cross_entropy,0
"  Yeah, since we talked so much about logistic regression now, I  thought that might be a good opportunity now to introduce two  terms logits and cross entropy, because that's what I will also  use quite often later in this class.",cross_entropy,0
 And that's because also  it's a very common.,cross_entropy,0
" So there is another  concept that is maybe a little bit confusing, but it's exactly  what we've covered before.",cross_entropy,0
" Of course, you don't  have to know that it's just like a fun tidbit here.",cross_entropy,0
 And this is what we've discussed in the previous videos.,cross_entropy,0
" But this is something we of course, haven't  discussed yet.",cross_entropy,0
" Alright, so in the next video, I will show you a  logistic regression code example.",cross_entropy,0
 And then we will take  a look at this multi category cross entropy.,cross_entropy,0
 All right.,cross_entropy,0
" You know in theory   maybe that could be slightly better because  you're kind of guaranteed that every row is…   appears… four times, you know, effectively.",cross_entropy,0
 It also has a benefit that you could average   those five validation sets because there's no kind  of overlap between them to get a cross validation.,cross_entropy,0
" Personally, I generally don't bother, and  the reason I don't is because this way   I can add and remove models very  easily.",cross_entropy,0
 So in this case we have to use a  different loss function.,cross_entropy,0
" But now that we are having to   pick it out manually I'm going to explain to  you exactly what cross-entropy loss does, okay?",cross_entropy,0
" But in this particular case where we want  to predict one and one thing only, we use softmax.",cross_entropy,0
 The first part of the cross entropy formula...,cross_entropy,0
" The first part of the cross entry formula...  in fact let's look it up, nn.CrossEntropyLoss.",cross_entropy,0
" Now if you've got an eagle  eye, you may have noticed that   I am currently looking at the documentation  for something called “nn.CrossEntropyLoss”   but over here I had something  called “F.cross_entropy()”.",cross_entropy,0
" The functions   live in a… I can’t even remember what the  sub module called, I think it might be like   torch.nn.functional but everybody including the  pytorch official docs just calls it capital-F,   so that's what this capital-F refers to.",cross_entropy,0
 This is not a  particularly great way to see it.,cross_entropy,0
" I prefer to kind   of cross tabulate it, like that, like this.",cross_entropy,0
 This  is the same information.,cross_entropy,0
 So that's why it's particularly full.,cross_entropy,0
" So yeah,   so this is what kind of a collaborative filtering  dataset looks like when we cross tabulate it.",cross_entropy,0
 So how do we fill in this gap?,cross_entropy,0
The difference is small but quite significative.,cross_entropy,0
"softmax_cross_entropy_with_logits takes logits (real numbers without any range limit), passes them through the softmax function and then computes the cross-entropy.",cross_entropy,0
Combining both into one function is to apply some optimizations to improve numerical accuracy.,cross_entropy,0
"The second code just applies cross-entropy directly to y_conv, which seems to be the output of a softmax function.",cross_entropy,0
"This is correct, and both should give similar but not the same results, softmax_cross_entropy_with_logits is superior because of numerical stability.",cross_entropy,0
Just remember to give it logits and not the output of a softmax.,cross_entropy,0
"While it is great that the accepted answer contains lot more info than what is asked, I felt that sharing a few generic thumb rules will make the answer more compact and intuitive:",cross_entropy,0
We have the softmax formula which is an activation for multi-class scenario.,cross_entropy,0
"For binary scenario, same formula is given a special name - sigmoid activation",cross_entropy,0
"Because there are sometimes numerical instabilities (for extreme values) when dealing with logarithmic functions, TF recommends combining the activation layer and the loss layer into one single function.",cross_entropy,0
This combined function is numerically more stable.,cross_entropy,0
TF provides these combined functions and they are suffixed with _with_logits,cross_entropy,0
"With this, let us now approach some situations.",cross_entropy,0
Say there is a simple binary classification problem - Is a cat present or not in the image?,cross_entropy,0
What is the choice of activation and loss function?,cross_entropy,0
It will be a sigmoid activation and a (binary)CE.,cross_entropy,0
So one could use sigmoid_cross_entropy or more preferably sigmoid_cross_entropy_with_logits.,cross_entropy,0
The latter combines the activation and the loss function and is supposed to be numerically stable.,cross_entropy,0
How about a multi-class classification.,cross_entropy,0
Say we want to know if a cat or a dog or a donkey is present in the image.,cross_entropy,0
What is the choice of activation and loss function?,cross_entropy,0
It will be a softmax activation and a (categorical)CE.,cross_entropy,0
So one could use softmax_cross_entropy or more preferably softmax_cross_entropy_with_logits.,cross_entropy,0
We assume that the expected value is one-hot encoded (100 or 010 or 001).,cross_entropy,0
"If (for some weird reason), this is not the case and the expected value is an integer (either 1 or 2 or 3) you could use the 'sparse' counterparts of the above functions.",cross_entropy,0
There could be a third case.,cross_entropy,0
We could have a multi-label classification.,cross_entropy,0
So there could be a dog and a cat in the same image.,cross_entropy,0
How do we handle this?,cross_entropy,0
The trick here is to treat this situation as a multiple binary classification problems - basically cat or no cat / dog or no dog and donkey or no donkey.,cross_entropy,0
Find out the loss for each of the 3 (binary classifications) and then add them up.,cross_entropy,0
So essentially this boils down to using the sigmoid_cross_entropy_with_logits loss.,cross_entropy,0
This answers the 3 specific questions you have asked.,cross_entropy,0
The functions shared above are all that are needed.,cross_entropy,0
You can ignore the tf.contrib family which is deprecated and should not be used.,cross_entropy,0
They are the same.,cross_entropy,0
"if you run that through a softmax, you get:",cross_entropy,0
"The distribution of the true label t, which we've denoted so far by Q is thus given by",cross_entropy,0
and this brings us back to the loss which can be expressed as,cross_entropy,0
which is the one you've given in your problem.,cross_entropy,0
"In your question, x_y is therefore the scores that the network outputs for the correct label that is associated with x.",cross_entropy,0
The code y_ * tf.log(y) in the code block:,cross_entropy,0
performs an element-wise multiplication of the original targets => y_ with the log of the predicted targets => y.,cross_entropy,0
[ (hyper-link)],cross_entropy,0
Most answers already cover getContext() and getApplicationContext() but getBaseContext() is rarely explained.,getBaseContext,0
"I agree that documentation is sparse when it comes to Contexts in Android, but you can piece together a few facts from various sources.",getBaseContext,0
"[This blog post (hyper-link)] on the official Google Android developers blog was written mostly to help address memory leaks, but provides some good information about contexts as well:",getBaseContext,0
"In a regular Android application, you
  usually have two kinds of Context,
  Activity and Application.",getBaseContext,0
Reading the article a little bit further tells about the difference between the two and when you might want to consider using the application Context (Activity.getApplicationContext()) rather than using the Activity context this).,getBaseContext,0
"Basically the Application context is associated with the Application and will always be the same throughout the life cycle of your app, where as the Activity context is associated with the activity and could possibly be destroyed many times as the activity is destroyed during screen orientation changes and such.",getBaseContext,0
"The
Context referred to from inside that ContextWrapper is accessed via
getBaseContext().",getBaseContext,0
this,getBaseContext,0
"The only information that zero padding adds is the condition of border (or near-border) of the features- pixels in the limits of the input, also depending on kernel size.",MaxPool2d,0
"(You can think of it as a ""passe-partout"" in a picture frame)",MaxPool2d,0
Because your assumption that it'll loose information about the exact location is wrong.,MaxPool2d,0
" It's  entitled striving for simplicity, they're all  convolutional network.",MaxPool2d,0
" So here, the authors propose replacing  max pooling with a convolution layer that has a stride of two.",MaxPool2d,0
 And this is sometimes also called strided convolution.,MaxPool2d,0
" So  now on traditional neural network or convolution network,  we have usually a convolution layer with stride equals one.",MaxPool2d,0
" And then we have max pooling, usually two by two max pooling,  also with a stride of two.",MaxPool2d,0
" And then we have a convolutional  layer again, with stride of one, and we continue like that.",MaxPool2d,0
" And  usually, the convolution layers, they preserve the size, because  we have a stride of one.",MaxPool2d,0
" And the max pooling will reduce the size  to fold, it will have the size.",MaxPool2d,0
 So the size that comes out of it  is one half.,MaxPool2d,0
" If we have, let's say a two by two max pooling  with a stride of two, and can maybe also write the stone.",MaxPool2d,0
" So  the kernel size is usually two by two, and this is stride by  two.",MaxPool2d,0
" And this helps us also, you're achieving a little bit of  this location invariance.",MaxPool2d,0
" However, it's not essential to  have that.",MaxPool2d,0
" So you can technically get rid of this, and  then just increase the stride here by two, or here, and you  will have the same effect that you reduce the size by one half.",MaxPool2d,0
 So I don't want to go into too much detail here.,MaxPool2d,0
" So, there might possibly not be too many ah different kind of manifestations which it has to learn up and that is one of the reasons why we are just taking down to this kind of a very simple ah baseline model of a ah segmentation convol convolutional semantic segmentation network .",MaxPool2d,0
" So, what we have in this network is pretty simple, I have a convolution layer ah where the kernel sizes are 3 cross 3, and I have 64 such ah kernels ah taken down for my work Now, I do ah convolution with the stride of one and a padding of one which necessarily means that whatever was the size of my ah x y dimension of my input image .",MaxPool2d,0
" After this convolution, I am going to retain the same x y dimension over there, because I am not changing down either the stride or the padding given over here .",MaxPool2d,0
" The next point is the we put in place a non-linearity, and this is a rectified ah linear unit which is ah put in place for a non-linearity to come down .",MaxPool2d,0
" Following this, we have a max pooling of ah 2 cross 2 kernel size with a stride of two.",MaxPool2d,0
" So, this is a 2 cross 2 max pooling which means that the x and y dimensions are going to get reduced to half of it .",MaxPool2d,0
" Now, in your decoder side what you have is, ah first you have a unpooling over there which skills it up, now once you have that unpooling after that you have your convolution ah a 2D convolution coming down .",MaxPool2d,0
" So, this 2D convolutions job is to convert down 64 channels onto 3 channels .",MaxPool2d,0
" Now, this is three channels is not corresponding to the image channels over there, but this is because you have 3 different classes .",MaxPool2d,0
" Now, if you took down 6 classes then you would have 6 channels, if you take down 10 classes, then you have 10 channels over here the convolutions are still with the kernel size of 3 plus 3 with a stride of one and a padding of one which meant that the same size xy size is preserve.",MaxPool2d,0
" Now, if we look into this kind of a network, so if my input is ah 3 cross 224 cross 224, then ah after this before this max pooling just after the ReLu, it is going to be 64 cross 224 cross 224 .",MaxPool2d,0
" So, we need to use a negative log likelihood loss which is going to calculate itself on a per pixel basis or a per neuron output over this 2 d matrix .",MaxPool2d,0
 And for that there is a separate kind of a function which is called as a nll ah loss 2d .,MaxPool2d,0
" So, this is just to give down that ah whatever comes out on your output side you are just going to operate it on the 2d space.",MaxPool2d,0
" So to compute each of the numbers on the right, we took the max over a two by two regions.",MaxPool2d,0
" So, this is as if you apply a filter size of two because you're taking a two by two regions and you're taking a stride of two.",MaxPool2d,0
" So, these are actually the hyperparameters of max pooling because we start from this filter size.",MaxPool2d,0
 It's like a two by two region that gives you the nine.,MaxPool2d,0
 Let's go through an example with some different hyperparameters.,MaxPool2d,0
" Okay, so this, with this set of hyperparameters f equals three, s equals one gives that output shown [inaudible].",MaxPool2d,0
" Now, so far, I've shown max pooling on a 2D inputs.",MaxPool2d,0
" If you have a 3D input, then the outputs will have the same dimension.",MaxPool2d,0
 It can happen at any given point.,MaxPool2d,0
 So we could max pool before we apply the convolution.,MaxPool2d,0
" But typically speaking, the convolutional filters are applied.",MaxPool2d,0
" This ultimately, right there-- the x++, y++-- that's the stride.",MaxPool2d,0
" So I could say x+ equals stride, y+ equal stride.",MaxPool2d,0
 And set the stride equal to one.,MaxPool2d,0
 So that's what was happening here.,MaxPool2d,0
" To describe this, I'm going to start with an 8 by 8 image.",MaxPool2d,0
 And I'm going to do max pooling with a 2 by 2 max pooling with a stride of 2.,MaxPool2d,0
 So there are no weights.,MaxPool2d,0
 The highest one is 10.,MaxPool2d,0
" And so the max pooling algorithm takes these little neighborhoods, 2 by 2 max pooling, skips, goes from one to the other with a stride of 2.",MaxPool2d,0
 I could have just moved these neighborhoods just by one or by even a larger amount.,MaxPool2d,0
 And I'm going to create a global variable for stride.,MaxPool2d,0
" But this stride is only referring to the pooling process because then I can say, create image dim divided by stride, dimensions divided by stride.",MaxPool2d,0
" Just to add some comments for a moment, this is convolutional layer.",MaxPool2d,0
 And I don't want to end up here.,MaxPool2d,0
 So this is plus equal stride.,MaxPool2d,0
 And this is plus equal stride.,MaxPool2d,0
 I can do the same exact thing.,MaxPool2d,0
 The first layer is a convolutional layer.,MaxPool2d,0
" And I'm writing 2D because a lot of times in a machine learning library, you can have convolutions in different dimensions.",MaxPool2d,0
 And we're working with a two-dimensional convolution here.,MaxPool2d,0
 And then the output there is another image.,MaxPool2d,0
" So we take this first image, pass it through a bunch of filters, max pool them and a whole bunch of other images that, if I'm using a stride of 2, now have half the resolution as the original image.",MaxPool2d,0
 So the question becomes what to do next.,MaxPool2d,0
" And to this layer, we're arbitrarily setting the filter value equal to 32.",MaxPool2d,0
 So this first convolutional layer will have 32 filters with a kernel size of three by three.,MaxPool2d,0
" So the choice of 32 is pretty arbitrary, but the kernel size of three by three is a very common choice for image data.",MaxPool2d,0
" Now, this first column to D layer will be followed by the popular relu activation function.",MaxPool2d,0
 And we are specifying padding equals Same here.,MaxPool2d,0
" And this just means that our images will have zero padding, padding the outside so that the dimensionality of the images aren't reduced after the convolution operations.",MaxPool2d,0
" So lastly, for our first layer only, we specify the input shape of the data.",MaxPool2d,0
" And recall we touched on this parameter previously, you can think of this as kind of creating an implicit input layer for our model.",MaxPool2d,0
" This comp 2d is actually our first hidden layer, the input layer is made up of the input data itself.",MaxPool2d,0
" And so we just need to tell the model the shape of the input data, which in our case is going to be 224.",MaxPool2d,0
" Since these images are an RGB format, we have three color channels, so we specified this input shape.",MaxPool2d,0
" So then we follow our first convolutional layer with a max pooling layer, where we're setting our pool size to two by two and our strides by two.",MaxPool2d,0
" So after this max pooling layer, we're then adding a another convolutional layer, that looks pretty much exactly the same as the first one except for four, we're not including the input shape parameter, since we only specify that for our first hidden layer, and we are specifying the filters to be 64 here instead of 32.",MaxPool2d,0
 I am taking him three by three by three one.,MaxPool2d,0
" It's over Let's see our output is you know Three by three even I you know, I have given an equation for the output the output, you know, I know that I know what is the size of the output it is represented by the equation n Minus F plus one and is the you know image size F is the kernel size and plus one So this equation we discussed at that time you remember that so let's assume that this is the map Obtained from the first term, ah, you know a kernel first filtered So, let's see.",MaxPool2d,0
" This is like, let's assume that this is you know 0.5 This is minus 1 This is a one point zero zero.",MaxPool2d,0
 So in pooling layer vish ring this image stack By using here.,MaxPool2d,0
" Also, we use a filter or kernel the filter.",MaxPool2d,0
 We call it as the max pooling filter Cv the filter we use here.,MaxPool2d,0
 It is a max pooling filter This is the filter we use here by using a max bowling filter Normally the size of the max bowling filter.,MaxPool2d,0
" We take it as 2 by 2 Or 3 by 3 This is the size of the max pooling filter C and we take a stride stride of - the stride we are taking here it is 2 Stride means jump, you know when we discuss about the convolution operation The filter has to jump there we have taken a stride of 1 so that's why if the formula we got it is, you know Y n minus F plus 1 but if we are considering the stride as well or Padding this formula will be modified.",MaxPool2d,0
 We will discuss this later So here the stride we are taking it as 2 so then we will move this you know max bowling filter in our across our filtered image and after that from each Window we take the maximum intensity or the maximum value from each window so what we are going to do I will explain so here we considered a max pooling filter of Size two by two and the stride is two and we are moving this max pooling filter across the map After we got it from the rail operation and we take the maximum intensity value Or maximum pixel value from that so I will explain you what exactly it is.,MaxPool2d,0
" So This is the let us assume that this is the you know Output you got it from the this is a seven by seven map I am going to you know, draw here 1 2 3 4 5 6 7 this is up 7 1 1 2 3 4 5 6 7 1 2 3 4 5 Six seven, so this map doesn't contain any negative value.",MaxPool2d,0
 So this is some random value some arbitrary values I have taken so what exactly I am trying to explain here What is the max pulling filter do so here the max pulling filter?,MaxPool2d,0
 What it is doing here it is our max pooling filter is of I am taking it as f size of two by two and astride of two so Here the size of the filter is two by two and stride is to the jump is 2 Usually we take the stride as 2 so 2 by 2 beans.,MaxPool2d,0
" Our max pooling filter will be Like this, this is our first we place like this.",MaxPool2d,0
 Then the maximum value is - I Put it here.,MaxPool2d,0
" Then the next step, you know, I will jump to know this location the position of the map, you know, the Max bowling kernel should be this so from this Also, I will take a 1/2 means the maximum value is 2 like this.",MaxPool2d,0
" I am shrinking the input Image, you know to a you know a smaller size after that I may be getting you know, I may be getting up, you know, one two three It would be I believe it would be one two, three a four it would be a four by four, you know Size the size of the you know output will be four by four.",MaxPool2d,0
From [https://keras.io/layers/convolutional/ (hyper-link)],MaxPool2d,0
"""same"" results in padding the input such that the output has the same
  length as the original input.",MaxPool2d,0
From [https://keras.io/layers/pooling/ (hyper-link)],MaxPool2d,0
"pool_size: integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal).",MaxPool2d,0
"(2, 2) will halve the input in both spatial dimension.",MaxPool2d,0
"If only one integer is specified, the same window length will be used for both dimensions.",MaxPool2d,0
"So, first let's start by asking why use padding at all?",MaxPool2d,0
"In the convolutional kernel context it is important since we don't want to miss each pixel being at the ""center"" of the kernel.",MaxPool2d,0
There could be important behavior at the edges/corners of the image that a kernel is looking for.,MaxPool2d,0
So we pad around the edges for Conv2D and as a result it returns the same size output as the input.,MaxPool2d,0
"However, in the case of the MaxPooling2D layer we are padding for similar reasons, but the stride size is affected by your choice of pooling size.",MaxPool2d,0
"Since your pooling size is 2, your image will be halved each time you go through a pooling layer.",MaxPool2d,0
So in the case of your tutorial example; your image dimensions will go from 28->14->7->4 with each arrow representing the pooling layer.,MaxPool2d,0
Imagine we are taking the maximum value in a 2x2 region of an image.,MaxPool2d,0
"The image is pre-convolved, though it doesn't matter for the purpose of this explanation.",MaxPool2d,0
"No matter where exactly in a 2x2 region the maximum value resides, there will be 3 possible 1-pixel translations of the image that result in the maximum value remaining in that particular 2x2 region.",MaxPool2d,0
"Of course an even greater value may be brought from a neighbouring region, but that's beside the point.",MaxPool2d,0
The point is you get some translation invariance.,MaxPool2d,0
"With a 3x3 region it gets more complex, as the number of 1-pixel translations that keep the maximum value within the region depends on where exactly in the region that maximum value resides.",MaxPool2d,0
The 5 translations they mention correspond to a location in the middle of an edge in a 3x3 pixel block.,MaxPool2d,0
"A corner location will give 3 translations, while the center one will give all 8.",MaxPool2d,0
"For a single slice at an arbitrary index, you have a simple image of NxN dimensions.",MaxPool2d,0
"You defined your filter size 2, and stride 2.",MaxPool2d,0
It's not clear to me  why you're using transpose of the max row for a single element in the pool.,MaxPool2d,0
Convolutional Neural Networks do a great job in dealing with high dimensional data.,MaxPool2d,0
Restricting the number of weights only to kernels weights makes learning easier due to invariance properties of images or sound.,MaxPool2d,0
Sum pooling works in a similiar manner - by taking the sum of inputs instead of it's maximum.,MaxPool2d,0
The conceptual difference between these approaches lies in the sort of invariance which they are able to catch.,MaxPool2d,0
Max pooling is sensitive to existence of some pattern in pooled region.,MaxPool2d,0
Sum pooling (which is proportional to Mean pooling) measures the mean value of existence of a pattern in a given region.,MaxPool2d,0
UPDATE:,MaxPool2d,0
The subregions for Sum pooling / Mean pooling are set exactly the same as for Max pooling but instead of using max function you use sum / mean.,MaxPool2d,0
You can read about [here (hyper-link)] in the paragraph about pooling.,MaxPool2d,0
" 
skimage.measure.block_reduce(activation, block_size=(1, 1, 2, 2), func=np.max)",MaxPool2d,0
Your backpropagation for mean pooling is not fully correct.,MaxPool2d,0
You should devide delta by number of pooled cells (4 in your case).,MaxPool2d,0
See equation on slide 11 at [http://www.slideshare.net/kuwajima/cnnbp (hyper-link)],MaxPool2d,0
To propagate max pooling you need to assign delta only to cell with highest value in forward pass.,MaxPool2d,0
"Hence, during the forward pass of a pooling layer it is common to keep track of the index of the max activation (sometimes also called the switches) so that gradient routing is efficient during backpropagation.",MaxPool2d,0
See [http://cs231n.github.io/convolutional-networks/#pool (hyper-link)],MaxPool2d,0
Very inefficient way to implement this:,MaxPool2d,0
I'll give an example to make it clearer:,MaxPool2d,0
"x: input image of shape [2, 3], 1 channel",MaxPool2d,0
"valid_pad: max pool with 2x2 kernel, stride 2 and VALID padding.",MaxPool2d,0
"same_pad: max pool with 2x2 kernel, stride 2 and SAME padding (this is the classic way to go)",MaxPool2d,0
The output shapes are:,MaxPool2d,0
"valid_pad: here, no padding so the output shape is [1, 1]",MaxPool2d,0
"same_pad: here, we pad the image to the shape [2, 4] (with -inf and then apply max pool), so the output shape is [1, 2]",MaxPool2d,0
"According to the [official documentation (hyper-link)], any optimizer can have optional arguments clipnorm and clipvalue.",RMSprop,0
"If clipnorm provided, gradient will be clipped whenever gradient norm exceeds the threshold.",RMSprop,0
Seems that someone have sorted out (2018) the question (2017).,RMSprop,0
Link to the paper [[https://arxiv.org/pdf/1711.05101.pdf] (hyper-link)] and some intro:,RMSprop,0
"In this paper, we show that a
  major factor of the poor generalization of the most popular
  adaptive gradient method, Adam, is due to the fact that L2
  regularization is not nearly as effective for it as for SGD.",RMSprop,0
L2 regularization and weight decay are not identical.,RMSprop,0
"Contrary to common belief, the two techniques are not
  equivalent.",RMSprop,0
"For SGD, they can be made equivalent by
  a reparameterization of the weight decay factor based
  on the learning rate; this is not the case for Adam.",RMSprop,0
"In
  particular, when combined with adaptive gradients, L2
  regularization leads to weights with large gradients
  being regularized less than they would be when using
  weight decay.",RMSprop,0
"The documentation you refer to explicitly mentions:

This implementation of RMSProp uses plain momentum, not Nesterov momentum.",RMSprop,0
AFAIK there is no built-in implementation for Nesterov momentum in RMSProp.,RMSprop,0
You can of course adjust the function according to your own needs.,RMSprop,0
"As @xolodec said, g_t is the gradient.",RMSprop,0
" All we need to do is start off by importing the stuff that we need, so we're going to import the Keras library and some specific modules from it.",RMSprop,0
" We have the MNIST dataset here that we're going to experiment with; the Sequential model which is a very quick way of assembling the layers of a neural network; we're going to import the Dense and Dropout layers as well, so we can actually add some new things onto this neural network to make it even better and prevent overfitting; and we will import the RMSprop optimizer, which is what we're going to use for our gradient descent.",RMSprop,0
" Shift Enter, and you can see we've already loaded up Keras just by importing those things it's using tensorflow as the back end.",RMSprop,0
 All right.,RMSprop,0
" And you can probably improve on this even more, so again, as before with TensorFlow I want you to go back and see if you can actually improve on these results, try using a different optimizer than RMSprop, try, you know, different topologies and the beauty with Keras is that it's a lot easier to try those different topologies now, right?",RMSprop,0
" Keras actually comes in this documentation with an example of using MNIST and this is the actual topology that they use in their example, so go back and give that a try and see if it's actually any better or not, see if you can improve upon things.",RMSprop,0
" This algorithm is called Adagrad if I apply this algorithm, then what how does the situation change, fine?",RMSprop,0
" So, this is what gradient descent momentum and NAG do.",RMSprop,0
" Now, at least the difference between momentum and NAG should be clear.",RMSprop,0
 NAG blue curve is inside the red curve right?,RMSprop,0
" So, what happens because of the aggressive killing, is the frequent parameters, they start receiving fewer updates.",RMSprop,0
" Now, this is what RMSProp does, ok.",RMSprop,0
" I want you to stare at this for a minute, ok.",RMSprop,0
 Yeah some people as just lazy fine.,RMSprop,0
 There is everything that you learned so far and my everything yeah.,RMSprop,0
" From there you are now focusing on the learning rates, but there were other things which you are doing earlier.",RMSprop,0
" Can you bring those back, add momentum.",RMSprop,0
 How many of you say add momentum as if I can just added.,RMSprop,0
" You are right actually, ok.",RMSprop,0
" So, let us see what we can do.",RMSprop,0
" So, it does everything that RMSProp does.",RMSprop,0
" So, what is this term doing?",RMSprop,0
" Just taking a moving average of your gradients, ok.",RMSprop,0
" The same analogy that I am going to phoenix market city, I am just taking all my history into account, ok and vt is again a cumulative history.",RMSprop,0
" This is the same as what was happening in RMSProp, right where you get lost.",RMSprop,0
 Is this fine?,RMSprop,0
 Does it make sense?,RMSprop,0
 This is same as momentum base gradient descent.,RMSprop,0
 How many of you get that?,RMSprop,0
" Ok and now, this quantity there is nothing new.",RMSprop,0
" So, just a combination of these two, one is take care of the learning rate and the other is use a cumulative history.",RMSprop,0
" So, what is happening, it is taking u-turns, right?",RMSprop,0
" So, again whatever happens because of momentum, it is happening in this case also and then, finally it will converge again.",RMSprop,0
 Let me be clear that in this case now it should be very clear.,RMSprop,0
" Even if you have not read the assignment, you should just tell me based on whatever you have learned you have gradient descent.",RMSprop,0
 Momentum.,RMSprop,0
 Momentum.,RMSprop,0
 Nag RMSProp.,RMSprop,0
 Adagrad.,RMSprop,0
" So,   in our previous class we have talked  about the Adagrad algorithm in particular.",RMSprop,0
" So, as we have seen in the previous class the  Adagrad algorithm is given something like this,   that at time t you compute the batch gradient;   gradient of the loss function with respect  to the weight vector or with respect to the   parameter vector.",RMSprop,0
" Now, let us see that what is  this exponentially decaying squared gradient.",RMSprop,0
" So, all  the previous sample values as I go   as I compute say s 50 the previous sample  values the effect of s 1 s 2 s 3 and so on,   that will go on reducing exponentially.",RMSprop,0
" And,  that is the advantage of taking the exponentially   decaying average of the square gradients and this  is what is used in case of RMSProp problem.",RMSprop,0
" So, we have a closely related algorithm known as  AdaDelta.",RMSprop,0
" So, what AdaDelta does is in case of   RMSProp you are taking the exponentially decaying  average of the squared gradient; AdaDelta instead   of taking the exponentially decaying average of  squared gradient it computes the moving window   average.",RMSprop,0
" So, you can take a more window size of  say W. So, when you compute v t, v t is computed   over a past window size of W. So, if I take the  window size W is equal to say 5, in that case in   order to compute say v 10 it will take the first  5; that means, it will take v 10 v 9 v 8 v 7 and   v 6 or the say s 6 s 7 s 8 s 9 and s 10.",RMSprop,0
" So, you are computing average over past   samples which are within this window size of W.  So, this is what is moving window average.",RMSprop,0
" So,   you find that this RMSProp which takes  expose exponentially decaying average,   the AdaDelta takes a moving window average of  the squared gradients.",RMSprop,0
" So, that is the only   difference between RMSProp and AdaDelta.",RMSprop,0
" And  in fact, both these algorithms were proposed   almost simultaneously, but independently, and  both of them gives almost similar performance.",RMSprop,0
" So, this is what is you are RMSProp algorithm,  you can also improvise upon RMSProp algorithm   with an Nesterov of momentum term.",RMSprop,0
" So,  as we have seen that in case of Nesterov   of accelerated gradient technique you take  the gradient not at location W t, but you   take the gradient at a loop ahead position.",RMSprop,0
" So,  assuming that your previous momentum term is v,   you are looking ahead at location W t plus alpha  times v and you are taking the gradient at that   location W t plus alpha times v instead  of computing the gradient at location W t. So, that is what your Nesterov accelerated  gradient is.",RMSprop,0
" So, you are instead of computing   the gradient at location W t, if I take  the gradient at location W delta where,   W delta is the position or to loop ahead position.",RMSprop,0
" And, rest of the algorithms remains the same; so,   it is as before you are taking the exponentially  decaying average of the sum of squared gradients,   exponentially decaying average of the squared  gradients and then your update rule remains as   before ok.",RMSprop,0
" So, this is what is RMSProp, which is  part that improvised with Nesterov of momentum.",RMSprop,0
" So, given this algorithm now the  other algorithm that we said that   we will be talking about is what is  known as Adam or adaptive moments.",RMSprop,0
" So, what is this Adam algorithm?",RMSprop,0
 Adam algorithm  you can consider this to be variant of the   combination of RMSProp and momentum.,RMSprop,0
" So, here we can incorporate both  the first order momentum and the second order   momentum.",RMSprop,0
" Second order momentum is nothing, but  the sum of squared gradients or exponentially   decaying average of the squared gradients as in  case of RMSProp which is used for scaling the   learning rate in individual directions.",RMSprop,0
" And,  along with that if we add the momentum term,   where the momentum will be scaled according to the  square root of the exponentially decaying average.",RMSprop,0
" So, if I use both this first order momentum  and second order momentum because sum of   squared gradients is nothing, but average of  the squared gradients is nothing, but equivalent to your second order momentum.",RMSprop,0
" So, you  use both this first order and second   order moment that becomes a variant of  RMSProp and this is what is known as Adam.",RMSprop,0
" So, in case of Adam you are including both  first and second moments for weight updation   or parameter updation and in addition  you Adam incorporates one more term,   that it tries to correct the bias  to account for initial to zero.",RMSprop,0
" So, once given this your weight updation of the  parameter updation rule simply becomes W t plus   1 is equal to W t minus eta times t hat, where  s t hat is the bias corrected first moment upon   square root of epsilon I plus r t hat, where r  t hat is the bias corrected second moment.",RMSprop,0
" So,   you find that this is nothing, but similar to  your RMSProp algorithm where you are incorporating   where, this Adam algorithm incorporates bias  correction operation and it also incorporates the   first moment in the update step.",RMSprop,0
" So, this is eta  times s t hat where, s t hat is the first moment   of the gradients.",RMSprop,0
" So, as you see over here this red curve,   the red curve is actually the pure SGD  algorithm or Stochastic Gradient Algorithm,   the blue one gives you the momentum.",RMSprop,0
" Then you  have NAG Nesterov accelerated gradient operation,   then you have Adagrad, then you have a  Adadelta, then you have RMSProp.",RMSprop,0
" So, you   find over here that the SGD which you find that  it is still diver converging whereas, the other algorithms have already come first.",RMSprop,0
" So, it moves like this.",RMSprop,0
" And, as  it moves down this hilly terrain,   it gains momentum.",RMSprop,0
" And subsequently the  ball reaches this minimum point which is   the plateau and because it has a momentum,  it will overshoot the plateau and will start   moving in the other direction.",RMSprop,0
 And you find that   you need large number of iterations because  the gradient in one direction or gradient in   that direction of W 2 is much larger than  the gradient in the direction of W 1.,RMSprop,0
" So,   I can avoid this problem if I  bring in a concept of momentum.",RMSprop,0
" So,   the concept is something like this that  again I assume the ball is over here,   its gradient force acting on this, so  the ball comes somewhere over here.",RMSprop,0
" Now, at this location the gradient working on this  ball may be in this direction, whereas if I also   consider the component of the momentum that is a  force due to momentum of the ball which is in this   direction.",RMSprop,0
" So, if I consider both this gradient  force as well as this momentum force to find out   what will be the net force which is acting  in this ball.",RMSprop,0
" So, if you remember the gradient descent algorithm   works like this, that I want to find out  the weight the parameter at time t plus 1,   and this parameter at time t plus 1 is obtained  from the parameter of the weight at time t minus   gradient of the loss function, where loss is  a function of the parameter or the function of   the weight vector W. And this gradient has to  be taken with respect to my parameter vector   which is W. This is what is my normal  stochastic gradient descent algorithm.",RMSprop,0
 And now what I do is in addition to this  I want to add a momentum term right.,RMSprop,0
" So,   I assume that at time instant t that is W t is  somewhere over here this is the location of W t.   And it comes to W t from location W t minus 1 and  with a gradient vector which is v t minus 1.",RMSprop,0
" And   at this location W t, I have  two forces acting on it once   is the gradient force which is this  gradient of L W with respect to W,   and other one I consider is the momentum  force which is some gamma times v t minus 1.",RMSprop,0
" So, what I am adding is I am adding this  momentum term to this gradient force.",RMSprop,0
" So,   as a result, W t minus 1 the weight updation  equation will now be W t plus 1 is equal to   W t minus this gradient term which I had before  it remains as it is L W t. In addition to this,   what I am adding over here is the momentum  term which is nu v t minus 1.",RMSprop,0
" So, this is   the momentum term, and this is the gradient term.",RMSprop,0
" So, under the influence of these two now the net   update direction of the weight vector will be  in this direction instead of in this direction,   and that is how your gradient descent  approach with momentum improves the   rate of convergence or makes this back  propagation learning more efficient.",RMSprop,0
" So, when I  consider this momentum term the gradient descent   algorithm or a stochastic gradient descent  algorithm becomes much more efficient.",RMSprop,0
" Now,   there has been another modification on  this momentum approach or the stochastic   gradient descent or with momentum that has been  suggested which improves this momentum optimizer   to a certain extent and that is what is Nesterov  accelerated gradient approach or known as NAG.",RMSprop,0
" So, what is the NAG?",RMSprop,0
" You find that in every case  with momentum, what I am assuming is same i at   time t, so I have W t somewhere over here, I come  to W t with a momentum with a previous force which   is equal to v t minus 1.",RMSprop,0
" And due to this I have  a momentum term which is nu times v t minus 1,   I have the gradient at this location which  is delta L. And under influence of these two,   the net displacement or the weight  updation which will be done parameter   updation will be given by the sum of these  two vectors which is in this direction.",RMSprop,0
" Now, how this acceleration can be done or  the gradient can be accelerated is this,   if I know beforehand because I know what  is my momentum.",RMSprop,0
 And I assume that due to   a effect of this momentum what will be my  position after effect of this momentum that   is where I am going to lead over here.,RMSprop,0
" And if  I know that what is going to be my position   because of the momentum effect in future,  then instead of considering the gradient at   this location I can find out what will be  the gradient at this particular location.",RMSprop,0
" So,   if I can have my step size in the vertical  direction lower than the step size in the   horizontal direction, the learning will be  much more efficient, but which is not done   with this momentum based optimizer or even this  Nesterov of accelerated gradient approach.",RMSprop,0
" So,   that is another problem which is faced  in momentum optimizer as well as in NAG.",RMSprop,0
 You also find that the high dimensional and  mostly non convex nature of the loss function   that may look to different sensitivity  or different dimensions.,RMSprop,0
"Sometimes, authors make clear that the subject expression is just a loose description, e.g.",RMSprop,0
in the (highly recommended) [Overview of gradient descent optimization algorithms (hyper-link)] (emphasis added):,RMSprop,0
"Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum.",RMSprop,0
"or in [Stanford CS231n: CNNs for Visual Recognition (hyper-link)] (again, emphasis added):",RMSprop,0
"That said, it's true that some other frameworks actually include a momentum parameter for Adam, but this is actually the beta1 parameter; here is [CNTK (hyper-link)]:",RMSprop,0
"momentum (float, list, output of momentum_schedule()) – momentum schedule.",RMSprop,0
Note that this is the beta1 parameter in the Adam paper.,RMSprop,0
"For additional information, please refer to the [this CNTK Wiki article (hyper-link)].",RMSprop,0
"So, don't take this too literally, and don't loose your sleep over it.",RMSprop,0
The bug is here:,RMSprop,0
"Since the reference to weights is shared between the two routines, every time Adam.minimize_trace and RMSprop.minimize_trace run, they modify the same array.",RMSprop,0
"Since the path is derived from the array, the path on both become the same.",RMSprop,0
"If you copy the array before passing it to the two constructors, it should work as expected.",RMSprop,0
"Wording (however unfortunate) of ""leaky"" refers to the fact how much of the previous estimate ""leaks"" to the current one, since",RMSprop,0
Adam (adaptive moments) Calls the 1st and 2nd power of the gradient moments and uses a momentum-like decay on both moments.,RMSprop,0
"In addition, it uses bias correction to avoid initial instabilities of the moments.",RMSprop,0
How to chose one?,RMSprop,0
Depends on the problem we are trying to solve.,RMSprop,0
It's more empirical than mathematical,RMSprop,0
Seems that someone have sorted out (2018) the question (2017).,RMSprop,0
Link to the paper [[https://arxiv.org/pdf/1711.05101.pdf] (hyper-link)] and some intro:,RMSprop,0
"In this paper, we show that a
  major factor of the poor generalization of the most popular
  adaptive gradient method, Adam, is due to the fact that L2
  regularization is not nearly as effective for it as for SGD.",RMSprop,0
L2 regularization and weight decay are not identical.,RMSprop,0
"Contrary to common belief, the two techniques are not
  equivalent.",RMSprop,0
"For SGD, they can be made equivalent by
  a reparameterization of the weight decay factor based
  on the learning rate; this is not the case for Adam.",RMSprop,0
"In
  particular, when combined with adaptive gradients, L2
  regularization leads to weights with large gradients
  being regularized less than they would be when using
  weight decay.",RMSprop,0
And you can refer to [[4] (hyper-link)] to see what benefits ReLU provides.,Tanh,0
Accordining to about 2 years machine learning experience.,Tanh,0
I want to share some stratrgies the most paper used and my experience about computer vision.,Tanh,0
Normalizing well could get better performance and converge quickly.,Tanh,0
"Most of time we will subtract mean value to make input mean to be zero to prevent weights change same directions so that converge slowly [[5] (hyper-link)] .Recently google also points that phenomenon as internal covariate shift out when training deep learning, and they proposed batch normalization [[6] (hyper-link)] so as to normalize each vector having zero mean and unit variance.",Tanh,0
More training data could generize feature space well and prevent overfitting.,Tanh,0
"In computer vision if training data is not enough, most of used skill to increase training dataset is data argumentation and synthesis training data.",Tanh,0
ReLU nonlinear acitivation worked better and performed state-of-art results in deep learning and MLP.,Tanh,0
"Moreover, it has some benefits e.g.",Tanh,0
simple to implementation and cheaper computation in back-propagation to efficiently train more deep neural net.,Tanh,0
"However, ReLU will get zero gradient and do not train when the unit is zero active.",Tanh,0
Hence some modified ReLUs are proposed e.g.,Tanh,0
"Leaky ReLU, and Noise ReLU, and most popular method is PReLU [[7] (hyper-link)] proposed by Microsoft which generalized the traditional recitifed unit.",Tanh,0
choose large initial learning rate if it will not oscillate or diverge so as to find a better global minimum.,Tanh,0
shuffling data,Tanh,0
"In deep learning the ReLU has become the activation function of choice because the math is much simpler from sigmoid activation functions such as tanh or logit, especially if you have many layers.",Tanh,0
"To assign weights using backpropagation, you normally calculate the gradient of the loss function and apply the chain rule for hidden layers, meaning you need the derivative of the activation functions.",Tanh,0
"ReLU is a ramp function where you have a flat part where the derivative is 0, and a skewed part where the derivative is 1.",Tanh,0
This makes the math really easy.,Tanh,0
"If you use the hyperbolic tangent you might run into the fading gradient problem, meaning if x is smaller than -2 or bigger than 2, the derivative gets really small and your network might not converge, or you might end up having a dead neuron that does not fire anymore.",Tanh,0
 The Neural Network is the result of combining multiple artificial neurons.,Tanh,0
" For example, in the image classification literature, if you input the image to this input layer and enable Neural Network computation, you will get the classification result on the output layer as the calculation result of the Neural Network.",Tanh,0
" For example, this is 0 in handwritten digit, and you get a classification result of 0.",Tanh,0
 The Neural Network shown here is a four-layer Deep Neural Network that performs handwritten digit classification.,Tanh,0
 The input is a monochrome 28x28 pixel handwritten digit.,Tanh,0
" And the number of neurons is 1000 in the first layer, 300 in the next layer, and then 100, and 10 in the last layer.",Tanh,0
" This 4-layer Neural Network can be represented by a combination of 8 functions, as shown below.",Tanh,0
" From the top are Affine, Tanh, Affine, Tanh, Affine, Tanh, Affine, and Softmax.",Tanh,0
 The Deep Neural Network can be represented by a combination of these eight functions.,Tanh,0
" The first one is Affine, which is a function called fully-connected layer.",Tanh,0
" Since the input neuron is a monochrome image of 28x28 pixels, there are 784 input neurons of 28x28 in total, in this Affine function.",Tanh,0
 And the output neurons in the first layer were 1000.,Tanh,0
 It is called a fully-connected layer because the input and output neurons are all connected in all combinations.,Tanh,0
" As explained in the previous video on artificial neurons, the value of this output neuron is determined by multiplying and adding the values of all the input neurons by different weights w, respectively.",Tanh,0
" Multiply this first neuron by a weight w and add, then multiply it by another weight w and add, then multiply it by another and add, then multiply it by another and add, for all 784 of these neurons, and you have the value of the output neuron here.",Tanh,0
" Then, to determine the value of the next output neuron, we again multiply these 784 input neurons by a different weight, w, than we did earlier, and add.",Tanh,0
 This function called Affine does all these processes for these 1000 output neurons.,Tanh,0
 The weight w exists for all combinations of inputs and outputs.,Tanh,0
" For example, if the number of input neurons in our example is 784 and the number of output neurons is 1000, that means that there are 784,000 weights w here, which is the result of 784x1000.",Tanh,0
 This function Affine may be called by a different name depending on the arrival of the Neural Network.,Tanh,0
" In addition, since all of these inputs and outputs are densely connected, it is called Dense, and many other variations exist.",Tanh,0
 Tanh looks like this when written in a graph.,Tanh,0
 The horizontal axis is the input value and the vertical axis is the output value.,Tanh,0
 And I would like to remind you of the formula for artificial neurons.,Tanh,0
 Then there was an activation function that performed nonlinear processing on the output result .,Tanh,0
 This time we use Tanh as the activation function.,Tanh,0
 There are many other types of activation functions besides Tanh.,Tanh,0
 Here I will explain using this traditional activation function Tanh .,Tanh,0
" By repeating this three times, a three-layer Neural Network is constructed.",Tanh,0
 Let's take a look at the functions used in the Convolutional Neural Network.,Tanh,0
" At the bottom of this section, we have Convolution, MaxPooling, and Tanh.",Tanh,0
 Here we have new functions called Convolution and MaxPooling.,Tanh,0
" After that, repeat this Convolution, MaxPooling, and Tanh .",Tanh,0
" And after this, it's exactly the same as the Deep Neural Network I mentioned earlier.",Tanh,0
" The combination of these functions, Afifne, Tanh, Affine and Softmax, make up this Convolutional Neural Network.",Tanh,0
" So, the difference from the previous one is that the functions Convolution and MaxPooling are used.",Tanh,0
" For now, it suffices to know that Convolution is to process images like this.",Tanh,0
" Normally, when processing an image, a single image is output for each input image.",Tanh,0
" However, in this Neural Network world, Convolution applies several different filters to the input image, in this case six different filters, and outputs six different images.",Tanh,0
" The MaxPooling function will down-sample the image in half lengthwise and widthwise, and at this point we will have six 12x12 pixel images in half lengthwise and widthwise.",Tanh,0
" Now, we'll run MaxPooling again to halve the resolution, in other words, to 16 4x4 pixel images with half the height and the width.",Tanh,0
" After this, we have the same structure as Deep Neural Network with Affine, Tanh, Affine and Softmax.",Tanh,0
" At this point we have 4x4x16 pixels, or in other words, 256 neurons, so we take 256 neurons as input and perform a full connection and Tanh with 120 neurons as output.",Tanh,0
" And from there, we do Affine with 10 neurons as output.",Tanh,0
" Now, let's talk about the Convolution process, comparing it to Affine.",Tanh,0
" In Affine, the input and output neurons are fully connected, that is, each output neuron receives the values of all the input neurons.",Tanh,0
" On the other hand, the difference in convolution is that in a 5x5 Convolution, for example, the top left pixel of this output image is only connected to the neurons in the top left neighboring area of the 5x5 pixel.",Tanh,0
" One more thing is the weight w. Earlier, in the case of Affine, if one output neuron was different, it was multiplied and added by a completely different weight w. In the case of Convolution, we use the common weight w when calculating the top left pixel, when calculating the one next to it, and when calculating the bottom right pixel.",Tanh,0
" In the case of the fully-connected layer, if there were 784 input neurons and 1,000 output neurons, there were 784,000 weights w of 784x1,000.",Tanh,0
" In the case of Convolution, the 25 weights w, which are 5x5 are commonly used throughout the image, so there are only 25 weights w per image processing session.",Tanh,0
" And what is often asked is the total number of Neural Networks, the number of neurons in each layer, and in the case of the Convolutional Neural Network, the number of images, and the type of activation function.",Tanh,0
" We have introduced Tanh as an activation function earlier, but there are many other types of activation functions.",Tanh,0
 I'm often asked how to determine this.,Tanh,0
" In the case of binary classification problems, we use a function called Sigmoid instead of Softmax as the last activation function.",Tanh,0
" This function, Sigmoid, is similar to the Tanh function we introduced earlier.",Tanh,0
 The value from 0 to 1 output by this Sigmoid is treated as a 0% to 100% probability.,Tanh,0
" So, let's wrap up this video.",Tanh,0
" The number of input and output neurons in the Neural Network is determined by the size of the input data, depending on the problem you want to solve.",Tanh,0
 The number of neurons in the output is then determined according to the size of the answer you want to get as the output of the Neural Network.,Tanh,0
" In the example of handwritten digit classification, the number of input neurons was 28x28 since the image was 28x28 pixels, and the answer we wanted to get was a digit from 0 to 9, so the number of output neurons was 10 neurons corresponding to each digit from 0 to 9.",Tanh,0
 We first determine the number of neurons in the input and output.,Tanh,0
 Then select the last activation and loss functions according to the problem you want to solve.,Tanh,0
 So this is sigmoid of i.,Tanh,0
 This would be sigmoid f. This would be tanh of g. And this one is sigmoid of o.,Tanh,0
" And next, we have the two main equations, really, of an LSTM.",Tanh,0
" X is this dimension of the input and T would be the sequence length, right?",Tanh,0
 That's how we define our X input here.,Tanh,0
" Now the question is, suppose that I want to process multiple batches of this at once.",Tanh,0
 So a little bit different.,Tanh,0
 Or H in for the input size.,Tanh,0
 H out for the hidden size.,Tanh,0
 We have all our dimensions for PyTorch to be happy.,Tanh,0
" So let's input h, h0, and c0.",Tanh,0
" And let's see, okay.",Tanh,0
 You have to add support for that.,Tanh,0
" And I mean, tanh actually isn't too bad because we already have exponential, so.",Tanh,0
" As long as you do all that, implement it in needle, then to get the gradients with respect to all your parameters, you just call loss.backward and, you know, opt.step, right?",Tanh,0
" Again, this is all conceptual, so I might be making a typo here.",Tanh,0
" Hopefully not, but... We would finally compute some loss between our output and the input there, in the target there, l equals some loss, and then we would just say l.backward and opt.step.",Tanh,0
 That would be our training of a deep LSTM.,Tanh,0
 We just go with the real part and that is sufficient for our calculation.,Tanh,0
" So well, that was all regarding the tanh function in activation functions for deep learning.",Tanh,0
 So hope you guys enjoyed this video if you found you got educated by this video.,Tanh,0
 You can find out the link as well into description.,Tanh,0
" So guys now coming to Rectified Linear Unit or ReLU function as name suggest it acts like a linear function, but is, in fact, a nonlinear function allowing complex relationships in the data to be learned.",Tanh,0
Sometimes it depends on the range that you want the activations to fall into.,Tanh,0
"Whenever you hear ""gates"" in ML literature, you'll probably see a sigmoid, which is between 0 and 1.",Tanh,0
"In this case, maybe they want activations to fall between -1 and 1, so they use tanh.",Tanh,0
"[This page (hyper-link)] says to use tanh, but they don't give an explanation.",Tanh,0
[DCGAN (hyper-link)] uses ReLUs or leaky ReLUs except for the output of the generator.,Tanh,0
Makes sense - what if half of your embedding becomes zeros?,Tanh,0
Might be better to have a smoothly varying embedding between -1 and 1.,Tanh,0
"I'd love to hear someone else's input, as I'm not sure.",Tanh,0
"Sigmoid specifically, is used as the gating function for the three gates (in, out, and forget) in [LSTM (hyper-link)], since it outputs a value between 0 and 1, and it can either let no flow or complete flow of information throughout the gates.",Tanh,0
"But before making a choice for activation functions, you must know what the advantages and disadvantages of your choice over others are.",Tanh,0
I am shortly describing some of the activation functions and their advantages.,Tanh,0
Sigmoid,Tanh,0
Mathematical expression: sigmoid(z) = 1 / (1 + exp(-z)),Tanh,0
First-order derivative: sigmoid'(z) = -exp(-z) / 1 + exp(-z)^2,Tanh,0
Hard Tanh,Tanh,0
Mathematical expression: hardtanh(z) = -1 if z < -1; z if -1 <= z <= 1; 1 if z > 1,Tanh,0
First-order derivative: hardtanh'(z) = 1 if -1 <= z <= 1; 0 otherwise,Tanh,0
Advantages:,Tanh,0
ReLU,Tanh,0
"Mathematical expression: relu(z) = max(z, 0)",Tanh,0
First-order derivative: relu'(z) = 1 if z > 0; 0 otherwise,Tanh,0
Advantages:,Tanh,0
Leaky ReLU,Tanh,0
"Mathematical expression: leaky(z) = max(z, k dot z) where 0 < k < 1",Tanh,0
First-order derivative: relu'(z) = 1 if z > 0; k otherwise,Tanh,0
Advantages:,Tanh,0
[This paper (hyper-link)] explains some fun activation function.,Tanh,0
You may consider to read it.,Tanh,0
Many of the answers here describe why tanh (i.e.,Tanh,0
"(1 - e^2x) / (1 + e^2x)) is preferable to the sigmoid/logistic function (1 / (1 + e^-x)), but it should noted that there is a good reason why these are the two most common alternatives that should be understood, which is that during training of an MLP using the back propagation algorithm, the algorithm requires the value of the derivative of the activation function at the point of activation of each node in the network.",Tanh,0
"While this could generally be calculated for most plausible activation functions (except those with discontinuities, which is a bit of a problem for those), doing so often requires expensive computations and/or storing additional data (e.g.",Tanh,0
"the value of input to the activation function, which is not otherwise required after the output of each node is calculated).",Tanh,0
"You have so many mistakes it's hard to fix them all, you also didn't give a data sample.",Tanh,0
I would suggest you start with a simpler model and read the documentation first.,Tanh,0
From the DCGAN paper [Radford et al.,Tanh,0
[https://arxiv.org/pdf/1511.06434.pdf] (hyper-link)]...,Tanh,0
"""The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output
layer which uses the Tanh function.",Tanh,0
"We observed that using a bounded activation allowed the model
to learn more quickly to saturate and cover the color space of the training distribution.",Tanh,0
"Within the
discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al., 2015) to work
well, especially for higher resolution modeling.",Tanh,0
"This is in contrast to the original GAN paper, which
used the maxout activation (Goodfellow et al., 2013).""",Tanh,0
"It could be that the symmetry of tanh is an advantage here, since the network should be treating darker colours and lighter colours in a symmetric way.",Tanh,0
"Sigmoid also seems to be more prone to local optima, or a least extended 'flat line' issues.",Tanh,0
[http://playground.tensorflow.org/ (hyper-link)] <- this site is a fantastic visualisation of activation functions and other parameters to neural network.,Tanh,0
Not a direct answer to your question but the tool 'provides intuition' as Andrew Ng would say.,Tanh,0
I will suggest using the fragment replacement with Tag  to produce the same result .,addToBackStack,0
Add the fragment B  to activity with tag,addToBackStack,0
"Fragment A -> Fragment B [onBackPressed] -->Fragment A
Override the onBackPressed() in the Activity files where ,",addToBackStack,0
// check for fragment B and you are viewing  fragment B,addToBackStack,0
Here is a picture that shows the difference between add() and replace(),addToBackStack,0
[ (hyper-link)],addToBackStack,0
So add() method keeps on adding fragments on top of the previous fragment in FragmentContainer.,addToBackStack,0
While replace() methods clears all the previous Fragment from Containers and then add it in FragmentContainer.,addToBackStack,0
Let's understand both,addToBackStack,0
Case 1:,addToBackStack,0
[ (hyper-link)],addToBackStack,0
Case 2:,addToBackStack,0
[ (hyper-link)],addToBackStack,0
"The 'tag' used in add/replace(id, fragment, tag) is used to retrieve the fragment by calling fragmentManager.findFragmentByTag(tag).",addToBackStack,0
" Now, a task, in and of itself, isn't very complicated.",addToBackStack,0
 It's simply a stack of activities.,addToBackStack,0
" As you call start activity, that pushes a new activity onto the task's back stack.",addToBackStack,0
" The Back button reverses this, calling finish on the current activity, popping it from the stack, and taking the user back to where they were.",addToBackStack,0
 Hence the name.,addToBackStack,0
 This symmetric push-pop model also applies to fragments.,addToBackStack,0
" Remember, there's that fancy overview screen for switching tasks.",addToBackStack,0
" So for a notification that points to an activity deep within your app, you really don't want that first press of the Back button to take you immediately to the launcher.",addToBackStack,0
" Not when every other time you're looking at that same screen, the Back button does something different, like go back in your app.",addToBackStack,0
 That's where TaskStackBuilder comes in.,addToBackStack,0
" It builds a synthetic, i.e.",addToBackStack,0
" fake, back stack.",addToBackStack,0
" By default, based on the parent activities you attached to each activity entry in your manifest.",addToBackStack,0
 You just saved them some time and effort getting there.,addToBackStack,0
" So check out the blog post linked in the description for all the details on tasks and the back stack, plus some of the other flags and launch modes you probably shouldn't use.",addToBackStack,0
" Well, right up until they're the perfect thing to use to build better apps.",addToBackStack,0
" So, 120 volts and 10 ohms, 12 amps, 1440 watts.",addToBackStack,0
 Now in the previous version of the app I had a back button here and-- or I could use this back button.,addToBackStack,0
 And that's the point that I want to return to.,addToBackStack,0
 So I'm going to add in my transaction addToBackStack.,addToBackStack,0
 And then optionally you can give this a name.,addToBackStack,0
 So I'll type a 120 volts and 10 ohms calculate.,addToBackStack,0
" Now, when I hit the back button it returns me to the input form.",addToBackStack,0
" And let's go ahead and do 220 calculate, 22 amps, back button and so on.",addToBackStack,0
 So now the back button is working the same.,addToBackStack,0
 And I just want to point out one more thing about this.,addToBackStack,0
 So let's go ahead and run it again.,addToBackStack,0
 It should do the same thing as before.,addToBackStack,0
" So 110, 10, 11 amps, back, I return to the same point, 220, back, return to the same point.",addToBackStack,0
 So bottom line is if this can be confusing when you're starting out because you think you're returning to the activity with the output form but you're actually returning to the activity before this commit happens.,addToBackStack,0
 And individually these problems  are pretty tractible but if you  look at a real world example you can see that they can get pretty hairy.,addToBackStack,0
 So say that I have an item  screen  saved in my app and it's accessible via deep link  but if someone navigated to the  screen opening the app from the  home screen they would have a  couple other screens on the back stack.,addToBackStack,0
" So hitting it up we want them to take it not out of the app from  the screen,  we want them to go to the  category screen and then the  home screen.",addToBackStack,0
 If someone deep  links into the app we  need to synthesize these screens and add  them to the up stack before  showing the screen.,addToBackStack,0
" It's when  you're in the middle of writing  the code to do this to  synthesize these screens and add them to  the up and back stack but only  on a deep link, that's when you  start to feel like maybe you're  solving a failure of the  framework.",addToBackStack,0
 So it helps all problems like  that that we are launching  navigation.,addToBackStack,0
" And that let's you define a set  of available navigation actions, arguments you can pass from  place to place, things like  visual transitions and a single  navigate call activates all that at run time.",addToBackStack,0
 And so last but not least that  means one thing you never have  to worry about  again is torching is touching a fragment  transaction with your bare  hands.,addToBackStack,0
 [Applause].,addToBackStack,0
" We are  going to say if they have gotten to this congratulations screen,  that means the game is over.",addToBackStack,0
 So hitting back shouldn't take them back into a game that no longer  exists.,addToBackStack,0
 So what that means is we want to on  that action say let's pop to the match screen and that means I'm  going to pop off everything on  the back stack in  between this destination and the match screen.,addToBackStack,0
" So when the user  gets to the congratulations  screen, when they hit  back, they're just going to go  straight to the match screen.",addToBackStack,0
 So a lot of other options I can  set but I'll just talk about  that for now.,addToBackStack,0
 Let's go back and look at the  congratulations screen again.,addToBackStack,0
" One other thing to mention, the  key thing that is set here is  the fragment class.",addToBackStack,0
 First  let's see what they're trying to resolve.,addToBackStack,0
 Let's go back to our  sample.,addToBackStack,0
 Our fragment where we tried to  negate  actually requires us the best  screen name argument.,addToBackStack,0
 Best category which has integer  type.,addToBackStack,0
 Let's go back to the  calling site.,addToBackStack,0
 Well in our  slides we made everything  correctly.,addToBackStack,0
 We don't program don't program critical medical  diseases medical devices from  our phones.,addToBackStack,0
" I'll talk about a few use cases, medical, financial and  enterprise but  the key innovation is protected  confirmation is the first time  that we now have the ability to  execute a high assurance  transaction, a user transaction  completely within secure  hardware, running in a trusted  execution environment or TE,  that runs separate from the main operating system.",addToBackStack,0
 So how does it work?,addToBackStack,0
 It's guarded in this  area.,addToBackStack,0
 And the entire  transaction is signed using a  transcriptd key that never  leaves that security area.,addToBackStack,0
 This provides higher assurance  to the relying party that the  integrity of this data was not  corrupted even if you had  root level malware it cannot  corrupt the integrity of that  transaction.,addToBackStack,0
" 1500  can't be changed to 15,000.",addToBackStack,0
" The relying party on the other  end has  high confidence that we intended to send  ravy $1,500 and the transaction  goes through.",addToBackStack,0
 Duo security is a firm that is  working on strong enterprise  authentication.,addToBackStack,0
 Payments is another.,addToBackStack,0
 In all of these use cases you  want to make sure that your  phone and only your phone can  make that transaction.,addToBackStack,0
 There  are quite a few other examples  where we benefit from stronger  protect from private keys.,addToBackStack,0
 With a camera API it will behave as if  you were preempted by a higher  priority camera clients.,addToBackStack,0
 With the sensors it's whether  it's continuously or via call  back.,addToBackStack,0
 If the app is in the background  you can no longer access data  from sensors.,addToBackStack,0
 They think  it's something only for experts.,addToBackStack,0
 If you look back about 60 years  ago this is definitely the case.,addToBackStack,0
 This is a picture of the first  neural  network invented in 1957 and  this was a device that  demonstrated an ability to  identify different shapes.,addToBackStack,0
 Excited to score tickets to  Hamilton.,addToBackStack,0
 Let's see what we get back here.,addToBackStack,0
 Cranking.,addToBackStack,0
 That is the  vision API.,addToBackStack,0
 If we can go back  to the slides.,addToBackStack,0
 Next I want to talk about the  natural language API which let's you analyze text with a single  rest API request.,addToBackStack,0
 It's going to look similar to  the API vision code we saw on  the previous page.,addToBackStack,0
 We send in  our text and get back the result from the model.,addToBackStack,0
 Let's jump to a demo of the natural language  API.,addToBackStack,0
 That is the natural language  API.,addToBackStack,0
 If we can go back to the slides.,addToBackStack,0
 I'll talk briefly about  companies using these APIs in  production.,addToBackStack,0
 The next part is generating  predictions on new data.,addToBackStack,0
 I'm  going to take this go back and  improve my training data.,addToBackStack,0
 The  next part is generating  predictions on new data.,addToBackStack,0
 So  that is auto ML vision.,addToBackStack,0
 If we  can go back to the slides.,addToBackStack,0
 A  little bit about companies that  are using Auto ML vision and  have been part of the alpha.,addToBackStack,0
 If you double click on the  component it will jump  automatically.,addToBackStack,0
 And if you jump to the arrow it  will jump back.,addToBackStack,0
 This is handy to be fast between layouts.,addToBackStack,0
 And finally  we have also added this  feature for recycler view .,addToBackStack,0
 You can just jump back to the file and preview your changes.,addToBackStack,0
 All the design time attributes  are automatically populated so  you don't have to do anything  and take full advantage of  sample data.,addToBackStack,0
 Okay.,addToBackStack,0
 Let's go back to our  example.,addToBackStack,0
 We have our adapter.,addToBackStack,0
" We observe the live data, send  the list to the adapter.",addToBackStack,0
 When we see the adepter we need  to give it a call back.,addToBackStack,0
 It has  two functions.,addToBackStack,0
" The first most important part of the boundary call back that you  implement is  that you pass it to -- you want  to provide it two different  sources of data, the database  and the network, because that's  its job.",addToBackStack,0
 So the important call back we  have here is on I'm end loaded.,addToBackStack,0
 The last item of the database  has been loaded from the paged  list and if there's more from  the network it's time to load  it.,addToBackStack,0
 And then we can re-set that at  the end.,addToBackStack,0
 So using boundary call back is  pretty simple.,addToBackStack,0
 You can add this in your Rx page list  builder and that gives you the  database  plus network solution all  isolated in that one call back.,addToBackStack,0
" As  you can see as we increase the  number of GPUs from 1 to 4 to 8, the  images per second process can  almost double every time.",addToBackStack,0
 We will come back to these  performance numbers later.,addToBackStack,0
" Before we get into the details  of scaling in TensorFlow, first  I want to  cover a few high level concepts  and architectures in distributed training.",addToBackStack,0
 We create an estimator object  with the  run config and then and those are all the code  changes you need to distribute  the res net model.,addToBackStack,0
 Let's go  back and see how our training is going.,addToBackStack,0
 So we have run for a few hundred steps.,addToBackStack,0
 What happens if we are inside our ListDetailRoute and the user wants to go back?,addToBackStack,0
" Well, if we're using an expanded window size, our list and detail are inside the same composable, so pressing back should go back to the previous destination in the stack.",addToBackStack,0
" But when using the compact window size, where the detail screen replaces the list screen, pressing back from the detail screen should take you to the list, not the previous destination.",addToBackStack,0
Basic difference between add() and replace() can be described as:,addToBackStack,0
add() is used for simply adding a fragment to some root element.,addToBackStack,0
replace() behaves similarly but at first it removes previous fragments and then adds next fragment.,addToBackStack,0
I's been a while but I hope this will help someone.,addToBackStack,0
I couldn't find the reason.,addToBackStack,0
"2) fragmentTransaction.replace(int containerViewId, Fragment fragment, String tag)",addToBackStack,0
Description - Replace an existing fragment that was added to a container.,addToBackStack,0
"This is essentially the same as calling remove(Fragment) for all currently added fragments that were added with the same containerViewId and then add(int, Fragment, String) with the same arguments given here.",addToBackStack,0
"3) fragmentTransaction.add(int containerViewId, Fragment fragment, String tag)",addToBackStack,0
Description - Add a fragment to the activity state.,addToBackStack,0
This fragment may optionally also have its view (if Fragment.onCreateView returns non-null) into a container view of the activity.,addToBackStack,0
"What does it mean to replace an already existing fragment, and adding
  a fragment to the activity state and adding an activity to the back
  stack ?",addToBackStack,0
There is a stack in which all the activities in the running state are kept.,addToBackStack,0
Fragments belong to the activity.,addToBackStack,0
So you can add them to embed them in a activity.,addToBackStack,0
You can combine multiple fragments in a single activity to build a multi-pane UI and reuse a fragment in multiple activities.,addToBackStack,0
This is essentially useful when you have defined your fragment container at different layouts.,addToBackStack,0
You just need to replace with any other fragment in any layout.,addToBackStack,0
"When you navigate to the current layout, you have the id of that container to replace it with the fragment you want.",addToBackStack,0
"findFragmentByTag does this search for tag added by the add/replace
  method or the addToBackStack method ?",addToBackStack,0
If depends upon how you added the tag.,addToBackStack,0
It then just finds a fragment by its tag that you defined before either when inflated from XML or as supplied when added in a transaction.,addToBackStack,0
References: [FragmentTransaction (hyper-link)],addToBackStack,0
Like,addToBackStack,0
Now we can check the fragment before adding it to the Stack :,addToBackStack,0
This will return null if the Fragment is not already added.,addToBackStack,0
You can read more about this [here (hyper-link)].,addToBackStack,0
text,text2,label
" This Tanh is a hyperbolic tangent itself, but in the world of Neural Network, Tanh converts the input value into a non-linear one and keeps it in the range of -1~1.",Tanh,1
" As you can see, when a large positive value is input, the output value is stuck to 1, and when a large negative value is input, the output value is stuck to -1.",Tanh,1
"And when the value in this area is input, the output changes almost linearly.",Tanh,1
"In this way, it can be shown that a combination of such functions can approximate any non-linear function.",Tanh,1
"On the other hand, to overcome the vanishing gradient problem, we need a function whose second derivative can sustain for a long range before going to zero.",Tanh,1
Tanh is a good function with the above property.,Tanh,1
"Tanh and the logistic function, however, both have very simple and efficient calculations for their derivatives that can be calculated from the output of the functions; i.e.",Tanh,1
"if the node's weighted sum of inputs is v and its output is u, we need to know du/dv which can be calculated from u rather than the more traditional v: for tanh it is 1 - u^2 and for the logistic function it is u * (1 - u).",Tanh,1
"This fact makes these two functions more efficient to use in a back propagation network than most alternatives, so a compelling reason would usually be required to deviate from them.",Tanh,1
 Tanh after this is exactly the same process as the previous one.,Tanh,1
It is a process to fit the result of the process so far to a range between -1 and 1.,Tanh,1
 The Tanh function is then used to fit the value between -1 and 1.,Tanh,1
" While Tanh fits the input value between -1 and 1, this Sigmoid fits the input value between 0 and 1.",Tanh,1
 But whatever application our problem domain requires that we have to extend our limit from 1 to minus 1 well for that case we can mainly go with tanh activation.,Tanh,1
So the H in tanh stands for hyperbolic.,Tanh,1
So some people call it as hyperbolic or like I say it is hyperbolic.,Tanh,1
So in mathematically how does it look and H of Z looks like he raised to minus Z minus E raised to e raised to z- e raised to minus X upon e raised to Z Plus e raised to minus Z.,Tanh,1
So this is how your tanh function looks like so it is also an exponential function which is in terms of Z.,Tanh,1
"So now what is the peculiarity of tanh its function that it ranges from minus 1 to 1 so it does not give your signal into the range 0 to 1, but it can expand to minus 1 also so it will just shift this particular axis.",Tanh,1
 This is nothing but your tan H so I can write the this as 1 minus tan H Square h of Z especially so that is nothing but 1 minus here output Square.,Tanh,1
So this is differentiative of your tanh function.,Tanh,1
"So if you want to see the geometrical interpretation, so how it basically looks like is you have this particular scale minus Z 2 Z you have one then you have 0.5 and then you have your minus 1.",Tanh,1
" Alright, guys let's discuss hyperbolic tangent activation function which is also known as tanh activation function.",Tanh,1
"Unlike the Sigmoid function, the range for tanh function is -1 to 1.",Tanh,1
"So it is similar to Sigmoid function, where we were catering the output in a range of 0 to 1 & setting up the threshold at 0.5. so in tanh we extend the lower side of the curve till negative 1 & our mid will be at 0.",Tanh,1
So guys by definition tanh function is derived from the trigonometric function that's why it's formula is written as tanh(x) = sinh(x) divided by cosh(x).,Tanh,1
A general problem with both the sigmoid and tanh functions is that they saturate.,Tanh,1
This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively.,Tanh,1
In truth both tanh and logistic functions can be used.,Tanh,1
"The idea is that you can map any real number ( [-Inf, Inf] ) to a number between [-1 1] or [0 1] for the tanh and logistic respectively.",Tanh,1
Now regarding the preference for the tanh over the logistic function is that the first is symmetric regarding the 0 while the second is not.,Tanh,1
"This makes the second one more prone to saturation of the later layers, making training more difficult.",Tanh,1
Tanh,Tanh,1
Mathematical expression: tanh(z) = [exp(z) - exp(-z)] / [exp(z) + exp(-z)],Tanh,1
First-order derivative: tanh'(z) = 1 - ([exp(z) - exp(-z)] / [exp(z) + exp(-z)])^2 = 1 - tanh^2(z),Tanh,1
Advantages:,Tanh,1
"As for your second question on learnable parameters, ReLU and Leaky ReLU are simply activation functions that perform a predefined operation.",LeakyRelu,1
"According to the docs [https://github.com/microsoft/onnxruntime/blob/master/docs/OperatorKernels.md (hyper-link)], LeakyRelu is only implemented for type float (32-bit), while you have double (64 bit).",LeakyRelu,1
" Whereas if you have  a activation function that can only produce positive numbers,  you're a little bit more limited.",LeakyRelu,1
"So if you use the chain rule, and your  derivative is one, if the inputs are positive, then yeah, you  don't diminish the product in the on general, okay, but it can  also be zero, which can be a problem.",LeakyRelu,1
"So if you have negative  inputs to this activation function, you are your output  would be zero, which will then basically cancel the weight  update for that corresponding weight corresponding to this  activation, or connected to this activation.",LeakyRelu,1
So that can be a  problem if you always have very negative input.,LeakyRelu,1
So there is a  problem called dying neurons or debt.,LeakyRelu,1
" A version of Relu  that some people find to perform sometimes a little bit better is  the leaky relu, which doesn't have the problem of these dying  neurons.",LeakyRelu,1
"So here, the difference is that we have, so if we look  at the simplified notation, the the piecewise linear function  here, um, what you can see here, or the piecewise function,  sorry, what you can see here is that the only difference is that  we have now this alpha here, which is a slope, if the input  is smaller than zero, so for the negative region here, we have  now a slope, what value we can choose for the slope, it's a  hyper parameter, right?",LeakyRelu,1
 So hyper parameters is something that you  as the practitioner have has to choose.,LeakyRelu,1
Yes MMdnn support supports LeakyRelu.,LeakyRelu,1
Check the link below for pytorch_emitter.py implementation from MMdnn.,LeakyRelu,1
"addToBackStack(""TAG"") and popBackStackImmediate(""TAG"") always revert to fragment condition  without any data in the UI right before fragment is created or added to activity !",addToBackStack,1
addtoBackStack method can be used with add() and replace methods.,addToBackStack,1
It serves a different purpose in Fragment API.,addToBackStack,1
" When specifying a fragment transaction, you can call add to back stack to add the fragment transaction to the back stack.",addToBackStack,1
"Then, when the user hits the Back button, instead of your activity being finished, the fragment transaction is reversed.",addToBackStack,1
Only when there are no more fragment transactions will the Back button finish your activity.,addToBackStack,1
We can see the exact difference when we use addToBackStack() together with add() or replace().,addToBackStack,1
"When we press back button after in case of add()... onCreateView is never called, but in case of replace(), when we press back button ... oncreateView is called every time.",addToBackStack,1
You can also go back to the previous fragment in the backStack with the popBackStack() method.,addToBackStack,1
For that you need to add that fragment in the stack using addToBackStack() and then commit() to reflect.,addToBackStack,1
This is in reverse order with the current on top.,addToBackStack,1
What is addToBackStack,addToBackStack,1
What is the purpose?,addToBackStack,1
Fragment API unlike Activity API does not come with Back Button navigation by default.,addToBackStack,1
If you want to go back to the previous Fragment then the we use addToBackStack() method in Fragment.,addToBackStack,1
 The second case is around notifications.,addToBackStack,1
We talked about how the Back button pops the back stack.,addToBackStack,1
A natural corollary is that the Back button shouldn't cross into different tasks-- a convention since the days of Android 3.0.,addToBackStack,1
It's just a single stack from where you are to the launcher.,addToBackStack,1
 And the fragment manager lets you do that by setting something called the back stack.,addToBackStack,1
So the back stack is a list of prior points in your activities that may can return to by hitting back button.,addToBackStack,1
And all you have to do to make this work is in your main activity before you make a change that you want to return to-- so here's where I will place the input activity with the output activity.,addToBackStack,1
1) fragmentTransaction.addToBackStack(str);,addToBackStack,1
Description - Add this transaction to the back stack.,addToBackStack,1
"This means that the transaction will be remembered after it is committed, and will reverse its operation when later popped off the stack.",addToBackStack,1
"When using dispatchTouchEvent , you take all touches in your activity, if you want only detect one touch, you have to filter the touch by its type, you can do this using the MotionEvent parameter.",dispatchTouchEvent,2
"Both Activity and View have method dispatchTouchEvent() and onTouchEvent.The ViewGroup have this methods too, but have another method called onInterceptTouchEvent.",dispatchTouchEvent,2
"The return type of those methods are boolean, you can control the dispatch route through the return value.",dispatchTouchEvent,2
"Another difference is that if dispatchTouchEvent return 'false' the event dont get propagated to the child, in this case the EditText, whereas if you return false in onInterceptTouchEvent the event still get dispatch to the EditText",dispatchTouchEvent,2
negative_slope in this context means the negative half of the Leaky ReLU's slope.,LeakyRelu,2
It is not describing a slope which is necessarily negative.,LeakyRelu,2
"When naming kwargs it's normal to use concise terms, and here ""negative slope"" and ""positive slope"" refer to the slopes of the linear splines spanning the negative [-∞,0] and positive (0,∞] halves of the Leaky ReLU's domain.",LeakyRelu,2
"However, the 'name' used in addToBackStatck(name) is used to control to which fragment you want to pop the fragment back stack by calling popBackStatck/Immediate(name, flags).",addToBackStack,2
"So if I have a fragment stack with named fragments: A, B, C and D with A at the bottom.",addToBackStack,2
"When you call popBackStack(B, XXX_EXCLUSIVE), then your fragment back stack will be like: A and B after the call.",addToBackStack,2
"Without the name, you can't do that.",addToBackStack,2
Passing null to addtoBackStack(null) means adding the fragment in the Fragment Stack but not adding any TAG which could be further use to identify the particular fragment in a stack before adding again.,addToBackStack,2
But passing TAG to addToBackStack helps in identifying the fragment in Fragment stack by TAG.,addToBackStack,2
"Just want to clarify, the 'name' used in addToBackStack(name) can't be used for retrieving the fragment by calling fragmentManager.findFragmentByTag(tag).",addToBackStack,2
The 'tag' is different from the 'name'.,addToBackStack,2
Would you even want to override [Activity|ViewGroup|View].dispatchTouchEvent?,dispatchTouchEvent,3
Unless you are doing some custom routing you probably should not.,dispatchTouchEvent,3
The main extension methods are ViewGroup.onInterceptTouchEvent if you want to spy and/or intercept touch event at the parent level and View.onTouchListener/View.onTouchEvent for main event handling.,dispatchTouchEvent,3
"For example, the simplest case is that of View.dispatchTouchEvent which will route the touch event to either OnTouchListener.onTouch if it's defined or to the extension method onTouchEvent.",dispatchTouchEvent,3
"So in case you are working with these 2 handlers use dispatchTouchEvent to handle on first instance the event, which will go to onInterceptTouchEvent.",dispatchTouchEvent,3
You may try to convert to 32-bit float right before LeakyRely in your PyTorch code.,LeakyRelu,3
And maybe create an issue on the ONNX Runtime Github to add double support for LeakyRelu.,LeakyRelu,3
"As for the idea of ""maintaining a state"", it refers to activation functions that would not behave independently on each and every fed-in sample, but would instead retain some learnable information (the so-called state).",LeakyRelu,3
"Typically, for a LeakyReLU activation, you could adjust the leak parameter through training (and it would, in the documentation's terminology, be referred to as a state of this activation function).",LeakyRelu,3
"I would suggest removing the sigmoid activation from last layer, and replace relu with LeakyRelu to improve the model's robustness.",LeakyRelu,3
fragmentTransaction.addToBackStack(null) won't work if you are extending AppCompatActivity.,addToBackStack,3
It works well in Activity.,addToBackStack,3
"The parameter for addToBackStack() is an optional name for the back state, you do not use the tag in the replace() method which is just an optional tag for the fragment.",addToBackStack,3
" But now I want to hit the back button, it doesn't take me to the input form, it just exits the app.",addToBackStack,3
"And when I return from that activity, that exits the app.",addToBackStack,3
So with-- but I-- what I really want to do is I want to be able to hit that back button and go back to my input form.,addToBackStack,3
So I want to be able to type 120 and 10 and calculate.,addToBackStack,3
"And if I hit this back button, I want to return to the input form even though it's the same activity.",addToBackStack,3
 So you'll notice that I have this addToBackStack before I replace the content with the output fragments.,addToBackStack,3
"And you might think that if I move this here, now when I hit the return it would return to the output fragment.",addToBackStack,3
That change isn't actually made until here when I do the commit.,addToBackStack,3
So addToBackStack Input Form should do the same exact thing whether I have it before or after this replace.,addToBackStack,3
the reason why the popBackStack() and popBackStackImmediatly() is due to the fact that you didnt add that fragment (that you want to pop) to the backStack.,addToBackStack,3
In order to do this you have to make a call to addToBackStack() at the moment of making the transaction to add/replace your fragment.,addToBackStack,3
The main difference :,dispatchTouchEvent,0
"•Activity.dispatchTouchEvent(MotionEvent) - This allows your Activity
  to intercept all touch events before they are dispatched to the
  window.",dispatchTouchEvent,0
Because this is the first result on Google.,dispatchTouchEvent,0
I want to share with you a great Talk by Dave Smith on [Youtube: Mastering the Android Touch System (hyper-link)] and the slides are available [here (hyper-link)].,dispatchTouchEvent,0
It gave me a good deep understanding about the Android Touch System:,dispatchTouchEvent,0
How the Activity handles touch:,dispatchTouchEvent,0
"Activity.dispatchTouchEvent()

Always  first  to  be  called
Sends  event  to  root  view  attached  to  Window
onTouchEvent()

Called  if  no  views  consume  the  event
Always  last  to  be  called",dispatchTouchEvent,0
How the View handles touch:,dispatchTouchEvent,0
"View.dispatchTouchEvent()

Sends  event  to  listener  first,  if  exists
  
  
View.OnTouchListener.onTouch()

If  not  consumed,  processes  the  touch  itself
  
  
View.onTouchEvent()",dispatchTouchEvent,0
He also provides example code of custom touch on [github.com/devunwired/ (hyper-link)].,dispatchTouchEvent,0
"Answer:
Basically the dispatchTouchEvent() is called on every View layer to determine if a View is interested in an ongoing gesture.",dispatchTouchEvent,0
You could imagine the [code of a ViewGroup (hyper-link)] doing more-or-less this (very simplified):,dispatchTouchEvent,0
" Here I'm saying, anything that's scrolling vertically I'm interested in, please send me more events.",dispatchTouchEvent,0
" So if you returned true from that one, every time the user scrolls in the list, I'll get this onNestedScroll event.",dispatchTouchEvent,0
" So you can do stuff with working out how much the ListView scrolls by itself, but the one I'm actually really, really interested in here is that last parameter, which is dyUnconsumed.",dispatchTouchEvent,0
" And then I work out where the top of the first item is, which I've set down with some padding.",dispatchTouchEvent,0
" So then, if the touch point where you're touching on the RecyclerView is above the first item, I directly call and dispatchTouchEvent onto the content behind.",dispatchTouchEvent,0
 This is the description field behind.,dispatchTouchEvent,0
 So that forwards on the touch event.,dispatchTouchEvent,0
 So again like letting the touch pass through the RecyclerView onto the content behind.,dispatchTouchEvent,0
 So it lets you do this kind of layering trick.,dispatchTouchEvent,0
" So here for example, I've got a couple of actions which use the floating action button pattern.",dispatchTouchEvent,0
" And when you touch it, sometimes you have more actions to be taken.",dispatchTouchEvent,0
" So if you're trying to like a shot, and you haven't logged in, I want you to log in.",dispatchTouchEvent,0
 So I instead use some dynamic coloring in order to make the app still feel alive and thoughtful.,dispatchTouchEvent,0
" So here for example, as you're clicking on an item, you can see the touch response-- the touche ripple you get is determined by the item you're clicking on.",dispatchTouchEvent,0
" So the gray and red one gets a nice red ripple, and the one below gets like a bluey-purple ripple.",dispatchTouchEvent,0
" In particular, I really want to use the vibrant color.",dispatchTouchEvent,0
 So picking out that red from that first image we saw is a great way to kind of tie the touch ripple back to the item.,dispatchTouchEvent,0
 So what I do is I walk through the swatches one at a time.,dispatchTouchEvent,0
 So how would we build that?,dispatchTouchEvent,0
" The first bit, raise on touch, that's easy.",dispatchTouchEvent,0
 That's my friend the stateless animator we looked at earlier.,dispatchTouchEvent,0
 So here's two versions of the same video.,dispatchTouchEvent,0
" So you touch on the item-- oh yeah-- one's with, one's without.",dispatchTouchEvent,0
 I hope you see that.,dispatchTouchEvent,0
 So the goal here is really about directing attention.,dispatchTouchEvent,0
" So when you touch on the search thing, I want it to feel like this transient experience.",dispatchTouchEvent,0
" So that it's just coming in transiently over the top of the content, which is why I do that scrim coming in slowly and gently.",dispatchTouchEvent,0
" For generic views which don't control their own values, this simplest alternative  is to set the content description within your app at runtime.",dispatchTouchEvent,0
 An even more robust solution is to send an accessibility event from within your view.,dispatchTouchEvent,0
 Whenever the visual content  has been modified.,dispatchTouchEvent,0
" Then override the dispatch populate accessibility event, and then the current control's  visual value, the accessibility event.",dispatchTouchEvent,0
" Go ahead and add  accessibility handlers to your view, and then click here when you're done.",dispatchTouchEvent,0
Here are some visual supplements to the other answers.,dispatchTouchEvent,0
My full answer is [here (hyper-link)].,dispatchTouchEvent,0
[ (hyper-link)],dispatchTouchEvent,0
[ (hyper-link)],dispatchTouchEvent,0
The best place to demystify this is the source code.,dispatchTouchEvent,0
The docs are woefully inadequate about explaining this.,dispatchTouchEvent,0
This is what onInterceptTouchEvent is there for.,dispatchTouchEvent,0
So it calls this method first before doing the hit testing and if the event was hijacked (by returning true from onInterceptTouchEvent) it sends a ACTION_CANCEL to the child views so they can abandon their touch event processing (from previous touch events) and from then onwards all touch events at the parent level are dispatched to onTouchListener.onTouch (if defined) or onTouchEvent().,dispatchTouchEvent,0
"Also in that case, onInterceptTouchEvent is never called again.",dispatchTouchEvent,0
All in all its overly complicated design imo but android apis lean more towards flexibility than simplicity.,dispatchTouchEvent,0
From Activity viewpoint:,dispatchTouchEvent,0
Touch events are delivered first to Activity.dispatchTouchEvent.,dispatchTouchEvent,0
It's where you may catch them first.,dispatchTouchEvent,0
"Here they get dispatched to Window, where they traverse View hierarchy, in such order that Widgets that are drawn last (on top of other widgets) have chance to process touch in View.onTouchEvent first.",dispatchTouchEvent,0
"If some View returns true in onTouchEvent, then traversal stops and other Views don't receive touch event.",dispatchTouchEvent,0
"Finally, if no View consumes touch, it's delivered to Activity.onTouchEvent.",dispatchTouchEvent,0
That's all your control.,dispatchTouchEvent,0
"And it's logical that what you see drawn on top of something else, has chance to process touch event before something drawn below it.",dispatchTouchEvent,0
Using this simple example:,dispatchTouchEvent,0
You can see that the log willl be like:,dispatchTouchEvent,0
"Your model initialization looks fine to me, just write the forward function to call the operations defined in __init__ and you should be all set.",LeakyRelu,0
"For instance, ReLU is defined as max(0,x), so it simply outputs the max value of the two inputs.",LeakyRelu,0
"While neither ReLU nor Leaky ReLU have learnable parameters, Leaky ReLU does have a hyperparameter, which is the negative_slope, as from its function definition max(0, x) + negative_slope * min(0,x).",LeakyRelu,0
"Your model initialization looks fine to me, just write the forward function to call the operations defined in __init__ and you should be all set.",LeakyRelu,0
 So there's also a  softmax function.,LeakyRelu,0
" But here, we compute this based on the we use  the lock softmax, because it's numerically more stable, if we  were to use the negative log likelihood loss.",LeakyRelu,0
 This is really  the only if we want to use the negative log likelihood loss.,LeakyRelu,0
" Otherwise, we would just use softmax if we are interested in  the probabilities.",LeakyRelu,0
" To be honest, now looking at this, I don't  know why I used lock softmax, I think, in the big code example,  when I created lecture five, I had the negative log likelihood  here.",LeakyRelu,0
" So also, technically, you don't have to compute the  probabilities within this class, you can do this separately, if  you care about because technically, you never have to  use the probability if you use the cross entropy function in  pytorch for optimization.",LeakyRelu,0
" And you  remember, this is very simple, it's actually just thresholded  at zero.",LeakyRelu,0
" So if the input is negative, the output is zero,  otherwise, it's an identity function.",LeakyRelu,0
" So it's almost an  identity function, but not quite.",LeakyRelu,0
 And the output  at zero is also zero.,LeakyRelu,0
" So it's producing positive and negative  values, which can be an advantage.",LeakyRelu,0
" There's also a hard  tension, which is essentially very similar to 10 h, except  that it's thresholded here, similar to relu.",LeakyRelu,0
 So  the event of 10 h is really like that we have this centering at  zero.,LeakyRelu,0
 So that we have positive and negative values.,LeakyRelu,0
 And you can  also see it's steeper.,LeakyRelu,0
" And then when you compute the partial derivatives in the  chain rule, then yeah, you will get very small gradients and the  learning will be very slow, which can be a disadvantage.",LeakyRelu,0
" Or  maybe one more thing about why it's good to have negative and  positive values, that just gives you more combinations.",LeakyRelu,0
" So  imagine you initialize your weights from let's say a small  from a random normal distribution, standard normal  distribution, let's say, or a scale to standard normal  distribution.",LeakyRelu,0
 So you initialize your weights such that they are  centered at zero.,LeakyRelu,0
 So you can have positive and negative  starting weights.,LeakyRelu,0
" And if you also use this 10h, which can  have positive and negative values, you get just more  combinations of possible values, whether you combine a positive  with a negative number and negative with a positive number  to negative numbers or to positive numbers, you have four  different ways you can combine these signs.",LeakyRelu,0
 It's something you have to try out in practice and change and  see what performs better.,LeakyRelu,0
" I have seen all kinds of values for  this negative slope here, or for the slope in the negative region.",LeakyRelu,0
" So in Keras, that's API for TensorFlow, I believe they use  point three as the default value.",LeakyRelu,0
 so formula for Sigmoid is shown on screen which is nothing but 1 divided by 1 + e to the power minus y & here y is summation of weights multiplied by the X + bias.,LeakyRelu,0
" so here what sigmoid function does is let this product be any value either negative or positive, whenever we replace this in formula it ranges the independent variables between 0 & 1. so here question is how can we assign any value as either 0 or 1. so it's very simple we decide some threshold over the graph.",LeakyRelu,0
 let say i draw a threshold of 0.5 & whenever it is greater than 0.5 output is considered as 1 & when it is less than 0.5 then output will be considered as 0. so guys from this you can understand that how it reduces the outliers or extreme values.,LeakyRelu,0
" Unlike the Sigmoid function, the range for tanh function is -1 to 1.",LeakyRelu,0
" So it is similar to Sigmoid function, where we were catering the output in a range of 0 to 1 & setting up the threshold at 0.5. so in tanh we extend the lower side of the curve till negative 1 & our mid will be at 0.",LeakyRelu,0
 So guys by definition tanh function is derived from the trigonometric function that's why it's formula is written as tanh(x) = sinh(x) divided by cosh(x).,LeakyRelu,0
" max(0, y).",LeakyRelu,0
 so what does it means is if our y is weighted sum of input + bias & this value is coming as some positive value then the max of 0 & that particular positive value would be that value itself but what if our y is some negative value then max of that negative value & zero would be 0 only.,LeakyRelu,0
 so by same intuition we can draw a graphical representation of ReLU function as shown on the screen.,LeakyRelu,0
" so guys for y=0 or any negative value our output will be transformed to 0 but it increases with the same y value if y is greater than 0. let say y is 2 then output will be 2, if y is 4 then output will be 4.",LeakyRelu,0
 So guys this is very powerful activation function as this is used at most of the places in deep learning.,LeakyRelu,0
 let's get back to graph again.,LeakyRelu,0
 so guys as you can see for any negative value or 0 our output is considering as 0 it means we are not activating those neurons but for positive value our neuron's strength is progressing exactly in the same way what value of y we are getting.,LeakyRelu,0
 let say if we are getting positive infinite value for y then in that case our output would be positive infinity only & for negative value our output will be 0. so once we have the output value with us we basically back-propagate the derivative of output value with respect to term x for adjusting the weight matrix as I have told you guys in my last video that we have to adjust weight matrix for getting the correct output.,LeakyRelu,0
 I will not go into deep of this back propagation because this is really a great topic to explain.,LeakyRelu,0
 now if i want to find the derivative of this line the value will always be 1 because this line is at an angle of 45 degree & we all know that tan 45 degree is 1 so for this line I can draw a straight line at 1. so for any positive value derivative of our output would always be 1 .,LeakyRelu,0
 now for negative side so you can see that this is just a constant value at 0th axis.,LeakyRelu,0
" so according to derivative rule, derivative of constant value is 0. so for any 0 or negative value derivative will be marked at 0. so range of derivative of ReLU function will be 0 for negative value & 1 for value greater than 0. guys now you must have one doubt that how is this different from Sigmoid function as Sigmoid was also ranging between 0 & 1. so In Sigmoid the derivative will always range between 0 to 0.5 & in tanh it is always less than 1. now let's apply derivative values in updation formula for derivative.",LeakyRelu,0
 so we have this formula y old = y new - learning rate into derivative of loss with respect to derivative of w. let say here we have 2 derivatives & it's a chain rule.,LeakyRelu,0
 now let see how can we fix our problem.,LeakyRelu,0
 so here we will try to add some value to constant side that means line will not be exactly 0 for negative number instead it will be some addition to this value.,LeakyRelu,0
 so here if y is greater than 0 then output would be y itself but if y is less than 0 then instead of putting it as 0 we will add some value to 0. let say we take 0.01 & this will be multiplied by x. now if we find the derivative of this term i.e.,LeakyRelu,0
 0.01 * y divided by derivative of w then our output will be some 0.01 value but not 0 & now if we subtract this value from y new then we will get some value which we can use for updation.,LeakyRelu,0
 so by this way we have solved dead neuron problem by just simply adding some value to negative side.,LeakyRelu,0
 so guys now we only have to discuss Softmax activation function.,LeakyRelu,0
 So how does it looks like so say this is our unit and here we have a linear part your computation and say We are Computing the R(z).,LeakyRelu,0
" This is the real work this now what happens is that when you take the derivative of this whatever portion on is there on this linear side right becomes differentiable, but when you just take the differentiative of this part that is on the negative axis, so the - axis on your number line, this is your positive axis.",LeakyRelu,0
" So for positive axis the differentiative when you do the gradient descent, it does not cause any problem so So if you want to see the graph of this if you want to differentiate this so say we are taking F Prime of Z.",LeakyRelu,0
" So this graph becomes something like this so that steady so that also the case for a step function, but in case of this negative values this attains 0, so this is where for- differentiation for negative d by dX This value is equal to 0 so suddenly, you're F dash X for greater than or equal to 0 it was one but as soon as it is less than 0 that is negative.",LeakyRelu,0
 It is falling down.,LeakyRelu,0
 So this part remains the Same.,LeakyRelu,0
" So this is same as it is, but on this portion that is on the negative side.",LeakyRelu,0
 You have a very small I got this so they're here.,LeakyRelu,0
" The slope is 0 here the slope is not equal to 0 so here if you have F of Z, it will be multiplied with some a of Z and here F of Z is equal to Z that is for greater than equal to 0 for R less than 0 you have a into Z.",LeakyRelu,0
 So this is again a hyperparameter that you decide.,LeakyRelu,0
" So when you just apply this value, so it's not equal to 0 it is approximately equal to 0.",LeakyRelu,0
 So it will make some movement in the slope.,LeakyRelu,0
" So, you know, that was our function loss function.",LeakyRelu,0
[pytorch_emitter.py (hyper-link)],LeakyRelu,0
If you check the implementation you will find all the supported operations and it doesn't include PRelu.,LeakyRelu,0
"I am not sure there is a strict definition of ""forward layers"" in this context, but basically what it means is that the ""classic"", keras-built-in types of layers comprising one or more sets of weights used to transform an input matrix into an output one have a activation argument.",LeakyRelu,0
"Typically, Dense layers have one, as well as the various kinds of RNN and CNN layers.",LeakyRelu,0
 they simply add a mechanism triggered at training to (hopefully) improve convergence rate and decrease overfitting chances.,LeakyRelu,0
Seems your model is not able to generalise to val set.,LeakyRelu,0
"Which means the model is unable to find the differences between (anchor, positive) vs (anchor, negative).",LeakyRelu,0
"In such case, distance of (anchor, positive) - distance of (anchor, negative) ~= 0.",LeakyRelu,0
And since your margin is set to be 1.,LeakyRelu,0
The loss will stay at 1.,LeakyRelu,0
Recall the loss function definition from [TripletMarginLoss (hyper-link)].,LeakyRelu,0
"torch.nn.functional.cross_entropy function combines log_softmax(softmax followed by a logarithm) and nll_loss(negative log likelihood loss) in a single function, i.e.",cross_entropy,1
"it is equivalent to F.nll_loss(F.log_softmax(x, 1), y).",cross_entropy,1
"Yes, the cross-entropy loss function can be used as part of gradient descent.",cross_entropy,1
"In short, cross-entropy(CE) is the measure of how far is your predicted value from the true label.",cross_entropy,1
"The cross here refers to calculating the entropy between two or more features / true labels (like 0, 1).",cross_entropy,1
"And the term entropy itself refers to randomness, so large value of it means your prediction is far off from real labels.",cross_entropy,1
Cross-entropy is commonly used to quantify the difference between two probability distributions.,cross_entropy,1
"In the context of machine learning, it is a measure of error for categorical multi-class classification problems.",cross_entropy,1
"Correct, cross-entropy describes the loss between two probability distributions.",cross_entropy,1
It is one of many possible loss functions.,cross_entropy,1
Sparse functions are a special case of categorical CE where the expected values are not one-hot encoded but is an integer,cross_entropy,1
"Usually the ""true"" distribution (the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution.",cross_entropy,1
Note that it does not matter what logarithm base you use as long as you consistently use the same one.,cross_entropy,1
"As it happens, the Python Numpy log() function computes the natural log (log base e).",cross_entropy,1
Cross entropy is one out of many possible loss functions (another popular one is SVM hinge loss).,cross_entropy,1
"These loss functions are typically written as J(theta) and can be used within gradient descent, which is an iterative algorithm to move the parameters (or coefficients) towards the optimum values.",cross_entropy,1
"In the equation below, you would replace J(theta) with H(p, q).",cross_entropy,1
"But note that you need to compute the derivative of H(p, q) with respect to the parameters first.",cross_entropy,1
" Alright, so yes, again, the cross entropy,  again, recall that there are two sums.",cross_entropy,1
"So if I go back here, so  we have these two sums here, I kind of entangled them a little  bit.",cross_entropy,1
"So we have, this is a sum over the training examples.",cross_entropy,1
And  this one is the cross entropy for the 100 encoding.,cross_entropy,1
"So this  one is the inner, the inner one here.",cross_entropy,1
So let's compute first the  cross entropy for each training example.,cross_entropy,1
So what I'm doing here  is I'm computing these terms.,cross_entropy,1
" So this function, the negative lock  likelihood loss expects the lock of the softmax values.",cross_entropy,1
 So you can see this  negative lock likelihood loss is the same as our cross entropy  here.,cross_entropy,1
" So where I mentioned  that the negative lock likelihood and binary cross  entropy equivalent in pytorch, it's actually the negative lock  likelihood and the multi category cross entropy  equivalent.",cross_entropy,1
"I mean, in a way, you can also think of it as a  multi category one, the multinomial logistic regression,  then this would be still true.",cross_entropy,1
" When we compute the cross entropy,  because we use the mathematical formulas, we compute first the  softmax.",cross_entropy,1
"And then from the softmax, we compute the cross  entropy in pytorch, they do all that work for us inside this  function, they do it for us.",cross_entropy,1
" And notice that I said, redact  reduction to none, which means it does not apply the sum or the  average, which is this outer one here.",cross_entropy,1
"So by default, when you  use this cross entropy, it will perform the average, you can  test it like this, see, it's the same same value.",cross_entropy,1
"If you wanted  to, you can also say reduction to consider reduction to some.",cross_entropy,1
" Actually, we had a seminar at UW  last week, where we also, yeah, it was briefly mentioned,  coincidentally, there was like a question whether it's the same,  the negative log likelihood and cross entropy.",cross_entropy,1
" So yeah, the  negative log likelihood and the binary cross entropy are  equivalent.",cross_entropy,1
"And in practice in deep learning, people just say  cross entropy, multi category cross entropy, which would be a  multi class version of the negative log likelihood, which  we will cover later in this lecture when we talk about the  softmax function.",cross_entropy,1
"So just to keep it brief, the negative log  likelihood that we just covered a few videos ago, is the same as  what people call the binary cross entropy, they were just  formulated in different contexts.",cross_entropy,1
"So negative log  likelihood comes more like from, I think it's like, it's probably  from a statistics context, I don't know the first paper, or  reference that mentioned that.",cross_entropy,1
"But this is something usually I  see in statistics papers, and the binary cross entropy thing  has originated from the field of information theory, or computer  science.",cross_entropy,1
"So we have actually seen that, or not, the cross  entropy, where we have seen the self entropy, or just entropy,  and statistics 451.",cross_entropy,1
" For those who took this class, in fall  semester, where we had used the entropy function in the context  of the information theory and decision trees, but we used a  lock to instead of the natural algorithm, but yeah, it's kind  of somewhat related, if you have taken any class where you  talked, for example, about the KL divergence, or callback  Leibler divergence, which measures the difference between  two distributions, the KL divergence is essentially the  cross entropy minus the self entropy.",cross_entropy,1
" The only  thing you have to know is or should know, because it's useful  to know, is that the negative log likelihood is the same as  the binary cross entropy, this is like a useful thing to know.",cross_entropy,1
" And  there's also a multi category version is the multi category  cross entropy, which is just a generalization of the binary  cross entropy to multiple classes.",cross_entropy,1
"So in order to make  that negative log likelihood or binary cross entropy work for  multiple classes, we assume a so called one hot encoding, where  the class labels are either zero or one for some reason, it was  cut off here.",cross_entropy,1
" So again, all I wanted to say here is the  logits in deep learning, usually refer to the net inputs of the  layer that just comes before the output.",cross_entropy,1
" We have   to use something called cross entropy loss,  and this is actually the loss function that   fastai picked for us before without us  knowing.",cross_entropy,1
 The first part of what cross-entropy loss  in Pytorch does is to calculate the softmax.,cross_entropy,1
" And so here is:   each of the part “y-i” times log of “p-y-i”,  and here is…(why did I subtract that's weird,   oh because I've got minus of both, so I  just do it this way, avoids parentheses…)   yeah, minus the are-you-not-a-cat times  the log of the prediction value not-a-cat,   and then we can add those together, and so that   would be the binary cross-entropy loss of  this dataset of five cat or not-cat images.",cross_entropy,1
" Basically it turns out that all of the loss  functions in pytorch have two versions – there's   a version which is a class, this is a class,  which you can instantiate passing in various   tweaks you might want, and there's also  a version which is just a function,   and so if you don't need any of these tweaks  you can just use the function.",cross_entropy,1
" All right   so that's all fine… we passed… so now when  we create a vision learner you can't rely on   fastaI to know what loss function to use, because  we've got multiple targets, so you have to say:   this is the loss function I want to use, this  is the metrics I want to use.",cross_entropy,1
There is just one real loss function.,cross_entropy,1
This is cross-entropy (CE).,cross_entropy,1
The goal of calculating the cross-entropy loss function is to find the probability that an observation belongs to a particular class or group in the classification problem.,cross_entropy,1
"In a ViewGroup the ViewGroup has the ability to steal the touch events in his dispatchTouchEvent()-method, before it would call dispatchTouchEvent() on the children.",dispatchTouchEvent,1
The ViewGroup would only stop the dispatching if the ViewGroup onInterceptTouchEvent()-method returns true.,dispatchTouchEvent,1
The difference is that dispatchTouchEvent()is dispatching MotionEvents and onInterceptTouchEvent tells if it should intercept (not dispatching the MotionEvent to children) or not (dispatching to children).,dispatchTouchEvent,1
The dispatchTouchEvent() method of a ViewGroup uses onInterceptTouchEvent() to choose whether it should immediately handle the touch event (with onTouchEvent()) or continue notifying the dispatchTouchEvent() methods of its children.,dispatchTouchEvent,1
"dispatchTouchEvent is actually defined on Activity, View and ViewGroup.",dispatchTouchEvent,1
Think of it as a controller which decides how to route the touch events.,dispatchTouchEvent,1
For ViewGroup.dispatchTouchEvent things are way more complicated.,dispatchTouchEvent,1
It needs to figure out which one of its child views should get the event (by calling child.dispatchTouchEvent).,dispatchTouchEvent,1
This is basically a hit testing algorithm where you figure out which child view's bounding rectangle contains the touch point coordinates.,dispatchTouchEvent,1
dispatchTouchEvent handles before onInterceptTouchEvent.,dispatchTouchEvent,1
•ViewGroup.onInterceptTouchEvent(MotionEvent) - This allows a   ViewGroup to watch events as they are dispatched to child Views.,dispatchTouchEvent,1
How a ViewGroup handles touch:,dispatchTouchEvent,1
"ViewGroup.dispatchTouchEvent()  onInterceptTouchEvent()  Check  if  it  should  supersede  children Passes  ACTION_CANCEL  to  active  child If it returns  true  once,  the ViewGroup consumes  all  subsequent  events  For  each  child  view  (in  reverse  order  they  were  added)       If  touch  is  relevant  (inside  view),  child.dispatchTouchEvent() If  it is not  handled  by  a previous,  dispatch  to  next  view  If  no  children  handles the  event, the listener  gets  a  chance       OnTouchListener.onTouch()  If there is no listener, or its not handled       onTouchEvent()",dispatchTouchEvent,1
Intercepted  events  jump  over the child step,dispatchTouchEvent,1
The event dispatch in Android starts from Activity->ViewGroup->View.,dispatchTouchEvent,1
"But before it can dispatch the event to the appropriate child view, the parent can spy and/or intercept the event all together.",dispatchTouchEvent,1
"When using dispatchTouchEvent , you take all touches in your activity, if you want only detect one touch, you have to filter the touch by its type, you can do this using the MotionEvent parameter.",dispatchTouchEvent,1
" onCreate is responsible for creating your tables inside your SQLite database, and onUpgrade will be responsible for upgrading your database if you do make any changes.",onCreate,1
This method will not be called if you’ve changed your code and relaunched in the emulator.,onCreate,1
"After executing in first time deployment, this method will not be called onwards.",onCreate,1
onCreate() is called when you call getWritableDatabase() or getReadableDatabase() on the helper and the database file does not exist.,onCreate,1
"If the file is already there and the version number is the requested one, no callback such as onCreate() is invoked.",onCreate,1
" Each time I say to the Android SDK I want to get a connection to my database, Android will determine whether the database exists or not.",onCreate,1
"If it doesn't exist yet, Android will call the onCreate method.",onCreate,1
" In  particular, the starter code gives us   one function or one method that is already being  overridden.",onCreate,1
"And that's called onCreate, to the   Android system will automatically invoke or call  this function when our application is starting up   when it's creating our screen.",onCreate,1
"However, this method will only be called if the SQLite file is missing in your app’s data directory (/data/data/your.apps.classpath/databases).",onCreate,1
1) onCreate(): This method invoked only once when the application is start at first time .,onCreate,1
So it called only once,onCreate,1
"As already explained in the older answer, if the database with the name doesn't exists, it triggers onCreate.",onCreate,1
If you want onCreate() to run you need to use adb to delete the SQLite database file.,onCreate,1
onCreate() is only run when the database file did not exist and was just created.,onCreate,1
Sqlite database override two methods,onCreate,1
"2)onUpgrade() This method called when we change the database version,then this methods gets invoked.It is used for the alter the table structure like adding new column after creating DB Schema",onCreate,1
Database version is stored within the SQLite database file.,onCreate,1
Another issue is that SQLiteOpenHelper lifecycle methods such as onCreate() are invoked inside an transaction.,onCreate,1
" It's a late  initialization because we're going to initialize   it inside of the onCreate method and not in the  constructor, which is why it's late initialization   is a variable and that we're calling it at base  mount the convention I follow that the name of   the variable is exactly equal to the name of the  ID.",onCreate,1
" After the set content   view, we are going to say at base mount find  view by ID et base amount.",onCreate,1
" And then because SQLiteOpenHelper class is an abstract class, you have to implement to abstract method here.",onCreate,1
Those are onCreate and onUpgrade.,onCreate,1
" And you can implement all the predefined methods, which are already provided by SQLiteOpenHelper class.",onCreate,1
 These methods will be called automatically by the Android SDK.,onCreate,1
 Next we have the onCreate method.,onCreate,1
This is where the SQL we've been talking about and our new contract class are used together.,onCreate,1
" The first time the database is used, SQLiteOpenHelper's onCreate will be called.",onCreate,1
 We need to write the correct SQL statement string so that we can create the table sunshine needs.,onCreate,1
 We then have the system execute this SQL by calling db dot execSQL.,onCreate,1
onCreate() method is creating the tables you’ve defined and executing any other code you’ve written.,onCreate,1
SQLiteOpenHelper should call the super constructor.,onCreate,1
The onUpgrade() method will only be called when the version integer is larger than the current version running in the app.,onCreate,1
"If you want the onUpgrade() method to be called, you need to increment the version number in your code.",onCreate,1
onCreate is called for the first time when creation of tables are needed.,onCreate,1
We need to override this method where we write the script for table creation which is executed by SQLiteDatabase.,onCreate,1
"[SQLiteOpenHelper (hyper-link)] [onCreate() (hyper-link)] and [onUpgrade() (hyper-link)] callbacks are invoked when the database is actually opened, for example by a call to [getWritableDatabase() (hyper-link)].",onCreate,1
The database is not opened when the database helper object itself is created.,onCreate,1
SQLiteOpenHelper versions the database files.,onCreate,1
"As an implication, you should not catch SQLExceptions in onCreate() yourself.",onCreate,1
onUpgrade() is only called when the database file exists but the stored version number is lower than requested in the constructor.,onCreate,1
The onUpgrade() should update the table schema to the requested version.,onCreate,1
"When changing the table schema in code (onCreate()), you should make sure the database is updated.",onCreate,1
Delete the old database file so that onCreate() is run again.,onCreate,1
This is often preferred at development time where you have control over the installed versions and data loss is not an issue.,onCreate,1
Some ways to delete the database file:  Uninstall the application.,onCreate,1
Use the application manager or adb uninstall your.package.name from the shell.,onCreate,1
Clear application data.,onCreate,1
Use the application manager.,onCreate,1
Increment the database version so that onUpgrade() is invoked.,onCreate,1
This is slightly more complicated as more code is needed.,onCreate,1
"For development time schema upgrades where data loss is not an issue, you can just use execSQL(""DROP TABLE IF EXISTS <tablename>"") in to remove your existing tables and call onCreate() to recreate the database.",onCreate,1
"For released versions, you should implement data migration in onUpgrade() so your users don't lose their data.",onCreate,1
(OnCreate() is not executed when the database already exists),onCreate,1
Points to remember when extending SQLiteOpenHelper,onCreate,1
override onCreate and onUpgrade (if needed),onCreate,1
onCreate will be invoked only when getWritableDatabase() or getReadableDatabase() is executed.,onCreate,1
And this will only invoked once when a DBName specified in the first step is not available.,onCreate,1
You can add create table query on onCreate method,onCreate,1
Whenever you want to add new table just change DBversion and do the queries in onUpgrade table or simply uninstall then install the app.,onCreate,1
Max pooling does not dilute the location of the maximum pixel - instead consider it as a way of downsizing.,MaxPool2d,1
Max pooling is just a way to reduce dimensionality of the problem such that your problem fits into device memory.,MaxPool2d,1
A nice side property is that it pools the strongest acitvations from your feature map.,MaxPool2d,1
"If max-pooling is done over a 2x2 region, 3 out of these 8 possible configurations will produce exactly the same output at the convolutional layer.",MaxPool2d,1
"For max-pooling over a 3x3 window, this jumps to 5/8.",MaxPool2d,1
Maximum pooling produces the same depth as it's input.,MaxPool2d,1
With that in mind we can focus on a single slice (along depth) of the input conv.,MaxPool2d,1
"Max pooling does nothing more than iterate over the input image and get the maximum over the current ""subimage"".",MaxPool2d,1
Max pooling decreases the dimension of your data simply by taking only the maximum input from a fixed region of your convolutional layer.,MaxPool2d,1
In the case of bbox prediction it also reduces the number of proposed regions for bboxes.,MaxPool2d,1
Which later in a non-maximum surpression step would kill all redundant proposed bbox locations.,MaxPool2d,1
" And if you are familiar with Max pooling, then you know this is going to cut our image dimensions in half.",MaxPool2d,1
There are 8 directions in which one can translate the input image by a single pixel.,MaxPool2d,1
"They are considering 2 horizontal, 2 vertical and 4 diagonal 1-pixel shifts.",MaxPool2d,1
That gives 8 in total.,MaxPool2d,1
" That means, it tries to make the learning rate inversely proportional to a sane cumulated history.",RMSprop,1
RMsprop keeps the exponentialy decaying average of squared gradients.,RMSprop,1
RMSprop uses a momentum-like exponential decay to the gradient history.,RMSprop,1
Gradients in extreme past have less influence.,RMSprop,1
It modiﬁes AdaGrad optimizer to perform better in the non-convex setting by changing the gradient accumulation into an exponentially weighted moving average.,RMSprop,1
" So, that is the problem with the Adagrad  algorithm and in case of RMSProp instead   of using this cumulative or acumulative sum of  squared gradients the RMSProp takes exponentially   decaying average of the squared gradient.",RMSprop,1
"And, it  does not consider the extreme past histories while   accumulating the sum of square gradient.",RMSprop,1
"And, as  a result of this the algorithm converges rapidly,   once it reaches once your vector reaches  locally convex ball short of surface that .",RMSprop,1
"And,   what you can do is we can assume this point once  it reaches that locally convex error surface,   that as if your Adagrad algorithm is initialized  at that point within that locally convex ball.",RMSprop,1
"However, RMSProp does not keep a moving average of the gradient.",RMSprop,1
"But it can maintain a momentum, like MomentumOptimizer.",RMSprop,1
" So, Adagrad got stuck when it was close to convergence because the learning rate was killed and it was no longer able to move in a direction of b, but for RMSProp, it overcomes this problem by not growing the denominator very aggressively, ok. Now, can you think of any further modifications?",RMSprop,1
" Actually it is taking a moving average of; there is the same as the momentum base role, right?",RMSprop,1
" This is the same as what RMSProp suggested that you divide the learning rate by a cumulated history of gradients, right?",RMSprop,1
" So, in today’s class we will  talk about two more algorithms,   one of them is RMSProp and the other  one is Adam and we will also see a   very closely related algorithm which  is very closely related to RMSProp.",RMSprop,1
" So, the algorithm  that we will talk about is what is RMSProp   which tries to address this problem  of Adagrad algorithm that is vanishing   learning rate as the time increases  as the number of iteration proceeds.",RMSprop,1
"So, what is this RMSProp algorithm does  is instead of taking the accumulative sum   of squares of the gradients of the sum of the  squares of the past and gradients or this past   gradient starts from time t equal to 0.",RMSprop,1
" So in   case of RMSProp, the scaling factor is not  the cumulative sum of gradient histories,   but it is the exponentially decaying  average of the squared gradients.",RMSprop,1
"So, if I go to the updation algorithm is in  RMSProb, the updation algorithm will be like this;   you will find that you will compute the  gradient in the same way as we have done in   case of Adagrad, right.",RMSprop,1
" So, in  case of RMSProp we did not have any concept   of momentum.",RMSprop,1
We are using gradient descent to calculate the gradient and then update the weights by backpropagation.,RMSprop,1
"There are plenty optimizers, like the ones you mention and many more.",RMSprop,1
The optimizers use an adaptive learning rate.,RMSprop,1
With an adaptive loss we have more DoF to increase my learning rate on y directions and decrease along the x direction.,RMSprop,1
They don't stuck on one direction and they are able to traverse more on one direction against the other.,RMSprop,1
Adam with�beta1=1�is equivalent to RMSProp with�momentum=0.,RMSprop,1
The argument�beta2�of Adam and the argument�decay�of RMSProp are the same.,RMSprop,1
A detailed description of rmsprop.,RMSprop,1
maintain a moving (discounted) average of the square of gradients  divide gradient by the root of this average  can maintain a momentum,RMSprop,1
" So, your  basically the operation that was done in Adagrad   algorithm is r t, the scaling factor which is  1 upon square root of epsilon plus r t i.",RMSprop,1
"So,   if you go for component wise this r t i is  nothing, but sum of g t i square of this   or let me put it as g tau square instead of  g t g tau square and you take the summation   of tau is equal to say 1 to t. So, you find  that this being a square term and which you   are going on adding.",RMSprop,1
"So, r t goes on increasing,  it monotonically increases , it does not reduce.",RMSprop,1
" We created our ADADELTA method to overcome the sensitivity to the hyperparameter selection as well as to avoid the continual decay of the learning rates."".",Adadelta,1
"The second good thing about AdaDelta is you don't have to choose the learning rate, it automatically computes it.",Adadelta,1
Adaptive gradient algorithms have learning rates for each parameter.,Adadelta,1
This is very helpful when you have models where some parameters might be more sparse (increase its learning rate) or not sparse (decrease its learning rate).,Adadelta,1
Adadelta is an adaptive learning rate method which uses exponentially decaying average of gradients.,Adadelta,1
"Adadelta optimizer has a way to adaptively change learning rate but still, it needs an initial value of learning rate.",Adadelta,1
"At step 0, the running average of these updates is zero, so the first update will be very small.",Adadelta,1
 If you have gone side-track just remember one thing we are going into so much hassle because normal gradient descent has its learning rate constant for the entire training phase... which is so LAME!,Adadelta,1
So the optimizers like AdaDelta have some techniques to vary the learning rate with every iteration that's it.,Adadelta,1
"As the first update is very small, the running average of the updates will be very small at the beginning, which is kind of a vicious circle at the beginning",Adadelta,1
"Looking at [Keras source code (hyper-link)], learning rate is recalculated based on decay like:",Adadelta,1
I suspect that decay is not intended to be used with Adadelta.,Adadelta,1
It just multiplies the variable updates (see [the update op implementation (hyper-link)]).,Adadelta,1
"For any ""automatic learning rate"" scheme, you can always scale the resulting updates by a constant (whether it's necessary to do so is a separate issue).",Adadelta,1
 To overcome this issue AdaDelta was born.,Adadelta,1
"In the AdaDelta paper by Matthew Zeiler, he talked about this drawback of AdaGrad saying ""due to the continual accumulation of squared gradients in the denominator, the learning rate will continue to decrease throughout training, eventually decreasing to zero and stopping training completely.",Adadelta,1
 This was made possible due to this rho hyperparameter.,Adadelta,1
It is also known as 'Decay Constant'.,Adadelta,1
"Instead of writing it with this equation, I have written it separately because we will need delta theta in this equation.",Adadelta,1
"So, this term over here is the learning rate calculated by AdaDelta.",Adadelta,1
 We are dividing square root of delta x with the square root of alpha... again this epsilon is a very small number to avoid zero division error.,Adadelta,1
Then we can sum it with our theta term... normal gradient descent stuff!,Adadelta,1
"At last, we will update our initialized delta x term by rho multiplied by delta x of the previous iteration plus one minus rho into update square.",Adadelta,1
" So, what AdaDelta does is in case of   RMSProp you are taking the exponentially decaying  average of the squared gradient; AdaDelta instead   of taking the exponentially decaying average of  squared gradient it computes the moving window   average.",Adadelta,1
"So, you can take a more window size of  say W. So, when you compute v t, v t is computed   over a past window size of W. So, if I take the  window size W is equal to say 5, in that case in   order to compute say v 10 it will take the first  5; that means, it will take v 10 v 9 v 8 v 7 and   v 6 or the say s 6 s 7 s 8 s 9 and s 10.",Adadelta,1
"So, you are computing average over past   samples which are within this window size of W.  So, this is what is moving window average.",Adadelta,1
Use an adaptive gradient algorithm like Adam or Adadelta or RMSProp.,Adadelta,1
The rule is related to updates with decay.,Adadelta,1
The full algorithm from the [paper (hyper-link)] is:,Adadelta,1
The issue is that they accumulate the square of the updates.,Adadelta,1
Here is my code to play a bit with the Adadelta optimizer:,Adadelta,1
The thing you need to know about AdaDelta is the general context of online machine learning.,Adadelta,1
"Online machine learning is where you get your data one-at-a-time (and thus have to update your model's parameters as the data comes in), as opposed to batch machine learning where you can generate your machine learning model with access to the entire dataset all at once.",Adadelta,1
" So where onCreate is called when the activity is first created, onPause it's going to be called when you leave the activity.",onPause,1
whenever a new ACTIVITY starts the previous activity's onPause will be defiantly called in any circumstances.,onPause,1
onPause is guaranteed to be called on Activity A (No matter whether Activity B is non-full-sized transparent or full-sized).,onPause,1
onPause() is always called on your Activity if it is in the foreground when Android wants to do something else.,onPause,1
It may start another Activity which may result in your Activity's onStop() getting called.,onPause,1
It may just call onResume() on your activity.,onPause,1
It may just kill your process without calling any more of your lifecycle methods.,onPause,1
"Since onStop() is not guaranteed to be called, you can't always do in onStop() what is done in onPause().",onPause,1
What is the good reason for always having onPause() before onStop().,onPause,1
We can do in onStop() what is done in onPause().,onPause,1
 onPause is called when your activity is about to resume any previous activity.,onPause,1
onPause() is called when your activity is no longer at the top of the activity stack.,onPause,1
"A Dialog by itself is not an Activity, so will not replace the current Activity at the top of the stack, so will not cause anything to pause.",onPause,1
"A dialog (lower-case) does not need to be implemented by a Dialog class, however.",onPause,1
"If you can still see any part of it (Activity coming to foreground either doesn't occupy the whole screen, or it is somewhat transparent), onPause() will be called.",onPause,1
"**I am not referring to an Android Dialog here, rather a conceptual idea of something that pops up and only obscures part of the user screen.",onPause,1
onPause() is always called.,onPause,1
This is guaranteed.,onPause,1
If you need to save any state in your activity you need to save it in onPause().,onPause,1
"onStop() may be called after onPause(), or it may not.",onPause,1
Depends on the situation.,onPause,1
 In onPause it could be possible when it is either about to resume a previous activity or it is partially visible but user is leaving the activity.,onPause,1
"This is typically considered as the best state to commit unsaved changes to persistent data, stop animations and anything that consumes resources.",onPause,1
 So use onPause or onStop to save the data or state.,onPause,1
" And onPause it will be onPause, onResume, onResume.",onPause,1
And start.,onPause,1
" [SOUND] onPause indicates that the activity has lost focus, followed by onStop when the app is no longer visible.",onPause,1
" Alternatively, through the static registration, you can also register and unregister a receiver at one time via the context class, Register receiver method either in onCreate, or in onResume activity methods.",onPause,1
 And unregister receiver methods either in onDestroy or in onPause activity methods.,onPause,1
"Practically, one should consider the difference between “onPause()” and “onPause() + onStop()”.",onPause,1
Whenever some new activity occurs and occupies some partial space of the Screen.,onPause,1
So your previously running activity is still visible to some extent.,onPause,1
"In this Case, the previously running activity is not pushed to Back Stack.",onPause,1
"So, here only onPause() method is called.",onPause,1
onPause()- Screen is partially covered by other new activity.,onPause,1
The Activity is not moved to Back Stack.,onPause,1
onPause() + onStop()- Screen is fully covered by other new activity.,onPause,1
The Activity is moved to Back Stack.,onPause,1
1- a part of previous activity is visible or the new activity is transparent: only onPause will be called.,onPause,1
"NOTE 2: if its an Activity whose theme is set to a dialog, the behavior will be just like a normal activity.",onPause,1
NOTE 3: apparently a system dialog like permission dialog since marshmallow will cause onPause.,onPause,1
"No, if some activity comes into foreground, that doesn't necessarily mean that the other activity is completely invisible.",onPause,1
onStop will only be called when Activity A is completely overridden by full-sized Activity B.,onPause,1
So in onPause you can save an Activity's state or some other useful info if required.,onPause,1
Being in the foreground means that the activity has input focus.,onPause,1
"It is simple: BatchNorm has two ""modes of operation"": one is for training where it estimates the current batch's mean and variance (this is why you must have batch_size>1 for training).",BatchNorm1d,1
"The other ""mode"" is for evaluation: it uses accumulated mean and variance to normalize new inputs without re-estimating the mean and variance.",BatchNorm1d,1
In this mode there is no problem processing samples one by one.,BatchNorm1d,1
" Alright, so batch normalization, or in short, batch norm goes  back to a paper published in 2015, called batch  normalization, accelerating deep network training by reducing  internal covariate shift.",BatchNorm1d,1
Pytorch does its batchnorms over axis=1.,BatchNorm1d,1
But it also has tensors with axis=1 as channels for convolutions.,BatchNorm1d,1
"BatchNorm1d can also handle Rank-2 tensors, thus it is possible to use BatchNorm1d for the normal fully-connected case.",BatchNorm1d,1
" In practice,  people nowadays, it's more common to actually recommend if  you use dropout to recommend having batch norm after the  activation.",BatchNorm1d,1
" Yeah, and also  note that now when we use batch norm, batch norm has learnable  parameters.",BatchNorm1d,1
"So if we use batch norm in a given layer, it has we  have an additional two vectors that have the same dimension as  the bias vector, right.",BatchNorm1d,1
"So if we have, we use batch norm here in  this layer, we will have two, four dimensional vectors, like  this bias vector here would also be four dimensional, right,  because there's one bias for each in layer activation.",BatchNorm1d,1
what does [BatchNorm1d (hyper-link)] do mathematically?,BatchNorm1d,1
" And also, we will talk briefly about how batch norm  behaves during inference, because yeah, you know that  during training, we have mini batches, but that's not  necessarily the case if we want to use our network for  prediction.",BatchNorm1d,1
" But again, this is again, highlighting  train and evil are important here that we during training set  our model into training mode, because that's where batch norm  will compute the running mean and the running variance, I will  talk about this in the next slide.",BatchNorm1d,1
"So here, batch norm will  actually compute some running statistics during training.",BatchNorm1d,1
" So usually, practice people keep a moving average of both the  mean and the variance during training.",BatchNorm1d,1
"So you can think of it  as also as the running mean, how it's computed is by having a  momentum term, it's usually a small value like point one.",BatchNorm1d,1
"And  this is multiplied by the running mean from the previous  on epoch, or sorry, previous mini batch.",BatchNorm1d,1
"And then what you so  you have this, this term, this is like the running mean times  momentum term, this is a point one value.",BatchNorm1d,1
"And then you have one  minus the momentum, this is like a point nine value, then plus  yet plus point nine times the current sample mean.",BatchNorm1d,1
"So that's  the mini batch mean, and you just do the same thing also for  the running variance.",BatchNorm1d,1
"So here, essentially, this is just like a  moving average or running mean.",BatchNorm1d,1
And you do the same thing for  the variance.,BatchNorm1d,1
 That's because there is a  slightly different version of batch norm for convolutional  networks.,BatchNorm1d,1
We will discuss this in the convolutional network  lecture where this would be called batch norm 2d for the  convolution networks.,BatchNorm1d,1
"So to keep them apart, this is called batch  norm 1d.",BatchNorm1d,1
 And how  you can think of it as an additional normalization layer.,BatchNorm1d,1
" So  yeah, here, that's the first step of batch norm, there are  two steps.",BatchNorm1d,1
So the first step is to normalize the net inputs.,BatchNorm1d,1
So the j is the  feature index again.,BatchNorm1d,1
So you can actually use batch norm for any  type of input.,BatchNorm1d,1
So we will also see there is a two dimensional  version for that for convolutional networks later on.,BatchNorm1d,1
" So let's  say we have, yeah, the J feature.",BatchNorm1d,1
"And if I go back, so if  you consider this activation here, what are the features, so  the features are all essentially all the previous layer  activations, right?",BatchNorm1d,1
So all these go into that activation.,BatchNorm1d,1
So all  of these here are the features of this activation here.,BatchNorm1d,1
So J  really is the index over the activations from the previous  layer.,BatchNorm1d,1
" But in the regular batch norm in the 1d version, we were  computing things for each feature.",BatchNorm1d,1
So we were computing  this gamma and beta for each feature over the batch  dimension.,BatchNorm1d,1
" There are places of computing myu and Sigma is running average, again you can do that as well as you know some exponentially varied average schemes are available but this is the way to, during testing you will calculate myu and Sigma for every layer, this myu and Sigma are for the activations of every layer, every neuron and every layer.",BatchNorm1d,1
 Calculated by running it through the entire forward pass dataset.,BatchNorm1d,1
 That will be added computation or or you can just maintain a running average during training.,BatchNorm1d,1
Which one to use depends on the dimensionality of input data.,BatchNorm1d,1
"BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice.",BatchNorm1d,1
Core ML does not have 1-dimensional batch norm.,BatchNorm1d,1
"If you want to convert this model, you should fold the batch norm weights into those of the preceding layer and remove the batch norm layer.",BatchNorm1d,1
The getBaseContext() is the method of ContextWrapper.,getBaseContext,1
"And ContextWrapper is, ""Proxying implementation of Context that simply delegates all of its calls to another Context.",getBaseContext,1
"Can be subclassed to modify behavior without changing the original Context.""",getBaseContext,1
(as per javadocs),getBaseContext,1
So this is used to delegate the calls to another context.,getBaseContext,1
The method getBaseContext() is only relevant when you have a ContextWrapper.,getBaseContext,1
The method getBaseContext() can be used to access the “base” Context that the ContextWrapper wraps around.,getBaseContext,1
"You might need to access the “base” context if you need to, for example, check whether it’s a Service, Activity or Application:",getBaseContext,1
getBaseContext() -  If you want to access Context from another context within application you can access.,getBaseContext,1
Or if you need to call the “unwrapped” version of a method:,getBaseContext,1
"The answer by Waqas is very clear and complete, however I'd like to further clarify the difference between using this vs. getBaseContext(), or getApplication() vs. getApplicationContext().",getBaseContext,1
"Both Activity and Application extend not Context itself, but ContextWrapper, which is a",getBaseContext,1
"""Proxying implementation of Context that simply delegates all of its calls to another Context"".",getBaseContext,1
"I doubt it makes any practical difference, though.",getBaseContext,1
The same logic applies to getApplication() vs. getApplicationContext().,getBaseContext,1
getApplicationContext(),getBaseContext,1
this is used for application level and refer to all activities.,getBaseContext,1
 I'm just going to create the table and its structure.,onCreate,2
"To do that, I'll remove the TODO comment, and I'll use the database argument that's being passed in.",onCreate,2
"It's named db, and I'll call a method called execute SQL or execSQL for short, and I'll pass in my constant that contains the SQL command that will create the table, that'll be TABLE_CREATE.",onCreate,2
The version number is the int argument passed to the [constructor (hyper-link)].,onCreate,2
"In the database file, the version number is stored in [PRAGMA user_version (hyper-link)].",onCreate,2
"If onCreate() returns successfully (doesn't throw an exception), the database is assumed to be created with the requested version number.",onCreate,2
"So when the database helper constructor is called with a name (2nd param), platform checks if the database exists or not and if the database exists, it gets the version information from the database file header and triggers the right call back",onCreate,2
catch is the constructor,onCreate,2
" I'll pass in the db argument, and now I've recreated the table structure.",onCreate,2
"super(context, DBName, null, DBversion); - This should be invoked first line of constructor",onCreate,2
"When we create DataBase at a first time (i.e Database is not exists) onCreate() create database with version which is passed in  SQLiteOpenHelper(Context context, String name, SQLiteDatabase.CursorFactory factory, int version)",onCreate,2
" So just to summarize, the hyperparameters for pooling are f, the filter size and s, the stride, and maybe common choices of parameters might be f equals two, s equals two.",MaxPool2d,2
"This is used quite often and this has the effect of roughly shrinking the height and width by a factor of above two, and a common chosen hyperparameters might be f equals two, s equals two, and this has the effect of shrinking the height and width of the representation by a factor of two.",MaxPool2d,2
"I've also seen f equals three, s equals two used, and then the other hyperparameter is just like a binary bit that says, are you using max pooling or are you using average pooling.",MaxPool2d,2
" If you want, you can add an extra hyperparameter for the padding although this is very, very rarely used.",MaxPool2d,2
 So the number of input channels is equal to the number of output channels because pooling applies to each of your channels independently.,MaxPool2d,2
"Adding padding is NOT an ""absolute must"".",MaxPool2d,2
"Sometimes it can be useful to control the size of the output so that it is not reduced by the convolution (it can also augment the output, depending on its size and kernel size).",MaxPool2d,2
" When you do max pooling, usually, you do not use any padding, although there is one exception that we'll see next week as well.",MaxPool2d,2
"But for the most parts of max pooling, usually, it does not use any padding.",MaxPool2d,2
You should try to put your learning rate at 0.001.,RMSprop,2
"The error clearly says, Tanh only takes 1 argument, a tensor.",Tanh,2
"Activation functions accept a single tensor, you are passing two random list elements.",Tanh,2
"So yes, lr is just starting learning rate.",Adadelta,2
"However, note that, by default, decay parameter for Adadelta is zero and is not part of the “standard” arguments, so your learning rate would not be changing its value when using default arguments.",Adadelta,2
learning_rate: A Tensor or a floating point value.,Adadelta,2
"If you really want to use Adadelta, use the parameters from the paper: learning_rate=1., rho=0.95, epsilon=1e-6.",Adadelta,2
"A bigger epsilon will help at the start, but be prepared to wait a bit longer than with other optimizers to see convergence.",Adadelta,2
To match the exact form in the original paper use 1.0.,Adadelta,2
"To print it after every epoch, as @orabis mentioned, you can make a callback class:",Adadelta,2
and then add its instance to the callbacks when calling model.fit() like:,Adadelta,2
"On the other hand, rho parameter, which is nonzero by default, doesn’t describe the decay of the learning rate, but corresponds to the fraction of gradient to keep at each time step (according to the [Keras documentation (hyper-link)]).",Adadelta,2
Although as you can see in tensorflow [source code (hyper-link)] to achieve the exact results of Adadelta paper you should set it to 1.0:,Adadelta,2
"Note that in the paper, they don't even use a learning rate, which is the same as keeping it equal to 1.",Adadelta,2
As per the documentation we need to specify num_features parameter which is the input size of tensor.,BatchNorm1d,2
"Batch normalization works when batch size is greater than 1, so an input of shape (1, 32) won't work.",BatchNorm1d,2
In most cases you should be safe with the default setting.,BatchNorm1d,2
"If you pass torch.Tensor(2,50,70) into nn.Linear(70,20), you get output of shape (2, 50, 20) and when you use BatchNorm1d it calculates running mean for first non-batch dimension, so it would be 50.",BatchNorm1d,2
" So yeah, I here insert batch norm after the  linear layer, notice that there's a one D, it may be  confusing.",BatchNorm1d,2
Why is there a one D?,BatchNorm1d,2
 But on by default bias is true if  you don't set anything and I found it was the same  performance.,BatchNorm1d,2
"For both functions, the d1 parameter is the number of features, and equals dim C of the input tensor.",BatchNorm1d,2
"Try a larger batch size, like 2.",BatchNorm1d,2
"It takes input of shape (N, *, I) and returns (N, *, O), where I stands for input dimension and O for output dim and * are any dimensions between.",BatchNorm1d,2
"PyTorch's CrossEntropyLoss expects unbounded scores (interpretable as logits / log-odds) as input, not probabilities (as the CE is traditionally defined).",cross_entropy,2
So the weights are changed to reduce CE and thus finally leads to reduced difference between the prediction and true labels and thus better accuracy.,cross_entropy,2
" So our loss, if we just care about disease  (we're going to be passed the three things)   we're just going to calculate cross_entropy  on our input versus disease.",cross_entropy,2
" So the way you read this colon means every row,   and then colon 10 means every column up to the  10th.",cross_entropy,2
"[ContextWrapper.getBaseContext() (hyper-link)]:  If you need access to a Context from within another context, you use a ContextWrapper.",getBaseContext,2
"That ""real"" context is what you get by using getBaseContext().",getBaseContext,2
getApplicationContext() - Returns the context for all activities running in application.,getBaseContext,2
getContext() - Returns the context view only current running activity.,getBaseContext,2
[View.getContext() (hyper-link)]:  Returns the context the view is currently running in.,getBaseContext,2
Usually the currently active Activity.,getBaseContext,2
[Activity.getApplicationContext() (hyper-link)]:  Returns the context for the entire application (the process all the Activities are running inside of).,getBaseContext,2
"Use this instead of the current Activity context if you need a context tied to the lifecycle of the entire application, not just the current Activity.",getBaseContext,2
Note that onCreate() is only invoked when the database file didn't exist so the DROP TABLE is really not needed.,onCreate,3
" And then that query you have to execute inside the callback method onCreate, which you have to override once you extend a class with the SQLiteOpenHelper class.",onCreate,3
So that query you have to execute inside the onCreate.,onCreate,3
Either you can execute it directly-- you can put the whole query directly inside a db.exeSQL.,onCreate,3
" And inside the SQLiteOpenHelper class, I will be putting a database-- I will be creating that whole structure.",onCreate,3
"There will be onCreate, there will be onUpgrade, insert query will be there, and update, delete-- all those queries will be there inside one class.",onCreate,3
That will be the subclass of SQLiteOpenHelper class.,onCreate,3
" If the database already exists, but I've indicated through the database version value that I'm changing the version, that is that I've incremented it, then the onUpgrade method will be called.",onCreate,3
" Then once I've dropped the table, in order to recreate it, I'll simply call my onCreate method.",onCreate,3
This is the one exception to what I just said that I wouldn't call the onCreate method directly.,onCreate,3
Below explanation explains onUpgrade case with an example.,onCreate,3
"Say, your first version of application had the DatabaseHelper (extending SQLiteOpenHelper) with constructor passing version as 1 and then you provided an upgraded application with the new source code having version passed as 2, then automatically when the DatabaseHelper is constructed, platform triggers onUpgrade by seeing the file already exists, but the version is lower than the current version which you have passed.",onCreate,3
Now say you are planing to give a third version of application with db version as 3 (db version is increased only when database schema is to be modified).,onCreate,3
"In such incremental upgrades, you have to write the upgrade logic from each version incrementally for a better maintainable code",onCreate,3
Example pseudo code below:,onCreate,3
Notice the missing break statement in case 1 and 2.,onCreate,3
This is what I mean by incremental upgrade.,onCreate,3
"Say if the old version is 2 and new version is 4, then the logic will upgrade the database from 2 to 3 and then to 4",onCreate,3
"If old version is 3 and new version is 4, it will just run the upgrade logic for 3 to 4",onCreate,3
" So  initially, the default tip is going to be 15%.",onCreate,3
"So with that diviner the constant before  we do anything in the listener, great in   the onCreate method, will they seek bar tip dot  progress is equal to initial tip percent.",onCreate,3
And we   also want to update the label appropriately.,onCreate,3
" Finally, we need to initialize the SDK.",onCreate,3
"So here, in the onCreate of our initial activity, we just type MobileAds.initialize, and you'll notice we can parse a context, and then there's a completion listener.",onCreate,3
"So if you're using mediation, you'll want to wait for the listener callback before loading an ad.",onCreate,3
 The first two steps are calling WindowCompat.set DecorFitsSystemWindows with false in your activities onCreate method and setting windowSoftInputMode to adjustResize in your activity manifest.,onCreate,3
"Together, these tell the platform that you're going to handle all insets yourself, including the soft keyboard insets.",onCreate,3
" As soon as you are ready, open the activity where you plan to show the Google Pay button and obtain a new instance of the paymentsClient inside of your onCreate method.",onCreate,3
 Now you see we've got it onCreate and an onUpgrade method.,onCreate,3
" In the onCreate method, we're going to start by creating a string to build the weather entry table using data defined within the weather entry contract.",onCreate,3
" Either you want to alter the table, or you want to drop the whole table which was previously used by the user.",onCreate,3
" And again, pass the control back to onCreate, because in onCreate, there will be a new table created for you.",onCreate,3
" There are some optional methods, also, which you can take a look at, like onDowngrade, onConfigure, or onOpen.",onCreate,3
 That will generate the Java class and add two methods named onCreate and onUpgrade.,onCreate,3
 Now let's go to the onCreate and onUpgrade methods.,onCreate,3
 I will never call these two methods directly.,onCreate,3
They'll only be called by the SDK.,onCreate,3
" In the onCreate method, you should add code that creates your database tables and if you like, you can also add code to add data.",onCreate,3
" So, that command is called and my table is created.",onCreate,3
" I'll pass in my LOGTAG constant and a literal string as a message, Table has been created, and that's all I need to do in the onCreate method.",onCreate,3
 So I'll move the cursor into the onCreate method.,onCreate,3
 I'll call it within this class but not from the rest of the application.,onCreate,3
 I'll call the onCreate method.,onCreate,3
" Now I'll go down to my onCreate method, and I'll place the cursor right here before I call the ArrayAdapter code and I'll instantiate the dbhelper object.",onCreate,3
"I'll say dbhelper = new, and then I'll use my new class constructor method and I'll pass in this as the context, and then I'll get a reference to the database.",onCreate,3
I'll call database = dbhelper.,onCreate,3
And then I'll call a method that this class has inherited from SQLite database.,onCreate,3
"This returns a reference to the connection to the database, and I'll be able to use that connection to do things like inserting data, retrieving data, updating, and deleting.",onCreate,3
"Simply by calling the method, that will trigger the onCreate method of my database open helper class, and in turn that will create the table structure.",onCreate,3
 Note how we're using all of the weather entry constants to write our SQL statements.,onCreate,3
"Suppose for the first time deployment , database version was 1 and in second deployment there was change in database structure like adding extra column in table.",onCreate,3
Suppose database version is 2 now.,onCreate,3
"For what it's worth, it's also a bad idea to catch exceptions in onCreate().",onCreate,3
"If the method returns successfully (doesn't throw), the framework thinks the database was created successfully.",onCreate,3
Pooling is of MUCH MORE IMPORTANCE in convnets.,MaxPool2d,3
" But you see, max pooling used much more in the neural network than average pooling.",MaxPool2d,3
"Pooling is not exactly ""down-sampling"", or ""losing spatial information"".",MaxPool2d,3
"Consider first that kernel calculations have been made previous to pooling, with full spatial information.",MaxPool2d,3
Pooling reduces dimension but keeps -hopefully- the information learnt by the kernels previously.,MaxPool2d,3
But if you look carefully at what's going on you may notice that the after first convolutional layer the dimension of your data might severely increase if you don't do the tricks like pooling.,MaxPool2d,3
"And, by doing so, achieves one of the most interesting things about convnets; robustness to displacement, rotation or distortion of the input.",MaxPool2d,3
"Invariance, if learnt, is located even if it appears in another location or with distortions.",MaxPool2d,3
"It also implies learning through increasing scale, discovering -again, hopefully- hierarchical patterns on different scales.",MaxPool2d,3
"And of course, and also necessary in convnets, pooling makes computation possible as number of layers grows.",MaxPool2d,3
" Here, I am going to use, sure you have a five by five input and we're going to apply max pooling with a filter size that's three by three.",MaxPool2d,3
So f is equal to three and let's use a stride of one.,MaxPool2d,3
"So in this case, the output size is going to be three by three.",MaxPool2d,3
"And the formulas we had developed in the previous videos for figuring out the output size for conv layer, those formulas also work for max pooling.",MaxPool2d,3
"So, that's n plus 2p minus f over s plus 1.",MaxPool2d,3
That formula also works for figuring out the output size of max pooling.,MaxPool2d,3
"But in this example, let's compute each of the elements of this three by three output.",MaxPool2d,3
"The upper left-hand elements, we're going to look over that region.",MaxPool2d,3
So notice this is a three by three region because the filter size is three and to the max there.,MaxPool2d,3
"So, that will be nine, and then we shifted over by one because which you can stride at one.",MaxPool2d,3
" So if you need to know more about max pooling, or you just need a refresher, same thing with padding here for zero padding, activation functions, anything like that, then be sure to check those episodes out and the corresponding deep learning fundamentals course on depot's or.com.",MaxPool2d,3
"Vanilla adaptive gradients (RMSProp, Adagrad, Adam, etc) do not match well with L2 regularization.",RMSprop,3
"Usually batch sizes around 40 gives better results, as for my experience training with 40 batch size for 3 epocs using default RMsprop gives around 89% accuracy.",RMSprop,3
Have you tried adjusting the learning rate of RMsprop optimizer ?,RMSprop,3
"try with a very small value first ( default is 0.001, in keras implementation) and try to increment it with factors of 10 or 100.",RMSprop,3
Adam is a recently proposed update that looks a bit like RMSProp with momentum.,RMSprop,3
"Just a question for you, why aren't you using Adam Optimizer which seem to be the best optimizer in a lot of cases ?",RMSprop,3
(It is even partially inspired from RMSProp that you use),RMSprop,3
"In   particular, when combined with adaptive gradients, L2   regularization leads to weights with large gradients   being regularized less than they would be when using   weight decay.",RMSprop,3
"Although the expression ""Adam is RMSProp with momentum"" is widely used indeed, it is just a very rough shorthand description, and it should not be taken at face value; already in the original [Adam paper (hyper-link)], it was explicitly clarified (p. 6):",RMSprop,3
"There are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running average of first and second moment of the gradient.",RMSprop,3
The best algorithm is the one that can traverse the loss for that problem pretty well.,RMSprop,3
"Tanh seems maybe slower than ReLU for many of the given examples, but produces more natural looking fits for the data using only linear inputs, as you describe.",Tanh,3
For [example a circle (hyper-link)] vs a [square/hexagon thing (hyper-link)].,Tanh,3
"Most of time tanh is quickly converge than sigmoid and logistic function, and performs better accuracy [[1] (hyper-link)].",Tanh,3
"A good neuron unit should be bounded, easily differentiable, monotonic (good for convex optimization) and easy to handle.",Tanh,3
"If you consider these qualities, then I believe you can use ReLU in place of the tanh function since they are very good alternatives of each other.",Tanh,3
"Update in attempt to appease commenters: based purely on observation, rather than the theory that is covered above, Tanh and ReLU activation functions are more performant than sigmoid.",Tanh,3
"For example, try limiting the number of features to force logic into network nodes in XOR and [sigmoid rarely succeeds (hyper-link)] whereas [Tanh (hyper-link)] and [ReLU (hyper-link)] have more success.",Tanh,3
" In the Deep Neural Network, which I mentioned earlier, this affine and tanh process is repeated three times.",Tanh,3
This combination of Affine and Tanh can be used once to function as a Neural Network for one layer.,Tanh,3
"From documentation, [https://pytorch.org/docs/stable/nn.html (hyper-link)]",Tanh,3
"Usually, you can use torch.cat to concatenate two tensors.",Tanh,3
"However, recently rectified linear unit (ReLU) is proposed by Hinton [[2] (hyper-link)] which shows ReLU train six times fast than tanh [[3] (hyper-link)] to reach same training error.",Tanh,3
" After this, we will update the alpha term unlike AdaGrad there is one more term in this equation known as 'rho' this is used to avoid infinitely increasing alpha value.",Adadelta,3
"As we saw in AdaGrad, the alpha value was increasing with each iteration but in the case of AdaDelta, the alpha first increased till 50 iterations and then continuously decreased so now there is no problem of learning rate decay.",Adadelta,3
" Now, let's visit our Adam again... Adam's main motive was to add the concept of momentum into the previous optimizer like AdaDelta.",Adadelta,3
" So,   you find that this RMSProp which takes  expose exponentially decaying average,   the AdaDelta takes a moving window average of  the squared gradients.",Adadelta,3
"So, that is the only   difference between RMSProp and AdaDelta.",Adadelta,3
Adadelta has a very slow start.,Adadelta,3
"I think Adadelta performs better with bigger networks than yours, and after some iterations it should equal the performance of RMSProp or Adam.",Adadelta,3
 As you can see from these plots the alpha is increasing with every iteration and the learning rate is decreasing so this is quite problematic.,Adadelta,3
" This is just like AdaDelta... instead of rho, we are writing beta 1 and beta 2.",Adadelta,3
" So, in today’s class we will  talk about two more algorithms,   one of them is RMSProp and the other  one is Adam and we will also see a   very closely related algorithm which  is very closely related to RMSProp.",Adadelta,3
" And we said   that there is a very closely related algorithm  very closely related to this RMSProp; so,   that closely related algorithm  is what is known as AdaDelta.",Adadelta,3
"So, we have a closely related algorithm known as  AdaDelta.",Adadelta,3
" And  in fact, both these algorithms were proposed   almost simultaneously, but independently, and  both of them gives almost similar performance.",Adadelta,3
" Then you  have NAG Nesterov accelerated gradient operation,   then you have Adagrad, then you have a  Adadelta, then you have RMSProp.",Adadelta,3
"If you are working with something like neural machine translation, this sparsity is an issue.",Adadelta,3
"Very few people use it today, you should instead stick to:",Adadelta,3
"This algorithm is very similar to Adadelta, but performs better in my opinion.",Adadelta,3
"A dialog**, for example, may not cover the entire previous Activity, and this would be a time for onPause() to be called.",onPause,3
"In most Activities, you will find that you will need to put code in onResume() and onPause().",onPause,3
" And let's see what happens when config changes, how it affects the activity.",onPause,3
"So, on configuration change, the Android first shuts down the activity by calling method in sequence, that is onPause, then onStop and then onDestroy.",onPause,3
"For instance, an activity can be visible but partially obscured by a dialog that has focus.",onPause,3
"In that case, onPause() will be called, but not onStop().",onPause,3
"When the dialog goes away, the activity's onResume() method will be called (but not onStart()).",onPause,3
 The implementation of onPause is very fast because the next activity is not resumed until this method returns.,onPause,3
"And is either followed by onResume, if the activity returns back to the front, or onStop, if the activity is becoming invisible to the user.",onPause,3
This is how we implement this onPause callback method.,onPause,3
" Firstly, knowing the OS can terminate your app's process without warning at any time you can't rely on having an onExit() handler that will be called when your app is closed.",onPause,3
"Instead, your Activities should listen for onPause handlers that indicate your app is no longer active.",onPause,3
"At this point is may be terminated at any time, so it should save any user data to prevent potential data loss.",onPause,3
"On other hands, if some new Activity occurs and occupies the full screen so that your previously running activity is disappeared.",onPause,3
"In this Case, your previously running activity is moved to Back Stack.",onPause,3
"Here, onPause() + onStop() are called.",onPause,3
NOTE 1: if a dialog starts on top of an activity NONE of onPause or onStop will be called.,onPause,3
"For example, it is not uncommon to implement one with an Activity whose theme is set to that of a dialog.",onPause,3
"In this case, displaying the dialog-as-an-Activity will cause the new Activity to be on the top of the stack, pausing what previously was there.",onPause,3
"If you cannot see any part of it, onStop() will be called.",onPause,3
"You usually don't have to do anything in onStop(), onStart() or onRestart().",onPause,3
" So, prior to being restarted again, it is called after your activity has been stopped.",onPause,3
" On onResume, called when the activity will start interacting with the user.",onPause,3
At this point your activity is at the top of the activity stack with user input going to it.,onPause,3
 onStop is called when your activity is no longer visible but still exists and all state information are preserved.,onPause,3
 It has now moved to the top of the activity stack and is always followed by onPause.,onPause,3
" Start by freeing resources, stopping services, disabling sensor listeners and turning off location requests, and otherwise disabling anything that consumes resources.",onPause,3
all within the onPause handler of your Activities.,onPause,3
Take this a step further by avoiding singletons and custom Application objects within your application whenever possible.,onPause,3
" So in this particular project I'm going to create my main activity and I'm going to put all the lifecycle methods: onCreate, onStart, onResume onPause, onStop, onRestart and onDestroy.",onPause,3
 So let me just override other methods also.,onPause,3
"OnStart and onResume... onPause... onStop... onRestart... and last, onDestroy.",onPause,3
" So, if I now try to click on Home button, what should be the lifecycle now?",onPause,3
It should go to onPause first.,onPause,3
Then onStop.,onPause,3
"So, onPause Finished and onStop Finished.",onPause,3
"In the same manner, it should show here, too.",onPause,3
"As you can see, onPause and onStop.",onPause,3
But this application is still not destroyed.,onPause,3
" Last but not the least, how we destroy our application, that is the destroying lifecycle.",onPause,3
" If I click my back button, it goes to onPause, onStop and all the way to onDestroy.",onPause,3
 Now my application is indeed destroyed.,onPause,3
" So you can see in Log also, onPause, onStop, onDestroy.",onPause,3
 But you must be thinking that I can still see my application in this task manager.,onPause,3
 And the other thing we want to do is to save the note.,onPause,3
"So to do that, we're going to use another method called onPause.",onPause,3
" Now in onPause, we can use that same database object.",onPause,3
 We've seen onCreate and we've seen onPause.,onPause,3
" If you forget this, the Android system reports a leaked broadcast receiver error.",onPause,3
"For instance, if you registered a receive in onResume method of an activity, you should unregister it inside onPause method.",onPause,3
2- previous activity is completely covered by new activity: both onPause and onStop will be called,onPause,3
"The first activity with the fields is obscured by another activity, and the user can no longer interact with it.",onPause,3
"However, it is still visible with all the resulting consequences.",onPause,3
That leaves a question which activity is considered fully opaque and covering the whole screen and which isn't.,onPause,3
This decision is based on the window containing the activity.,onPause,3
"If the window has a flag windowIsFloating or windowIsTranslucent, then it is considered that the activity doesn't make the underlying stuff invisible, otherwise it does and will cause onStop() to be called.",onPause,3
For instance: When a user traverses from Activity A to Activity B (FullScreen Non Transparent) following things happens,onPause,3
Activity A's state changes to paused state (which calls onPause on Activity A),onPause,3
Then Activity A's state changes from paused to stopped state (which calls  onStop on Activity A) .,onPause,3
onStop is called because Activity B was full screen  non transparent activity.,onPause,3
(If Activity B is non-full-sized or transparent then onStop is NOT called on Activity A),onPause,3
" And  having batch norm before the activation, that's usually how  yeah, that was originally how it was proposed in the paper.",BatchNorm1d,3
 That  means it may be that we need fewer epochs to get the same  loss that we would achieve if we don't use batch norm.,BatchNorm1d,3
"So you  usually with batch norm, the networks train faster.",BatchNorm1d,3
" And this is  essentially it's in that way, step one of batch norm is  similar to the input standardization.",BatchNorm1d,3
"For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.",BatchNorm1d,3
"However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].",BatchNorm1d,3
"BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice.",BatchNorm1d,3
"The BatchNorm1d normally comes before the ReLU, and the bias is redundant also",BatchNorm1d,3
When evaluating your model use [model.eval() (hyper-link)] before and [model.train() (hyper-link)] after.,BatchNorm1d,3
" But  sometimes people also, nowadays, it's even more common to have  it after the activation.",BatchNorm1d,3
" So  I know, instead of here having it before the activation, I now  have it after the activation in both cases.",BatchNorm1d,3
" And yeah, one little fun memory aid to remember that  is, if you consider this case, so you have batch norm, then  you have the activation and then you have dropout, you may call  it bad, it might be better to have batch norm after the  activation, that's typically a little bit more common.",BatchNorm1d,3
" So let's say you have the Google  search engine, and there's just one user running a query, and  you have a network that has batch norm.",BatchNorm1d,3
"So you have to  normalize, but you don't have a batch of users.",BatchNorm1d,3
"So there are two  ways to deal with that scenario, the easy one would be to use a  global training set mean and variance.",BatchNorm1d,3
So you would compute  these means for the features and the variances for the features  for the whole training set.,BatchNorm1d,3
 That's something you would also  usually do or could do when you compute the input standardization.,BatchNorm1d,3
" The same with batch  norms, instead of using batch norm, one D, which we used  earlier, when we talked about multi layer perceptrons of fully  connected layers, for the convolution layers, we use batch  norm 2d shown here.",BatchNorm1d,3
" So if n is my batch size here, we have an input that  is two dimensional, it is n times m, where let's say, m is  the number of features.",BatchNorm1d,3
" So we had if we  had three features, we had three gammas and three betas.",BatchNorm1d,3
"Now, we  extend this concept here to the two dimensional case where we  compute these four inputs that are four dimensional, right,  because we have now the batch size, we have the channels, we  have the height, and we have the width.",BatchNorm1d,3
"So we compute the batch  norm now, over the number of inputs height and width.",BatchNorm1d,3
"So in  that sense, we, we combine these.",BatchNorm1d,3
It depends on your ordering of dimensions.,BatchNorm1d,3
Add the model.eval() before you fill in your data.,BatchNorm1d,3
This can solve the problem.,BatchNorm1d,3
(I don't think PyTorch has a way to automatically do this for you.),BatchNorm1d,3
" In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.",cross_entropy,3
This is numerically more stable.,cross_entropy,3
" Yeah, conceptually this is a tensor of integers, they can only be 0 or 1, but we, we�re going to be using a cross entropy style loss function, so we're going to actually need to do floating-point calculations on them.",cross_entropy,3
"That's going to be faster to just store them as float in the first place rather than converting backwards and forwards, even though they're conceptually an �int� we're not going to be doing kind of �int style calculations� with them.",cross_entropy,3
"We say the loss is minimized because the lower the loss or cost of error, the better the model.",cross_entropy,3
The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss.,cross_entropy,3
"This terminology is a particularity of PyTorch, as the nn.NLLoss [sic] computes, in fact, the cross entropy but with log probability predictions as inputs where nn.CrossEntropyLoss takes scores (sometimes called logits).",cross_entropy,3
"For a special case of a binary classification, this loss is called binary CE (note that the formula does not change) and for non-binary or multi-class situations the same is called categorical CE (CCE).",cross_entropy,3
"Technically, nn.NLLLoss is the cross entropy between the Dirac distribution, putting all mass on the target, and the predicted distribution given by the log probability inputs.",cross_entropy,3
"In your example you are treating output [0, 0, 0, 1] as probabilities as required by the mathematical definition of cross entropy.",cross_entropy,3
"But PyTorch treats them as outputs, that don’t need to sum to 1, and need to be first converted into probabilities for which it uses the softmax function.",cross_entropy,3
"So H(p, q) becomes:",cross_entropy,3
"Translating the output [0, 0, 0, 1] into probabilities:",cross_entropy,3
"For example, suppose for a specific training instance, the true label is B (out of the possible labels A, B, and C).",cross_entropy,3
The one-hot distribution for this training instance is therefore:,cross_entropy,3
"You can interpret the above true distribution to mean that the training instance has 0% probability of being class A, 100% probability of being class B, and 0% probability of being class C.",cross_entropy,3
"Now, suppose your machine learning algorithm predicts the following probability distribution:",cross_entropy,3
How close is the predicted distribution to the true distribution?,cross_entropy,3
That is what the cross-entropy loss determines.,cross_entropy,3
Where p(x) is the true probability distribution (one-hot) and q(x) is the predicted probability distribution.,cross_entropy,3
"The sum is over the three classes A, B, and C. In this case the loss is 0.479 :",cross_entropy,3
" I'm just  saying that we will use them in practice, we will actually use  the cross entropy in practice.",cross_entropy,3
" But in practice, this is more stable, I  recommend using this one.",cross_entropy,3
" So this particular function, which is identical to MNIST loss plus �.log� jhas a specific name and it's called binary cross entropy, and we used it for the threes vs. sevens problem, to, to decide whether that column is it a three or not, but because we can use broadcasting in PyTorch and element-wise arithmetic, this function when we pass it a whole matrix is going to be applied to every column.",cross_entropy,3
" Random numbers and movies by 5, okay.",cross_entropy,3
"And so to calculate the result for some movie and some user we have to look up the index of the movie in our movie latent factors, the index of the user in our user latent factors and then do a cross product.",cross_entropy,3
"So in other words we would say, Like oh okay, for this particular combination we would have to look up that numbered user over here and that numbered movie over here to get the two appropriate sets of latent factors.",cross_entropy,3
Thanks everybody and I will see you next week or see you in the next lesson whenever you watch it,cross_entropy,3
" What happens   if we're not predicting which of five things  it is but we're just predicting “is it a cat?”   So in that case if you look at this approach  you end up with this formula, which it's   exactly… this is identical to this formula but  in for just two cases, which is you've either:   you either are a cat; or you're not a cat,  right, and so if you're not-a-cat, it's one minus   you-are-a-cat, and same with the probability  you've got the probability you-are-a-cat,   and then not-a-cat is one minus that.",cross_entropy,3
" So here's  this special case of binary cross entropy,   and now our rows represent rows of data, okay,  so each one of these is a different image,   a different prediction, and so for each  one I'm just predicting are-you-a-cat,   and this is the actual, and so the actual  are-you-not-a-cat is just one minus that.",cross_entropy,3
" And so then these are the predictions  that came out of the model,   again we can use soft max or it's  binary equivalent, and so that will   give you a prediction that you're-a-cat, and the  prediction that it's not-a-cat is one minus that.",cross_entropy,3
" So we're just going to decide, all right, we're  just going to decide the first 10 columns,   we're going to decide are the  prediction of what the disease is,   which is the probability of each disease.",cross_entropy,3
"So we can now pass to cross_entropy   the first 10 columns, and the disease target.",cross_entropy,3
"The cross-entropy loss that you give in your question corresponds to the particular case of cross-entropy where your labels are either 1 or 0, which I assume is the case if you're doing basic classification.",cross_entropy,3
"As to why this happens, let's start with the cross-entropy loss for a single training example x:",cross_entropy,3
"where P is the ""true"" distribution and ""Q"" is the distribution that your network has learned.",cross_entropy,3
"The ""true"" distribution P is given by your hard labels, that is, assuming that the true label is t, you'll have:",cross_entropy,3
which means that the loss above becomes,cross_entropy,3
"In your case, it seems that the distribution Q_s is computed from the logits, i.e.",cross_entropy,3
the last layer before a softmax or cost function which outputs a set of scores for each label:,cross_entropy,3
The traditional matrix multiplication is only used when calculating the model hypothesis as seen in the code to multiply x by W:,cross_entropy,3
"It is this measure (i.e., the cross-entropy loss) that is minimized by the optimization function of which Gradient Descent is a popular example to find the best set of parameters for W that will improve the performance of the classifier.",cross_entropy,3
 If we use negative  log likelihood loss or cross entropy in pytorch.,cross_entropy,3
"But  numerically, like stability wise on the computer, the cross  entropy one is more stable.",cross_entropy,3
"So and also for this one, really pay  attention to this one, it's taking the logits as input.",cross_entropy,3
 And the term binary  cross entropy and negative log likelihood are essentially the  same.,cross_entropy,3
So overall it seems preferable to use the global application context when possible.,getBaseContext,3
The benefit of using a ContextWrapper is that it lets you “modify behavior without changing the original Context”.,getBaseContext,3
"For example, if you have an activity called myActivity then can create a View with a different theme than myActivity:",getBaseContext,3
ContextWrapper is really useful to work around device/version specific problems or to apply one-off customizations to components such as Views that require a context.,getBaseContext,3
"So although this (for Activity) and getBaseContext() both give the activity context, they  (a) do not refer to the same object (this != getBaseContext()) and  (b) calling context through this is slightly less efficient, as the calls go through an extra level of indirection.",getBaseContext,3
getContext() and getBaseContext()  is most probably same .these are reffered only current activity which is live.,getBaseContext,3
"I couldn't find really anything about when to use getBaseContext() other than a post from Dianne Hackborn, one of the Google engineers working on the Android SDK:",getBaseContext,3
"Don't use getBaseContext(), just use   the Context you have.",getBaseContext,3
"That was from a post on the [android-developers newsgroup (hyper-link)], you may want to consider asking your question there as well, because a handful of the people working on Android actual monitor that newsgroup and answer questions.",getBaseContext,3
ContextWrapper is really powerful because it lets you override most functions provided by Context including code to access resources (e.g.,getBaseContext,3
"openFileInput(), getString()), interact with other components (e.g.",getBaseContext,3
"sendBroadcast(), registerReceiver()), requests permissions (e.g.",getBaseContext,3
checkCallingOrSelfPermission()) and resolving file system locations (e.g.,getBaseContext,3
getFilesDir()).,getBaseContext,3
Android provides a ContextWrapper class that is created around an existing Context using:,getBaseContext,3
"To further add missing points here, as per the request by Jaskey",onCreate,0
exeqSQL() executes just one SQL statement.,onCreate,0
"You need to split your SQL to individual calls, one for DROP TABLE and another for CREATE TABLE.",onCreate,0
You should not start a nested transaction yourself.,onCreate,0
"SQLite itself does not support nested transactions, and Android's SQLite wrapper only adds partial support for transaction nesting.",onCreate,0
" We simply create an Intent with Intent intent = new Intent() We pass the context, which is ""this"" And we pass the class we want to open Which is Activity2.class And in the next line we write: startActivity() and we pass the intent we just created And that's basically the whole process This part here will open the second activity.",onCreate,0
" Let's test it Okay, as you can see here we are in Activity 1 currently Now we click our button And we changed to Activity2 When we click the back button, we get back to Activity 1 And if you click again, we go back to Activity 2 I hope this was helpful.",onCreate,0
 Take care,onCreate,0
" So  I have in our studio running here, and I'm running   arctic fox, but any recent version should do tap  on new project.",onCreate,0
 And I'm going to pick the empty   activity template.,onCreate,0
" These other ones are useful  sometimes, but they bring in a lot of cruft into   our application, which could be confusing.",onCreate,0
" So  usually, I'll just go with empty activity.",onCreate,0
 Let's   call our application Tippie.,onCreate,0
" The package name,  usually what I'll do is I'll take my domain name   or my email address and flip it backwards.",onCreate,0
" So with the project that we have, right now, we  only have one screen.",onCreate,0
 And that's referred to as   the main activity or the main screen.,onCreate,0
 And that's  the business logic that we're defining right here   in this file called main activity.,onCreate,0
 And the important  line here is line nine setcontentview r dot layout   activity main.,onCreate,0
 So R stands for resources.,onCreate,0
 So now in the onCreate.,onCreate,0
" And   the logs that we care about are the ones  from only our application, which is calm,   calm, rk Pandey Tippi and we also only  care about info level logs because that   is what log.io represents info level  logs.",onCreate,0
" We also only care about logs,   which have this particular tag, which is main  activity.",onCreate,0
 So let's add main activity as a filter.,onCreate,0
" And here as we change the seat bar, you can  see how we get one line of logcat output,   and it represents exactly what is the current  indicator of the seat bar showing.",onCreate,0
" And then   on a string, we know it's going to be a currency  like a decimal, so we'll call to double on it,   in order to turn it into a number that we can  work with.",onCreate,0
 And we'll call this base amount.,onCreate,0
" And then similarly, for the 2%, we want to get  the value of the progress on the seat bar, this   attribute called progress, and that'll be saved  in a variable that I'm going to call tip percent.",onCreate,0
" Let's increase the margin here to be 56   Next, let's drag out one more text you.",onCreate,0
 And we'll call this TV tip description.,onCreate,0
" And then the text should be empty, because  that will depend on the initial tip percent,   which is set programmatically.",onCreate,0
" And then if we bring   it down to zero, we get poor and if we go all the  way up, we should get amazing which we do.",onCreate,0
 One   quick thing want to do is back in activity main.,onCreate,0
 I want to set the font weight of this to be bold.,onCreate,0
 Let's see how you can do that.,onCreate,0
 You can do that by adding the following tag inside of your application node.,onCreate,0
 Note that the getPaymentsClient method takes a walletOptions parameter.,onCreate,0
" Now, you'll note I have made  the database name public, and that's because we're going to use it in our tests in the future.",onCreate,0
 Now I'm just going to add this comment so you know where to go back and add the location entry stuff later.,onCreate,0
" For delete, we have delete, and update and count, too.",onCreate,0
" After you define your SQLiteOpenHelper class, then you have to get to the activity-- your main activity or in whichsoever activity you want to access your database.",onCreate,0
 You have to create the instance of your SQLiteOpenHelper class.,onCreate,0
 Your database is ready.,onCreate,0
 All you have to do is instantiate that OpenHelper class inside your activity and access your database.,onCreate,0
 How do we do that?,onCreate,0
" So inside your main activity, or any other activity, you will be initializing the object of that SQLiteOpenHelper class.",onCreate,0
" And if you remember, we pass the context object there, so you will be passing this there, because activity contains the context object.",onCreate,0
" If the SQL string is invalid, throws SQLException.",onCreate,0
" Inside activity, where you want to use database, create an object of DatabaseHelper (which extends from SQLiteOpenHelper) and  call the method getWritableDatabase() on that DatabaseHelper object.",onCreate,0
 And this is going to give you an object of  SQLiteDatabase.,onCreate,0
 I'll describe how to do this in this project named CreateDatabase.,onCreate,0
" In this version of my project, I moved my XML parser classes to a new package that ends with .xml, and I've created another new package that ends with .db.",onCreate,0
 This is where I'll place my java classes that manage my database.,onCreate,0
 I'll make sure I have the option to create method stubs for Inherited abstract methods and click Finish.,onCreate,0
 I'll come back to these methods in a moment.,onCreate,0
 You can name your database file anything you want.,onCreate,0
" It's common to use a file extension of .db, but it's not required.",onCreate,0
 The database version is required.,onCreate,0
 So I'll get rid of the TODO comment.,onCreate,0
 I'll use the I method.,onCreate,0
" When the onUpgrade method is called, I'll receive arguments named oldVersion and newVersion, and I might want to write some very finely-tuned code that examines those values and upgrades the database in some complex way, but again, I'm going to keep this simple.",onCreate,0
" I'm just going to drop the existing table, the tours table, and then I'll recreate it.",onCreate,0
" I'll call db.execSQL and I'll pass in this explicit SQL Command, DROP TABLE IF EXISTS, and then I'll append to that the name of the table, TABLE_TOURS.",onCreate,0
" So, that's what a basic open helper class looks like.",onCreate,0
" That database type will be SQLiteDatabase, and I'll name that database.",onCreate,0
" I'll save my changes, and now I'm ready to test.",onCreate,0
 These values get passed into the constructor to initialize the database helper.,onCreate,0
onCreate(),onCreate,0
onUpgrade(),onCreate,0
execSQL method.,onCreate,0
"onUpgrade
This method is called when database version is upgraded.",onCreate,0
Two main approaches:,onCreate,0
Uninstall your application from the emulator or device.,onCreate,0
Run the app again.,onCreate,0
"I found some relevant information on [this Github issue (hyper-link)], and by asking a [similar question (hyper-link)].",Adadelta,0
The problem here is that [tf.initialize_all_variables() (hyper-link)] is a misleading name.,Adadelta,0
"It really means ""return an operation that initializes all variables that have already been created (in the default graph)"".",Adadelta,0
"When you call tf.train.AdadeltaOptimizer(...).minimize(), TensorFlow creates additional variables, which are not covered by the init op that you created earlier.",Adadelta,0
Moving the line:,Adadelta,0
...after the construction of the tf.train.AdadeltaOptimizer should make your program work.,Adadelta,0
N.B.,Adadelta,0
"Your program rebuilds the entire network, apart from the variables, on each training step.",Adadelta,0
"This is likely to be very inefficient, and the Adadelta algorithm will not adapt as expected because its state is recreated on each step.",Adadelta,0
I would strongly recommend moving the code from the definition of batch_xs to the creation of the optimizer outside of the two nested for loops.,Adadelta,0
"You should define tf.placeholder() ops for the batch_xs and batch_ys inputs, and use the feed_dict argument to sess.run() to pass in the values returned by mnist.train.next_batch().",Adadelta,0
tf.train.MomentumOptimizer with 0.9 momentum is very standard and works well.,Adadelta,0
The drawback is that you have to find yourself the best learning rate.,Adadelta,0
tf.train.RMSPropOptimizer: the results are less dependent on a good learning rate.,Adadelta,0
[ (hyper-link)],Adadelta,0
The first 10 lines:,Adadelta,0
 Let's see how.,Adadelta,0
 DONE!,Adadelta,0
" In Adam, we will initialize two variables namely 'm' and 'v' as zero.",Adadelta,0
" Now, iteration starts... the first equation is to update initialized m variable and the second equation is to update initialized v variable the only difference between these equations is in the first one we are multiplying one minus beta1 with gradient but in the second equation we are multiplying one minus beta2 with gradient square.",Adadelta,0
" Typically, beta1 is set to 0.9, and beta2 is set to 0.999 as suggested by the author of adam's paper.",Adadelta,0
 Same with the second equation just instead of m_t here it is v_t and in the denominator instead of beta1 it is beta2.,Adadelta,0
" Now, it's time to write the final update rule of Adam... Our learning rate parameter is back because in Adam you have to set a learning rate, unlike AdaDelta in which it is automatically computed.",Adadelta,0
 And then we can compute gradient by dividing bias-corrected m_t or m_t hat with the square root of v_t hat.... again this epsilon is a small value to avoid zero division error.,Adadelta,0
" So, as we have seen in the previous class the  Adagrad algorithm is given something like this,   that at time t you compute the batch gradient;   gradient of the loss function with respect  to the weight vector or with respect to the   parameter vector.",Adadelta,0
" And, then what you do is,  you go on accumulating the squared gradient   or you take the sum of squared gradient of  all the historical gradient values and this   sum of squared gradient is used to scale  the gradient of all individual locations.",Adadelta,0
" So, as a result our upgradation or weight  upgradation algorithm that we have seen earlier   is given by this expression, where if W t is the  parameter vector or weight vector at time t. Then at time t plus 1 we get our updated weight  vector as W t minus eta upon square root of   epsilon I plus r t times g t where, these  operations are actually done element wise;   that means, whenever I rewrite 1 over squared  root of epsilon I plus r t. So, for individual   components this will actually be 1 over square  root of epsilon plus r t i.",Adadelta,0
" So, this is the sum   of the squared gradient; sum of the squares of  the ith component of the gradient vector and you   take the square root of this and times g t; that  means, this will be multiplied by corresponding   ith component of the gradient g t and that  will be added with the ith component of W t. So in fact, your expression will be that if I  go for component wise W t plus 1 ith component   will get W t ith component of this minus eta by  square root of epsilon plus r t i times g t i.",Adadelta,0
" So, this is sum of all the historical values.",Adadelta,0
" And, this is what is actually scaling the  ith component of the gradient vector which   effectively is updating the ith component of  the weight vector or the parameter vector.",Adadelta,0
" So,   what is the effect of the scaling is that if  for certain component say for W i you find   that del L del W i which is nothing, but our  g t i.",Adadelta,0
" And, if this is  small; r t i is small and then the corresponding   learning rate of the ith component of  the parameter vector will also be large.",Adadelta,0
" So, this is how Adagrad algorithm is  adaptively tuning the learning rate   of individual parameters or individual  weight components of the weight vector.",Adadelta,0
" So, we have also seen that what are the  positive points of this Adagrad algorithm.",Adadelta,0
" So, these are  the positive points of the Adagrad algorithm.",Adadelta,0
" And, the negative point is if the function is  non-convex which is quite possible given your   high dimensional space in which the error function  is defined, then while the algorithm proceeds the   trajectory of the weight vector or the parameter  vector may pass through many complex terrains   before coming to a locally convex region.",Adadelta,0
" So,  the moment it comes to locally convex region,   we want that the algorithm will quickly converse  to the minima of this locally convex region.",Adadelta,0
" So, only difference is this r  t is not the cumulative sum of squared gradients,   but this r t is the exponentially decaying  average of the squared gradients.",Adadelta,0
" So, you  use both this first order and second   order moment that becomes a variant of  RMSProp and this is what is known as Adam.",Adadelta,0
" So, in case of Adam you are including both  first and second moments for weight updation   or parameter updation and in addition  you Adam incorporates one more term,   that it tries to correct the bias  to account for initial to zero.",Adadelta,0
" So,   what he said is that when you are  taking exponentially decaying average,   you are initializing the average value  at zero at t equal to zero, right.",Adadelta,0
" So, this corrected  first moment is represented as s t hat,   similarly the corrected second moment which is  r t hat is nothing, but r t by 1 minus beta 2.",Adadelta,0
" So, once given this your weight updation of the  parameter updation rule simply becomes W t plus   1 is equal to W t minus eta times t hat, where  s t hat is the bias corrected first moment upon   square root of epsilon I plus r t hat, where r  t hat is the bias corrected second moment.",Adadelta,0
" So,   you find that this is nothing, but similar to  your RMSProp algorithm where you are incorporating   where, this Adam algorithm incorporates bias  correction operation and it also incorporates the   first moment in the update step.",Adadelta,0
" So, as you see over here this red curve,   the red curve is actually the pure SGD  algorithm or Stochastic Gradient Algorithm,   the blue one gives you the momentum.",Adadelta,0
" So, you   find over here that the SGD which you find that  it is still diver converging whereas, the other algorithms have already come first.",Adadelta,0
"I tend to use Adam, and always in combination with clipped gradients.",Adadelta,0
Adam is a bit more computationally expensive I suppose but gives good results.,Adadelta,0
The learning rate.,Adadelta,0
"As discussed in a relevant [Github thread (hyper-link)], the decay does not affect the variable lr itself, which is used only to store the initial value of the learning rate.",Adadelta,0
"In order to print the decayed value, you need to explicitly compute it yourself and store it in a separate variable lr_with_decay; you can do so by using the following callback:",Adadelta,0
as explained [here (hyper-link)] and [here (hyper-link)].,Adadelta,0
"In fact, the specific code snippet suggested there, i.e.",Adadelta,0
comes directly from the underlying Keras [source code for Adadelta (hyper-link)].,Adadelta,0
"As clear from the inspection of the linked source code, the parameter of interest here for decaying the learning rate is decay, and not rho; despite the term 'decay' used also for describing rho in the [documentation (hyper-link)], it is a different decay not having anything to do with the learning rate:",Adadelta,0
rho: float >= 0.,Adadelta,0
"Adadelta decay factor, corresponding to fraction of gradient to keep at each time step.",Adadelta,0
tf.train.MomentumOptimizer with 0.9 momentum is very standard and works well.,Adadelta,0
The drawback is that you have to find yourself the best learning rate.,Adadelta,0
tf.train.RMSPropOptimizer: the results are less dependent on a good learning rate.,Adadelta,0
[ (hyper-link)],Adadelta,0
The first 10 lines:,Adadelta,0
"The activation of a ReLU is unbounded, making its use in Auto Encoders difficult since your training vectors likely do not have arbitrarily large and unbounded responses!",Adadelta,0
ReLU simply isn't a good fit for that type of network.,Adadelta,0
"You can force a ReLU into an auto encoder by applying some transformation to the output layer, as is [done here (hyper-link)].",Adadelta,0
"However, hey don't discuss the quality of the results in terms of an auto-encoder, but instead only as a pre-training method for classification.",Adadelta,0
So its not clear that its a worth while endeavor for building an auto encoder either.,Adadelta,0
"Basically, you have a set of datapoints along with the targets you're trying to predict in the form",Adadelta,0
where A_k is the k-th piece of training data and b_k is the correct answer.,Adadelta,0
You generally want your machine learning model (e.g.,Adadelta,0
"a classifier or a regressor, for example a neural network or perhaps a linear model) to update its internal parameter",Adadelta,0
"as it reads in the datapoints (A_k,b_k) one at a time, i.e.",Adadelta,0
"you want the model to update x in ""realtime"" as the data comes in.",Adadelta,0
"This is as opposed to something like batch learning, where your model has access to the entire dataset D all at once.",Adadelta,0
"Now, we generally have a notion of ""cost"" --- in linear regression, the cost function that we're trying to minimize is the root mean square (RMS) of the difference between the predicted target value and the actual target value.",Adadelta,0
"Recalling the definition of online linear regression, you have a [stochastic gradient descent (hyper-link)] based update step where the parameters x get updated based on all the data the model has seen so far:",Adadelta,0
"What update rules like AdaGrad and AdaDelta do is to provide a ""nicer"" way to perform updates --- this can mean things like making sure the parameters converge to their optimal value faster, or in the case of AdaDelta, it means that the parameters x ""step closer"" to their optimal values in appropriately-sized steps, with the step size changing based on past performance.",Adadelta,0
"Let's go through your questions, one question at a time:",Adadelta,0
Question 1:,Adadelta,0
The gradient in higher dimensions (i.e.,Adadelta,0
when x is represented by an array) is defined as,Adadelta,0
"where f(x1,x2,...,x_n) is the function you're trying to minimize; in most cases it's the cost function at a single example as a function of x, the model parameters.",Adadelta,0
In other words: take the derivatives of the cost function and then evaluate it at the current parameters xt.,Adadelta,0
Question 2:,Adadelta,0
"Based on my understanding, the RMS of delta-x is defined as",Adadelta,0
where,Adadelta,0
initialized with,Adadelta,0
Question 3:,Adadelta,0
AdaDelta is purely an update rule.,Adadelta,0
The general structure is something like this:,Adadelta,0
where,Adadelta,0
because the point of AdaDelta is to make the learning rate into a dynamic value rather than forcing us to choose an arbitrary value for it at the start.,Adadelta,0
Question 4:,Adadelta,0
"The idea behind ""correcting units with Hessian approximation"" comes from physical intuition, in some sense; that is to say, if you give each variable some sort of unit (length/meters, time/seconds, energy/joules, etc.)",Adadelta,0
then the Hessian has the appropriate units to correct the update term so that the dimensional analysis works out.,Adadelta,0
Question 5:,Adadelta,0
Delta-x is the difference in x after updating.,Adadelta,0
"So if x_i is your parameter before an update and x_{i+1} is your parameter after updating, then Delta-x is (x_{i+1} - x_i).",Adadelta,0
"(∂f/∂x) is the derivative of the function you're trying to minimize (usually in ML, f is the cost function).",Adadelta,0
This note was added to clarify based on a comment from @GMsoF below,onPause,0
There are a lot of lifecycle methods.,onPause,0
You don't need to override all of them.,onPause,0
You only need to override the ones where you need (or want) to customize the behaviour for your activity.,onPause,0
There are a lot of lifecycle methods because different applications have different requirements.,onPause,0
The lifecycle of an Activity is well-documented and well-behaved.,onPause,0
"This allows programmers to put the code exactly where it is needed, based on the particular requirements of the application.",onPause,0
You have asked,onPause,0
 The first thing that we are going to do is we are going to talk about activity lifecycle and how activities transition from and into different states.,onPause,0
" And then, we will be going to learn about those callback methods which are called during the state change.",onPause,0
 We will also cover the saving and restoring of instance state when dealing with Android activities.,onPause,0
 Then it gets started where the app is visible but not its components.,onPause,0
" And then, on onResume state at this state the user will see the activity and it's in the foreground, and users can interact with it.",onPause,0
" Now, if the user gets a pop-up dialog box, the activity will go into the paused state where it is still partially visible to the user.",onPause,0
 Let's understand the activity states through callbacks graph.,onPause,0
" For example, if you've been through the onCreate, onStart, onResume, we have our activity running and then the activity becomes paused.",onPause,0
" For example, someone clicks a button and launches another activity on the top of your activity but, then they hit the back button and they're going back to your activity, onResume is going to be called and your activity is going to be running again.",onPause,0
" Also, if the application is stopped and a user has left the application, completely left that activity, onStop may get called.",onPause,0
" But when the user goes and relaunches your application, onRestart is likely to be called which will restart your application but not recreate your application because it's still in the memory, the Android hasn't actually killed it.",onPause,0
" So, in that case, we are going to go back through that cycle and onResume is going to be called and your activity is going to be running again.",onPause,0
" Now, if the application was actually killed, then the next time it gets launched it's going to restart at the onCreate.",onPause,0
 This is how it is implemented.,onPause,0
" Here in onStart the activity is becoming visible to user, which is followed by onResume if the activity comes to the foreground, or onStop if it becomes hidden.",onPause,0
 And this is how it is implemented.,onPause,0
 It is a transient state which occurs very fast and is always followed by onStart.,onPause,0
 This shows the implementation of onRestart callback.,onPause,0
 onResume where activity will start interacting with the user and has started accepting the user input.,onPause,0
 This is how we implement onResume method.,onPause,0
" onStop is the stopped state which is called when the activity is no longer visible to the user, when new activity is being started and is brought in front of the one, and this one is being destroyed.",onPause,0
" And remember, the system may destroy the activity without calling this method sometimes.",onPause,0
 This is how we implement onDestroy method.,onPause,0
" Now, the second step is where Android started all over again, by calling onCreate, onStart and finally onResume.",onPause,0
" Either using onCreate which is more preferred where it ensures that your user interface, including any saved state, is back up and running as quickly as possible.",onPause,0
" Or another way, by calling or implementing callback onRestoreInstanceState which is called after onStart.",onPause,0
" So, if savedInstanceState doesn't equal null, then we can go ahead.",onPause,0
 I'm going to call my main activity as MainActivity.,onPause,0
" I'm going to override all the methods, and while I'm doing that, inside each method I'm going to put a Toast and a Log.",onPause,0
 Okay.,onPause,0
 For onStop it will be onStop message in logcat and onStop Finished for Toast.,onPause,0
" So, here, now we will try to understand how exactly the application-- our application's main activity works.",onPause,0
" If I can see my activity right on the screen, that means it has already reached to onResume state.",onPause,0
" But to reach it to onResume state, it has to go through first onCreate and onStart.",onPause,0
 See here?,onPause,0
" OnCreate, onStart Finished and onResume Finished.",onPause,0
" So, you see the chronological order here.",onPause,0
" First, the onCreate Toast was called, then onStart, and then onResume.",onPause,0
" In the same manner, you can look at the Android monitor tool also, inside which there will be a logcat, and in the logcat you can see first the onCreate.",onPause,0
" So, Mainactivity onCreate is shown first.",onPause,0
" Next, onStart, then onResume.",onPause,0
" And you can see my application is still there, and if I click it, what will be the lifecycle?",onPause,0
" It will start from onRestart, then onStart, and then onResume.",onPause,0
" So, onRestart, onStart and then, onResume.",onPause,0
" If I can see my applications, that means it's already in onResume.",onPause,0
" In the back, onRestart and onStart  has already finished.",onPause,0
" Same goes with the Toast, so onRestart, onStart and onResume.",onPause,0
 See?,onPause,0
" onCreate, onStart and onResume.",onPause,0
" So, in the same way, you can see Logs, also, onCreate, after onDestroy.",onPause,0
" It never started from onRestart, onCreate, onStart and onResume.",onPause,0
 So that was all about activities lifecycle.,onPause,0
" So just as before, let's make sure we call super.",onPause,0
 And we'll call that id and we'll just give it a default value of 0.,onPause,0
 So we'll say MainActivity.database.noteDao().,onPause,0
" So to do that, we're going to use a third method.",onPause,0
 We're going to use one more called onResume.,onPause,0
 So onResume is going to be called every time an activity is brought to the foreground.,onPause,0
" So when the activity is first created, it's going to call onCreate.",onPause,0
" And then it's going to call onResume, because the activity just entered the foreground.",onPause,0
" Then we could jump off to some other activity and come back and then onResume is going to be called again, because we're back in the foreground.",onPause,0
 So this is a great method to reload our RecyclerView.,onPause,0
" That same sequence, then happens in reverse.",onPause,0
" Finally, there's an onDestroy method, indicating the end of the app lifecycle.",onPause,0
 You can see the code snippet on the bottom of the slide.,onPause,0
 Broadcast receiver is very important component of Android.,onPause,0
To Summaries-,onPause,0
Know more about- [Back Stack (hyper-link)].,onPause,0
actually there will be two circumstances:,onPause,0
----Good to state some notes:,onPause,0
Consider the following case:,onPause,0
[image],onPause,0
Here we see both activities at the same time.,onPause,0
The relevant code can be found in com.android.server.am.ActivityRecord:,onPause,0
"To keep it simple, here is the small info from the android [developers documentation (hyper-link)].",onPause,0
Please refer to [this (hyper-link)] life cycle demo application.,onPause,0
It's really helpful in understanding the Activity lifecycle.,onPause,0
"Also as Raghunandan suggested, you must read [this (hyper-link)].",onPause,0
Added:,onPause,0
Good Luck :),onPause,0
try and write down the equation for the case of batch_size=1 and you'll understand why pytorch is angry with you.,BatchNorm1d,0
How to solve it?,BatchNorm1d,0
Here is the official documentation for PyTorch's BatchNorm:,BatchNorm1d,0
[BatchNorm1D (hyper-link)] and [BatchNorm2D (hyper-link)],BatchNorm1d,0
Ok.,BatchNorm1d,0
I figured it out.,BatchNorm1d,0
So for example:,BatchNorm1d,0
" So Jack so that is we we wanted to do these 2 videos together, basically data normalisation as well as batch normalisation because I think it helps you understand this better, okay thank you.",BatchNorm1d,0
"  All right, let's now talk about how we can use batch norm in  practice.",BatchNorm1d,0
 So how  would that work?,BatchNorm1d,0
" So but yeah, first, let me show you how  batch norm works.",BatchNorm1d,0
 When we use pytorch.,BatchNorm1d,0
" And first, notice  that I'm using again, flatten, that's because I was working  with MNIST and MNIST is on n times one times 28, times 28  dimensional, and flatten will essentially flatten this to a  vector to n times 784 dimensional vector, where n is  the batch size, and then it will work with the fully connected  linear layer here.",BatchNorm1d,0
 This is essentially just the batch norm that we  discussed here in the previous video.,BatchNorm1d,0
 So the one is just to  keep them apart.,BatchNorm1d,0
" I ran it with and without it, I didn't notice any difference in  this case.",BatchNorm1d,0
" Okay, so this is how your batch norm looks like a  full code example can be found here.",BatchNorm1d,0
" But there's really  nothing, nothing really to talk about, because it's just two  lines of code here.",BatchNorm1d,0
" Um, yeah, so I was also just for fun running  some experiments without the bias that I just showed you.",BatchNorm1d,0
 I then also inserted batch norm after the  activation instead of before the activation like here or here.,BatchNorm1d,0
 But I could at least  see there was no overfitting anymore.,BatchNorm1d,0
" So when we look again at our training function  here, this is exactly the same training function that I used  last week in dropout.",BatchNorm1d,0
 And  these running statistics are then used in the evaluation mode  Asian mode when we evaluate our model on new data.,BatchNorm1d,0
" And in inference, you may only have a  single data point, right?",BatchNorm1d,0
" But yeah, here's just like the explanation what's  gonna happen under the hood.",BatchNorm1d,0
" Okay, so yeah, that is how batch  norm works in pytorch.",BatchNorm1d,0
" And in the next video, I want to  briefly go over some very brief, a brief rundown of all the types  of literature that try to explain how batch norm works.",BatchNorm1d,0
 Or at least we will discuss some of the  theories trying to explain why it works.,BatchNorm1d,0
" And just to for simplicity, I will  actually ignore the layer index in the next couple of slides  just so that the other notations are a bit simpler to read.",BatchNorm1d,0
 So this is like what we had in a  previous video as standardization.,BatchNorm1d,0
" Except now, we are  looking at a hidden layer.",BatchNorm1d,0
" So yeah, this is essentially it.",BatchNorm1d,0
" And yeah, this is how batch norm works.",BatchNorm1d,0
" Um, do I have anything  more?",BatchNorm1d,0
" So you can actually  apply an argument, say bias equals false.",BatchNorm1d,0
" Alright, so I think this is all I have about batch norm.",BatchNorm1d,0
 This is  how batch norm works.,BatchNorm1d,0
" In my slides, I actually had also  short or not short, it was actually like 10 slides or so  how we do backpropagation with batch norm, but I promised you  not to torture you with these nitty gritty details and math  mathematical details, because that's not super important  because we use auto grad and practice anyway.",BatchNorm1d,0
 I hope you don't mind.,BatchNorm1d,0
" So in that way, next  video, I will show you batch norm in pytorch.",BatchNorm1d,0
" Now, we will end with something known as Batch Normalization, which is again almost a defacto standard at least in convolutional neural networks.",BatchNorm1d,0
 And this is where we apply batch normalization.,BatchNorm1d,0
 So we specify type equals batch norm.,BatchNorm1d,0
" In the batch normalization layer, this is where we apply our activation function.",BatchNorm1d,0
" Um, the, the data, the patterns in the data is going to be less when you're using batch normalization.",BatchNorm1d,0
"  All right, let's now learn how we can translate familiar  concepts, such as dropout and batch norm to the convolutional  settings.",BatchNorm1d,0
 So these are also called spatial dropout and  batch norm.,BatchNorm1d,0
" And it sounds fancier than it really is,  because you will see it's pretty straightforward.",BatchNorm1d,0
" So usually in the  later stages of the network, these channels represent higher,  higher or larger concepts, as we've talked about in the last  last lecture, where we have a bigger picture concepts like a  channel represents the eye that was detected one the mouth one  the nose and so forth.",BatchNorm1d,0
" So here, the idea is really to drop these  higher order features in that way, dropping entire feature  maps instead of individual pixels.",BatchNorm1d,0
" So essentially, drop  out to D is drop out applied to the channels rather than the  pixel.",BatchNorm1d,0
 It's not very complicated.,BatchNorm1d,0
" So just to briefly recap, I don't want to  explain the batch norm again, because we have a video for  that.",BatchNorm1d,0
 So this is my input dimension.,BatchNorm1d,0
" So we  had usually, let's say a table where we have different  features, let's call them f1, f2, and f3.",BatchNorm1d,0
 So we have three  features here.,BatchNorm1d,0
" And here, this is our batch dimension.",BatchNorm1d,0
 Let's use yellow.,BatchNorm1d,0
 Okay.,BatchNorm1d,0
 So each of these are are fine.,BatchNorm1d,0
"Moreover, you're trying to use ReLU in the form x = nn.ReLU(x).",BatchNorm1d,0
"This is wrong, as nn.ReLU is a layer.",BatchNorm1d,0
This line of code returns you the ReLU layer itself rather than a tensor.,BatchNorm1d,0
"Either define nn.ReLU() layers in your init method, or use F.relu(x) or nn.ReLU()(x).",BatchNorm1d,0
Like so:,BatchNorm1d,0
Tensorflow has has channels in the last axis in convolution.,BatchNorm1d,0
So its batchnorm puts them in axis=-1.,BatchNorm1d,0
One can find the answer inside [torch.nn.Linear documentation (hyper-link)].,BatchNorm1d,0
That's the reason behind your error.,BatchNorm1d,0
I met this problem when I load the model and started to test.,BatchNorm1d,0
Here is the official documentation for PyTorch's BatchNorm:,BatchNorm1d,0
[BatchNorm1D (hyper-link)] and [BatchNorm2D (hyper-link)],BatchNorm1d,0
The tensor must have at least rank 3.,BatchNorm1d,0
whence:,cross_entropy,0
Code:,cross_entropy,0
output:,cross_entropy,0
Read more about torch.nn.functional.cross_entropy loss function from [here (hyper-link)].,cross_entropy,0
[Deep Learning with PyTorch (hyper-link)],cross_entropy,0
Use this formula:,cross_entropy,0
[ (hyper-link)],cross_entropy,0
Here is the above example expressed in Python using Numpy:,cross_entropy,0
"So that is how ""wrong"" or ""far away"" your prediction is from the true distribution.",cross_entropy,0
A machine learning optimizer will attempt to minimize the loss (i.e.,cross_entropy,0
it will try to reduce the loss from 0.479 to 0.0).,cross_entropy,0
We see in the above example that the loss is 0.4797.,cross_entropy,0
"Because we are using the natural log (log base e), the units are in [nats (hyper-link)], so we say that the loss is 0.4797 nats.",cross_entropy,0
"If the log were instead log base 2, then the units are in bits.",cross_entropy,0
See [this page (hyper-link)] for further explanation.,cross_entropy,0
"To gain more intuition on what these loss values reflect, let's look at some extreme examples.",cross_entropy,0
"Again, let's suppose the true (one-hot) distribution is:",cross_entropy,0
Now suppose your machine learning algorithm did a really great job and predicted class B with very high probability:,cross_entropy,0
"When we compute the cross entropy loss, we can see that the loss is tiny, only 0.002:",cross_entropy,0
"At the other extreme, suppose your ML algorithm did a terrible job and predicted class C with high probability instead.",cross_entropy,0
The resulting loss of 6.91 will reflect the larger error.,cross_entropy,0
"Now, what happens in the middle of these two extremes?",cross_entropy,0
Suppose your ML algorithm can't make up its mind and predicts the three classes with nearly equal probability.,cross_entropy,0
The resulting loss is 1.10.,cross_entropy,0
[ (hyper-link)],cross_entropy,0
So to answer your original questions directly:,cross_entropy,0
Is it only a method to describe the loss function?,cross_entropy,0
"Then we can use, for example, gradient descent algorithm to find the
minimum.",cross_entropy,0
Further reading: one of my [other answers (hyper-link)] related to TensorFlow.,cross_entropy,0
 thanks okay great alright thanks everybody  see you next time for our last lesson,cross_entropy,0
"  Yeah, as promised, let me now show you a brief code example  illustrating the concept of the cross entropy in code using  pytorch.",cross_entropy,0
" So I have prepared a notebook I will share it with  you, you can find the link as usual on canvas, or here just  on GitHub.",cross_entropy,0
" Because if we wouldn't make a  mistake, we would get a loss of zero, which would be kind of  boring, I think.",cross_entropy,0
 But I'm trying to explain how  these functions and pytorch work.,cross_entropy,0
" Like I explained in when we go up, I think I had a video  on that in logits and cross entropy.",cross_entropy,0
 And then the  networks are not training.,cross_entropy,0
" So here, again, this is  our net input matrix.",cross_entropy,0
" So here, we give it the net inputs.",cross_entropy,0
" Sorry, should be like this.",cross_entropy,0
" But in this case, we also have a mask that we must apply.",cross_entropy,0
" So what we will do is we will first compute the loss in every node, applying the usual cross_entropy with logits function and calling the appropriate inputs.",cross_entropy,0
 Now we need to use the mask to only use the parts of this loss function for the nodes that belong in that particular set.,cross_entropy,0
 We'll use a TensorFlow GradientTape to record all of the gradients as we go along.,cross_entropy,0
" So we apply our Cora GNN on the features and adjacency to compute the predictions at this step, and we can compute the loss using the masked_softmax_cross_entropy on the predicted logits and the ground truth labels.",cross_entropy,0
" And in particular, we want to compute the loss on the training set, so we pass on the training mask.",cross_entropy,0
 Why is it a tensor of floats?,cross_entropy,0
" We saw in the MNIST Notebook, we didn't need it but we're gonna train faster and more accurately, if we use it, because it's just more, it's going to be better behaved, as we've seen.",cross_entropy,0
" So is the first column, you know, what...",cross_entropy,0
" So this is where it's so cool in PyTorch, we can kind of run, write, one thing and then kind of have it expand to handle higher dimensional tensors, without doing any extra work.",cross_entropy,0
" We don't have to write this ourselves, of course, because PyTorch has one and it's called �F.binary_cross_entropy�.",cross_entropy,0
 We can just use PyTorch�s.,cross_entropy,0
" As we've talked about there's always a equivalent module version so this is exactly the same thing as a module �nn.BCELoss�, and these ones don't include the initial sigmoid, actually.",cross_entropy,0
 If you want to include this initial sigmoid you need F.binary_cross_entropy_with_logits or the equivalent nn.BCEWithLogitsLoss.,cross_entropy,0
 So BCE is binary cross-entropy.,cross_entropy,0
" And so those are two functions, plus two equivalent classes for multi-label or binary problems, and then the equivalent for single label like MNIST and pets is �nll_loss� and �cross_entropy�.",cross_entropy,0
 That's the equivalent of �binary_cross_entropy� and �binary_cross_entropy_with_logits�.,cross_entropy,0
" So these are pretty awful names, I think we can all agree, but it is what it is.",cross_entropy,0
 Sure.,cross_entropy,0
 �In traditional machine learning we perform cross validations and k-fold training to check for variance and bias trade-off.,cross_entropy,0
" Is this common in training deep learning models as well?� So cross-validation is a technique where you don't just let your data set into one training set and one validation set, but you basically do it five or so times, like five training sets, and like five validation sets representing different overlapping subsets, and basically this was this used to be done a lot because people often used to have not enough data to get a good result, and so this way, rather than, kind of, having 20% that you would leave out each time you could just leave out like, 10% each time.",cross_entropy,0
" �.head� will give us the first five rows, and we mentioned just before what it looks like.",cross_entropy,0
" It's not a particularly friendly way to look at it, so what I'm going to do is I'm going to, cross tab it, and so what I've done here is I've grabbed the top, I can�t remember how many it was.",cross_entropy,0
" Well, who cares?",cross_entropy,0
"  Yeah, since we talked so much about logistic regression now, I  thought that might be a good opportunity now to introduce two  terms logits and cross entropy, because that's what I will also  use quite often later in this class.",cross_entropy,0
 And that's because also  it's a very common.,cross_entropy,0
" So there is another  concept that is maybe a little bit confusing, but it's exactly  what we've covered before.",cross_entropy,0
" Of course, you don't  have to know that it's just like a fun tidbit here.",cross_entropy,0
 And this is what we've discussed in the previous videos.,cross_entropy,0
" But this is something we of course, haven't  discussed yet.",cross_entropy,0
" Alright, so in the next video, I will show you a  logistic regression code example.",cross_entropy,0
 And then we will take  a look at this multi category cross entropy.,cross_entropy,0
 All right.,cross_entropy,0
" You know in theory   maybe that could be slightly better because  you're kind of guaranteed that every row is…   appears… four times, you know, effectively.",cross_entropy,0
 It also has a benefit that you could average   those five validation sets because there's no kind  of overlap between them to get a cross validation.,cross_entropy,0
" Personally, I generally don't bother, and  the reason I don't is because this way   I can add and remove models very  easily.",cross_entropy,0
 So in this case we have to use a  different loss function.,cross_entropy,0
" But now that we are having to   pick it out manually I'm going to explain to  you exactly what cross-entropy loss does, okay?",cross_entropy,0
" But in this particular case where we want  to predict one and one thing only, we use softmax.",cross_entropy,0
 The first part of the cross entropy formula...,cross_entropy,0
" The first part of the cross entry formula...  in fact let's look it up, nn.CrossEntropyLoss.",cross_entropy,0
" Now if you've got an eagle  eye, you may have noticed that   I am currently looking at the documentation  for something called “nn.CrossEntropyLoss”   but over here I had something  called “F.cross_entropy()”.",cross_entropy,0
" The functions   live in a… I can’t even remember what the  sub module called, I think it might be like   torch.nn.functional but everybody including the  pytorch official docs just calls it capital-F,   so that's what this capital-F refers to.",cross_entropy,0
 This is not a  particularly great way to see it.,cross_entropy,0
" I prefer to kind   of cross tabulate it, like that, like this.",cross_entropy,0
 This  is the same information.,cross_entropy,0
 So that's why it's particularly full.,cross_entropy,0
" So yeah,   so this is what kind of a collaborative filtering  dataset looks like when we cross tabulate it.",cross_entropy,0
 So how do we fill in this gap?,cross_entropy,0
The difference is small but quite significative.,cross_entropy,0
"softmax_cross_entropy_with_logits takes logits (real numbers without any range limit), passes them through the softmax function and then computes the cross-entropy.",cross_entropy,0
Combining both into one function is to apply some optimizations to improve numerical accuracy.,cross_entropy,0
"The second code just applies cross-entropy directly to y_conv, which seems to be the output of a softmax function.",cross_entropy,0
"This is correct, and both should give similar but not the same results, softmax_cross_entropy_with_logits is superior because of numerical stability.",cross_entropy,0
Just remember to give it logits and not the output of a softmax.,cross_entropy,0
"While it is great that the accepted answer contains lot more info than what is asked, I felt that sharing a few generic thumb rules will make the answer more compact and intuitive:",cross_entropy,0
We have the softmax formula which is an activation for multi-class scenario.,cross_entropy,0
"For binary scenario, same formula is given a special name - sigmoid activation",cross_entropy,0
"Because there are sometimes numerical instabilities (for extreme values) when dealing with logarithmic functions, TF recommends combining the activation layer and the loss layer into one single function.",cross_entropy,0
This combined function is numerically more stable.,cross_entropy,0
TF provides these combined functions and they are suffixed with _with_logits,cross_entropy,0
"With this, let us now approach some situations.",cross_entropy,0
Say there is a simple binary classification problem - Is a cat present or not in the image?,cross_entropy,0
What is the choice of activation and loss function?,cross_entropy,0
It will be a sigmoid activation and a (binary)CE.,cross_entropy,0
So one could use sigmoid_cross_entropy or more preferably sigmoid_cross_entropy_with_logits.,cross_entropy,0
The latter combines the activation and the loss function and is supposed to be numerically stable.,cross_entropy,0
How about a multi-class classification.,cross_entropy,0
Say we want to know if a cat or a dog or a donkey is present in the image.,cross_entropy,0
What is the choice of activation and loss function?,cross_entropy,0
It will be a softmax activation and a (categorical)CE.,cross_entropy,0
So one could use softmax_cross_entropy or more preferably softmax_cross_entropy_with_logits.,cross_entropy,0
We assume that the expected value is one-hot encoded (100 or 010 or 001).,cross_entropy,0
"If (for some weird reason), this is not the case and the expected value is an integer (either 1 or 2 or 3) you could use the 'sparse' counterparts of the above functions.",cross_entropy,0
There could be a third case.,cross_entropy,0
We could have a multi-label classification.,cross_entropy,0
So there could be a dog and a cat in the same image.,cross_entropy,0
How do we handle this?,cross_entropy,0
The trick here is to treat this situation as a multiple binary classification problems - basically cat or no cat / dog or no dog and donkey or no donkey.,cross_entropy,0
Find out the loss for each of the 3 (binary classifications) and then add them up.,cross_entropy,0
So essentially this boils down to using the sigmoid_cross_entropy_with_logits loss.,cross_entropy,0
This answers the 3 specific questions you have asked.,cross_entropy,0
The functions shared above are all that are needed.,cross_entropy,0
You can ignore the tf.contrib family which is deprecated and should not be used.,cross_entropy,0
They are the same.,cross_entropy,0
"if you run that through a softmax, you get:",cross_entropy,0
"The distribution of the true label t, which we've denoted so far by Q is thus given by",cross_entropy,0
and this brings us back to the loss which can be expressed as,cross_entropy,0
which is the one you've given in your problem.,cross_entropy,0
"In your question, x_y is therefore the scores that the network outputs for the correct label that is associated with x.",cross_entropy,0
The code y_ * tf.log(y) in the code block:,cross_entropy,0
performs an element-wise multiplication of the original targets => y_ with the log of the predicted targets => y.,cross_entropy,0
[ (hyper-link)],cross_entropy,0
Most answers already cover getContext() and getApplicationContext() but getBaseContext() is rarely explained.,getBaseContext,0
"I agree that documentation is sparse when it comes to Contexts in Android, but you can piece together a few facts from various sources.",getBaseContext,0
"[This blog post (hyper-link)] on the official Google Android developers blog was written mostly to help address memory leaks, but provides some good information about contexts as well:",getBaseContext,0
"In a regular Android application, you
  usually have two kinds of Context,
  Activity and Application.",getBaseContext,0
Reading the article a little bit further tells about the difference between the two and when you might want to consider using the application Context (Activity.getApplicationContext()) rather than using the Activity context this).,getBaseContext,0
"Basically the Application context is associated with the Application and will always be the same throughout the life cycle of your app, where as the Activity context is associated with the activity and could possibly be destroyed many times as the activity is destroyed during screen orientation changes and such.",getBaseContext,0
"The
Context referred to from inside that ContextWrapper is accessed via
getBaseContext().",getBaseContext,0
this,getBaseContext,0
"The only information that zero padding adds is the condition of border (or near-border) of the features- pixels in the limits of the input, also depending on kernel size.",MaxPool2d,0
"(You can think of it as a ""passe-partout"" in a picture frame)",MaxPool2d,0
Because your assumption that it'll loose information about the exact location is wrong.,MaxPool2d,0
" It's  entitled striving for simplicity, they're all  convolutional network.",MaxPool2d,0
" So here, the authors propose replacing  max pooling with a convolution layer that has a stride of two.",MaxPool2d,0
 And this is sometimes also called strided convolution.,MaxPool2d,0
" So  now on traditional neural network or convolution network,  we have usually a convolution layer with stride equals one.",MaxPool2d,0
" And then we have max pooling, usually two by two max pooling,  also with a stride of two.",MaxPool2d,0
" And then we have a convolutional  layer again, with stride of one, and we continue like that.",MaxPool2d,0
" And  usually, the convolution layers, they preserve the size, because  we have a stride of one.",MaxPool2d,0
" And the max pooling will reduce the size  to fold, it will have the size.",MaxPool2d,0
 So the size that comes out of it  is one half.,MaxPool2d,0
" If we have, let's say a two by two max pooling  with a stride of two, and can maybe also write the stone.",MaxPool2d,0
" So  the kernel size is usually two by two, and this is stride by  two.",MaxPool2d,0
" And this helps us also, you're achieving a little bit of  this location invariance.",MaxPool2d,0
" However, it's not essential to  have that.",MaxPool2d,0
" So you can technically get rid of this, and  then just increase the stride here by two, or here, and you  will have the same effect that you reduce the size by one half.",MaxPool2d,0
 So I don't want to go into too much detail here.,MaxPool2d,0
" So, there might possibly not be too many ah different kind of manifestations which it has to learn up and that is one of the reasons why we are just taking down to this kind of a very simple ah baseline model of a ah segmentation convol convolutional semantic segmentation network .",MaxPool2d,0
" So, what we have in this network is pretty simple, I have a convolution layer ah where the kernel sizes are 3 cross 3, and I have 64 such ah kernels ah taken down for my work Now, I do ah convolution with the stride of one and a padding of one which necessarily means that whatever was the size of my ah x y dimension of my input image .",MaxPool2d,0
" After this convolution, I am going to retain the same x y dimension over there, because I am not changing down either the stride or the padding given over here .",MaxPool2d,0
" The next point is the we put in place a non-linearity, and this is a rectified ah linear unit which is ah put in place for a non-linearity to come down .",MaxPool2d,0
" Following this, we have a max pooling of ah 2 cross 2 kernel size with a stride of two.",MaxPool2d,0
" So, this is a 2 cross 2 max pooling which means that the x and y dimensions are going to get reduced to half of it .",MaxPool2d,0
" Now, in your decoder side what you have is, ah first you have a unpooling over there which skills it up, now once you have that unpooling after that you have your convolution ah a 2D convolution coming down .",MaxPool2d,0
" So, this 2D convolutions job is to convert down 64 channels onto 3 channels .",MaxPool2d,0
" Now, this is three channels is not corresponding to the image channels over there, but this is because you have 3 different classes .",MaxPool2d,0
" Now, if you took down 6 classes then you would have 6 channels, if you take down 10 classes, then you have 10 channels over here the convolutions are still with the kernel size of 3 plus 3 with a stride of one and a padding of one which meant that the same size xy size is preserve.",MaxPool2d,0
" Now, if we look into this kind of a network, so if my input is ah 3 cross 224 cross 224, then ah after this before this max pooling just after the ReLu, it is going to be 64 cross 224 cross 224 .",MaxPool2d,0
" So, we need to use a negative log likelihood loss which is going to calculate itself on a per pixel basis or a per neuron output over this 2 d matrix .",MaxPool2d,0
 And for that there is a separate kind of a function which is called as a nll ah loss 2d .,MaxPool2d,0
" So, this is just to give down that ah whatever comes out on your output side you are just going to operate it on the 2d space.",MaxPool2d,0
" So to compute each of the numbers on the right, we took the max over a two by two regions.",MaxPool2d,0
" So, this is as if you apply a filter size of two because you're taking a two by two regions and you're taking a stride of two.",MaxPool2d,0
" So, these are actually the hyperparameters of max pooling because we start from this filter size.",MaxPool2d,0
 It's like a two by two region that gives you the nine.,MaxPool2d,0
 Let's go through an example with some different hyperparameters.,MaxPool2d,0
" Okay, so this, with this set of hyperparameters f equals three, s equals one gives that output shown [inaudible].",MaxPool2d,0
" Now, so far, I've shown max pooling on a 2D inputs.",MaxPool2d,0
" If you have a 3D input, then the outputs will have the same dimension.",MaxPool2d,0
 It can happen at any given point.,MaxPool2d,0
 So we could max pool before we apply the convolution.,MaxPool2d,0
" But typically speaking, the convolutional filters are applied.",MaxPool2d,0
" This ultimately, right there-- the x++, y++-- that's the stride.",MaxPool2d,0
" So I could say x+ equals stride, y+ equal stride.",MaxPool2d,0
 And set the stride equal to one.,MaxPool2d,0
 So that's what was happening here.,MaxPool2d,0
" To describe this, I'm going to start with an 8 by 8 image.",MaxPool2d,0
 And I'm going to do max pooling with a 2 by 2 max pooling with a stride of 2.,MaxPool2d,0
 So there are no weights.,MaxPool2d,0
 The highest one is 10.,MaxPool2d,0
" And so the max pooling algorithm takes these little neighborhoods, 2 by 2 max pooling, skips, goes from one to the other with a stride of 2.",MaxPool2d,0
 I could have just moved these neighborhoods just by one or by even a larger amount.,MaxPool2d,0
 And I'm going to create a global variable for stride.,MaxPool2d,0
" But this stride is only referring to the pooling process because then I can say, create image dim divided by stride, dimensions divided by stride.",MaxPool2d,0
" Just to add some comments for a moment, this is convolutional layer.",MaxPool2d,0
 And I don't want to end up here.,MaxPool2d,0
 So this is plus equal stride.,MaxPool2d,0
 And this is plus equal stride.,MaxPool2d,0
 I can do the same exact thing.,MaxPool2d,0
 The first layer is a convolutional layer.,MaxPool2d,0
" And I'm writing 2D because a lot of times in a machine learning library, you can have convolutions in different dimensions.",MaxPool2d,0
 And we're working with a two-dimensional convolution here.,MaxPool2d,0
 And then the output there is another image.,MaxPool2d,0
" So we take this first image, pass it through a bunch of filters, max pool them and a whole bunch of other images that, if I'm using a stride of 2, now have half the resolution as the original image.",MaxPool2d,0
 So the question becomes what to do next.,MaxPool2d,0
" And to this layer, we're arbitrarily setting the filter value equal to 32.",MaxPool2d,0
 So this first convolutional layer will have 32 filters with a kernel size of three by three.,MaxPool2d,0
" So the choice of 32 is pretty arbitrary, but the kernel size of three by three is a very common choice for image data.",MaxPool2d,0
" Now, this first column to D layer will be followed by the popular relu activation function.",MaxPool2d,0
 And we are specifying padding equals Same here.,MaxPool2d,0
" And this just means that our images will have zero padding, padding the outside so that the dimensionality of the images aren't reduced after the convolution operations.",MaxPool2d,0
" So lastly, for our first layer only, we specify the input shape of the data.",MaxPool2d,0
" And recall we touched on this parameter previously, you can think of this as kind of creating an implicit input layer for our model.",MaxPool2d,0
" This comp 2d is actually our first hidden layer, the input layer is made up of the input data itself.",MaxPool2d,0
" And so we just need to tell the model the shape of the input data, which in our case is going to be 224.",MaxPool2d,0
" Since these images are an RGB format, we have three color channels, so we specified this input shape.",MaxPool2d,0
" So then we follow our first convolutional layer with a max pooling layer, where we're setting our pool size to two by two and our strides by two.",MaxPool2d,0
" So after this max pooling layer, we're then adding a another convolutional layer, that looks pretty much exactly the same as the first one except for four, we're not including the input shape parameter, since we only specify that for our first hidden layer, and we are specifying the filters to be 64 here instead of 32.",MaxPool2d,0
 I am taking him three by three by three one.,MaxPool2d,0
" It's over Let's see our output is you know Three by three even I you know, I have given an equation for the output the output, you know, I know that I know what is the size of the output it is represented by the equation n Minus F plus one and is the you know image size F is the kernel size and plus one So this equation we discussed at that time you remember that so let's assume that this is the map Obtained from the first term, ah, you know a kernel first filtered So, let's see.",MaxPool2d,0
" This is like, let's assume that this is you know 0.5 This is minus 1 This is a one point zero zero.",MaxPool2d,0
 So in pooling layer vish ring this image stack By using here.,MaxPool2d,0
" Also, we use a filter or kernel the filter.",MaxPool2d,0
 We call it as the max pooling filter Cv the filter we use here.,MaxPool2d,0
 It is a max pooling filter This is the filter we use here by using a max bowling filter Normally the size of the max bowling filter.,MaxPool2d,0
" We take it as 2 by 2 Or 3 by 3 This is the size of the max pooling filter C and we take a stride stride of - the stride we are taking here it is 2 Stride means jump, you know when we discuss about the convolution operation The filter has to jump there we have taken a stride of 1 so that's why if the formula we got it is, you know Y n minus F plus 1 but if we are considering the stride as well or Padding this formula will be modified.",MaxPool2d,0
 We will discuss this later So here the stride we are taking it as 2 so then we will move this you know max bowling filter in our across our filtered image and after that from each Window we take the maximum intensity or the maximum value from each window so what we are going to do I will explain so here we considered a max pooling filter of Size two by two and the stride is two and we are moving this max pooling filter across the map After we got it from the rail operation and we take the maximum intensity value Or maximum pixel value from that so I will explain you what exactly it is.,MaxPool2d,0
" So This is the let us assume that this is the you know Output you got it from the this is a seven by seven map I am going to you know, draw here 1 2 3 4 5 6 7 this is up 7 1 1 2 3 4 5 6 7 1 2 3 4 5 Six seven, so this map doesn't contain any negative value.",MaxPool2d,0
 So this is some random value some arbitrary values I have taken so what exactly I am trying to explain here What is the max pulling filter do so here the max pulling filter?,MaxPool2d,0
 What it is doing here it is our max pooling filter is of I am taking it as f size of two by two and astride of two so Here the size of the filter is two by two and stride is to the jump is 2 Usually we take the stride as 2 so 2 by 2 beans.,MaxPool2d,0
" Our max pooling filter will be Like this, this is our first we place like this.",MaxPool2d,0
 Then the maximum value is - I Put it here.,MaxPool2d,0
" Then the next step, you know, I will jump to know this location the position of the map, you know, the Max bowling kernel should be this so from this Also, I will take a 1/2 means the maximum value is 2 like this.",MaxPool2d,0
" I am shrinking the input Image, you know to a you know a smaller size after that I may be getting you know, I may be getting up, you know, one two three It would be I believe it would be one two, three a four it would be a four by four, you know Size the size of the you know output will be four by four.",MaxPool2d,0
From [https://keras.io/layers/convolutional/ (hyper-link)],MaxPool2d,0
"""same"" results in padding the input such that the output has the same
  length as the original input.",MaxPool2d,0
From [https://keras.io/layers/pooling/ (hyper-link)],MaxPool2d,0
"pool_size: integer or tuple of 2 integers, factors by which to downscale (vertical, horizontal).",MaxPool2d,0
"(2, 2) will halve the input in both spatial dimension.",MaxPool2d,0
"If only one integer is specified, the same window length will be used for both dimensions.",MaxPool2d,0
"So, first let's start by asking why use padding at all?",MaxPool2d,0
"In the convolutional kernel context it is important since we don't want to miss each pixel being at the ""center"" of the kernel.",MaxPool2d,0
There could be important behavior at the edges/corners of the image that a kernel is looking for.,MaxPool2d,0
So we pad around the edges for Conv2D and as a result it returns the same size output as the input.,MaxPool2d,0
"However, in the case of the MaxPooling2D layer we are padding for similar reasons, but the stride size is affected by your choice of pooling size.",MaxPool2d,0
"Since your pooling size is 2, your image will be halved each time you go through a pooling layer.",MaxPool2d,0
So in the case of your tutorial example; your image dimensions will go from 28->14->7->4 with each arrow representing the pooling layer.,MaxPool2d,0
Imagine we are taking the maximum value in a 2x2 region of an image.,MaxPool2d,0
"The image is pre-convolved, though it doesn't matter for the purpose of this explanation.",MaxPool2d,0
"No matter where exactly in a 2x2 region the maximum value resides, there will be 3 possible 1-pixel translations of the image that result in the maximum value remaining in that particular 2x2 region.",MaxPool2d,0
"Of course an even greater value may be brought from a neighbouring region, but that's beside the point.",MaxPool2d,0
The point is you get some translation invariance.,MaxPool2d,0
"With a 3x3 region it gets more complex, as the number of 1-pixel translations that keep the maximum value within the region depends on where exactly in the region that maximum value resides.",MaxPool2d,0
The 5 translations they mention correspond to a location in the middle of an edge in a 3x3 pixel block.,MaxPool2d,0
"A corner location will give 3 translations, while the center one will give all 8.",MaxPool2d,0
"For a single slice at an arbitrary index, you have a simple image of NxN dimensions.",MaxPool2d,0
"You defined your filter size 2, and stride 2.",MaxPool2d,0
It's not clear to me  why you're using transpose of the max row for a single element in the pool.,MaxPool2d,0
Convolutional Neural Networks do a great job in dealing with high dimensional data.,MaxPool2d,0
Restricting the number of weights only to kernels weights makes learning easier due to invariance properties of images or sound.,MaxPool2d,0
Sum pooling works in a similiar manner - by taking the sum of inputs instead of it's maximum.,MaxPool2d,0
The conceptual difference between these approaches lies in the sort of invariance which they are able to catch.,MaxPool2d,0
Max pooling is sensitive to existence of some pattern in pooled region.,MaxPool2d,0
Sum pooling (which is proportional to Mean pooling) measures the mean value of existence of a pattern in a given region.,MaxPool2d,0
UPDATE:,MaxPool2d,0
The subregions for Sum pooling / Mean pooling are set exactly the same as for Max pooling but instead of using max function you use sum / mean.,MaxPool2d,0
You can read about [here (hyper-link)] in the paragraph about pooling.,MaxPool2d,0
" 
skimage.measure.block_reduce(activation, block_size=(1, 1, 2, 2), func=np.max)",MaxPool2d,0
Your backpropagation for mean pooling is not fully correct.,MaxPool2d,0
You should devide delta by number of pooled cells (4 in your case).,MaxPool2d,0
See equation on slide 11 at [http://www.slideshare.net/kuwajima/cnnbp (hyper-link)],MaxPool2d,0
To propagate max pooling you need to assign delta only to cell with highest value in forward pass.,MaxPool2d,0
"Hence, during the forward pass of a pooling layer it is common to keep track of the index of the max activation (sometimes also called the switches) so that gradient routing is efficient during backpropagation.",MaxPool2d,0
See [http://cs231n.github.io/convolutional-networks/#pool (hyper-link)],MaxPool2d,0
Very inefficient way to implement this:,MaxPool2d,0
I'll give an example to make it clearer:,MaxPool2d,0
"x: input image of shape [2, 3], 1 channel",MaxPool2d,0
"valid_pad: max pool with 2x2 kernel, stride 2 and VALID padding.",MaxPool2d,0
"same_pad: max pool with 2x2 kernel, stride 2 and SAME padding (this is the classic way to go)",MaxPool2d,0
The output shapes are:,MaxPool2d,0
"valid_pad: here, no padding so the output shape is [1, 1]",MaxPool2d,0
"same_pad: here, we pad the image to the shape [2, 4] (with -inf and then apply max pool), so the output shape is [1, 2]",MaxPool2d,0
"According to the [official documentation (hyper-link)], any optimizer can have optional arguments clipnorm and clipvalue.",RMSprop,0
"If clipnorm provided, gradient will be clipped whenever gradient norm exceeds the threshold.",RMSprop,0
Seems that someone have sorted out (2018) the question (2017).,RMSprop,0
Link to the paper [[https://arxiv.org/pdf/1711.05101.pdf] (hyper-link)] and some intro:,RMSprop,0
"In this paper, we show that a
  major factor of the poor generalization of the most popular
  adaptive gradient method, Adam, is due to the fact that L2
  regularization is not nearly as effective for it as for SGD.",RMSprop,0
L2 regularization and weight decay are not identical.,RMSprop,0
"Contrary to common belief, the two techniques are not
  equivalent.",RMSprop,0
"For SGD, they can be made equivalent by
  a reparameterization of the weight decay factor based
  on the learning rate; this is not the case for Adam.",RMSprop,0
"In
  particular, when combined with adaptive gradients, L2
  regularization leads to weights with large gradients
  being regularized less than they would be when using
  weight decay.",RMSprop,0
"The documentation you refer to explicitly mentions:

This implementation of RMSProp uses plain momentum, not Nesterov momentum.",RMSprop,0
AFAIK there is no built-in implementation for Nesterov momentum in RMSProp.,RMSprop,0
You can of course adjust the function according to your own needs.,RMSprop,0
"As @xolodec said, g_t is the gradient.",RMSprop,0
" All we need to do is start off by importing the stuff that we need, so we're going to import the Keras library and some specific modules from it.",RMSprop,0
" We have the MNIST dataset here that we're going to experiment with; the Sequential model which is a very quick way of assembling the layers of a neural network; we're going to import the Dense and Dropout layers as well, so we can actually add some new things onto this neural network to make it even better and prevent overfitting; and we will import the RMSprop optimizer, which is what we're going to use for our gradient descent.",RMSprop,0
" Shift Enter, and you can see we've already loaded up Keras just by importing those things it's using tensorflow as the back end.",RMSprop,0
 All right.,RMSprop,0
" And you can probably improve on this even more, so again, as before with TensorFlow I want you to go back and see if you can actually improve on these results, try using a different optimizer than RMSprop, try, you know, different topologies and the beauty with Keras is that it's a lot easier to try those different topologies now, right?",RMSprop,0
" Keras actually comes in this documentation with an example of using MNIST and this is the actual topology that they use in their example, so go back and give that a try and see if it's actually any better or not, see if you can improve upon things.",RMSprop,0
" This algorithm is called Adagrad if I apply this algorithm, then what how does the situation change, fine?",RMSprop,0
" So, this is what gradient descent momentum and NAG do.",RMSprop,0
" Now, at least the difference between momentum and NAG should be clear.",RMSprop,0
 NAG blue curve is inside the red curve right?,RMSprop,0
" So, what happens because of the aggressive killing, is the frequent parameters, they start receiving fewer updates.",RMSprop,0
" Now, this is what RMSProp does, ok.",RMSprop,0
" I want you to stare at this for a minute, ok.",RMSprop,0
 Yeah some people as just lazy fine.,RMSprop,0
 There is everything that you learned so far and my everything yeah.,RMSprop,0
" From there you are now focusing on the learning rates, but there were other things which you are doing earlier.",RMSprop,0
" Can you bring those back, add momentum.",RMSprop,0
 How many of you say add momentum as if I can just added.,RMSprop,0
" You are right actually, ok.",RMSprop,0
" So, let us see what we can do.",RMSprop,0
" So, it does everything that RMSProp does.",RMSprop,0
" So, what is this term doing?",RMSprop,0
" Just taking a moving average of your gradients, ok.",RMSprop,0
" The same analogy that I am going to phoenix market city, I am just taking all my history into account, ok and vt is again a cumulative history.",RMSprop,0
" This is the same as what was happening in RMSProp, right where you get lost.",RMSprop,0
 Is this fine?,RMSprop,0
 Does it make sense?,RMSprop,0
 This is same as momentum base gradient descent.,RMSprop,0
 How many of you get that?,RMSprop,0
" Ok and now, this quantity there is nothing new.",RMSprop,0
" So, just a combination of these two, one is take care of the learning rate and the other is use a cumulative history.",RMSprop,0
" So, what is happening, it is taking u-turns, right?",RMSprop,0
" So, again whatever happens because of momentum, it is happening in this case also and then, finally it will converge again.",RMSprop,0
 Let me be clear that in this case now it should be very clear.,RMSprop,0
" Even if you have not read the assignment, you should just tell me based on whatever you have learned you have gradient descent.",RMSprop,0
 Momentum.,RMSprop,0
 Momentum.,RMSprop,0
 Nag RMSProp.,RMSprop,0
 Adagrad.,RMSprop,0
" So,   in our previous class we have talked  about the Adagrad algorithm in particular.",RMSprop,0
" So, as we have seen in the previous class the  Adagrad algorithm is given something like this,   that at time t you compute the batch gradient;   gradient of the loss function with respect  to the weight vector or with respect to the   parameter vector.",RMSprop,0
" Now, let us see that what is  this exponentially decaying squared gradient.",RMSprop,0
" So, all  the previous sample values as I go   as I compute say s 50 the previous sample  values the effect of s 1 s 2 s 3 and so on,   that will go on reducing exponentially.",RMSprop,0
" And,  that is the advantage of taking the exponentially   decaying average of the square gradients and this  is what is used in case of RMSProp problem.",RMSprop,0
" So, we have a closely related algorithm known as  AdaDelta.",RMSprop,0
" So, what AdaDelta does is in case of   RMSProp you are taking the exponentially decaying  average of the squared gradient; AdaDelta instead   of taking the exponentially decaying average of  squared gradient it computes the moving window   average.",RMSprop,0
" So, you can take a more window size of  say W. So, when you compute v t, v t is computed   over a past window size of W. So, if I take the  window size W is equal to say 5, in that case in   order to compute say v 10 it will take the first  5; that means, it will take v 10 v 9 v 8 v 7 and   v 6 or the say s 6 s 7 s 8 s 9 and s 10.",RMSprop,0
" So, you are computing average over past   samples which are within this window size of W.  So, this is what is moving window average.",RMSprop,0
" So,   you find that this RMSProp which takes  expose exponentially decaying average,   the AdaDelta takes a moving window average of  the squared gradients.",RMSprop,0
" So, that is the only   difference between RMSProp and AdaDelta.",RMSprop,0
" And  in fact, both these algorithms were proposed   almost simultaneously, but independently, and  both of them gives almost similar performance.",RMSprop,0
" So, this is what is you are RMSProp algorithm,  you can also improvise upon RMSProp algorithm   with an Nesterov of momentum term.",RMSprop,0
" So,  as we have seen that in case of Nesterov   of accelerated gradient technique you take  the gradient not at location W t, but you   take the gradient at a loop ahead position.",RMSprop,0
" So,  assuming that your previous momentum term is v,   you are looking ahead at location W t plus alpha  times v and you are taking the gradient at that   location W t plus alpha times v instead  of computing the gradient at location W t. So, that is what your Nesterov accelerated  gradient is.",RMSprop,0
" So, you are instead of computing   the gradient at location W t, if I take  the gradient at location W delta where,   W delta is the position or to loop ahead position.",RMSprop,0
" And, rest of the algorithms remains the same; so,   it is as before you are taking the exponentially  decaying average of the sum of squared gradients,   exponentially decaying average of the squared  gradients and then your update rule remains as   before ok.",RMSprop,0
" So, this is what is RMSProp, which is  part that improvised with Nesterov of momentum.",RMSprop,0
" So, given this algorithm now the  other algorithm that we said that   we will be talking about is what is  known as Adam or adaptive moments.",RMSprop,0
" So, what is this Adam algorithm?",RMSprop,0
 Adam algorithm  you can consider this to be variant of the   combination of RMSProp and momentum.,RMSprop,0
" So, here we can incorporate both  the first order momentum and the second order   momentum.",RMSprop,0
" Second order momentum is nothing, but  the sum of squared gradients or exponentially   decaying average of the squared gradients as in  case of RMSProp which is used for scaling the   learning rate in individual directions.",RMSprop,0
" And,  along with that if we add the momentum term,   where the momentum will be scaled according to the  square root of the exponentially decaying average.",RMSprop,0
" So, if I use both this first order momentum  and second order momentum because sum of   squared gradients is nothing, but average of  the squared gradients is nothing, but equivalent to your second order momentum.",RMSprop,0
" So, you  use both this first order and second   order moment that becomes a variant of  RMSProp and this is what is known as Adam.",RMSprop,0
" So, in case of Adam you are including both  first and second moments for weight updation   or parameter updation and in addition  you Adam incorporates one more term,   that it tries to correct the bias  to account for initial to zero.",RMSprop,0
" So, once given this your weight updation of the  parameter updation rule simply becomes W t plus   1 is equal to W t minus eta times t hat, where  s t hat is the bias corrected first moment upon   square root of epsilon I plus r t hat, where r  t hat is the bias corrected second moment.",RMSprop,0
" So,   you find that this is nothing, but similar to  your RMSProp algorithm where you are incorporating   where, this Adam algorithm incorporates bias  correction operation and it also incorporates the   first moment in the update step.",RMSprop,0
" So, this is eta  times s t hat where, s t hat is the first moment   of the gradients.",RMSprop,0
" So, as you see over here this red curve,   the red curve is actually the pure SGD  algorithm or Stochastic Gradient Algorithm,   the blue one gives you the momentum.",RMSprop,0
" Then you  have NAG Nesterov accelerated gradient operation,   then you have Adagrad, then you have a  Adadelta, then you have RMSProp.",RMSprop,0
" So, you   find over here that the SGD which you find that  it is still diver converging whereas, the other algorithms have already come first.",RMSprop,0
" So, it moves like this.",RMSprop,0
" And, as  it moves down this hilly terrain,   it gains momentum.",RMSprop,0
" And subsequently the  ball reaches this minimum point which is   the plateau and because it has a momentum,  it will overshoot the plateau and will start   moving in the other direction.",RMSprop,0
 And you find that   you need large number of iterations because  the gradient in one direction or gradient in   that direction of W 2 is much larger than  the gradient in the direction of W 1.,RMSprop,0
" So,   I can avoid this problem if I  bring in a concept of momentum.",RMSprop,0
" So,   the concept is something like this that  again I assume the ball is over here,   its gradient force acting on this, so  the ball comes somewhere over here.",RMSprop,0
" Now, at this location the gradient working on this  ball may be in this direction, whereas if I also   consider the component of the momentum that is a  force due to momentum of the ball which is in this   direction.",RMSprop,0
" So, if I consider both this gradient  force as well as this momentum force to find out   what will be the net force which is acting  in this ball.",RMSprop,0
" So, if you remember the gradient descent algorithm   works like this, that I want to find out  the weight the parameter at time t plus 1,   and this parameter at time t plus 1 is obtained  from the parameter of the weight at time t minus   gradient of the loss function, where loss is  a function of the parameter or the function of   the weight vector W. And this gradient has to  be taken with respect to my parameter vector   which is W. This is what is my normal  stochastic gradient descent algorithm.",RMSprop,0
 And now what I do is in addition to this  I want to add a momentum term right.,RMSprop,0
" So,   I assume that at time instant t that is W t is  somewhere over here this is the location of W t.   And it comes to W t from location W t minus 1 and  with a gradient vector which is v t minus 1.",RMSprop,0
" And   at this location W t, I have  two forces acting on it once   is the gradient force which is this  gradient of L W with respect to W,   and other one I consider is the momentum  force which is some gamma times v t minus 1.",RMSprop,0
" So, what I am adding is I am adding this  momentum term to this gradient force.",RMSprop,0
" So,   as a result, W t minus 1 the weight updation  equation will now be W t plus 1 is equal to   W t minus this gradient term which I had before  it remains as it is L W t. In addition to this,   what I am adding over here is the momentum  term which is nu v t minus 1.",RMSprop,0
" So, this is   the momentum term, and this is the gradient term.",RMSprop,0
" So, under the influence of these two now the net   update direction of the weight vector will be  in this direction instead of in this direction,   and that is how your gradient descent  approach with momentum improves the   rate of convergence or makes this back  propagation learning more efficient.",RMSprop,0
" So, when I  consider this momentum term the gradient descent   algorithm or a stochastic gradient descent  algorithm becomes much more efficient.",RMSprop,0
" Now,   there has been another modification on  this momentum approach or the stochastic   gradient descent or with momentum that has been  suggested which improves this momentum optimizer   to a certain extent and that is what is Nesterov  accelerated gradient approach or known as NAG.",RMSprop,0
" So, what is the NAG?",RMSprop,0
" You find that in every case  with momentum, what I am assuming is same i at   time t, so I have W t somewhere over here, I come  to W t with a momentum with a previous force which   is equal to v t minus 1.",RMSprop,0
" And due to this I have  a momentum term which is nu times v t minus 1,   I have the gradient at this location which  is delta L. And under influence of these two,   the net displacement or the weight  updation which will be done parameter   updation will be given by the sum of these  two vectors which is in this direction.",RMSprop,0
" Now, how this acceleration can be done or  the gradient can be accelerated is this,   if I know beforehand because I know what  is my momentum.",RMSprop,0
 And I assume that due to   a effect of this momentum what will be my  position after effect of this momentum that   is where I am going to lead over here.,RMSprop,0
" And if  I know that what is going to be my position   because of the momentum effect in future,  then instead of considering the gradient at   this location I can find out what will be  the gradient at this particular location.",RMSprop,0
" So,   if I can have my step size in the vertical  direction lower than the step size in the   horizontal direction, the learning will be  much more efficient, but which is not done   with this momentum based optimizer or even this  Nesterov of accelerated gradient approach.",RMSprop,0
" So,   that is another problem which is faced  in momentum optimizer as well as in NAG.",RMSprop,0
 You also find that the high dimensional and  mostly non convex nature of the loss function   that may look to different sensitivity  or different dimensions.,RMSprop,0
"Sometimes, authors make clear that the subject expression is just a loose description, e.g.",RMSprop,0
in the (highly recommended) [Overview of gradient descent optimization algorithms (hyper-link)] (emphasis added):,RMSprop,0
"Adam also keeps an exponentially decaying average of past gradients mt, similar to momentum.",RMSprop,0
"or in [Stanford CS231n: CNNs for Visual Recognition (hyper-link)] (again, emphasis added):",RMSprop,0
"That said, it's true that some other frameworks actually include a momentum parameter for Adam, but this is actually the beta1 parameter; here is [CNTK (hyper-link)]:",RMSprop,0
"momentum (float, list, output of momentum_schedule()) – momentum schedule.",RMSprop,0
Note that this is the beta1 parameter in the Adam paper.,RMSprop,0
"For additional information, please refer to the [this CNTK Wiki article (hyper-link)].",RMSprop,0
"So, don't take this too literally, and don't loose your sleep over it.",RMSprop,0
The bug is here:,RMSprop,0
"Since the reference to weights is shared between the two routines, every time Adam.minimize_trace and RMSprop.minimize_trace run, they modify the same array.",RMSprop,0
"Since the path is derived from the array, the path on both become the same.",RMSprop,0
"If you copy the array before passing it to the two constructors, it should work as expected.",RMSprop,0
"Wording (however unfortunate) of ""leaky"" refers to the fact how much of the previous estimate ""leaks"" to the current one, since",RMSprop,0
Adam (adaptive moments) Calls the 1st and 2nd power of the gradient moments and uses a momentum-like decay on both moments.,RMSprop,0
"In addition, it uses bias correction to avoid initial instabilities of the moments.",RMSprop,0
How to chose one?,RMSprop,0
Depends on the problem we are trying to solve.,RMSprop,0
It's more empirical than mathematical,RMSprop,0
Seems that someone have sorted out (2018) the question (2017).,RMSprop,0
Link to the paper [[https://arxiv.org/pdf/1711.05101.pdf] (hyper-link)] and some intro:,RMSprop,0
"In this paper, we show that a
  major factor of the poor generalization of the most popular
  adaptive gradient method, Adam, is due to the fact that L2
  regularization is not nearly as effective for it as for SGD.",RMSprop,0
L2 regularization and weight decay are not identical.,RMSprop,0
"Contrary to common belief, the two techniques are not
  equivalent.",RMSprop,0
"For SGD, they can be made equivalent by
  a reparameterization of the weight decay factor based
  on the learning rate; this is not the case for Adam.",RMSprop,0
"In
  particular, when combined with adaptive gradients, L2
  regularization leads to weights with large gradients
  being regularized less than they would be when using
  weight decay.",RMSprop,0
And you can refer to [[4] (hyper-link)] to see what benefits ReLU provides.,Tanh,0
Accordining to about 2 years machine learning experience.,Tanh,0
I want to share some stratrgies the most paper used and my experience about computer vision.,Tanh,0
Normalizing well could get better performance and converge quickly.,Tanh,0
"Most of time we will subtract mean value to make input mean to be zero to prevent weights change same directions so that converge slowly [[5] (hyper-link)] .Recently google also points that phenomenon as internal covariate shift out when training deep learning, and they proposed batch normalization [[6] (hyper-link)] so as to normalize each vector having zero mean and unit variance.",Tanh,0
More training data could generize feature space well and prevent overfitting.,Tanh,0
"In computer vision if training data is not enough, most of used skill to increase training dataset is data argumentation and synthesis training data.",Tanh,0
ReLU nonlinear acitivation worked better and performed state-of-art results in deep learning and MLP.,Tanh,0
"Moreover, it has some benefits e.g.",Tanh,0
simple to implementation and cheaper computation in back-propagation to efficiently train more deep neural net.,Tanh,0
"However, ReLU will get zero gradient and do not train when the unit is zero active.",Tanh,0
Hence some modified ReLUs are proposed e.g.,Tanh,0
"Leaky ReLU, and Noise ReLU, and most popular method is PReLU [[7] (hyper-link)] proposed by Microsoft which generalized the traditional recitifed unit.",Tanh,0
choose large initial learning rate if it will not oscillate or diverge so as to find a better global minimum.,Tanh,0
shuffling data,Tanh,0
"In deep learning the ReLU has become the activation function of choice because the math is much simpler from sigmoid activation functions such as tanh or logit, especially if you have many layers.",Tanh,0
"To assign weights using backpropagation, you normally calculate the gradient of the loss function and apply the chain rule for hidden layers, meaning you need the derivative of the activation functions.",Tanh,0
"ReLU is a ramp function where you have a flat part where the derivative is 0, and a skewed part where the derivative is 1.",Tanh,0
This makes the math really easy.,Tanh,0
"If you use the hyperbolic tangent you might run into the fading gradient problem, meaning if x is smaller than -2 or bigger than 2, the derivative gets really small and your network might not converge, or you might end up having a dead neuron that does not fire anymore.",Tanh,0
 The Neural Network is the result of combining multiple artificial neurons.,Tanh,0
" For example, in the image classification literature, if you input the image to this input layer and enable Neural Network computation, you will get the classification result on the output layer as the calculation result of the Neural Network.",Tanh,0
" For example, this is 0 in handwritten digit, and you get a classification result of 0.",Tanh,0
 The Neural Network shown here is a four-layer Deep Neural Network that performs handwritten digit classification.,Tanh,0
 The input is a monochrome 28x28 pixel handwritten digit.,Tanh,0
" And the number of neurons is 1000 in the first layer, 300 in the next layer, and then 100, and 10 in the last layer.",Tanh,0
" This 4-layer Neural Network can be represented by a combination of 8 functions, as shown below.",Tanh,0
" From the top are Affine, Tanh, Affine, Tanh, Affine, Tanh, Affine, and Softmax.",Tanh,0
 The Deep Neural Network can be represented by a combination of these eight functions.,Tanh,0
" The first one is Affine, which is a function called fully-connected layer.",Tanh,0
" Since the input neuron is a monochrome image of 28x28 pixels, there are 784 input neurons of 28x28 in total, in this Affine function.",Tanh,0
 And the output neurons in the first layer were 1000.,Tanh,0
 It is called a fully-connected layer because the input and output neurons are all connected in all combinations.,Tanh,0
" As explained in the previous video on artificial neurons, the value of this output neuron is determined by multiplying and adding the values of all the input neurons by different weights w, respectively.",Tanh,0
" Multiply this first neuron by a weight w and add, then multiply it by another weight w and add, then multiply it by another and add, then multiply it by another and add, for all 784 of these neurons, and you have the value of the output neuron here.",Tanh,0
" Then, to determine the value of the next output neuron, we again multiply these 784 input neurons by a different weight, w, than we did earlier, and add.",Tanh,0
 This function called Affine does all these processes for these 1000 output neurons.,Tanh,0
 The weight w exists for all combinations of inputs and outputs.,Tanh,0
" For example, if the number of input neurons in our example is 784 and the number of output neurons is 1000, that means that there are 784,000 weights w here, which is the result of 784x1000.",Tanh,0
 This function Affine may be called by a different name depending on the arrival of the Neural Network.,Tanh,0
" In addition, since all of these inputs and outputs are densely connected, it is called Dense, and many other variations exist.",Tanh,0
 Tanh looks like this when written in a graph.,Tanh,0
 The horizontal axis is the input value and the vertical axis is the output value.,Tanh,0
 And I would like to remind you of the formula for artificial neurons.,Tanh,0
 Then there was an activation function that performed nonlinear processing on the output result .,Tanh,0
 This time we use Tanh as the activation function.,Tanh,0
 There are many other types of activation functions besides Tanh.,Tanh,0
 Here I will explain using this traditional activation function Tanh .,Tanh,0
" By repeating this three times, a three-layer Neural Network is constructed.",Tanh,0
 Let's take a look at the functions used in the Convolutional Neural Network.,Tanh,0
" At the bottom of this section, we have Convolution, MaxPooling, and Tanh.",Tanh,0
 Here we have new functions called Convolution and MaxPooling.,Tanh,0
" After that, repeat this Convolution, MaxPooling, and Tanh .",Tanh,0
" And after this, it's exactly the same as the Deep Neural Network I mentioned earlier.",Tanh,0
" The combination of these functions, Afifne, Tanh, Affine and Softmax, make up this Convolutional Neural Network.",Tanh,0
" So, the difference from the previous one is that the functions Convolution and MaxPooling are used.",Tanh,0
" For now, it suffices to know that Convolution is to process images like this.",Tanh,0
" Normally, when processing an image, a single image is output for each input image.",Tanh,0
" However, in this Neural Network world, Convolution applies several different filters to the input image, in this case six different filters, and outputs six different images.",Tanh,0
" The MaxPooling function will down-sample the image in half lengthwise and widthwise, and at this point we will have six 12x12 pixel images in half lengthwise and widthwise.",Tanh,0
" Now, we'll run MaxPooling again to halve the resolution, in other words, to 16 4x4 pixel images with half the height and the width.",Tanh,0
" After this, we have the same structure as Deep Neural Network with Affine, Tanh, Affine and Softmax.",Tanh,0
" At this point we have 4x4x16 pixels, or in other words, 256 neurons, so we take 256 neurons as input and perform a full connection and Tanh with 120 neurons as output.",Tanh,0
" And from there, we do Affine with 10 neurons as output.",Tanh,0
" Now, let's talk about the Convolution process, comparing it to Affine.",Tanh,0
" In Affine, the input and output neurons are fully connected, that is, each output neuron receives the values of all the input neurons.",Tanh,0
" On the other hand, the difference in convolution is that in a 5x5 Convolution, for example, the top left pixel of this output image is only connected to the neurons in the top left neighboring area of the 5x5 pixel.",Tanh,0
" One more thing is the weight w. Earlier, in the case of Affine, if one output neuron was different, it was multiplied and added by a completely different weight w. In the case of Convolution, we use the common weight w when calculating the top left pixel, when calculating the one next to it, and when calculating the bottom right pixel.",Tanh,0
" In the case of the fully-connected layer, if there were 784 input neurons and 1,000 output neurons, there were 784,000 weights w of 784x1,000.",Tanh,0
" In the case of Convolution, the 25 weights w, which are 5x5 are commonly used throughout the image, so there are only 25 weights w per image processing session.",Tanh,0
" And what is often asked is the total number of Neural Networks, the number of neurons in each layer, and in the case of the Convolutional Neural Network, the number of images, and the type of activation function.",Tanh,0
" We have introduced Tanh as an activation function earlier, but there are many other types of activation functions.",Tanh,0
 I'm often asked how to determine this.,Tanh,0
" In the case of binary classification problems, we use a function called Sigmoid instead of Softmax as the last activation function.",Tanh,0
" This function, Sigmoid, is similar to the Tanh function we introduced earlier.",Tanh,0
 The value from 0 to 1 output by this Sigmoid is treated as a 0% to 100% probability.,Tanh,0
" So, let's wrap up this video.",Tanh,0
" The number of input and output neurons in the Neural Network is determined by the size of the input data, depending on the problem you want to solve.",Tanh,0
 The number of neurons in the output is then determined according to the size of the answer you want to get as the output of the Neural Network.,Tanh,0
" In the example of handwritten digit classification, the number of input neurons was 28x28 since the image was 28x28 pixels, and the answer we wanted to get was a digit from 0 to 9, so the number of output neurons was 10 neurons corresponding to each digit from 0 to 9.",Tanh,0
 We first determine the number of neurons in the input and output.,Tanh,0
 Then select the last activation and loss functions according to the problem you want to solve.,Tanh,0
 So this is sigmoid of i.,Tanh,0
 This would be sigmoid f. This would be tanh of g. And this one is sigmoid of o.,Tanh,0
" And next, we have the two main equations, really, of an LSTM.",Tanh,0
" X is this dimension of the input and T would be the sequence length, right?",Tanh,0
 That's how we define our X input here.,Tanh,0
" Now the question is, suppose that I want to process multiple batches of this at once.",Tanh,0
 So a little bit different.,Tanh,0
 Or H in for the input size.,Tanh,0
 H out for the hidden size.,Tanh,0
 We have all our dimensions for PyTorch to be happy.,Tanh,0
" So let's input h, h0, and c0.",Tanh,0
" And let's see, okay.",Tanh,0
 You have to add support for that.,Tanh,0
" And I mean, tanh actually isn't too bad because we already have exponential, so.",Tanh,0
" As long as you do all that, implement it in needle, then to get the gradients with respect to all your parameters, you just call loss.backward and, you know, opt.step, right?",Tanh,0
" Again, this is all conceptual, so I might be making a typo here.",Tanh,0
" Hopefully not, but... We would finally compute some loss between our output and the input there, in the target there, l equals some loss, and then we would just say l.backward and opt.step.",Tanh,0
 That would be our training of a deep LSTM.,Tanh,0
 We just go with the real part and that is sufficient for our calculation.,Tanh,0
" So well, that was all regarding the tanh function in activation functions for deep learning.",Tanh,0
 So hope you guys enjoyed this video if you found you got educated by this video.,Tanh,0
 You can find out the link as well into description.,Tanh,0
" So guys now coming to Rectified Linear Unit or ReLU function as name suggest it acts like a linear function, but is, in fact, a nonlinear function allowing complex relationships in the data to be learned.",Tanh,0
Sometimes it depends on the range that you want the activations to fall into.,Tanh,0
"Whenever you hear ""gates"" in ML literature, you'll probably see a sigmoid, which is between 0 and 1.",Tanh,0
"In this case, maybe they want activations to fall between -1 and 1, so they use tanh.",Tanh,0
"[This page (hyper-link)] says to use tanh, but they don't give an explanation.",Tanh,0
[DCGAN (hyper-link)] uses ReLUs or leaky ReLUs except for the output of the generator.,Tanh,0
Makes sense - what if half of your embedding becomes zeros?,Tanh,0
Might be better to have a smoothly varying embedding between -1 and 1.,Tanh,0
"I'd love to hear someone else's input, as I'm not sure.",Tanh,0
"Sigmoid specifically, is used as the gating function for the three gates (in, out, and forget) in [LSTM (hyper-link)], since it outputs a value between 0 and 1, and it can either let no flow or complete flow of information throughout the gates.",Tanh,0
"But before making a choice for activation functions, you must know what the advantages and disadvantages of your choice over others are.",Tanh,0
I am shortly describing some of the activation functions and their advantages.,Tanh,0
Sigmoid,Tanh,0
Mathematical expression: sigmoid(z) = 1 / (1 + exp(-z)),Tanh,0
First-order derivative: sigmoid'(z) = -exp(-z) / 1 + exp(-z)^2,Tanh,0
Hard Tanh,Tanh,0
Mathematical expression: hardtanh(z) = -1 if z < -1; z if -1 <= z <= 1; 1 if z > 1,Tanh,0
First-order derivative: hardtanh'(z) = 1 if -1 <= z <= 1; 0 otherwise,Tanh,0
Advantages:,Tanh,0
ReLU,Tanh,0
"Mathematical expression: relu(z) = max(z, 0)",Tanh,0
First-order derivative: relu'(z) = 1 if z > 0; 0 otherwise,Tanh,0
Advantages:,Tanh,0
Leaky ReLU,Tanh,0
"Mathematical expression: leaky(z) = max(z, k dot z) where 0 < k < 1",Tanh,0
First-order derivative: relu'(z) = 1 if z > 0; k otherwise,Tanh,0
Advantages:,Tanh,0
[This paper (hyper-link)] explains some fun activation function.,Tanh,0
You may consider to read it.,Tanh,0
Many of the answers here describe why tanh (i.e.,Tanh,0
"(1 - e^2x) / (1 + e^2x)) is preferable to the sigmoid/logistic function (1 / (1 + e^-x)), but it should noted that there is a good reason why these are the two most common alternatives that should be understood, which is that during training of an MLP using the back propagation algorithm, the algorithm requires the value of the derivative of the activation function at the point of activation of each node in the network.",Tanh,0
"While this could generally be calculated for most plausible activation functions (except those with discontinuities, which is a bit of a problem for those), doing so often requires expensive computations and/or storing additional data (e.g.",Tanh,0
"the value of input to the activation function, which is not otherwise required after the output of each node is calculated).",Tanh,0
"You have so many mistakes it's hard to fix them all, you also didn't give a data sample.",Tanh,0
I would suggest you start with a simpler model and read the documentation first.,Tanh,0
From the DCGAN paper [Radford et al.,Tanh,0
[https://arxiv.org/pdf/1511.06434.pdf] (hyper-link)]...,Tanh,0
"""The ReLU activation (Nair & Hinton, 2010) is used in the generator with the exception of the output
layer which uses the Tanh function.",Tanh,0
"We observed that using a bounded activation allowed the model
to learn more quickly to saturate and cover the color space of the training distribution.",Tanh,0
"Within the
discriminator we found the leaky rectified activation (Maas et al., 2013) (Xu et al., 2015) to work
well, especially for higher resolution modeling.",Tanh,0
"This is in contrast to the original GAN paper, which
used the maxout activation (Goodfellow et al., 2013).""",Tanh,0
"It could be that the symmetry of tanh is an advantage here, since the network should be treating darker colours and lighter colours in a symmetric way.",Tanh,0
"Sigmoid also seems to be more prone to local optima, or a least extended 'flat line' issues.",Tanh,0
[http://playground.tensorflow.org/ (hyper-link)] <- this site is a fantastic visualisation of activation functions and other parameters to neural network.,Tanh,0
Not a direct answer to your question but the tool 'provides intuition' as Andrew Ng would say.,Tanh,0
I will suggest using the fragment replacement with Tag  to produce the same result .,addToBackStack,0
Add the fragment B  to activity with tag,addToBackStack,0
"Fragment A -> Fragment B [onBackPressed] -->Fragment A
Override the onBackPressed() in the Activity files where ,",addToBackStack,0
// check for fragment B and you are viewing  fragment B,addToBackStack,0
Here is a picture that shows the difference between add() and replace(),addToBackStack,0
[ (hyper-link)],addToBackStack,0
So add() method keeps on adding fragments on top of the previous fragment in FragmentContainer.,addToBackStack,0
While replace() methods clears all the previous Fragment from Containers and then add it in FragmentContainer.,addToBackStack,0
Let's understand both,addToBackStack,0
Case 1:,addToBackStack,0
[ (hyper-link)],addToBackStack,0
Case 2:,addToBackStack,0
[ (hyper-link)],addToBackStack,0
"The 'tag' used in add/replace(id, fragment, tag) is used to retrieve the fragment by calling fragmentManager.findFragmentByTag(tag).",addToBackStack,0
" Now, a task, in and of itself, isn't very complicated.",addToBackStack,0
 It's simply a stack of activities.,addToBackStack,0
" As you call start activity, that pushes a new activity onto the task's back stack.",addToBackStack,0
" The Back button reverses this, calling finish on the current activity, popping it from the stack, and taking the user back to where they were.",addToBackStack,0
 Hence the name.,addToBackStack,0
 This symmetric push-pop model also applies to fragments.,addToBackStack,0
" Remember, there's that fancy overview screen for switching tasks.",addToBackStack,0
" So for a notification that points to an activity deep within your app, you really don't want that first press of the Back button to take you immediately to the launcher.",addToBackStack,0
" Not when every other time you're looking at that same screen, the Back button does something different, like go back in your app.",addToBackStack,0
 That's where TaskStackBuilder comes in.,addToBackStack,0
" It builds a synthetic, i.e.",addToBackStack,0
" fake, back stack.",addToBackStack,0
" By default, based on the parent activities you attached to each activity entry in your manifest.",addToBackStack,0
 You just saved them some time and effort getting there.,addToBackStack,0
" So check out the blog post linked in the description for all the details on tasks and the back stack, plus some of the other flags and launch modes you probably shouldn't use.",addToBackStack,0
" Well, right up until they're the perfect thing to use to build better apps.",addToBackStack,0
" So, 120 volts and 10 ohms, 12 amps, 1440 watts.",addToBackStack,0
 Now in the previous version of the app I had a back button here and-- or I could use this back button.,addToBackStack,0
 And that's the point that I want to return to.,addToBackStack,0
 So I'm going to add in my transaction addToBackStack.,addToBackStack,0
 And then optionally you can give this a name.,addToBackStack,0
 So I'll type a 120 volts and 10 ohms calculate.,addToBackStack,0
" Now, when I hit the back button it returns me to the input form.",addToBackStack,0
" And let's go ahead and do 220 calculate, 22 amps, back button and so on.",addToBackStack,0
 So now the back button is working the same.,addToBackStack,0
 And I just want to point out one more thing about this.,addToBackStack,0
 So let's go ahead and run it again.,addToBackStack,0
 It should do the same thing as before.,addToBackStack,0
" So 110, 10, 11 amps, back, I return to the same point, 220, back, return to the same point.",addToBackStack,0
 So bottom line is if this can be confusing when you're starting out because you think you're returning to the activity with the output form but you're actually returning to the activity before this commit happens.,addToBackStack,0
 And individually these problems  are pretty tractible but if you  look at a real world example you can see that they can get pretty hairy.,addToBackStack,0
 So say that I have an item  screen  saved in my app and it's accessible via deep link  but if someone navigated to the  screen opening the app from the  home screen they would have a  couple other screens on the back stack.,addToBackStack,0
" So hitting it up we want them to take it not out of the app from  the screen,  we want them to go to the  category screen and then the  home screen.",addToBackStack,0
 If someone deep  links into the app we  need to synthesize these screens and add  them to the up stack before  showing the screen.,addToBackStack,0
" It's when  you're in the middle of writing  the code to do this to  synthesize these screens and add them to  the up and back stack but only  on a deep link, that's when you  start to feel like maybe you're  solving a failure of the  framework.",addToBackStack,0
 So it helps all problems like  that that we are launching  navigation.,addToBackStack,0
" And that let's you define a set  of available navigation actions, arguments you can pass from  place to place, things like  visual transitions and a single  navigate call activates all that at run time.",addToBackStack,0
 And so last but not least that  means one thing you never have  to worry about  again is torching is touching a fragment  transaction with your bare  hands.,addToBackStack,0
 [Applause].,addToBackStack,0
" We are  going to say if they have gotten to this congratulations screen,  that means the game is over.",addToBackStack,0
 So hitting back shouldn't take them back into a game that no longer  exists.,addToBackStack,0
 So what that means is we want to on  that action say let's pop to the match screen and that means I'm  going to pop off everything on  the back stack in  between this destination and the match screen.,addToBackStack,0
" So when the user  gets to the congratulations  screen, when they hit  back, they're just going to go  straight to the match screen.",addToBackStack,0
 So a lot of other options I can  set but I'll just talk about  that for now.,addToBackStack,0
 Let's go back and look at the  congratulations screen again.,addToBackStack,0
" One other thing to mention, the  key thing that is set here is  the fragment class.",addToBackStack,0
 First  let's see what they're trying to resolve.,addToBackStack,0
 Let's go back to our  sample.,addToBackStack,0
 Our fragment where we tried to  negate  actually requires us the best  screen name argument.,addToBackStack,0
 Best category which has integer  type.,addToBackStack,0
 Let's go back to the  calling site.,addToBackStack,0
 Well in our  slides we made everything  correctly.,addToBackStack,0
 We don't program don't program critical medical  diseases medical devices from  our phones.,addToBackStack,0
" I'll talk about a few use cases, medical, financial and  enterprise but  the key innovation is protected  confirmation is the first time  that we now have the ability to  execute a high assurance  transaction, a user transaction  completely within secure  hardware, running in a trusted  execution environment or TE,  that runs separate from the main operating system.",addToBackStack,0
 So how does it work?,addToBackStack,0
 It's guarded in this  area.,addToBackStack,0
 And the entire  transaction is signed using a  transcriptd key that never  leaves that security area.,addToBackStack,0
 This provides higher assurance  to the relying party that the  integrity of this data was not  corrupted even if you had  root level malware it cannot  corrupt the integrity of that  transaction.,addToBackStack,0
" 1500  can't be changed to 15,000.",addToBackStack,0
" The relying party on the other  end has  high confidence that we intended to send  ravy $1,500 and the transaction  goes through.",addToBackStack,0
 Duo security is a firm that is  working on strong enterprise  authentication.,addToBackStack,0
 Payments is another.,addToBackStack,0
 In all of these use cases you  want to make sure that your  phone and only your phone can  make that transaction.,addToBackStack,0
 There  are quite a few other examples  where we benefit from stronger  protect from private keys.,addToBackStack,0
 With a camera API it will behave as if  you were preempted by a higher  priority camera clients.,addToBackStack,0
 With the sensors it's whether  it's continuously or via call  back.,addToBackStack,0
 If the app is in the background  you can no longer access data  from sensors.,addToBackStack,0
 They think  it's something only for experts.,addToBackStack,0
 If you look back about 60 years  ago this is definitely the case.,addToBackStack,0
 This is a picture of the first  neural  network invented in 1957 and  this was a device that  demonstrated an ability to  identify different shapes.,addToBackStack,0
 Excited to score tickets to  Hamilton.,addToBackStack,0
 Let's see what we get back here.,addToBackStack,0
 Cranking.,addToBackStack,0
 That is the  vision API.,addToBackStack,0
 If we can go back  to the slides.,addToBackStack,0
 Next I want to talk about the  natural language API which let's you analyze text with a single  rest API request.,addToBackStack,0
 It's going to look similar to  the API vision code we saw on  the previous page.,addToBackStack,0
 We send in  our text and get back the result from the model.,addToBackStack,0
 Let's jump to a demo of the natural language  API.,addToBackStack,0
 That is the natural language  API.,addToBackStack,0
 If we can go back to the slides.,addToBackStack,0
 I'll talk briefly about  companies using these APIs in  production.,addToBackStack,0
 The next part is generating  predictions on new data.,addToBackStack,0
 I'm  going to take this go back and  improve my training data.,addToBackStack,0
 The  next part is generating  predictions on new data.,addToBackStack,0
 So  that is auto ML vision.,addToBackStack,0
 If we  can go back to the slides.,addToBackStack,0
 A  little bit about companies that  are using Auto ML vision and  have been part of the alpha.,addToBackStack,0
 If you double click on the  component it will jump  automatically.,addToBackStack,0
 And if you jump to the arrow it  will jump back.,addToBackStack,0
 This is handy to be fast between layouts.,addToBackStack,0
 And finally  we have also added this  feature for recycler view .,addToBackStack,0
 You can just jump back to the file and preview your changes.,addToBackStack,0
 All the design time attributes  are automatically populated so  you don't have to do anything  and take full advantage of  sample data.,addToBackStack,0
 Okay.,addToBackStack,0
 Let's go back to our  example.,addToBackStack,0
 We have our adapter.,addToBackStack,0
" We observe the live data, send  the list to the adapter.",addToBackStack,0
 When we see the adepter we need  to give it a call back.,addToBackStack,0
 It has  two functions.,addToBackStack,0
" The first most important part of the boundary call back that you  implement is  that you pass it to -- you want  to provide it two different  sources of data, the database  and the network, because that's  its job.",addToBackStack,0
 So the important call back we  have here is on I'm end loaded.,addToBackStack,0
 The last item of the database  has been loaded from the paged  list and if there's more from  the network it's time to load  it.,addToBackStack,0
 And then we can re-set that at  the end.,addToBackStack,0
 So using boundary call back is  pretty simple.,addToBackStack,0
 You can add this in your Rx page list  builder and that gives you the  database  plus network solution all  isolated in that one call back.,addToBackStack,0
" As  you can see as we increase the  number of GPUs from 1 to 4 to 8, the  images per second process can  almost double every time.",addToBackStack,0
 We will come back to these  performance numbers later.,addToBackStack,0
" Before we get into the details  of scaling in TensorFlow, first  I want to  cover a few high level concepts  and architectures in distributed training.",addToBackStack,0
 We create an estimator object  with the  run config and then and those are all the code  changes you need to distribute  the res net model.,addToBackStack,0
 Let's go  back and see how our training is going.,addToBackStack,0
 So we have run for a few hundred steps.,addToBackStack,0
 What happens if we are inside our ListDetailRoute and the user wants to go back?,addToBackStack,0
" Well, if we're using an expanded window size, our list and detail are inside the same composable, so pressing back should go back to the previous destination in the stack.",addToBackStack,0
" But when using the compact window size, where the detail screen replaces the list screen, pressing back from the detail screen should take you to the list, not the previous destination.",addToBackStack,0
Basic difference between add() and replace() can be described as:,addToBackStack,0
add() is used for simply adding a fragment to some root element.,addToBackStack,0
replace() behaves similarly but at first it removes previous fragments and then adds next fragment.,addToBackStack,0
I's been a while but I hope this will help someone.,addToBackStack,0
I couldn't find the reason.,addToBackStack,0
"2) fragmentTransaction.replace(int containerViewId, Fragment fragment, String tag)",addToBackStack,0
Description - Replace an existing fragment that was added to a container.,addToBackStack,0
"This is essentially the same as calling remove(Fragment) for all currently added fragments that were added with the same containerViewId and then add(int, Fragment, String) with the same arguments given here.",addToBackStack,0
"3) fragmentTransaction.add(int containerViewId, Fragment fragment, String tag)",addToBackStack,0
Description - Add a fragment to the activity state.,addToBackStack,0
This fragment may optionally also have its view (if Fragment.onCreateView returns non-null) into a container view of the activity.,addToBackStack,0
"What does it mean to replace an already existing fragment, and adding
  a fragment to the activity state and adding an activity to the back
  stack ?",addToBackStack,0
There is a stack in which all the activities in the running state are kept.,addToBackStack,0
Fragments belong to the activity.,addToBackStack,0
So you can add them to embed them in a activity.,addToBackStack,0
You can combine multiple fragments in a single activity to build a multi-pane UI and reuse a fragment in multiple activities.,addToBackStack,0
This is essentially useful when you have defined your fragment container at different layouts.,addToBackStack,0
You just need to replace with any other fragment in any layout.,addToBackStack,0
"When you navigate to the current layout, you have the id of that container to replace it with the fragment you want.",addToBackStack,0
"findFragmentByTag does this search for tag added by the add/replace
  method or the addToBackStack method ?",addToBackStack,0
If depends upon how you added the tag.,addToBackStack,0
It then just finds a fragment by its tag that you defined before either when inflated from XML or as supplied when added in a transaction.,addToBackStack,0
References: [FragmentTransaction (hyper-link)],addToBackStack,0
Like,addToBackStack,0
Now we can check the fragment before adding it to the Stack :,addToBackStack,0
This will return null if the Fragment is not already added.,addToBackStack,0
You can read more about this [here (hyper-link)].,addToBackStack,0
text,text2,label
" This Tanh is a hyperbolic tangent itself, but in the world of Neural Network, Tanh converts the input value into a non-linear one and keeps it in the range of -1~1.",Tanh,1
" As you can see, when a large positive value is input, the output value is stuck to 1, and when a large negative value is input, the output value is stuck to -1.",Tanh,1
"And when the value in this area is input, the output changes almost linearly.",Tanh,1
"In this way, it can be shown that a combination of such functions can approximate any non-linear function.",Tanh,1
"On the other hand, to overcome the vanishing gradient problem, we need a function whose second derivative can sustain for a long range before going to zero.",Tanh,1
Tanh is a good function with the above property.,Tanh,1
"Tanh and the logistic function, however, both have very simple and efficient calculations for their derivatives that can be calculated from the output of the functions; i.e.",Tanh,1
"if the node's weighted sum of inputs is v and its output is u, we need to know du/dv which can be calculated from u rather than the more traditional v: for tanh it is 1 - u^2 and for the logistic function it is u * (1 - u).",Tanh,1
"This fact makes these two functions more efficient to use in a back propagation network than most alternatives, so a compelling reason would usually be required to deviate from them.",Tanh,1
 Tanh after this is exactly the same process as the previous one.,Tanh,1
It is a process to fit the result of the process so far to a range between -1 and 1.,Tanh,1
 The Tanh function is then used to fit the value between -1 and 1.,Tanh,1
" While Tanh fits the input value between -1 and 1, this Sigmoid fits the input value between 0 and 1.",Tanh,1
 But whatever application our problem domain requires that we have to extend our limit from 1 to minus 1 well for that case we can mainly go with tanh activation.,Tanh,1
So the H in tanh stands for hyperbolic.,Tanh,1
So some people call it as hyperbolic or like I say it is hyperbolic.,Tanh,1
So in mathematically how does it look and H of Z looks like he raised to minus Z minus E raised to e raised to z- e raised to minus X upon e raised to Z Plus e raised to minus Z.,Tanh,1
So this is how your tanh function looks like so it is also an exponential function which is in terms of Z.,Tanh,1
"So now what is the peculiarity of tanh its function that it ranges from minus 1 to 1 so it does not give your signal into the range 0 to 1, but it can expand to minus 1 also so it will just shift this particular axis.",Tanh,1
 This is nothing but your tan H so I can write the this as 1 minus tan H Square h of Z especially so that is nothing but 1 minus here output Square.,Tanh,1
So this is differentiative of your tanh function.,Tanh,1
"So if you want to see the geometrical interpretation, so how it basically looks like is you have this particular scale minus Z 2 Z you have one then you have 0.5 and then you have your minus 1.",Tanh,1
" Alright, guys let's discuss hyperbolic tangent activation function which is also known as tanh activation function.",Tanh,1
"Unlike the Sigmoid function, the range for tanh function is -1 to 1.",Tanh,1
"So it is similar to Sigmoid function, where we were catering the output in a range of 0 to 1 & setting up the threshold at 0.5. so in tanh we extend the lower side of the curve till negative 1 & our mid will be at 0.",Tanh,1
So guys by definition tanh function is derived from the trigonometric function that's why it's formula is written as tanh(x) = sinh(x) divided by cosh(x).,Tanh,1
A general problem with both the sigmoid and tanh functions is that they saturate.,Tanh,1
This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively.,Tanh,1
In truth both tanh and logistic functions can be used.,Tanh,1
"The idea is that you can map any real number ( [-Inf, Inf] ) to a number between [-1 1] or [0 1] for the tanh and logistic respectively.",Tanh,1
Now regarding the preference for the tanh over the logistic function is that the first is symmetric regarding the 0 while the second is not.,Tanh,1
"This makes the second one more prone to saturation of the later layers, making training more difficult.",Tanh,1
Tanh,Tanh,1
Mathematical expression: tanh(z) = [exp(z) - exp(-z)] / [exp(z) + exp(-z)],Tanh,1
First-order derivative: tanh'(z) = 1 - ([exp(z) - exp(-z)] / [exp(z) + exp(-z)])^2 = 1 - tanh^2(z),Tanh,1
Advantages:,Tanh,1
"As for your second question on learnable parameters, ReLU and Leaky ReLU are simply activation functions that perform a predefined operation.",LeakyRelu,1
"According to the docs [https://github.com/microsoft/onnxruntime/blob/master/docs/OperatorKernels.md (hyper-link)], LeakyRelu is only implemented for type float (32-bit), while you have double (64 bit).",LeakyRelu,1
" Whereas if you have  a activation function that can only produce positive numbers,  you're a little bit more limited.",LeakyRelu,1
"So if you use the chain rule, and your  derivative is one, if the inputs are positive, then yeah, you  don't diminish the product in the on general, okay, but it can  also be zero, which can be a problem.",LeakyRelu,1
"So if you have negative  inputs to this activation function, you are your output  would be zero, which will then basically cancel the weight  update for that corresponding weight corresponding to this  activation, or connected to this activation.",LeakyRelu,1
So that can be a  problem if you always have very negative input.,LeakyRelu,1
So there is a  problem called dying neurons or debt.,LeakyRelu,1
" A version of Relu  that some people find to perform sometimes a little bit better is  the leaky relu, which doesn't have the problem of these dying  neurons.",LeakyRelu,1
"So here, the difference is that we have, so if we look  at the simplified notation, the the piecewise linear function  here, um, what you can see here, or the piecewise function,  sorry, what you can see here is that the only difference is that  we have now this alpha here, which is a slope, if the input  is smaller than zero, so for the negative region here, we have  now a slope, what value we can choose for the slope, it's a  hyper parameter, right?",LeakyRelu,1
 So hyper parameters is something that you  as the practitioner have has to choose.,LeakyRelu,1
Yes MMdnn support supports LeakyRelu.,LeakyRelu,1
Check the link below for pytorch_emitter.py implementation from MMdnn.,LeakyRelu,1
"addToBackStack(""TAG"") and popBackStackImmediate(""TAG"") always revert to fragment condition  without any data in the UI right before fragment is created or added to activity !",addToBackStack,1
addtoBackStack method can be used with add() and replace methods.,addToBackStack,1
It serves a different purpose in Fragment API.,addToBackStack,1
" When specifying a fragment transaction, you can call add to back stack to add the fragment transaction to the back stack.",addToBackStack,1
"Then, when the user hits the Back button, instead of your activity being finished, the fragment transaction is reversed.",addToBackStack,1
Only when there are no more fragment transactions will the Back button finish your activity.,addToBackStack,1
We can see the exact difference when we use addToBackStack() together with add() or replace().,addToBackStack,1
"When we press back button after in case of add()... onCreateView is never called, but in case of replace(), when we press back button ... oncreateView is called every time.",addToBackStack,1
You can also go back to the previous fragment in the backStack with the popBackStack() method.,addToBackStack,1
For that you need to add that fragment in the stack using addToBackStack() and then commit() to reflect.,addToBackStack,1
This is in reverse order with the current on top.,addToBackStack,1
What is addToBackStack,addToBackStack,1
What is the purpose?,addToBackStack,1
Fragment API unlike Activity API does not come with Back Button navigation by default.,addToBackStack,1
If you want to go back to the previous Fragment then the we use addToBackStack() method in Fragment.,addToBackStack,1
 The second case is around notifications.,addToBackStack,1
We talked about how the Back button pops the back stack.,addToBackStack,1
A natural corollary is that the Back button shouldn't cross into different tasks-- a convention since the days of Android 3.0.,addToBackStack,1
It's just a single stack from where you are to the launcher.,addToBackStack,1
 And the fragment manager lets you do that by setting something called the back stack.,addToBackStack,1
So the back stack is a list of prior points in your activities that may can return to by hitting back button.,addToBackStack,1
And all you have to do to make this work is in your main activity before you make a change that you want to return to-- so here's where I will place the input activity with the output activity.,addToBackStack,1
1) fragmentTransaction.addToBackStack(str);,addToBackStack,1
Description - Add this transaction to the back stack.,addToBackStack,1
"This means that the transaction will be remembered after it is committed, and will reverse its operation when later popped off the stack.",addToBackStack,1
"When using dispatchTouchEvent , you take all touches in your activity, if you want only detect one touch, you have to filter the touch by its type, you can do this using the MotionEvent parameter.",dispatchTouchEvent,2
"Both Activity and View have method dispatchTouchEvent() and onTouchEvent.The ViewGroup have this methods too, but have another method called onInterceptTouchEvent.",dispatchTouchEvent,2
"The return type of those methods are boolean, you can control the dispatch route through the return value.",dispatchTouchEvent,2
"Another difference is that if dispatchTouchEvent return 'false' the event dont get propagated to the child, in this case the EditText, whereas if you return false in onInterceptTouchEvent the event still get dispatch to the EditText",dispatchTouchEvent,2
negative_slope in this context means the negative half of the Leaky ReLU's slope.,LeakyRelu,2
It is not describing a slope which is necessarily negative.,LeakyRelu,2
"When naming kwargs it's normal to use concise terms, and here ""negative slope"" and ""positive slope"" refer to the slopes of the linear splines spanning the negative [-∞,0] and positive (0,∞] halves of the Leaky ReLU's domain.",LeakyRelu,2
"However, the 'name' used in addToBackStatck(name) is used to control to which fragment you want to pop the fragment back stack by calling popBackStatck/Immediate(name, flags).",addToBackStack,2
"So if I have a fragment stack with named fragments: A, B, C and D with A at the bottom.",addToBackStack,2
"When you call popBackStack(B, XXX_EXCLUSIVE), then your fragment back stack will be like: A and B after the call.",addToBackStack,2
"Without the name, you can't do that.",addToBackStack,2
Passing null to addtoBackStack(null) means adding the fragment in the Fragment Stack but not adding any TAG which could be further use to identify the particular fragment in a stack before adding again.,addToBackStack,2
But passing TAG to addToBackStack helps in identifying the fragment in Fragment stack by TAG.,addToBackStack,2
"Just want to clarify, the 'name' used in addToBackStack(name) can't be used for retrieving the fragment by calling fragmentManager.findFragmentByTag(tag).",addToBackStack,2
The 'tag' is different from the 'name'.,addToBackStack,2
Would you even want to override [Activity|ViewGroup|View].dispatchTouchEvent?,dispatchTouchEvent,3
Unless you are doing some custom routing you probably should not.,dispatchTouchEvent,3
The main extension methods are ViewGroup.onInterceptTouchEvent if you want to spy and/or intercept touch event at the parent level and View.onTouchListener/View.onTouchEvent for main event handling.,dispatchTouchEvent,3
"For example, the simplest case is that of View.dispatchTouchEvent which will route the touch event to either OnTouchListener.onTouch if it's defined or to the extension method onTouchEvent.",dispatchTouchEvent,3
"So in case you are working with these 2 handlers use dispatchTouchEvent to handle on first instance the event, which will go to onInterceptTouchEvent.",dispatchTouchEvent,3
You may try to convert to 32-bit float right before LeakyRely in your PyTorch code.,LeakyRelu,3
And maybe create an issue on the ONNX Runtime Github to add double support for LeakyRelu.,LeakyRelu,3
"As for the idea of ""maintaining a state"", it refers to activation functions that would not behave independently on each and every fed-in sample, but would instead retain some learnable information (the so-called state).",LeakyRelu,3
"Typically, for a LeakyReLU activation, you could adjust the leak parameter through training (and it would, in the documentation's terminology, be referred to as a state of this activation function).",LeakyRelu,3
"I would suggest removing the sigmoid activation from last layer, and replace relu with LeakyRelu to improve the model's robustness.",LeakyRelu,3
fragmentTransaction.addToBackStack(null) won't work if you are extending AppCompatActivity.,addToBackStack,3
It works well in Activity.,addToBackStack,3
"The parameter for addToBackStack() is an optional name for the back state, you do not use the tag in the replace() method which is just an optional tag for the fragment.",addToBackStack,3
" But now I want to hit the back button, it doesn't take me to the input form, it just exits the app.",addToBackStack,3
"And when I return from that activity, that exits the app.",addToBackStack,3
So with-- but I-- what I really want to do is I want to be able to hit that back button and go back to my input form.,addToBackStack,3
So I want to be able to type 120 and 10 and calculate.,addToBackStack,3
"And if I hit this back button, I want to return to the input form even though it's the same activity.",addToBackStack,3
 So you'll notice that I have this addToBackStack before I replace the content with the output fragments.,addToBackStack,3
"And you might think that if I move this here, now when I hit the return it would return to the output fragment.",addToBackStack,3
That change isn't actually made until here when I do the commit.,addToBackStack,3
So addToBackStack Input Form should do the same exact thing whether I have it before or after this replace.,addToBackStack,3
the reason why the popBackStack() and popBackStackImmediatly() is due to the fact that you didnt add that fragment (that you want to pop) to the backStack.,addToBackStack,3
In order to do this you have to make a call to addToBackStack() at the moment of making the transaction to add/replace your fragment.,addToBackStack,3
The main difference :,dispatchTouchEvent,0
"•Activity.dispatchTouchEvent(MotionEvent) - This allows your Activity
  to intercept all touch events before they are dispatched to the
  window.",dispatchTouchEvent,0
Because this is the first result on Google.,dispatchTouchEvent,0
I want to share with you a great Talk by Dave Smith on [Youtube: Mastering the Android Touch System (hyper-link)] and the slides are available [here (hyper-link)].,dispatchTouchEvent,0
It gave me a good deep understanding about the Android Touch System:,dispatchTouchEvent,0
How the Activity handles touch:,dispatchTouchEvent,0
"Activity.dispatchTouchEvent()

Always  first  to  be  called
Sends  event  to  root  view  attached  to  Window
onTouchEvent()

Called  if  no  views  consume  the  event
Always  last  to  be  called",dispatchTouchEvent,0
How the View handles touch:,dispatchTouchEvent,0
"View.dispatchTouchEvent()

Sends  event  to  listener  first,  if  exists
  
  
View.OnTouchListener.onTouch()

If  not  consumed,  processes  the  touch  itself
  
  
View.onTouchEvent()",dispatchTouchEvent,0
He also provides example code of custom touch on [github.com/devunwired/ (hyper-link)].,dispatchTouchEvent,0
"Answer:
Basically the dispatchTouchEvent() is called on every View layer to determine if a View is interested in an ongoing gesture.",dispatchTouchEvent,0
You could imagine the [code of a ViewGroup (hyper-link)] doing more-or-less this (very simplified):,dispatchTouchEvent,0
" Here I'm saying, anything that's scrolling vertically I'm interested in, please send me more events.",dispatchTouchEvent,0
" So if you returned true from that one, every time the user scrolls in the list, I'll get this onNestedScroll event.",dispatchTouchEvent,0
" So you can do stuff with working out how much the ListView scrolls by itself, but the one I'm actually really, really interested in here is that last parameter, which is dyUnconsumed.",dispatchTouchEvent,0
" And then I work out where the top of the first item is, which I've set down with some padding.",dispatchTouchEvent,0
" So then, if the touch point where you're touching on the RecyclerView is above the first item, I directly call and dispatchTouchEvent onto the content behind.",dispatchTouchEvent,0
 This is the description field behind.,dispatchTouchEvent,0
 So that forwards on the touch event.,dispatchTouchEvent,0
 So again like letting the touch pass through the RecyclerView onto the content behind.,dispatchTouchEvent,0
 So it lets you do this kind of layering trick.,dispatchTouchEvent,0
" So here for example, I've got a couple of actions which use the floating action button pattern.",dispatchTouchEvent,0
" And when you touch it, sometimes you have more actions to be taken.",dispatchTouchEvent,0
" So if you're trying to like a shot, and you haven't logged in, I want you to log in.",dispatchTouchEvent,0
 So I instead use some dynamic coloring in order to make the app still feel alive and thoughtful.,dispatchTouchEvent,0
" So here for example, as you're clicking on an item, you can see the touch response-- the touche ripple you get is determined by the item you're clicking on.",dispatchTouchEvent,0
" So the gray and red one gets a nice red ripple, and the one below gets like a bluey-purple ripple.",dispatchTouchEvent,0
" In particular, I really want to use the vibrant color.",dispatchTouchEvent,0
 So picking out that red from that first image we saw is a great way to kind of tie the touch ripple back to the item.,dispatchTouchEvent,0
 So what I do is I walk through the swatches one at a time.,dispatchTouchEvent,0
 So how would we build that?,dispatchTouchEvent,0
" The first bit, raise on touch, that's easy.",dispatchTouchEvent,0
 That's my friend the stateless animator we looked at earlier.,dispatchTouchEvent,0
 So here's two versions of the same video.,dispatchTouchEvent,0
" So you touch on the item-- oh yeah-- one's with, one's without.",dispatchTouchEvent,0
 I hope you see that.,dispatchTouchEvent,0
 So the goal here is really about directing attention.,dispatchTouchEvent,0
" So when you touch on the search thing, I want it to feel like this transient experience.",dispatchTouchEvent,0
" So that it's just coming in transiently over the top of the content, which is why I do that scrim coming in slowly and gently.",dispatchTouchEvent,0
" For generic views which don't control their own values, this simplest alternative  is to set the content description within your app at runtime.",dispatchTouchEvent,0
 An even more robust solution is to send an accessibility event from within your view.,dispatchTouchEvent,0
 Whenever the visual content  has been modified.,dispatchTouchEvent,0
" Then override the dispatch populate accessibility event, and then the current control's  visual value, the accessibility event.",dispatchTouchEvent,0
" Go ahead and add  accessibility handlers to your view, and then click here when you're done.",dispatchTouchEvent,0
Here are some visual supplements to the other answers.,dispatchTouchEvent,0
My full answer is [here (hyper-link)].,dispatchTouchEvent,0
[ (hyper-link)],dispatchTouchEvent,0
[ (hyper-link)],dispatchTouchEvent,0
The best place to demystify this is the source code.,dispatchTouchEvent,0
The docs are woefully inadequate about explaining this.,dispatchTouchEvent,0
This is what onInterceptTouchEvent is there for.,dispatchTouchEvent,0
So it calls this method first before doing the hit testing and if the event was hijacked (by returning true from onInterceptTouchEvent) it sends a ACTION_CANCEL to the child views so they can abandon their touch event processing (from previous touch events) and from then onwards all touch events at the parent level are dispatched to onTouchListener.onTouch (if defined) or onTouchEvent().,dispatchTouchEvent,0
"Also in that case, onInterceptTouchEvent is never called again.",dispatchTouchEvent,0
All in all its overly complicated design imo but android apis lean more towards flexibility than simplicity.,dispatchTouchEvent,0
From Activity viewpoint:,dispatchTouchEvent,0
Touch events are delivered first to Activity.dispatchTouchEvent.,dispatchTouchEvent,0
It's where you may catch them first.,dispatchTouchEvent,0
"Here they get dispatched to Window, where they traverse View hierarchy, in such order that Widgets that are drawn last (on top of other widgets) have chance to process touch in View.onTouchEvent first.",dispatchTouchEvent,0
"If some View returns true in onTouchEvent, then traversal stops and other Views don't receive touch event.",dispatchTouchEvent,0
"Finally, if no View consumes touch, it's delivered to Activity.onTouchEvent.",dispatchTouchEvent,0
That's all your control.,dispatchTouchEvent,0
"And it's logical that what you see drawn on top of something else, has chance to process touch event before something drawn below it.",dispatchTouchEvent,0
Using this simple example:,dispatchTouchEvent,0
You can see that the log willl be like:,dispatchTouchEvent,0
"Your model initialization looks fine to me, just write the forward function to call the operations defined in __init__ and you should be all set.",LeakyRelu,0
"For instance, ReLU is defined as max(0,x), so it simply outputs the max value of the two inputs.",LeakyRelu,0
"While neither ReLU nor Leaky ReLU have learnable parameters, Leaky ReLU does have a hyperparameter, which is the negative_slope, as from its function definition max(0, x) + negative_slope * min(0,x).",LeakyRelu,0
"Your model initialization looks fine to me, just write the forward function to call the operations defined in __init__ and you should be all set.",LeakyRelu,0
 So there's also a  softmax function.,LeakyRelu,0
" But here, we compute this based on the we use  the lock softmax, because it's numerically more stable, if we  were to use the negative log likelihood loss.",LeakyRelu,0
 This is really  the only if we want to use the negative log likelihood loss.,LeakyRelu,0
" Otherwise, we would just use softmax if we are interested in  the probabilities.",LeakyRelu,0
" To be honest, now looking at this, I don't  know why I used lock softmax, I think, in the big code example,  when I created lecture five, I had the negative log likelihood  here.",LeakyRelu,0
" So also, technically, you don't have to compute the  probabilities within this class, you can do this separately, if  you care about because technically, you never have to  use the probability if you use the cross entropy function in  pytorch for optimization.",LeakyRelu,0
" And you  remember, this is very simple, it's actually just thresholded  at zero.",LeakyRelu,0
" So if the input is negative, the output is zero,  otherwise, it's an identity function.",LeakyRelu,0
" So it's almost an  identity function, but not quite.",LeakyRelu,0
 And the output  at zero is also zero.,LeakyRelu,0
" So it's producing positive and negative  values, which can be an advantage.",LeakyRelu,0
" There's also a hard  tension, which is essentially very similar to 10 h, except  that it's thresholded here, similar to relu.",LeakyRelu,0
 So  the event of 10 h is really like that we have this centering at  zero.,LeakyRelu,0
 So that we have positive and negative values.,LeakyRelu,0
 And you can  also see it's steeper.,LeakyRelu,0
" And then when you compute the partial derivatives in the  chain rule, then yeah, you will get very small gradients and the  learning will be very slow, which can be a disadvantage.",LeakyRelu,0
" Or  maybe one more thing about why it's good to have negative and  positive values, that just gives you more combinations.",LeakyRelu,0
" So  imagine you initialize your weights from let's say a small  from a random normal distribution, standard normal  distribution, let's say, or a scale to standard normal  distribution.",LeakyRelu,0
 So you initialize your weights such that they are  centered at zero.,LeakyRelu,0
 So you can have positive and negative  starting weights.,LeakyRelu,0
" And if you also use this 10h, which can  have positive and negative values, you get just more  combinations of possible values, whether you combine a positive  with a negative number and negative with a positive number  to negative numbers or to positive numbers, you have four  different ways you can combine these signs.",LeakyRelu,0
 It's something you have to try out in practice and change and  see what performs better.,LeakyRelu,0
" I have seen all kinds of values for  this negative slope here, or for the slope in the negative region.",LeakyRelu,0
" So in Keras, that's API for TensorFlow, I believe they use  point three as the default value.",LeakyRelu,0
 so formula for Sigmoid is shown on screen which is nothing but 1 divided by 1 + e to the power minus y & here y is summation of weights multiplied by the X + bias.,LeakyRelu,0
" so here what sigmoid function does is let this product be any value either negative or positive, whenever we replace this in formula it ranges the independent variables between 0 & 1. so here question is how can we assign any value as either 0 or 1. so it's very simple we decide some threshold over the graph.",LeakyRelu,0
 let say i draw a threshold of 0.5 & whenever it is greater than 0.5 output is considered as 1 & when it is less than 0.5 then output will be considered as 0. so guys from this you can understand that how it reduces the outliers or extreme values.,LeakyRelu,0
" Unlike the Sigmoid function, the range for tanh function is -1 to 1.",LeakyRelu,0
" So it is similar to Sigmoid function, where we were catering the output in a range of 0 to 1 & setting up the threshold at 0.5. so in tanh we extend the lower side of the curve till negative 1 & our mid will be at 0.",LeakyRelu,0
 So guys by definition tanh function is derived from the trigonometric function that's why it's formula is written as tanh(x) = sinh(x) divided by cosh(x).,LeakyRelu,0
" max(0, y).",LeakyRelu,0
 so what does it means is if our y is weighted sum of input + bias & this value is coming as some positive value then the max of 0 & that particular positive value would be that value itself but what if our y is some negative value then max of that negative value & zero would be 0 only.,LeakyRelu,0
 so by same intuition we can draw a graphical representation of ReLU function as shown on the screen.,LeakyRelu,0
" so guys for y=0 or any negative value our output will be transformed to 0 but it increases with the same y value if y is greater than 0. let say y is 2 then output will be 2, if y is 4 then output will be 4.",LeakyRelu,0
 So guys this is very powerful activation function as this is used at most of the places in deep learning.,LeakyRelu,0
 let's get back to graph again.,LeakyRelu,0
 so guys as you can see for any negative value or 0 our output is considering as 0 it means we are not activating those neurons but for positive value our neuron's strength is progressing exactly in the same way what value of y we are getting.,LeakyRelu,0
 let say if we are getting positive infinite value for y then in that case our output would be positive infinity only & for negative value our output will be 0. so once we have the output value with us we basically back-propagate the derivative of output value with respect to term x for adjusting the weight matrix as I have told you guys in my last video that we have to adjust weight matrix for getting the correct output.,LeakyRelu,0
 I will not go into deep of this back propagation because this is really a great topic to explain.,LeakyRelu,0
 now if i want to find the derivative of this line the value will always be 1 because this line is at an angle of 45 degree & we all know that tan 45 degree is 1 so for this line I can draw a straight line at 1. so for any positive value derivative of our output would always be 1 .,LeakyRelu,0
 now for negative side so you can see that this is just a constant value at 0th axis.,LeakyRelu,0
" so according to derivative rule, derivative of constant value is 0. so for any 0 or negative value derivative will be marked at 0. so range of derivative of ReLU function will be 0 for negative value & 1 for value greater than 0. guys now you must have one doubt that how is this different from Sigmoid function as Sigmoid was also ranging between 0 & 1. so In Sigmoid the derivative will always range between 0 to 0.5 & in tanh it is always less than 1. now let's apply derivative values in updation formula for derivative.",LeakyRelu,0
 so we have this formula y old = y new - learning rate into derivative of loss with respect to derivative of w. let say here we have 2 derivatives & it's a chain rule.,LeakyRelu,0
 now let see how can we fix our problem.,LeakyRelu,0
 so here we will try to add some value to constant side that means line will not be exactly 0 for negative number instead it will be some addition to this value.,LeakyRelu,0
 so here if y is greater than 0 then output would be y itself but if y is less than 0 then instead of putting it as 0 we will add some value to 0. let say we take 0.01 & this will be multiplied by x. now if we find the derivative of this term i.e.,LeakyRelu,0
 0.01 * y divided by derivative of w then our output will be some 0.01 value but not 0 & now if we subtract this value from y new then we will get some value which we can use for updation.,LeakyRelu,0
 so by this way we have solved dead neuron problem by just simply adding some value to negative side.,LeakyRelu,0
 so guys now we only have to discuss Softmax activation function.,LeakyRelu,0
 So how does it looks like so say this is our unit and here we have a linear part your computation and say We are Computing the R(z).,LeakyRelu,0
" This is the real work this now what happens is that when you take the derivative of this whatever portion on is there on this linear side right becomes differentiable, but when you just take the differentiative of this part that is on the negative axis, so the - axis on your number line, this is your positive axis.",LeakyRelu,0
" So for positive axis the differentiative when you do the gradient descent, it does not cause any problem so So if you want to see the graph of this if you want to differentiate this so say we are taking F Prime of Z.",LeakyRelu,0
" So this graph becomes something like this so that steady so that also the case for a step function, but in case of this negative values this attains 0, so this is where for- differentiation for negative d by dX This value is equal to 0 so suddenly, you're F dash X for greater than or equal to 0 it was one but as soon as it is less than 0 that is negative.",LeakyRelu,0
 It is falling down.,LeakyRelu,0
 So this part remains the Same.,LeakyRelu,0
" So this is same as it is, but on this portion that is on the negative side.",LeakyRelu,0
 You have a very small I got this so they're here.,LeakyRelu,0
" The slope is 0 here the slope is not equal to 0 so here if you have F of Z, it will be multiplied with some a of Z and here F of Z is equal to Z that is for greater than equal to 0 for R less than 0 you have a into Z.",LeakyRelu,0
 So this is again a hyperparameter that you decide.,LeakyRelu,0
" So when you just apply this value, so it's not equal to 0 it is approximately equal to 0.",LeakyRelu,0
 So it will make some movement in the slope.,LeakyRelu,0
" So, you know, that was our function loss function.",LeakyRelu,0
[pytorch_emitter.py (hyper-link)],LeakyRelu,0
If you check the implementation you will find all the supported operations and it doesn't include PRelu.,LeakyRelu,0
"I am not sure there is a strict definition of ""forward layers"" in this context, but basically what it means is that the ""classic"", keras-built-in types of layers comprising one or more sets of weights used to transform an input matrix into an output one have a activation argument.",LeakyRelu,0
"Typically, Dense layers have one, as well as the various kinds of RNN and CNN layers.",LeakyRelu,0
 they simply add a mechanism triggered at training to (hopefully) improve convergence rate and decrease overfitting chances.,LeakyRelu,0
Seems your model is not able to generalise to val set.,LeakyRelu,0
"Which means the model is unable to find the differences between (anchor, positive) vs (anchor, negative).",LeakyRelu,0
"In such case, distance of (anchor, positive) - distance of (anchor, negative) ~= 0.",LeakyRelu,0
And since your margin is set to be 1.,LeakyRelu,0
The loss will stay at 1.,LeakyRelu,0
Recall the loss function definition from [TripletMarginLoss (hyper-link)].,LeakyRelu,0
"torch.nn.functional.cross_entropy function combines log_softmax(softmax followed by a logarithm) and nll_loss(negative log likelihood loss) in a single function, i.e.",cross_entropy,1
"it is equivalent to F.nll_loss(F.log_softmax(x, 1), y).",cross_entropy,1
"Yes, the cross-entropy loss function can be used as part of gradient descent.",cross_entropy,1
"In short, cross-entropy(CE) is the measure of how far is your predicted value from the true label.",cross_entropy,1
"The cross here refers to calculating the entropy between two or more features / true labels (like 0, 1).",cross_entropy,1
"And the term entropy itself refers to randomness, so large value of it means your prediction is far off from real labels.",cross_entropy,1
Cross-entropy is commonly used to quantify the difference between two probability distributions.,cross_entropy,1
"In the context of machine learning, it is a measure of error for categorical multi-class classification problems.",cross_entropy,1
"Correct, cross-entropy describes the loss between two probability distributions.",cross_entropy,1
It is one of many possible loss functions.,cross_entropy,1
Sparse functions are a special case of categorical CE where the expected values are not one-hot encoded but is an integer,cross_entropy,1
"Usually the ""true"" distribution (the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution.",cross_entropy,1
Note that it does not matter what logarithm base you use as long as you consistently use the same one.,cross_entropy,1
"As it happens, the Python Numpy log() function computes the natural log (log base e).",cross_entropy,1
Cross entropy is one out of many possible loss functions (another popular one is SVM hinge loss).,cross_entropy,1
"These loss functions are typically written as J(theta) and can be used within gradient descent, which is an iterative algorithm to move the parameters (or coefficients) towards the optimum values.",cross_entropy,1
"In the equation below, you would replace J(theta) with H(p, q).",cross_entropy,1
"But note that you need to compute the derivative of H(p, q) with respect to the parameters first.",cross_entropy,1
" Alright, so yes, again, the cross entropy,  again, recall that there are two sums.",cross_entropy,1
"So if I go back here, so  we have these two sums here, I kind of entangled them a little  bit.",cross_entropy,1
"So we have, this is a sum over the training examples.",cross_entropy,1
And  this one is the cross entropy for the 100 encoding.,cross_entropy,1
"So this  one is the inner, the inner one here.",cross_entropy,1
So let's compute first the  cross entropy for each training example.,cross_entropy,1
So what I'm doing here  is I'm computing these terms.,cross_entropy,1
" So this function, the negative lock  likelihood loss expects the lock of the softmax values.",cross_entropy,1
 So you can see this  negative lock likelihood loss is the same as our cross entropy  here.,cross_entropy,1
" So where I mentioned  that the negative lock likelihood and binary cross  entropy equivalent in pytorch, it's actually the negative lock  likelihood and the multi category cross entropy  equivalent.",cross_entropy,1
"I mean, in a way, you can also think of it as a  multi category one, the multinomial logistic regression,  then this would be still true.",cross_entropy,1
" When we compute the cross entropy,  because we use the mathematical formulas, we compute first the  softmax.",cross_entropy,1
"And then from the softmax, we compute the cross  entropy in pytorch, they do all that work for us inside this  function, they do it for us.",cross_entropy,1
" And notice that I said, redact  reduction to none, which means it does not apply the sum or the  average, which is this outer one here.",cross_entropy,1
"So by default, when you  use this cross entropy, it will perform the average, you can  test it like this, see, it's the same same value.",cross_entropy,1
"If you wanted  to, you can also say reduction to consider reduction to some.",cross_entropy,1
" Actually, we had a seminar at UW  last week, where we also, yeah, it was briefly mentioned,  coincidentally, there was like a question whether it's the same,  the negative log likelihood and cross entropy.",cross_entropy,1
" So yeah, the  negative log likelihood and the binary cross entropy are  equivalent.",cross_entropy,1
"And in practice in deep learning, people just say  cross entropy, multi category cross entropy, which would be a  multi class version of the negative log likelihood, which  we will cover later in this lecture when we talk about the  softmax function.",cross_entropy,1
"So just to keep it brief, the negative log  likelihood that we just covered a few videos ago, is the same as  what people call the binary cross entropy, they were just  formulated in different contexts.",cross_entropy,1
"So negative log  likelihood comes more like from, I think it's like, it's probably  from a statistics context, I don't know the first paper, or  reference that mentioned that.",cross_entropy,1
"But this is something usually I  see in statistics papers, and the binary cross entropy thing  has originated from the field of information theory, or computer  science.",cross_entropy,1
"So we have actually seen that, or not, the cross  entropy, where we have seen the self entropy, or just entropy,  and statistics 451.",cross_entropy,1
" For those who took this class, in fall  semester, where we had used the entropy function in the context  of the information theory and decision trees, but we used a  lock to instead of the natural algorithm, but yeah, it's kind  of somewhat related, if you have taken any class where you  talked, for example, about the KL divergence, or callback  Leibler divergence, which measures the difference between  two distributions, the KL divergence is essentially the  cross entropy minus the self entropy.",cross_entropy,1
" The only  thing you have to know is or should know, because it's useful  to know, is that the negative log likelihood is the same as  the binary cross entropy, this is like a useful thing to know.",cross_entropy,1
" And  there's also a multi category version is the multi category  cross entropy, which is just a generalization of the binary  cross entropy to multiple classes.",cross_entropy,1
"So in order to make  that negative log likelihood or binary cross entropy work for  multiple classes, we assume a so called one hot encoding, where  the class labels are either zero or one for some reason, it was  cut off here.",cross_entropy,1
" So again, all I wanted to say here is the  logits in deep learning, usually refer to the net inputs of the  layer that just comes before the output.",cross_entropy,1
" We have   to use something called cross entropy loss,  and this is actually the loss function that   fastai picked for us before without us  knowing.",cross_entropy,1
 The first part of what cross-entropy loss  in Pytorch does is to calculate the softmax.,cross_entropy,1
" And so here is:   each of the part “y-i” times log of “p-y-i”,  and here is…(why did I subtract that's weird,   oh because I've got minus of both, so I  just do it this way, avoids parentheses…)   yeah, minus the are-you-not-a-cat times  the log of the prediction value not-a-cat,   and then we can add those together, and so that   would be the binary cross-entropy loss of  this dataset of five cat or not-cat images.",cross_entropy,1
" Basically it turns out that all of the loss  functions in pytorch have two versions – there's   a version which is a class, this is a class,  which you can instantiate passing in various   tweaks you might want, and there's also  a version which is just a function,   and so if you don't need any of these tweaks  you can just use the function.",cross_entropy,1
" All right   so that's all fine… we passed… so now when  we create a vision learner you can't rely on   fastaI to know what loss function to use, because  we've got multiple targets, so you have to say:   this is the loss function I want to use, this  is the metrics I want to use.",cross_entropy,1
There is just one real loss function.,cross_entropy,1
This is cross-entropy (CE).,cross_entropy,1
The goal of calculating the cross-entropy loss function is to find the probability that an observation belongs to a particular class or group in the classification problem.,cross_entropy,1