"It is simple: BatchNorm has two ""modes of operation"": one is for training where it estimates the current batch's mean and variance (this is why you must have batch_size>1 for training).  The other ""mode"" is for evaluation: it uses accumulated mean and variance to normalize new inputs without re-estimating the mean and variance. In this mode there is no problem processing samples one by one."
" Alright, so batch normalization, or in short, batch norm goes  back to a paper published in 2015, called batch  normalization, accelerating deep network training by reducing  internal covariate shift. "
Pytorch does its batchnorms over axis=1.  But it also has tensors with axis=1 as channels for convolutions.
"BatchNorm1d can also handle Rank-2 tensors, thus it is possible to use BatchNorm1d for the normal fully-connected case. "
" In practice,  people nowadays, it's more common to actually recommend if  you use dropout to recommend having batch norm after the  activation. "
" Yeah, and also  note that now when we use batch norm, batch norm has learnable  parameters.  So if we use batch norm in a given layer, it has we  have an additional two vectors that have the same dimension as  the bias vector, right.  So if we have, we use batch norm here in  this layer, we will have two, four dimensional vectors, like  this bias vector here would also be four dimensional, right,  because there's one bias for each in layer activation. "
what does [BatchNorm1d (hyper-link)] do mathematically? 
" And also, we will talk briefly about how batch norm  behaves during inference, because yeah, you know that  during training, we have mini batches, but that's not  necessarily the case if we want to use our network for  prediction. "
" But again, this is again, highlighting  train and evil are important here that we during training set  our model into training mode, because that's where batch norm  will compute the running mean and the running variance, I will  talk about this in the next slide.  So here, batch norm will  actually compute some running statistics during training. "
" So usually, practice people keep a moving average of both the  mean and the variance during training.  So you can think of it  as also as the running mean, how it's computed is by having a  momentum term, it's usually a small value like point one.  And  this is multiplied by the running mean from the previous  on epoch, or sorry, previous mini batch.  And then what you so  you have this, this term, this is like the running mean times  momentum term, this is a point one value.  And then you have one  minus the momentum, this is like a point nine value, then plus  yet plus point nine times the current sample mean.  So that's  the mini batch mean, and you just do the same thing also for  the running variance.  So here, essentially, this is just like a  moving average or running mean.  And you do the same thing for  the variance. "
" That's because there is a  slightly different version of batch norm for convolutional  networks.  We will discuss this in the convolutional network  lecture where this would be called batch norm 2d for the  convolution networks.  So to keep them apart, this is called batch  norm 1d. "
 And how  you can think of it as an additional normalization layer. 
" So  yeah, here, that's the first step of batch norm, there are  two steps.  So the first step is to normalize the net inputs.  So the j is the  feature index again.  So you can actually use batch norm for any  type of input.  So we will also see there is a two dimensional  version for that for convolutional networks later on. "
" So let's  say we have, yeah, the J feature.  And if I go back, so if  you consider this activation here, what are the features, so  the features are all essentially all the previous layer  activations, right?  So all these go into that activation.  So all  of these here are the features of this activation here.  So J  really is the index over the activations from the previous  layer. "
" But in the regular batch norm in the 1d version, we were  computing things for each feature.  So we were computing  this gamma and beta for each feature over the batch  dimension. "
" There are places of computing myu and Sigma is running average, again you can do that as well as you know some exponentially varied average schemes are available but this is the way to, during testing you will calculate myu and Sigma for every layer, this myu and Sigma are for the activations of every layer, every neuron and every layer. "
 Calculated by running it through the entire forward pass dataset. 
 That will be added computation or or you can just maintain a running average during training. 
Which one to use depends on the dimensionality of input data. 
"BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice. "
Core ML does not have 1-dimensional batch norm. 
"If you want to convert this model, you should fold the batch norm weights into those of the preceding layer and remove the batch norm layer. "
