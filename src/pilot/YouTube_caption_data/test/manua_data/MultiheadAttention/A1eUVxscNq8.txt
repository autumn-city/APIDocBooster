 Yeah, now that we spent so much work on understanding this
 scaled dot product attention mechanism in the previous video,
 let's rest on our laurels and just extend this multiple times,
 which we will call multi head attention. So it's also part of
 the transformer model, which we will eventually get to. So for
 now, let's focus on the multi head attention though. So in the
 previous video, I showed you this, where we computed this
 attention matrix. And we can think of this as one attention
 head. And yeah, surprise, surprise, multi head attention
 is this thing here, just multiple times using a
 different weight matrices. So to summarize, we are now going to
 apply self attention multiple times in parallel, similar to
 how we use multiple kernels for the different channels in the
 CNNs. So remember, when we talked about CNNs, we had, for
 instance, input image with three color channels, and we went from
 three channels to 64 channels, for example, so for that, we
 used 64 kernels in parallel, these would be, for example,
 three by three by three kernels, and we had 64 of those. So in
 that in a similar manner, we can also use multiple of these
 processes here, these multiple attention heads, I'm not sure
 why it's called a head, but let's just use the original
 terminology, we can use multiple of these attention heads to
 compute things in parallel, and yet to attend different parts of
 the sequence differently. So for each head, we use different
 weight matrices. So we had three mid weight matrices for the
 query, the key and the value. And we will use different ones
 now, and then we will concatenate those the results
 of those. So it will be more clear in the next slide, I
 guess. So in the original attention is all you need paper,
 they had eight attention heads. So essentially, they had eight
 times three matrices. So each attention had had a set of the
 query key and value matrix, and we had eight of those sets. And
 this allows the model to attend to different parts in the
 sequence differently for each head. It's similar to the concept
 behind color channels, essentially, or output channels.
 Okay, so again, so this is our scaled dot product attention
 that we talked about, that's essentially this. And here is
 the multi head attention. That's just a screenshot from the
 original paper. And you can kind of guess what's going on here.
 So here in the center is the scaled dot product attention.
 It's essentially this one summarized. But you can see,
 this is just stacked. So you have you have a stacking here.
 So use you repeat this eight times, whereas in the original
 paper, they had this eight times. So the linear corresponds
 my guess is my guess to the matrix multiplication between
 the weight matrices and the inputs. Okay, um, yeah, the
 input sequences t times 512 dimensional, like we talked
 before. And in the transformer model that we will be talking
 about in the next video, they use 512 for the input embedding
 size. And for the value size for so when we have the weight
 matrix, sorry, TV, so we will have the dimensionality of that
 would be t times dv, where dv is 512. So this is t divided by h
 that's the number of attention heads. So this is 64. And this
 is such that when you multiply it by the number of attention
 heads, you get the input and input embedding size back. And
 that is useful when you want to use something like skip
 connections, and you have a skip connection is like, if you
 remember, like this, so you have a layer x plus x, the input, so
 you have to have the same dimension. Otherwise, the
 addition doesn't work. Okay, so the concatenation here is we
 have done the scaled dot product attention, if I go back here, the
 scaled dot product attention is t times dv dimensional, right?
 That's one scale to dot product attention. So it's one. One of
 these here is t times dv dimensional. So one of these
 essentially times, now we have h of them, right? So we repeat
 this h times, for instance, in the original paper eight times,
 and then we concatenate. So what we will get is this year, eight
 times, so we will get h times, so we'll get dv times h. And if
 dv is 64, we will get 512 dimensional input back here. So
 again, one, one attention that is t times dv, we concatenate
 and get this t times 512 here in this concatenation step. So
 that's essentially what's going on in the in the multi head
 attention, we repeat this multiple times this scale dot
 product attention, each time with different weight matrices,
 and then we concatenate the results. A few more things. So
 there's another matrix involved here. So you can see there's
 this linear matrix. So this linear matrix is just to
 provide more parameters for learning, for instance. So we
 have the concatenation, and this matrix is dv times h
 dimensional, and then we have an output dimension. And the
 output dimension is equal to the rows and the columns is the
 same number. So it's a quadratic matrix. So the output would be
 also 512 here. Okay, and this is it for multi head attention.
 Pretty simple concept. It's essentially just applying the
 scaled dot product attention mechanism multiple times in
 parallel, concatenating the results, putting it through
 another fully connected layer. And yeah, that's it. So and now
 we have talked about all these concepts, but we haven't learned
 yet how they fit together into a model. And this is the
 transformer model, which will be the topic of the next video.