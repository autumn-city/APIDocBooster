okay so we are about so sorry for these
are some people
don't
um so
i'm wearing the mask so if something is
not clear please people shout out should
shout out and uh i will repeat it um
thank you everybody for being on this
tutorial this um afternoon uh this um
one
this one is going to be
on uh deep learning for sequential data
so we will
focus mainly on models for sequential
data deep learning models and uh we'll
discuss a little bit of the applications
where these models have been applied or
used
doesn't move
okay
so the outline of the talk is that i'll
give a context what i mean by sequence
data some examples of sequence data
we'll talk about the recurrent neural
networks which is the deep the
variations of deep learning networks
that have been
traditionally employed for sequential
data that means the model understands
has knowledge of the sequence
um one issue that um these models had to
overcome which is long-term dependencies
we say what is long-term dependence and
how they
i overcome that
uh i'll go on and discuss a little bit
attention mechanism which is a more
recent
approach of handling long-term deeper
dependencies we'll discuss the
properties of vary and answer
care and neural networks once we
understand them and then we'll give some
examples of applications and i'll finish
the talk with a hands-on a example of
how to predict
a to do prediction time series using one
of those models but by then we'll know
what it is
okay
so
sequential data is um any data where the
orderman text
order matters
for example text is some a kind of this
so when you read what comes what word
comes after and before matters and by
sequences of proteins or genes and
other things are sequential patterns
like sequences of purchases that's also
an example of a sequential um data where
a one purchase follows some other
depending on that you might do some
other purchase and so on and so forth
and one big um example of a sequential
data is time series for example stock
prices music audio video so and so forth
okay
the formal definition for us for a
sequential data is that it will be a
sequence of an index set into a domain
the index sets the index values have to
be discrete
so we can
measure them we don't
consider here continuous indexes and
they have to be also a
a monotonic that means that we go from
one to three to four and
in our in my talk i will also assume
that these indexes they um
they don't have a they a you don't use
the same index again that means that we
don't have multi-dimensional data but
these models can
be applied to different kinds of data as
well
and
the sequence the values the domain of
the sequences can be categorical values
normal nominal values alphabets
enumeration types and so forth or
continuous values like real numbers
for example temperature
okay
now the recurrent neural networks is a
family of neural networks or deep
learning models we've been calling them
in the previous tutorials that are used
to
to process this kind of data this
sequential data
they they understand the sequence they
can scale too much longer sequences that
would be practical for networks without
sequence and wearing units that so these
uni these models have units we'll see
what this is that they understand the
sequence
and um
also most in regular neural networks can
process sequences of variable length
for which wouldn't be for example
possible for a mlp like multi-layer per
second that we saw in the beginning of
this tutorial series
okay
so
uh the require uh recurrent neural
networks allow a what is their i
the main characteristic is that they
allow the previous output to be used as
input and while uh to be used as input
to na to next steps and also they have
something what's called the hidden unit
some hidden state
um so the high level architecture is
what is shown here x are the inputs
eights is the hidden state is some state
that the
a model is keeping in order to represent
a
the all the information that it has
learned from the
sequence so far and why are the outputs
is there what the
neural network will produce
and as well as we said that it's the
time step both the
hidden state eights
and um the
output are dependent of the previous
output for example as we saw in the
formula at the bottom of the slide the
hidden state is a direct um
um
[Music]
is a direct has a direct
dependency of the previous hidden state
and the output has a indirect through
the hidden state
now what is this when we say hidden
state as we said
it's a
a state a series of variables the series
of values that the model keeps
and um
it quickly it
basically represents the summary of um
what the model has learned so far and
here is a task relevant aspects of the
fast for example um
if the model is getting temperatures and
getting temperatures every hour at every
hour the state will keep a
representation of what it has learned
about the distribution of the
temperatures previously
this summer is usually lossy that means
that we lose information because the
hidden state is usually is a vector that
means it is a
fixed length vector and it and we try to
represent with this fixed leg vector and
our battery length sequence for example
at each
at its um um state at a time say we have
a vector of a hundred and we keep if our
input is um a millions of years before a
large
a previous sequence okay
now
uh any questions so far about the high
level overview of this recurrent neural
networks
now how this um
how this
a sequential awareness can be a
can be a achieved the is it can be
achieved with different ways um one of
the and this is what i call here the
different rnn variants um there is a one
variant that is quite popular is that um
i did some state
at its times as type
the
model is producing an output
and they have
a
connections recurrent connections
between hidden units and i'll show you
an example here say that for example um
the hidden unit that i have in my circle
it is used to produce the output at that
time t and also it's getting information
from the previous hidden unit and that's
where the recurrence happens like it
lands from the past through the hidden
state
um and in this um
models the input sequence and the output
sequence is um the same because as we
said that at its time then a
neural network is producing an output
and also
this this kind of rna variant is a
turing complete formula model okay
now another a way of um creating this
recurrence is that the hidden state
is um depending is the learning
information from the pre from the past
only through the output for example as i
said i'll see so here in the assume that
we have this um hidden state within the
circle um
it's used in order to produce it's
getting some information from a input x
is it's used to produce um the output at
the same time t and it's getting some
information from the past only through
the output of the previous step t minus
one okay
this um a variants are less powerful but
easier to train because um
at its time at each step the model is
getting all the information from things
that it's known the input and the output
and the output is known in during a
training time because
of the training data that we have okay
now another one uh which is quite common
variant that we use a lot is that
we have um the hidden states a learning
information from the past through the
previous hidden states
as we saw in the same
variant
but it's only producing output after it
has seen the whole sequence so in this
case it will be
only then a hidden state at the
end of the sequence t greek tough
cricket is used in order to produce and
the output and um
for these kinds of models are very
versatile because um the input and the
output can be of different
length
and they are usually used to summarize a
sequence and produce the fixed size
representation
of
a of what it has seen of this past of
the sequence that it has seen and this
effect size representation is used for
further processing
one
very popular
example concrete example of this kind of
architecture is what is called the uh
sequence the sequence and auto encounter
okay uh this model is um
um
is a um
is consists of the
three parts
a one is a a one a neural network what
we call the encoder that gets the input
x1 x2 x3 and through it's a hidden state
it lands
it outputs
a one vector what we just discussed and
this is what we call the context here
and then the context is used in order to
a to a produce another sequence okay and
these two sequences can have
different lengths okay
so basically
that's exactly what we say here the
encoder process the input sequence as we
saw in the example here the context is
this vector is the output that
it is produced by the final state and it
encapsulates all the information that
the model learned from the input
elements in the order that it sold them
and
the decoder is conditioned on the
context to generate an output sequence
as we saw the input the
input to the decoder is this context and
is used in order to produce some other
sequence for example an example of a
simple of this architecture could be a
translation like translation from one
language to another okay
now uh so far the
recurrent neural networks that we have
seen
they
get information only in one direction as
the as they see the sequence for example
if it is a time series based on the time
that the values are coming
but there in many applications it would
be beneficial if we can actually see a
both sides of the um sequence for
example in case of when we read the book
and we want to understand it would make
sense it it would probably be helpful to
know what the and what the text
next to the current work means
and um this is you this is this is where
a bidirectional
neural networks have been used and some
applications is handwriting recognition
must see translation um the one that the
example that i uh described named entity
resolution for example an example here
is that if you have paris hilton parish
paris opera when the model sees paris in
order to be able to
er
to understand what it is whether
it is um
the social light or the
building perishable or the foundation it
it makes sense to see that a word next
to the current word with space
and also in
secondary protein structure prediction
it's been used there okay
now the by the erection a neural net
liquid neural networks they are not a
too different they actually just have
two usually two hidden layers one that's
reading the
sequence in the one direction and the
other is reading the sequence in the
other direction
and
everything else is the same the output
is um
the two no the two hidden states its
hidden state is um a dependent directly
from this previous hidden state and the
output is um dependent both on the
hidden states in the forward layer and
the back backward layer okay
um
this is one way of doing it like another
way could be just to stack different
models uh
the
liquor neural networks but usually one
of the ways to do it any questions here
we'll stop a little bit
and the final version i think a
variation that we have is
a deep and
as we saw so far we have we saw a neural
network that has an input a hidden state
and an output but that doesn't need to
be the
the weight you can actually stack as
many hidden states as possible
and as many sorry as many hidden states
as it makes sense for your problem and
you and your computation availability
okay because more hidden states make the
model more expressive but also more
harder to compute more expensive to
compute
and
usually and now one can ask what exactly
this
more these more layers additional layers
would be helpful and although it's a
very hard
question to answer because it's very
hard to know what the model does there
is some work on specific kinds of deep
learning models to try to understand a
spec specific output but the theory here
is that
or the hypothesis is that each layer
learns a higher representation of the
previous so you might actually learn
the same temporal
concept at different levels of
granularity
now
so handling long-term dependencies as we
said
the these recurrent neural networks they
try to learn the dependencies of the
current output with um a the past and
future or and or depending future values
of the input
now
when these
sequences are large
and
when the dependencies um
the dependencies are long term that
means that the information that you need
to know in the past is too far away from
your current time or it's too far far
away in the future
then in practice this um um long this
neural networks they fail because the
gradient descent which we have seen in
the previous um talk that is the main
way of training the theme
model
it has the problems of small gradients
tend to vanish and large gradients tend
to explode what does this mean is that
the gradient descent we use in order to
train the weights to to train the model
mean the lengths and values from the
weights
but when a you have a long-term
dependency that means that your a
sequence is large or your
neural network
than folding it is long
then
once you have a small
gradient it will stay tend to varnish
that will become
zero that means then that
once it becomes zero um the gradient is
the way of updating the the weight so
the weight won't be updated so then that
means that the model will not learn
anymore because the way the specific
weight is not going to change the value
or if in the case of large
gradients in the intend to explode so
the the
model there might overflow okay
so this is one a so that means that if
you get into this situation
your model cannot learn in practice
cannot be trained so there a we have to
there has to be a way to
solve this or to recover from that and
that's where a specific neural networks
have been proposed to mitigate the issue
before that especially the lstm these
neural networks were not used because of
this region
and um the most
popular under the common is the lstm
network and the dru network and what
follows we're going to i'll try to
explain you how this is happening how
they do it
okay before continuing questions here
okay
so
uh
and now here we'll start in order to
understand we start to we will have to
get into these cells this will open
these rectangles that we have been shown
so far uh to see what it is inside these
directions basically it's some kind of
formula mathematical formula we'll try
to understand it
the simple a the vanilla rnn which is a
simple one the one that we doesn't as i
solve the issue of longest term
dependencies as we said is a formula is
a cell that gets the previous um hidden
state h t minus one the current input x
t it's some somehow
it um combines them this concatenation
that it shows there is a combination
and um the tan is used in or because the
combination is some kind of a nonlinear
function
and
the output of this
is a
is a
is used in order to produce
both the hidden state and the output
okay
now a
so they
in order now to be able to solve the to
be able to recover from the long term
dependencies problem the lstm and g are
used employ what is called gated units
so it will be a unit like this gated
that allow the network to accumulate
information over time and discard it one
is not needed anymore so when we say
gates or gated units in um
deep learning it's usually um kind of um
um
how to say
a definition of a threshold
that
it lends
when to use the input that it gets from
the layers before the stack layers or
when it just
leaves the information to pass it
through okay
so it's usually something some learnable
threshold that we learned okay
now i am uh the this uh
so
these gates
they look like this in lstm and gru and
we'll get into uh more details on the
lstm
um so you see that the cell is much more
complicated because it has more
information
it keeps more information from the one
and also
it has
a knowledge it has formulas
when to discard or keep the information
and what part of the information to keep
okay
so and i'll go through the lstm and we
try to understand so the lstm except
from the hidden unit
sorry the hidden state the input and the
output it also introduced what is called
the cell state
which is um uh you can think of it as
the representation of the global
information that it has kept but it's
the time at each
time unit okay so this is uh this is a
representation of everything i've
learned from the past
and minus the ones that i chose to
filter out in previous steps
okay
so
uh when they um so uh
and how it lands which one to um
to keep or to to maintain so it's using
the state of the previous cell
um it is um getting a combin a it is um
um
um so it is getting the input which is
the hidden state of the previous tell
and their input at the current level at
the current um time um it creates a
weighted um
no no um it's basically it creates a
um
a non-linear function of um
the within the the the input and the
previous state sorry
the previous hidden state
um this is basically this function it
represents what the uh
what is the new information that um it
is coming
to them uh to this cell to this
yes to the cell
um this uh
this is a
is um
how i say is combined with a as a cell
state of the previous cell
and
[Music]
the weights in this function it's
learning which parts of the state uh
which parts of the previous stage to
keep and what parts um
no this is just it says which parts of
the state to keep at this point okay is
it clear
i hear no yes
so the first thing the cycle that i saw
here is what we call the forget state so
it gets the previous cell state it gets
the input and the output and it creates
a function which says which part of the
cell state the previous cell state to
keep so far that's why it's called
forget state it only identifies which
part of the state the previous states to
keep
okay
then um
the input is also um
going through um to a
what is called the input logic which is
uh basically um
a
one more g one more gate
which decides which values to retain
okay so that means that
we we take again the previous hidden
state and the input
with the
take it through a learnable nonlinear
function and we identify which parts of
the
current input we we maintain
this
will be used
first to update the previous um uh cell
state so the ct minus one which has
already forgotten parts of the
input
the parts of its state that we don't
want to re we don't need anymore
will now be augmented with the
information that a the mod the model
decided is useful to keep and that is
the in the circle that i have here the
plus sign on the top
at the same time um this information um
is also producing um
a what is also a
is also a
moving through the third part of the
a logic the output gate
which is again another learnable
function and which is learning what is
the new hidden state and what's the
output that it is going to be produced
here okay
so there are basically three a different
gates or three different functions one
that
a that is um a filtering out parts of
the heat of the cell state that we don't
want to keep and the cell state we said
is the representation of the global
information that we want to keep the
second one is
the one that decides which part of the
um input which is the previous hidden
state and the current input we want to
keep and we add that to the cell state
and the last part is the part that
actually that looks more like the
vanilla rnn
which is producing the output and the
next hidden state or the hidden state of
this cell
based on the
values of the input and the
value of the hidden state of the
previous cell
okay so uh that's basically the lstm and
all these um
a complicated logic basically what it
does
is that it tries to mean to decrease the
number of the state the information that
we want to keep that means that even if
you have long term dependencies
if the
information that is between
the two dependencies is irrelevant the
lstm will forget it
like we'll discard it so it will have
much less information to keep
okay
uh okay before continuing uh so this is
concludes the lstm now if you have
questions on the lstm it is the time
to ask them
yep
okay
um
now
uh
another way which is a more a recent way
of
solving this problem of long-term
dependencies is what we call the
attention mechanism or attention
the basic premise of a vanilla then is
to press every pars every item in the
input series and keep updating the
hidden state the hidden state
at every vector we already said it
represents
all the information that the model knows
a prior to that um
state to that
times the step
so the last hidden state represents the
context of the entire sequence okay
now
again on this thing
if we see the encoder decoder
architecture that would be that the
context vector that we already discussed
has to keep
it maintains all the information about
this um um the whole sequences like the
whole sequence that we show in the
encounter
now
as we said this um a maintenance is
lossy because these sequences then input
sequences can be
um very large
and the longer the input sequence the
more difficult is for this context this
is
this vector this finite vector to k to
maintain all the information that's
needed that's where the attention comes
um and the attention first of all the
idea for the attention is that
um
at every step
a
in order to make a decision you don't
need to remember the entire previous
sequence but you just need to remember
or a future sequence depending if it's a
bi-directional
a problem but there are specific parts
of the input that are important for a a
for that particular time to make a
decision
and the attention is trying to do
exactly that like is trying to find at
every
step
what are the important parts of the
input
that are needed in order to make a good
prediction for this particular time
stamp the time step excuse me
so it creates shortcuts between the
context vector and the entire
source input so the context vector a
attempt for every
time
a and the prediction time output time is
different and the weights of these
connections are customizable for its
output element that means that a as we
said it's the the context vector is
different and
this difference is learnt during the
model training okay so in in the case of
encoder and decoder that we saw the
encoder is exactly the same as an rnn
uh we added an attention layer
uh which is um with it can be as simple
as a parameterized field is a feed
forward network
before the context factor which also
gets some information from the output
and this context vector
is um a
is this dynamic context vector
is used in order to produce the output
at the every different
at every step of the output
okay
now
a little bit more detail on this say
that um i we have the context um a we
have the cell that we the the output
cell the decoder cell
at time t
okay this gets some information from the
previous output y minus minus
and also it gets some information from
the input and this input is encoded
exactly at this context vector the ci
now this context vector
is
getting the information from the input
sequence which is the top
the top right rectangle which we call
the encoder but also is enhanced with
the attention mechanism which is a
the dense layers that we see in the
middle as we saw the
uh the attention mechanism gets in
the attention gets input both from the
encoder the actual input sequence as
well as
the s
a i minus one which is a the information
the output
[Music]
before the
the output that we get before the
current time um this is going through a
dense layer let's mlp
it is um
a normalized in order to get uh
to create
these weights are a are normalized and
then this produces the ias which is the
weights
that that for its input um
for each input cell it says how
important these weights is the important
is this information that you get from
the input plus the previous output for
this current cell and we'll see that in
um
okay here
so as we said the attention
is the dense layers which get
information both from the encoder and
the previous
output
um
it is going through a soft max a so that
it um
so that
the values are
a
sorry zero one and then these values are
used in order to
um
to decide what is the information what
is the importance of every cell
for this particular context in the
context the whole context at this time
is a weighted a
sum
of all the previous hidden states all
the
input that we got from the
encoder
okay
now this is a simple attention attack
attention mechanisms are pretty much
like this but there are variations but
uh
for the case that we discussed this is
how a at each time the output decides
what uh
how
how it gets information this way
important information for the
input sequence for all the cells in the
input sequence and how this is
a aggregated in order to create the
context for the current state
now since the attention a mechanism has
been
um
described or has been invented or has
been
a pro
used
a it has been
a
very successful and it has been
employed for a lot of um sequence stuff
sorry applications uh some of them that
they were um
that was traditionally be doing be done
were done by the um
sequence
the liquid neural networks
and the models that they are produced
other they're required neural net
networks enhanced with attention like
the one we saw
or more recently
the
regular neural networks themselves these
lstm sequence cells
are completely um
how the attention they has completely
[Music]
eliminated the need so there is this
paper transformer which has been has a
lot of um attraction that had this idea
that the
the only thing you need to know even for
the sequence
models is
you only need to have his attention a
lot of attentions and this is a
transformer
that was produced that was described in
2017 since then especially in the field
of nlp a transformer based models which
is a model that
handles sequence data only through a
different attention mechanisms has
seemed to have taken over the field and
namely the bert and the gtp variants are
once the time
dominate the field and more recently a
models like t5 where um not only where
the difference is again it is
transformer based models but in the
input you also define it describe the
task so 85 the 85 model can
homogeneously a handle diff a lot of
different tasks for example the same
model you can train it to learn both um
language understanding translation named
entity resolution by just changing the
way the
input is provided so you don't train a
different model for a different
for every task but you train the same
model homogeneously and that's where
especially the field of nlp is heading
to
okay
what is
t5
t5 is a model exactly it is basically a
bert model and bert is a trans a old
transformer based model
where um the
the idea there
now you have the the transformer models
which they've been used for a lot of
tasks for example in the case of nlp for
translation language understanding named
entity resolution but for each one one
of these tasks you have to
to train the model uh specific for that
task and you can train the model for
multiple tasks but the for each one of
these tasks you have to have a loss
function that describes this task
so far that's it t5 is changing that
it only it um
change one model that means you in the
training a process you only have one
loss function for all these different
tasks and how this is done is that
with the input you also encode the task
so the input is both your sequence and
the task that you want to put to
uh perform on the sequence and
that's why i'm saying that especially in
llp that's where the it seems like the
field is heading
but d5
has been designed for language probably
yeah yes it's nlp yes yes yes
now
having said that
that shows here that attention is taking
over it's not true lstms are still used
for two things first of all because
they're much
um they're very robust for
specific tasks they're much
simpler easier and expensive cheap
cheaper to train than bert or gbtpo
transformer
variants which are usually very large
and um
they work
okay
so and also in fields like time series
or
foreign casting they still seem to they
seem to be the model of choice
and recently especially in forecasting
forecasting is a field in a machine
learning which is
quite hard because you learn to predict
the future
we are not gods we cannot breathe so um
and it it has been one
field
where um deep learning
has
not produced them um how to say the
spectacular results that you have seen
in other fields and very simple models
like even
moving averages tended to to beat it but
very recently like in the last two or
three years lstm based models have been
shown to um
to
beat all these other ones like to become
the
lang the model of choice for that so
lstms are still
um
big players in the sequence based model
okay
now pro having seen the rns a requires
neula networks i want to summarize the
properties as we said they can
process input of any length which is
important sequence data because usually
we don't know the length um
beforehand
the model size is not increasing with
the size of the input again very
important
because um
they can be
that that enables them to be trained um
[Music]
to be able to be
a change in practice because if
it was uh increasing with the size of
the input then we will end up with huge
models that we wouldn't know what to do
with
and um
a computation considers both the
historical information that we saw
through the hidden states
the
um
the some disadvantage is that usually
the computation is low comparing to
other models like a convolution or
networks that we have seen
in previous seminar
um
they have this difficulty accessing
information and using and keeping
information from a long time ago and
that's where attention it can a
considerable alleviate that
on the other hand attention has another
thing that the astute
um
reader or
a audience would have figured out by
so far is that it is computationally
expensive it is um it is uh i am
a
and square to the number of to the input
sequence so that means that in practice
you cannot i am process too large too
too long to yes two long synthesis
and um
a cannot in general consider any future
input for the current state in except if
you go to specific models like the
bi-directional recurrent neural networks
that we saw
okay
now the application of rnns have been um
everywhere that you have
text data um has been have been employed
and
in most cases with
good success like in text summarization
generation a time series prediction a
normal detection android recognition
music composition anything where we have
seen um
sorry sequences like sentiment and
analysis which is the sequential
patterns that we discussed in the
beginning and so on and so forth
now i
before finishing the talk i also want to
say some
alternatives to rns for the models that
we discussed
first of all which i have second is the
attention based models that we already
saw um
so for specific fields like the nlp and
natural language processing they seem to
become
the language of the model of choice
uh especially if um
the data science or the organization has
the computational resources to train
them because they are notoriously um
large and hard to train
and um
recently well not recently but there is
a same
recent um a
thought process of using a convolutional
neural networks as alternatives to cnns
before the idea was the pushback was
that cnns cannot
um
they cannot
perform comparatively with rnns because
they cannot keep all these uh
long dependencies due to their um
limited kernels that they have they they
have
but there is recent work that they have
shown that um they perform comparatively
if not better than rnns and they usually
as i said they run much faster if you
can model um convolutional new if your
model is based on convolutions is much
faster than an lstm model okay
and for cnns also
there are specific
feeds for example named eddie
recognition where in industry they are
the model of choice as opposed
to lstms because they're so um
comparative they perform comparatively
they're so much faster okay
and that concludes my talk
here i'll take some any questions that
you can have and
uh
i'll wait otherwise we'll have um
a
two minutes um
recess
before i'll show you a small example of
how to use ls teams but please feel free
um
to
ask any questions you have
i will mute
so yeah
hi
can you hear me
yes
uh thank you for your talk i have a
question about the
new models that you talked about
that are based mainly on
lstm
are they using same lstm with
in addition to
other
modifications in the whole model
or they are modifying the already known
version of lstm
where
they modify in terms of how much
information they keep in the adcm
or
other modifications yeah
are you talking about the lstms for
forecasting that i said recently have
beaten
more traditional models
yeah yes exactly you said that
the
the new these models that are based on
lecm they are performing they have
good performance
for forecasting because i also said that
attention-based models are beating lstm
so that's why i asked so i think i'm uh
a good source for this there is a
um
a series of forecasting competitions
they're very
well they accept it that you can see i
think it is uh by macri lakis i'm not
sure like i can't i can follow up on
that
and um
uh now to your question when we say
elastin based models they don't change
the basic lstm cell the lstm cell is the
cell that i described in the slides it's
always the same so the model what it
says is first of all how you encode the
input what kind of input you give
and then what layers how many layers you
start are you using bi-directional lstms
or not how many layers of lstms you have
but other things do you have dropout do
you add
a mlps in the end and so on and so forth
so it doesn't change there's a basic
lstm cell
this is what we discussed but it's
changing the architecture the layer
architecture how what layers you use and
how you
combine with each other how is your
computational graph and most very
important how you encode your input
information your input okay
i see okay thank you
any other questions please feel free
okay while waiting i will start the
um
[Music]
example
so um this is a simple example
of
how to use um a
last year create a very very simple
lstm and what we want to do is time
series prediction like we get the time
series and here is stocks
and
we get we have some historical data for
the stock and we try to a produ to
predict the next um value of the time c
so if it is hourly stocks we'll see what
it is we try to predict the next
hour that the value of the stock in the
next hour given the value of the stocks
in some previous hours
okay okay can you see them
the notebook
yes
[Music]
sorry also a one thing is that uh as
part of the
a
presentation we will upload both the
presentation the uh
recording of the presentation as well as
this um the code and the data for this
um example so you can um
be recreated you'll be able to recreate
it and i write it on yourself by
yourselves
so
um
[Music]
i just want to go here so the first
thing we load the data it's a stock
now
here
the data come
from
the code that i have here the link for
you but it's basically data from them um
yahoo stocks if i'm i'm not mistaken
uh as we can see it has a different
information it has the data
and the
values at the
different information like when what is
the open um
value of the particular stock was the
close the open is at the beginning of
the day close is
um at the end of the day then trading
day and so
some other information we are going to
use the date which is what is going to
be our sequencing index
and
the close value which is the value of
the stock at the end of the trading day
okay
just to visualize the data how it looks
so
uh we had different dates to have this
talk from 2016 to 2018
[Music]
2021 i
i think let me see my
cases
okay
uh first of all for our example we'll uh
split that
we'll assume well we'll
create a test set which is that i said
we want to predict
um
and we'll split the
the other part uh the rest of the time
series for training and validation test
now
one thing here is because it is um
a time series so sequence matters and we
don't want to create some leakage any
liquids that means a situation where the
model learns know something for the
future will use us test set the values
at the end of the time series and as
training set the values
the rest of the values from the
beginning until that time okay that's
basically what the this
cell is doing here we run it
um
okay so we have um
the the the data set size is uh
1259 that means that that's how many
values we have we use the 252
last as our tester the ones that we want
to predict and the 1007 um arrest as the
training okay
so to create our training data what we
are going to do is that we are going to
uh
yes to create
sequences of some length and the length
that i'm using here let's see how much
is 60
and so
for the training data will we will chop
it using them a sliding window into a
set sequences of 60 a.m
where the we want to we will use the
uh a 60
previous values
a as our um context as our history and
we will be um
predicting the
the next one okay that's basically what
this does
so why do you scale everything between
zero and one
um
it is a good um um
practice
yeah it's not uh it's not it's not a
requirement okay
and other things that usually you do
with the stocks or
prices is you you get the logarithm of
this this is the what i was saying is
that how you represent
then
the
input
and
actually
anecdotally in
trading when people where people try to
predict stocks all these transformations
really matter because you might actually
get the transformation that for that day
is the great one and then you make money
okay
but it is not a requirement here for
example it would be a requirement if
we're having image data okay
um now we create the model which uh as
we see it has three
uh
lstm layers
lstm1 here and each one is um um
uh
is followed by a dropout droop out and
we discussed that in previous um
seminar is a form of uh regularization
so you use that to avoid
overfitting and
in the end you have a dense layer which
is getting the input from the
last dropout lstm plus dropout and is
producing one value and this is the
value that we will be using as our
prediction okay
we run that
and here we just define the model and
we'll see it it is um
in keras
[Music]
this one
so it's tensorflow but
a simple model like this
the code will look very similar
define the model to if you were doing
with pytorch okay so what is the loss
function oh we haven't put it now we put
it we just defined the model so far
okay just the model the model
architecture and what we see here is
that um we see when we the layers
and the number of parameters that we
have and all of them are trainable that
we learned because i didn't constrain
anything to fixed okay
now i'm curious that's how
um
in the next
step we define first of all what
optimizer we are going to use adam and
the loss function is a mean square error
so it's going to be the mean square
error of the predicted values versus the
actual training values that we have okay
you define it here
so run that
now and then we run it i'm going to do
something because it takes a little bit
of time i'm just going to run 10 epochs
10 box up means an epoch
in deep learning no mech latter
is
one computation where you go through all
your the entire data set
and yeah it will take some time uh and
also the other thing is that we i think
we discussed before the the poke as we
said this um
the model is going the train process
going through the hole
um a data set and the bad size is how
much every how much of the input you're
going to um
um
to update your um
your parameters okay so
an epoch usually has multiple bad sizes
okay
just form a deep learning number clutter
this one
why is it saying 60 i thought i put it
i don't know
10.
yeah yeah
so what's what size we have here one
okay
so
uh okay it will two more epochs and will
finish
and here we only use the training data
but we say how we create the training
data we chopped it using them as sliding
window
okay
now once we have the train model
okay we can use it in our test data set
that we have not touched so far once we
create it we have that's it
and
again
um
we create we do the same for the test
like we create as we saw the the model
takes
a sequences of 60 and producing the next
one and we chop the input like that
and um
we transform again and then model the
input we go into zero one because we did
the same thing for
the training
we predict it here
and we do the inverse transform because
now the model has learned to predict
zero ones so we want to
revert that to the actual values
and then to visualize the results let's
see how it will look
see that's an issue
i think we did not learn to predict it
sorry
i run it here
and yeah
so
the actual
is the red line when we predict it
is the
um
the blue line
it's not great
so any questions on the example
yeah one thing here is that especially
now for prediction especially this um
large values that you see here this is
one in the time series models are hard
to predict usually you need additional
features probably here is an event
that happened
2003 on the market was 2020 and the
stock went up
um yeah we said it already so the whole
data set was um
1200 values
the train was a thousand uh seven
and the test was 252
okay
just a simple
but the sequence that we we are using is
the model is getting a sequence of 60
history and it le and it predicts the
next
any other questions
stop
sharing here you can see if there are
any questions or in the
chat
if not that will conclude
a today's seminar and once again thank
you for
being here and uh participating in that
and
as i said the recording and the example
will be shared with you through the deep
website
thank you