welcome to meetup group as intelligence
and now this meetup we are going to
discuss about Alice team and
implementation of lsdm using them PI and
using the Jupiter multiple Ford briefly
is going to start arresting him briefly
we have I'm going brief about Aaron and
Sally then I will discuss about hella
stem cells so the briefly what is the RN
n here in RN and if a plea use for in
the making natural language processing
they have sequential sequence we aware
any sickness there for example sentence
for example language model like that
so in ordinary what is the origin here
what will the output say output and into
both are like we can say in the term of
technical like whatever t minus 1 times
tape out put and T times fed input feed
both at a time so if for in the in
example I am looking here the team untie
my t minus 1 time step this is the
activation is there and at T time step X
is the input Assyria and W a is the wet
matrix of activation a with on the tea
time t minus 1 times 2 and this is the
wet matrix for that time step T and both
are filled to the tan H functions and
that we are getting the output
activation 18 and that I take the output
activation getting feed to the softmax
concern that getting the final output
and getting the final output there there
also when landing parameters in there
that call that output weight matrix WIA
so these are the basically brief about
the RN n sale and iron and say learn
from the past
posture information but the problem is
there only there only one stay here one
is don't handle the long term dependency
don't handle the long term whatever like
for example I am going to boom by and I
will stay in the towel holder so like I
am
so I am talking I and I will stay the
village the problem is singular yeah I
can use use like that so that dependency
should be handled by the model but
ironer unable to capable this it's kind
of the dependency for that purpose Alice
team will use so Alice team briefly has
been here the Alice team here so this is
the Alice teams are here in the Alice
teams here this the upper line you find
here a paradigm is the memory cell by
yourself and myself and the below this
line called the previous 15 stage and XT
is the current input at time step T so
what advantage here we are in advantage
there are the three gate is there one is
the forget head and after gate and
output rate and what fall gate will do
it with the forget yeah
deselect the whatever information we
have to prevent to the next memory say
and updating gate it to the select
whatever coming the information up to
the for forgetting and it will the
update and then next thing next is the
finally the output gate output gate
whatever final output we have there
getting that for you and they are the
lot of learning parameter is there in
the equation you will find there this is
the expression of the for gate gate this
forget gate why sigmoid EW yeah because
we have to select a dissident we have to
how to log 0 to 1 here 0 1 like that for
that we are using sigmoid here the
sigmoid all of the like WI freedom is
governor
forget get wet matrix and singular W no
knob did get wet matrix and this is the
updating cell gate met wet matrix at the
t minus one and this is the steady state
t minus 1 CT is the steady state t minus
1 so these all of that equation is
especially for that are STM cell and
finally we are getting the equation
using the dot product of how to be gate
and 10h activations that is the N power
to our believe the output so the s
optimize concern well we have we have to
feed a tee time 88 tuition at times T to
the search machine well gating times now
output of time step T so these are all
of the information about that long-term
saw long short-term memory that call is
Atlas TM networks so now we are going I
am going to explain it as a coding power
how to implement this tile STM forward
cell so for that you know for example
the sentence is that I am going to the
bombe and I will stay in touch so this
is the sentence to lengthen for example
length of the sentence of the TX TX is
the length of the sentence that call the
time step T so that time step T States
but the complete time step first to be I
have to build for one time I have to
build a feed for a forward cell for the
one time through for the one x times
where it's the fourth Alice team self
forward that is called like only the for
one time step that is the cell is there
so we have to like according to discuss
purely I will pass current time step and
previous activation that is RN and I
already explained and and one more
parameter is there here previous memory
cell whatever PS memory your memory
state is there that is the previous
memory and that is the parameter so what
they parameter like your weight matrix
of object rate matrix like for bias and
update with my drinks by update bias
weight and weight matrix for the first
weight might be
by switches for the first 10 eyes and
our weight matrix of the output gate and
weight matrix for the relating the
hidden state without put these all of
the parameters we have to define that is
the parameters and X is the input time
strip the T and April the hidden state
are the times t minus 1 and see the
memory stable at that t minus 1 so they
are there
all of these are we are getting as a
input of the function parameters and
first for this four people whatever you
are getting like input XT is the input
now I'm going to tell about how we can
write a equation using the lamp I write
using numpy in the numpy you will get
the function like sigmoid right like an
pedo Tana is like that right to sigmoid
function that's about how we can get
Sigma points and there is our enemy
utility is there already then you can
use for the sigmoid function those
sigmoid function is there and for dot
product NP dot dot is there Tokerau MP
dot a ot he R and P todo dia dot work
for a pen H also and pedo tenets un pedo
tennis for tennis out there okay now I
come back to that explanations so here
this line what you think you right now
you're thinking about we're parameter we
are getting so just just talk the
information I'm how I generate the
parameter here I'm calling the less less
team sail forward and I create a matrix
so you create a dictionary of that these
parameters right and that parameter
randomly are generated as it example
only for the example be howdy we can
send it kilometers and how can we pass
for the one time step so this is for
this parameter coming over here XT just
XT a random three three into ten size of
inputs input sequence and it it provides
what is the previous just what
definition purposes what is deafness of
opposed previous this render handsome
like
so this how we can pass it according to
this we can pass right so now I'm going
to explain against whatever safe coming
that XD coming from about so that exit
actually coming from our X denote shape
you are getting annex person and same WY
coming from here so what WI WI don't do
whatever say we are getting saved to
your ass right now we are concat both a
previous hidden state at t minus 1 and
input at T so that is the funda right
give you whatever past information plus
current input weed-free to the network
that learn from the past and current
thoughts so that so we are concat it and
then we pass con Kate Plus that update
so this is therefore forget gate this is
for update gate this is for our trading
sale and and using that we multiply out
both we multiply both are gotting
according to this any we are you know
operating gate and tennis-wear multiply
both and feed and summation of their
whatever up to forget gate we are
getting the output right the dot product
to obtain its operating cell or updating
Gators so here here both are we are
doing here like this is an 10 tenets of
CCT updating that is operating cell and
the water will be the C next is
multiplication of the operating cell
updating cell and see preview in
multiplication of the operating gate and
updating cell right so both are using
for that I am sure here like both a
summation of both like forget gate and
CT but dot product of this and dot
product of this and summation of this
theta we are using C next to this is the
see next the senior she next is dot
product this and dot product of this so
here you are getting here what is she
next is dot in a multiplication of this
multiplication of this
this is for for
getting it and previous history this is
for updating it and updating sail and
finally we are getting the output output
is this is the output this output so
this is the sigmoid or output and
finally find our output activation
eighty we are getting dot product off
that input tenets or this and and that
then a next we have to pass to the
sigmoid function as optimist ones and
for getting the final output and all of
these whatever you are getting
information like NFC next a previous
estate to currently state operating
speed we pass through the case for they
used for the next from the next purpose
and we have to read an hour of this
information so in the in this tutorial I
am explaining I explained here you how
we can create a lsdm sale
yeah how can implement ALS theme for the
one time step and for the forward
propagation only and similarly for them
for example this before the one time
spec if you want to run it for the TX
time spent so this is the example for
running TX time time so we have to build
it we create a less team for when X is
the input computable complete input of
TX and initialize parameter or whatever
parameter we are passing right and here
we are calling here a listing sail
forward whatever written previously no
so that we are I am I am calling here
and the that is the folder for complete
time step T X and we are appending all
of the sentence that information so this
is the I am explaining in the brief like
how forward cell will work and in the
next tutorial I will explain how back
propagation of the Alice train will work
so thanks for watching this video thank
you so much thank you