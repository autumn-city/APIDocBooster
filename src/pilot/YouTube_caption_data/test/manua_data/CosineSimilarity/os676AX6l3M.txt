friends welcome to my channel. Dhanesh 
here . a chatbot is a conversational agent  
capable of answering user queries 
in the form of text speech  
or via a graphical user interface . in simple 
words a chatbot is a software application that  
can chat with a user on any topic . chatbots 
can be broadly categorized into two types . task  
oriented chatbots and general purpose chatbots 
. the task-oriented chatbots are designed to  
perform specific tasks . for instance a task 
oriented chatbot can answer queries related to  
train reservation , pizza delivery . it can 
also work as a personal medical therapist  
or personal assistant . on the other hand general 
purpose chat boards can have open-ended discussion  
with the users . there is also a third 
type of chat bots , called hybrid chat bots  
that can engage in both task oriented and 
open-ended discussion with the users .
there are two approaches for chatbot development 
. one is learning based chat bot and rule-based  
chat bot . learning based chat bots are the type 
of chat bots that use machine learning techniques  
and a data set to learn to generate a response 
to user queries . learning based chat bots can be  
further divided into two categories . retrieval 
based chatbots and generative chat bots .  
rule-based chat bots are pretty straightforward as 
compared to learning-based chat bots . there are  
a specific set of rules . if the user query 
matches any rule , the answer to the query  
is generated , otherwise the user is notified 
that the answer to user query doesn't exist .  
so we are doing the rule-based 
chatbot development with python . 
that's what we are going to do in this 
implementation . i am using cosine similarity  
algorithm and the tfidfvectorizer . cosine 
similarity . cosine similarity is the cosine  
of the angle between two vectors . see in math 
, quantities can be classified into scalars and  
vectors . scalars are physical quantities having 
only magnitude . vectors are physical quantities  
having both magnitude and direction . you are 
aware of these things . because we learn it in  
math . see when we consider vectors , the scalars 
obey the ordinary rules of algebra like addition  
multiplication . scalars will follow the same 
rules of algebra . if i am giving example for  
scalars . mass , time these are all scalars . 
vectors one classic example i can give you is  
force . force is an example . at that time we have 
to tell the direction as well . when you consider  
force or electric field or magnetic field you 
need to consider the direction as well . such  
quantities we call it as vectors . and one 
important point is vectors won't obey the  
rules of algebra like addition , multiplication 
. how scalars can be added . for example 5 kg  
mass plus 5 kg mass it is 10 . 5 + 5 = 10 . but 
vectors it is like 5 newton plus 5 newton can be  
10 . can be 0 . or some other value . so vectors 
won't follow the ordinary rules of algebra . for  
vector multiplication there are two types of 
rules we follow . one is the scalar product .  
for vector multiplication . and another one is 
the vector product . so this is scalar product and  
vector product . these two are for the vector 
multiplication . for vectors . the cosine  
similarity is coming from the scalar product . 
so i will discuss it . how it is . this cosine  
similarity we are defining from the scalar 
product of vectors . i am not discussing about  
the vector product here . cosine similarity is 
the cosine of the angle between two vectors . it  
is the product of two vectors divided by the 
product of the two vector's magnitude . that  
means how we are defining scalar product . 
it is like a dot b . this is the formula . a  
vector . a dot product we will denote dot . for 
cross product or vector product you will put a  
cross here . in between a and b . so a dot b 
is equal to a into b into cos theta . so this  
is the formula we use here . a dot b is equal 
to a b cos theta . see what is a this ? vector  
a . what is this this ? is vector b . what 
is 'a' here magnitude of vector a . what is b  
here magnitude of vector b . what is theta here 
? theta is the angle between vector a and vector  
b . theta is the angle between vector a and vector 
b . so a dot b is equal to a b cos theta . so from  
here you can write cos theta is equal to a dot b 
divided by a b . that is cos theta . that is the  
dot product divided by a b . so that's what it is 
written here . the cosine similarity is the cosine  
of the angle between two vectors . it is the dot 
product of two vectors divided by the product of  
the two vector's magnitude . the cosine similarity 
algorithm was developed by the uh you know neo4j  
lab team and it is not officially supported . the 
closer the cosine value to 1 the smaller the angle  
and greater the match between vectors . that means 
we know the values of cos . see cos 0 is equal to  
1 . and cos 90 is equal to 0 . you know in math 
we learn it cos 0 is equal to 1 and cos 90 equal  
to 0 . that means the closer the cosine value to 
1 that means the smaller the angle , and the match  
between the two vectors . cosine similarity 
is generally used as a metric for measuring  
distance when the magnitude of the vectors does 
not matter . that is very important . when the  
magnitude of the vectors does not matter . this 
happens for example when working with text data  
represented by word counts . so these things we 
can use it in nlp , when we work with text data .  
where it used ? cosine similarity is generally 
used as a metric for metric for measuring the  
distance when the magnitude of the vectors does 
not matter . this happens for example when working  
with text data represented by word counts . that 
means text data means when we are working with nlp  
we use this . a commonly used approach to match 
match similar documents is based on counting  
the maximum number of common words between 
the documents . but this approach has an  
inherent flaw . that is as the size of the 
document increases the number of common words  
tends to increase even if the document talk about 
different topics . the cosine similarity helps  
overcome this fundamental flaw in the count the 
common words or Euclidean distance approach.
this is the math behind cosine similarity . i 
already discussed . cos theta is equal to a dot  
b divided by modulus of a into modulus of b . this 
one when we use a . see any vector . see vector  
a . if its coordinate . so this is a vector 
o a . if it coordinates is (x , y , z) . o  
a can be represented as x i plus y j plus 
z k . this is vector a . in the same way  
vector b can also be represented . then a dot b 
is equal to you know uh if this is x 1 y 1 and z  
1 and vector b is equal to x 2 i plus y 2 j plus 
is a z2 k then a dot b is equal to x 1 x 2 plus  
that is the product so this term you will get it 
as a dot b is equal to that is the formula you use  
a dot b is equal to x 1 x 2 plus y 1 y 2 plus ez 
1 z 2 . this is the formula for dot product if it  
is in this form .in you know unit vectors. 
i j k are the unit vectors . i is the unit  
vector along x axis j is the unit vector along 
y axis and k is the unit vector along z axis  
so vector a is equal to x 1 i plus y 1 j plus z1 
k and vector b is equal to x two i plus y two j  
plus z2 k. then a dot b is equal to x one x two 
plus y one y two plus z1z2 . this is a dot b . so  
that value can substitute . what is modulus of a 
? see modulus of a vector here the mathematical  
formula is root of x 1 square plus y 1 square plus 
z 1 square . that is the modulus of vector a . in  
the same way dot modulus of vector b is equal to 
root of x 2 square plus y 2 square plus is it is  
a z2 square . so that is modulus. in in terms of 
x and y so from that if we are generalizing this  
we can write it in this form sigma of 1 to n a i 
b i plus sigma of 1 to n a i square plus 1 to n  
bi square that is a dot b so that is the math of 
cosine similarity mathematically it measures the  
cosine of the angle between two vectors projected 
in a multi-dimensional space in this context  
the two vectors i am talking about are arrays 
containing the word counts of the two documents .  
algorithm we call it as word frequency 
algorithm as well tf idf algorithm  
or word frequency algorithm tfidf stands for 
term frequency inverse document frequency  
this is a technique to quantify a word in 
documents we generally compute a weight to each  
word which signifies the importance of the word in 
the document and corpus this method is widely used  
it is a widely used technique in 
information retrieval and text mining  
and it is easy for us to understand the sentence 
as we know the semantics of the words and the  
sentence the computer can understand any data only 
in the form of numerical value so for this reason  
we vectorize all of the text so that the 
computer can understand the text better
now we will see the see these are all the terms 
we this is the math of tfidf ctf idf that is  
that is equal to this is the equation you need 
to understand tf idf is equal to term frequency  
into inverse document frequency term frequency 
is tf i will i will tell you what is term  
frequency we will be discussing what is inverse 
document frequency idf we are going to discuss  
ct the letters we are using t means term or 
word term or d means document or set of words  
n means count of corpus and corpus 
means the total document set  
these are all the letters we 
use in the coming discussion  
so the t of idf the formula for tf idf is the 
term frequency into inverse document frequency idf  
uh let's get into try to understand term frequency 
this is very simple see this measures the  
frequency of a word in a document term frequency 
measures the frequency of a word in a document  
when we are vectorizing the documents we 
check for each word's count in worst case  
if the terms doesn't exist in the document 
then that particular tf value will be 0  
and in other extreme case if all the words 
in the document are same then it will be 1.  
the final value of the normalized tf value will 
be in the range of 0 to 1. now term frequency tf  
is how often a word appears seen in the simplest 
way we can define term frequency tf is how often  
a word appears in a document divided by how 
many words there are for example the formula for  
term frequency is number of times term or word t 
appears in the document the whole divided by total  
number of terms in the document for example you 
have a document with 1000 words in that document  
and you want to find out the term frequency of 
the word you know pen you want to find out the  
term frequency of the word pen so pen is repeated 
how many times in the document let us assume that  
it is repeated 10 times if it is repeated 10 
times how do you calculate it is 10 divided by  
1000 it is 10 divided by 1000 is the answer see 
what is 10 here for example you are calculating  
the term frequency of the word pen this is 
the word p-e-n the term frequency of the word  
pen in the document the word pen repeats 10 
times number of times the word pen comes in  
the document is 10. the total number of terms 
in the document is thousand then what is the  
term frequency it is one this is equal to 1 by 
100 this is the way you calculate it this is term  
frequency now i will discuss document frequency 
this is rarely you use and i will explain what  
is document frequency as well this measures the 
importance of document in whole set of corpus  
this is very similar to the term frequency the 
only difference is that term free tf is frequency  
counter for a term t in a document d whereas df 
document frequency is the count of occurrence of  
term t in the document set in that means df is 
the number of documents in which the word is  
present so that is another term you will use 
in this algorithm that is document frequency
see another important point this is really 
important you need to understand here is  
the inverse document frequency see 
what is inverse document frequency  
see we already discussed term frequency is how 
common a word is how common the word appears in  
a document that is term frequency inverse 
document frequency idf is how unique or  
rare a word is that is inverse document frequency 
the equation for inverse document frequency is  
logarithm of total number of documents the whole 
divided by number of documents with term t init  
that is the formula see you have 
the log of total number of documents  
see logarithm of it's a log to the base it 
is not underscore it is log to the sorry
it is the formula is log to the base  
e here then total number of documents divided 
by the number of documents with term t in it  
this is uh the inverse document frequency 
for example if you have 10 documents
see you have the total number of documents with 
you is 10 this is 10 number of documents with term  
t in it will be you know you can see let us assume 
you know more easier way i will then it would be  
more see let's assume that we have 1000 documents 
the number of documents with the term t init is
we have only 10 so you will take logarithm of 
1000 divided by 10 logarithm of 1000 divided by 10  
see what is logarithm of thousand by 10 that is 
log off thousand by ten is hundred right what  
is logarithm of hundred it is two so two is the 
you can take it as the inverse document frequency  
inverse document frequency is how unique or 
rare the word is start the development first  
i will import the required libraries let me do 
that let me run this i have imported nltk numpy 
random all these libraries i have imported and 
i am going to use a beautiful soup 4 library  
to parse the data from wikipedia and furthermore 
you know python's regular expression library re  
will be used for some you know pre-processing 
tasks on the text next step i am going to do is  
the creating the corpus as we said earlier we will 
use the wikipedia article on tennis to create our  
corpus the following script you know the script 
i am going to write or retrieve the wikipedia  
article and extracts all the paragraphs from the 
article text finally the text is converted into  
the lower case for easier processing let me do the 
script let me write it so this is the script uh  
you know for the you know for creating the corpus 
see let me first run this let me yeah it's working  
fine if you see this url  , this is the input 
to our chat bot input in the sense from this only  
it learns the you know text and gives the output 
so this article this is the input to our chatbot
it takes this data from the wikipedia and 
process it fine next step i am going to do is t  
text pre-processing and i will be writing a helper 
function for that so we need to pre-process our  
text to remove all the spatial character sorry 
all the special characters and empty spaces from  
our text if you are familiar with the nlp 
life cycle this is the step next we need to  
follow that is the text pre-processing let me 
write the script for me run this yeah it's working
so we need to divide our text 
into sentence and words since it's  
since the cosine similarity of the user input 
will actually be compared with each sentence  
so for that you know we are 
going to execute the next script
fine it's working now we need to create a helper 
function that will remove the punctuations from  
the user into text and will also lemmatize 
the text lemmatization roughly refers to  
reducing a word to its root form you may be 
familiar with the stemming and lemmatization
lemmatization is more accurate compared to 
stemming for instance lemmatization we uh  
lemmatization of the word ate returns 
eat the word throwing will become  
throw and the word worse will be reduced to 
bad so like that so let's write the script for
let me run the script
so in the script we first instantiate 
the word lemmatizer from nltk library  
next we define a function performance 
lemmatization which takes a list of  
words as input and lemmatize the corresponding 
lemmatized list of words the punctuation  
underscore removal list removes the punctuation 
from the past text finally the get underscore pre  
sorry get underscore processed text correct get 
underscore process to text method takes a sentence  
as input tokenize it lemmatize it 
and then removes the punctuation  
from the sentences now how our chatbot responds to 
greetings see since we are developing a rule-based  
chatbot we need to handle different types of user 
inputs in a different manner for instance for  
greetings we will define a dedicated function 
to handle greetings we will create two lists  
greetings underscore inputs and greeting 
underscore outputs when a user enters a greeting  
we will try to search it in the greetings 
underscore inputs list if the greeting is found we  
will randomly choose a response from the greetings 
underscore outputs list let me write the script
here you can see the script greetings underscore 
inputs and greetings underscore responses  
and i have written a definite sorry a function 
generate underscore greeting underscore response  
as i discussed here the greetings sorry generate 
underscore greeting underscore response method is  
basically responsible for validating the greeting 
message and generating the corresponding response  
now how the how we will respond to user queries 
how the chat bot responds to user queries as  
that's what i am going to discuss so here we are 
using two algorithms one is the tfidf vectorizer  
and the cosine similarity as we said earlier 
the response will be generated based upon the  
cosine similarity of the vectorized form of the 
input sentence and the sentences in the corp in  
the corpora once again this is very important the 
response will be generated based upon the cosine  
similarity of the vectorized form of the input 
sentence and the sentences in the corpora the  
following scripts i am going to write imports 
the dfidf vectorizer and the cosine similarity  
functions let me do that let me run this see from 
sklearn i have imported the tfidf vectorizer and  
the cosine similarity now we have everything 
set up that we need to generate response to the  
user queries related to tennis we will create a 
method that takes in user input finds the cosine  
similarity of the user input and compares it with 
the sentences in the corpus let me write that  
method the generate underscore response method 
accepts one parameter which is user input next  
we define an empty string tennis robo underscore 
response we then append the user input to the  
list of already existing sentences after that 
uh the following lines right word underscore  
vectorizer so those lines uh will we initialize 
the tfid effect riser and then convert all the  
sentences in the corpus along with the input 
sentence into their corresponding vectorized form
and of the line you can see here similar 
underscore vector underscore values that  
is equal to cosine similarity so that what it is 
doing is we use the cosine similarity function to  
find the cosine similarity between the last item 
in the all underscore word underscore vectors list  
which is actually the word vector for the user 
input since it was appended at the end and the  
word vectors for all the sentences in the context 
similar underscore sentence underscore number  
that is we sort the list containing the cosine 
similarities of the vectors the second last item  
in the list will actually have the highest cosine 
after sorting with the user input the last last  
item is the user input itself therefore we did 
not select that finally we flattened the retrieved  
cosine similarity and check if the similarity is 
equal to 0 or not you may be familiar with the  
flattening it will convert the spatial dimension 
into channel dimension that it is it convert  
the two dimensional matrices into one dimensional 
vectors if cosine similarity of the matched vector  
is zero that means our query did not have an 
answer in that case we will simply print that we  
do not understand the user query otherwise if the 
cosine similarity is not equal to 0 that means  
we found a sentence similar to the input in our 
corpus in that case we will just pass the index  
of the matched sentence to our article underscore 
sentences list that contains the collection of all  
sentences so that's what this function is doing 
i have explained you clearly now let me run this  
fine it's it's working fine now 
how do we chat with the chat bot  
so as a final step we need to create a 
function that allows us to chat with the  
chat bot that we just designed to do so we 
will write another helper function that will  
keep executing until the user types bye. code i 
will explain after you know writing the script  
let me write the script we will first set 
the flag continue underscore dialog to true  
after that we print a welcome message to the 
user asking for any input next we initialize a  
while loop that keeps executing until the continue 
underscore dialog flag is true inside the loop the  
user input is received which is then converted to 
lower keys the user input is stored in the human  
underscore text variable you can see in 
the code if the user enters the word bye  
the continue underscore dialog is set to false 
and goodbye message is printed to the user  
on the other hand if the input text is not 
equal to bye it is checked if the input contains  
words like thanks thank you etc or not if such 
words are found a reply most welcome is generated  
otherwise if the user input is not equal to none 
the generate underscore response method is called  
which fetches the user response based 
on the cosine similarity as i explained  
once the response is generated the user input 
is removed from the collection of sentences  
since we do not want the user input to be 
part of the corpus the process continues  
until the user types bye you can see you know 
why this type of chatbot is called a rule-based  
chatbot there are plenty of rules to follow 
and if we want to add more functionalities  
to the chatbot we will we will have 
to add more rules and it's easy here
anyway let me run this subscript i 
didn't run it fine yeah so you can see  
see hello i am your friend tennis robo 
you can ask any questions regarding tennis  
i am asking uh who is uh 
federer f-e-d-r-e-r fine enter
yeah roger federer is considered by many observers 
to have the most complete game in modern tennis  
next question i am asking now who is
nadal let me see what's the answer yeah tennis 
robo is telling nadal is regarded as regarded  
as the a greatest clay court player of all time see 
when i typed by goodbye and take care of yourself  
so after that there is no 
further you know dialog box  
appeared thanks for watching please 
like share and subscribe thanks a lot