(upbeat music)
- Hi, and welcome to the tutorial series
on Distributed Training with PyTorch.
My name is Suraj,
and I'm an AI Developer
Advocate at Meta Open Source.
In this video series,
we are gonna look at a paradigm
called "Distributed Data Parallel"
or DDP for short.
Before we start,
a quick note on why we would like
to use Distributed Training.
The quick and short answer is
that it saves us time.
When we use more machines than one,
we increase the amount of
compute we have at hand,
and that helps us train
our models much faster.
And, moreover, with models just
becoming larger and larger,
it can be difficult to fit a single model
on a single GPU.
And this is also where
distributed training
via model parallel,
or FSDP,
comes into play.
If all of this sounds
somewhat unfamiliar to you,
fear not.
We are gonna start
with a very simple training job
that can run on your local machine,
perhaps, on your laptop,
and we are gonna see step-by-step
how that can scale up,
first to use multiple
GPUs on the same machine,
and then also to use multiple
machines on a cluster.
In these examples,
I'll be using a very simple model,
so that we can focus
on the distributed training aspects
of the code.
And once we have those down,
we'll take a look at how DDP can be used
to train a large language model,
like the GPT from OpenAI
to train really fast.
In the first video,
we'll take a look at
what DDP is doing
under the hood,
so that we get an intuitive sense.
That's gonna just make the rest
of the series easier to follow.
I'm also gonna be hands-on keyboard.
So if you'd like to follow along
feel free to clone this repository.
The links are below in the description.
(upbeat music)