" And  having batch norm before the activation, that's usually how  yeah, that was originally how it was proposed in the paper. "
" That  means it may be that we need fewer epochs to get the same  loss that we would achieve if we don't use batch norm.  So you  usually with batch norm, the networks train faster. "
" And this is  essentially it's in that way, step one of batch norm is  similar to the input standardization. "
"For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.  However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)]."
"BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice. "
"The BatchNorm1d normally comes before the ReLU, and the bias is redundant also "
When evaluating your model use [model.eval() (hyper-link)] before and [model.train() (hyper-link)] after. 
" But  sometimes people also, nowadays, it's even more common to have  it after the activation. "
" So  I know, instead of here having it before the activation, I now  have it after the activation in both cases. "
" And yeah, one little fun memory aid to remember that  is, if you consider this case, so you have batch norm, then  you have the activation and then you have dropout, you may call  it bad, it might be better to have batch norm after the  activation, that's typically a little bit more common. "
" So let's say you have the Google  search engine, and there's just one user running a query, and  you have a network that has batch norm.  So you have to  normalize, but you don't have a batch of users.  So there are two  ways to deal with that scenario, the easy one would be to use a  global training set mean and variance.  So you would compute  these means for the features and the variances for the features  for the whole training set. "
 That's something you would also  usually do or could do when you compute the input standardization. 
" The same with batch  norms, instead of using batch norm, one D, which we used  earlier, when we talked about multi layer perceptrons of fully  connected layers, for the convolution layers, we use batch  norm 2d shown here. "
" So if n is my batch size here, we have an input that  is two dimensional, it is n times m, where let's say, m is  the number of features. "
" So we had if we  had three features, we had three gammas and three betas.  Now, we  extend this concept here to the two dimensional case where we  compute these four inputs that are four dimensional, right,  because we have now the batch size, we have the channels, we  have the height, and we have the width.  So we compute the batch  norm now, over the number of inputs height and width.  So in  that sense, we, we combine these. "
It depends on your ordering of dimensions. 
Add the model.eval() before you fill in your data.  This can solve the problem.
(I don't think PyTorch has a way to automatically do this for you.) 
