[Chair] Our next speaker is Kristian Kersting, 
who's been doing wonderful work on probabilistic  
programming and its intersection with logic 
programming. [Kristian] Thanks. So actually I  
have to complain, because first of all I learned 
a lot yesterday and I was not expecting so much  
learning, but I'm really complaining to 
give a talk after Stuart [Russell] and  
Josh [Tenenbaum] because what can I do now? 
I mean, I'm screwed. But I also want to thank  
you for that, because I was never expecting to 
be in one session with these two guys and also  
with you Frank [Wood], so it's really a big, 
big honor for me. So I hope I'm not completely  
disappointing. Actually in the program I had a 
different title but yesterday I thought I should  
change somehow. So my idea is to talk a little 
bit about probabilistic programming but also  
maybe showing that there are still some issues 
to be solved. But there's also a lot of done,  
stuff has been done already to do that. So 
yesterday for example, we heard a lot about  
just imperative probabilistic programming, but 
now imagine that you get all this fancy data  
that is typically stored in a relational database 
and one of these examples are electronic health  
records where we did a lot. And now you have to 
condition on the relational database you have,  
right? So you're a medic. You're asking what 
is the prob...what is the probability you  
have a certain disease given the whole electronic 
health record of a person. So yesterday I got the  
impression also during the poster session that 
people feel like there is structural learning,  
that it's so hard. I fully agree that structural 
learning is hard but you can still do it. And  
so what we were doing a lot and maybe we can also 
turn that into probabilistic programming languages  
that are more imperative, so we were following a 
boosting approach, a functional gradient boosting  
approach, where you're not trying to learn a 
single line of your code one after the other but  
you're actually trying to learn several of these 
rules always a little step, keep going like that.  
So what you see here is then part of a model 
that was learned on the cardio health studies,  
so it's about detecting or predicting heart attack 
risk. It completely makes sense, so for example  
the first decision is about whether you're male or 
female. We know then from the standard literature  
in the medical literature that females have lower 
risk of heart attacks. So the sub-tree for females  
is much less complicated than for males. But then 
actually when you marry you lose this advantage.  
Anyhow there's a lot of interesting statistics 
going on but I wanted to point out this,  
that with a little bit of gradient boosting 
you can actually learn these kinds of models  
on an electronic health record on a full database 
within nine seconds. So just want to let you know,  
I'm not saying we are solving structural learning 
here, Stuart [Russell] was pointing out a lot of  
interesting issues there, Josh [Tenenbaum] was 
pointing out super interesting problems, but I  
just want to say it's not that there's nothing. So 
you can do something and it's not taking ages. And  
if you would like to know more about that, because 
that's what I learned yesterday in the session on  
unheard voices, so I'm not pretending that we 
are unheard voices but I just want to say there  
are tons of tutorials here - our NIPS tutorial but 
there are others out there on all these kinds of  
techniques and there are even books, so you can 
also check Lise Getoor's and Ben Taskar's book,  
but you can also check our ones, and if you want 
to try out there are even toolboxes out where  
you can do that in Java but also in Python. So 
it's super easy to integrate and you also have  
natural language interfaces to put the human 
into the loop in an easy way. So in a sense  
we can say probabilistic programming is great. 
But after we did all this work I was wondering,  
okay but now what happens if I want to feed this 
database into a support vector machine, right? So  
we hear a lot about probablistic programming but 
I think we have to get the whole toolbox of AI  
and machine learning involved. The main problem 
is that you can't do it in the traditional way.  
Actually I don't know how many of you are still 
working with support vector machines. You don't  
like actually to work with this standard algebraic 
view because if you work with a database you have  
to think about indexing and it's getting really 
hard to do that. So what we did is we developed  
a more declarative language for mathematical 
programming for linear and quadratic programs.  
Here you can almost directly translate the 
paper form that you saw here into program code, 
and then the underlying compiler is taking care 
of indexing and everything. So you still have  
somehow at least a shallow semantics of what is 
the meaning of single symbols, and then actually  
you can try to make use of the fact that typically 
you have a lot of data but your model is rather  
small so you're expecting some form of symmetries 
in there. So maybe we can speed up and this is  
what Stuart [Russell] was talking about and while 
he is fully correct if you look at the theory of  
lifted inference then trickilly they tried to get 
an exponential reduction in a sense. So wherever  
you take exponential time they would like to scale 
linearly. If you're a little bit more relaxed,  
and I'm more relaxed there at least, and you're 
just interested in getting some speed up, if  
you can speed up Grobe by 50% I'm happy already, 
right? So if you do that, then you can actually  
show that you can run a rather simple algorithm 
that is known from the 70s and maybe even earlier  
going back to Tarjan, to figure out what is called 
a fractional automorphism of your mathematical  
program, and then you can collapse automatically. 
And this can give you speed up in terms of  
modeling. So here you see on a very simple link 
prediction task that instead of publishing 10 or  
20 papers, where you always go for very specific 
graph kernels, you can just add these two lines  
of constraints saying, okay if two papers are 
citing each other they should be on the same  
side of the hyperplane, if you want to classify 
them. Just two lines of codes and you're on par  
with all these very specific and hand-tailored 
collective classification approaches. On the  
other hand you see here that noise is not a 
problem, might not be a problem. So typically  
people say if you put a little bit of epsilon on 
top of your data you're destroying all symmetries,  
and that's true. But you can develop, now that you 
have an algebraic understanding, you can develop  
approximate lifting up approaches.So this is very 
much related to clustering in a sense, and then  
for SVM's for example you can develop based on 
that PAC learnability result so you get guarantees  
on the future performance. Additionally, you 
get speed ups, right? So here we are just doing  
data augmentation, but then when you take your 
image and you rotate it a bit and you rotate it  
again and so on and so on, you're now computing 
approximate symmetries in this huge data space  
you have there. And that's done automatically and 
you get a speed-up now over all of 300 almost 400  
times, but still getting the same result as the 
original SVM. Because it's algebraic it should  
extend to deep learning... there's actually some 
nice papers now out from the CMU guys. So again,  
if you want to try out, this is all implemented 
and this lifting, this, I haven't explained how  
to do that but it's a standard algorithm and 
it's taking just quasi-linear time. So why were  
we interested in that? Well with that you know, 
there was a lot of hype going on on how can we  
exploit symmetries in algorithm a? How can we 
exploit symmetries in algorithm B? and so on,  
and so on. So here we were now able to prove in 
a principled fashion that all of these algorithms  
that are based on linear programs or quadratic 
programs can make use of symmetries. And it  
takes only quasi-linear time to do so and this 
quasi-linear time is really important because  
if you try to tackle a a hard problem, then this 
little overhead doesn't hurt very much, right?  
I'm happy to spend one second additionally if 
overall the inference would take ten minutes. So  
that's pretty, pretty nice. So as said, our main 
motivation was to get a general understanding and  
just show you can do that in general. Now I really 
would like to tell you that I think this is only  
the tip of the iceberg. What is happening right 
now is that there's a lot of new interest in,  
why should we only look at sparse matrices?Why 
can't we make use of all the cool data structures  
out there developed in other the sub-disciplines 
of AI, and make use of them for optimization? So  
here you see one idea.If you're any how deal 
with a probabilistic program, most likely you  
can easily compute a path tree for your formulas. 
Now with that you can easily build what we, what  
is called an algebraic decision diagram, or you 
can even go for affine versions of them and more  
extended versions. Now you would like to code your 
optimizer like in an interior point solver using  
these data structures instead of sparse matrices. 
So imagine you have a matrix full of 1s - in a  
sparse matrix you're screwed, right? I mean of 
course you can substract and keep the 1 in mind,  
but otherwise you're screwed because there's 
no 0. In the algebraic decision diagrams you  
get that for free and everything collapsed to 
a single node. Now the only problem is that you  
can't apply the standard technique, so if you 
go for a Cholesky factorization, for example,  
the decision diagram explodes again. So you have 
to think a little bit and check out literature and  
there's a lot of interesting stuff on matrix free 
optimization, where you never materialize your  
whole matrix. If you use those guys together with 
these decision diagrams, you can get speed up of  
five times for free. So I'm just saying, old code 
- our old algorithm - with a new implementation,  
you get speed ups. So I think, again we can 
say probabilistic programming is great. But  
yesterday and also to be honest when checking many 
of the posters, and maybe also including our ones,  
I feel really bad because I'm not sure that I am 
so much into statistics that I can really talk  
to you. It's really really hard I think if you're 
not an expert and you don't want to push the next  
NIPS papers, but you would like to implement that 
in a company, if you don't like Poissons and you  
don't like genralized gammas and you don't like 
distributions. So Josh was already referring to  
the statistician, to the automatic statistician, 
super cool stuff, but I think they mainly focus  
too much for now on regression. So our idea was 
"can we have something like the automatic Judea  
Pearl in a sense." Can we have a system that 
is coming up with a graphical model if you  
have tabular data? So, of course the idea is, 
can we make use of deep learning? Can we put it  
into the stack? And here's one problem, at least 
for me, that standard neural networks are not  
really faithful probabilistic models. What we did 
here is we trained on MNIST, and then we evaluate  
on other data sets, but you can use your favorite 
image data set. So you can have dogs and cats and  
then you show a tree, and the system will tell you 
"yeah, it's a cat". Right? That's the main issue,  
because it can't tell you "I haven't seen that." 
Can we do that somehow differently? And if you  
look at, there's a long tradition going back 
to Adnan Darwiche and maybe even earlier on  
arithmetic circuits and and variants of that, 
where you also have very much like in TensorFlow  
and in deep learning, the idea that you come 
up with a computational graph and now tailor  
towards probabilities. So your activation 
functions are essentially plus and product,  
right? Sum and product. So very simple, you just 
put your input in here and then you compute,  
so different is only that you typically get only 
one output which is the probability of your joint  
state there. But otherwise everything is very 
much the same and you can even do structure  
learning in a very simple way by making use of, 
for example, nonparametric independency tests,  
if you want to be agnostic to your distribution. 
And then you always try to split random variables  
into independent groups. If you can't find 
independencies anymore you do clustering or  
random splits to do a local conditioning and 
then hopefully in the next iteration you can  
find again some independencies. So now still 
although simple, it takes a lot of time doing  
the structure learning. So what we did this year 
is to come up with a random forest idea but now  
for sum product networks, which is essentially 
building these different region graphs in a sense,  
randomly in a way that you can still make use of 
tractable inference. And what is happening here  
now is that you can run the same experiment and 
then you see on MNIST you get this probability  
mass only for MNIST, and for the other ones of 
course we still get a probability but people will  
tell you or the system will tell you, "hmm, I have 
never seen this kind of pixel arrangement so I'm  
a little bit unsure and I don't want to tell you 
with high probability that I've seen that." Here  
you see that you can then automatically compute 
outliers and so on. And this is all naturally  
built-in right? It comes for free. You can also 
do now learning with nonparametric independency  
tests if you now do the base distribution in your 
leaves also in a nonparametric fashion. Here for  
example you just use histograms, very simple. You 
can also learn on data sets where at least I don't  
know the distributions, so that's the Alzheimers 
data set and I'm not sure what is the distribution  
of satisfaction at work. I mean I'm satisfied so 
maybe it's a big probability mass on being happy,  
I don't know, but in general I don't know the 
distribution there really. In particular if  
you don't take also time into account, and so on. 
So it's getting really tricky but then generally  
it's still a problem because you would like to 
know it's a Gaussian, it's a Poisson, it's a  
generalized gamma, or whatever. So that's why we 
now also combined the sum product networks with  
more traditional and Bayesian models. Particularly 
the one by Zubin and Isabel on trying to identify  
automatically the statistical type of a random 
variable, and because they're both probabilistic  
modes you can easily combine them and you get a 
system that can decide at least in the language  
of distributions you're interested in or a family 
of languages you're interested in, decide on what  
is the most likely distribution use there. The 
model can also tell you the confidence. It can  
tell you "well I think it's a Gaussian, up to a 
certain probability", and "I think it's a Poisson,  
up to a certain probability". Then because it's 
anyhow super trivial, keep in mind inference is  
linear in the network, you can also easily build 
up now grammars that turn your probabilistic model  
there into text describing what is interesting, 
how much variance is explained by which subpart of  
your network... you can compute an intractable way 
many explainable techniques. So you get this now,  
what we call deep notebooks. So in a sense instead 
of starting with an empty Python notebook you  
can start with a pre-programmed by the machine 
notebook, so that you understand already a little  
bit your data, which is typically quite important. 
So again if you want to try out, you can download  
the code. I also would like to emphasize that 
this is all by compilation. So I should ask Stuart  
whether this is the form of compilation he had 
a mind for Brian, but it's really that you code  
because whether you have a symbolic representation 
or you put it into C code that's just a different  
language, right? So you really compile into C code 
or you compile into specific languages for example  
FPGAs, and for at least our models we are up to 
ten times faster than TensorFlow, which is maybe  
because we're having rather sparse models that 
are higher up in the hierarchy. I'm just saying  
maybe we can do much more with FPGAs. Second, 
I would like to emphasize that I think we can  
have deep learning that can quantify in a natural 
way their uncertainties. And just to illustrate,  
of course we are not there, we are one group, but 
I hope we can encourage you guys. But just to show  
you that we can do similar stuff that you know 
from deep learning, on the left hand side you see  
now pixel SPNs not pixel CNN's right? And SPNs can 
also generate images of course he has still the  
gray images and we will add color soon, it's just 
a question of computation. And on the other hand  
you see attend, infer, repeat models where you can 
speed up just by using SPNs. It's about, I don't  
know, 10 times faster. So what is the future? 
I think we should really work more on symmetry  
aware deep probabilistic learning. I think we 
should, which I discussed not with you Stuart,  
but with [someone else] that I think we should 
have open universe mathematical programming. I'm  
wondering if we talk to financial people, they do 
it wrong right? They always assume that they know  
the population and I think that would be super 
super interesting having strong interactions with  
statistical mathematical programming.At the poster 
you can talk to Carl, one of my students, on sum  
product probabilistic programming, at least the 
idea that we have there. With that I would like  
to thank you and again I'm complaining and I hope 
I have not bored you to death. I just wanted to  
remind you guys that all this business is really 
making money. Companies are putting millions into  
that and and I don't like that it's always only 
said deep learning was generating so much money  
with 500 million or whatever the numbers were. 
With DeepMind we are in a similar situation,  
so I can only encourage everyone and to just enjoy 
and push this field. It's amazing and so thank you  
for organizing this conference. If you are...ah, 
wait one minute. The final message - sometimes  
it's hard to publish papers because you need a 
right, high diverse background in in judging our  
papers, so that's why we started a new journal as 
an add-on to the standard machine learning and AI  
journals. And that's with the frontiers you can 
think about frontiers whatever you want to think,  
but anyhow i can tell you it's an easier way 
of getting maybe some papers accepted in a  
particular reproducibility which i think is 
really important, we should play a major role  
there. Thank you guys. [Audience applause] 
[Chair] Okay. We have time for one question  
and then we're gonna make a brief schedule 
change to accommodate the rest of the day.  
Question? Yes. [Question 1] Really great talk. 
Could you provide a bit of a roadmap for SP Flow,  
in your mind, how are you thinking of the project? 
It's a really exciting project. [Kristian] So I'm  
not sure whether officially I'm supposed to 
tell you that, but we are currently trying  
to join forces with Uber and other people, so 
right now the debate is more like, so we will  
also compile... we have it already but it's not 
pushed to PyTorch, so we would like to have a  
more general domain-specific language. So I think 
it's happening more, but I'm a little bit more  
like what other people told me, I think we first 
have to show in some applications that it's really  
worth doing it and then the rest is coming. But 
yeah we are standing on their shoulders hopefully  
of some other companies, yeah. [Chair] Thanks so 
much, Kristian. [Kristian] Thank you. [Applause]