Tanh seems maybe slower than ReLU for many of the given examples, but produces more natural looking fits for the data using only linear inputs, as you describe.  For [example a circle (hyper-link)] vs a [square/hexagon thing (hyper-link)].
Most of time tanh is quickly converge than sigmoid and logistic function, and performs better accuracy [[1] (hyper-link)]. 
A good neuron unit should be bounded, easily differentiable, monotonic (good for convex optimization) and easy to handle.  If you consider these qualities, then I believe you can use ReLU in place of the tanh function since they are very good alternatives of each other.
Update in attempt to appease commenters: based purely on observation, rather than the theory that is covered above, Tanh and ReLU activation functions are more performant than sigmoid. 
For example, try limiting the number of features to force logic into network nodes in XOR and [sigmoid rarely succeeds (hyper-link)] whereas [Tanh (hyper-link)] and [ReLU (hyper-link)] have more success. 
