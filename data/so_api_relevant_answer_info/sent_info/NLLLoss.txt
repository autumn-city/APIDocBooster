Your input shape to the loss function is (N, d, C) = (256, 4, 1181) and your target shape is (N, d) = (256, 4), however, according to the docs on [NLLLoss (hyper-link)] the input should be (N, C, d) for a target of (N, d).
Alternatively since NLLLoss is just averaging all the responses anyway, you avoid creating copies of data by just reshaping x and y into (N*d, C) and (N*d) tensors and get the same result:
I agree with you that the documentation for nn.NLLLoss() is far from ideal, but I think we can clarify your problem here, firstly, by clarifying that "class" is often used as a synonym of "category" in a Machine Learning context.
Fortunately, PyTorch's nn.NLLLoss does this automatically for you.
So, if I'm reading your question correctly, you would calculate LogSoftmax, and then feed that into NLLLoss and also exponentiate that to use in your filtering.
Yes, NLLLoss takes log-probabilities (log(softmax(x))) as input.
Because if you add a nn.LogSoftmax (or F.log_softmax) as the final layer of your model's output, you can easily get the probabilities using torch.exp(output), and in order to get cross-entropy loss, you can directly use nn.NLLLoss.
There is no log in nn.NLLLoss.
nn.CrossEntropyLoss() combines nn.LogSoftmax() (log(softmax(x))) and nn.NLLLoss() in one single class.
