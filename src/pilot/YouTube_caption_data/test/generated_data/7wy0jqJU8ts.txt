so I'm Josh from Google and today we're
gonna do a getting started with
tensorflow
2.0 just fall I'm setting up could you
give me a favor and raise your hand if
you're taking a machine learning class
can be academic or online awesome having
a deep learning class half okay so I
know a couple people haven't taken a
machine learning class I will assume but
most have so I'll assume that you're
familiar with machine learning but
you're relatively new to deep learning
we'll come back to that all right so
let's see can everyone talked about
tensorflow to here's what we're gonna
try and get through as much as we can so
I will talk for like five minutes and
then I'll stop talking and you can do a
quick exercise and as always we're gonna
start with a mist because why not
after that we'll look at a convolution I
have a lot more to say about convolution
because I find it much more interesting
when you're learning deep learning
oftentimes you'll start by looking at
these ridiculous pictures of these fully
connected deep neural networks with a
stack of dense layers and personally
when I see something like that I have
approximately zero intuition for how and
why they work so that sucks and it's
horrible but if you look surprisingly at
a convolutional neural network which
sounds much fancier I find it much more
intuitive there's deep learning is
unfortunately concepts OOP and the state
of deep learning right now is that you
can write a deep neural network in five
minutes I'll show you how to do that
actually less than five minutes but it
can take you know six plus months to
really get familiar with all the
concepts but this is a good thing
and it means you can spend more of your
time thinking and less of your time
messing around with the frameworks so
that's great and then assuming we get
through that I will talk through some
more advanced stuff some my favorite
examples deep cream style transfer time
series stuff like that all right
so I will come back to actually let me
just talk about what deep learning is so
here's a picture that I pulled from
Wikipedia where we are and I ran it
through our latest tutorial for a deep
dream and this is that photo deep dream
of fied so if you take a look at this
for a second
what do you see occurring in the deep
dream of flight oh and then I'll explain
how this relates to the fundamental
ideas behind deep learning I know this
is a bit of a random aside but I wanted
to start by talking about something a
little bit more interesting than M inist
so what do you see in this photograph
that wasn't here before yeah there's a
beaver yeah there's lots of cute little
beavers and there's Quadra eyed beavers
there's sheep dogs eyes everywhere
peacocks snakes stuff like that so deep
learning is representation learning and
let me explain what that means when I
started studying machine learning the
models that I learned about were
decision trees and I absolutely love
decision trees because if you train a
tree and you ask the question how is it
that this tree is classifying a piece of
data you can print out the tree and read
the rules awesome really really
important so for those of you who have
taken a machine learning class think
about what would happen if you tried to
classify a photograph like this using a
decision tree the features that the tree
can look at are going to be the pixels
and so that means if you're the root
node in the tree you'll find whatever
pixel in your training set happens to be
the most informative to split the data
and you'll ask a really silly question
you'll say like if pixel intensity is
greater than 128
then ask about the next pixel intensity
and on a thousand by thousand by three
image three because there's three color
channels red green and blue you have
three million features none of which are
really informative at all and so if you
think about how wide and how deep the
decision tree that you might trained to
classify in images it's useless it
doesn't mean anything what deep learning
does is you basically I'm gonna fast
forward like 80 slides just to show the
idea that I wanted to talk about here
what deep learning in a nutshell is why
I like talking about convolution this is
what deep learning does so basically the
way I like to think of it is what we're
looking at here is this is a deep
convolutional neural network and we'll
come back to this but the way I like to
think about deep learning is there's two
parts the first part is what you see on
the bottom
here and this is what you would cover in
a machine learning class and here this
is a schematic for multi-class logistic
regression what each of these little
cubes represents is a feature and if we
were working with raw image data these
features would be pixels and they're
fully connected to an output layer and
you can imagine maybe this output node
or neuron is collecting evidence that
it's a cat it's a dog it's a sheep
whatever so this is just a multi-class
logistic regression unit what the deep
in deep learning does is the
convolutional base that we're looking at
above this what the base does it's a
series of convolutional layers in this
case or a series of dense layers which I
hate in other cases and what they're
doing is they're looking at the raw raw
pixels from the input image and they're
extracting features as they go so the
purpose of the first layer is to
transform pixels to any edges the second
layer from edges to textures textures to
more complex textures and so on and so
forth
what this means is that by the time
you're training logistic regression
model it's no longer taking the pixels
as input instead these are high level
features and their high level features
that were automatically learned from the
data so deep learning learns a
representation of the data that you can
classify with a linear layer in a
nutshell so other words for deep
learning or automatic feature
engineering or representation learning
and unlike in traditional machine
learning where 10 years ago you might
have come up with features like shapes
and textures using a library like open
CV or you would have written a whole
bunch of giant Python pre-processing
scripts you can learn all these features
automatically and the reason I'm going
to flip back like 50 slides which you
should never do in a presentation so
flipping back 50 slides
the reason deep dream is interesting the
reason we can modify this image to make
all these psychedelic shapes appear is
because we begin with an image
classifier and this is an experiment
where we're asking the classifier to
show us the type of features that has
learned from data but we'll come back to
that anyway tensorflow is an open source
machine learning library for the
purposes of your research there are many
awesome open source machine learning
libraries the truth is learning one is
hard after you've learned one learning
multiple gets easier what's nice about
tensorflow 2 which is what I work on and
I'll talk about today is you can very
very roughly think of tensorflow 2 as
chaos plus pi torch plus a lot of other
awesome stuff and the reason I teach
tensor flow in my classes at night is
not because I work for Google it's
because when students learn how to use
tensor flow it's easy for them to branch
to wherever so it's a good place to
start here's some resources for you
we're like a few weeks away from
releasing tensor flow - it's in beta
right now
our website is tensorflow tat org slash
beta and you should skip everything else
on the website that's not there for news
and updates we have a blog in it Twitter
I'll share these slides afterwards by
the way so you don't have to write
everything down and then what I wanted
to mention - we're gonna use Python
today but machine learning is very
rapidly branching out beyond Python and
I was totally wrong about this when I
was first introduced to this idea I'm
gonna pointed at you and I'm probably
gonna accidentally unplug things but if
I manage not to screw this up this
should be really cool you are now
immortalized on the video as well future
generations of tensorflow students will
anyway so all this is running
client-side in the browser so nothing is
being sent to a server which is a big
deal and you'll notice so a that was
private B that was fast
and that's running in JavaScript and the
basic idea there is that a model was
trained in Python using tensorflow to
converted to a JavaScript format using
something called tensorflow J s and then
deployed in a webpage and there's other
things you can do to this I believe only
works for one person but here we're
getting a part map so there's a lot of
value in running things in the browser I
just wanted to mention that as an FYI
it's not just Python anymore I'm gonna
blaze through this also FYI another
thing you can do is you can train models
in Python and deploy them on iOS Android
raspberry pi embedded devices whatever
I don't have a slide for this but let me
just tell you briefly tensorflow 2 is a
C++ engine by the time you write your
code in Python what happens is behind
the scenes that code is accelerated by
C++ the reason this is important is it's
easier to write your code in Python but
also a lot of the time when you run your
model you don't want to run it using a
Python interpreter you might want to run
it on a phone or in a browser and so
tensorflow gives you a way to save your
models in a machine independent format
that lets you deploy it where you want
that's valuable anyway this is the
picture that I hate but what we're
looking at here is this is a fully
connected deep neural network and we're
looking at a series of three dense
layers I'll break this down in a bit
each dense layer is taking a linear
combination of the input features and
some weights and applying a
non-linearity and forwards that result
to the next layer instead of looking at
this because no one has the intuition
for that I want to give you intuition
for what a single dense layer does so
the data set that I want to talk about
is M mist and if we google éminence
really quickly and this is a it's an old
computer vision data set it's the hello
world of deep learning there's 60,000
images of digits the important thing
about these is they're 28 by 28 by 1 so
they're black and white digits what I
want to show you is what a dense layer
does one dense layer if you train it to
classify M this digit and this is fast
but what we're looking at here
that's our cartoon dense layer at the
top those would be the pixels from a
single image that we're feeding through
the network each of the gray lines
represents a weight and each of the
green nodes represents an output let's
imagine that we've trained this dense
layer for a long time on all 60,000
digits and now we ask the question what
is it that the weights are doing that
lets us classify the images what we're
looking at here is a visualization of
the learned weights
so there's this is a fully connected
layer which means there's one wait for
every input pixel so every pixel in the
image would be connected to one wait
this guy here would be the upper left
input pixel and the reason that's an
array is you can imagine that dense
layers can only take a raises input so
we've unstacked the rows of the image
and lined it up into an array so this
might be pixel 1 pixel to pixel 3 and
what we're doing here is if I'm sorry
the weight 4 pixel 1 pixel to pixel 3
what we're doing here is we've colored
the weights so if a weight is very high
we've colored it in red and if a fade
weight is very low we've colored it in
blue and if you visualize the weights
you see this red band around the output
for the 0 and that's because there's
many different ways to draw zeros but
most people don't cross the centre of
the image which is blue and so what a
single dense layer is doing for every
input feature it's basically assigning
one weight that says if this feature is
present how much evidence does that give
me that it corresponds to the output
class so a single dense layer is that's
fast but a single dense layer is
something we can interpret in terms of
writing the code let's let me stop
talking and I'll give you an exercise
and let me show you one more thing
before we do that I just want to show
you how to write a deep neural network
in tensor flow to in like two seconds
just so you know
this code right here we're defining a
model we're adding a single dense layer
that would correspond to that diagram
right there if we wanted to go from a
dense layer to a neural network we would
add one line of code like this and now
we have a neural network we've added a
second dense layer and that's a hidden
layer if we wanted a deep neural network
we would copy and paste this and now we
have a deep neural network and now you
will have a deeper neural network and so
on and so forth
so what I'm trying to communicate here
is this part there's like six months of
learning here like I said earlier maybe
faster for you but it took me a while to
go through all these concepts when you
look at this I see not a lot of code and
I see a lot of things first of all let
me give you some terminology there's
different ways to define deep neural
networks in tensorflow - this is the
simplest one and here we're saying our
network is a stack of layers that's what
it means by sequential this is great 95%
of the time as it happens dense layers
can only take a raises input flatten is
a special layer it's a pre-processing
layer it basically says give me an image
and I will unroll it into an array so
the output of this layer is an array
great terminology this is the depth of
the network the number of the layers
that we've added the rough intuition
you'll see this with convolution but
roughly the more layers you have the
more combinations of features you can
detect so maybe pixels come in edges
textures and then you have logistic
regression and classify it based on the
textures it's discovered you also have
the width of the network and that's the
number of units or neurons per layer
here we've pulled 128 out of a hat the
more units per layer the more patterns
you can detect at that layer great
lots of hyper parameters here designing
neural networks for a problem is a bit
of an art and over time you learn
basically good starting points so I've
looked at M this for a long time so I
know from experience what designs might
unfortunately we often have to search
around a little bit and to be honest
it's pretty hacky how people do it if
you're working on a more substantial
problem usually the way you get a
starting design is you find a paper
that's close use that as a starting
point modify from there there's more
hyper parameters - there's things like
the type of activation function which
we'll talk about later good news ray
lose almost always the one you want here
what we're saying is well whatever
softmax is a fancy way of saying give me
a probability distribution so the output
of some network the way to read this you
start by looking at only two things the
first is the input so this network is
going to take some data that's 28 by 28
so in a a square is going in in this
case an image and the thing that's
coming out you can ignore all this junk
in the middle the thing that's coming
out is going to be ten numbers all of
which range between zero and one and
they sum to one so basically this means
give me a probability distribution based
on some input data all right let me give
you an exercise and explain how to run
it just you know so tensorflow is open
source you can definitely install it on
your laptop it's great today we're gonna
run it in the cloud just to save time
and we're gonna use one of my favorite
tools which is called collab does anyone
seen a collab half okay we're just gonna
use collab if you've any collab
questions i'm happy to help let's start
this right now so on your laptop what
you should do
is
I have two exercises for you and the
first one is what I recommend you start
with if you've been working with
tensorflow for a long time I have an
advanced exercise which I'll show you
right after this but if you're new to it
you should do this please go to this
link and what this will do is this will
connect you to our hello world tutorial
for M mist it's it's close to the
minimum amount of code you need to write
an image classifier and I'll put this
slide back actually no you're gonna need
this because I have to have a second
slide up in a sec so definitely write
this one down or go to a bitly slash
amnesty - s seq seq for sequential
let me show you this has a long link
you're gonna need this as a reference
I'm gonna bring it up on my screen and
show you how to get to it so you don't
have to write down this long link if you
go to tensorflow tat org slash beta then
you go to machine learning basics
classify images so this is tensorflow
tat org slash beta machine learning
basics classify images this is a good
reference if you want to learn more
about em missed and I'm gonna i haven't
described it to save some time but if
you need reading this is much more
detail on what you're about to do so
classify images and let me show you what
I want you to get to
what our beginner tutorial is missing is
this diagram and you're going to you
adness so a blessing of deep neural
networks is that if you create a deep
enough Network and the layers are wide
enough and you train it for long enough
it will memorize pretty much any data
set and this is great they're very
powerful we don't want to memorize the
training data though what you usually
need to do is get high accuracy on the
validation set so the way there's a key
parameter that you need to set when you
are training your networks of all the
hyper parameters meaning like how many
layers what's the width of the layer the
one that you really need to get right is
this one at the very end it's a box and
roughly this corresponds to how long
you're training the model for an epoch
means you've used every example from the
training set wants to update your
weights so here we're using them all
five times if this number is too large
you will over fit to the training set if
it's too small you'll under fit so to
set it properly it's not rocket science
usually what we do is we make plots like
this and here we're plotting our law
well here we're plotting our accuracy on
the training set and the validation set
over time so at boxes on the x-axis
accuracy is on the Y if we set a pox to
like 20 probably the accuracy on the
training set is going to hit 1 but what
will happen you'll notice that the
accuracy on the validation set it's
going to start to drop and the goal is
basically the correct value for the
number of epochs you'd want to stop
training this thing when the accuracy on
the validation set begins to decrease
because that means you're beginning to
overfit so what you're gonna do go to
bitly /m this - seq add plots for the
training and validation accuracy and
loss and then find the right number of
epochs to train that model for and here
is the code I'm giving you the code that
you can use so you can start modifying
that example with code like this and try
and get those plots and then find the
right number of epochs and why don't we
work on that for I'll start talking
again at 4:05 so 15 minutes
if you've been working with tensorflow -
for a long time or tensorflow one for a
long time I'll put that back in a sec
here's a more advanced exercise whoops
nice well you can see the answer but try
not to see the answer and look at this
later we read each guy a couple days ago
in Macau and so it's a bitly / / and my
friend had a typo here's a bizarre go
link or bitly link bitly / i jcav
underscore a DV and that's a more
advanced exercise where you write some
pieces of a neural network from scratch
so if you want the events one each CAV
underscore a DV and let me put back the
beginner one which you should probably
start with bitly / end this - seq and
here's some code that you can you can
use and then if anyone has any questions
please raise your hand and I'll come
around there are one or two people new
to collab it might be distracting for a
lot of you but I'm happy to I can give a
quick intro to collab while you're
working on this quick intro to collab
anybody
awesome okay and if you have any
questions in any of this please please
raise your hand another thing you can
try if you add the plots quickly the
next task that a lot of you're gonna
care about is how accurate of a model
can you train on em missed without
overfitting on the validation set and
the way to train a more accurate model
is to add more dense layers or to
increase the width of the dense layers
you have that will give you more
capacity but the larger your model the
more likely you are to over fit and
you'll see that there's layers you can
play with like drop out and things like
that that you can read about or I'll
talk about a little bit okay I'll keep
talking we can keep working on this in a
little bit so tensorflow - first of all
here's how you install the thing if
you're not working in collab basically
what I want to mention right now is
wallets in beta it's important to get a
named release so if you want to install
the latest beta here it is just FYI
although in collab if you used it last
time you can able you can enable a GPU
with the edit notebook settings if you
want to enable a GPU you also need to
install the GPU version of tensorflow -
good practice while we're working on
upgrading at the top of your scripts
just print out what version of
tensorflow you have just to make sure
things are working so here's the first
difference of tensorflow - in tensorflow
one and let me explain what the name
means so a tensor is a fancy word for an
array so a scalars a tensor an array is
a tensor a matrix is a tensor a cube is
a tensor so tensors in a ray flow refers
to a data flow graph under the hood in
C++ a data flow graph is built for your
program compiled and executed in tensor
flow - you don't need to be aware of
that or see it unless very rarely you
care about it so with tensor flow -
installed what we're doing is we're
creating two constants and one and two
and we're going to add them together and
if we print this out you'll see one plus
two is three as you would expect the
shape is saying it's a tuple
it has a data type roughly tensorflow to
works like numpy um instead of numpy and
D arrays we have tensor flow tensors the
main difference is the tensor flow
tensor can be accelerated on a GPU and
we can backprop through it let me show
you how this is different from tensor
flow one actually before I get the
tensor flow one here's how this starts
being useful in tensor flow - in the
last exercise you poked around briefly
with dense layers here what I've done is
I've just written some code and I've
been ported a dense layer and I've got
the setting in such a way that the
behavior is very simple and I know what
it's do and then I'm creating some data
and I'm forwarding the data through the
dense layer and you can see just by
running this in Python exactly what the
result is so this is a great way to poke
around and exactly understand the
behavior of your layers very easily so
this is really useful also tensor flow
tensors if you get tired of tensor flow
they have a dot numpy method so you can
switch back from tensors to numpy and
tensor flow operations will work with
numpy and d arrays and numpy operations
will work with tensor flow tensors so
they're close friends and that should
work most of the time tensor flow one
was sadly different so here we're gonna
try and add some numbers again and this
won't work as expected
so in tensor flow one this was a long
time ago in 2015 and we've basically
tensor flow one is what you would have
wanted if you were an engineer at a very
large software company and your problem
was how can I do massively distributed
deep learning and tensor flow one you
build a data flow graph and you need to
be aware of what that graph is and then
you run the graph and here if we make
those constants again and we print Z we
don't get three instead what we get is Z
prints out to be this add operation and
that's an operating on some data flow
graph to actually do the addition you
had to make a session and then in the
session you would execute this should
say Z naught X you would run Z and this
added a lot of mental
overhead so this is gone
it works imperative Lee by default which
is great if you want to make so I'm
gonna skip forward like 50 million
slides again and I'm just gonna cut
right to it and show you the one line of
Python you need in tensorflow to to make
your code run fast and in graph mode
the only piece of code you need to know
in tensorflow - that doesn't look like
regular Python is going to be a single
Python annotation so here's some code
i've created some random LS TM cell this
is just Python in tensorflow - I'm
making some data
I'm calling the cell and I have some
crappy benchmark to see how long that
takes to run to accelerate that I can
add a single line which is at TF dot
function and let me explain what this
does in this example which is old by the
way it made it like nine times faster
that will be probably slower now but it
makes it much faster here's how this
works
so the reason we're using a C++ back-end
as Python is slow at multiplying
matrices this is why numpy is so popular
you write your code in Python and numpy
the matrices are multiplied on see the
results go back to Python you get 100x
or 10x feed up depending what you're
doing awesome one problem with the
tensorflow program or numpy program is
you're going from python to numpy I'm
sorry python is b python to c python to
c so your ping-ponging back and forth
between these environments so you get
latency if you're a compilers engineer
which i'm not there's other things you
can do to accelerate programs if you can
look at the whole program at once you
can compile it you can prune pieces that
aren't used anyway
what TF function basically says and it's
applied recursively just take any code
that appears in this block send it to
the back end all at once the back end
compiles it does it's magic does the
math delivers the result once so you run
the whole code and C and then you get
the result once so it saves you from the
ping-pong and it can do some tricks so
that's it
so tensorflow 2 is python plus at TF
function and anything you can stick and
TF function you can stick in a save
model that will run on devices without a
Python interpreter so that's good news
while we're here in case you do
distributed training down the road
distributed trading in tensorflow 2 is
also much much easier so basically
here's ignoring the indentation mistakes
here's something that looks very similar
to that little emne smote 'l we looked
at a second ago to run this on one
machine with multiple GPUs it's just
this
so we have different distribution
strategies the way it works is you
create every strategy has scope ignore
indentation problems oh no it's not I'm
just tired from jet lag create your
model inside the scope compile it when
you do fit this will do data parallelism
which is the easiest way to do
distributed training what I should tell
you is that this is the easy part so
wrapping your model in a scope and
there's scopes for different context
this is not hard you just have to read
about the scopes and figure out what
they are what's hard is your input
pipeline so the bottleneck is basically
going to be reading data off disk and
getting it on to the GPUs fast enough
that they're not starving
which means sitting around waiting for
data and that's getting easier let me
just show you what that involves right
now when you imported the data set in
this little hello world example we use
these Kaos data sets and Karos is a
wonderful library of talk not sec it's
built in a tensorflow - it has a lot of
small data sets that you can import in
memory almost always your data sets are
not going to be in memory they're going
to be sitting on disk in tensorflow -
the way you get a data set off disk and
on to the GPUs quickly is you use
something called TF data and briefly the
best tutorial that we have that you can
check out right now and don't do this
now but for the future just say of
reference we're working on cleaning
these up load and pre process data and
strangely images is the one in my
experience that's the ones you want even
if you're not working with images but
let me explain how this works that it's
not images
TF data is a tool to build input
pipelines
the way it basically works is this
so the first thing I want to show you is
that tensorflow - the way to think about
it is none pipe so if you have an umpire
/ a ssin like NPS um you can usually
find an equivalent tensorflow - there's
some other stuff - like we have image
modules with things for like loading
images off disk and resizing them and
decoding them and doing stuff like that
so there's different utility modules but
here we have some code that takes some
image and decodes it into a JPEG and
does some math on it whatever so there's
some tensor flow to code tensor flow
data is a tool that you can use to build
data pipelines out of these so basically
you can start constructing a data set
and here you can say things like a data
set is a stream of data TF data has
different operations that you can apply
that stream that are useful so here
we're saying shuffle the data and maybe
later we'll say batch the data and
repeat the data set for a long time and
then there's these interesting tools
like prefetch and this is something you
can do with TF data that you can't do
easily with num pipe so what prefetch is
trying to say is get the next batch of
data onto the GPU so it's there when it
finishes processing the current batch so
you don't have latency and there's all
sorts of fancy tricks like this TL DR TF
data is useful it's a bit of a hassle
and it's complex but if you're doing
larger experiments it's worth using and
worth worth learning alright let's see
all right so Karros is built into
tensorflow tube and it's a huge part of
sensor flow too careless is a separate
library let me explain what this is if
you go to Kerala oh this is one of my
favorite all-time libraries next to
scikit-learn it's a deep learning
library it's wonderful
and what Karos does it's basically an
API without an implementation so Karos
defines different ways of defining deep
neural networks and everything at Kara
Co works in tensorflow to Kharis defines
- is sequential and a functional
sequential is for building a stack of
layers functionals for building a graph
what Karos doesn't say and you've seen
these things dense and sequential Karos
doesn't say anything about how you
actually run this code on a GPU if you
do if you do pip install Karos you get
what you get a chaos that i/o and
automatically behind the scenes
caris will install what it calls as a
tensor processing library so it will
install tensorflow or MX net or c TK
call that library to do the math you
never see it in tensorflow - Karros is
built-in and tensorflow - is a superset
of what you get at Kerr osteo so if
tensorflow - is installed you can say
from tensorflow doc eros import whatever
you want and any code you find at Kerr
osteo will work identically in tensor
flow - just by changing an import so
instead of import chaos from tensor flow
import chaos and that's it so if you're
new to this stuff
Karos is famous for being one of the
easiest to use libraries in the
best-documented it's a perfectly good
place to start learning nothing you
learn at Karis thought i/o is a waste of
your time because it all works
identically in tensor flow - just so you
know collab has Karis installed also by
default so you have to be careful with
your imports if you're importing things
from care us and you see using tensor
flow back-end that's a mistake you don't
want that you just want to get your
imports from tensor flow care us
I put some notes on the slides for you
and when I upload them you can get them
all right so you've seen sequential
models so that's a stack of layers that
by the way existed in tensorflow one
it's the same in tensorflow two it
didn't change at all functional models
are what you would use to build a model
it's a dag and so if you start learning
about things like residual networks and
things when you've skipped connections
between layers you can define them using
the functional API there's a third
method that I'll talk more about which
is the sub classing API and this feels a
little bit like object-oriented numpy
this is very very similar to a library
called chainer and similar to a library
called pi torch and the way this works
is here we're defining our model by
extending a class and this class happens
to be model it's provided by the library
but you can write your own if you don't
like this one and what we're doing is in
the constructor we're defining a couple
of layers and in the call method we're
defining the forward pass of our model
so or our layer so if you call this
model on some data you can see that the
data will pass through the dense layer
if you're curious exactly what the
output is you can just print that out
because it's just Python this is really
really great from the research side if
you're defining new layers and stuff
like that so you can interactively see
how they work all three of these model
Styles can be trained in two ways one
way is model dot fit which you've seen
which you should always use unless you
need to write custom code the way to
write a custom training loop in
tensorflow too and we'll do this in a
second with linear regression is called
a gradient tape so here what we're doing
is we're creating our model and then all
these models are trained by gradient
descent the way we get the gradients is
back propagation the way tensorflow will
give you the gradients for the weights
in your model it's using something
called a tape what we start doing is we
record all the operations under this
with block on a tape it builds a
computational graph plays the graph
backwards day at the gradients but
basically what's happening is we're
calling the model on some images we're
getting the output of the model we're
computing our loss and then we're
getting the gradients with respect to
the loss of all the variables in the
model and if you print these out these
are your gradients if
doing research in optimization and
you're working on like Rachel optimizer
of the jaj optimizer you can rewrite
this however you want if you were doing
just SGD you would multiply the
gradients by a learning rate and update
your model variables so or if you're
doing gradient clipping it's really easy
to write that so this is a really really
nice way to do auto basically to do
backdrop and tensorflow
the best way to get started with poking
around with the gradients in my
experience is linear regression so we're
gonna skip this stuff let's take a look
at the next exercise is linear
regression but it's written this slow
way and so we're gonna pretend like we
don't have dense layers we don't have
model dot fit let's do linear regression
with a gradient tape and this is good so
you can actually see what the gradients
are that you get and let me see what
this notebook gives you you might have
to clear the output so it's bitly / t f
- WS 1
and I think the real power of these deep
learning libraries it's um regardless of
which library you're doing is that they
can do Auto DIF once you have an easy
way to get gradients here we're gonna
get them for linear regression great but
almost with exactly the same code we get
the gradients for a deep dream so this
scales up in a really surprising way
so TF - WS 1 and probably you're gonna
need to clear the output I think I
forgot to clear it but what we're doing
here is we're gonna fit a model y equals
MX plus B to some data so we created
some random data we're gonna create a
model y equals MX plus B and we're gonna
use two tensor flow variables normally
you don't have to write code at this low
level unless you're doing some sort of
research but here we're creating
variables this is the forward pass of
our model y equals MX plus B M is the
slope B is the intercept our loss is
going to be squared err when you see
names in tensor flow that aren't quite
identical to numpy that usually means
there's a subtle difference in how these
work and I think the reason that this
says reduce mean instead of just mean is
you can imagine if you have a GPU and
you have a long list of floating-point
numbers and you're taking the average if
the GPU doesn't guarantee the order in
which it takes the average is possible
you'll have very slightly different
results every run based on
floating-point arithmetic errors so
that's just trivia why that's called
reduced mean anyway what I wanted to
show you is at the end of this notebook
if you knew the gradient descent it
makes this nice little plot that you can
look at and what we're seeing here we're
visualizing the loss of our model as a
function of the slope and the intercept
and that's our starting loss we get the
gradient the notebook will give you the
code to take a step in the negative
direction of the gradient and down we go
and here's the gradient tape loop that I
showed you from the slide in action and
what's cool is if you run this you can
literally print out the gradients for M
and B and see exactly what they are
which is cool so why don't we take let's
take like eight minutes and poke around
with this so if you want to run it from
scratch edit clear all output I'll start
talking again shortly I like a for 25
so it's bitly /t f - WS 1 also in case
you're new to backprop let me just point
you to a really nice article we don't
have time to cover it right now but if
you google for this if you want to learn
how Auto diff works wonderful article by
crystal of calculus and computational
graphs and the reason I like this
article as a teaching tool for backprop
is it actually does it so it has an
example it's not just like here are some
here's some equations so it's really
nice and Chris's article what he does is
he builds up a very simple computational
graph and this is what tensorflow does
to behind the scenes this is a
computational graph for we're doing like
a you know a plus B times C or something
like that and he'll build the graph I'll
show you the forward pass and the
backward pass to get the gradients it's
really really nice so calculus on
computational graphs by the way if
people are getting an error message with
length colab has tensorflow one
installed on it by default we'll get rid
of that as soon as tensorflow 2 is out
so if you're getting like tensor has no
like property length the very first cell
will install tensorflow 2 and you'll
have to run that one and then that error
message should go away so let me let me
briefly explain gradient descent in
gradients and so there's I'll do this in
two ways so one is the numeric gradient
and the other is the analytic gradient
basically deep neural networks work the
same way as linear regression in this
sense you always start if you're doing a
deep neural network or linear regression
there are two things you need the first
thing you need is a model here our model
is y equals MX plus B with the neural
network our model is going to be Karel
sequential dense dense dense it's much
bigger model same thing when you call
the model that's called the
paths you take some data passing through
the model get a result the next thing
you need which is very important is
called a loss function which is
synonymous with air and all that is is a
way to quantify how bad of a prediction
you've made in linear regression the
loss function is our squared error so
for the entire training set or whatever
data we forward it through the model we
take the point we predicted which is the
blue line subtract the point we wanted
which is the blue dot square it and we
sum that up over the whole training set
the point is loss is just a number in
classification we'll use something
called cross entropy but it still gives
us just a number and this is gradient
descent as soon as you can plot your
loss as a function of your variables
linear regression there's two the slope
and intercept deep neural networks there
might be million but the concepts
identical we're almost done because our
loss quantifies how bad of a job we're
doing if we minimize the loss that means
we have a good model so we want to go
down the hill deep neural networks don't
have a global minimum like this or
they're not convex like this this is a
special case but we'll get to some
minimum there's a concept in calculus
called the gradient and the gradient is
a vector of partial derivatives that
points uphill which is why the negative
gradient is the direction that points
downhill the good news is if you haven't
taken a calculus class in 20 years and
you don't remember what that means you
can sort of understand it intuitively so
loss is a function of our variables
looking at the gradient looks at each
variable independently so let's just
look at B our variables are just numbers
there's only two things we can do to a
number we can make it bigger or we can
make it smaller by some amount if you
forget calculus you can calculate the
numeric gradient like this for each
variable in your model make it slightly
bigger recompute your loss then make it
slightly smaller recompute your loss
figure out which way makes your loss go
down well actually in this case the way
that makes your loss go up is the
gradient and the negative gradient is
the direction that makes it go down
so you wiggle each one a little bit
recompute it that gives you the
direction wiggle each one a little bit
that gives you the direction the problem
with just doing this numerically is if
you have you know a million variables
you have to do a million four passes
Datta so this is really slow if you
remember tricks from calculus you can
get it in time that's linear in the size
of the number of nodes on the
computational graph
so basically calculus is a much faster
way to get the gradient but the point is
regardless of how you compute it the
gradient descent step is easy
that just means apply the gradient
literally take a step so wiggle your
parameters a little bit get the gradient
again and again and again and again and
again and so you know networks are
trained identically alright really
really quickly I just want to look at
some of the building blocks of these
dnns so basically you'll see this like a
billion times
there's cartoon diagrams of a neuron
which I like to think of as a little
logistic regression unit so here what we
have is some input data
these can be pixels on an image each
pixel is being multiplied by a weight we
sum it up we apply non-linearity and
that gives us the output of one neuron I
don't like this diagram I don't like the
math either but you can look at the math
it's a sum of the inputs multiplied the
weights multiplied by the weights and
then a non-linearity but let me show you
a diagram that makes a little bit more
sense so here's the way I like to start
thinking about it so here's a diagram
that corresponds to that here's the
diagram of our little neuron and here's
what's happening when it actually
computes on some data so we have an
image let's pretend this just has four
pixels and we'll pretend it's
black-and-white ignore the colors the
flat layer that we've been working with
unrolls that image into an array so
after we flatten it here's the pixel
values from that image here we have four
pixels so we have four weights I ran out
of room so there should be four inputs
up there but I just drew three what we
do is we do a dot product of the weights
and the inputs we add a bias and we get
a result so what a single neuron we
haven't done nonlinearities yet what a
single neuron is doing is giving you a
score for something and you can think of
this neuron it's telling us how plain
like is that image maybe what's nice is
we can start adding see how this is
already starting to look like a little
neural network now we have instead of
one neuron we have a dense layer all we
had to do to get a dense layer is we
added one more output actually we could
have had a dense layer and care us you
could have written exactly what you see
here is you know model dot ad dense one
for one neuron this would be dense too
and what we have here is now we have two
outputs adding a second output because
it's fully connected it means we've
added a second layer of weights and
what's really nice about this is instead
of a dot product we're doing a matrix
multiply so the forward pass of one
dense layer is one matrix multiply which
is really really nice and other cool
things you can do too here we're
multiplying
classifying one image at a time we can
also still with one matrix multiply
classify multiple images at a time and
what we've done here is we've added a
batch of images and here we have two and
what I'm trying to show you here is
still matrix multiply but now we get
scores multiple images at the same time
and so we're classifying two images at
once and this is what a dense layer is
doing
if you look at model dot fit let's see
if this works
I'm not connected that's why
you can look at the documentation for
all these little different methods we're
calling and you'll see that one of the
parameters you can set inside modeled
fit is the batch size and the batch size
in tensorflow if you're using these Kaos
api's defaults to 32 which is fine when
you're doing gradient descent the larger
your batch size the more accurate of an
update you're going to make but the
slower it is to compute a batch size of
one would be one example at a time
that's to cast a gradient descent a
batch size equal to the length of your
training set would be batch gradient
descent and what everyone does in
practice is a mini batch which is number
greater than one and less than size of
your dataset and 32 is usually what you
want but the point is matrix multiply
to get a neural network from that you
just need one more dense layer so you
need a non-linearity and you need a
dense layer the intuition for the
non-linearity I guarantee you some of
you have a much better sense of this
than I do I don't like this but I'll
show you a demo of how it works
so to get to a neural network we just
need two more things we have our matrix
multiply we've non-linearity and we have
another dense layer there are a bunch of
nonlinearities a lot of you have an
awesome math background if you're
multiplying a series of matrices without
the nonlinearities that reduces to
multiplying just one matrix so there are
a stack of different activation layers
you can add some of the ones originally
used were things like sigmoids
now a good default would be Ray Lu I
know these are tiny diagrams sigmoid
looks really nice it takes a number and
squashes it to be between 0 & 1 which
makes a lot of intuitive sense but has
really bad properties for gradient
descent and the bad properties when
you're using sigmoid activation and
these weren't understood for a while if
you have a very large value or very
small value going into a sigmoid and you
think about the derivative of sigmoid it
flattens out towards the extremes so
this using sigmoids can cause your
gradient descent to run very very slow
later it was found that Ray Lu which it
looks a little silly it's basically an
on-off switch will make your models
train much faster so the good news is
applying the nonlinearities is simple
they're applied piecewise
so here maybe we've done you know our
dense layer we've done the matrix
multiply and if these are the scores we
got we can apply Rea Lu to them like
this it's just going to be if it's less
than zero it's zero if it's greater than
zero it just passes through unchanged so
that's how you would apply ray Lu to the
output of your matrix multiply and this
in Kerris would be Karos that add layers
you know dense three activation equals r
a loop and then to get a neural network
you just need a single one more dense
layer on top of that basically on the
slides if you want to poke around with
why you need the nonlinearities I linked
some code that trains the deep neural
network without nonlinearities and tries
to classify this data set if you delete
the nonlinearities it gives you a linear
decision boundary if you add the
nonlinearities it gives you a nonlinear
decision boundary I'll show you a demo
of this in a second but basically the
idea is if you forget your Ray loose if
you replace here's some DNN if you stick
these Ray lose if you write none instead
of Ray Lu
this has the same power as this network
right here the interior the inner me
there's two nothing and let me show you
a quick demo of this there's a cool
website it's playground tensorflow org
and this is a little neural network
running in the browser this was before
tensorflow j s just FYI and like a it's
sort of a funny thing it's awesome and
really powerful and horribly documented
and can be a little bit hard to
understand but basically if I delete the
hidden layers we're looking at a single
dense layer or one neuron and if we pick
a linear data set and I hit play we can
classify the data set with our neuron if
I have a nonlinear data set here we have
these two circles through dots in the
center orange dots outside we can't
split the thing we can't draw a line to
split them if you add a hidden layer now
we have a neural network and the hidden
layer will do feature engineering and it
will I don't have a slide for this but
we'll just skip it there's a there's a
trick you can use to classify a
nonlinear data set with a linear layer
and that's to do feature engineering but
it doesn't matter the neural network is
doing feature engineering to let us
classify the data if you delete the
activations though so if we switch the
activation to linear which is none our
neural network can't do it and so you
have to have the activation functions to
have the hidden layers do something all
right
really quickly and then we'll do some
more code there's just two concepts that
I wanted to briefly mention because we
have alphabet soup so the output of a
dense layer is just some scores and
after we apply the activation function
we still just have scores usually when
you're doing classification what you
want are probabilities so there's a
function that you'll see at the end of
your networks called soft Max and soft
max takes scores and it returns a
probability distribution so that's what
soft Max is doing the other thing you'll
see is in linear regression the loss
function is squared error when you're
doing classification the loss function
is usually cross-entropy
and all I wanted to say right now when
you see the term cross-entropy what
you're saying is compared to probability
distributions so soft max gives us
scores and we need to compare those
scores to the thing that we wanted so
this is called a one hot encoding and
let's say we were classifying this image
of a bird and maybe there's ten possible
outputs for the image our label or the
value that we want for the bird is let's
say two corresponds to bird so we'd have
a one here in zeros everywhere else
that's the probability of distribution
we wanted this is the probability
distribution we got from making a
prediction with our model cross-entropy
we'll compare these and return number so
it's it's another loss function just FYI
all right so here's another notebook and
then after this we'll do convolution
which is much more interesting than
these dense layers so this is another
notebook where you're gonna write a
neural network for a fashion in list and
this is a dense layer still the link is
H Chi this time underscore one dash a
and let's take ten minutes and you can
hack on that by the way the goal that
was not to give you all the details of
softmax across entropy it's just so you
know okay that's ballpark what those
terms are trying to do and go from there
oh I just wanted to mention so you don't
get stuck on this the goal of this
notebook was just to briefly introduce I
have two things dead the goal of this
notebook is briefly introducing TF data
so you're seeing instead of when you're
pre-processing images
caris has awesome really thoughtful easy
to use pre-processing utilities things
like flow from directory data
augmentation they're wonderful and
awesome they
our concern flow to also the goal this
notebook is to show you a lower-level
way to do it which is why we're using
things like data set map and writing
your own pre-processing functions from
scratch just in case you're stuck the
first step is to batch the data and the
way you batch the data is just nice if
you're seeing can't I so my friend open
wrote this in Google Drive
if you can't edit the notebook it's
because it's not on github you have to
click on open and playground and that
will give you a copy of it but let me
just show you how to do the batching
step just for step one you can just do
that batch and then the batch size so
that's that's all you need
alright so continuing our warp speed
intro to deep learning so convolution
basically you'll hear a lot about CN NS
and convolutional neural networks are
way more they're much better suited to
image classification than dense networks
and I'll briefly explain why so first of
all convolution not a deep learning
concept and you'll see this a lot in
deep learning I know some of you have an
electrical engineering background you'll
know way more about convolution than I
ever will in deep learning we take
concepts from other fields and we use
the full kind of remedial way so first
of all convolution not a deep learning
concept and I have some code that I
wrote in Syfy and we're gonna convolve
over a picture of an astronaut to detect
the edges on the photo and quickly does
anyone know who the picture of the
astronaut is in Syfy who got built into
Syfy what do you have to do to become
part of sci-fi
anyway that's that's alene collins and
she was the first woman to command the
space shuttle of Columbia which is where
I stole the slide from so anyway the way
we're gonna detect edges on a lien is
we're gonna use a filter or a kernel
things in deep learning often have like
five names for no reason so we're gonna
use a filter or a kernel the brief idea
is there nine numbers eight of which are
negative one one of which is eight if we
same number of negative ones as the
eight if we put the kernel on top of the
image and we do the dot product of the
values in the kernel with the pixels and
there's just code inside pi you can look
at later let me show you what I mean by
that
so here's our image and here's our
kernel or our filter and the way we
convolve where we slide over this image
is we stick the filter on top of the
image we take the dot product of the
filter in the image values and we write
it in the output image and then convolve
literally means slide slide dot product
output slide dot product output slide
dot products output and so we get an
output image by convolving
and in cnn's the filter values are
learned exactly like parameters inside
dense layers are learned so they're
learned by gradient descent they start
life as small random numbers and what's
interesting about convolution is if you
have the right numbers for the kernel
you can get really powerful things so
this is an edge detector and this is the
way Photoshop works as well it's
convolution to detect edges to blur
images to sharpen images the difference
is in Photoshop they have these really
nice kernels that very carefully hand
design this is like the crappiest one
you can write but it works and the
difference with convolution and dense
layers this is already much more
powerful than a dense layer so with just
nine numbers we can find edges anywhere
on the image to do that with a dense
layer a dense layer would have to
separately learn to detect edges at
every location in the image so this
little thing has the same power as like
dense 1000 so much more efficient it's
slower because we have to convince lied
it around the image to do the math but
it's much more efficient in terms of the
number of parameters so this is a big
deal what's great is in deep learning
well first of all here's how you use
convolution inside tensorflow you can
write a little convolutional layer and
i'll explain what this means here we
have some layer that's going to take an
input image as input the input image is
going to be ten by ten by three meaning
it has three color channels red green
and blue here pulled it out of a hat
we're gonna learn a filter that's four
by four the larger your filters the more
sophisticated detect but the slower they
are common filter sizes are not four by
four they're usually three by three or
five by five I stole these slides from a
friend I had to change the kernel size
to match the slides and we're gonna
learn four filters and I'll show you
what that means in a second so here's
convolving in 3d and this becomes very
powerful very quickly so convolution in
3d
instead of having a 2d filter we now
have a 3d filter and already this gets a
little bit harder to wrap our heads
around exactly what this filter is doing
but it's basically looking at every
color channel separately the good news
is we can convince aim way that we can
evolve in 2d so we stick the filter over
the image we take a dot product and we
write that down as the output value and
I'm skipping things like padding and
stride and stuff like that but basically
if you do a lot of sliding and take a
lot of dot products you end up with an
output image what this is called this is
an activation map so this is showing you
the regions where the filter was most
strongly activated and that just means
the dot product was high so if it was an
edge detection filter this would be the
locations of the edges what's nice again
these filters are learned this is also
an image there's no reason that we can't
just stick this in matplotlib and
display it as an image like we did with
the result of convolving over lean and
so you can visualize very easily exactly
what the output is is of all these
filters so that's that's a nice property
and then it gets powerful if we add
another filter we get another output
image and all the filters are learned
random weights initialization so
hopefully we'll be detecting different
things and here's what's cool I guess I
deleted the slide accidentally but you
can imagine if we had four output
filters or ten let's say we had four
output filters that would mean we've
gone from a 10 by 10 by 3 image to
attend by 10 by 4 output image so we've
left color space and we've entered
activation space and the hope is that
this will learn edges in some
orientation edges in another colors
different kind of colors so we're
getting maps describing where features
are on the image and it starts getting
powerful very quickly it's when you add
a second convolutional layer and the
important thing is convolutional layer
one if you're a filter in this layer you
have to look at pixels and compute
features but if you're a filter in this
layer you get to look at the features
this guy already computed and compute
features of them so if these are edges
maybe you'll learn to detect shapes
which is really cool and here's how this
works so let's say in our first
convolutional layer we learned four
filters pull out a hat
the next convolutional layer looks to
all for activation maps of the previous
one if we had learned 32 activation maps
in the last layer these filters would
look through all 32 of them so these
filters are really really powerful
basically they're taking dot products of
features different types of features the
things they can compute are very
powerful and they get powerful very fast
but again convolution works in exactly
the same way as every filter produces a
single activation map and if we had
eight of them we get eight activation
maps so what happens is basically the
image gets deeper and then as I've drawn
it here I didn't have time to talk about
things like max pooling but there's ways
you can make the image basically this is
a big chunk of image and it's slow to
convolve over as a way to speed this up
you might see things like max pooling
layers and what max pooling layers are
they reduce the width and the height of
the image but they leave the depth
unchanged so basically what max pooling
will do one funny thing you learn by the
way if you if you start teaching this
stuff there's like two or three more
than two or three but there's a small
number of people that make really
excellent diagrams and every other class
in the world steals them the best
example of this I'd say like 95% of the
classes I've seen have borrowed this
diagram which is written by you've all
seen it yeah which is written by Chris
Ola and it's the same thing with max
pooling from Stanford but anyway what
what max pooling does it's just taking
this is max pooling of two so what we're
saying is this is too hard to process
computationally we want a hack to make
it smaller what we're gonna do is for
every 2x2 region we're just going to
copy out the strongest activation to the
output so this is the image size by 75%
it's lossy but whatever that's max
pooling
and then yeah we talked about that
earlier what I want to do is talk about
deep dream really quick well there's a
couple things you can do with this
anyway one question you might have is by
the time you get to layer 17
what are these filters actually
responding to before we write a CNN let
me just talk about a couple things you
can do with this so here are three
things you can do with cnn's the first
is you can write one from scratch and
that's the next exercise and that's
writing a model care of sequential
convolution max pooling convolution max
pooling dense and that's great the other
thing you can do is transfer learning
and this is a really really powerful
concept so the idea of transfer learning
usually machine learning you have a
small amount of data let's say your
friend in the past had a lot of data and
maybe she trained a motional network on
an image net from Stanford so the
million pictures and a thousand
different classes takes a day to train a
model on it so let's say she trained it
and then you wanted to reuse her model
to train your own instead of starting
from scratch what you could do is let's
say this is her model and this dense
layer the end is classifying things from
imagenet so cats dogs snakes peacocks
whatever let's say you have like Honda's
& Toyotas to do transfer learning you
delete this dense layer you keep the
rest of the CNN that she previously
trained unchanged you add your own dense
layer and outputs just for the classes
that you care about and then you relearn
just this dense layer but you leave the
convolutional base unchanged and the
idea here is you use her CNN as a
preprocessor so it takes an image it
gives you good features for the image
and then you learn a dense layer using
those features that's called transfer
learning it's a really really
interesting idea if the idea is using
knowledge that you've learned on a
previous task on another task you might
know of other examples of this the only
one I know that works well that's no
longer true this works really well for
images and it's starting to work really
well for NLP but that's very very recent
with models like Bert but I bet there's
more more potential here too
the third thing you can do with
convolution is trying to understand what
these filters do so basically let me see
what we have here because of the time
I'm just going to talk about deep dream
for a minute and then I'll give you some
exercises you can do to write a CNN and
then to do transfer learning so here's
here's the idea with deep dream
all right so
has anyone seen deep dream before
doesn't even know why deep dream
existence was the goal of deep dream
like let's smoke too much and generate
psychedelic images from neural networks
so deep dream is so in a really
hand-wavy way been like trust me we get
this magical feature hierarchy it's
gonna be great and deep dream is a way
to actually show that this exists so
it's a way to investigate the
representations learned by a neural
network so basically let me show you the
results and then explain what they are
just in terms of terminology by the way
when you add these layers you can name
them so if we wanted to I could have put
a parameter comma you know name equals
layer 1 or whatever this layer has four
filters each of which are four by four
and let me show you what the authors of
the deep dream paper before I go into
deep dream let me just show you what the
filters are learning to detect so each
of these is one filter in the first
convolutional layer in a neural network
and these images were produced by
starting with random noise and modifying
the noise until the filters maximally
excited so this is an image if you are
the first filter in the first
convolutional layer and you saw this you
would produce your highest possible
activation and in layer 1 we're seeing
that filters are responding to different
colors right this layer probably by the
way looks like calm 2d 30 to 3x3 or
something like that so there might be 32
of these individual filters they respond
to different colors and edges and
different orientations so that's the
first layer of a CNN the names here are
a little funny in this network but as we
go deeper these are the images that the
filters in the next layer get really
excited by and already they're getting a
little complex these are like texture II
right
as you go deeper into the network I'm
not gonna go through all of them take
forever they get more and more complex
as you go and what's interesting is if
you start poking around really deep they
start to look like things we recognize
so we see like peacocks and feathers and
cool textures and I don't know what all
this stuff is the reason that we're
seeing these particular images is this
model was trained on imagenet and these
are the features that it found to be
useful to classify image net images like
presumably if the image net image had
features that look like this might be a
B and the reason that you see these
features tessellating along the image is
probably because convolution does this
slide operation where you apply the
filter in different regions and then if
you go really deep you see things that
start making sense to us like saxophones
broccoli and who knows what but anyway
let me explain how we get these and this
is what deep dream is doing first of all
there's two things you can ask one is
you can say let's find an image that
excites one filter from some layer and
that's what we're doing here - you can
say let's find an image that excites the
entire layer so if this is layer 5 let's
make the image make this layers excited
as it can be and here's how that works
it's a really really powerful idea and
the code is surprisingly short so in
deep dream you start with the picture
great the next thing you need is a model
that was trained on a large data set of
images and it doesn't matter what model
you use here we're importing this is
transfer learning almost we're importing
a model called Inception there's a if
you learn more about CNN ziyal there's
like a box of famous models there's
things like bgg Inception ResNet
whatever Inception is one of them and
these are all different architectures
and what I mean by an architecture is
basically when you do you know model
equals sequential add dents add dents at
dents that's an architecture this would
just be a fancier architecture built
with the functional API as a sub
classing api with whatever that's
inception it's some CNN with something
it's added if you were doing transfer
learning this line here says give me the
CNN but not the dense layer and later
you could add your own dense layer to
this here we're saying give me the
weights that we previously learned on
imagenet so this is a train model the
next thing we're gonna do is we're gonna
take an image and we're gonna pass it
through the model and what we want are
the activations at a certain layer so
our goal is going to be to modify the
image to get those activations as high
as possible and as my the image
presumably will add more things that
whatever that layer is detecting we want
to appear in the image so because the
layers in this model are named we can
look at the summary find the names we
want and then here we're using the
functional API just to write a new model
where we pass in an image and we get the
activation maps out of the layers and if
you pass an image through this and you
run it you'll see a bunch of matrices
which are literally the output of the
convolutional filters at those layers
and there's gonna be a lot of numbers
but here's how deep dream works and this
is kind of magical
some of this code is boilerplate but you
always need a loss function and the loss
function here is just this we're summing
up all the activations so if we want to
find an image that excites this layer we
want to maximize this list of
activations so we literally sum them or
here we're taking the mean but same
thing and that's almost it yep just you
know where we updated this like
yesterday so I haven't actually seen
this brand new version yet but I can
give you the idea
so what we do when we call this we're
going to pass some image through a model
inside here we get the list of
activations and this is the sum or the
average of the activations we need to
maximize this and here's the insight
behind deep dream so normally when you
have a deep learning model you adjust
the weight on your model to fit the data
here we're gonna leave the model alone
and we're going to adjust the image to
fit the model so we get the gradient of
the loss with respect to the pixels on
the image and if you print this out
these gradients will have exactly the
same shape as the image which means you
can directly add them to the image and
we want to do gradient ascent because we
want to make the loss go up and so
there's some normalizing code here but
the important part they change this
slightly but it's right here we're doing
gradient ascent so we're adding the
gradients to the image multiplied by a
learning rate and at every step of this
what's it's amazing that's all the code
you need to deep dream fi an image so if
you picked a layer that responds to
sheep those gradients will make your
image slightly more sheep like which is
nuts and there's two versions of the
deep dream tutorial or so the first half
of it we tried to write like the minimum
amount of code to make it work and that
produces these slightly staticky images
the second half of the tutorial that's
the research insight the second half
which is a little bit more complicated
has different tricks to make them really
high resolution and stuff like that but
the point is I just wanted to talk about
deep dream after convolution because it
really proves the point I really like it
it means this isn't BS and these layers
are actually learning this feature
hierarchy and we can see it and we can
reuse it and it's cool anyway let's do
this for ten minutes I know it's fast
but the goal here is to start writing a
CNN for a dataset called C fart N and C
part n is in the M nest family it's a
small data set but it's color so it's a
little bit more interesting and
there's a reference tutorial you can
look at which has background on how
convolutional layers work in tensorflow
too okay so let me let me point you to
one or two more things I'm just gonna
take like two minutes and just point you
to we've spent a lot of the summer
working on the tutorial so let me just
point you to some of the latest ones
just to save some time
so basically
for transfer learning we have two
different tutorials you can check out
and let me explain like why we have two
different ones from an industry
perspective so basically there are two
repositories of pre trained models in
tensorflow two
one list of pre-trained models which are
awesome are these chaos applications and
if you google around for chaos
applications there are these one-liners
where you can import a lot of famous CN
NS with usually weights trained on
imagenet so a lot of our tutorials are
using mobile net b2 by the way here's a
really simple line of research that's
interesting
previously with CNN's the goal was how
accurate of a model can we train the new
goals today are often how small the
model can we train that's accurate
enough and the goal is to get it fast
enough to run on a phone or in a web
browser it's not rocket science
basically what people do is they do
experiments with different numbers of
layers and they look at like the
accuracy speed trade-off anyway so one
tutorial has these applications from
Charis they're great the other has a
larger repository of pre-trained models
from tensorflow hub and tensorflow hub
is a more recent collection of models
and we're working on expanding this for
tensorflow - the truth is it doesn't
really matter which one you use
sometimes big companies build two of
everything and see which one works
better
caris applications are older and they
existed before tensorflow - they're
great anyway you can try either and
whichever one you feel is easier is the
one that you should use
so that's transfer learning a really
cool thing today is Ganz and tensorflow
- has really really awesome tutorials
for Ganz we've got three of them plus a
VA e plus an adversarial well actually
let's just look at the games that is not
again
has anyone worked with Gantz couple
alright so basically for people that are
new to games they ask a really hard
question so everything we've looked at
so far is here's a picture classify the
picture the question ganz asked are
generate me a picture and the goal is to
generate a picture that looks real and
the challenge with generating things
with deep learning is that we need a
loss function so everything we do in
deep learning is we're doing gradient
based optimization against some loss
function like squared error cross
entropy or maximize the activation of
some layer it's hard to get a lost
function for generating cats and the way
gans work was research for me in
Goodfellow in 2014 and what ian realized
is we can get a loss function for
generating images for free and we
already have it it's an image classifier
and so if you train an image classifier
to say is this image of a cat real or
fake that's just a standard
convolutional Network you can train a
second CNN to generate images of cats
and you can train them against each
other and so you have this game we have
a generator and a discriminator and
they're trained in parallel and
basically over time the generator learns
to generate more realistic pictures of
cats and the discriminator becomes
better and better at telling real cats
apart from pick cats over time they
hopefully reach equilibrium at which
point you can generate pictures of cats
and this tutorial here is the minimum
amount of code you need to train a Dan
or generative adversarial Network to
generate images of em missed and that is
a visualization of the feminist images
being generated over time and if you
look at this it will look very very
similar there's two chucks chunk one is
the discriminator and if we have more
time this would have looks almost
identical to the image classifier you
would have written on the last exercise
the discriminator is just a
run-of-the-mill CNN the trick is the
generator and when you're new to deep
learning you'll start looking at code
like this and you'll recognize some
layers and you won't recognize others so
let me just walk through what some of
these layers are and by the way the best
way to go through these the papers are
linked at the top of the
toriel's if you read the paper you track
the code at the same time it's much much
easier than just the paper so basically
you'll see layers like dents you've seen
that leaky Ray Lou is a friend of Ray
Lou with just slightly different
properties this would work fine with Ray
Lou also you haven't seen batch
normalization and you haven't seen these
calm 2d transpose
let me see if I have slides on comp TD
transpose really quick so one challenge
with Ganz is we don't want to generate
the same image every time otherwise the
discriminator would just learn that
that's the fake image so we need to
randomly seed the generator so the way
that generator and Ganz work is here
some random numbers use these to
parameterize the image that you generate
like maybe the first random number tells
you how one like it is and the second
one tells you how to like it is and what
the Gann has to do in the generator it
has to go from a list of numbers to an
image and we usually do up sampling to
do that comp to D transpose is an up
sampling layer there's two ways to do up
sampling one is you can just double the
size of the image and average the pixels
you can do a learned up sampling and
what I'd recommend do you see layer like
this take a few moments or a day or two
and dig around and trying to understand
what it's trying to do and so like when
I was going through convolution to D
transpose and like what the hell is that
and so what I usually do is these are
slides from a summer workshop what I
usually do is try and simplest mini
example of the layer and just work out
an example and autumn artist if they
look so this is convolution transpose
and here's a quick example of how we can
go from a small image to a larger image
with a learned up sampling so this is a
lot like convolution like here's our
filter and the basic idea the details
aren't important I just want to show you
that it's a thing you would take the
small image this is how the chaos compa
to D transpose layer works you take the
small image and use its grammar ties the
filter so instead of the dot product
it's the image value multiply it against
all the filter values and you write that
down on the output image and then just
like convolution you slide so we slide
again we multiply that by the filter we
write down the output value
and we summit slide again we slide again
but the point is that's all the layers
are doing it's complicated but not so
bad if you have the time to go through a
small example comp 2d is a learned up
sampling the other layer in there that
we haven't talked about it's batch
normalization and let me just briefly
explain the idea there so in a lot of
the code you'll look at for M NIST if
you're training an image classifier one
of the first things we do is we
normalize the data so we import some
images and usually the pixels range
between 0 and 255 and the first thing
you'll see in a lot of tutorials is we
divide by 255 which makes them range
between 0 & 1 the reason we do that
briefly is that basically neural
networks don't like large numbers as
input and there's different reasons why
they don't like large numbers one of
which is if you remember the slides from
a dense layer input value multiple
weight if we have a very large input
value or a very large weight we could
get overflow numeric overflow we have
floating-point problems and they can
have bad properties for gradient descent
so we normally normalized the numbers to
be between 0 & 1 here's the insight
between this amazing layer called batch
normalization and let me explain what's
happening and why this is important and
you'll also see that like a lot of the
research today in deep learning is not
rocket science it's just it's very early
in the field so here's our beginners
tutorial and we import M mists and we
normalize it to be between ship
we import em this we normalize it to be
between zero and one this means if you
are the first dense layer if you are
this guy all the input values are
between zero and one you learn a weight
however if you're the second dense layer
your input values are not necessarily
between 0 & 1
they're the outputs of whatever this
previous dense layer has produced which
means your job is harder so this dense
layer just has to learn weights for that
fixed distribution but the distribution
coming into this guy is changing which
makes its job harder than it needs to be
so what batch normalization does it's a
layer that you would add right here and
if you wanted to right here and the
basic idea is
it's a normalizing layer and there's a
lot more details you can read about but
the basic idea is let me see if I'm
awake enough to go through this here's
some features coming into a layer and
these are examples and what we're doing
is batch normalization computes the mean
and standard deviation of each feature
and it just normalizes it before it goes
into the next layer so batch norm
basically in a nutshell it's let's
renormalize the data to make the
distribution going into the layer change
slower so it can speed up learning a lot
another layer you'll see is drop out in
DC can and here's drop out the good news
is let's see
you see these drop out layers drop out
is a really really nice layer and it's
easy to use drop out is a great way to
prevent overfitting and here's the basic
idea so we have some Cartoon Network
full of dense layers and let's say we're
overfitting we're memorizing the
training data
what drop out does is it dimly
deactivates on every batch a subset of
the neurons and it does that by setting
their activations to zero so drop out
basically says this network is too
powerful let's randomly turn off a bunch
of neurons at every step and the reason
that this helps prevent overfitting it
makes it harder for the network to learn
and the idea is that because it can't
rely on any individual neuron being on
at any individual step so it has to
learn redundant representations so it
has to learn different ways of detecting
the same feature so drop out it's a
layer to prevent overfitting what's cool
is if you learn about drop out it was
invented by geoff hinton and geoff
hinton had this really cool thing on
reddit where somebody asked him like
what's the intuition behind drop out and
this is what Jeff said
so basically Jeff was saying like well
he was I think he must be a really nice
guy he's trying to make friends with his
bank teller and he was having trouble
making friends with his bank teller
because the teller kept being changed
and basically he asked the bank why are
you changing the bank teller and it's so
he can't defraud the bank and so because
he can't rely on any individual bank
teller being there at any individual day
you can make form friendship you can't
come up with a conspiracy to defraud the
bank
and drop out in the same way prevents
neurons from always being present so
that's the intuition I don't think like
this when I go to the bank
alright let's just uh I just want to
point you to one more thing then we're
gonna play a game and then we're gonna
stop if you have time there's two
awesome new Gann tutorials you can go
through and read the papers the reason I
like these is it's complete code that
works and it works with a clique which
is nice
the first is picks two picks out of
Berkeley which is beautiful this is a
conditional Gann and the goal here is
not generate me a random image that
looks real its generate me an image that
looks real that also resembles an input
image I give you so this is an input
image for the building facade or for
sayed not sure this is the building that
image actually corresponds to and this
is what's generated by pix two pics and
so this is an image with similar pixel
values to this image that the
discriminator can't isn't able to
distinguish from real or fake and if you
start looking through these games the
main thing to look at is the loss
function and so if you want to
understand the evolution from DC Gann
which is M nough stoop Ickx depicts look
at the loss function and the main
difference in the loss function the loss
function for DC gam is trick the
discriminator the loss function in pics
depicts is trick the discriminator and
minimize the l1 distance between the
input image and the output image which
forces the output image to look similar
to right that's the main difference and
then the same reasoning applies to cycle
game which is the latest one we have
and cycle Gann does unpaired image
translation and so with pics of pics you
have to have paired training data so a
facade building map image satellite
image there's lots of things you might
want to do ganz for that you can't get
paired training data for like day to
night like even day to night is hard to
get paired training data if we took a
picture of the MBL at night and in the
day things change like cars move around
people move around so it's hard to get
and of course like there's no pair
training data for horses to zebras
because it doesn't exist but cycle gang
can do this and the insight was cycle
Gann was you don't have to have a
one-to-one mapping what you need is a
directory so if you have a directory of
horse pictures and a directory of zebras
pictures you can exploit supervision at
the level of sets so cycle game was a
really cool thing all the code is there
I flip stop with that lets play one game
really quick and I'll point you to two
games that if you're teaching they can
be fun to help keep students engaged and
there's a point to them so I have a
quick volunteer and this this person
should be proud of their artistic
ability very good artist which I am NOT
thank you come on up has anyone seen
quick-draw before so this this is great
with kids and adults anyway so
quick-draw by the way just got way
harder and I'll explain why in a sec so
if you can do you've seen this go for it
so let's let's try and do like two or
three quick draws you okay okay so let's
try and do two or three well I hope this
isn't blazingly loud let's draw shorts I
see a music note oh I know it's sure so
we don't that was amazing so we don't
have audio but usually it speaks to you
as you're playing it so I see baseball I
see shorts Society and that was great
I see shoe or suitcase or square or
camera or stereo I see stove yeah it's a
stove oh I know it's ready there you go
so let's see one more that's actually
really good cannon I see lime or rainbow
or potato or peanut or pond
I see watermelon or steak oh I know it's
cannon this is actually surprisingly
okay keep going
I see nose or line or pond or pool
I see skateboard or sandwich or hockey
puck oh I know it's hamburger let's do
one more you might have set the new
record for icy line or diving board or
circle or peanut I see potato oh I know
it's steak let's we're gonna stop there
so you did so nice job thank you very
much today
[Applause]
all right so the first thing has to tell
you about quick-draw if you're teaching
a class
amnesty is boring as hell but it's a
good place to start a good homework for
the students is quick-draw so let me let
me point you to some code
if anyone wants the URL you can grab
this screenshot and there's probably
official versions of this too but uh
what this is it's a little Python file
you can use to make a quick-draw data
set so quick-draw which your images are
now a part of quick-draw has an academic
data set of probably like 20 million
plus quick draw diagrams at this point
and this code you can say you can pick
which class names so you can say like
you'll give me all the planes cars
trucks and you can say how many images
you want anywhere from like five all the
way up to millions and what's nice about
this is students when they go home they
can get some experience like training a
model on a large amount of data and it
doesn't have to be a blocker because
they can select how much they want so
it's a nice thing too and like yeah and
this will walk you through like how to
get the images out of quick-draw and
stuff like that the other thing I have
to say about quick-draw which is really
interesting it's if you look at these
drawings they're not just pictures but
they're they're sequences of
brushstrokes and so what's cool is the
these are different elephants from the
quick-draw elephants set and what's cool
is all the different colors are
different brush strokes I don't know the
order but that's - and what's cool is
there's some really good research and
given that we have this database what
else can we do with these brush strokes
and you can use rnns which are usually
used to generate text and stuff like
that but the insight with david has
group you can generate quick-draw images
using an RNN and there's a really cool
game for this it's called sketch RNN
sketch Arnon is an RNN that's been
trained trained on the quick-draw
dataset and what's cool is you can pick
an image from quick-draw so if we pick I
don't one will pick penguins it's the
marine biology rights
close link and then you start drawing a
penguin and then you stop sketch RNN
attempts to autocomplete your penguin
and it looks silly but it's super
impressive to think about like how hard
how hard this is to write and so
basically what we're seeing is of the
people in the quick-draw dataset that
started drawing a penguin in a way that
I did these are the brushstrokes that
might follow and like it's kind of cool
like I'm not sure that's worked with
penguins but maybe people start drawing
the beak and so it's really cool and
there's a surprisingly large number of
images that you can draw so this one's
not as immediately actionable you can
show students the quick-draw dataset
then they can go train a classifier this
one is more of like hey fYI this is
super cool all the code for this is
online it's just uh I would you know
someday I would love for us to have like
a short tutorial and the reason I wanted
to mention this is it can think about
how this generates images here's some
research this is very different from
Ganz so ganz synthesized these beautiful
photorealistic images pixel by pixel but
that's not how people draw right like if
you start drawing a scene you're not
gonna draw a pixel by pixel you draw in
brushstrokes so this is an RNN based
solution that learns to draw images
brush stroke by brushstroke which is
very very different anyway that's all I
got so basically here's some tutorials
you can look at also for people that are
teaching the there's three book
recommendations the first two books are
not academic they're like 40 bucks these
are how you do the thing books how do
you train an image classifier how do you
turn the test text classifier how do you
make an RNN work
they're both great if you get the first
book only get the tensorflow 2 version
which is in pre-release right now so the
second edition the deep learning with
python book this is a manning book it's
by francois Shola who wrote Karros and
everything from this book works in
tensorflow too just by changing an
import and
if you want a textbook it's free it's
e'en good pillows deep learning book
this is a little bit it might be
instructive for some of you I struggled
with this a bit it was really hard to me
this is more of like a really great
reference yeah that's all I got
thanks a lot can I answer any questions
[Applause]