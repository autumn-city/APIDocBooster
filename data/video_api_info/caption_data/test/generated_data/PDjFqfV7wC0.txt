okay
just for the purposes of this video so
that i can share that for the other
uh people the students that i have like
that are not
speaking with latvian i will speak in
english
and yeah so i have
i shared the template for today's
work and the
paper that we're going to look at and
there's going to be also
another variation very interesting
variation of the what
what is known as a kind of
state-of-the-art lstem
so the phase double stem the main
purpose is
long event-based sequences
uh it's very similar like uh
advantages as for the transformer
as far as i know that
the this this kind of university group
that uh published this work
they also have the patent for some of
things that are included in this uh this
architecture and um as i mentioned uh
so today we'll we'll make a very simple
lstem then with
the uh peepholes and then we'll start
making the phased allostem
and uh then after that i have one
day of another variation of lstm that i
know works really well
so this is all about recurrent neural
networks right
so there are multiple ways how to
display these
these architectures you can
have many variations of uh like uh
schematics so this is one of them and
here
you see that on the left side
this is a this is actually not
completely
vanilla lstem but there is a peephole
also
so the what are the peepholes it's meant
that you have um
some kind of projections from the
c vector to to the gates
not only having so the vanilla version
is actually looking
uh like uh this where you have um
like the c the c vector is not connected
with the gates so it's just the x is
connected and the h
is connected with gates this is another
way how to look at
these equations
but here you have the peepholes as well
and here for the
phased lstm we also have additional
gates
additional gates k here this is uh
conditioned
on the c i guess here and
here with h i think so we'll see the
equations and
they are one thing that need to be seen
here is that
the t is also coming in so the t is
um in this case is actually the as i
understand
the index of the timestamp
but it could also possibly
be encoded in some other way but i guess
for because of the
the formulas that they have it would
work
the best if it's uh encoded in some kind
of scalar
manner and the way how then it works is
this is the simple architecture so with
the the
peep holes added so if we compare with
with this one we'll first just implement
the simple one and then we'll add the
with the peepholes
so we see that there we have these
uh these equations uh
here that's actually they come on the
pairwise products
coming in in these positions
also here so in three three places here
here here
there are peep holes if you look back
here we have
one arrow here the second the
third so three peoples right then uh
the other thing is if you look at the
architecture in another way
then you see there is an input and the t
index and the layers are actually the
just the cells
of of these phase dallas time equations
so that there could be one layer of
course or multiple
and the main idea of phased lstm is that
you have
learnable phases and shifts of
the gates so the gates are open only
when you learn when you need them open
so
even with the peephole based simple
yellow stem we have vanishing
gradient problems still so if you have
very
like the long sequence would be longer
than let's say
32 or maybe 64 inputs
then the phase they'll like the simple l
stem
degrades quite rapidly there's there are
some tricks that i will maybe showed
next time
but in general it's not good and for the
phased lstm you can have
thousands of of time steps no problem it
will work great
so and the day is that it will learn
the parameter t the parameter s and it
would
simulate the effect of opening and
shutting
closed the gates so you can imagine that
there is just
like this vector that is uh
like the openness of the gates depending
on the phase and the shift
and then the low frequency the high
frequency is here
the low frequency is here for example
and yeah and then you get
the output c state the the equation
for so if you come back to to this this
uh cell
here there is the k so the k gate
is calculated using these equations
so with with the uh so with
with three or three conditions so here
in the uh here you have
one hyper parameter this is good that
there is no
there's actually not many hyper
parameters there's just one
the r on this one and they gave
give the what is kind of open ratio what
is what is good value to start
looking and then you have the learn
the t is is the index of the of the time
step
s is learnable the tau is learnable
then the alpha is learnable alpha this
this is like a leaky param
parametric relu or something like that
where you
if if you don't hit the uh phase
intervals then you have not killed the
gradient completely it's it's hopefully
it
goes through with some kind of value so
this is basically that part and then
they
modified only the the last two lines
they add to the c vector
some modification here by adding the k
here k here and with the h
also k here and k here so that's that's
about
it about the the architecture of the
function graph
and um i think okay this was regards i
think to the gradient loss
then they have some examples quite quite
really uh really good ones i mean so
if you have uh some different kind of
sampling of
some toy data sets so if you're if this
is a uniform sampling then there's no
not a big difference between different
kinds of lstms as soon as you have
really uh non-uniform or like uh
even maybe well so what what
is happening something is going okay so
even uh
some some kind of sampling that is
really uh
not uh like with the with the same
frequency then then
the phase the last theme is like so like
like by but by by the magnitude better
and
also some other tasks also shows
significant improvement the one way how
they test all of these things
is using uh the mnist data set so they
they you also test with other ways but
the one way is
uh using using the just the visual data
set of the mnist
of the 28 by 28 pixels and then just
treating them as a long sequence that
that is a little
but that's not like the best data set
but also
it's a way how to evaluate it and we
will do that today as well
so so far any questions uh regards to
to the architecture itself or the
reasoning
why why you would want this or
everything clear
what do you like what do you think
so let me check the your faces so no
faces but
hopefully it's clear i hope really
so anyway if there is any questions not
something not clear then let me know
so and let's go then to the code so if i
open the
the this this code that i had before
they said they shared you
so if i post this video somewhere i will
also give the the template with this one
so here we have um let me go
over it
so first of all i i made this that that
for the gpu it would it would take the
whole data set but for cpu it would take
only small part of the data set
just for the validation part purposes so
that it if it works
then the other other thing is that i
selected only uh the three classes
uh for the letter zero two and seven
and um i used the mnist data set you
could also try to use
extended mnist that is a little bit
harder one
and also you could include more classes
include more classes
uh them so as soon as we get
this 99 accuracy with these classes
then i suggest including more and more
and more until we
see significant digger get degradation
so we can see the benefits of the phased
lstm so this is uh there is also like uh
so
if you think about ext mnist dataset so
just for most of you like you already
know it
very well but if you just look at it a
little bit
so the the the number
is actually in the middle part always in
the middle part of the image
and depending on the class there is some
kind of frequency going on
so if there's like a one it's it's
always like mostly
till the middle part of here the
frequency uh is
if you look at rows so we can look at in
many ways
one way is that we looked at the time
dimension is over
uh y axis so the vertical axis
and if it's like a time dimension then
we see that first of all like it starts
not immediately but at some delay and
then
it always kind of did the frequency is
somewhere in the middle
and then at the end again the nothing
or we can also look at this as a as a
long
sequence of one dimension so we can put
together
each row as a long sausage and then
again there would be some frequency
so depending on the
the these these numbers there should be
different frequencies and that's i think
quite interesting
so um
here you think so oh sorry i i forgot to
show you that
picture sorry i am really sorry about
that so if i if at some moment
you i'm talking about something and you
don't see the picture please tell me
that
so again different kind of frequencies
for different kind of letters the uh
rows let's say our time dimension or
the if we combine all the rows together
that could be also a time dimension as
one dimension
or as 28 dimension so okay that's that's
about the
the data set so
here we also add the min max scaler just
to make that the input features are a
little bit
more uniform it could actually maybe not
be necessary then also in this case i
use
input features as 28 so that we would
have 28
time steps with 28 input features that's
our data set
then we have train test split of course
hopefully
hopefully yeah it is trying test split
so then we have the
lstm uh module where we will quickly
code
the actual lstm equations and then add
some modifications to get
to the phase lstm here we have the
uh very simple uh model so the model
uh how like also this is a highly
advisable advice
so from the research that i have seen
you should always have
the linear projection before and after
the lstm
there i can later show you reference
basically that that
improves the uh quality of the output
also
also there is there are some tricks that
that we will not use today but there are
that that should be used
and here at the end so
let me quickly uh give you some examples
what what what is coming
in here there was going to be the batch
coma comma
sequence comma features
so the it actually would be let's say
some kind of batch comma
28 comma 28 then
then after the lstm here
um so here after the lstm
call we would get the batch
sequence and the hidden size
hidden size
then then we would calculate the
average value over the
sequence for each of the samples of the
batch
and in that way we would get the just
the output as the batch comma
hidden size and if you have this one
then we can easily
project that to log its softmax and then
pass to the cross entropy here
so i wrote out the cross entropy with
the equation i always like to write the
loss functions with equations
that that way i get a little bit more
control over
the way how it works even with the cross
entropy there are a lot of things that
you can
play around so here let's start with the
lstm
so what we have there is that
we would need to have at least in the
beginning
without the peep holes we need two
matrices
and i think bias if i
not mistaken because i think it was like
that
and i will uh so and i also when i
tested it a little bit around that it
was interesting to see that
initial initialization is very important
actually there is one trick with regards
to
for get gate that is also
beneficial so here
let's start typing so self hidden size
uh hidden size is this one so just like
for the ease of use that we can use it
also here but actually already written
so also like in a pie torch the
hidden vectors the h and c can
be skipped as well that you can give
them immediately or you can skip them
just so that for for example
if you would like to roll out the
sequence
for example you have some kind of uh
language modeling task and you would
like to
extend it over the uh like the
so if you have different size sentences
or or you would like to see what would
be the next word
you know then you can pass in the
previous
hidden state and just generate the next
step
or you can if you if the sequence size
is already known
then basically you don't need to do that
and the
and especially if the sequence size is
the same
for all input uh samples so it is not
always the case but in this case it will
be
so then let's start by adding the
the so would if you look at the
equations there is a
this this white equation for the inputs
also uh usually
like almost almost every time the people
define these matrices as um
for every gate just like a part of that
matrix
so probably it just learns the the
correct uh
like these dot product
correlations and that's why it works
after the convergence
but uh of course if somebody wants you
can split it
in four parts but like it usually
doesn't make sense it
have the same same result after the
convergence
uh if you just use one so if you just
use one
matrix for the projections of the
inputs then it means that we need to
have
for it for the four gates so it's not
enough that we just have the
input size here and the hidden size here
sorry it's a little bit wrong notation i
need
another parenthesis here so the input
by hidden that that would be just one
small matrix that would constitute to
the
uh like a projection from input to the
hidden size
but in order to have four gates so we
would have four gates
uh covered by just one dot product then
we would multiply this by four and also
multiply this by four
so with the gates there are if we look
at the equations
there are the input gate for get gate
there is output gate and there is
internal this this cell state update
gate
so that's why we have four gates for
lstms
that's that's here then let's
let's think so i think it's actually
let's see i think you could you could
leave out also without those
double parentheses but maybe you need
them so then the
the initialization so usually you can
initialize that by uniform
the the one thing that i found out that
it works much better
actually that is also very interesting i
think i would look up
a little bit more why exactly it works
so much more better
is that if you calculate the uh
standard deviation of of that
so i see that walters joined it joined
as well so
uh on i posted the template in the chat
for
for today's workshop
i also hope that you see the code in a
correct manner because
in my screen it's a in uh like a mirror
side but
so is is everything visible okay
is everything so maybe somebody can tell
me
okay cool so yeah so uniform here
then the maybe somebody knows the answer
why this works so well
it would be great to hear and understand
a little bit better and so i will read
myself as well about it so if you
calculate
the standard deviation of the uniform
like what is the amplitude of the
uniform distribution here
depending on the hidden size like self
hidden
size and then initialize it
by this manner then rather rather than
just from
i guess from minus one to one no from
zero to one
i guess it's because okay so my
hypothesis is actually that's not real
regarding
the standard deviation but because it's
just from the zero
so i mean we can experiment later i
think that if we just
initialize it from the minus one to one
it would also work
quite well but let's leave it with the
standard deviation
as some implementations use it
but let's add also that we can try out
later
test test minus one to one because by
default is actually
default is zero to one and i think
that's why
it didn't work as good because at zero
these functions kind of like for example
10h
is uh minus one to one
so i mean uh the the
the so it will not be beneficial if it
starts from the zero
so that is this one then the next one
the next one is for the hidden hidden
state right so this is the
uh w then the u again
exactly the same thing just copied with
the different parameter name
and the last thing that's needed is the
bias of course so
bias again torch float
tensor
and bias we initialize the zeros of
course
like in most cases it's a be good
uh oh and they're remembered so
for the if i remember correctly the
optimal way how to do this
would be to initialize for forget gates
for the um for
for the part of the four get gates as
ones so we can
if i remember correctly so
initialize for get
gate part
as one
not be dif it will be not not be
difficult to do so you will see it very
soon
so if you initialize it for get gate as
one
if i remember correctly then um the
the the model in the very beginning of
the training
would uh pass everything so it would not
not forget
anything so that is that is good thing
in the very beginning
and later it will start forgetting
things so
but i think some frameworks actually
don't do this by default so
it should be checked like that
so that that is the initialization part
then of course the
the actual uh execution part here
so uh if you don't use the torch so
if you would use let's say keras or or
the original implementation of the
pytorch lstm or
or other popular recurrent neural
networks
then it would use the coda
then and i think was called where you
have
of highly optimized uh this uh this
rnn function call without
the for loop that is inside your own
code
and they handle this on like they try to
optimize as much
as they can but in in our code we need
the for loop because
they are dependent on the previous stage
states
that's why let's uh uh flip around a
little bit our dimensions so if
the x right now is in the shape of
patch so the batch sequence
and features that is what we have for
the x
what we want is to have the
x x sequence sequence in the front
and that we can get by permute the
pythons have the permute function
so we can split around the bastion
sequence
and get the sequence
batch features and actually if you use
the default implementation of the
pytorch
you must i think by default it's
actually the sequences first one
so you must specify which
part you want the first one and um
like you could mess up a lot of the
things probably if you
don't think about it and also here for
the hidden
states that that is also i think
important
let me think uh is it
important maybe i think not actually so
anyway
uh the interesting why did this was the
he so i'm not i think the hidden state
for the
original implementation is a little bit
different but here it will work like
that
so the h out is a list here that's
fine so cool so let's start four x t
in um x sequence so we go
each step of the x and produce
x t then we calculate the gates
so the gates that is from the equation
that i will maybe briefly show you again
so that we know the equation
okay this is the with the peepholes
without the peepholes
maybe so that you could see it a little
bit better
so just a second changing the
so here we have the w
multiplied by not multiply the dot
product
by x and the u dot product by h so this
is a
like uh split in four parts so the
each each of the parts produce the gate
with the sigmoid
and with the plus bias and then you have
these
uh human products and the calculation
of the uh basically the c and
h state that this is without the peep
holes and that is what we'll quickly do
and then we'll add the peepholes as well
so
if you go back to the code
so the gates the gate skate skates
okay it's gates maybe i will
also share this video later so if
somebody's
not catching up with this or if somebody
needs
the code afterwards then just let me
know so
torch the math move so they
if you know that matmo is the dot
product
just repeated over the patch dimension
that is very useful so x t
then the repeat so yeah x x right now
is actually so if you look at the shape
of x it is right now
here at this stage it's batch comma
features
but we actually need patch command
features times 4
like that that's why we will repeat it
so
we'll repeat it over the dimension of
the picture so
the batch dimension stays the same four
times the
feature dimension then the self
with the weights then
plus oh sorry
on the bottom part so plus plus plus
this one
so with a hidden state now the hidden
state
is something that will be changed by
every iteration so we just
let's say how h here from the
beginning of the h h in the beginning is
zeros
that other thing that we can just add
here there are a lot of
cool things that you can do i will
explain them maybe next time as well
but here you can what you can do is to
do learnable
learnable h0 so you already
know you already see that you can make b
learnable right it's super simple right
so you could also make h0 also as a
learnable parameter
and in some cases it would also improved
the
results so that that is also a neat
trick to do
so here would add
10 db so b
okay so now we have the gates so the
gates
let's just make clear what the gates
would produce for us
so the gates shape will be the
patch of course and then
what will be the next one the hidden
state times
four so four times hidden state
that is what we get from the gate
then we can calculate these gates
as in the equation so input gate at
point at t at time step t
so it's a torch of sigmoid i think in
the face downstairs paper they
they they did not so nice thing that
they used the um for the 10h also the
sigmoid design
but i'm not sure like is there any other
sign
for 10 h so that maybe you can just type
in tan h in the equation that would be
more clear
so gates
over the batch dimension copy everything
and over the
uh the this this hidden dimension
remember we have four times the hidden
size so we need to split it in four
parts
so again the the python pythonic way of
accessing the arrays very simple
very useful you know just your self
hidden state hidden size
very easy way to to select out only that
part that is interesting for us
and the auto grad takes care of
everything it will
keep track of the gradient between all
of these selections
so here again i think this is like much
more much nicer way how to operate than
in tensorflow like without the eager
execution where you would need to
prepare all this in advance with the i
think it was called scan operations or
something like that
so here you then collect it by the
second part
for get gate for get gate here
okay that means that the this
initialization of the for get gate
should happen basically in the range
of this one so i will add maybe in this
to do
command here but you could initialize it
in this range by one and that would be
beneficial
so if you think about the sigmoid
basically it will open the gate
if we initialize it in that way
so this is here then the output gate
or output gate again the next one from
two to three
and finally the cell gate
so the c t so right
so the cell gate is from three to
four or of course you can you could
simplify
these lines a little bit but i want to
make them uniform
so like that
so i will make a small pose maybe so you
can catch up
so if somebody is is missing something
please let me know
so if somebody is ready to move on also
let me know
a chat or or some other place
meanwhile i will try to find where is my
microphone so i can mute it a little bit
i will just give let's say a few minutes
and then we'll more
thank you for feedback i see that at
least two people gave me pluses and
ready signs in the chat really helpful
if you
communicate
oh okay where is the mistake
yes thank you thank you walters is great
with spotting mistakes
thank you really good that that's that
that's why i like working uh with the
with smart people so okay then h here
also a mistake if you look at the
equation
10h there is so then the next one
is c torch uh sigmoid so this will
update the uh um
like the equation is sometimes written
in the one line but i like to split it
in two parts
so for the cell state update so ft
times c plus
e so by the way this these are the
common products
pairwise multiplications
not dot products
okay that's cool so from that equation
then the h
torch tan h
and from the ct
multiplied by the output gate how much
we want to
let the signal come out of the cell
and then we add the signals to the h out
so
because we are developing our own
architecture we are
like i'm not tracking actually the c
state here
from the other point of view if i would
like to pass it back then i would need
to track it
so anyway if you would like to use this
for the rollout purposes so that you
would not give h
uh c as none then you also would need to
track not only h out as a list but also
c
as at least because otherwise you would
not be able to keep rolling the
next cell state without those two
vectors but still like for for our
purposes it's not needed
so not spending time on collecting c's
so here let's write this uh t
out just like a uh t h
out maybe i don't know like tens or
h state out whatever so torch
stack of h
out
and then the last thing is that the next
operations in our
function graph probably wants to work
like
like a normal like the batches in the
front
so we should get back the batch in the
front
so i will also change this one so t
out the h permute the
uh solid102 to get
the batch in the front like it was
so that that's
that's it for the vanilla version
basically
so let's see if it works also by the way
to actually train it fully you can just
copy all of this this stuff
into google collab and it should
immediately work i will also
do that in a second
so sorry i'm just like run it in in the
background to see that there is no bugs
hopefully there is no bugs
so i will also wait a little bit so yeah
let me know that
that you're ready and then we'll launch
this in
google call up you can okay it's working
for me so and
you can also try to launch it in google
call up you yourself right now
and we'll then we'll add also the uh
peep calls right
and then we'll move to the face the last
name implementation
um so yeah yeah let me know if it's
running for you
and i will also make a little pause now
again thank you for interaction i see at
least two people saying that you're
ready
hopefully others also will manage to do
that
um yeah meanwhile
and i will still still keep open this
this code here
so others can catch up and i will
run maybe
so what should i do so maybe you can
take a screenshot or
it will just return back to this in a
second
uh i just want to show you that how it
works also in google column
so if you go to the google club
that you can log in with your google
account
and create these
jupyter notebooks
with these notebooks the great thing is
that you can also access the gpu and tpu
for free of course so that's great and
also you can have
the funny animals running around so
there is a
there was some kind of way how you can
enable those so here
in the uh runtime um
so you you go into runtime and
the change runtime type to the gpu so if
you
so the only thing that you cannot do
here is to
mine for the crypto if you do that they
will ban you
and um i don't know like they probably
have a really
good protection of that somebody knows
how they do this this interesting topic
so then if i copy in all of our code
just in one cell
and i run it and look at the nice catch
going around here
it will copy the data set first
if i remember correctly so the exte
the mnist dataset and then
it will detect that there is gpu
available
first of all and it will include the
whole data set
then and run it on gpu so that that is
that should be a little bit beneficial
and we'll see how good it works
so you can try that yourself
just to copy this in in the google
column choose to have the gpu and run it
and in a second i will uh return to the
code so if somebody
didn't uh catch the the last part
meanwhile it's still training and then
we'll move to the
uh these peep holes so we'll implement
the peep holes
and i i suspect that the actual face the
last time
implementation based on the same code
that we just did
we will do the the next time probably
but um
you could also already you already know
how to do
do these the the basic skeleton so i
suggest i think that
most most of you should be able to also
implement facebook stem yourself now
but but still we'll do this together
also so
the one thing that's missing here is the
peep calls so
uh well while it's training on the
google column
for me already in the second epoch for
the three
class scenario i get 96
accuracy so if i show you this
so the correct way how to approach next
is to actually increase the difficulty
of the task
so as as you remember you can increase
the difficulty of the task by adding
more classes for example
or like uh changing the uh sequence
length from the 28
time steps to um over 700 time steps
so just by by changing the way how you
feed the data so there are multiple ways
how to increase the difficulty and break
the lsdm so we can see the benefits of
all the
things that we can add okay so for the
beginning part
let's we can start breaking some of this
so
i'm not sure like what exactly will
constitute to the
divorced performance but i suppose
i suspect longer dimensions but let's
start by adding just um
all of the classes so one two three
so maybe i don't i don't like doing that
so
these are mp so np uh
range 10
to list so now i have
all 10 classes included
all 10 classes included so if
the code is correct and i run it
then i should see a little bit
degraded performance hopefully
and then that would be a good um like
benchmark for adding
more features
okay also it will work a little slower
because now we have
at least like three times more data
points
so that is what we have there
okay let's move on to the people so if
you look here
so in this implementation
we have three places of multiplication
with this one
so i'm just thinking how to do this more
efficient
so we'll need to split we'll need to
split the ga
sorry for coughing really sorry for that
i i don't i don't have covered hopefully
so um because there's i have some other
issues a little bit
so here uh we have haman products
and learnable basically this seems like
uh just a learnable vector
and we also have that we need to split
the gates
actually in in some for the sea gate
we will need to split it because the of
course the p hole for the c doesn't make
sense at all
and also we can see that the c uh is
actually not
so here's the c my t t minus one and
here's the c
t so that means again we all need to
split gates
even more but let's look at how it looks
like here the c goes here the c
so in this schema if you don't have the
equations
then it's misleading because it seems
like that
ct is going inside the input and output
gates but if you look at the equations
then in input gate it's a c mean
t minus one so but that
actually would not be really the
so it's it's interesting i'm not sure
about that so we can try out as a t
minus one or
just t maybe that is a mistake here i'm
not not really sure so
they say that i'm not sure like from
which paper they take these peep calls
from here from the 11th paper here
we can double check that a little bit
later so just implement that and
i will double check if it's actually ct
in all of these places
if somebody else double check please let
me know in the chat
so that because it seems strange a
little bit
so if we go back to the code that that's
that should be not so
hard so we need a extra learnable
vector so what we can do
is to add the extra learnable vector
here
so self this will be the uh
peep so w peep
maybe with so this is a vector so let's
keep it with the small letter
yeah so like that so
and this one will be um
so if it's a like uh if it is a common
product there
maybe it would be no i'm not really sure
so we can
we can add it as a uniform so need to
learn
yeah i'm not sure really what should we
initialize it
so if you have ideas what you do so i
think once could work probably
i'm not sure about uniform so we can try
out and then change it
what what what would be the best
initialization for the peepholes
so
we have three three uh three these uh
three places where we have those
so the gates gates gates are here
so we'll need to start splitting the
gates unfortunately
now let me look at the equation
for input gate for get gate
and for output gate everything is fine
so input forget output and it's great
that
those are the first three ones
so the first three ones are are good
so we can split it
out here the using
xt by repeating three times not four
times
so we will repeat it three times not
four times
then this one we also repeat it three
times and then we'll select
this part also just for the first
three uh parts so again
the colon and then the hidden size times
three
and with this one with the u
and also here we'll need to select this
one
okay but only like that
great and we'll add the these peepholes
but
of course there's the issue with regards
to c
or ct
maybe it will not be exactly the same
implementation
so you can fix that a little bit later i
will also look at the
original paper but basically the idea is
that uh
you would try to add them here i will
use them the previous
c for that so
here let's we let's we can add the the
other one
so the peep
this one multiplied by
the c so the c
from the previous state repeated
repeat by one by
the by three
uh just as uh just a pairwise
multiplication
so these gates are good these gates are
good
then what about this ct
c t c t c t don't have the
uh that part so we need to
actually have that multiplication as a
separate part
so we can copy probably the
the first part of the gates
just so that we don't need to change the
code
we can copy also this part and give it
only for the last section
then also the same fragment should be
used
here and this without repeat of course
because now we have only
one fragment of this this whole thing so
here
and here without the batch
the mention here i guess
actually that's not the batch dimension
the input dimension
so like that
i think the peep holes in like a very
rudimentary
way is done so i will just check it a
little bit more
so not sure about that c t minus
1 or c t need to double check with the
reference paper so i will do that for
the next time as well
and next time we'll complete this with
with the complete phased lstm
now you already get a sense how to i'll
do all those connections in the
equations so i will copy this part
first of all i will try to run it of
course so
and then i will also show it a little
bit up and down
so you can catch up i will maybe
also give the the the my my code
as a finished one so yeah
i will add that uh but when this is
working i will post it in chat
and also add to the video
so there are a lot of things that you
can experiment yourself as well
i already added those to do comments
and we'll do interesting things next
time as well so what
is here what is here there is a bug
maybe walter
can help me quickly find it so what
is the bug this is the so
this part the sea
um here
it's hidden times four in
here is hidden times four so that's
that's good
so the good news about the pie torch we
can add the breakpoint
that is something we can br like the
bucket in the middle of the function
graph
we'll quickly find what is the problem
because i i specifically split it in
not in four but now in three so three
parts
and using those three parts together so
i i hope that it should work like that
okay why it's taking so long
hello hello maybe i need to restart it
i always have the temptation to restart
it oh no the breakpoint didn't work
why so okay let's add another breakpoint
here
try again
maybe somebody else can fix it faster
than me i'll check the chat
no it is
just the second
so so so it's working slowly slowly
slowly
why the brake point not firing okay yes
so and then we go low
just skip this the part that i wanted
oh no kind of don't know how to do the
breakpoint correctly
so i'll do or something like that now it
will work
again
i will find the problem
somebody with a faster computer can find
the problem faster than me
meanwhile i can look what we had with
the
google column google collab
actually after the sixth epoch with all
10 samples i get
also so let me share the google column
so i after the seven type of i get 97
accuracy so but the the curve is not so
steep as before so that's good
so at least we have some reference
basically so we can compare with
the uh phase dallas time later and with
the peep calls as well of course
so now it breaks in the right position
so and
the way how i would
oh
interesting
[Music]
you might be right
let me yeah so of course like the the
input
so this is not the batch actually you're
right
that is that is the input dimension
like that but then i'm also thinking
about the output dimension
hmm so if it's like
three times in this place
i think it should be also the the three
times of the
output so i don't want to
have them multiplied in the middle part
so
let's let's add like like this so the
selection also for the output so let's
see if that works
so i'm running it
uh and then also here i have the bot the
same bug so
um here as well
the same bug but let's see if it fails
here
or some other place um and
takes so long okay so now it
failed at hopefully the other yeah the
other position
so yeah great so the problem was that i
was looking at the matrix and i was
thinking that it
it contains the batch dimension as well
but actually it contains the
input dimension times the output
dimension and of course we need to
select out only that part
and if you run it again then hopefully
it will work
so i think it should work so
i will start copying the code in the oh
no it didn't work
so just a second what
didn't didn't like here
okay i i unfortunately messed up
something with the
okay what did i mess up the from
from the 3 to the 4 from the 3 to the 4
from the 3 to the 4 the size of the 16
by 0 16 by 0 how that's possible
16 by 0 16 by 128
128 h
vector that is fine
the x vector
so interesting so i will add the
breakpoint maybe walter has already
fixed this
if not maybe so i will try to fix it
so this is basically the same equation
as before
just for the only the last part of the
gates for the uh sea gate for the cell
gate
and in this part we don't use the
peephole because it doesn't make sense
from other side other side other side if
it's c
minus t minus one maybe it makes sense
so okay again you don't make the
breakpoint in the right position
unfortunately
so just a second
just a second this is the last bug
hopefully
before making this peephole architecture
work
and that will be the the architecture
that will
modify the next time so
here what is the problem let's start by
splitting the first equation the first
equation
works that's good good
if you look at the size of the first
equation 16 by
20 128 looks good
the second part of the equation size
looks good the last part of the equation
size uh-huh
okay but that should not be a problem
because oh no but that is problem
okay why this is a problem
so the bias
but but but
[Music]
hmm
okay let me check this one size
oh yeah you're right
okay that's not a problem because we can
uh
so what we can do is is is you that's
that's good observation
so we can just here have this as a gate
c as an another just another
variable and don't use the previous
variable
and then just use this here as a gate c
with a 10 h and
it should be fine then so if i run it
again
yep and i will prepare the some of the
materials for the next time as well
where you will see some very interesting
hacks like for example adding layer
norms for the
uh projections it it really helps to
increase the
performance of the lstms and also we'll
add these phase dell stem gates
and that's going to be great so yeah it
works
so thank you walters for saving me
multiple times today
yes so for example here we need a
specific new variable for the gate
and here we have the all three gates
of the those those together so the only
question here that is not really
answered
is this like so the question is is this
is this should be is should
this be c t
or c t minus one
so i will check that in the reference
paper
so if it's a c t and c
t u minus one then you'll need to split
this equation even more
but you already know how to do this so
just summarize for today so if we copy
all of this code and i will share this
also
shortly and go to the job board
first of all we get quite good results
with
the vanilla but if we add the peepholes
we should get even better results
hopefully
so it will not be that quick probably
so if i copy it here run it
so it's running so it'll be fun if if
actually uh the these people also
degrade the performance
then maybe uh we have some kind of bug
with the idea of that c
t c t minus minus one but still
so yeah let me know how it how it works
for you uh
meanwhile while this is running right
now
i will scroll back to the code
so you can copy it a little bit and i
will
also add this to the right now to the
uh to the google drive and share the
link for you
so if somebody had a problems following
it
and somebody wants to look at the code
that i have
you will have that opportunity just in a
moment and the next time
we'll complete the paper that i
presented you in the beginning
with all of those extra gates for the
phase the lstm
and also i really want to show you these
cool things that that we discovered also
with asia and some other research
projects
where you can have all those hacks that
you can add for the lstm to improve the
performance
so that that's about it for today
hopefully it was fun i finished
um next so the last thing missing for
the fun is the
fun data set so next time we'll use the
kaggle data set
for the accelerometer uh detection of
if the person is standing or or sitting
or something like that i'd
not exactly remember what was the the
labels
and so it's a high frequency
accelerometer data
and that will be more fun than just
using the
mnist but still this is interesting so
i'm adding the finished one here
so i added the finished one in the
so i added the finished finished version
inside the chat
window um yeah so let me know
if you have some comments questions
ideas
for the next time and i think in general
that's it for today
next time we'll we'll work on the phase
dell stem
and those hacks that i mentioned uh
hopefully you will see that lstm is
actually quite powerful model
and yeah the last thing if we go back to
the
jamboard here so with the peep
connections we see immediately
a very significant improvement after the
second epoch immediately we get 96
94 that was what we get before after the
sixth
epoch so again if we get the very high
performance after two epochs we need to
like increase the difficulty of the task
and then see what we can get
so yeah that's it for today hopefully
you liked it
hopefully next time also will be fun and
meet next time right