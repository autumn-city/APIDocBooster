[music playing]
Good morning.
Oh, it feels like a shame
that there's only one day left
of Amazon re:MARS.
And when we talked about new technology,
there's this old saw
that I think
a lot of us love to try it out,
that our phones
have more computing power
than the Apollo Guidance Computer
that got Neil Armstrong to the moon.
I would like to point out
that Moore's Law
has continued linearly such that
it's very likely that the chip
in your charging cable now
has more computing power than
the Apollo Guidance Computer.
Over the past few days,
we've taken a look
at the incredible real inventions,
innovations and products
that are altering
our day-to-day life, from AI,
to robots, to spaceflight.
It really wasn't that long ago
that these were concepts
only in our imagination
that could be brought to life
only through things
like television and books and movies.
And now, we have them in our cities,
in our homes, and in our pockets.
Today, we're going to hear
a bit about how-- Excuse me.
Today we're going to hear a bit
about how that work is
made possible through AWS.
The engineering work that takes
state-of-the-art theory and science
and puts that lightning in a bottle,
making tools that allow builders
to make their wildest ideas
into a reality.
We'll start with Swami Sivasubramanian,
who will cover machine learning,
working through everything
from AI-powered productivity tools,
all the way down to looking at
how we think about data structures.
Then we'll hear from
Roni Sole and Bill Vass,
showing us how simulation technology
is pushing
the envelope of the possible
from machine learning to robotics,
and how robots
are more capable than ever.
I'm still holding out
from my own personal BB-8.
And what better way to finish than
with the final frontier in space?
Tom Soderstrom will show us
how far we've come in exploring
our vast universe
and the technological shifts
that are changing
what will be possible for us all.
So without further ado,
please give a warm welcome
to Sivasubramanian,
VP of Data at Amazon Web Services.
[music playing]
Hello, and welcome to second
re:MARS conference.
So today you will hear from myself
and several other leaders in AWS
talking about machine learning,
robotics and space domains at AWS.
Re: MARS is a special conference for me
where we come together
as a team to showcase
and discuss the technologies that are
pushing the frontiers of technology,
and discussing the largest
changes in our generation.
By the end of today, I hope you will see
that the experiences we have envisioned
for tomorrow are much closer,
and these technologies
across these domains
are more intersectional
than you previously thought.
Now, let's start with the M in MARS
which stands for machine learning.
Machine learning is one of
the most transformative technologies
we will encounter in our generation.
It is already transforming
every aspect of our daily lives
and really revolutionizing
every industry as well.
And this revolution is possible
because builders of all types
are innovating in AWS.
Not just the expert practitioners
who are coding up
the next big algorithms
on the automize
infrastructures that we provide,
but the machine learning model builders
who are building these models
and training and deploying them
using state-of-the-art technologies
like Amazon SageMaker.
And then also developer services
that we offer
for incorporating AI
into our applications as well.
So today, what I wanted to do
is show you three massive areas
in machine learning
that is pushing the
machine learning space forward.
The first one is all around
natural language processing, or NLP.
Specifically, I'm going to zoom
into large language models
how it is pushing
the tech space forward.
Then the second area
I'm going to discuss
is around applying
cutting-edge ML techniques
specifically with respect
to data and around graphs.
And then the third one is how do
you build these ML models quickly,
but also highly accurate
using some of the latest
technologies that are available?
Let's dive right in.
In the past few years,
we have seen NLP space grow leaps
and bounds with the ability
to move beyond just simple sentiment
analysis and entity detection
to now these machine learning models
can understand the entire documents
and large language models such as BERT
and GPT-3 have really pushed
the boundaries on what's possible.
For instance, you can actually
with a simple prompt
tentrate entire text
and synthesize them.
So much innovation is happening
in this space.
And when you look at what does this
all mean for us on a daily basis,
beyond improvements on day-to-day life,
it is changing the way work gets done.
It is improving our productivity,
allowing us to focus our energy
on what we love to do,
instead of what needs to be done.
Let me give a few example
of how this NLP
is actually in the hands
of AWS customers today.
NLP is unlocking the potential
for data and users already.
If you look at Amazon Kendra,
which is our deep
learning-powered search service,
you can ask a very simple
query in English,
and it will automatically search
all the documents
and give you a highly accurate
answer within the enterprise.
If you look at the BI space,
where you used to be--
gone are the days where you are
actually connecting
to a data warehouse
and spend couple of days
if not weeks to find the right answer
and build a dashboard.
Now using NLP and several
other techniques,
you can ask questions in simple English
and get answers within a few seconds
with an entire dashboard.
The same thing we are trying to do
now with Connect and Contact Lens
where you get near-real-time access
to customer sentiment and feedback
and issues as well.
So when we look at what are
the other areas
NLP can really push forward
on what's possible
and makes life better,
one area that we seriously invested in
is in the area of software
development and operations.
ML tools have fundamentally changed
how software gets built
and operated in the cloud.
Now, if you look at what this does,
typically software developers build
these softwares and then they test it
and then deploy, monitor, and so forth.
This is fair.
In AWS, we are offered technologies
like CloudWatch anomaly detection,
and DevOps Guru,
which actually ingest all these
CloudWatch metrics and logs,
and ensures that it
automatically looks for what patterns
and is actually having an anomaly.
And that helps developers
identify these anomalies,
and remediate outages
so that they can actually focus
on what they really want to do,
which is to alter this application
logic and build the next big thing.
So let's dive into what does it take
to build an application then?
Now, as I said, developers have
incredible power
at their disposal today,
thanks to cloud machine learning,
and several other technology.
But when it comes to coding from an idea
I have to actually writing
the application,
there are a lot of hurdles
one needs to go through.
Ideally, I mean, they can reuse
a piece of code
that they have already written.
But if that is not available,
then they got to search
a documentation or search the web
and find which code is very relevant
to what they need to build.
And then they actually can start
thinking about saying, like,
hey, they didn't find the code
in which the programming language
I'm already using,
they find what they can,
and then tweak it to their language
of choice as well.
And so they may even have to
adapt it all together.
So if you look at this problem
in general,
the result is that developers
end up spending
a lot of their time in doing
what I call as
the undifferentiated heavy lifting.
And when I say undifferentiated
heavy lifting,
it's not the time spent in actually
thinking about the business logic,
about how to build the application.
They had to think about syntax
and semantics
and all the various things
related to, how do I build it
so that it is secure,
and how it is highly available
and ensures it meets my operational
criteria and best practices.
So we asked ourselves saying like,
so, how can machine learning
specifically NLP help here?
So we looked at some of the ways
that we can push
the latest advancements in NLP
and then address this problem
on behalf of our customers.
So that's why today I am really
excited to announce
the preview of Amazon CodeWhisperer.
It is an ML-powered code generation
service
that aims to help the developers more
productive in authoring their code.
CodeWhisperer generates code
recommendations
based on contextual information,
such as the code I have already
previously written in my IDE,
but also using simple
natural language prompts,
such as I can just write
in the comment saying, like,
"Generate a subroutine
to upload files to S3."
Then CodeWhisperer automatically
determines
which cloud services I need to use,
and what are the best practices to
use it for your specific languages.
And it directly generates the
entire subroutine in the IDE itself.
Unlike traditional autocomplete tools,
which work only one line at a time,
CodeWhisperer generates
an entire subroutine.
And the best part about CodeWhisperer
is that it lives directly in your IDE
and supports major
programming languages,
Java, JavaScript, and Python.
And it also supports IDEs,
various IDEs, such as VS Code,
PyCharm, WebStorm,
Lambda console, IntelliJ, and Cloud9.
CodeWhisperer leverages the latest
in large language models,
and it is trained
on a huge amount of data sets,
including Amazon code,
and various open-source code as well,
along with extensive API documentation
that is used to build
all this knowledge base.
And to make building applications
on AWS easier and faster,
we have trained the model
on the most common patterns
for building cloud applications as well,
so that you can build and innovate on
cloud much faster than ever before.
So I know that CodeWhisperer will
boost developer productivity immensely,
but it is equally important that we
do this in a responsible manner.
There is a lot of messy code
out there and containing bias,
security bugs and secrets.
So it is absolutely critical
that we incorporate capabilities
that mitigate these risks
for developers.
That's why CodeWhisperer
is highly differentiated,
where it comes in
with inbuilt security scanning.
To help developers build
applications responsibly,
we provide security scans to detect
vulnerabilities in developers' projects.
And also it has an inbuilt reference
tracker feature,
which detects whether a source
code recommendation
may be similar to the one
on the particular training data,
making it easier for developers
to decide
whether to use that code
in their data projects or not.
And finally, CodeWhisperer
also empowers you to avoid bias
by removing code recommendations
that may be considered
biased and unfair as well.
We see immense promise
in applications powered by NLP,
especially CodeWhisperer to help improve
the productivity of software developers
and help them write more
consistent performant code quickly.
Now, while we see innovation and what
ML-powered features can achieve,
we also see significant changes
in the tooling landscape
on how you process data,
and how you build
ML models on top of them.
Underlying all of these services,
platforms,
and tooling is the need to represent
our data in a logical manner,
in a way that allows us
to process this easily.
But historically, if you look
at the machine learning techniques,
or even data store techniques,
many of the streams of
the information in our world
have been analyzed in isolation
from one another.
One webpage at a time
or one search query,
one text passage, or one video.
But in reality, the real world
is very different
and much more nuanced and complicated.
We all know that these things
are intrinsically connected.
Let's take webpages for an example.
A webpage is linked to another web page
through either a hyperlink or a sitemap.
Or let's take an outpatient
electronic health record,
which has links to doctor's notes
and then prescriptions,
but also radiological images as well.
Now, to push the boundaries
of understanding all this data,
not only to store,
process and query them,
but also to run compute
and machine learning on top of these,
you need to represent this data
on an interconnected dataset.
We are seeing the emergence of graphs
as a much more natural data structure
for storing
and traversing this type of data.
And when I say graphs, I don't mean
an x-y axis
with the line or points in it.
A graph consists of two core
concepts, nodes and edges.
Nodes represent entities,
which talks about--
and edges are the lines between them
representing the relationships
between the entities.
There are fine-grained details
such as key-value properties
of the nodes and directionality
for these relations as well.
Graphs are very natural way
when we think about examples
like map of airports,
and the flight paths between them,
or a social graph,
where you can map the relationships
of individuals and groups as well.
So what can we do with these graphs?
If we were to construct a dataset
of an airport in a graph model,
we can easily answer queries like,
find me a flight path from,
let's say, Vegas to London Heathrow
with no more than one layover.
Now, as humans, we naturally
reason about graphs
because our mind works
with interconnected datasets
much more naturally.
But for computers, it is not
always that straightforward.
First, you'd need technology
to be amenable
to actually be able to store,
process and query them
at the data store level.
And then you need
machine learning algorithms
to be able to process them,
these graphs, at an efficient manner.
And this is where Amazon
already have been making
a service available
called Amazon Neptune,
which we launched
to help customers store,
process and query
these large graphs easily.
Now thousands of customers today
have been using Neptune
to show their graph data
for knowledge graphs,
security graphs,
and identity graphs and more.
And bar graphs
are representations of data.
Graph Neural Nets or GNNs
allow us to apply machine learning
to our graphs to gain
even more valuable insights.
I have deep conviction that GNNs
are the next big thing
powering an emerging set of technologies
like how Janssen
is developing a novel protein
folding solution with GNNs
and large language models,
or how COVID-19 drugs
are being discovered
with drug repurposing
knowledge graphs as well.
GNNs will alter the way you
think about solving problems.
We make extensive use
of graphs in Amazon.
They power our Customer
Trust Organization,
a 30,000-person team
leveraging tools built on graphs
to identify Marketplace abuse
such as fraudulent review schemes.
Across Amazon, we have multiple
15 to 20 billion node graphs
and at our largest 100 billion nodes.
And at AWS, the fraud and abuse team
has a graph with 100 trillion edges
performing class predictions
for every node in the graph.
With GNNs, this team was able
to identify 10 times
as many malicious accounts as
the previous rules-based systems.
They even power some of
our AWS services today,
such as Amazon Personalize
for personalization
and Amazon Fraud Detector
for fraud detection as well.
But how are graphs and GNNs
different from traditional
deep learning methods on normal data?
Let's compare images and graphs.
In images, concepts like orientation
are important for pixels,
but graphs do not have
the same limitation
where a node's representation is the
same even if neighbors are shuffled.
Images also have a fixed size
allowing for efficient computation
with dense tensors
and numerical kernels,
whereas graphs are sparse
and irregular with wildly
different shapes,
sizes and a massive range
of interconnectedness.
This leads to computations
with both sparse and dense matrices,
which are super hard to optimize.
So when you think about training--
running training and inference
on image datasets
which involve a large number
of discrete objects,
it's very easy to scale them out
because they can be scaled
in parallel as well,
whereas with graphs, you got to
operate on a single giant graph
that often requires machines to load
the entire dataset in memory as well.
So given the unique properties
of graph data structures
and GNNs constant need for tools
to be able to train them.
Today you can use Amazon Neptune ML,
a capability for our
purpose-built graph database
offering to make node, edge
or graph level prediction tasks.
Now to further expand
the access to GNNs,
we have been contributing to DGL
the deep graph library,
an open-source Python project
with more than 10,000 stars on GitHub,
and over 200 contributors globally.
Amazon started with graphs
and GNNs way out of the cuff
as it enables us
to query complex datasets
even at immense scale.
We see graphs as allowing us
to think with an extra dimension.
If you're not using graphs to gain
better insights with your data,
I encourage you to try setting one up.
If you're already using graph data,
now is the time to train your first GNN.
And AWS will continue to be
at the forefront
of ensuring this technology
is effortless to use as possible.
Now, having the right tooling
for machine learning
is equally important for GNNs
as it is for every other part
of the machine learning process.
Practitioners are always looking
for better ways to build,
train and deploy models,
no matter the data type,
business problem or model architecture.
Here, open-source frameworks
like PyTorch, TensorFlow,
MXNet and other tools have shaped
the way the builders train
and deploy ML models.
And many of these frameworks are
regularly used on Amazon SageMaker.
Today, I wanted to take the time
to talk to you
about one of these frameworks
and that is AutoGluon.
AutoGluon started as an internal project
from two Amazonians with the humble
goal of automating repetitive
tasks that existed
during ML experimentation.
Today, AutoGluon is an open-source
Apache-licensed project with over 1.
9 million downloads
that is purpose-built
to streamline the ML journey for text,
tabular and numerical use cases.
And it has contributors
from Amazon, Nvidia, Intel,
[PH] Berkeley and many more.
And in a world where these
training methods and models
have pushed towards larger and larger
models with more parameters,
AutoGluon achieves these improvements
by working smarter not harder
using a combination of techniques
like ensembling, stacking and fusion.
The result is the most accurate
multi-modal training
and fine-tuning available today.
AutoGluon takes on the burdens
that are challenging
even to great Ph.D.
engineers like feature engineering,
model selection
and hyper-parameter optimization
and automates traversing
all these through
during the experimentation phase.
This functionality
is commonly known as AutoML,
which automates all the exploration
of various options and configuration
when it comes to ML training to help us
arrive more quickly
at an accurate model.
With AutoGluon, developers can tap
into all of this functionality
to train their models
with only three lines of code.
Now, when you think about it,
keeping up
with machine learning frameworks
is like a full-time job
with so many new frameworks
and models being created
every single day.
Today, state-of-the-art may be obsolete
in the next few months altogether.
So that's why AutoGluon is created
with the mind
at a higher level of abstraction,
allowing you to automatically take
advantage of all these ML frameworks,
even as new ones become available
via a single elegant API.
Let's take a look at some
of these techniques
that make AutoGluon unremarkable.
With ensembling, we move away
from trying
to actually find the single best model
and instead construct
a blend of multiple models
that contribute to single inference
resulting in a more accurate predictor.
Then the second technique
is AutoGluon performance multi-layer
stacking, which is the most
powerful ensembling technique.
Commonly used by Kaggle grandmasters,
what stack models do is it trains
layer by layer rather than all
at once with backpropagation
like a traditional CNN does,
allowing them to be rapidly trained
on a regular hardware.
And the last one is fusion,
which helps create a model
with inputs of various data types.
With input fusion, AutoGluon can
aggregate multiple models
that fuse data types like images,
text and tabular data
when making a single prediction.
At Amazon, we navigate a broad set
of questions about our product listing
that we sell to our customers
that usually result
in a binary yes or no answers.
Does the product need an outlet
or does it contain materials
that may be banned in a certain country?
Or is it child safe?
These are questions that are rather
not explicitly enumerated sometimes.
Now this is where AutoGluon helps.
Now, let's take, for example,
this Gillette Fusion product listing.
AutoGluon allows you to use
all of this data at our disposal
to answer a complex question,
but what is intuitive like
how long does this product
last across mixed data types
like product images,
description, title, and many more.
The model trained by AutoGluon
can accept each of these fields
as inputs and use them to surface
a single answer to the problem.
The best part is this works for
any product for any type of problem.
At Amazon, we had 20,000 different
binary classification problems
that are powered by models trained
with AutoGluon just like this.
And neither the use case
nor the flexibility cancel
the cost of accuracy.
So we tested AutoGluon against
a number of live
and historical challenges
on Kaggle and MachineHack,
and AutoGluon consistently scored
at top 1% scores
across sentiment classification,
book price prediction,
and Mercalli price prediction,
and many more as well.
To me, the superpower of something
like AutoGluon is not about accuracy,
it is the incredible time to value.
In one example, AutoGluon reached
first place in less than four hours.
And it took a team of data scientists
over two months
to beat its accuracy
in the leaderboard score.
Think of the opportunity value
you get with the power of AutoGluon
and AutoML at your disposal,
just with three lines of code.
One company making use
of AutoGluon is Capcom,
a videogame company that has created
some of the most iconic characters
and franchises
since its inception in 1983.
They used AutoGluon to do customer
churn prediction
and built a model with 94% accuracy
that resulted in significant
cost savings for them.
Now AutoGluon is natively integrated
in Amazon SageMaker,
so I encourage you to try it out today.
And with some of our newest features,
in Version.5,
like multi-modal fusion and support
for time series forecasting,
we're excited to see
the unique models AutoGluon
is going to help our developers build.
But in machine learning tools
are just one part
in training more effective models.
We have been seeing innovative
techniques like transfer learning,
simulated training enrollments
and synthetic data refinement,
pushing the boundaries on what
is possible with machine learning
as well as what industries
it gets applied to.
To tell us a bit more about
these emerging trends
and the role they play
in the robotics space,
please welcome to the stage,
Roni Sole, Software Development
Manager at AWS.
[music playing]
Thank you, Swami.
How the robots know how to deliver
a package,
flip a burger or open a door.
Some robots have tasks
that can easily be defined
by a common programming language.
It's a mixture of inputs, outputs,
conditions, loops and logic.
But the more autonomous a robot becomes,
the more likely to relying on
some form of machine learning.
One of the most significant advances
in robotics in the last five years
is the adoption of advanced
perception systems
that allow robots to see and
understand the world in nuanced ways.
These perception models are driven
by machine learning applications,
many of which are developed
and running on AWS.
Humans possess a remarkable ability
to grasp
and recognize objects in the dynamic
environments of everyday life.
For robots to assist us, we need to
be able to emulate these abilities.
Machine learning is the key
to enabling this.
Many machine learning breakthroughs
are coming from research institutes
like Columbia, MIT, and UCLA.
We collaborate to solve real-world
challenges using machine learning.
And robotics is one industry
where you see many of these
innovations show up in applied forms,
just like a robot delivering
a package to your door.
Here at Amazon.
We also have a large and growing
community of scientists
who are actively researching and
publishing innovative papers,
over 500 publications in 2021.
Together, we're providing
new ways and opportunities
for students and researchers.
Amazon is where research meets reality.
We're not only envisioning
the future, we're building it.
We have over 200 facilities
and fulfillment centers
all over the world today
powered by robotics.
We have more than 520,000
robotic drive units
across 15 different types
of robots in our fleet,
which process over 70%
of all customer orders.
Let's take a closer look at
our robotic fulfillment centers.
You can see our robots move
the pods to our human workers.
It's a great example of how
humans and robots collaborate,
each playing to their strengths.
In the video, we don't have
people running up and down aisles.
The robots do all the running.
You can see those robots,
carefully designed to operate
safely and avoid collisions.
That's all controlled by systems
at the edge and in the cloud.
This robotic automation helps Amazon
deliver packages
more quickly and safely.
This type of robotics brings
together many disciplines,
including computer vision,
path planning, localization,
and mapping.
Many of our customers are using
AWS machine learning
to build robots in interesting ways.
Boston Dynamics is
a great example of that.
Not only are they building robots
that can
walk around industrial settings,
they can also use thermal sensors
to detect defects and alert
maintenance staff to issues
before they turn into big problems.
They can be trained to walk specific
routes,
again and again,
capturing analog readings
which computer vision systems
built on top of machine learning
can convert into digital insights,
resulting in safer and
more efficient site operations.
Machine learning models learn best
when provided with an abundance
of simple images.
Yet, it's often hard to find
enough training data
to capture the enormous
variation and edge cases.
And even if you get a hold
of these real-world images
representing all possible scenarios,
the process of annotating them
accurately can take months.
It is, of course, possible to train
a machine learning model
with just a few 100 images.
But if you need to build a robot
like Robin here,
that is able to accurately
recognize and pick up items,
to do that your machine learning
model needs to be trained
on many thousands of sample images.
So what do you do if you just don't have
the number of sample images you require?
The answer is synthetic data.
And that is why I'm excited
to announce synthetic data generation
for Amazon SageMaker Ground Truth.
[applause]
We're so excited about this.
Synthetic data generation
for Amazon SageMaker Ground Truth
is a feature that enables data
scientists to generate image data
so they can train more accurate
machine learning models.
With synthetic data generation,
customers can source
high-quality and accurate
training data at scale in weeks.
And this is at a lower cost
than traditional data
collection mechanisms,
saving time and money.
We're not only talking about the cost
and time required to annotate images,
this synthetic images wouldn't
even exist in the first place.
Let's use the Amazon Box as an example.
To start generating synthetic
training images,
you provide a 3D model of an object,
perhaps a model of your product.
Here we've provided multiple
3D models of Amazon packaging
in different shapes and sizes.
If you don't have 3D images ready
to work from,
our team is highly specialized
technical artists
that can help you create them.
You also supply a number of scenes
that represent the environment
that the object will be placed in.
And then, Ground Truth will create
as many images
as you need of that object,
generating synthetic images
from 3D virtual environments
that represent real-world scenarios,
including physical forces and conditions
such as gravity and lighting.
You might think the photo
on the screen is real,
but in fact, it is
a synthetic photorealistic image.
Each photograph is created
with your 3D model or asset
mimicking your environment
and your camera configuration.
This camera point is really important.
If the robot you're building uses
a 720p camera,
you want the synthesized data
to also be at that resolution.
The end goal is to train an accurate
machine learning models.
So everything should match what your
sensor will capture in production,
from the camera resolution
to the lighting.
You'll configure the properties
and variances
that define each potential scenario,
such as object pose, size,
quantity,
surface variations, and lighting.
Next, you specify the number
of images to generate
as well as the desired label types,
such as 2D bounding boxes,
instance segmentation, and controls.
Within hours,
the service will generate thousands
of diverse auto-labeled images
and place them into
your Amazon S3 bucket.
And in case you find out that you
need a few more thousand images,
or if a new object type or variation
becomes available,
you can always kick off a new project
and generate a new batch
of synthetic images.
Finally, you can use
these generated images
with other machine learning
services on AWS,
such as Amazon SageMaker
to train new computer vision models.
Just yesterday, we heard here
how synthetic data
was used to build Amazon One.
I can't wait to see how you will use
this new feature.
And there is so much more to robotics.
I am so excited to share more
about robotics
and other ways in which the cloud
is helping customers to build robots.
Please welcome Bill Vass,
Vice President Engineering
at Amazon Web Services.
[music playing]
Thanks, Roni.
It's really great to see
how synthetic data
can be used
to improve machine learning models.
We're going to ramp it up a bit
and talk about synthetic worlds
and how to do simulation.
But you know, the technology
that Roni showed there can be used
in a whole bunch of other
industries outside of robotics.
For example, improving
satellite imagery,
or quality control in the food industry.
But I'm really here to talk
about robotics and the cloud
and how they work together.
During an average day,
I think a lot about the cloud.
And I also think a lot about robotics,
and how the cloud has transformed
so many industries.
I'm heavily involved here
in at Amazon in robotics.
And you know, when I talk about
the cloud and robots together,
a lot of times people kind of look
puzzled to me as like,
"Well, the cloud doesn't run on robots."
I think people just look at the robot
and see what the hardware looks like
and those kinds of things but they
don't think about the software
and the machine learning behind it
that was developed in the cloud
and gets pushed to the robot.
And I think that's where it's
really transforming things.
Robots are thought of as
independent entities,
the collection of wires and servos
and those kinds of things
with some smart,
clever software running on board
like R2-D2
or Johnny Five from the movies.
And there's still a lot of edge
processing.
And we'll talk about how important
that edge processing
is for low latency and high
performance on the robot as well.
But the real power of robots
is not the physical hardware,
it's going to be the software
and machine learning
that are used to train
and orchestrate and manage them.
And that's where the cloud is just
transforming the robotics industry.
For example, as we move
to driverless vehicles,
you know,
the hardware of a car really isn't
changing that much, motors and engines
and those kinds of things,
wheels and tires,
but it's really the software
and the machine learning
that enables autonomous driving.
Historically we've, you know,
had this manual way to build robots,
just trial and error.
I remember putting my source code
on a thumb drive
and sticking it on the robot
and you manually tested in the lab,
and it would bump into things
and you try it again,
and you try it again,
really laborious process,
and you're constrained to
the physical environment of a lab.
Took a long time to do that.
But, you know, if you really
want to accelerate robotics,
you have to use simulation.
As I often say, if your business
is not leveraging simulation,
you're basically standing
still to your competition, right?
Because simulation
is changing everything.
Like a lot of you out there today,
I flew here on a plane
with a well-trained pilot.
And, you know, just as we're talking
about using machine learning,
and a lot of data and simulation
to train a robot,
we've been doing it for a long time
with things like pilots
and I even learned to drive
initially on a driving simulator.
Today, the simulations
are quite advanced.
In the past, not so much.
But it's amazing now the kinds
of things you can do in simulation.
You can have extraordinary
circumstances,
you can have varied weather,
even to the extreme.
You can simulate failures,
things that are really too dangerous
to try in a real aircraft.
And you can vary airports and terrain
and those kinds of things.
This also saves a lot of money
because you don't have to
have planes dedicated to it.
Just like we see with machine learning,
the more data and training you do,
the better you get to be
as a skilled human.
Think about teaching a robot to drive.
That's a huge problem.
Roni explained earlier that, you know,
these complex models
require a lot of data.
You need radar, camera,
LiDAR, things like that.
And you have to fill that data in
and build a complex training model.
You really need to drive
millions of miles to do that.
If you wanted to do that
in the physical world, let's say,
do a million miles of training
in the physical world,
you need 2,500 cars traveling nonstop
at 50 miles an hour
for eight hours a day.
That's not very practical.
But to really build a driverless car,
you need tens of millions of miles
that you need to drive every day.
And that's physically impossible.
And you just can't do it in the
real world without simulation.
Aurora is one of our customers
that's building this new technology
to allow cars and trucks
to drive themselves.
They're testing their vehicles
in the real world
all over the country as well.
They use a combination of LiDAR
and cameras
and things like that and radar to see,
you know, hundreds of meters
around them, and 360 degrees.
They run millions of simulations
every day to accelerate this.
Simulations allow them to test
in virtual worlds at a scale
and at a cost
could not do in the real world.
They've built this amazing technology
that allows them to simulate
the way light moves
through the physical world
in these simulations.
And they can vary roads
and vary vehicles,
they can have multiple intersections,
changes, weather change and lighting,
and a lot of it is
procedurally generated.
And I think that's where
things are going.
When you're building software
for robots,
like we do in our fulfillment centers,
simulation has a lot of advantages.
So first of all, you're not
constrained physically, right?
You can have an infinite world
if you're like in the virtual world
and you're not constrained by hardware.
You can simulate all sorts of scenarios
that are really unusual
and happen infrequently
so you can make sure
the robot can adjust to it.
You could run things in parallel
and do years of training in just hours.
So just like in the driverless
car examples,
trying to do this in the physical
world is not really effective.
At Amazon, we've adopted
simulation techniques
to really speed up development.
And a lot of the robots you've seen
at this conference from Amazon
are taking advantage of that simulation
where it's replacing manual tests.
We're using simulation.
It really becomes part
of the training process.
We're building high-fidelity models
that bridge the gap between the
real world and the simulation world.
And robots can learn
directly from the simulations.
It allows you to analyze the data
in real-time,
manage fleets of robots,
and even deploy from the cloud
to the robots.
For example, you know,
when we started to put together
our fulfillment centers,
they were really built for humans.
And a human would probably put things
in alphabetical order
like they do in a decimal system
and those kinds of things.
But through simulation, we learned
that as you randomize
the product much more,
you can do a lot more in parallel,
rather than serial,
and you can move a lot faster.
So those kinds of insights
only come from simulation,
and you can do this kind of testing
before you build a fulfillment center.
Okay.
So we're all really excited
about simulations, now what?
You know, building these virtual
worlds like that
to simulate your robot in.
So how do we make it easy for anyone
to build large-scale simulations?
So, to create a simulation,
you'd have to hire a 3D artist,
put the physics together, it gets to
be expensive and time-consuming.
However, with procedural generation,
we can do it a lot faster
at a lot lower cost.
So we introduced RoboMaker WorldForge.
It uses procedural algorithms
to generate 3D environments.
And you can see up here, those are
all generated by WorldForge.
They guarantee individual
random houses in this case.
You can choose your surfaces,
you can choose number of doors,
number of windows, number of bathrooms,
all those kinds of things.
And it'll generate those for you.
Of course, people are starting
to leave their houses these days.
So we're working on extending this
to generate office buildings
procedurally,
fulfillment centers procedurally,
and outside spaces as well.
Once you have your virtual world,
you can spawn out
to hundreds of
thousands of simulations if you want.
And that's an actual robot.
They're in a RoboMaker simulation.
You might notice the resolution
is lower than what Roni showed.
That's because the resolution
matches that of the robot.
And you can do it without
really understanding the cloud.
Just click a button and spawn
out these simulations.
But in the real world, you're not
just managing a single robot.
Usually, you're having them
work together for a common cause,
and you need to orchestrate them.
So that's why we developed
and delivered IoT RoboRunner.
So it allows you to have a bunch
of pre-built libraries
that developers can use for managing
how the robots work
spatially together, orchestrate them
and collaborate for a common task.
Of course, you know, you've seen
simulation,
here's a really powerful tool.
But what if you want to simulate
more complex, larger things?
And this is where digital twins
become important.
So digital twins are
a virtual representation
of the physical object, right?
Process or service.
It mimics the object.
You could have a digital twin
of a jet engine, a car, a robot,
a conveyor belt, a whole factory,
a whole smart building, a whole city.
In the past, when you wanted to do
physical tests,
you had to like change
your production line manually
and to try to optimize it.
Now you can do that virtually, right?
And this is a game-changer
in the long term.
We're pretty excited about it.
But you know, it's really hard
to build these.
You've got, you know, the 3D
environment, how you can pull it
together,
you're connecting your IoT data,
your cameras, all those kinds of things.
So we released IoT TwinMaker.
And it's a service that makes it easy
for developers to build digital twins.
So you can, you know, build dashboards,
you get your 3D environments,
you can integrate your cameras,
you can do buildings, industrial
equipment, all sorts of things.
And it's going to really transform
how digital twins are made
from an ease-of-use perspective.
However, you know,
robots are operating well
beyond the factory floor.
They're doing the dull, dirty
and dangerous jobs out there.
And they're out in the field
performing tasks like search
and rescue
in mountains or in ships, oceans,
you know, working in wind farms
and solar farms,
those kinds of things,
and even on other planets.
And they need extra storage
and compute as well.
So we built a ruggedized device
that brings AWS to them.
It's called the AWS Snowcone.
This allows our robotics customers
to connect it to their robots
and do edge processing and storage,
especially when there's no network,
and they can ship the data back.
You can run it pretty much anywhere.
And because it weighs less than
five pounds,
it was specifically designed to fit
on drones, on commercial drones
and I actually used one here
to create a 3D model of my barn
so that I could put solar panels
on it in the best way.
They're designed to handle
extreme environments
and run that compute
and storage locally.
And when I say rugged, I mean it.
I asked my friend Adam
who was out here earlier
to see how just durable they were.
So remember, please don't try
this at home.
Adam is what we call a professional.
In the 1970s, Tonka had an ad where
they showed how durable their trucks
were by having an elephant stand
on one of their toys.
And I remember being nine
and yelling at the television,
"Give me a hammer and an hour."
[music playing]
Put that out.
Thank you.
I had to admit defeat.
But that was fun.
I'm relieved that he had
to admit defeat there
because robot's operating some pretty
extreme conditions out there.
And the Snowcone and the Snowball
are perfect companions for robots.
I hope today I've given you an idea
of how we're helping people
build robotic systems and
autonomous systems from end to end.
Our goal here is to provide
a simple environment to develop,
test, simulate, train and deploy
and with this centralized control
and decentralized execution,
with security every step of the way.
We want you to be able to run
your robot anywhere, anytime,
with and without the network.
For example, recently
one of our collaborators,
Axiom in the space area asked us
to find a way to help
astronauts manage on-orbit research
and data more effectively.
So we took the snow products
to a brand new rugged
mobile edge location space.
So that's the picture on ISS, right.
So today, I'm pleased to announce
and share that Axiom ABS
work together to fully integrate,
certify, and operate AWS Snowcone
on the International Space
Station, a part of the Ax-1 mission.
The Snowcone-- Yeah, it's pretty cool.
I admit.
So we were able to communicate
to the Snowcone,
make changes to the code if needed,
run machine learning applications
right there on the edge in space,
automatically generate tags
for imaging, analyze onboard photos.
And I think it really is going
to accelerate things
on the edge in space.
This experiment successfully
demonstrates
how some of the storage, compute
and machine learning capabilities
are possible in low Earth orbit
with existing AWS technology.
And in the long term,
we're committed to continuing
to remove barriers
for our space customers
and enabling on-orbit
needs in future missions.
So watch this area carefully.
And to tell you more about it,
and the future of space technology,
I'd like to introduce my friend,
Tom Soderstrom,
to come up here
and talk to you about it.
Thanks, everyone.
[music playing]
Well, I'm excited.
How cool is that?
It's snowing in space.
Thank you, Bill.
It's great to be back at re:MARS
in-person.
And I must say you are
all doing your job.
You're inspiring each other,
you're being inspired by each other,
and all of the intersections
of machine learning,
robotics, and automation in space.
Before landing at AWS, I was
Chief Technology Innovation
Officer at NASA's Jet Propulsion
Laboratory, where I helped infuse
new technologies into space missions
and make engineers,
hopefully, more productive.
Now I serve as Director of Chief
Technologist globally at AWS.
So my head is now in the cloud,
but I must admit,
my heart is still in space.
So it's fabulous to be here.
I'd like to talk about three things.
Basically, how do we answer
the age-old questions?
I'll tell you what they are.
And how do we have… take benefit
from this new space economy,
and what is it?
And finally, how do we use all this data
to help save our own planet?
We as humans, we've been looking
at the sky for millennia.
2000 years ago, the data
scientist of the time
asked all these questions.
Were they data scientists?
Yes, they were.
We just call them mathematicians,
and philosophers.
And they published their findings
in the great library of Alexandria.
And it took years for other people
to start understanding it.
And of course, Earth was
the center of the universe.
Well, imagine now that
you're a scientist and engineer,
which probably a lot of you are,
and you're using a handheld telescope
to help navigate the oceans,
then you turn it to the sky,
and you realize that there
are moons orbiting
Jupiter.
You can see the surface of the moon.
And you realize that maybe
Earth Copernicus was right,
maybe Earth is actually not
the center of the universe.
Well, we've come a long way
since Galileo.
In this just last century,
we walked on the moon,
we explored Mars,
we sent human objects such as the one
you've seen behind me, Voyager,
to the outer reaches
of the solar system and beyond.
We even discovered exoplanets,
if we one day needed to export humanity,
would we have a place to go?
We discovered 5,000 of them
so far roughly,
and roughly 50 of them
are in the habitable zone.
So the difference now,
is that in the last five years
and for the next several years,
it's no longer a spectator sport.
Space is a participation sport.
We have several people from NASA here,
from Jet Propulsion Laboratory,
and lots of builders and data
scientists and citizen scientists,
all talking and working together.
And let's take a look at what
enables this new space economy.
A good example you've heard
about yesterday,
which is the Orbital Reef space station.
So it's the commercialization of space
where commercial companies
and governments will work together.
You can think of this
as a mixed use business park,
where it's for tourism,
and that's a good thing
because it generates interest in space,
but also as a place to do research,
to do research that can
only be done in microgravity.
And if you want to be a tourist
today, go into the exhibit space
and take the virtual reality tour
of this wonderful creation.
So what are some other innovations
that are enabling space?
Well, you heard about machine learning.
It's fundamental for any space mission,
whether it's for analyzing
imagery of satellites,
or planets, figuring out where to land,
or if it's something as exciting
as flying or driving on Mars.
So this Perseverance rover
that NASA Jet Propulsion Laboratory
created has to be able to drive itself,
roughly 100 and 40 million miles away.
No small feat.
But what are we seeing now?
We're seeing new generation,
you've heard a lot about
machine learning from Swami.
And this is… we’re just
in the infant beginning.
This having the Snowcone in space,
you can analyze the machine images.
It's going to be amazing.
You don't have to go all the way
back to Earth every time.
Another enabler is autonomy.
Autonomous system is a must for space.
It takes an average of two
and a half seconds
to get to the moon,
five to 20 minutes to Mars,
20 hours to Voyager.
These systems have to be able
to perform autonomously,
and they have to be able to be updated.
Jet Propulsion Laboratory, for instance,
even updates the satellites
while they're flying,
or once they've landed on Mars, amazing.
Robots.
Robots are enabling the space economy.
They're helping us
explore the foreign planet
and make them safe for humans.
And even something
we can now fly on Mars
with the Ingenuity Helicopter,
which is like flying helicopter
at 100,000 feet on Earth,
which is pretty much impossible.
So the impossible is becoming possible.
We can think of these robots as things,
so you have Internet of Things,
you'll soon have Internet
of Space Things.
And we saw from Lunar Outpost
that you can now have a
commercially-built rover on the moon.
It's an amazing future.
So another driver here of the new space
economy is dramatically
increased interest in space.
For instance, there are 23
new space agencies
created in just the last 15 years.
In additional, the barriers
to entry have been decimated.
Compared to 15 years ago,
it's roughly 15 times
less cost to build one spacecraft,
20 times less to launch it,
and hundreds of times
less to analyze it.
So everybody can participate.
It's much cheaper.
So what are some examples
of these innovations?
Wildfires.
I live in Southern California,
I care a lot about wildfires.
And if you can detect them earlier,
you can save lives and structures.
A company called Exci in Australia,
can use satellite imagery
and detect wildfires
within three minutes,
alert the first responders that can
then respond much more quickly.
So what if you could actually move
that app into the satellite itself?
Now you can detect those wildfires
in seconds,
if those satellites
are communicating with each other.
And you could have more innovators
participate in building this
if you had more internet access
in the world,
which projects like Project Kuiper
is doing by launching satellites
that will provide this access
so we will have even
more builders participating.
We're also fighting climate change
using more, these satellites.
And we're looking for water.
These are two NASA instruments
that will be launched,
one very shortly and one in few years.
And they are examples of these
governments across the world
working together to look
for the key to life,
in space and on Earth is water,
we think.
So same thing on Earth, SWOT,
for instance,
is going to understand
the chemistry of water,
and help with more freshwater and
reduce floods and things like that.
The other piece that's key here
is it's generating a lot of data.
These two alone is going to generate
roughly 100 terabytes of data every day.
And that's a lot of data.
So space innovation is not limited
to just space agencies
and governments
with huge resources anymore.
AWS is helping to democratize
the access to space.
Let's look at some examples.
Maxar. Maxar delivers
high resolution images
and geospatial intelligence 24 by seven.
They also predict major storms.
To do that they use
high-powered supercomputers,
except they no longer have to,
they spin it up in AWS cloud.
And it finishes all those
amazing calculations in 45 minutes.
And then they're done.
So instead of having a data center
sitting powered up all the time,
it just 45 minutes of work, then done.
And so they generates a lot of money
that's left over
for more research and science.
So these are the type of examples
of purpose-built high performance
computing now democratized and available
to everyone that's going to help.
Also, you don't need to own
even your own antennas anymore.
You can use AWS Ground Station
as a service.
It has 10 stations across the world.
So if you missed one data dump,
in orbit takes about 90 minutes,
you can do the next one
and the next one.
And you can now start controlling
constellations of satellites.
And when you talk
about the machine learning
and what you can do,
as soon as the data hits the dish,
it can be calculated on,
so you can do real-time decisions.
And you pay only for what you use.
One example is Capella Space.
So Capella Space has a constellation
of satellites
using AWS Ground Station, and they're
using synthetic aperture radar
so they can seek through clouds,
they can see at night.
So first responders or in
a defense situation,
they get their high-resolution
images within minutes.
Highly unprecedented.
Amazing.
So cloud and space
go together very nicely.
It's about getting the data into a place
where people across the world can use it
and then getting the compute
right next to it
so you can calculate
on it very, very quickly.
So it means that you as builders
can reach for the stars
without a big upfront investment
and generate amazing results,
an amazing amount of data.
So if we look at data,
we're creating a lot of data.
IDC is estimating that we'll have
97 zettabytes of data
created this year alone,
that's 97 million petabytes.
Much of it will be generated by users.
But when you think about the Internet
of Space Things,
it's going to be even more data.
So what do we do with it?
How can we benefit
from this explosion of data?
Well, we need to move it into
what you're interested in.
So if you're an oceanographer,
or if you're interested
in thermal or topographical,
or want to save our coasts,
we need to make it easy to use.
So I'm really proud to work at AWS
because, one thing is
the Amazon Sustainability Data
Initiative that's taking advantage
of making all this data accessible.
So if you're a builder,
and you're creating data,
and you are creating data
for the public good,
you can store it in AWS for free.
Storage is free, egress is free.
We even give grants that you can
convert this data
so that other people can use it
without being pure data scientist.
Companies, customers like NASA, NOAA,
the UK Met Office is using this a lot.
And the data sits along other datasets,
including from customers
like the Space Telescope
Science Institute, the James Webb folks,
and 100 petabytes of data
across 337 high-value datasets.
What can you do with that?
Well, you can do a lot.
For instance, National Center
for Atmospheric Research
and Silver Coolining, AWS,
are doing probably
the most complex calculations
that are global climate change.
There are millions of lines of code,
thousands of computers
crunching on petabytes of data
to calculate and generate,
what does global climate change
look like?
This is now being made accessible
to modelers and models
across the world through the cloud.
So we will see even better results
on this,
and you're saving our planets,
should be our key priority.
So we're seeing builders accelerate
the space economy with this,
generate massive amount of interest
and lower barriers to entry.
And it's customized
into where they need to be.
So in Zanzibar, looking at mangrove,
and because it's a barrier to storms
and rising seas,
Digital Earth Africa,
is leveraging this.
And it this public-private partnerships
is really driving
this amount of innovation,
and we're seeing it at this conference.
Another one is the African
Earth Observation Challenge.
I was a mentor and a judge for them.
And I was very proud to see all of
the innovation that's happening.
The winner was Astral
Aerial Solutions based in Kenya,
they use drones to support farmers
with actionable data,
like health monitoring, precision
spraying and yield prediction.
So making this data available
is helping the local
and the global economy.
So we believe that having these
innovation challenges
is a great way forward.
So I'm really pleased to announce
that this, at re:MARS,
we launched the Amazon Sustainability
Data Initiative Hackathon.
It's a giant hackathon,
you can use all of this data,
you can enhance your right if you're
a builder, enhance your reputation,
enhance your resume, make some money
and help save the planet.
You can sign up right outside here
in the exhibition space
at the ASDI booth.
So Space Accelerators
we did it last year.
We got 160 entries, and we picked 10.
And we gave them up to $100,000
worth of credits.
And they did amazing work.
These were startups.
Together, they raised over $350 million.
So space is a booming business.
A couple of them, you heard from
Lunar Outpost, Justin Cyrus,
yesterday, having a habitat
on moon and commercial rovers.
Edgybees is another one.
They take a different approach.
They took satellite data and they
augment to put augmented reality on.
So if there's a fire, first responders
can see through the smoke
and see where people live even though
if the signs are burned up.
Satellite Vu is looking at
through thermal from satellite images
to see if buildings
are leaking energy or heat.
That's a great way to start
reducing the energy use,
and for the building owners
to not waste energy.
It was so successful
that we did it again.
And this time you can see the winners
or the ones
the 10 that were selected,
the difference from last year
to this year,
very much of an international
gathering from all over the world.
Last year, it was mostly U.S.
This year, it's all over the world,
and each of them will receive
up to $100,000 worth of credits.
So when you look at these names
on the screen behind you,
there are innovators behind them.
They're builders just like you.
So talk with them, meet with them,
find your niche and partner to help
further drive the space economy.
So let's take a look into the future
for a second.
Let's start with, you know what it is?
James Webb Space Telescope.
It is both sophisticated and hearty.
It's a 21-foot telescope, 100
millions away in Lagrange point two.
It's powered by 132 actuators
with nanometer accuracy,
to keep these
18 hexagonal mirrors aligned.
And it's already survived
the micro meteor strike.
It's tough and it's sophisticated.
It's so sensitive.
It could detect the heat
of the Bumblebee on the moon.
And it's going to help us look into
the beginning of the Big Bang,
back 400 million years ago,
and after the Big Bang.
And it's going to hopefully
find new exoplanets, if again,
if one day need to have a place to go.
So how sophisticated is it?
On the left is what
Spitzer space telescope saw.
On the right is the same image
from James Webb space telescope,
and it was still
just configuring itself.
In two weeks or so, we will see
the first images
from space by James Webb,
and can't wait to see it.
So we've come a long ways
since Galileo’s space telescope,
this telescope
and to James Webb space telescope,
seeing the moon to seeing into
the beginning of the universe.
So we're going from publishing
in books to as soon as the data
hits the dish is available
everywhere across the world,
so builders can help us move it forward.
So I think we will answer
these big questions
that affect humanity very shortly.
I'm also extremely inspired,
and I hope you are too, by Artemis.
We're returning humans to the moon,
and it's going to launch
its first uncrewed mission
in just a few months.
And it's going to have a permanent…
at least the goal
is to have a permanent moon base
at the end of this decade,
and then practice to go to Mars
and then have a Mars-moon base,
people are estimating, maybe by 2050.
That's an amazing future.
And you can all make it happen.
Your children or their children
may walk on Mars.
So as the universe is accelerating,
so is innovation in space.
Imagine being a space builder
400 years from now,
where do you think you'll work?
What planet?
What exoplanet?
And what tools will you have
at your disposal?
And what questions
will remain unanswered?
So what questions can we answer
in the next year, the next month,
maybe even by just doing some moonshots
that you're all doing together
and hear wonderful feedback
that you all really inspired
by each other.
So hopefully, you're enjoying
this conference.
And hopefully you were in
the exhibit space
and seeing those amazing exhibits,
and talk to each other.
And please help us save this planet.
Thank you very much.
[music playing]
I saved my favorite suit for last.
Hello again. We hope
that you are enjoying today's show.
Day three of re:MARS
definitely doesn't disappoint
and the next three speakers
won't either.
Next up, you're going to hear
from one of my heroes Nicola Fox,
who leads NASA's efforts
to explore the star
that makes life
on Earth possible, our sun.
Then Deepu Talla from Nvidia,
who’ll show us how devices
can be programmed using AI
and deep learning to perceive
and understand the world around us.
And finally, Richard Browning,
my friend,
will wrap up the day for us
with the demonstration of what it is
like to defy the laws of gravity
and make human flight possible.
So let us not keep you
waiting any longer.
Let's welcome to the stage, Nicola Fox,
Director of NASA's Heliophysics
Science Division.
[music playing]
Our ability to make dramatic progress
is often only limited by our ability
to access the right technology
and the right information.
We find ourselves
at a crossroads in science.
On one hand, we operate
in the same paradigm
that has guided the field
over the past decades,
ruled by data, theory, and simulations.
On the other hand, we're beginning
to recognize
that powerful new opportunities
for scientific discovery
are possible through
our increased data volume
and sophisticated methods
to explore these data.
The emergence of the
hyper-connected digital society
and massive quantities of data
that it generates,
has led to new analysis capabilities
that literally scale to the vastness
of our solar system.
NASA's mission is to expand
human knowledge
about the universe, the solar system,
and the planet Earth.
In this presentation,
I want to tell you some of the ways
that we are accelerating
our scientific discovery
by marrying the wealth of data
and modeling techniques
to the cutting edge technology
provided by Amazon Web Services.
But first, I want to tell you
about heliophysics.
Heliophysics is basically
the study of the sun.
And why do we study the sun?
Well, fundamentally,
we are curious people,
from the very first time
that people looked up
at that bright light in the sky
and wondered what it was,
we've studied the sun.
Humankind has literally studied
the sun for millennia.
Ancient Babylonians recorded
eclipses on stone tablets,
Renaissance scientists peered through
their telescopes tracking sunspots.
Eventually, we took to space
and the first satellites captured
solar particles streaming past Earth.
Each generation has always run
against the limits of their tools.
And so what do we do?
We build new ones.
And so today that allows us to have
a bounty of new research questions
that we need to build
yet more tools to answer.
But the Sun is also a star,
but it's a star that we can go up
and visit and study
up close in personal.
So what we learn about our star
is directly transferable
to other stars in other stellar
systems, in other galaxies.
And that will help us as we continue
our search for life elsewhere.
But most importantly to us here
on Earth,
the Sun can send billions
of tons of solar material
hurtling towards our planet
at millions of miles an hour.
We call those events, solar storms,
and they can be very, very destructive
when they get here to Earth.
Fortunately, for us, our Earth
is protected by a magnetic field,
and that blocks all of
the harmful radiation from the sun.
It also protects our atmosphere
from this stream of charged particles
that is released
by the sun's magnetic field.
But the sun actually
does more than that,
more than just impact the Earth.
If I take you over the poles
of the sun now,
so you're looking down, you can see
a sort of spiral pattern of particles
continually emanating from the sun.
We call that stream the solar wind.
And not only does the solar wind
actually impact us here at Earth,
but it in fact carves out
a protective bubble
for the whole solar system
as we orbit through the Milky Way.
So it essentially inflates a cavity
that protects our solar system
from the vagaries of interstellar space.
Now, NASA's Heliophysics Division
studies everything
from the core of the sun
to that tenuous boundary
with interstellar space,
and even actually beyond.
But what I want to talk to you
about today
is what happens when a big
solar event arrives at Earth.
As I already noted,
the planet's magnetic field
is protecting us from
the very worst impacts.
You can see here how the magnetic
field lines
on the Sunwood side of our planet
become interconnected
with those of the solar wind.
And that allows a lot of energy
and plasma
to enter into our magnetosphere
or our magnetic environment.
In turn, our atmosphere can also
get very super heated,
particularly the oxygen ions there.
And that actually causes
the whole atmosphere to expand,
and that can prove
problems for spacecraft
that suddenly find themselves
in a different environment.
Also, energetic electrons flow
down those magnetic field lines
hit the gases in our atmosphere,
causing them to glow and form
the very beautiful aurora.
Now, you know, space weather
is important,
and we talk about it a lot today.
But even though I said that we've
studied the sun for millennia,
we actually didn't realize
this kind of profound link
between the Sun
and the Earth until 1859.
And here you see Richard Carrington,
a very famous astronomer who was
looking through his brass telescopes
and sketching sunspots,
as he did every day.
And he noticed while he was looking
at this particular sunspot group,
that's there was sort of bright
white flashes of light that appeared,
and then they kind of disappeared.
He knew something had happened,
but didn't really know
what it was going to do.
Just hours after he saw this,
compass needles at the earth
began to spin,
the aurora was seen
at much lower latitudes
than they'd ever been recorded before.
And in fact, the whole
telegraph system in the U.S.
was taken down for a number of days,
about four days.
Now, that doesn't sound very important,
particularly if I talk to kids.
And I say the telegraph system
goes down for four days,
and they go, “Oh.”
And then I say, yes.
But imagine if you lost
the internet for four days,
and panic, we'll see you in a classroom.
So if we compare it with today,
so I'm showing you data here from,
this is actually from an event in 1989.
The aurora overhead is like
a current system in the sky.
And that current system
can interfere with our power grids.
This one cause white widespread
power outages
in the northern U.S. and Canada.
This is very important event for us
because it actually demonstrated
how vulnerable our power grid can be.
We know that there are much bigger
events in our historical record
and as we, you know, we sort of
put more and more technology up,
mega constellations now in space,
out huge reliance on power grids.
We really feel that reducing our
vulnerability is a national priority.
Now, these big events, fortunately,
don't happen that often,
but what I'm showing you here is one
that happened in July of 2012.
It was actually the most intense
of solar storm event
in the 150 years
since the Carrington event,
and you're probably wondering
why you didn't hear about it?
Well, the one thing that was lucky
for us
was it went off the side of the sun.
But had this event happened
one week earlier,
it would have been
on a direct collision course
with us here at Earth,
and the estimates
are that it would have caused
about $2 trillion
of damage to the U.S. alone.
So we really want to take
these things seriously,
and we really want to know,
you know, how to study these.
And so starting in the mid-1990s
and onwards,
we've really focused
on taking the data that will allow us
to make these sort of
transformational improvements
in our ability to predict
when one of these events
is going to come.
So we look, we have our spacecraft
there looking at the sun
all the way through
the space environment,
and then into all the different
layers of the Earth's atmosphere.
And we know that
within this huge volume of data,
there has to lie the clues that will
tell us which of these solar storms
will in fact become
one of these giant superstorm,
very destructive events,
but it's very hard for us to look
at all of these different data
and find these sort of often
quite hidden signatures.
And even though there are dozens
of spacecraft collecting data,
the data is still very sparse.
Distances covered by
a solar storm are vast,
I mean, they're 93 million miles
from the Sun to the Earth.
On top of that, disturbances
that leave the sun can change
during their transit to Earth.
So, you know, something that looks
quite benign on the sun
can then be very destructive
when it reaches the Earth.
So just like major storms
that you're used to here on Earth,
a solar storm is an interconnected
set of processes,
both in space and on Earth,
a developing storm can be intensified
by background conditions,
or by merging of disturbances that
don't normally encounter each other.
Currently, NASA Data Archives
have housed
about 12 petabytes
of space weather data.
And this is increasing daily.
And it's comprised of all kinds
of different space
weather observations,
different instrument types,
different things going from the sun,
all the way down
to our upper atmosphere.
And as I said, we know
that captured in this dataset
is the web of processes
that can produce a superstorm.
But how do we detangle the
information to find these signatures?
So we were approached by AWS,
[PH] Hannah Malo and her team,
to partner on an AI/ML project
using our space weather dataset.
And so we thought,
what a great opportunity
to actually work on superstorms.
But there was one huge complication
when we were going to apply traditional
machine learning techniques,
the rarity of superstorms.
So what we did, instead of doing
sort of the supervised learning,
was we focused
on knowledge-aided machine learning,
specifically anomaly detection.
So we're looking for those signatures
that really don't conform
to the normal expected behavior
of the space environment.
Extreme events in space weather
are particularly amenable
to anomaly detection,
because fair to moderate space weather
makes up the vast
majority of our datasets.
And so it kind of provides
a perfect quiet baseline
on which to identify anomalies.
And the other thing is using AI and ML,
we can use every single piece of data
that we have on superstorms.
It kind of relies on classifying
superstorms based on anomalies
instead of the usual magnetic indices
and various other parameters
that scientists use
to categorize their storms.
So the other issue, there are
very few historical examples
that can be used
to train our algorithms.
The vast majority of the observations
actually take place much earlier,
and so what we did is we leveraged
for space science,
the investment
that the industry has made
and developing sophisticated
anomaly detection algorithms,
things that you would use
to identify unusual patterns
maybe associated with bank fraud,
or webserver attacks.
And that's what we use
to investigate superstorms.
It also allowed us to look at
all of the data
kind of simultaneously
and pick up trends,
either happening at the same time
or even time lagged,
as we looked at the different areas.
So one thing that was a problem
for us is finding something
that we absolutely knew was
a common signature of a superstorm.
And that was one of the most
spectacular phenomena
that I've ever seen occurring in nature,
and that is the great red aurora.
These things can extend to
exceptionally low magnetic latitudes
and cover up to 95% of the sky.
During that Carrington event in 1859,
blood red skies could be seen as far
equatorward, as Cuba and Jamaica.
People could literally read
by the light of the aurora,
and fire trucks
were actually sent out to find
and fight what were thought
to be forest fires.
So now we needed to identify datasets
which existed at the time
of reported great red auroras.
Unfortunately, not many were available,
as most of the storms
associated with these
predated our very sophisticated
data system that we have now.
So we had to turn
to operational datasets
that were not originally
intended for science.
And here we partnered
with Colorado University in Boulder
to work on sort of
older operational datasets
and kind of clean them up,
remove the nuisance signals,
remove the detector anomalies
and get a scientific product
that we could actually use.
So one thing that we found was,
we didn't know that in order
to produce a large, great red aurora,
we need to have an awful lot
of very low energy electrons.
And I mean a lot of them.
And this is sort of counterintuitive,
because this is a really big event
and we're looking for
low energy signatures.
If you had higher energy electrons,
it would actually produce
the more common green aurora,
but we don't see green aurora,
we just see red.
So you know, again, counterintuitive,
the whole energy during a
big superstorm actually drops.
So it was hard to actually explain that.
But you know, but we found that,
you know,
they does recover after the storm,
and that maybe was
a signature of a superstorm.
So after working with literally years
worth of data, we found another clue.
And that is that the precipitation
associated with energetic ions
or positively charged particles
was actually bifurcating,
and it was coming much lower
in latitude than the electrons.
Normally, you'd see them
at the same latitude.
We realized that these sort
of soft ions were actually kicking
what would normally be a sub-visual
signature of the aurora.
It was actually giving it the energy
and kicking it into the visible range,
hence spreading the aurora and making
it cover that much bigger latitude.
So now we have a couple of signatures
that are in common.
We have the dropping in energy,
we have the large population
of low energy electrons,
and we have this bifurcation
of the two populations.
Now we have signatures
that are peculiar to superstorms.
So this led us to the production
of a storm scoreboard,
where the extreme scores
indicate superstorms,
you can see the more usual one there,
which is with a sort
of a more typical storm,
and then those beautiful red aurora
with the Superstorm.
So the next steps for us include
adding sophisticated datasets
from our Heliophysics System Observatory
that aggregate the observational data
from more than 50 spacecraft
containing now images, time series
and telemetry data
spanning from the solar corona,
all the way through the heliosphere
to Earth's upper atmosphere.
And with the aid of our sophisticated
machine learning technology,
we will sort of continually preprocess
and combine and develop visualizations
that will drive future
heliophysics research and innovation.
So using this very simple signature
that literally was previously hidden,
we can now study what other
signatures may exist
that will allow us
to identify, understand
and eventually predict
these most devastating space
weather events.
And it's important that we do it now,
we are in a new solar cycle.
The sun has an 11-year solar cycle,
you're looking at data there
from the last solar cycle,
you can see the contrast
of solar minimum
to solar maximum with the sun.
December 2019 marked
the beginning of solar cycle
25, our current solar cycle,
and the sun's activity
is already ramping up,
and we'll continue to do so
until we get to solar maximum
that is predicted for 2025.
Now in this new solar cycle,
we are already seeing much
higher levels of activity
than were predicted
based on the last solar cycle.
It will of course impact our lives
and our technology here on Earth
as well as our astronauts in space.
But more importantly, this is
the first solar cycle
that many new commercial
and government stakeholders
will actually have to navigate in orbit.
We've already seen a pretty big event
that happened earlier this year.
It was actually a fairly small storm,
a fairly mediocre storm.
It arrived.
It mixed with background conditions
and different processes.
And it actually had a dramatic
heating of the whole atmosphere.
It lifted up our atmosphere
and meant that the newly launched
Starlink spacecraft,
were now in an area
of very intense orbital drag,
and they could not reach
their operational altitude
and they reentered.
And so it is very important
that we work on these events,
we understand what is causing them,
and then we can actually predict
and mitigate the biggest events.
And so I'll finish.
I'll wrap up by saying, you know,
the more we understand
what causes such space weather,
and the more we can improve
our ability to forecast
and mitigate the effects,
the power and speed of AWS
with the analysis to predict
our superstorms
can be carried out
by sifting through as many
as over 1000 datasets at a single time.
The knowledge process is guided
by our accumulated knowledge
about superstorm features
and by simulations to understand
those interconnections.
And since our anomaly detection
algorithms
improve automatically, with experience,
we know that there's so much
more potential
for discovery of new anomalous features
that have significance to superstorms
and even to everyday space weather.
So with this unsupervised learning,
we are likely to discover things
that we just literally did
not know before.
And with this partnership
between NASA and AWS,
we are discovering the questions
that we need to ask today
to drive science
and innovation of tomorrow.
Thank you very much.
[music playing]
Please welcome the Vice President
of Embedded and Edge Computing
in Nvidia, Deepu Talla.
[music playing]
Good morning.
Impact of computer vision
is increasing tremendously
across all industries.
Billions of cameras are either deployed
or are being deployed
in all sorts of spaces.
You take a look at some of
these industries,
manufacturing, for optical inspection,
defect inspection, worker safety,
optimizing the flow of goods.
Autonomous vehicles, so we have
approximately 100 million cars
and trucks come in every year.
Almost 1.5 billion of these vehicles
are on the road today,
and most of them will have
increased autonomy in the future,
so computer vision is going
to play a pivotal part.
Medical imaging for MRI,
CT scans and cancer detection.
Retail stores for self-checkout,
for inventory management, and heat maps.
Public spaces like stadiums
for people safety and access control.
Smart buildings,
employee access control,
parking management.
Robots for navigation,
robots for pick and place,
and robots for
human machine collaboration.
And then you have traffic
transportation hubs and warehouses.
All of these are increasingly
using computer vision.
The last decade has seen
a great progress
in computer vision
with the advent of deep learning.
In 2012, for the first time,
we saw AlexNet was using deep learning
and beat all conventional
computer vision techniques,
which were handcrafted features.
And then in 2015, for the first time,
ResNet achieved superhuman level
accuracy for image classification.
And in the last five years,
we've seen more
and more neural networks,
amazing ones coming out
for both object detection
and classification things like SSD,
YOLO, and RCNNs.
And recently, transformers are
increasingly being used
in computer vision,
originally designed for speech.
In 2018, they released StyleGAN
for image modeling in open source
and that continues to be used
quite a bit as well.
And then one of the hottest topics
in computer vision recently
has been NeRF, neural radiance field.
Incidentally, computer vision
pattern recognition conference
is happening right now.
And I believe there are 50
research papers on NeRF.
So NeRF essentially
takes a few 2D photographs
from various camera angles and poses,
and delivers a 3D scene representation.
And we at Nvidia,
we've been working on this.
We've released what's
called Instant NeRF,
which actually speeds this up
by a factor of 1000 times,
effectively creating a 3D scene
from 2D data in tens of milliseconds.
So these are some of the great
research works
that in the last 10 years
we've seen in computer vision.
Even commercially, you've seen
some amazing progress.
Amazon Go, the amount of technology
that has gone into it
with the number of camera sensors
in the stores,
if any of you actually had
the experience to try that out,
a lot of image processing
computer vision needs to be done.
Nvidia Drive, we've been working
on autonomous vehicles
for several years now.
And level two plus functionality
is going to come in Mercedes Benz
and Jaguar Land Rover vehicles
with our software in 2024 and 2025.
John Deere has been using
computer vision
for automating the tractors
fuel for both,
you know, without human operation,
but also reducing
the amount of herbicide
that you spray for weeds
by an order for magnitude.
And Siemens healthcare has been
you using
computer vision for oncology radiation.
However, we still have a problem.
I think we're still within
the first inning
or the first 10% of the journey.
Computer vision deployments with AI
is still really in research
and big corporations.
We still haven't figured out a way
to make this democratized
where every farm,
every manufacturing plant,
small, big doesn't matter
can use computer vision.
And so there's a reason
why we are not yet seeing
the broad deployment of computer vision.
And that's for the lack
of certain tools.
So we've been working
on these tools at Nvidia,
and also working with Amazon on this.
And there’s four main things
that I'd like to touch upon.
First is how do we create
AI models efficiently?
Second is simulation and digital twins.
Third is building efficient
runtime applications.
And fourth is deploying
these applications
and managing these applications
for the whole product life.
Now, let's double click into
each of these blocks briefly.
So AI model training.
As you all know, AI deep learning
requires data, lots of data.
And in fact, human level data.
Human level data and real-world data
is necessary
but still not sufficient.
Recently, there's been a lot of progress
made in the area
of synthetic data generation.
The advantages of synthetic
data generation are obvious.
First of all, the data comes auto-label.
You can actually randomize
the domains, for example,
changing the background or the lighting,
and it's super fast to do.
So at Nvidia, we've been building
this platform called Omniverse.
And one of the features of Omniverse
is something called
Omnivore's replicator,
that allows you to generate
synthetic data very efficiently.
So using a combination of real-world
data and synthetic data,
we can really speed up
the data generation for AI models.
Second, the AI model
that you come up with,
is really going to depend on
what your starting point is.
I'll give you an example.
Imagine you're in NBA basketball team.
And you're trying to win
the championship this year,
your one player shot,
who are you going to select,
you're going to probably select somebody
who's 5, 10-year player
to add versus adding a rookie.
But unless, of course, you’re Magic
Johnson that will be okay.
But otherwise, the starting
point really matters, right.
So what we're doing it
is building high quality
pre-trained models for various
applications in computer vision
that you can use as a starting point,
instead of starting from zero.
So start from something like 70, or 80.
And then the last one
is taking that model
and adapting it to your environment.
It is very hard to take any
computer vision model
and expect it to work
in all deployment scenarios.
It's actually pretty much
an impossibility.
So which means you need to have
some level of customization.
So we've created a toolkit called TAO,
stands for train, adapt and optimize
that, you know, you can use
to fine tune your model.
So the combination of these
three things,
we have several hundreds
of thousands of customers
that are using these technologies
to really speed up
AI model creation by up
to an order for magnitude.
Second, let's talk a little bit
about simulation and digital twins.
The case for simulation
is very elementary.
Well, it's simple.
I mean, it's...
cheap, fast and safe
to do things in simulation.
When I was just walking, you know,
walking to come to the keynote stage
I saw a huge Fanuc robot arm,
as probably 500 pounds
or 1,000 pounds with the TV stuck to it.
Now in the future you want this,
you want to have humans and robots
collaborate with each other.
Now imagine this, that's a 500
or 1,000 pound of metal,
would you as a human be comfortable
experimenting, operating beside it?
I certainly would not be that one.
So if we can move this to simulation,
you can simulate it
a million or a billion times.
And then once you're sure, you can
then bring it into the real world.
Now the challenge traditionally
for simulation
has been the three things
that you need to do
to simulate something very well.
Number one, the accurate
physical world modeling,
so you need high performance
computing and physics processing.
So we're able to do that on our GPUs.
Second, visual fidelity
of the simulation
matters in many cases, not all cases,
so retracing graphics.
And third, artificial intelligence
running both as hardware in the loop
and software in the loop.
So we've created
Nvidia Omniverse essentially
is authoring platform
for creating digital twins.
Number three, now you
created the model, you simulated it.
Now you need to run it
on your target device,
whether it's at the edge
or in the cloud, doesn't matter.
Well, it turns out, you know,
there's a lot of complexity
in AI inference
given the number of models
and the number of modalities
that you need to support.
So at Nvidia, we’ve been
working on this for,
well, over five years,
creating libraries and tools
for accelerating runtime inference.
We have tools such as TensorRT
and Triton,
and over 2 million developers
from over 20,000 organizations
have been using our AI
runtime acceleration tools.
Now, that's one part of the equation
when we're talking
about computer vision.
A lot of people actually
fall into this pitfall
that running a computer vision
is just about AI inference.
No, it's not.
You have to, of course,
run AI inference first,
but then there's a lot of preprocessing
and post-processing that you need to do.
So you need efficient
runtime frameworks,
especially when you look
at modern architectures,
computing architectures
that are CPUs, and GPUs,
and accelerators, and video encoders,
and image signal processors.
And the number of things
that you need to process
simultaneously is mind blowing.
So we've created a tool
for that called DeepStream
for optimizing runtime,
computer vision applications,
and over 300,000 downloads
that we've seen just
in the last two years.
So combining all of our
AI acceleration tools
with runtime acceleration
for the complete end-to-end
CV application
with DeepStream,
we are making it easy for developers
to deploy computer vision applications.
And then number four,
deployment and management.
If you think about deploying
computer vision
into a manufacturing facility,
or inside a robot,
well, that robot is going to highly…
it's highly likely to be operating
for the next decade,
if not more than that.
You definitely want to make it
smarter over time,
so it needs to be updated
constantly, right?
And it needs to be done
in a secure manner.
So you need all these software tools
that are able to deploy
and manage all these individual devices.
And that takes a lot of frameworks,
several frameworks
companies are working on.
At Nvidia, we have a tool
called Fleet Command.
AWS is working on similar tools.
There’s a lot of tools
that are necessary for us
to deploy computer vision broadly.
So those are the four
fundamental pillars
that are necessary in order
to democratize computer vision.
Let me touch upon, you know,
how we are partnering with AWS
and Nvidia in this space.
We've partnered with AWS
for several years now,
started with providing our GPUs
into high performance
computing and training.
We also have our latest GPUs
in the AWS Cloud
for both inference
and graphics application.
We've been working with AWS in the area
of accelerating all the libraries
for machine learning.
In fact, SageMaker uses
some of our TensorRT,
and training tools as well.
A couple of years ago,
we partnered with AWS
in this area of edge AI
and computer vision,
and a product called AWS Panorama.
So I want to share with you
some of the success stories
as to how AWS and we, collectively,
by solving those four things
that I mentioned earlier,
have been able to take
computer vision from big deployments
to taking it into smaller deployments.
So four examples here.
One is Phillips 66,
it's basically a gas station.
The problem is actually pretty simple.
The problem that they're trying
to solve is,
there are, you know, attendance,
but there are multiple counters.
And depending on the heat map,
depending on the flow of traffic
into the gas station,
you know, there's a line,
the queue length is significant,
they can ask monitoring to come in.
Or the other one is with
summer travel coming up,
you know, the restroom cleaning
is actually, you know, defeats.
It's a simple use case.
But, you know, sometimes you do it
based on every hour,
but sometimes depending
on the foot traffic,
you might have to do it
in every 15 minutes.
So providing simple insights
with no data scientists
required in any of that
you just plug in a device,
and it does all the management.
Number two, Tyson, chicken.
They had a simple problem to solve.
The problem was that, I believe each…
I'm a vegetarian,
so I don't know exactly
but I think there's
eight pieces of chicken
typically in that most people
that buy in those things.
And they were trying to pack
those efficiently,
accurately into big pallets.
Would you want a human to do that,
sit down and watch the number
of things that are going in?
That’s such a boring job.
So using AWS Panorama, and using
all the accelerated frameworks,
the application was able
to accurately count
and basically make sure
that every pallet
was exactly the same number
as it was intended.
Cincinnati Airport,
solving against simple problems.
Before you go into the airport,
one of the biggest problems
we have is this,
I have, I'm driving up
trying to pick up somebody,
few people are sitting there
for like one or two minutes, right.
And they argue with the security
personnel there,
how long they have been in there.
Now with computer vision,
you can accurately decide,
figured out exactly,
you know, what it is,
and you know,
there's no confusion there.
And then even after security,
all the airport operations
can be automated,
have been automated
using computer vision.
And last, Port of Vancouver, massive,
massive containers are shipped
from the Port of Vancouver.
Well, you can do computer vision,
of course, in the port,
but they've actually employed
computer vision
all the way when the container
is on the truck way,
way behind somewhere.
And they're going to track it
all the way as it enters the port.
So these are just some of the examples.
You can imagine that none of these
will have their own data scientists
to solve these problems.
These are, you know,
truly getting into the, you know,
into the common areas.
And basically, that's the future
of computer vision.
So I think in the next 10 years,
we still in the first inning of this,
we have a 10x growth more to go.
And it's only going to happen
if we make this simpler and simpler
with all these different tools.
With that, I'd like to thank you
for your time
and hope you all enjoy Vegas.
[music playing]
Please welcome the Founder
and Chief Test Pilot
at Gravity Industries, Richard Browning.
[music playing]
Flight, it's a subject
that I think has fascinated me
since I was a child.
I remember many happy an hour,
many hours actually,
as a kid with my late father
trying to fly these very simple
little model gliders,
they're so basic compared to
what you get with [PH] joins nowadays,
but I remember lots of fun times
trying to fly those
and also rebuild them when having
crashed them loads of times.
Back in 2016, I had this
seemingly ludicrous idea
of could you have a run out
the whole challenge of flight,
a particularly human flight?
Could you reimagine it
in a way from a sort of
ground-up point of view
where rather than sitting
inside a flight vehicle
or being strapped to a flight vehicle,
and we've done that very well,
obviously in the last 100 years,
otherwise, I wouldn't be standing here.
What if you actually tried
to employ the human mind and body
in a much more kind of intimate way
and kind of trying to,
as far as possible
standing here now, actually fly.
It still sounds weird, I can't find
a better way of describing it.
And part of the inspiration was the time
I spent with the British Royal Marines,
I guess, learning myself
to be able to push my body
and my mind further, the military
is very good at doing that.
And I used to get into things
like this calisthenics,
where you can learn to support
your body weight
in weird positions, highly impractical.
That's not how we usually
get on the London Underground.
And it was an idea
that I think evolved in my mind.
And I got to the point
where I realized the missing link,
very simply, it was horsepower,
I'm not quite mad enough to think
you can kind of flap
your way to this outcome.
So 2016, around March of that year,
in a little country lane in the UK.
And this is all alongside a day job.
I was an oil trader for 16 years
in the City of London,
I didn't tell them what I was doing
at the weekends.
This was ground zero, this is getting
a little micro gas turbine,
baby version of what's on a jet fighter
with a few design changes,
only weighs about four or five pounds
and puts out about 40 pounds of thrust.
And actually, the big learning
from that moment
was that that feeling was
just this spongy force.
It's almost just like leaning
on that table.
So kind of emboldened by that,
there was an obvious step to take,
which is now you have two
and now not a mop bucket
with the fuel tank inside, now mobile.
Now if you watch this little clip
here where I'm pushing,
you can see now that's a lot of force.
I mean, in kilos,
it's 40 odd kilos of force,
so you're getting close
to 100 pounds of force.
And it's really interesting to know,
when you lean on it, that's easy.
When you put it sideways, it starts
to really kind of beat you around.
Now, having had two work quite well,
you can see the sort of natural
progression of this journey.
This was a great stage, because
actually, your arms are really quite,
you know,
you're in control of your arms.
And that wasn't enough power
to actually take off.
So it was a nice stage to get to.
This trial and error with
the emphasis on the error process
took place over the summer,
we tried to work out
where else you could attach thrusts
to the human body
that would allow it to become
as intuitive as a bicycle.
It's amazing how you just add
a bicycle to a human body
and use the mind for balance
and how intuitive
and what a brilliant partnership
that is.
That was not such a good example.
And the tether system
really didn't work very well.
However, the model that we started
to kind of gravitate towards
was one of two engines on each arm
and the net thrust feeling like it
was going up your arm
and then an engine
on the back of each leg.
But you see the problem in that clip,
it's weird as your leg
starts to get lifted,
your foot can't feel the ground anymore,
and your brain starts to make
your kind of legs pedal.
It's a very weird thing.
Anyway, I had to not only try
and work out how to
turn that off in my head,
but also stop damaging the engines
every time I fell over from
a couple of feet on that farmyard.
But by November that year,
the, I suppose at the time
seemingly impossible actually happened,
which was this very lame fuzzy little
flight, still fighting one leg.
And that was it, that was actually
turning mad idea
into something that actually flew
for six seconds across that farmyard.
And it's quite fun to reflect
at this moment,
over the intervening five years
since then, just how far we've come.
And this is a sort of no attention span,
60 seconds run through this
already now out of date
by quite some way of the 200 events
in 36 countries that we've flown.
And they're either commercial events
or otherwise military or search
and rescue medical ones
that you'll see soon.
So it's quite nice just to see how
far you can take a ludicrous idea.
And I'll touch on this at the end.
I'm a passionate believer
in the process of innovation,
it's one of take a seemingly crazy idea
and then as safely and survivable
as possible,
fail your way to what you never
imagined was possible, as long as,
and the rule we follow
is if failure occurs,
and it occurs all the time
or used to in the early days,
I'm pleased to say
it doesn't too much now.
Can you get back up again,
from a safety, reputation,
and financial perspective?
That's my-- you know, in the short time
I've got with you today,
that's my offer as a takeaway.
And that's what I used to use
actually running a trading book as well.
It's all about being able
to get back up from your failures.
And it allows you to do
crazy things like that.
The fire has nothing to do with me,
by the way,
that is a bad timed slide
when I'm talking about failure.
Something I'm also very proud of
with this ludicrous journey
is that we haven't
raised money since 2017
because we've actually sort of
generated our own income from this,
in doing things like training over
500 people to learn to fly these.
So there's a great location in the UK
about an hour outside of Heathrow
at the Goodwood estate
if people know that's where
the Festival of Speed
in the UK takes place.
It's all spitfires and racing cars,
and we fit in great.
And there are now louder things
there than us.
We're actually setting up
a training center
in just outside of San Francisco
in Napa, hopefully this summer.
And it's the most strange process.
You're never gonna believe me, but
you get tethered up to this thing.
You stand there, you feel the force
on your arms,
you realize you can lean on it,
just like leaning on a table,
it literally feels like that,
the rear engine lifts
you slightly forward.
And then that magic moment
that you see with a kid
when you take your hand
off the saddle, and they just get it
and they never think about it again,
the same thing happens here.
And the probably the best example is
one of these clients here you'll see
is actually one of my two boys
doing a great job in a second of
applying
the mindlessness you need to apply
when you're learning
to fly one of these.
There he is, not got a clue
what's going on.
He's just smiling and enjoying the view.
And if you think about it,
that's what happens
with riding a bike
or surfing or skiing or snowboarding.
This taps into your natural
human ability
to balance in a way that's frankly more,
I suppose intuitive than walking
or running in some ways.
Now, in the same vein,
in terms of how, you know,
fast, we're progressing,
a massive catalyst of that,
that I really feel
I was trying to shout out
is additive manufacture, 3D printing.
We were able to have an idea,
get it carried up,
and then get that printed.
Nowadays, in polypropylene
of all things,
we've moved away from aluminum
and titanium and steel,
just because it's, frankly,
more appropriate, bizarrely enough.
That can change in design,
within a sort of turnaround time
of about two weeks,
you know, a couple of days
do the CAD work,
a week or so to get the print back,
and then we'll put it together
go from test-flight.
So the ability for us to just
iteratively progress
is hugely enhanced
by this wonderful technology
of additive or 3D printing.
Dancing around here.
Another example of why we're never done
with this whole journey as well is,
and you could do a whole talk
on this as well,
this is the journey of Vitol transition.
So if you imagine a Harrier aircraft
or you imagine the F-35,
they blow air downwards, you rise up,
and then you start to transition
into becoming an aircraft.
We thought, "Well, we can--
we do the first bit,
I don't know how close we can get
to doing the second bit."
Turns out, we could do
the second bit pretty well.
Now we kept a bit of a lid
on just how fast you go,
because you've got a round with the
latest suit around 1500 horsepower.
That's about 350 pounds
of thrust in that.
So given the frontal layer of a human,
you're going to get really fast.
So this little film here,
which I don't like as much
as the previous one,
the last one was an amazing flight
to actually experience.
This is where we set the current
Guinness World Record
for how fast you've actually flown
when we've flown in one of these.
I'll save the surprise when you see
the dial coming up in a minute.
You get a nice example of how you
gradually transition
into flatter and flatter
and you have to use your body
to actually maneuver
rather than thrust vectoring.
That is kilometers an hour,
before you think it's miles an hour.
I'm not quite that mad.
And this is a good example as well of
how you transition back out.
That's something that the Harrier and
the F-35 people had to learn as well.
We had to learn it in real-time,
which is quite exciting.
You actually have to turn the power-up,
because you've lost
the aerodynamic lift.
So another example of something an
amazing sort of foray we went down.
However, in the limited time
I have with you,
I want to just touch on two areas,
which I'll let play while I'm talking.
And you might have possibly seen
some of this online, it tends to--
it's gone around pretty far and wide.
I was with the Royal Marines in the UK
for about six years
alongside my day job.
And they put on this exercise.
It was probably the, I don't know,
combination of six or seven exercises.
And the idea was, could you use
the equipment for maritime boarding?
So you've got bad guys on the ship?
I know it's a Royal Navy ship,
but run with me.
How quickly could you get onto
the bridge deck of the ship?
And if you think about the alternatives,
it's a pole with a caving ladder
and you have to try and snag
the railings of the ship.
It's a horrendous process.
I'll let it play out at the end
and you'll see what they do now.
Or you're a hovering helicopter
with a fast rope and the rest of it,
neither of which you can use your hands,
for people who think
you can use your hands.
We have a helmet-steered
suppressive weapon capability
for those who are interested in that.
This was 16 seconds to get to that
bridge deck
and I could have just provided cover,
but instead of that
I'm using the old technology
and providing the ladder for those
who don't have a jet suit,
strange amalgamation
of the new and the old.
But this has become a huge area for us.
We've worked-- we are working now
with about seven different
special forces
around the world on the allied front.
And the more theatres and environments
we end up getting to play in,
the more we keep changing the rules
because you really can just move
human resources up
and move in a couple of miles
within seconds
and do a task and come back again.
So there you go.
You can see that process.
I mean, personally, flying is a hell
of a lot easier than doing that,
especially at night in heavy sea.
There is the demonstration
of exfilling back out
exfiltrating back out
although I'm weirdly landing
on the same ship
I've just attacked but anyway,
ignore that detail.
Now the other big area that I'm
really proud of just bearing in mind
where this started from
in a farm yard alongside the day job
is the whole search and rescue,
or let's say medical response role.
If you imagine a paramedic
on a motorbike,
they cut through the traffic
and they get to the casualty
and they sought out breathing
or blood flow
or something else critical that's
potentially going to end your life
within the first, you know,
critical 40 minutes, 60 minutes.
This is a mountain in the UK
in the Lake District.
It takes about an hour and 20 minutes
to walk up at pace.
The weather you'll see in a minute
was really bad
and it tends to be really bad
in this place.
So you can't often get helicopters
on to the top,
so the only option is that walk,
and if you've got a cardiac patient,
they tend to frankly not make it.
So we parked up-- this is obviously
a planned exercise,
it didn't just turn up.
We parked up and left the vehicle
roughly where the search
and rescue guys,
well, the mountain rescue guys
would usually depart from,
and managed to get to the summit in
I think it's three and a half minutes.
That was 1.2 miles that way
and 2000 feet in vertical ascent.
It was quite a demonstration because--
and I'll let it just play
into the foggy part.
It was quite exciting
to actually be flying along
with probably visibility down
to barely from me to that screen.
And I ended up just flying
looking at, frankly,
it was an iPhone app map system
where I could actually
just follow waypoints.
It got so bad that the two
drone systems dropped out,
I'm going to-- I should be able to--
It's a shame,
I can't fast forward this one,
but there you are,
you get an idea of the fog.
And we got to the top, as I said,
in about three and a half minutes.
That is something we're trying
to roll out.
We've got a paramedic
already trained in the UK,
and they should be operational
by later this year, that's our hope.
And it's quite nice because
it's quite closely allied
to the military role.
But it just tends to be
when you get there,
you're being more friendly
than the military folks.
But in terms of working in bad weather
and in difficult conditions,
it's a great analogy.
There you go. You can see
an example of the weather.
Now I'm going to shuffle that on.
I want to just take the opportunity
to reflect on what I said
right at the beginning,
which is this subject
I'm very passionate about,
about innovation,
because a lot of words
are used around innovation.
And I'm not always sure
they're from people
who actually are walking that walk.
I'm passionate that you have an idea,
even if it's a crazy one,
even a childlike thought
that seems impossible
there is a process you can go in
where you work out
as quickly as possible,
you can go and test that idea.
This is what we do all the time,
and go and take the risk of it failing.
The critical thing is and I used to
do this running a trading book,
and now I ever do it running gravity
is if failure occurs,
because it'll happen all the time
if it's a meaningful test,
can you get back up again, obviously,
from a safety point of view,
but also reputationally and financially?
And that is our magic sauce that we use
to keep pushing this whole
crazy journey ever forward.
And somebody who knows
a lot about that, and is,
I suppose a gleeful embracer
of things going wrong is your host,
my good friend, Adam Savage,
who I think is going to appear,
hopefully,
if we've coordinated this well.
There he is.
Well, thank you.
[applause]
Can I just start by telling the story
of how we met?
Go for it.
So I'm at the TED conference,
I'm wandering around in the haze
that you wander around
in the TED conference.
And this Brit comes over and says,
"Mate, you're a legend.
I love MythBusters, and I've just
gotten here with my rocket pack."
And I'm like, "Wait, the rocket pack
I saw on YouTube last week?"
This was the first,
the very first flight
other than the accidental raise
in a parking lot in San Francisco,
let's ignore that one,
the very first public flight.
So it was pretty scary in 2017.
And I had seen the video
like a week before
so I knew it was all really fresh.
And you said, "Well, I haven't
brought anyone with me?
Would you be my ground crew?"
And I was like, "I've waited my
whole life to be asked this question.
Of course, I'll be your ground crew."
It was brilliant. We'd never met before.
And he literally dropped
everything he was doing at that point
and then just followed me
to the behind-the-scenes room
and helped greatly with the huge stress.
I mean, it's bad enough doing
a TED talk for the first time anyway.
Lo, and 10 minutes later to go fall over
in front of the world's media,
potentially with this creation.
- So you were a massive help.
- I was wearing this exact jacket.
Yeah, you feature in the film.
[PH] It's worth mention.
You're just standing in front of me
helping it out.
I love, love, love that you talk so much
about failing your way to success.
And you know, what we mean
by failure is iteration.
It is all just iterative,
you know, burning through options.
I'm curious if you have more
of the failures on tape
because those are my favorites.
So just so happens, yes,
for special audiences.
And I'd be a fraud if we hid these.
We can roll a few of those
learning opportunities.
Let's call them learning opportunities.
Right now. So here we go.
So many of them are very old,
pleased to say.
That was running out of fuel.
That one was actually
the first Guinness World Record
attempt we tried.
This is fun.
I just ended up trying to land
whilst forgetting
I had about 20 mile
an hour forward speed
and I can't land at 20 miles an hour.
There's nothing wrong
with the gear here.
He just let go of the throttle trigger.
That was a compressor stall
which is an unusual one.
I'm hovering away in the background.
This is where we really rinsed
the engines way harder
and at sea level where
the air density is greater.
We rinsed the engines way
harder than we should.
And we dropped two suits
in the water at that airshow,
but you can see a good example
of how we do float.
A lot of people online say
"Well, you're gonna die,
you're gonna die
if you get in the water."
It's like if we hadn't thought
of that, we shouldn't be doing this.
I love how much this feels like
that whole sequence
in the original Iron Man film where...
Where the garage stage.
- Yeah.
- Yes, exactly.
Yes, I mean, yeah.
We didn't-- I don't have a lineup
of quite those cars
to smash into, but yes.
This is my main
aeronautical engineer, Alex.
You've met Alex.
He piled into the woods.
And you've definitely met Sam,
my car designer
who goes in headfirst here.
That was actually the same flight.
And this is one of my stuntmen team
who didn't quite jump late enough,
by the time he got the power in.
If you're gonna do a superhero launch,
you've got to dial the power in first.
But anyway, that was
a learning opportunity.
So yeah, there you go.
There is some of our, you know,
all very low,
all very sustainable
if you like, you know,
you just really graze
your knee at worst.
The thing that I love
about showing the failures,
and being really honest about them
is something we learned
on MythBusters from the fans
is that it feels like a hug.
- Yeah.
- Right?
It's easy to be awed
by something like this and think,
"That is impossible. How did someone
brilliantly come up with that?"
And they didn't, you didn't,
you have slowly made it better
and better and better over time.
Just showing that as the end result
is 5% of the journey.
And I often get fed up with this
when you read like business books about,
just give everything,
never give up, never say die.
No, that's rubbish.
Constantly learn and pivot and feedback.
If you've got somebody lucky enough,
who never made any mistakes
and got to become some billionaire,
well, I'd like to meet them.
And if you just read a book that says
that's how they did it,
you really need to read the book
about all the things that didn't work,
because that's the process that will
screw you over if you get it wrong.
You've got to keep recovering
from endless failures.
The last thing I want to say is,
when you finally got me to fly,
you finally taught me how to fly
and I brought my arms in
and it's not about strength,
it's about feeling it in your body.
And I rose off the ground,
it really did feel like every dream
I have ever had about flying.
It is-- I mean, you know,
I'm biased, obviously.
But everybody that gets in this suit,
there is a strange process
where you do feel like that dream
we have,
sometimes have walking, running
and you step
and you just go wherever you want to,
it is kinda like that.
It's a lot louder.
But it is like that, yeah.
Another big round of applause
for Richard Browning.
Richard, thank you so much, man.
- Yeah.
- A pleasure.
Thank you.
[applause]
I hope you enjoyed the final keynote
here at Amazon re:MARS
and there is still more to see.
We've got more sessions
throughout the day
and you are not going to want
to miss the party tonight.
Shuttles start at 7:15 pm.
It'll have live music entertainment,
and my personal favorite,
a BattleBots challenge.
Yeah, you're gonna get
to see some burning,
burning twisting medal
before the end of the day.
I hope to see you guys there.
Thank you guys so much.
It has been my honor
to host this with you guys.
Thanks for letting me
wear my beautiful costumes.
And thanks to all
the incredible speakers.
You guys have a wonderful day.