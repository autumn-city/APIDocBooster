- Are you enjoying your
AWS conference so far?
Go re:MARS, whoo-hoo! All right.
Also happy Pride Month to people
who are celebrating Pride.
So my name is Trish Lugtu.
I am the principal data scientist
for healthcare at Teradata.
We'll be talking also with Kris Skrinak,
I practiced that, okay,
from Amazon Web Services,
and also Scott DuPertuis from Teradata,
who is our practice
lead for manufacturing.
So we're gonna be talking a little bit
about challenges of analytics
and how we work together
to solve some of these problems.
We know from this survey by Gartner
where there were like 2,300
CIOs or technology execs
who said data and analytics is one
of the top three priorities that we have,
behind security and neck
and neck with cloud.
We also know, as we should,
if we are doing this in practice,
we know that we can find
great value from analytics.
9.5 trillion is the lower end
of that estimate, by the way.
It goes up to 15.4 trillion.
These are big numbers.
We also know that the breakaway companies,
the innovative companies,
are spending 25% of their IT spend,
of their IT spend on analytics by itself.
So they're spending all of this money,
and yet, we're still finding challenges
in implementing analytics at scale.
74% of organizations have not moved
their big data and the AI initiatives
into widespread production.
And then we have like 80% failure rates
of getting AI to production as well.
Some of those common
reasons they talk about
are inefficient data preparation,
lack of flexibility to
use the preferred tools.
"Don't take away my tools. I love them.
I already know how to use them."
So we also have difficult,
it's difficult to score
data live at scale.
These are the challenges
that we're facing out there.
And one more thing I'll talk about,
if you follow the works of Andrew Ng,
he talks a lot about,
you know, we have a lot of data,
but it's not the best quality.
We're really gonna have a lot of success
if we just pay attention
to the data we have
and build the quality, right?
So he's talking about
building data-centric AI.
And, at this time, I think
I'm gonna invite Kris up here
to talk about some of
the scaling challenges
you're seeing at AWS.
- Thank you, so in my role at AWS,
I get to see a lot.
I'm really on sort of the front car
of the roller coaster of machine learning.
But there's a few principles
that I think help guide
everyone onto this track
without so many ups and downs.
First is that, with machine learning,
you probably already know
more than you think you do
about getting it into production.
Because the basic story here is,
with machine learning,
data writes the code.
You use the same agile processes,
you use the same DevOps,
but here's the key: data writes the code.
If you don't have data,
you don't have code.
And here's where the
barriers really come in,
first of all, there's a challenge in that
we don't have enough folks
that maybe just heard what you just heard,
that maybe have sort of a little bit
of a fear factor to get into this.
You know, it's huge.
There just aren't enough
builders out there
that really understand deep learning,
PyTorch, all of the tools that, say,
surround the NVIDIA ecosystem
to even experiment on your own laptop.
So this is a problem.
And, especially, there's
a number of personas
that need to get to this data,
everything from business analysts
to decision makers,
to developers who maybe
never even took a course
in data science.
When I say that data writes the code,
the principle way to begin
to create predictive models
is with data that's already been labeled.
Now, sometimes data is labeled for you.
For example, in a transaction log,
you know a customer has
purchased something.
Well, that's helpful,
but in many instances you
don't have that label.
You need manual labeling.
You need assisted labeling to be able
to create those predictions.
Next, there's a lot of tools out there.
This is an emerging field.
It's not quite as bleeding edge as, say,
it was four or five years ago,
but there's a lot of tools out there
and it can be confusing.
Version compatibility alone
once you settle on your tool set
can really be a problem.
And, certainly, we write software
because it creates value
for our enterprise.
You wouldn't be doing
this in the first place
unless it brought more
customers, reduced churn,
eliminated supply chain outages,
things like this, right?
So you have to put it into production.
It's really all about deployment.
Now, with machine learning,
we call that deployment MLOps.
And as I said in the very beginning,
you don't have code unless you have data.
So what's the difference
between MLOps and DevOps?
It's the data.
Data has to be versioned,
it has to be tracked, auditable.
You may need to keep things
like personal information
out of the training process.
So it becomes a really cumbersome process.
Now, that said, we are here together today
because, with Teradata and Amazon,
we've overcome quite a
few of these barriers.
So, Trish, I'll hand it right back to you.
Oh, you may need that.
- Yes.
(Kris chuckles)
And I think you're
actually still on.
- Oh, I'm sorry.
This is still mine. Forgive me.
(laughs) All right.
Well, yeah, that's why I'm
here 'cause I'm from Amazon.
So I did mention that there
are a number of personas
involved in this process.
So certainly business analysts.
Business analysts are absolutely key.
And to date, for the most part,
data analytics has been creating metrics,
what could almost be
called a rear-mirror view
of your business and what's happening.
The very nature of machine
learning is prediction.
Turning metrics into predictions
is why we begin to a adopt
machine learning algorithms
and introduce that then
into our analytics.
What if, for example,
and this is a question I
frequently ask decision makers,
"What if there something
that, if you could predict it,
it would transform your business?
Maybe if it's only two or
three months in the future?"
And that begins the conversation,
because that's what we
do with machine learning.
Now, data scientists, of course,
are sort of that piece
that we just don't have
enough of out there.
And the ones that are there need
to have efficient tools to work quickly
to get through that cumbersome process.
We talked about big data,
but that big data is not
ready for machine learning.
It's up to 85% still of
a data scientist's time
to prepare data before it can see
the light of day of an algorithm.
And that's only the training part.
It's not inference where
you're actually using it
in deployment.
MLOps engineers may not need, for example,
data science training at all
to be able to choose between,
say scikit-learn and PyTorch
for a particular problem.
But they are generally developers
or DevOps professionals
and they do need to have the tools
that are familiar to them
to get apps into production,
to version them, to blue-green test them,
and make them reliable
at enterprise at scale.
Tens of thousands now of...
Hmm? There we go.
Tens of thousands now of customers on AWS
have discovered these tools
through the SageMaker family of tools.
So that's everything from, of course,
Amazon Retail itself
to all these customers
that you see here now.
Now, when we say SageMaker,
we are really talking about a
pretty broad family of tools.
There's more than at least 30,
perhaps even 40 now,
discrete components within that.
And we're gonna show a few examples
in a few minutes of some of those tools
and how they overcome these blockers.
So now, Trish, I believe it's back to you.
- There you go. Thank you.
(Kris chuckles)
Right, so let me tell you a little bit
about Teradata Vantage.
Do we have any Teradata
customers in the audience?
Let me see some hands.
All right, so I'm gonna
really introduce this well.
Teradata is a company that
has been around since 1979.
So remember, maybe you weren't alive,
but in 1979 there was
no real cloud, right?
So we were stuck in a box
and we got really, really good
at being efficient with data.
So I think that's one of
our greatest strengths is,
because wherever we go,
we're super efficient with data.
And now we're in the cloud,
which means that we're
gonna keep our costs lower
in the cloud as well.
But what we call it
is this connected
multi-cloud data platform
for enterprise analytics.
And the reason, let me break down
what that actually means,
because it's a lot of terms put together
that could mean lots of many things.
It's connected, meaning we do
really great integration work
with our data platform.
We can bring together files and data
from different data sources.
And we have APIs that connect.
We even allow Python, or
we support Python and R
to be used to do SQL
pushdown in the database,
which I'll explain in a minute.
We also integrate very well with AWS,
say, Amazon SageMaker tools.
And so it's this connected data platform,
the multi-cloud part.
And we're not the only
multi-cloud database
or data warehouse or
cloud analytics platform.
But we are, I believe,
the only one who can query across clouds.
Now, think about that.
So even if you're in AWS,
and that's why we're here,
if you have a sister
company or other partners
that are in different clouds,
we could query across the data.
I mean, that's mind blowing to me.
Let me point out this one part of it,
Native Object Store.
Basically we provide the
native ability to query data
in JSON or CSB or Parquet files.
We also have the ability to write natively
from the Advanced SQL Engine
into those files in an S3 bucket.
So think about co-mingling your data
with files when you're
doing machine learning.
We do that a lot.
Our solution engineers like to say,
"Hey, it's a cheap way to
store data in the cloud
to offload some of that data."
But I keep telling all of them,
"You're forgetting about
us data scientists.
We love to bring in other
external sources of data
and this is another way we get to do it."
So this is where I could just say
we have now created a
strategic partnership with AWS.
We've had a three-year
commitment to do this
where we're both investing
in the integrations that we have
between Vantage and SageMaker
to start and a few other things.
So I'm very excited about that.
So that's like, "Yay!"
We just announced that.
So let's talk about
how we scale analytics.
We've heard about all of the challenges,
we've heard about what's stopping people
from getting to, excuse me,
development to production.
Well, let's talk about those bottlenecks,
and then we'll frame up
how we solve those issues.
So we know they occur at every
stage, these bottlenecks.
And I'm going to put this
in the most simple 1, 2, 3 method.
We're gonna prepare data,
we're gonna train data,
we're gonna deploy the model,
train the model and then deploy the model.
So we know 80% of the project time
is spent preparing data
rather than creating value.
We know that five months,
it takes five months on average
to take a model from training,
development to deployment.
And then it takes about...
Okay, this is actually a number
that I think is low.
65% of predictive models
are never implemented into production.
And we did get that
statistic from somewhere,
but in practice, I've seen it much higher.
So let's focus on this one, prepare data.
What can we do? How can we make it faster?
So if you're not familiar
with Teradata Vantage,
you're not familiar
with Teradata Data Labs.
Data labs are these sandbox boxes, right?
Data scientists need a
place to explore their data,
to bring their data
together, to make a mess.
Well, at least I'll speak for myself.
I am messy with data.
I know a few others are as well,
but that's what we're there to do.
We're there to look at data.
We have to sometimes make it up.
Sometimes we need to delete it.
Sometimes we need to just try and fail.
So we need a place to do that.
The data labs let you do this.
It's part of the data analytics platform.
So if you have your data
warehouse in the same platform,
you already have access to that data.
You don't have to bring
it to another database,
because we know that takes time,
it takes skill, it takes a different sort
of work effort than what
we do as data scientists.
You can also combine
your data sources there.
You can access that Native Object Store
that I talked about.
And, again, I say you
can natively query this
and people tend to think
that means SQL, SQL,
but we support Python and R also
to be able to do these things.
So that's definitely an
important aspect of that.
And the other piece that is
not that fun to talk about
but really important, I
said I was in healthcare.
I work with healthcare accounts.
My experience historically
has been 20-plus years
of working with healthcare.
Compliance, security, governance,
all of that is important.
They wanna know who has the data,
where it is, why it's being used,
being accountable for that data.
And this is what allows, I'll even say,
this is what makes compliance folks happy,
to know where their data is
and how it's being used
and being able to manage it
but still giving the data
scientists the freedom
to do what we need to be
able to do with that data.
Another aspect of preparing
data is that, historically,
if you weren't in the
practice of feature stores,
then you're in the practice
of creating an analytic data set,
using it for a model, deploying it maybe,
moving on, next data set, next model,
and then so on and so forth.
Well, we don't think that this is helping
the sustainability practices
for implementing machine
learning at scale.
We want to encourage folks
to build feature stores.
Now, I have a master's
in predictive analytics
from DePaul University.
And in that program, we were not required
to learn about data warehousing.
I took it as an elective.
It kind of blew my socks off
because I've worked with data warehouses
just as an end user.
I didn't understand why it
was being built that way.
'Cause it's weird.
But now that I understand it,
I think we definitely
need to leverage this.
We don't have to be the ones to build it.
We can find our partners
to do the ETL and stuff to do that,
but this is how we can continue
to make data have that sustainability
and also the shareability
and the democratization of data
to bring it forward to
use and reuse for models.
So still in the phase 1
of the analytics, 1, 2, 3 story,
now we're gonna start talking
about some of the data integrations
with Amazon S3 buckets,
because here we have on the very left,
we have one scenario.
Now, if you're used to
just working with data,
you pull data into your laptop
or your shared environment
or wherever you're pulling it,
you're working with data there.
You're creating your machine learning
and then you implement it somewhere.
This is not that, okay?
This is where we use either
the Teradata ML Python library
or the tdplyr library.
And I'll probably just talk about Python
because it's my favorite.
So what it's doing is it's
allowing SQL pushdown.
Are you guys familiar with SQL pushdown?
Yes, no, maybe? Yes?
I see one guy shaking, who said no.
Okay. I'm gonna explain this.
So say you really love
to work with Python,
'cause who doesn't?
(Kris chuckles)
All right, so Python is a
language, it's an experience.
You work with data frames,
and usually the data
frames are pulling the data
onto your laptop.
But what we're doing is we
are providing this package
that creates virtual data frames.
And the experience to work
with it is very pandas-like.
It's very Pythonic, it's a
very Pythonic experience.
But what it's doing is
it's taking that object
that you're creating,
the virtual data frame,
and that object is holding SQL script
in the background, right?
It's translating SQL
script in the background
and it's pushing that SQL
down to the Advanced SQL Engine
and running the data there, great.
It's returning some
results that you can see,
like top 10 rows or whatever you specify,
but the data is in the database
and it's in there within your session.
And it maintains in your session
until you either want to end your session
or write it to a permanent table.
But what this does for us
is it makes it really, really fast.
So you take that virtual data frame,
you do additional manipulations
because it's very pandas-like.
We also have the methods available
to pandas data frames and to SQLAlchemy
to manipulate that data
so you're preparing your data in database.
We also have in-node processing
where we're actually implementing a Python
or our interpreter
on each database node
so you're bringing your
analytics to the data.
You can run that Python in the database,
just natively in there.
And then we can, of
course, from the database,
because remember it's not in your laptop,
we can push it to an S3 bucket
with simple write statements.
And it is fast,
because I don't know if I mentioned it,
but Teradata Vantage
has a massively parallel
processing architecture.
I will now call that MPP,
because it's too many words.
But the Vantage MPP is incredibly fast.
It writes that data very
fast for the S3 buckets.
Okay, I'm seeing a blank there,
just throwing me off.
All right, so here's that.
Now, we're preparing our
analytic data set at scale.
We work with customers
whose data scientists
might have 21 million rows
in their record set that
they have to work with.
And I have customers I work with
that have a couple thousand.
And still, it's just a
seamless way to do this.
So you're using your in-database
functions to prepare that.
And here's a little idea
of what the Teradata ML
Python package is built,
so it's extending pandas,
it's the extending SQLAlchemy,
and we're putting these Teradata packages
around those two to work
well with our system.
And here are just some customer benchmarks
that show you why we think
this is a great solution
for working with your data at scale.
I mean, the first example,
this is a local R script sped up
when you did it in database.
It went from four hours to 10 minutes.
The same with local Python script.
It went from 48 hours to 18 minutes.
That's a pretty cool number.
Even SAS, if you're using SAS,
and I know you're still out there,
maybe in this audience, maybe not,
but incredible speeds.
The thing that, you know, faster, better,
more relevant governance,
blah, blah, blah.
What I love to point out on this slide
is that you fail faster.
Because of these efficiencies
of working with the data, you fail faster.
You know if something's not gonna work.
You know if it's not a good direction
that you're going sooner.
Here's just a sampling of some
of our in-database functions,
'cause I didn't even mention these yet.
You can run these with SQL,
write in a select statement
if you like that sort of thing,
or you can use Python or R
to access these as well.
I mean, think about it,
here's something where
we're doing moving averages
in a database.
Think about that for a second.
That's pretty cool.
And here's just some examples
of the pandas-like functions
that we have available for data frames.
Lots of things you can do.
So the bottlenecks at the
train-the-model section,
we're going to hand this over to Kris
and Kris is going to talk
about how SageMaker fits into this story.
- Thank you very much.
So, just briefly, if you haven't trained
a machine learning model in the past,
we've covered data prep.
And as just a reminder,
most data does not come prepared
to go to an algorithm, right?
So you have to have that step.
You have to acquire it, prepare it.
Now you're ready to put it face to face
with an algorithm.
When you put it against that algorithm,
it might be a classifier,
it might be predicting
time series, et cetera.
It might be just taking
image data, for example,
and classifying what's on those images.
So that's training the model.
The output of training a model,
you use an algorithm to train the model,
and the language gets
a little tricky here,
it's what we call at AWS a model artifact.
And you could think of, you know,
sometimes they say it's
like compiled code, right?
It's not the code that you wrote,
it's the compiled code.
Well, the model was built by the algorithm
when it met the data.
Now, those model artifacts
then go into production.
And when they're in production,
that's when we're dealing with
all of the MLOps issues.
So SageMaker supports every phase
of that life cycle.
First of all, we provide Jupyter Notebooks
that are very complimentary
with the tools that we just saw.
In fact, that last slide
with all the features for analytics
I wanna use that as a cheat sheet
next time I interview.
At any rate, the training aspect
of what happens on AWS,
if you've never used SageMaker, is unique.
We have a number of
features that reduce cost
and increase productivity.
You can, of course, use a Jupyter Notebook
the same way you do on your laptop,
but SageMaker provides,
actually, a batching function.
You can actually, when
you fit your algorithm,
you actually send it off to a batch
where it can run on a
machine of any scale.
So, for example, when
you're using SageMaker
and JupyterLabs within
the notebook itself,
you wanna use something that's like,
you know, 5 cents an hour or something.
But if you're ready to throw a big,
deep learning model to heavy duty GPUs,
you might want an NVIDIA server
with eight V100 GPUs in them,
$8,000 each, they're not so cheap, right?
They could be $30 an hour.
So you can work on that
5 cents an hour machine,
send it to a $30 an hour
machine for processing.
Of course, it's charged by the second.
And that's just two options.
There's also batch processing.
You may not need to do
this in real time at all
but you're sending it off in batches.
We have some other techniques
where you can actually
work on your own laptop
because SageMaker comes as a Python module
that you can PIP install.
And then when you do fit on your laptop,
provided you had set up
everything correctly,
run that in the cloud.
So you have all of these options.
The reason that it's so easy
to put all these together
and get that functionality on training
is the fact that we're sharing S3 buckets.
So we know where the data is,
we're pointing at the same data,
and so it makes it really easy
to take advantage of all these cost
and time optimization tools.
Now, deployment is key.
That's why we're building these things
in the first place.
Tracking the data, auditing the data,
having model monitors, having auditability
at every step of the way is,
for the most part,
automatic within SageMaker.
So when you're using some of
our model monitoring tools,
you get real time information
on how those models are performing.
You can capture every ints data
that's being used for
inference on that data.
You also have options for maybe not having
full-time endpoints in production, right?
Because they could be costly.
So we have serverless inference
where, just on-demand,
you can stand up an endpoint
and do inference on that.
We also have asynchronous inference.
Now, that might be a real-time situation
where you have big data,
like media data, coming in
and you need it processed in real time
but you don't wanna
wait for it, basically.
So you can launch that instance.
It actually, kind of behind the scenes,
uses our messaging protocols, SQS,
to run the job, tell you when it's done,
and from your code, it all looks seamless.
So from build, train, deploy, and monitor,
and I should also mention
certain other things
like getting human in the loop.
We have a number of tools
that are very complimentary
with the Vantage 1, 2, 3 process
where you can integrate humans in the loop
with model monitoring.
Of course these days
responsible AI is a key topic.
And every enterprise should
have that bit covered.
It really just comes down to auditability,
monitoring, and making sure
you have humans in the loop
when data that's writing that code
starts to go off course.
So let's take a look at SageMaker itself
and how some of these
features are implemented.
This is a familiar JupyterLabs interface.
If you've used JupyterLabs in the past,
these are just JupyterLab
notebooks running Python.
So if we take a look here,
these are the cells, which
you're familiar with,
running pandas here.
And we could see on the
upper-right hand side of the screen
that we actually have a choice
with just a pull-down menu to choose,
really two things, number one,
what's the kind of environment
that we wanna run it in?
If you're using Conda, for example,
there's several prepackaged
Conda environments
that you can just go that
could be, once again,
PyTorch, it could be R,
it could be whatever it is that you need
that you're familiar with,
but you can also, in a pull down,
switch the CPU you're using.
On the fly, you can switch
from a CPU instance to a GPU instance.
So some of those are set up
for fast transfer mode.
In other words, you could just pull down,
wait a few seconds,
it's not instantaneous,
and you can switch to
another instance type.
Now, that's all in the notebook.
That's, "What do I need in
the notebook at this time
to get to what I need?"
So in this example here,
we're actually taking a look
at how we're scaling two GPU ints
to a GPU cluster.
Now, as you might imagine,
we have some pretty
heavy-duty GPU clusters
up on AWS,
including the NVIDIA GPU
backplane connected clusters.
So we're talking about, really,
the highest power of GPU
training capabilities.
So that's especially important
for large language models,
things like Hugging Face, et cetera.
All right, that's all pretty heavy-duty
data scientist stuff.
And that's one persona.
But let's talk about
business analysts, right?
We really want to democratize this data.
You shouldn't need a PhD
to do machine learning.
And I apologize if you
have one, (chuckles)
but let's say if you do have one,
you wanna bring more colleagues in, right?
If you wanna run fast, run alone.
If you wanna go far, you bring a team.
So that's the whole idea here
of this SageMaker component called Canvas.
Canvas, it's designed
for business analysts.
So when you bring the data in,
you can immediately do
some basic data analysis.
You can quickly identify
bias within the data.
When I say bias, what I really mean
is it's too heavy on one side
of your labels than the other.
Of course, right within this,
you can do quick modeling,
so if you wanna do quick
predictive models within that.
And these are all from the GUI.
They're all built into the user interface.
So it's really just a matter
of getting some familiarity with your data
with the Canvas tool.
You can run models and get quick results.
And then, if you decide,
"Okay, I do need to hand this
over next stage to SageMaker,"
It's a one button push
and the Jupyter Notebook
that was used to create
these samples in Canvas,
once again, without any coding,
can then go as a Jupyter Notebook
to your data science team.
So they can pick up where you left off,
they can get all the trial results
from your preliminary training,
and you can move on to the next step.
So when we take a look
at some of the benefits
of having this kind of approach
to training and deploying
models using SageMaker,
well, some of these are pretty obvious.
To use Trisha's expression,
you wanna fail fast because, you know,
that's how we get to our goal, right?
When I'm coding at home,
my daughters always ask me,
"You always look angry when you're coding,
but I thought you loved it."
Well, that's because coding
is just blocker after blocker
after blocker to success, right?
That's failed fast.
So if you take a look at some
of these really heavy duty models here,
a BERT model, Hugging Face,
RoBERTa model right here,
we have anywhere from 27
to 54% time optimization
using these same techniques
that I just described.
All right, so that's time optimization.
It's also cost optimization,
but what about governance?
Using SageMaker tools,
you can get visibility into the bias
that might be in your data.
When you begin to train,
you could see how some of
those results are coming
so when you choose your
model to put in production,
you have all those metrics there.
And they're easy to share
right from the SageMaker notebook
to print out and share
with other colleagues
when you're making those hard decisions
of, you know, am I optimizing for latency
or throughput or cost,
or perhaps for a particular customer class
that you have, you can
get all those metrics.
We have two tools that
are built into SageMaker
that help enable that process.
One is Clarify, which has a number of...
Well, it's really a curated set
of responsible AI tools,
I guess is the term that's used now,
that look for all kinds
of bias in your data.
And we can produce metrics
not only when you're training the data,
but when that model goes into production.
So as you put your model into production,
we've all had this experience,
human behavior changes once people detect
that they're interacting with a machine.
We get it when we do customer
service calls all the time
and we get a chat bot, for example.
Well, it's the same thing
almost anywhere ML models
are put into production
because that data wrote the code.
You put the code into production,
and you will begin to see changes
in behavior in the data that's coming in.
That can have a number of consequences.
You may need to rerun the model.
You may need to retrain regularly.
You may need to bring
data scientists back in,
human in the loop, for example.
So all of that is tracked.
It's audited, it's auditable,
and it can ultimately be used then
to make better decisions
about how those models
are going to work in production.
So the next step is what about MLOps?
It's a really important term right now.
There's a number of aspects of DevOps
that are very different
when it comes to working
with machine learning models.
First of all is cost, right?
So we have to keep track of how much
our models are gonna cost in production.
We have something called
the Inference Recommender.
So once you've trained your model, well,
while you're training the
model, for that matter,
you can automatically determine
how much that model is
gonna cost in production
at various stages of deployment.
So when you run that
Inference Recommender,
it goes in, you can get these costs.
You can also see alternative costs
for you to deploy those, for example,
on different instance types of systems.
So operating at scale requires
a number of tools for inference.
Inference is 70% of the costs
for all of your successful
machine learning models.
I think that's a very
conservative estimate,
because that's why we do this
in the first place, right?
Real time is obviously key.
If you were checking your phone
while we were chatting here,
maybe checked your bank balance,
I happen to know that a lot of those banks
have maybe six customized models
that were just fired up
while you're taking a look
at some of the options at your bank.
That's what happens today.
That's real time inference,
but the customers could
have millions of models
laying at wait ready for that.
And they may not need
to run at that moment
when you touch that particular
screen on your phone.
That's where batch transform comes in,
where you can process large data sets
that produce some of that data
that might be not necessarily
used for real time.
Asynchronous inference
is a real important, critical step.
It sort of fills this gap
between real time and batch
and what can just be
done sort of serverless.
When I say serverless, serverless,
if you're familiar with Lambda
you know we have Lambda services
that just do compute serverless.
Well, we have serverless ML right now.
So what that is is you
can have, once again,
for each customer, maybe
dozens of these models.
You will sacrifice a little bit
for the first call on
that particular model
once it comes up, just
like you do in Lambda.
But subsequent calls are
more or less instantaneous.
And that middle gap, the
asynchronous inference
is really for, once again,
these large data sets,
large payloads, more than a gigabyte.
But you're in an app.
You're running real time.
You can have the little
spinning wheel for the customer
while they're waiting for that to process,
and then you move on.
And it's all really easy
to sort of integrate.
All right, so that's
the SageMaker picture.
Thank you, Trish.
- Thank you. Yes.
So let me just recap for where we've been,
because this is important.
So you're preparing your
data in Vantage at scale
with really fast
processing and parallelism
that is being automated.
So your features are going to continue
to be available for
those sustainable models.
And then, once you're ready to deploy,
we have integrations that work as well.
You don't have to copy your
data over to S3 buckets.
You can also connect
with the Teradata ML Python package.
It also has a driverless
connection to the database
so you could actually work
with it directly there.
Once you have that data prepared,
you're bringing it over to SageMaker
to develop and train your models.
And now you have your model,
which you can deploy
with an endpoint, right?
You can access that endpoint
from APIs directly from the database,
from the Teradata Vantage database.
So when you put it into production,
you don't have to move data
back and forth anymore.
It's already there. It's
already been prepared.
And that's one way that we
are allowing these models
to be implemented at
scale and very quickly.
So this is the integration
flow that we have
where you prepare to transform
your data in Vantage.
That's on the very left.
And whether you're using the feature store
or an analytic data set,
or even the Native Object Store,
in combining those,
you're gonna transform
that sample data set
a significant size
that's going to be useful
for training in a SageMaker,
whether it's an open source
or whether it's a SageMaker algorithm.
You can train and build them there.
And then you're going to push those,
that ML model back to Vantage.
So the way we push it back
is because we've serialized it
into a predictive model markup language,
or a PMML file, or an ONIX file
or H2O.ai MOJO file.
Bring those back into Vantage,
install it in to the
database as a BLOB object,
and it's accessing that
machine learning there.
And we have interpreters
within the Vantage system
that's going to understand
those machine learning algorithms
and be able to produce the results
and scoring and inference.
We also have these API
integrations in the works.
So I'm excited about these.
So we're creating these APIs
to be able to leverage
things like Amazon Forecast,
Fraud Detector, Augmented AI,
and Amazon Comprehend.
I'm super excited about that,
because I know I work with
the healthcare clients
and I know that, in particular,
Amazon Medical Comprehend is something
that we're very excited
about working with.
So let's transition.
We'll spend a little bit of time
talking about some of the use cases
that we're seeing out with our customers.
So in healthcare, I mean,
I work with the healthcare accounts
so this is what you're gonna get from me,
we have customers that are
creating propensity models,
basically making patient 360.
And if you're not in healthcare,
it's a customer 360, same thing.
It's a propensity to have a
certain disease or condition
or have a propensity
to do whatever thing
that you're trying to do,
but it's for customer
or patient segmentation
to make the personalized experience
for our patients.
Same thing with our customers,
whether you're retail or other.
We're seeing use cases,
and these are our top
ones that we see often,
they're working with supply
chain prediction, of course,
recommender engines for
healthcare network optimization,
meaning which specialty
provider should I go to,
resource utilization.
So, as we all know, during
the pandemic especially,
resources at a hospital
have been stretched.
Prior authorizations, cost prediction,
point of care, those are
some of the use cases we see.
In this particular one,
we had a large healthcare network.
I mean, millions and millions of patients.
So they had this very fragmented,
siloed systems where they have many
organizations trying to come together.
Well, we were able to
help them bring together
over 40 different data sources
amongst all of their
organizations within their network
so that they had that
one view of the patient
and they could follow their patient
in their journey, in their care journey,
through a specific condition.
You know, this is the
type of data integrations
that we're good at.
And then what did they do with those?
Well, they created
basically recommenders for
their patient portals to say,
"Hey, I noticed you were looking at this.
You filled out this assessment.
Hey, have you considered
going to this specialist?"
You know, those types of things
just to improve their care journey.
And that's been something
that is helping them
to increase their revenue incrementally.
But even for one service,
it was like a $90 million increase.
So that was pretty spectacular for them.
All right, so I'm gonna
hand it over now to Scott.
We're gonna finally let you talk.
- (chuckles) Thanks. I do
need to apologize though.
I'm not in healthcare.
I'm not a data scientist.
I'm not a technician.
I'm the business guy.
I've only been in manufacturing
for about 44 years,
and I still haven't earned
a nice, black golf shirt.
(Kris laughs)
Again, I apologize for that.
I don't really fit in up here.
But, no, we're gonna talk
about ML in manufacturing,
what some of the leading use cases are,
and in the interest of time,
probably only a couple of these.
But these are probably
the leading six areas
that we see in our customer base right now
where ML is being used:
predictive maintenance,
predictive failure, quality and yield
on the shop floor, supply chain,
supply chain including
forecasting demand planning,
energy consumption,
forecasting in the plants.
And sustainability is really
starting to come on strong.
Digital twins, which I
know I was in a session
at nine o'clock this morning
that talked about digital twins.
And then, of course, all the new latest
and greatest smart industry
for auto manufacturing techniques also.
But it meant, in the predictive failure,
predictive maintenance,
it's really important
that data is being used
to look at all the history
that has taken place
and all the activity that's
taken place on the machine,
the current demand forecast.
Everything is gonna be able to go into
identifying what could be the
trigger points for a failure.
That's the scheduled maintenance records,
any kind of unscheduled
repair that's been done.
We have customers in
the aerospace industry
that use a lot of text
and unstructured data,
log data as well,
and any other kind of usage information.
In this case, in the aerospace area,
it's the flight logs and flight patterns
and pilot behavior, weather, altitude.
You know, it's basically every piece
of information that you can get
about how that piece of equipment
is behaving in the field
to help build the models
to predict that failure.
And ML capabilities, along with SageMaker,
can identify those
patterns that you can use
and those models that you can use
to build those predictive algorithms.
Lots of benefits in manufacturing, right?
Reducing unplanned downtime,
diagnostic base repair.
So if the models can help you determine
that X part is going to
fail, they'll know why
and they'll know, theoretically...
We have a customer they're working with
that is tracking behavior
of over-the-road trucks
and the engines on those trucks.
And they're able to diagnose
the problem with the engine
while it's still driving down the road
after the check engine light went off
and are actually looking at saying,
"Okay, you're out on I-95 somewhere.
You need to be in a service center
within the next 24 hours.
The next closest service
center on your route
is, you know, somewhere in Florida
and we can get you in there.
They have the part.
We can do the repair in three hours
and we can get back down the road
with the minimal kind of downtime
for that truck on the road
without impacting its customer service."
So those kinds of things
to be able to do that diagnostic repair
or diagnostic analytics is
helping improve service life
and customer service for those products.
Obviously, lengthening the service life
of a product like that,
better supply chain
and inventory planning.
As I said, they can tell
you that that dealer
that you're gonna take that truck to
actually has the repair part you need.
Otherwise, we would've sent
you to a different one.
And then just overall
planning capabilities
for where inventory should be distributed
across your network.
Those kinds of things around
predictive maintenance.
I know that in our airline
and Air Force customers,
it's understanding what
kinds of planes are flying,
where, out of what bases, and
where the parts should be,
based on their failure records
and their plant maintenance records,
where we should optimize the inventory.
So it becomes a much bigger picture
than just, this thing's about to fail.
It really affects the entire
ecosystem of the enterprise.
And these techniques that
we're using with SageMaker,
and we're gonna talk about
one here in a minute,
are just beyond compare
with what we could just do
just a few short years ago.
Engine predictive failure,
we have a heavy equipment manufacturer
that is in the mining business,
and any unscheduled
downtime is literally money.
If they're not hauling dirt
over one of those big dump trucks,
then that dirt's not going
to the processing plant
and they're not finding gold.
So if that truck goes down unplanned,
they're losing revenue by the minute.
So it's a very, very high-value value prop
to be able to predict the
failure of those engines,
of that equipment.
We use data on Vantage with AWS
for the primary data on the vehicle,
the maintenance history,
any sensor data that's
coming off the equipment
to build those predictive models
and then deploy them in the field
so that they can avoid
that unscheduled downtime.
Predictive quality, we're gonna talk about
a specific use case here in a second.
This gets to the point where we can do
deep-root cause failure analysis
of any kind of quality and yield problem.
We've had a customer that
makes hydraulic hoses.
And we were looking at everything
associated with that hose
as it was going through
the manufacturing process
to ensure there were no areas on that hose
that may puncture out in the field,
'cause it's under high, high pressure.
And it could pop,
literally pop in the field.
And so being able to understand things
like overlay wraps on the wire
as it's going through
the machine being built,
or the temperatures that's
going through the autoclave,
was its set right?
And one of the things
that's really important
when you're looking at all the sensor data
that's coming off the manufacturing floor
to predict a quality
problem or a yield problem,
is you also wanna look and see
what the operator is doing on the machine.
Did he just tweak the machine
because he heard something
in it that didn't make sense?
And when he did, he completely
violated all the rules
of the predictive model that
we just spent, you know,
days and weeks building that
just blew it out of the water.
And now you gotta start over from scratch
for that production line for that day.
So it's understanding
the human intervention
and what damages that causes
after you've put the
predicted models in place
to tell you how to run that
product in the first place.
So one of our customers,
joint customers with AWS
in an industrial cloud,
is a big automobile manufacturer.
And they have put a predictive
failure model in place
on the robots on the shop floor.
Prior to doing this,
they were able to inspect
one out 1,000 spot welds
on every vehicle that was produced.
And you can see some of the numbers there,
1,300+ vehicles a day, 7
million weld spots a day.
Deployed over 42,000 analytical models
to identify each and every weld
as it was going down the line
to ensure the welds were good, 100%.
And you can see where
there are good welds,
borderline welds, and bad welds.
It's a function of the resistance curves
and time and the temperature
of the weld itself.
So by doing this, not
only did they now inspect
100% of all welds,
that data goes out to the cloud,
it goes through the models,
identifies the data by a weld spot,
and then reports back to the operator
in less than 10 minutes
where there's a vehicle
and what weld is possibly bad
and go inspect it physically
and make a decision on the line
whether or not to rework
it, to pull it off the line,
do it while you're going down the line,
whatever the case may be.
But on 100% of the welds,
and again, 7 million welds a day,
they're able to identify
the individual welds
that are failing.
So again, following the 1, 2, 3,
they had to prepare the data,
they had to train the models,
over 40,000 different models,
and they had to deploy
the model in a cloud,
an industrial cloud, AWS Cloud,
and again, using QuickSite,
getting that data back to the
operator within 10 minutes.
So I'm going through this,
but we're about to get
the yellow light here.
So I'm gonna be a little quick.
The benefits of manufacturing
are pretty obvious.
If you look at this list,
it's to improve the
quality on the shop floor
and process driven loss reductions,
cost reductions on the shop floor
through preventative and
predictive maintenance.
A boost in capacity, right?
If I wasn't able to
inspect 100% of the welds,
I may not be producing as many vehicles
'cause I gotta make sure
the quality is there.
Now that I can inspect 100% of the welds,
I can ramp up the production lines
and we can rock and roll, right?
The ability to scale.
We're gonna take that
model for those welds
and we're gonna go across
multiple lines in that plant
and then multiple plants in that company
across multiple countries.
So the scalability factor is there.
The ability to do that very quickly
and still return results
within those few minutes of a weld spot
are critical going forward.
That helps everything associated
with inventory up and down that line,
planning and forecasting, supply chain.
It improves, obviously, quality control.
I mean, it is just, again,
that enterprise value
of being able to use those models
on one specific area,
but then apply that across the enterprise
for overall cost savings
in the organization.
So with that, there are a couple
of kind of go-tos we wanna talk about.
- Yeah, so we've talked a lot about this
and showing you a lot of slide where,
if you wanna come see it in action,
come to the Teradata booth
in the Tech Showcase.
We'll show it to you.
We'll show you how fast
it is. Come on down.
Number two, if you have
dinner plans tonight,
cancel them, come with us.
(panelists laugh)
There are folks in the
audience with cards.
I forgot to bring them up here.
They have cards with a QR code
to sign up for dinner at
Carbone's here in ARIA.
It's gonna be a good time because,
you know, Scott will be there.
(Scott speaks indistinctly)
I'll be there too. I'm kidding.
All right. Yeah, there it is.
But yeah, you can hold it up.
Thanks, Vanna. All right.
And then number three-
- Oh, sorry.
- One more thing, we've got
some virtual hand-on labs
or in-person immersion workshops.
If you're interested in having those,
email us att aws@teradata.com.
And with that, I'm gonna say, thank you.
Enjoy the rest of your conference.
We'll take questions
until they kick us out.
- In 4 minutes and 25 seconds.
- So, any questions?
- Okay. Thank you very much.
- Yeah, thank you.
- Have a good show.
(audience applauds)