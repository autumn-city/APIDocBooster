 So in this video, we are going to talk about something called
 long, short term memory LSTM, which is for modeling long range
 dependencies. And the LSTM cell, you can think of it as a
 modified version of a recurrent neural network. So that is
 particularly helpful for working with longer sequences and really
 kind of essential to get good performance out of a recurrent
 neural network. So previously, we talked about other solutions
 to vanishing and gradient vanishing and exploding gradient
 problems. So long time ago, we when we talked about multi layer
 perceptrons, we talked about using the relu function instead
 of sigmoid functions, like the 10 h or the logistic sigmoid,
 because it can help with mitigating or vanishing gradient
 problems. So, but even then, even if we use a relu function
 itself, sigmoid or 10 h, we may find that making a multi layer
 perceptron deeper than one or two layers doesn't really work in
 practice. So if you've tried in the homework to have a multi
 layer perceptron, like with four or five layers, it still didn't
 work very well. And yeah, that is still because there might be
 problems with the vanishing gradients. So another technique
 that can help a little bit with that is batch normalization. So
 if you have batch normalization, you may find that you can have
 no multi layer perceptron with maybe three, maybe, maybe even
 four layers, but it's still it's not great. In the context of
 convolutional networks, because they are simpler, we found that
 having more layers might work. So we had the VGG 16 network
 with 16 layers. But then going beyond 16 layers, even if we
 have batch norm, again, doesn't really help. So we have to we
 had to add another trick called skip connections. And they are
 these skip connections and residual networks helped then
 with constructing networks more than 3050 or 100 layers. So
 there are multiple tricks that we can use to help with the
 backpropagation when we might have vanishing or exporting
 gradient problems. So but this was usually for the number of
 layers. Now in this recurrent new network setup, we also have
 to consider the time steps, right? So on the previous hidden
 states, so because you're like we've seen in the previous
 video, there's this product that we compute, there might be
 another level of encountering vanishing and exporting gradient
 problems. So there are here three different techniques for
 dealing with these issues. So one is the simplest one is a
 gradient clipping. So that is actually a quite widely used
 technique also in other contexts. So this is essentially
 I'm setting a maximum or minimum cutoff value for the gradient.
 So for instance, we can say, never have gradients greater
 than, let's say two or smaller than two, minus two, and things
 like that. So we can cut manually the gradient, so that
 we don't have very extreme updates. Another one is called
 truncated backpropagation through time. And this is just a
 simple technique for limiting the number of time steps during
 backpropagation. So you when you have long sequence, let's say
 you have a very long sequence, and these all go to the hidden
 hidden layer, and then to the output layer and so forth. So
 for forward propagation, you may use the whole sequence here. So
 but then when you back propagate for the hidden layers, you only
 maybe back propagate through the last 20 time steps or so. So you
 don't back propagate through the whole sequence. This is called
 truncated back propagation through time, and it might work
 pretty well. However, an even better way for working with
 longer sequences is the so called long shorter memory,
 which uses a memory cell for modeling, yeah, long range
 dependencies to avoid vanishing gradient problems. And this goes
 back to a paper from 1997, which was pretty influential
 proposing this type of memory cell. There has been another
 one called JIRA, you will just mention it again, maybe at the
 end of the video, which is a simplified version of that
 slightly simpler. But the LSTM is still widely used in both
 LSTM and JIRA, you are approximately perform
 approximately similarly. Well, there are sometimes problems
 where LSTMs perform better, sometimes where JIRA use perform
 better. So none of the two is universally always better than
 the other. It's just like another, let's say hyper
 parameter to investigate whether we should use LSTM or JIRA use
 cells. But nonetheless, LSTM cells, I think are still more
 popular than JIRA use cells and the most widely used on RNN
 memory cells, I would say. And in this video, we are going to
 talk about these LSTM cells. And then also, yeah, later, we will
 also implement this in code using pytorch. So what I'm
 going to show you in this video may look super complicated. If
 you don't understand it fully, don't worry about it too much. I
 mean, here, we are really in this class in general, also for
 this RNN lecture, just trying to get a big picture overview. This
 is still an introductory class. So most people find these LSTM
 cells very complicated. So if you don't understand it, like
 fully, don't worry about it too much. If you want to learn about
 it more, yeah, you would have to probably spend more time reading
 these papers and maybe trying to implement it from scratch by
 yourself. So that might be probably taking multiple weeks or
 months to really get a good feeling for how that works. So
 you can't expect really like from an overview, you have to be
 an expert immediately. So I'm trying to say, when you see the
 next couple of slides, don't freak out, it looks more
 complicated than it might really be. But also, if you don't fully
 understand it, it's not your fault. It's a complicated topic.
 So here is how the LSTM cell looks like. So there are many,
 many things going on. And in the next couple of slides, I will
 walk through this now step by step. So where is this before I
 explain all these letters and notations? Where's this LSTM
 cell located? So you would actually put this LSTM cell here
 in the center. So you would have one, I mean, you would use it
 for different time steps. But if we would focus, let's say on
 this time step here, you would put it into the center here for
 instead of having the regular hidden state or hidden layer,
 you would insert this LSTM here. And if you have a multi layer,
 RNN, like that one, you would also have, or you can have
 multiple LSTM cells, for instance, you could have two
 LSTM cells. So how again, this connects, so let me erase this.
 So in blue here, this is here, the hidden state from the
 previous time step. So if you consider this one, this is here
 this input. So if we think of this whole thing, as this red
 box here, on the left, lower side, this blue is resembling
 the input from the previous state state, then there's
 something new we have, that's the cell state, I will explain
 it in the next couple of slides. So there's a cell state. And
 then here, this green one, I should have produced for the
 cell state, something else that's black cell state. And
 then here in green, that oops, see, I will have explanations.
 So yeah, so here, this in green one, this will be the output to
 the next time step. And then here in pink, or purple will be
 the output to the next hidden layer. So you can see, this red
 one really would fit here into this RNN. So instead of having
 just a regular hidden layer, we would now have this, yeah, LSTM
 memory cell. So yeah, going through things step by step, we
 have this cell state at a previous time step. And then we
 update here in this whole computation, the cell state and
 pass it on to the next time step. So there's this so called
 state of the LSTM. And another thing we have is, like I said
 before, we have the activation from the previous time step. So
 the hidden state from the previous step step, and we will
 pass on the activation computed from this memory cell to the
 next time step. Then here for the mainly for the memory for
 the cell state, we have two different operations. So we have
 here the element wise multiplication here, we have an
 addition. And these sigmas here, they represent our logistic
 sigmoid activation functions. Now here, we have so there are
 three different gates. So talking now about the gates. So
 we have this forget gate here. So it controls essentially which
 information is remembered and which one is forgotten, and it
 can reset the cell state. So here, we receive the input at
 the current time step, and we receive the input from the
 previous hidden state like in a regular RNN layer. And then it
 goes through some computation here. And this one, this
 computation is essentially an evolve involving two matrices
 like we had before. So we have before we had this matrix H, H,
 right, and w, h x, here, we are calling it for forget gate
 instead of h h, let's call it f h and f x. And then we have
 also bias unit BF. So I just see I would actually here. And this
 goes through a sigmoid unit. And you know that in a sigmoid, the
 output can be either or is between zero and one. So if the
 output is zero, for instance, and there's a multiplication
 here, element mice multiplication, we can essentially erase
 the previous cell state. So here, at this time step, the
 network has the option to forget the cell state. So in terms of
 not doing anything from the previous arm, so not including
 any information from the previous time step, or it could
 learn a one, so it could fully use information from the
 previous time step. So this is the so called forget gate, so
 which it's controlling which information is remembered and
 which is forgotten. So it can zero out the previous cell
 state. So then there is this input gate. So here, the forget
 gate again, is potentially erasing information. And here in
 the input gate, we are adding no information. So there are
 actually two things going on. There's a sigmoid and a 10 h
 here. There's an input node and an input gate, and they get
 element wise multiplied. And so this one can be zero or one. And
 this one can be between minus one and one. And the output then
 can essentially also be between zero and between minus one and
 one. And on this gets added to the previous cell state, which
 might be erased through the forget gate or kept. And each of
 those, they are really, yeah, just computed like in a regular
 neural network where we have. So here, we have a plus, of
 course, so here, we have w times x. And here, we have this one,
 actually, I know that that should think it's correct.
 Sorry. Okay. Um, yeah, okay. So this is our setup here. So
 that's how we compute this. And then the same thing for the
 input gate, we have logistic sigmoid instead of a 10 h now.
 And these are the values we compute, we multiply them and
 then add them to the cell state. And yeah, this information comes
 also, of course, from the previous cell state and the input
 here. Now, just looking at everything together, how we
 compute the output cell state for the next time step. So
 again, just to summarize, we multiply the previous cell state
 by this forget gate, then we add the product of the input node
 and the input gate. And then we also have a third gate, that's
 the output gate, that's for updating the values of the
 hidden units. So here, before what we had is this just for the
 cell state. But now the cell state here also goes through a
 10 h and then gets multiplied. So this can begin between minus
 one, one. And this gets multiplied with this output
 gate here. So the output gate itself is also computed like the
 previous ones. And the product of this 10 h output and this
 output gate goes then to the next time step on then also to
 the next layer in the neural network. So with that, I mean,
 if I go back to it goes, goes here. Well, yeah, okay. So it
 goes either here and here if we talk about this red one. So this
 would be to the next layer here. And this one would be to the
 next hidden state. Forwarding in. Alright, so this is what we
 had. So that's the output gate. And yeah, here is how the next
 hidden state is computed. That's what I just mentioned the output
 gate times the 10 h or the cell state. And yeah, this is
 essentially it. And it's a pretty complicated setup. It's
 kind of some justification behind it. So that's embedded in
 the original paper, if you're interested in reading about
 that. So in practice, if I mean, this sounds very obscure, I
 think, I mean, it works well in practice. So that's probably
 why it stood the test of time why people use it. And yeah,
 like I said, it's so widely used and popular. But there's also a
 version where people try to simplify it a little bit. So
 there's this gated recurrent unit, which is, I mean, it's
 not that recent. It's also already nine, eight. Sorry,
 seven years old. So it's not that new anymore, not that
 recent anymore. But it's also popular implementation of
 something similar, that is a little bit simpler. It has a
 few, it has fewer parameters. And in practice, they are both,
 like I mentioned before, may work well in practice, because
 of your time reasons, we're not going to cover the gated
 recurrent unit. It's also not, I would say, super interesting in
 that way, just to talk about it in the same way we talked about
 it before. So it's just a snapshot of it. And if you're
 interested, there's a nice article exploring both LSTM and
 comparing them to gira use if you would like to read more
 about these types of architectures and get a feeling
 when they work well and looking at some applications of those.
 Alright, so again, here in this lecture, we are just trying to
 get a big picture of how recurrent networks work. There
 are many other topics we wanted to cover. So we are not going
 into too much detail about this particular architecture. If
 you're interested, I will post some more reading resources on
 Piazza on canvas. So in the next video, I will then talk about
 many to one RNNs on a word level for implementing a classifier.
 So that will be our example. And then later in a later week, in
 two weeks, approximately, we will be revisiting also
 recurrent new networks for many too many tasks for generating
 new text.