 And  having batch norm before the activation, that's usually how  yeah, that was originally how it was proposed in the paper. 
 That  means it may be that we need fewer epochs to get the same  loss that we would achieve if we don't use batch norm.  So you  usually with batch norm, the networks train faster. 
For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.  However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].
BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice. 
 So we had if we  had three features, we had three gammas and three betas.  Now, we  extend this concept here to the two dimensional case where we  compute these four inputs that are four dimensional, right,  because we have now the batch size, we have the channels, we  have the height, and we have the width.  So we compute the batch  norm now, over the number of inputs height and width.  So in  that sense, we, we combine these. 
