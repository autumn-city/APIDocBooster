so welcome and ah now today we would be doing
ah one exercise which is on ah sparse autoencoders
and ah while in in the last lecture i have
explained you exactly where and how sparse
autoencoders come to place so ah while we
are doing this particular exercise using the
mnist ah digits classification problem you
would be exposed on to how to incorporate
sparsity within your code and then ah how
does this have a subsequent down role to play
down
so you clearly remember ah when we were doing
down ah our earlier lecture on ah sparse autoencoder
so there was ah l one norm which was introduced
over there or what was also called as a l
one penalty loss and then thats thats basically
to find out ah the total number of ah zeros
ah which are present down and then ah just
just do some sort of a kl divergence ah between
the total number of zeros present and the
total actual desired number of zeros to come
down so that was a extra loss which we were
ah adding down as as an extra amount of error
to the whole system and the whole objective
was that ah till we come down to the point
where we desired to have ah we actually have
that many number of zeros within the weight
matrices as we would desire to have over there
this error is still going to be a higher value
and then it would come down
so technically if i desired that thirty percent
of my ah weights over there are completely
zeros and if its not ah up to that level then
ah it has a higher error and on the other
side of it say i desired that it is thirty
percent of them are zeros but then ah eighty
percent of my weights turn over zero so that
thats mean that i have a system which is learning
to be heavily redundant but i dont want the
system to be that rate on it so i can actually
ah because of a positive error coming down
because ah of that k l divergence function
over there you would see this ah metric actually
coming to a point where it would come down
to that thirty percent point over there
so thats a typical example which we will be
doing however given one thing in mind that
we have just looked into one kind of our cost
function one kind of a loss function and not
these two kinds of different loss functions
so how to incorporate that within your network
and whether that has any changes in in the
forward pass and in the backward pass is what
we will be doing today ok so ah starting from
the first part which is ah quite simple and
ah just on loading down your ah libraries
and then eventually coming down to you are
loading down the data so we are using the
same ah architecture and same kind of a notion
as ah in any of our earlier experiments with
mnist so you have sixty thousand ah training
samples and ten thousand testing samples and
what we are doing is ah we call up the data
set from our torch vision data sets ah point
and the advantage which we get down is that
i have already downloaded everything and kept
it over there
so ah with iterative runs i i dont need to
so if if you are not doing a ah refresh clone
or ah not just ah deleting the older ah archive
wherever its put down and then putting down
the newer one then you know that pretty much
everything is over there so your data sets
are also preserved and at this point of thing
one suggestion which i would give is that
ah you can keep on downloading newer assignments
as they keep on coming on the jit link over
here but then please ah dont delete the older
folders over that in which your earlier data
sets were kept otherwise every time you do
a run ah and if your data set folder is completely
deleted then you would see ah this this whole
ah messing up so one simple thing is that
always ah jit clone onto the same location
so you know that its always refreshed by the
newer version of files which have come down
from the ah jit repository and if you have
made some modifications and kept it then please
make a copy on your separate ah jit account
and and upload and keep it for your own purposes
and ah dont disturb the main ah repository
which is downloaded and kept on so here ah
we start with ah the same way of you having
for ah workers in parallel loading down in
batch sizes of one thousand ah samples so
that goes out and ah then ah i have my g p
u availability check over here which checks
out my my g p u is available so i i can actually
use my best way of ah accelerating out my
codes
now we start with defining what comes down
as a ah sparse auto encoder ok so now you
see two different parts over there one is
that ah i initially start by defining a newer
kind of a class and this class of ah this
this particular ah class which i define over
here is what is called as a l one penalty
ok now ah it it becomes a bit confusing at
this point of time to a lot of people but
lets keep it simple and do it ok ah yeah so
here ah the whole point was that while i am
actually ah doing this autoencoder so where
wheres the first time where you would be looking
into your ah sparsity
so you had ah say some seven hundred and eighty
four neurons which were connected down to
four hundred neurons and you just had these
four hundred neurons again mapping back to
seven eighty four neurons and this was the
simple auto encoder which you were creating
ok now i would like to add down sparsity at
the end of this first layer or which will
look into the sparsity of the weights in my
ah connections from the first hidden layer
to my output which is being created over there
ok great and the whole objective is that these
layers will be my connections from my hidden
layer to my output layer will be sparse only
if there is a over complete representation
present in my earlier layer which is for my
input to my hidden layer
this has to be there otherwise we cannot ensure
sparsity in any way and and it goes ah around
the web in order to ensure sparsity you will
have an over complete representation and you
will whenever you have an over complete representation
you will have a sparsity so this has to go
hand in hand with the learning paradigm over
there now comes the two different aspects
of it so one is that i would need to define
what is on my forward pass ok so in my forward
pass what i am trying to get down is basically
ah i find out that the total number of weights
which have a ah value of closer to zero
this is what i just find it out whereas when
i do my reverse pass or my back propagation
then i dont want ah any of these to have any
impact and changes while ah i need to keep
one thing in mind that these parts autoencoders
they would ensured that you keep on ah like
wherever it becomes a zero then that zero
is stored over there and and you while in
back propagation you also have the same thing
in that so here what i would do is that in
my back propagation i need to have some sort
of a memory to remember down my exact number
of ah elements within the weights which were
zero and that comes down from this cloning
based on a ah sign operator
ok so whether it was a zero valued thing or
something closer to a zero value thing or
whether it was a nonzero value of thing so
these are two things which i have to explicitly
define into something called as an l one penalty
function now once thats defined so first i
will have to execute this particular part
of my l one penalty function over there ah
thats executed then i come down to my auto
encoder definition okay so my auto encoder
was that i had ah seven eighty four neurons
connected down to four hundred neurons and
four hundred neurons to seven eighty four
neurons on my decoder set
now comes my forward pass of the auto encoder
so what i do is i have my first forward pass
which is from my ah ah seven eighty four neurons
whatever is my input they go down to four
hundred neurons now at four hundred neurons
i will be putting out my sparsity for the
first time so and i want to have a sparsity
in position of point one or ten percent of
my weights are supposed to be zero thats by
definition what i am saying down over here
now ah this l one penalty is basically a function
which has been defined in this earlier case
over here and i have defined both my forward
and my backward definitions for this particular
new layer which we are proposing for the first
thing
so you can actually use this as a standard
definition for proposing any newer kind of
a layer architecture as well so maybe you
you come up with your own neural ah layer
architectures at at any point of time and
you can actually use a separate function for
defining that we do the same thing in order
to do this l one ah penalty loss over here
and then whatever is the output which comes
down over here which is now sparse in some
sense and you have a decoder through which
it goes down ok next ah i can just print my
auto encoder and then if ah i have ah g p
u available i would just be using ah g p u
to do it
now from there what i do is ah i use two temporary
variables and these are basically to copy
down my weights the whole rationale for ah
copying down these two weights are ah like
before i start my whole training process i
would actually like to see ah like how they
are initialized and and how they look like
and what happened after my training with this
kind of a sparsity coming now so lets run
this part and ah so you print down your network
architecture now if you look into your network
architecture at no point of time this l ones
penalty ah loss is coming down over here because
this is just a function which has been defined
for the forward pass
so you you dont have a modification over the
architecture but you are just modifying the
way your data would be passing in the forward
pass of your network ok now my next part is
to go on and train this autoencoder now ah
based on our experience we did see that over
twenty iterations it does run out now again
from our prior experiences with the earlier
ah examples which were doing it does take
a significant amount of time to finish it
out so a few ah seconds i will just set this
running while ah i actually start explaining
you what goes down over here
ok so we decided to go with twenty ah epochs
of learning and a learning rate of point nine
eight and since i am training just a simple
autoencoder architecture over here which is
just trying to find out the reconstruction
loss and trying to minimize it so my criterion
my loss function over here becomes m s e loss
function ok now what i do over here is that
i would like to ah have ah like these ah data
points over there also stored for my like
a la[ter] later on point of viewing over there
now what i start from that is i will start
my ah training my my training will just be
something loop which keeps on rotating over
the iterating over the number of epochs and
thats my epochs counter which is running down
ok so now ah as in with the earlier case if
you have a g p u then we convert it onto a
g p u variable a c u d a variable which sends
it over to the g p u otherwise you just still
have it residing on the c p u then you do
a zero of all the residual gradients present
down over there and then comes down the first
part which is a forward passing ok
okay so there is one forward pass of the whole
batch in one single epoch after that what
i do is i find out what is the loss function
so using my criterion which is a mse loss
function and this is just the difference between
the input and output because i was just trying
to reconstruct whatever was given as input
over there okay now i need to do a backward
or the derivative of my loss ah del a a nabla
of my ah loss function a nabla of j and here
i get my ah derivative of the loss coming
down and then is my vanilla gradient descent
or or the simple strain ah plain method of
gradient descent which i am using if you remember
from my last lecture where i was explaining
you the theory of ah sparse autoencoders you
dont have any specific ah kind of a change
coming down in terms of your learning rule
so the only thing is that we add a certain
term onto the cost function and how it gets
added is because you have ah this forward
pass and the backward pass both of them defined
and in the backward pass is where i am adding
this extra part ah on the gradient over there
and and nothing more so what this introduces
necessarily is that my gradient is what comes
down from the ah backward operator over the
criterion function and backward operator over
the network now that i have my gradient ah
of my input available i would just be adding
this extra gradient which is the loss computed
in case of when i have my ah sparsity incorporated
over there so thats just the addition of the
loss which i do over there and thats what
ah includes my equations whatever ah we had
done over there
now ah from there what i do over here is that
this is just a simple trick to ah display
down your ah outputs after a certain ah given
point of time and and then nothing beyond
it so lets just see ah ah how it has gone
down so yeah it was good that ah you did finish
training the whole thing and ah over with
twenty epochs we have somewhere ah achieved
a loss of point zero five eight now if you
remember in your earlier mnist case ah with
training epochs you were coming down to somewhere
around ah point six one if i clearly remember
whereas ah here with using a sparsity ah
we have not been able to come down to that
level but ah after twenty epochs we did go
down and and clearly from the earlier example
where you had two different data set which
are of the same ah empirical size in terms
of the number of pixels and and almost in
the same dynamic range of gray values you
also did recall that ah its not necessary
that you would come down to the same error
limits for both of them so here also its its
something of the same sort now here what i
try to do is basically ah in order to look
down into what is the performance of my auto
encoder ah over here i would just like to
check out so
lets look into the performance of the autoencoder
so initially when ah it was trained on ah
like at the start of the training which is
the zeroth epoch so you have all rates which
are randomized and taken down from some sort
of a gaussian distribution and plot it down
over there so with that if this is my input
then this is something which comes down as
my output over there and and you cannot necessarily
make out anything out of it whereas after
training this is something which i get it
it comes down as if a consolidated blob not
necessarily so noisy its its a blob which
comes out it looks like a mixture of maybe
seven three nine eight a lot of things and
some decent probability around being a seven
so provided that you are training it for really
longer period of time or you you are using
some different kind of ah training method
instead of the gradient descent you use some
some of the other optimization techniques
which i i deleted on point of time when we
would be using we study about them you will
be knowing that using some other techniques
it might come down to a better point but as
of now lets ah freeze at this point so this
comes down one where you have achieved some
sort of ah fidelity in in the whole reconstruction
now i would like to look into my visualization
of these weights
now you remember that what we ah had done
earlier ah somewhere over here is we had actually
copied down all the weights ah while the network
was defined and these are all the random initializations
which which my network gets randomly initialized
ok so i actually bring those weights over
here okay one is i can have my trained weights
which i can extract actually after the training
over there and then i have my initialized
ah initialization weights which were before
ah the whole process were started that is
what is given down over there
now when i write down this capital e that
is actually the weights of the encoder when
i write down ah those as ah d then thats thats
weights on the decoder which comes down so
here lets look into these ah initial weights
so now this is what it was like at the start
of the whole problem and you remember that
you had twenty eight cross twenty eight which
were connected down to four hundred neurons
over there so technically that means that
i have twenty eight cross twenty eight or
seven hun[dred] and seven eighty four weights
which connect down to one single neuron and
this is that twenty eight cross twenty eight
weight matrix which you see and and typically
you like if you look across the spacings
so you have such ah twenty cross twenty tiles
and you can just count it one two three four
five six seven eight nine ten eleven twelve
thirteen fourteen fifteen sixteen seventeen
eighteen nineteen twenty twenty such ones
and on this side also you have twenty over
there so in in total that should be ah four
hundred such small tiles present over there
now once the whole training is process training
process is over after ah twenty iterations
over there this is what these weights start
looking like and and
one thing what you can see is that ah they
have changed not necessarily that they have
come down to a convergent point or something
but there has been subtle changes coming down
and some of them look like as if they are
taking structures like zero one seven something
over here something rough so provided you
train them over a really long period of time
maybe set it down for two hundred epochs and
and go and have your cup of coffee tea or
ah finish off your dinner lunch whatever you
are doing and then come back in ah thirty
minutes or so in most of your systems it would
be trained on for two hundred epochs by then
so deep learning doesnt take that much of
time as a lot of people are scared about it
so then you would be able to actually see
down ah the changes which come down the next
part what we looked into is what is the actual
way of weights updates which have happened
now this is a very crucial point because what
we have done over here is just subtracted
the original random weights from the current
version of the weights are after training
and these indicates those particular locations
where there has been changes now if you look
into this changes over here these changes
are something where you would be able to see
down numbers
now typically most of the updates have been
in the center region and thats quite obvious
because whatever images of these numbers hundred
and digits you had they had most of these
ah actual line pandas which were at the center
point they were not located at corner places
or anywhere and that was the reason why you
did not have any updates coming down in the
corner but most of the updates were from the
center region also you see that ah they are
somehow congruent to the numbers where it
was trying to come down and this is the first
belief that if you are training it down over
a longer period of time so this would keep
on converging over and over again this would
be the ah sort of a ah guidance principle
of how the weights are getting updated and
eventually the final thing would start looking
something like this where each of these bits
are tuned down to a particular number
so this is what i leave on to you guys to
ah do out of your ah own interest so this
is about how all of these things are connected
from my encoder to decoder this is one part
of it the next part is to look into the weight
visualization for my decoder and thats where
my this capital d comes from so all my weights
from the decoder so initially i had already
stored down my weights from the decoder so
that was something if we go up over here ah
when i had defined it so i have my weights
of the decoder as well which i had stored
it down and then after my whole training is
over i again pull out those decoder weights
from my network ok
now my whole objective is to repeat the same
thing and then start looking into the decoder
weights as well now this is something which
is present on the initial part of it and clearly
ah if you look into ah these patches so i
have seven eighty four neurons which were
connected to four hundred neurons in my encoder
now i have four hundred neurons in my encoder
as as output of my encoder which i connected
down to seven eighty four neurons on my decoder
ok
so now what i would have is that there is
a small matrix of twenty cross twenty or four
hundred bits so thats this small matrix over
here and then since i am connecting it to
seven eighty four so that should be seven
hundred and eighty four such small patches
or matrices of weights so it is a twenty eight
cross twenty eight array which should be there
lets lets just count it one two three four
five six seven eight nine ten eleven twelve
thirteen fourteen fifteen sixteen seventeen
eighteen nineteen twenty twenty one twenty
two twenty three twenty four twenty five twenty
six twenty seven twenty eight and countdown
on the vertical direction you would see such
twenty eight tiles again okay
now over here is where i see my weights which
are after my training now it doesnt look much
of a difference technically saying like most
of you would say its its as noisy as it was
in the earlier case as well yes its noisy
but then the intensity of the noise has somewhat
decreased one thing you need to keep in mind
is you remember that ah these gray values
what we had seen in the earlier cases also
i was explaining that they are what are zero
values and all the blacks are negative values
all the whites are positive values which are
other and this is normalized into its dynamic
range and thats why
we dont have that this is just for visualization
purpose you cannot technically infer out on
the values just looking over here and then
if you look into these weight updates over
there you you dont necessarily see to i tend
to see any patterns there are some wavy things
and and some really discreet way however there
is one one interesting point which you need
to look into over here here when you see in
this one most of the values were either highly
negative and highly positive here a majority
of these values tend to become down as zeros
and thats the point of this l one sparsity
which comes down
because now that you have all of these gray
values which you see they are zero value numbers
which comes from and this is something which
gets imposed from here while you had just
high positive and high negative values to
hear when majority of the values are actually
zero valued on the weight matrix over there
so this whole thing comes down because you
have an l one penalty you had imposed over
there and that was possible because a lot
of these weights you see over here they look
similar to each other say this weight looks
very similar to this weight it looks pretty
similar to this weight it looks pretty similar
this weight it looks pretty similar to these
weights now the moment is you will be able
to get down whereas this way it looks very
dissimilar okay
so you will have very less number of such
unique weights which are available and then
ah since whatever you are doing over here
is just a combination of weighted summation
of the outputs coming down from one of these
layers now most of these neurons they would
ah actually have a similar kind of an output
which comes down and you can pretty much do
away with ah most of the neurons and keep
done one or two neurons over there and and
still get an output coming down and that was
the reason the whole rationale why over here
do you see a lot of these bits actually go
down to a zero value and thats really helpful
in in case of dealing with sparsity
now ah you would see that ah the weight updates
which happen over here they are also quite
near to zero value but there are certain high
and low changes coming down because you had
to bring down those high positive and high
negative values on to quite closer down to
actual zero values coming out so that was
ah all about these visualizations and finally
what we do over here is the simple old trick
which was i have trained down my whole encoder
with ah l one sparsity over there and now
i would like to modify this autoencoder in
order to form a classifier so what i need
to do is i necessarily need to remove out
my last decoder layer and then add my classifier
module which connects from four hundred neurons
to ten neurons
and then i can start my ah classification
trainer and this classification trainer over
here trains over this standard ten iterations
ah using negative log likelihood ah loss function
as we had done in the earlier case so lets
just ah set this ah running over here okay
now as i see over here you would see that
it starts with a ah starting accuracy of seventy
eight percent now if this is if you remember
it from the earlier one this is really high
those earlier case we started somewhere around
sixty six percent on the ah starting accuracy
whereas here we have started down with a starting
accuracy itself of seventy eight percent now
one question you might ask is why is it so
why did we start with a higher accuracy
one thing you need to keep in mind is that
these kind of networks the moment we started
introducing this sparsity ah there was one
thing in order for the network to be sparse
you need the earlier part before sparsity
to be over complete otherwise the network
cannot be sparse now the moment you are having
an over complete representation in the earlier
case you see that you tend to have similar
ah representations grouped down so you are
now coming down to dominant group of representations
and that means that ah the noisy representations
are getting ruled out in the whole process
so as we keep on getting noisy representations
ruled out which means that features which
are really irrelevant to this particular problem
of classification they dont come into play
and for that reason we start with a very higher
accuracy however ah the final accuracy is
almost of the same plot it it doesnt change
because in the earlier case you had about
ninety two percent we trained it over and
over for twenty epochs or you went down to
ninety two percent and and ah here you might
not necessarily ah get done but you are starting
estimate in this case is a much better estimate
than you have in the earlier case without
having the sparsity
so if we look into ah each of these classes
and ah how much they come down you still see
that for class five ah in the earlier case
it was about fifty five percent accurate eighty
five percent accurate and here we are barely
eighty four point seven which is almost the
same amount of accuracy there hasnt been much
of a change however the features which it
has been learning over here are ah something
which are more congruent to ah ah be representative
of the numbers which are handwritten so with
this ah we come to an end of ah lecture nineteen
on ah our whole ah aspect of trying to train
down an actual autoencoder with ah sparse
sparsity included over there
and with this concept we have shown you for
the first time ah about how to actually start
defining your own kind of layers as well in
including if you want to just have some customized
cost functions coming down so in the next
lecture we would be covering down ah on top
of this one which is to come down with the
next aspect of a denoising autoencoder and
and how does a denoising autoencoder ah help
you to learn better weights as well as ah
really clean down the noise ah from any of
these results so for then ah stay tuned and
thanks