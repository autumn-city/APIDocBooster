"Then we will go on here again, it will receive receive the  hidden state and the cell state from the previous time step, the current time step input, output something and so forth. So that's how the LSTM cell class works."""
Then we have an LSTM cell that takes in the previous hidden state and the new observation and action and spits out a new hidden state.
"So in basic sense, these are generally comprised of bi-directional LSTM cells or bi-directional. So generally we have the X as the input features and we have the hidden state from the previous timestamp of the previous cell."
"An LSTM cell consists of three gates-- the input, forget, and output, as well as a cell state."
The cell state is like a conveyor belt.
"Now, the LSTM cell class is, it's kind of like  part of the LSTM, it's a smaller unit, like LSTM cell is, is  only a small unit. And we can actually use both on either the  LSTM or the LSTM cell for implementing the character RNN."
LSTMCell is an object (which happens to be a layer too) used by the LSTM layer that contains the calculation logic for one step.
A recurrent layer contains a cell object.
"The cell contains the core code for the calculations of each step, while the recurrent layer commands the cell and performs the actual recurrent calculations."
"LSTM is a recurrent layer.
 An LSTM layer is a RNN layer using an LSTMCell, as you can check out in the [source code (hyper-link)]."
"Alghout it seems, because of its name, that LSTMCell is a single cell, it is actually an object that manages all the units/cells as we may think."
LSTMCell is the basic building block of an LSTM network.
"Basically you want to use one LSTMCell for each layer, and you should be careful on how to go from input to output, layer by layer taking into account the hidden states."