Then add Activation(custom_elu) to your model.
Generally you want to use both BN and RELU, no matter what is your network size.
The elu function is in the tf.nn module, see the [documentation (hyper-link)].
I'm not an expert in TensorFlow, but I guess the reason why the built-in ELU activation (which calls tf.nn.elu) works fine is because tf.nn.elu has its own gradient op.
You can just specify .activation(new ActivationELU(myAlpha)) in the layer builder.
When keras receives tf.keras.layers.Dense(10, activation='elu') it will go into the activation function module and will literally call activation function by its name if it is present there.
So, activation='elu' will be converted into  tf.keras.activations.elu().
You should remove some of your regularization, use only ReLu, use less layers, use the standard adam optimizer, a larger batch, etc.
Neither ReLU nor ELU have learnable parameters, but they still require compute to execute.
ELU executes the exponential function on all x >= 0.
ReLU is a computational cheap because of the ease of the operation x[x < 0] = 0 and so you don't really see a spike in time.
This is one of the reasons why ReLU is a common choice as activation function.
Looks like you're facing the problem of exploding gradients with ReLu activation function (that what NaN means -- very big activations).
The preferred initialization strategy for the ReLU activation function (and its variants, including ELU) is He initialization.
If you register selu with it's custom name, you would be able to use it for your runtime.
In fact, most of the most succesfull DL techniques (based on sigmoids, tanhs and relus) are almost linear, and so the numerical instabilities come mostly from exp operations in probability estimates.
conv1 = Conv2D(16, (3, 3), activation='elu', padding='same', name='convl1e')(image)
conv.add(Convolution1D(filters=32, kernel_size=3, activation='elu', use_bias=True, input_shape=(None,16))).
Elu or linear activations allow negative values, which will cause infinite loss, when using cross-entropy.
As for the selu activation, you need to reshape the input to (n_samples, n_output):
The ELU activation won't change the order of magnitude and hence the input to the last layer is still on the order of magnitude 1.
For example, in your current design,  1) Conv2D layers without any activation are very rare, and you may want to use relu or tanh, and 2) ELU(alpha) is known to be sensitive to the alpha value.
The lambda_1 level outputs a single RGB image per test image, convolution2d_1 and elu_1 output sixteen smaller (25%) greyscale images - one for each filter.
I'm actually not sure that your activation is much better than [tflearn.activations.leaky_relu (hyper-link)], but if you really want to provide a custom activation, you'll have to code the gradient and register it like described above.
For example, you can define melu as follows:
There is a problem that might happen with "relu" when you have learning rates too big.
Since I'm not an expert on adjusting the parameters in detail for using "relu", and my results with "relu" are always bad, I prefer using "sigmoid" or "tanh".
