Let's now turn to learning the 
parameters for logistic regression.  
We'll start with the cross entropy loss function.
Logistic regression is an instance 
of supervised classification  
in which we know the correct label y 
either zero or one for each observation X.
But what the system produces is 
an estimate. We'll call it Y hat.
What we want to do is learn parameters,  
meaning W and B that make Y hat for each training 
observation as close as possible to the true Y.
So we'll need two things for this. 
We'll need a distance estimate, a  
loss function or a cost function,
and we'll need an optimization algorithm to 
update W and B to minimize this loss function.
So that's two components, a metric 
for how close the current label y hat  
is to the true gold label y.
And rather than measure similarity, we 
usually talk about the opposite of this,  
the distance between the system 
output and the gold output.
And we call this distance the loss 
function or sometimes the cost function.
We'll introduce the lost function that 
is commonly used for logistic regression  
and also for neural networks.
That's the cross entropy loss.
And the second thing we need 
is an optimization algorithm  
for iteratively updating the weight 
so as to minimize this lost function.
And the standard algorithm 
for that is gradient descent.  
And we'll introduce the stochastic gradient 
descent algorithm in the following lecture.
So we need a loss function that expresses for an 
observation X how close the classifier output.
That's y-hat which we get by running 
W, X plus B through a sigmoid.
How close that classifier output is to the 
true output, which is either zero or one.
So how close is Y hat to Y. And we'll 
call the difference between these  
the loss L: how much y hat 
differs from the true Y.
And we do this via a loss 
function that prefers the correct  
class labels of the training 
examples to be more likely.
This is called conditional 
maximum likelihood estimation.
We choose the parameters W and B 
that maximize the log probability  
of the true Y labels in the training data,
given the observation X. And this resulting loss 
function is the negative log likelihood loss  
generally called the cross entropy loss.
Let's derive this lost function 
applied to a single observation x.
We'd like to learn weights that maximize 
the probability of the correct label Y.
So that's P of Y given X. Now, since there 
are only two discrete outcomes, zero or one,
we can express the probability P of Y given X from 
our classifier as the product of these two terms,
Y had to the Y and one minus 
Y hat to the one minus Y.
Now what happens if we plug in the true values.
Y equals one or Y equals zero to this 
equation. If Y equals one we get.
Y hat to the one one minus Y hat 
to the zero, and this cancels out  
and we get just Y hat, that's if Y equals one.
If the true Y equals zero, we get Y hat to 
the zero times one minus Y hat to the one.
And now this one cancels out. In other words, if 
Y equals one, this equation simplifies to y hat.
If Y equals zero. This 
simplifies to one minus Y hat.
So our goal is to maximize this probability.
Our job is to learn parameters 
that will make the correct label  
y the most likely have the highest probability.
And for mathematical convenience, we'll 
take the log of both sides. So we went.
Now we want to maximize the 
log probability of Y given X.
And so now we're we're maximizing Y 
log Y hat plus one minus Y log what.
One minus Y hat. We can do 
this because whatever values  
maximize the log of the probability 
will also maximize the probability.
So here I've just put the same equation up here. 
Our goal is to maximize log P of Y given X.
And instead of maximizing a probability, it's 
more common to talk about minimizing a loss.
So we're gonna turn this thing that we're trying  
to maximize into something 
we're trying to minimize.
And that will turn it into the 
cross entropy loss. So all we do is  
take this log P of Y given X and and we negate it.
So the cross entropy loss. 
We're trying to minimize  
the negative log probability of Y given X.
And as we've seen, this can be 
estimated with this sum of terms.
We can also plug in our definition of y had to 
remind ourselves how we're going to compute this.  
And the across entropy loss
again, between Y hat and Y, is the 
negative of Y log sigma of W X plus B  
plus one minus Y log of 
one minus Sigma W X plus B.
All right. Let's see if this cross entropy loss 
works in our sentiment example intuitively.
We'd like the loss to be smaller. If 
the model estimate is close to correct  
and bigger if the model's confused.
So let's look at both those cases and see 
if the estimate is doing the right thing.
So let's first take our sentiment example. I" 
t's hokey there are virtually no surprises.
So why was it so enjoyable?..." Let's suppose the 
true label of this example is one Y equals one.
This is a positive review, let's say, and 
let's see if that works with our features.
What probability does our model 
assign to the value Y equals one?
Well, we can compute that just from the 
sigmoid, sigma of W, X plus B, which is again,
we dot product our weights with our 
feature values, our Ws and our Xs.
And we add our bias term B and we 
end up with a probability of .7
So pretty good. What's the loss? 
Well, we can just plug in that .7
into our equation for cross entropy loss. And 
we see a loss of .36, a relatively small loss.
By contrast, let's pretend instead 
that the example was actually negative.
That is the true why was zero instead of one.
So perhaps the reviewer said, bottom line, 
this movie's terrible, I beg you not to see it.
So in this case, our model, which 
assigned a pretty high probability  
to this being a positive example, 
is confused the models wrong.
So we'd hope that the loss will be 
higher. So let's plug in Y equals zero.
So what's the probability of Y equals 
zero? And that probability is one minus  
the probability of Y equals one, 
which we got from Sigma W, X, plus B.
And that's point three. So the model doesn't think  
that the that the answer is zero, 
doesn't think this is a negative review.
Cited a small probability of only 
.3. And if we look at the Loss
Plugging in that point three now into, um,
now that y is zero - the true 
y we were estimating is zero.
So we can this all cancels out and 
now we get a negative log of .3,
we get a higher loss 1.2 than the much 
smaller loss we saw on the previous slide.
When the model was right, we had a 
small loss when the model was wrong.
We had a big loss. So sure enough, a cross 
entropy loss is doing the right thing.
The loss is bigger when the models wrong.  
We've derived the cross entropy loss and 
seeing how it applies to our sentiment example,
cross entropy loss is equally important for 
neural networks, as we'll see in future lectures.