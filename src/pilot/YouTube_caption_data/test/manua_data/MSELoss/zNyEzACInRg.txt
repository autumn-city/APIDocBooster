 All right, let's now take a look at some code examples. But before
 that, I have a few slides I wanted to go through, just to
 summarize, yeah, what we are going to take a look at in the
 code. But then also to add a few things about multi layer
 perceptrons that I didn't mention in the previous slides.
 So I prepared two code notebooks, and I will also show
 you some Python scripts as alternative. So in these code
 notebooks, I implemented a multi layer perceptron with a
 sigmoid activation and a mean squared error loss, and the
 multi layer perceptron, the same one, but with softmax activation
 and cross entropy loss. So there are, let's take a look at the
 left hand side. There are two plots here. So one is showing
 the loss for each mini batch. So you can see it goes down. That's
 what you want to see the trend goes down. While we are training
 it, it's a bit noisy, this is expected because it's on
 stochastic gradient descent. So these updates are noisy. At the
 bottom, I'm plotting the loss computed of like computing over
 it over the whole training set. So after each training epoch,
 I'm computing the loss of the whole training set. So this is
 why it's smoother here. So you can see I'm training for 100
 epochs. So x axis is the number of epochs. Actually, I think I
 think I've probably wrote epoxy, I can't see always the bottom of
 the slide, because there is like, for the pen, the color
 bar, but I think that it says epoch. Anyways, so you can see
 this is training nicely. But the accuracy is quite low, it only
 reaches 90% training accuracy, 91% test accuracy. One good
 thing about it is it's not overfitting. But if you recall
 from the softmax regression lecture, we already achieved 92%
 something using just softmax regression. So um, yeah, the
 reason why I think this multi layer perceptron is not working
 well, my suspicion is it's because the sigmoid activation
 and the MSE loss are not a good combination. Like I outlined
 earlier, when because the terms don't cancel so nicely compared
 to softmax and cross entropy, or even sigmoid and cross entropy,
 what happens is that we have the derivative of the sigmoid
 function in the chain rule. And this will be a number smaller
 than one, which can be a little problematic. Because yeah, you
 may have like these small numbers multiplying things with
 a small number, and then you have a vanishing gradient
 problem. So you make the gradient very small, and then
 maybe you don't update, that could be one problem, I can see
 with that. If you use the cross entropy loss, um, it's a little
 bit less of an issue. So cross entropy loss is usually better
 with sigmoid activation, or can also use a softmax activation,
 which I would recommend, because we have, yeah, a mutually
 exclusive classes. So like we talked about, why not also using
 softmax activation, you can play around, actually, you can take
 this good example and change softmax to sigmoid, you will see
 there's not a big difference, though. But on here, here on the
 right hand side, um, you can see though, what the main takeaway
 is, that, again, the loss goes down, that's what you want to
 see. But what you can see is also the training accuracy is
 99%. And the test accuracy 90, almost 98%. There's a little
 bit of overfitting, we will talk about overfitting after the
 Jupyter Notebook video in the next video. But yeah, what you
 can see is the cross entropy loss actually works much better
 here than the MSC loss. Um, yeah, a few more things I
 already talked about dead neurons. So we can also use the
 relu activation function, which is very popular, you can
 actually try it on in the code notebooks. It's, like I said,
 probably the most popular activation function, I use it
 all the time for most of my stuff, because it works well
 usually. However, theoretically, there's this problem that it
 might have this dead neuron problem, like the relus might
 die during training, that happens if the input is too
 large or too small, so that the net input is yes, negative. So
 if you have a large input and a negative weight, then yeah, the
 net input will be negative. Or if you have a positive weight,
 and are very small input, very negative input, then also the
 net input will be negative. And if it's so negative, that it's
 extremely negative, then maybe it will never be possible to
 escape this problem. Because even though you have the
 multivariable chain rule, you combine certain things, you may
 never be able to reach this threshold have a positive
 number. So you will never maybe update a certain weight, and
 then you end up with this dead neuron. However, like I said,
 it's maybe not necessarily bad, because if you have a lot of
 neurons, your network has a tendency to overfit, then yeah,
 removing some neurons removes some parameters, and it can
 maybe help actually achieving a better performance because it's
 kind of simplifying the network like pruning excessive weights.
 Also, one advantage of value compared to let's say, the
 sigmoid or 10 h function is that it suffers less from this
 vanishing gradient problem that I explained on the previous
 slide, because the gradient is either zero or one. So you
 either have the dead neuron, in the worst case, or no gradient
 in the worst case. But in the other case, if you have a
 positive net input, you always have a strong gradient of one,
 but it could technically leads to exploding gradient problems
 if you have some values that are greater than one from other
 parts of the network. But yeah, I will talk more about vanishing
 and exploding gradients when we talk about that would be a good
 point for recurrent neural networks. We will talk about
 that when we talk about recurrent neural networks later.
 But yeah, in general, just wanted to summarize these few
 points.
 Another thing is, shall we use a wide or deep multilayer
 perceptron? So if we have to choose, which one would be
 better. So let's say we could make a multilayer perceptron,
 when we only have a small number of units in each layer, let's
 say, five in each, but have a lot of layers, or we could make
 a very wide network where we have, let's say, only one hidden
 layer. But then let's say we have 100 units in that layer. So
 what is preferable? I mean, theoretically, there has been
 some work on the universal approximation theory or theorem.
 In that case, I think it's a theorem, even I haven't actually
 read these papers, I only know they exist. So there are
 theorems showing that multilayer perceptron with one arbitrarily
 large hidden layer can already approximate arbitrary functions.
 So that case, if a multilayer perceptron with only one very,
 very big hidden layer can already do that, why would we
 even care about having multiple layers? I mean, the ability to
 approximate arbitrary function doesn't mean it's practical to
 train such a network. So first of all, there are some
 challenges with training, also just large matrix
 multiplications. But then also, you need more parameters really,
 to have the same expressiveness as using more layers, with fewer
 number of parameters, if you have fewer parameter parameters,
 but more layers, you have more combinatorially more
 combinations possible. So this way, you have the same
 same expressiveness as with multilayer perceptron with only
 few large hidden layers. But then if you have a lot of these
 layers, then you may suffer from vanishing and exploding gradients
 that I just mentioned on the previous slide. So usually,
 there's a trade off in practice. For multilayer perceptrons, you
 never really go deeper than one or two layers, because then you
 have like these vanishing, exploding gradient problems. But
 later, we will talk about convolutional networks, and other
 types of networks where we can actually go deeper, without
 having these exploding and vanishing gradient problems. So
 there are some tricks. And this is basically what deep learning
 is about. It is about designing things cleverly, so that we can
 go deeper without having vanishing, exploding gradient
 problems for multilayer perceptrons, because it's not
 really a deep learning architecture. One or two hidden
 layers is enough, you will notice if you try it, to
 implement a multilayer perceptron with three or four
 layers, you will usually notice it doesn't train so well anymore,
 because the arrows don't back propagate so far anymore. So a
 practical consideration is usually using one or two hidden
 layers in a multilayer perceptron. If we talk about
 convolutional layers, we can go up to 500 or maybe 500. But
 let's say 150 200. That's even very common these days. But
 convolution networks are topic for another lecture. So yeah,
 like I said, just to summarize again, we can achieve the same
 expressiveness with more layers, but fewer parameters compared to
 having like one large layer with a lot of parameters. But also,
 what's nice about having more layers than compared to one is
 that we have some form of regularization, because later
 layers are constrained on the behavior of earlier layers. So
 we have also, in that way, having more layers can also,
 sometimes in theory, at least be helpful. But then, like I said,
 we have the vanishing exporting greater problems. And later, we
 will talk more about that. So now it's time that I show you
 the code example. So let me pause this video and then start
 my Jupyter Notebook.