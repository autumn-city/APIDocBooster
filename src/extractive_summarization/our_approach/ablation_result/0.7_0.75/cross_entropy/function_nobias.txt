The cross here refers to calculating the entropy between two or more features / true labels (like 0, 1). 
Correct, cross-entropy describes the loss between two probability distributions.  It is one of many possible loss functions.
Cross entropy is one out of many possible loss functions (another popular one is SVM hinge loss).  These loss functions are typically written as J(theta) and can be used within gradient descent, which is an iterative algorithm to move the parameters (or coefficients) towards the optimum values. In the equation below, you would replace J(theta) with H(p, q). But note that you need to compute the derivative of H(p, q) with respect to the parameters first.
 When we compute the cross entropy,  because we use the mathematical formulas, we compute first the  softmax.  And then from the softmax, we compute the cross  entropy in pytorch, they do all that work for us inside this  function, they do it for us. 
 So yeah, the  negative log likelihood and the binary cross entropy are  equivalent.  And in practice in deep learning, people just say  cross entropy, multi category cross entropy, which would be a  multi class version of the negative log likelihood, which  we will cover later in this lecture when we talk about the  softmax function.  So just to keep it brief, the negative log  likelihood that we just covered a few videos ago, is the same as  what people call the binary cross entropy, they were just  formulated in different contexts.  So negative log  likelihood comes more like from, I think it's like, it's probably  from a statistics context, I don't know the first paper, or  reference that mentioned that.  But this is something usually I  see in statistics papers, and the binary cross entropy thing  has originated from the field of information theory, or computer  science.  So we have actually seen that, or not, the cross  entropy, where we have seen the self entropy, or just entropy,  and statistics 451. 
