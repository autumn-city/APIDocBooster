This case can arise in the case of the second MultiHeadAttention() attention layer in the Decoder.
This will be different as the input of K(key) and V(value) to this layer will come from the Encoder() while the Q(query) will come from the first MultiHeadAttention() layer of Decoder.
The [MultiheadAttention (hyper-link)] is instantiated with d_model, nhead equal to those values and [k_dim, v_dim are left to the default value of None (hyper-link)].
You may also parallelize the attention layer (MultiHeadAttention) and configure each layer as explained above.
For more details, look at the source code : [https://pytorch.org/docs/master/_modules/torch/nn/modules/activation.html#MultiheadAttention (hyper-link)]
Specially this class : class MultiheadAttention(Module):
Does the Tensorflow/Keras MultiHeadAttention layer actually already contain an Embeeding layer that takes care of the positional encoding?
