 And this is  essentially it's in that way, step one of batch norm is  similar to the input standardization. 
For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.  However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].
BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice. 
 And yeah, one little fun memory aid to remember that  is, if you consider this case, so you have batch norm, then  you have the activation and then you have dropout, you may call  it bad, it might be better to have batch norm after the  activation, that's typically a little bit more common. 
 The same with batch  norms, instead of using batch norm, one D, which we used  earlier, when we talked about multi layer perceptrons of fully  connected layers, for the convolution layers, we use batch  norm 2d shown here. 
