welcome to dw/bi add a channel please
subscribe for latest training videos
hello welcome to pie torched tutorial
series today we shall discuss about lsdm
that is long short-term memory
LS tiem is a variant of recurrent neural
network recurrent neural network learns
feature from in time from data set we
call it temporal future learning but
connecting various neural networks in a
series and feeding each of them input
data is inefficient instead we can
modify our structure and we can have
better results one of the biggest
drawbacks of recurrent neural networks
is vanishing and exploring gradients
that takes place during back propagation
over time your series of neural networks
can be very huge and
have to do backpropagation overtime
you have to back propagate in each of
the neural networks and when you reach
from the last time stamp to first time
stamp in time your gradients might tend
to vanish or explode
that is beyond the calculation limit of
a computer also this back propagation
over time is a costly process that takes
lots of computation which are very
complicated to reduce this
a tional overhead and remove this
problem of exploding and vanishing
gradient lsdm that is long short term
memory was introduced you can think of
it as how we remember important things
in our lives for instance our credit
card numbers our mobile numbers etc and
we also choose to forget various
unimportant things for instance what we
had for dinner one month ago or
what we did a year back on this day
these are unimportant information and we
tend to forget it because we don't need
it so why can't a neural network do the
same thing remember 10h function is a
very powerful function and can h
activation function has a range from
minus 1 to plus 1
the zero to minus one part of damage
function is used in lsdm or recurrent
neural network to forget the memory and
the positive half of the carriage
function is used to retain a memory and
like this way the neural network decides
which data to remember and which data to
forget after various experimentation it
has been found that it is very helpful
so let's jump in and write our first
code in my talk about lsdm to do that
as usual you have to import torch torch
at an intelligent torch vision dot
transforms torch vision dot data sets
and especially m nest so this is the
structure of an LST M model it looks
very complicated and the figure has been
taken from this link this is a very good
link written this is a blog written by
XI and so once you would read these
labels you will understand what's going
on so here we have the input HT minus
one and we have two outputs H T there is
one input vector X T so these C's are
memories from previous block and HT
minus one is output from the previous
block as we saw in the simple rln the
memory block from current block is in
this one
and we have lots of sigmoid function in
this part and we have tan H function to
retain on forget memory like I said
sometimes we want to retain a memory
sometimes we want to forget a memory and
this cross and plus are basically
addition and multiplication in
element-wise vectors in pi dot so this
is just one cell of lsdm and like this
we set up a lot of LST models which is
very efficient we call CUDA we download
our data set and number of classes is as
usual ten that size is hundred we are
running to epochs only learning rate is
0.01 train loader and test loader as
usual we did in the previous videos in
the RNN module there is there will be
some slight changes in the init function
we are calling input size hidden size
number of layers and number of classes
we define our hidden layer hidden size
number of layers and instead of n n dot
R n n we are calling n n dot LS tiem and
that takes care of everything don't
forget to make the batch first flag true
and in this case you don't have to add
linear non-linearity because it will
automatically take care of this sigmoids
and 10h activation functions in the
forward pass I include the input from
the previous block for zeroth instance
that is a bunch of zeros and we also
have to include this C 0 that is memory
from the previous block so now we have
two parameters one is output from the
previous block
and the other one is memory from the
previous block and we'll get an output
from the current block and a memory from
the current block because we don't want
to retain 100% of our output we want
either partial or sometimes full
depending on the context and finally we
print our output so the sequence length
is 28 as we saw in the previous video I
explained why is it like that the input
size is 28 number of layers is 2 as we
saw in the as we saw here the number of
layers hidden size input size goes into
this RNN module we instantiate our
remand model the
loss function is cross entropolis like I
have mentioned multiple times optimizer
is Adam you can change it to rmsprop and
it will run as good as it will run now
and this is the training block
and here we have total number of steps
equal to the length of the train loader
which is basically 600 and we're on our
loop for two epochs and each one loads
the input data and to the same things we
did in the previous videos we took
images we took labels we in the forward
pass we predicted in the forward pass we
predicted the outputs from the model
then we calculated our loss function and
then we did optimize a dot 0 underscore
graph that removes all the gradients
then we calculated the backward
propagations / loss and finally optimize
the dot step updates the weights and
biases and one thing to remember here is
that this all these things is been done
on each of the celestial modules so this
is back propagation over time so model
is ready after our training and the
final loss is zero point zero two eight
five two let's see how much accuracy it
gives so it gave ninety seven point four
three percent accuracy on ten thousand
test images which is really really high
thanks for watching