This is the implementation of BatchNorm2d in pytorch ([source1 (hyper-link)], [source2 (hyper-link)]).
The outputs of nn.BatchNorm2d(2)(a) and MyBatchNorm2d(2)(a) are same.
You have a problem with the batch norm layer inside your self.classifier sub network: While your self.features sub network is fully convolutional and required BatchNorm2d, the self.classifier sub network is a fully-connected multi-layer perceptron (MLP) network and is 1D in nature.
Try replacing the BatchNorm2d in self.classifier with BatchNorm1d.
From the documentation of [nn.BatchNorm2d (hyper-link)]:
Existing layers you add to your model (such as [torch.nn.Linear (hyper-link)], [torch.nn.Conv2d (hyper-link)], [torch.nn.BatchNorm2d (hyper-link)]...) all based on [torch.nn.Module class (hyper-link)].
Or set the nn.BatchNorm2d parameters yourself and apply .eval on the whole wrapper to lock the batch norm layers and the UNet for evaluation.
THANK YOU @MichaelJungo turns out you were right in that one of my BatchNorm2d wasn't being set to eval mode.
Hence, that is the reason why this rebuild batchnorm method could be called FrozenBatchnorm2d.
