now bernoulli tries to explain the
concept of maximum likelihood estimation
to agatha
as you can see maximum electrode
destination is a bit more involved
compared to the traditional machine
learning algorithm so it has a variety
of concepts that are interconnected so
that's something we are going to see how
we connect all the dots like what is
likelihood we are going to understand
how that
connects to bernoulli distribution which
is the reason why we have chosen jacob
bernoulli to explain this algorithm and
then we look at what is log likelihood
and then we'll see what how
log likelihood is connected to cross
entropy and which is then related to
chain rule which is then related to
gradient descent
right to quickly recap what has happened
so far
we are going to see as you can see
agatha reached out to benoli to kind of
understand
how machine learning can help her
grandson
figure out what is the
best gmat score that he can aim for in
order to get admission into carnegie
mellon temporary university for in its
mba program
right and then we're going to we also
already seen what exactly is logistic
regression how exactly first regression
will work the kind of data set i got the
brought to the table so we're going to
quickly recap what those are and we're
going to pick it up once
bernoulli came up with the equation
which explains the coefficients and
we're going to in this discussion we're
going to see exactly how those
coefficients were obtained in using all
these techniques
where maximum electrode estimation plays
a significant role
right now if you remember this is the
problem that we had so we had the gmat
score as input variable and we are
trying to figure out given the gmat
score of a candidate we have the output
which is admission status yes or no
accepted or rejected
right and then what uh agatha did was
she collected this data and she had this
data of gmat's course and whether the
particular student got admitted or
rejected and benolli said he can come up
with an algorithm because regression
that can
figure out what is the cutoff score that
is required to have 95 percent chance of
getting into the university 90 chance 50
percent 60 chance that's what we have
discussed in the earlier part of the
episode right and then
when i got asked is this related linear
equation bandarana says somewhat
it is a classification algorithm it's
not exactly trying to predict the output
variable which is continuous square in
the case of linear equation here we're
dealing with a different type of output
variable and bernoulli said this is the
equation minus 6.7 plus 0.012 into gmat
score and this is where we stopped
now agatha has an interesting question
right she was trying to understand how
did
those coefficients come about how did we
get minus six point seven and zero point
zero one that's what i got is asking how
does the training happen philosophy
equation is this
similar to linear regression how exactly
does it learn the coefficient of this
bernoulli said
it used a technique called maximum
likelihood estimation that finds the
coefficients of input variables that
ensure the maximum likelihood of seeing
the probability values in the output
variable now as you can see we have
introduced a term called likelihood
and agatha wants to understand that what
is this likelihood is it the same
as probability most of us think it is
the same likelihood probability english
way you see that most both of them kind
of are related to the same thing right
but bernalillo says not exactly there is
subtle difference between probability
and likelihood function as you can see
from the figure here probability values
can be between 0 and 1. likelihood
values however did not be did not sum up
to one
right so probability of you have
successful events answers for events
they sum up to one in the case of
probability in the case of likelihood
likelihood of different possible events
need not sum up to one
right and as you can see in the case of
probability you are always looking at
under a particular condition where the
condition is fixed we are looking at
probability of event happening right
like say for example given that the
we have a fair coin what is the
probability of seeing a head it is 0.5
given that it's already a fair coin
right now in the case of likelihood we
are going to reverse the question that
the event remains fixed event has
already happened now given that the
event has happened what is the
probability of seeing that event what is
that condition that should be given that
we have seen a sequence of coin tosses
is that coin a fair coin or not as you
can see we flipped the coin or the
question
as you can see here right and that's
what bernoulli says let us take an
example let us say i flipped a fair coin
five times what is the likelihood of
seeing this sequence the first two
tosses
were heads next two tosses were tails
and the fifth and final toss is ahead
what is the likelihood of seeing this
sequence as most of you must be thinking
it should be three by five zero point
six right so that's exactly what i got
but she also clarified do you mean the
probability of seeing a head and she
said that would be 0.6
problem solved easy peasy right but then
when only says no
i do not care about the problem of
seeing a single head that's not my
question my question is
what is the probability of this exact
sequence appearing there might be n
number of sequences that might come up
it can be when you throw a coin five
times it can be all heads it can be all
tails it can be three heads two tails
two heads three tails four heads one
tail one head four tails like that you
have so many combinations in fact you
have 32 combinations right out of all
the solid combinations what is the
likelihood what is the probability of
this exact sequence
happening
right so probability of a sequence
happening is nothing but likelihood of
that sequence happening right so then i
got to ask okay how do we calculate that
so which bernoulli says
as we just discussed there are 32
possible combinations that can show up
when you throw a coin five times out of
those 32 the probability of saying the
sequence is 1 by 32 and how do we
calculate that as you can see because
each of these tosses are independent you
can see that we are looking at one by
two which is the probability of let us
say if you calculate for this sequence
right or probability of seeing a head
which is one by two again one by two
probability single tail one by two again
one by two and one by two that's one by
two multiplied 5 times you have 1 by 2
to the power of 5 which is nothing 1 by
32 which is nothing but 0.03 now how do
i interpret 0.03
if you repeat that by experiment with
100 people
right you ask 100 people to throw the
coin five times
and you then record all the sequences
that they've seen
out of those hundred people
approximately three people would be
seeing this exact sequence two heads
followed by two tails and then followed
by a hit
that's how you interpret this
probably which is three percent three
point one two five percent
okay navigator says okay this is good
but why did you separate out the
multiplication with respect head centers
if you notice instead of just writing
one by two to the power of five
i would write it differently 1 by 2 to
the power of 3 and 1 by 2 to the power
of
2 right why did you separate because as
you can see this 3 corresponds to number
of heads we see in the sequence right
and this 2 corresponds to number of
tails that we see in the sequence
okay why did we do this that's what
agatha is asking
and to which bernoulli says this is
because if i have to generalize this
formula
for any event that has only two outcomes
right one outcome is p other outcome is
q where p corresponds to the probability
of saying outcome one and q corresponds
to the probability of saying outcome
true right we get a distribution of
probabilities where there are only two
possible outcomes and this is similar to
our problem what is our problem we have
the output variable which is admission
status accepted or rejected there are
only two possible outcomes and we have a
distribution of probabilities we have
the first
student in the data set not getting
admitted second student admitted third
student not admitted four student
admitted like that similar to the coin
toss scenario we also have let us say
100 students example we have 100
probability values and that is nothing
but a distribution right and this is
exactly how you can generalize it as you
can see from this formula notation just
like how we had p equal to one by two
earlier one by so uh if i if i just put
in the values here so this is one by two
to the power of three and a 1 by 2 to
the power of 2
going back to a pointless example now
what's 1 by 2 corresponds to probability
of success because there are only two
events we consider one event as success
fundamental space you can choose
whichever one where you want to denote
it right and then n corresponds to the
total number of tries how many times
should we toss the coin five times right
and k corresponds to how many times we
saw hits that's as you can see the
sequence is heads heads
tail tail and head right as you can see
we saw head three times if we consider
that as a successful event then we are
going to get k value as 3 and
n minus k this is going to be nothing
but 5 minus 3 which is going to be 2 and
that's exactly what we wrote earlier
right so this formula
came from observation most of the
formulas in probability they come from
make performing experiments and you're
generalizing that and that's exactly how
we have got this
right and k corresponds to number of
times you see head here and this is
similar to overall that's what bernoulli
is saying the watch agatha says
what exactly is the distribution you are
talking about right what what what do
you mean by this distribution it's what
does that mean why is that popping up in
this discussion about maximum likelihood
estimation and this is where we come to
our second checkpoint in this journey
which is bend only distribution now ben
noli proudly says this is my
distribution a special case of
distribution where you're going to
throw the coin only once
right and that
distribution is called bernard in the
previous example we throw the coin five
times
right but what his bernoulli is saying
is there is a miniature version of the
previous experiment you just throw the
point once
and whatever you see is outcome it can
be a head or a tail
now that probability value if you just
repeat that experiment once right only
one trial and if you note down that
value probability value that is nothing
but a bernoulli distribution
right and the output variable in our
problem in any classification problem
follows a bernoulli distribution because
each think of each student as
independent as a separate experiment you
are not uh the same student is not
right taking gmat multiple times and
applying to the university you have
different students spread across
different countries taking gmat and
applying to that university so each
student is a separate bernoulli trial
right and you have the outcome of this
student's attempt either he's getting
accepted or rejected just like head out
pay right here he can get accepted or
rejected but here we are having a
combination of bernoulli tries in your
data set right and because it's a
combination of ben know it tries all the
probability values are nothing but
whether they're getting accepted is it
all the zeros and ones yes or no
admission status that distribution is
nothing but a bundle distribution you
think of this as the admission column
right and you have let us say this is
the first student second student third
student right so fourth student like
that you have the all their admission
statuses and this is nothing but a
bengali distribution because we're
talking about probabilities and because
there is only one trial because each
student
is separate right each student is
the second student's efforts are not
dependent on the first student's efforts
the first student gmat score is not
influencing any other student's um score
that says we say the number of trials
has one and that's where n value gets
substituted with one so p to the power
of k q to the power of one minus k and
we call this as the bernoulli
distribution form this is where the
likelihood concept gets extended to
bernoulli distribution the watch agatha
says okay now i understand why you are
featuring so prominently in the
discussion surrounding globalistic
education that's remember chosen
as a person to explain how is this
connected to our problem okay i
understand benoit distribution coming to
target variable and all this but i was
asking you about
how does algorithm learn the
coefficients and you are going ahead
talking to me about maximum likelihood
estimation and then you told me about
likelihood and now you're talking about
burner distribution so where are you
headed
to which bernoulli says i'm coming there
just like the coin example we need to
find out
for what value of coefficients
will we see the maximum value for the
below likelihood function basically
what's happening is
we are answering this question
given the data set
how much weightage should we give to the
gmat score
so that
the distribution that we already have in
the target variable in a data set
actually happens
right so it is something similar it is
as we discussed in the earlier one of
the earlier states we are looking at the
likelihood of this distribution the data
set actually happening
right in order for that likelihood to be
maximum meaning to be absolutely certain
what should be the coefficient value so
as you can see this is a
question that is in reverse earlier in
the case of probability we already have
a condition under the condition we're
going to calculate probability now what
we are saying we already have the proper
distribution we have the probability
distribution which is the actual target
variable distribution
right now given that we already have
that probability distribution what
should be the coefficient values what
how much important should we give to the
input variable so that this distribution
actually happens
right and that is so it is connected and
as you can see because each student's
scores are independent each student each
student's
admission status is independent we're
going to you denote using this greek
notation pi right using p r of v i y i
meaning nothing but we're just going to
just like how we calculated the coin
tosses multiplied all those
1 by 2s we are going to do the same
thing here and that is how we are going
to connect this to binaural distribution
right and now we have an equation and
what do we do when we have an equation
machine learning algorithms we are going
to differentiate it and here's where
we're going to differentiate the
particular equation and find the
combination of values when the slope is
zero which is similar to what we have
done with linear equation that's what
agatha asked agatha is asking as she has
done it in episode one and ben knowledge
says precisely
from the earlier discussion we know that
likelihood of a binary distribution is
like this if i just convert this
right into the notation that we've seen
earlier this is nothing but
p to the power of y into 1 minus p to
the power of 1 minus 3 that's a p p x
into
q
1 minus 6 that was our earlier formula
and i just replaced q with 1 minus p and
x in the
exponent power with y right and then if
i just
have to calculate this right whenever a
student's status is admitted in which
case y equal to one right in which case
the prob the likelihood will be nothing
but p
if the student's value is 0 if it is not
accepted then it is going to be 1 minus
p right and if i generalize this this is
how it is going to look like if i just
substitute this then this is the formula
this is the equation that
we stock as the likelihood equation and
the algorithm tries to maximize this
likelihood that maximizes likelihood
find out those coefficient values where
this whole value becomes the highest
that for each sample in the data set
we're going to substitute the values
calculate p i y i and then try to see
how this at what coefficient value will
we get the maximum likelihood value for
this particular equation and now agatha
has one more question so what does p to
the power of phi represent what is y
here
right as you can see p i is the
predicted probability value so your
algorithm
right if you guys remember from the
earlier discussion we had the
natural logarithm what's ratio equal to
w naught plus w one x that's how the
probability value is predicted by the
algorithm right so p is a predicted
probability it's a predicted output
variable and y is the actual
output variable value so actual value is
nothing but one or zero either very
emitted or rejected whereas predicted
value can be between zero to one because
it can be between it can be zero point
one zero point two zero point two zero
point four
right and that is so these two values
are represented here and if you also
notice this y
also represents right in a bernoulli
trial
what does y represent number of
successes
right and as you can see it is very easy
to find out let us say let us take one
student right whose name is nitin
right and his
data is bernoulli try right so 720 let
us see he got admitted
okay and as you can see banal trail is
related to just one sample how many
successes are we looking at only one
only one success because he
successfully got admitted how many
failures are we looking at zero
right and this is the beauty of uh
bernard distribution in the case of
lowest ration problem right this y which
is actual value represents number of
successes and number of failures
right actual number of successes as you
can see right now what we do we just do
p of 1 here and this value will be 0 and
we have another
students name let us say kiran
right and then let us say he got a score
of 650
and he is rejected and here how many
successes are we looking at zero this
value will be zero and how many
failures are we looking at one and this
value will be one right because 1 minus
y which is nothing but
1 minus 0 which is nothing but 1 so it
will be 1 minus p
okay so that is so this
likelihood function
perfectly summarizes the problem that
we're trying to solve with
let's regression algorithm
okay now
this is what bernoulli says why he
represents whether the student got
admitted or rejected remember in a
bengali trial there are only two
outcomes zero out one and y represents
one of these two values and that's
reason why y represents the number of
successes
right for any given sample okay and this
is how the whole equation looks like
right and that's that is what i got
since the number of successes equal to
value of you have replaced the parameter
k in the bundle distribution as i just
said with y here is my understanding
correct to which the knowledge is that
is correct
now we need to maximize the below
likelihood function n
however there is a problem
not everything is hunky-dory
this concept looks good but there's a
problem with this equation any idea what
the problem is
agatha is asking what is the problem
what's the problem with this equation we
have equation why can't we just go ahead
and maximize it
this is where we come across the third
concept in this learning curve which is
log likelihood
and what bernoulli says is the problem
is that since the probability value is
so small between 0 to
1. it will take a long time for the
algorithm to learn small small values
right after every we know with respect
to gradient descent and other techniques
it has to run through iterations it has
to look at data set look at we were
talking about maybe thousands of rows
millions of rows in the data set and it
will take a long time for it to just
because we're talking we are trying to
reduce 0.6 into 0.5554
and then 0.5567 like that you are going
to keep on reducing it will take a lot
of time right and whenever we deal with
when we were confused when we cannot
handle those miniatures small small
values that's where we take the help of
logarithms
right we apply logarithms precisely in
those scenarios right because once we
apply logarithm what it will do it will
spread out the scale of the fractional
values and it will make it easy for the
algorithm to converge because now it is
going to deal with actual
numbers rational numbers
right and then agatha has a question if
i apply log to this wouldn't that
deviate from our goal of finding the
optimal coefficients we are not changing
the rules of the game that earlier you
said l right we are going to maximize
the log like maximize the
likelihood now we are saying we are not
going to maximize the likelihood we are
going to maximize the
log likelihood wouldn't that
deviate should be again re-normalize the
coefficients and all that right and
bernoulli says nope be the original
likelihood curve which is this curve or
the log version of it which is i let us
call the original curve as l
right and i is the natural logarithm of
the term
we are still trying to find out the
coefficient values when the slope of the
likelihood function is zero so be it l
here
which is this value we are trying to
count to this value or the log version
of it right in both cases we are still
trying to find out this value the value
where the slope is zero
right and so
or in order to make our calculations
easy we have approximated to by applying
log but that doesn't really deviate from
the original intention which is to find
out the coefficients that result in the
maximum likelihood value we just need to
find out what combination of values
what combination of coefficient values
actually result in
hitting that point that i just circled
in red here
right so we're good we're good with
maximizing log likelihood
right and that
summarizes the log likelihood aspect of
it so now we apply log on both sides we
apply now now that we are good to go
ahead with log likelihood we apply the
log on both sides and here is where you
need to again brush up your logarithm
formulas that some of you might have
been introduced during your childhood
that's where we are going to apply the
natural logarithm here and then remember
ln of a b is equal to ln of a plus ln of
b multiplication right so you can
actually split them out into some
summative terms after applying a natural
logarithm and similarly ln of a over b
is nothing but b lna
right now applying these two rules we
get we have the ln that we have two
terms here this is a
and this is b
that you see here and then we are going
to separate them out as we discussed
right it's going to be ln of a plus ln
of b we separate the mode and now we
have a power b kind of notation right
now what we do we just do y i ln p i
plus 1 minus y i l n 1 minus p i so this
is the equation that we are going to now
find out the maximum value for this
equation so this is nothing but the log
likelihood equation
right now that is what agatha is asking
obviously i have to maximize this
function
isn't that right
right and this is where there is one
more
problem
right so that's where cross entropy
comes in now what is our problem now
we have to maximize it but there is
always a part when it comes to not
everything is so straightforward in
machine learning
but we know gradient descent how it
works
it works better when we are trying to
max minimize something rather than
maximizing it instead of maximizing a
function to which case we don't know
where the
upper limit can be it can be anything
upper limit
right programming whereas it becomes
easy or even mathematically when you
look at calculations it becomes easy
rather than maximizing it we actually
minimize that function and how do we do
that we just minimize the negative of
that like log likelihood instead of
maximizing log likelihood positive log
likelihood we are going to minimize the
negative log likelihood which is the
same
right but the only advantage that we
have is it becomes easy gradient descent
understood okay this is similar problem
and we are going to get the results much
faster
right so minimizing the negative
function
which is nothing but mags will likely
maximize the log likelihood and this as
agatha rightly guessed is the popular
cross entropy cost function this is the
cost function for logistic requisition
right as you can see minus
y i which is nothing but actual value ln
of pa which is nothing but the predicted
probability right plus 1 minus y intel
and 1 minus pa
okay so this is how we get the cross
entropy cost function for logistic
location
and that's what he says that's what
bernoulli says correct this is what is a
popular cross entropy function and we
are going to now calculate the gradient
for this cost function using the
gradient descent technique we're going
to differentiate the below likelihood
function with respect to the weights
because we're going to calculate dou j
which is nothing but minus ln of l by
definitely not which is that equation if
you guys remember minus six point seven
plus zero point zero one two into g
minus four how did we get minus six
point seven zero point zero one
and we're going to differentiate the
cost function with respect to weights w
naught and w so dou j by dou w naught
and dou j by dou w one
now you guys must be wondering there is
something wrong
here not everything is so plain here
right
that's what agatha is asking wait this
equation you're saying we can
differentiate with respect to those
words but this equation doesn't have the
weights
we do we don't have the terms we're not
in w1 so how will the differentiation
work how are you going to differentiate
this that's a fair question isn't it so
from where are we going to get this
general differentiation works when you
are differentiating with respect to that
value if double dot w 1 is not there if
you are just applying differentiation
directly here dou j by dou w that this
term will become 0 this term will also
become 0 because there is no that term
right directly seen in the equation
that's what agatha is asking but as you
notice
that term is not directly here but
indirectly distant
and what do we mean by indirectly
it is nothing but we already know in the
previous discussion ln of p i by 1 minus
p i which is nothing but odds ratio as
we discussed earlier
is equal to w naught plus w 1 x
right so it is lurking somewhere it is
there behind the scenes we just need to
dig it up
and that's exactly what
bernoulli says and the way we dig it up
is by actually throwing a chain
and that's where chain rule comes in
chain rule lets you differentiate
complex functions the popular chain rule
plus again calculus one zero one
that's exactly what we are going to do
now in order to calculate this
experience and now agatha says now this
is another new concept what is this
chain rule we are so close to the
end of the tunnel that where did this
chain come from
that brings us to the last button
milestone in this learning path it's
what exactly is chained to so the
knowledge is when dealing with complex
functions like our cost function here
chain two allows us to break down
complex functions into simpler functions
and then differentiate as you can see
here
in order to
differentiate a complex function which
contains the sub function we actually
differentiate the sub function inner
function and then we differentiate the
outer function with respect to it
right now it looks a bit complicated
right so as you can see from the figure
kind of makes sense if you look at it
though d h by d t is equal to first
because h contains a function of x we
first
differentiate with respect to the inner
function b h by d x and then d x by
d
and that will uh you guys must be
thinking we should be cancelling things
up but that is not how differentiation
works that we are going to actually
diminish it with respect to inner
function and then get the outer function
right now
other than okay this is sounds
theoretically fine but can you just show
an example sorry to which
bernoulli says so let us take this
function here this is how we apply the
chain rule let us take this complex
function ln of x square minus one
okay and how we apply the chain rule is
first we differentiate the inner
function as we said x squared minus one
right one by x squared minus one right
and then we are going to then
differentiate the value ln of
so simple formula is ln of x is equal to
1 by x
right because we are first
differentiating x minus 1 we just said 1
by x square minus
right and then we are going to
differentiate this x squared minus 1
because this value we still need to
differentiate right we're going to
differentiate x squared minus 1 and that
will be x square minus 1 by dou x right
so by dou by dou x and this value will
be as you can see here two x
right and this is exactly what chain
rule is we go step by step and this is
exactly the concept behind deep learning
also back propagation chain rule concept
and that's what
bernoulli says but then agatha says okay
this is good we understood what this
chain rule is but how does it apply to a
cost function this looks like a toy
example but how does this work out for a
which cost function so which ben ali
says let us first look at what exactly
is at stake here we have this equation
which is the equation that we already
have seen negative log likelihood which
we are trying to
for which we are trying to calculate the
gradient
and we also have this term this
particular function right where we are
going to calculate the
probability so this is nothing but we
rearrange the terms with respect to
natural logarithm horse ratio equal to
alpha alpha is nothing but w naught plus
w 1 x is the linear equation equation
right we rearrange the terms and we get
this and this is nothing but sigmoid
function the popular sigma extension and
this function we already have so we know
that this is a relation this this is
where the logic discussion that we
discussed earlier comes into picture now
we already observed the logic function
links
probabilities to linear regression
equation through this equation
now we already have the regression
equation so we have these three terms
now we need to calculate the
gradient
right so now
agatha also says okay this is
complicated now we are not just dealing
with one function dealing with three
different equations three different
gradients
right and to which
ben only says we are going to simplify
it this is how we apply the chain to
here we have these three functions
this is what we are going to do so
see the demonstration for with respect
to w1 how we differentiate it definitely
not also the process will be the same
and this is where the chain rule comes
first we are going to differentiate the
negative log likelihood with respect to
bi predictor probability value and then
we're going to differentiate this
equation from greater probability value
with respect to linear regression
equation and then we are going to
differentiate this linear regression
equation with respect to w1
right so this is how we are going to
navigate this to which i gotta say so we
need to calculate the derivatives for
each of the three components
exactly
right and let us first differentiate the
cost function the most complicated one
first with respect to the predicted
probability value p i i remember the
variant for log function which we
already just discussed ln of k is equal
to 1 by 2
now what bernoulli is saying is this is
what is a actual cost function now we're
going to differentiate and we're going
to differentiate it
simply right so y i is a constant term
here dou by dou pi of ln of pi right and
then similarly 1 minus phi is also a
constant term here uh dou by dou p i of
ln of 1 minus pi here right and then if
i just do that ln of p is nothing but 1
by p here and then here ln of 1 minus pi
is nothing but 1 minus pi applying chain
rule dou by dou pi of 1 minus pi is
nothing but minus
right and if i just rearrange the terms
this is how it will look like this is
the gradient of the cost function with
respect to predicted probability value p
i y i by p i minus 1 minus 5 by 1 minus
p i
now
i got to say that was easy we did not
really we just use this one simple
formula ln of k equal to 1 by k right so
what what next which function are we
going to take next
second comes a slightly more complex
function right we differentiate the
predicted probability value with respect
to
regression equation alpha so dou p i by
alpha so before we differentiate we are
going to rearrange the terms let's
instead of the same
equation can be written in this formula
why because
e power minus alpha is nothing but 1 by
e power also
right and then if i do 1 by 1 plus 1 by
e power alpha
right so what we get is nothing but e
power alpha
by 1 plus e power alpha which is exactly
what is this term let's instead of
writing e power alpha numeric
denominator i'm just going to put it in
the denominator because this will make
it easy for me to differentiate that is
how we got this equation here
and then i'm going to apply dou alpha
here and one thing that you need to
remember is this formula here right as
you can see here
dou by dou k of 1 by k is equal to minus
1 by k square
right and then dou by dou k of e k equal
to
e k
e to the power of k
right so that's exactly how we are
looking at
differentiating those are the
classic differentiation concepts if i
just do that right then what i'm going
to do i'm going to just do because 1 by
k is nothing but minus 1 by k square so
minus 1 by 1 plus e power minus alpha
whole square and then i'm going to
uh just
differentiate e power minus alpha right
e d by dou by dou dou k of e power k is
e power k again because if we
differentiate it again it will be the
same because the rate of change of
exponential function is always going to
be
the function value itself that's e power
minus alpha and then because i'm also
going to
do for alpha minus alpha differentiation
is going to be minus one
right if i just arrange these terms if i
multiply
this terms this is how i get it right i
just have this equation and then i
multiply them then i'm going to get test
e power minus also in the numerator the
denominator i have 1 plus e 4 minus
alpha whole square
now what i do is because i want to keep
things simple i'm going to rearrange the
terms i'm going to just write the
numerator as 1 and t power minus alpha
and denominator i'm going to split them
into 1 plus e power minus alpha and 1
plus support minus alpha and any does it
ring a bell if i see this equation this
is nothing but p i into 1 minus p n
this is the equation for p i value isn't
it if you guys remember here that's the
pi value is nothing but e power alpha by
1 plus e4 also and that's exactly what
we got here 1 by 1 plus e power minus
half is nothing but pi and that value is
nothing but 1 minus pi
right now this is the gradient for the
second component in the cost function
that we are trying to differentiate
that's a 2 down
one to go we have two values earlier we
had that fraction now we have this value
for with respect to p i with respect to
alpha now the final one is the easiest
one finally we differentiate the
regression equation with respect to the
weight w1 right and this one is nothing
but you just apply dou w1 here you'll be
getting the value as x
because w 1 by dou w 1 x by dou w 1 is
nothing but
w 1 x is a constant here right and then
w 1 by w 1 is nothing but
as you can see it is one
right so i believe the next step now we
have we got the three gradients i
believe the next step is to multiply
these three gradients that is precisely
what we should we'll be doing now we
multiply all the three gradients we have
obtained using the chain rule this is
the chain rule right we have got the
gradient with respect to
rated probability greater probability
gradient with respect to alpha and alpha
gradient with respect to
w1 right we just multiply all these
values y by pi this is this is the one
that we see here that this is the one
that we are looking at here and this is
the one that we have for this one here
right and then we multiply that and
again we rearranged it so y i by p i
into 1 minus p i into p i into x right
so this particular term will be
multiplied by
this these three terms here right and
similarly this term is also multiplied
with three terms here that's what you're
saying using x pi and one minus p
right and then what we are saying is the
final gradient of the positive cost
function if i just do that right and if
i just calculate it and if there'll be
the cancellations happening these two
terms will cancel and finally this is
the gradient
of the cost function with respect to w
this is what we are we set out this is
what we wanted to calculate which is
nothing but x into predicted probability
value minus actual value this is the
gradient value with respect to w
and with respect to w naught the value
will be the same except x value will be
one if you repeat the same steps using
with respect to w naught you will get
the exact same thing the only difference
is going to be instead of
dou j by dou w one by
dou double even we got x earlier for the
third component right and if you do it
with respect to w naught it will be the
constant will be
one
right just mathematical arrangement
that's it nothing much nothing less
right and this is how we got the
gradients
this is the light at the end of the
tunnel that we are looking at this is
the end of the calculus adventure
right now what is it is is it done did
we get the coefficients not yet we just
calculate the gradient now we need to
apply the gradient and that's exactly
where the final checkpoint comes which
is gradient descent
just like linear regression algorithm
here also we're going to use the same
gradient descent algorithm the only
difference is the gradient calculation
in the case of linear regression direct
values are different
here the gradient values are different
because it's a different cost function
right we are going to update the weights
with the gradient using the gradient
distance algorithm which will give the
final coefficients that we have seen
earlier so step one just like new
regression we're just going to randomly
choose the weight values for the gmat
score
aspect of it and the constant of the
linear equation equation and then we'll
calculate the gradient for the cost
function right this is how the
just like in the earlier slides we have
seen we have calculated the cost
function and it is going to use a
learning rate eta and it is going to
calculate the gradient it will adjust
the weights by subtracting the gradients
and then
it will repeat the steps 2 and 3 till
the time the cost function value is
minimum which is nothing but we are
trying to minimize the long likelihood
which is nothing but we are trying to
maximize the
log likelihood that minimize the
negative log likelihood is equivalent to
maximizing the positive log likelihood
which was our whole intention because we
want to ensure that the benoit
distribution that is there in the data
set is upheld by what combination of
course what is that hidden quotient
value in the data set that will ensure
this probability distribution that we
are seeing with the split admission rate
right and this is how we get the final
equation natural logarithm of force
ratio equal to minus 6.7 plus 0.01 this
is how all this weight updation
ultimately resulted in this is how we
got that final equation this this
answers what agatha was asking at the
start of this discussion
right and agatha says this indeed is a
fascinating algorithm thanks much for
explaining this in so much detail
right so with that we come to the
conclusion as we have seen we have
covered the concept as you see where we
have started and where we ended it all
started with the simple pointers
likelihood and then we tried to
understand what likelihood has to do
with bundle distribution we are trying
to maximize the likelihood of the
pendulum distribution of acceptance
values
in the data set
right in order to maximize it we said
okay logarithms will make it easy for us
to perform calculations that's where we
took a log likelihood and then we said
minimizing a negative value is equal to
maximizing a positive value that's where
cross entropy came into picture which is
nothing but the so negative log
likelihood is close and group and that
is the cost function velocity equation
and then we
looked at how we calculated gradient
with respect to weights using chain rule
and then finally we applied the gradient
descent once we got the gradient
calculated with respect to weight using
chain rule we applied the gradient
algorithm and that's how we got the
quotient values
hope you guys enjoyed the discussion as
much as ben dolly and agatha did and
until next time
this is