Why not transpose the input to BatchNorm1d and then transpose it back?
You should use 2D tensor as input, since BatchNorm1d works with mini-batches:
[https://pytorch.org/docs/stable/nn.html#torch.nn.BatchNorm1d (hyper-link)]
[BatchNorm1D (hyper-link)] and [BatchNorm2D (hyper-link)]
BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice.
However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].
what does [BatchNorm1d (hyper-link)] do mathematically?
If you pass torch.Tensor(2,50,70) into nn.Linear(70,20), you get output of shape (2, 50, 20) and when you use BatchNorm1d it calculates running mean for first non-batch dimension, so it would be 50.
BatchNorm1d can also handle Rank-2 tensors, thus it is possible to use BatchNorm1d for the normal fully-connected case.
The BatchNorm1d normally comes before the ReLU, and the bias is redundant, so
