we will be looking at tensorboard today
it is a tool that allows you to debug
the issues with neural network
it is also helpful in tracking and
visualizing metrics such as loss and
accuracy this is a official tensorflow
tensorflow a page for tensorboard and
tensorboard is a tool that
comes with tensorflow so you don't have
to do a separate installation for it
now in the same deep learning series uh
one of the videos that we had was
building a neural network for
recognizing handwritten digits so i'm
gonna take the same notebook that we had
so this is the same notebook where you
load all this data set handwritten
digits
and then you train it using a neural
network and in this neural network there
was a flattened layer
then there was one hidden layer with 100
neurons and the third layer had
10 output neurons 10 is like basically 0
to 9
and we have already done this coding so
if you're not seeing the
that video please go in this same deep
learning series and watch that video
because that video is a prerequisite
so we have done all of this in that
previous video and now
we are at a point
where we want to look into some of the
internals
of this neural network so let me execute
this
code once again so
if you want to execute all the cells in
the jupyter notebook you can say
run all the cells so that way i am
running
all the cells and here what's happening
is
i'm running phi epoch and with every
pocket is showing me the loss
and accuracy for my uh
training data set now i came up with
this
parameters but you might want to try it
with different parameters for example
instead of adam i want to try
sgd okay and when you try std
you see like here with adam i get point
98
in the end but with std you might get a
different
type of loss and different behavior of
your neural network
so here you got point 92 and
we are using very less epochs only five
but there might be a case where you
might have to run less a thousand epoch
and you like reading these numbers
might not be that visually appealing so
it will be very nice if we have some
kind of visualization
or graphical representation which tells
you
how much your accuracy is going up or
your loss loss is going down
per epoch okay so if we had that kind of
nice graph
that might be better and tensorboard
provides that
tensorboard provides some other
functionality as well
but that's uh one of the functionality
that it provides
so let's uh try it out and see how the
whole thing works
so the first thing you need to do is
uh go to the directory so here i have
some
old logs file so i'm just gonna remove
it so this is my current working
directory
in here i am going to create
a tensorflow callback
it is a tensorboard callback okay so
this is how it looks so here is the log
directory to save all your logs
and this is your callback so i will just
call it
tensorboard callback and that you supply
in your fit function so here you will
say
callbacks is equal to
tb callback and what happens is when
tensorflow is running the training it
will use this callback and it will
supply all this information per epoch to
tb callback
and that will put that information or it
will log all those events in this logs
directory
so in my current working directory where
i have this notebook i will see
logs directory and that will have all
the events
okay so let's run it once
so in order to run it let me just run
all the cells so that way
we start fresh and we have a clean slate
and i'm running sgd which is a
stochastic gradient descent
this is something we have discussed
again in the previous tutorial so
please follow the entire uh series
before you come to this video
okay so now it has recorded into tb
callback so how do you
exactly visualize it so for
visualization
you can uh go to
the prompt here i have my git bash
and here you can run this command so it
is saying that run tensorboard
with this logs directory and you will
notice that
now i have this locks directory it has
all the
logs okay so when you run it
it's gonna run this ui
on this particular url
and you can see here
that it launched the tensorboard now the
other way of launching this tensorboard
is by using this command
so if you use for example
if you just want to launch it within
jupyter notebook you can launch it like
this
so we can try it
and it will render it at the bottom but
it will be similar to what you see here
and i prefer rendering it in a separate
browser url
so here what you're seeing is how your
accuracy is increasing per
epoch okay so let's see
here so this is training
and this is just smoothening of the
curve
and let me just expand it so at step
number one
my accuracy was point nine zero two
eight you see the point is nine zero two
eight
so let's see you see point nine zero two
eight
then point ninety one point ninety two
point ninety three two
five so you see all of that see
this one is 91.92
0.993 actually
9325 five
you see so these values are matching
with the values that you see in this
column
also for loss we started with point
eight nine three eight
went to point two four four four five
five
so here with the loss you can click here
to
to kind of fit the graph or you can
click here to kind of make it bigger so
let me make this graph bigger
and you will see that we started at some
point then 0.35
0.3 0.26 and 0.24
see 0.242 fixed so this is just
a graphical representation of how your
loss is reducing and your
is increasing
okay here it timed out maybe because i
have this other thing
running so just i'm not gonna worry
about it i have it running
okay here the other parameters that it
has
are graphs so if you look at graphs
um by the way i i ran it multiple
times that's why you're seeing multiple
graphs but
you can um as such you will see only one
graph so let
let's just do it all over again
because i don't want to see that
multiple graph because
those might be confusing so i will
what i will do is i will stop the
execution here
by the way i'm just running everything
from scratch okay
and what i will do is i will
also stop my jupyter notebook
so my kernel is shut down
and
i'll just restart it again
okay so restarting this one
and i will say run basically everything
so if you do
run all cells it's gonna just run all
the commands from scratch and i want to
see
tensorboard after it has around
everything
so once it is done you can do
run this command and it should open up
tensorboard once again
so local hostess and now when i look at
graphs i see only one graph
so scalars is the same thing that we saw
before
okay i see multiple charts here
all right i don't know what's going on
with this one but we'll just go straight
to graph
so in the graph what you see is the
inner representation of your neural
network so in the neural network you
will notice that there is 28 by 28
shape for the image that we are feeding
so you saw that in the first network our
input shape was 28 by 28
that was followed by a dense network
with hundred neurons
value as activation function so you will
see that
if you click on this plus icon
you will see that
you got this uh flattening layer so when
you
feed 28 by 28 into flattened layer it
will
convert it to 784 like single dimension
array and that is fed
into this dense network and you know in
any neural network
it has every neuron has two components
weighted sum and then activation
function
so weighted sum is matrix multiplication
here
and by we also add a bias and then we
have a value
as an activation function so you'll see
784 was fed here then there was a matrix
multiplication
then there was addition of bias again
all this theory we have seen in our
previous video so it should be clear
and then you see dance similarly if you
double click here
then you see what's going on in that
layer so in that layer
i have 10 output neurons activation is
sigmoid so here you will see
my activation is sigmoid output neurons
are 10
and again there is matrix multiplication
and bias
going on okay if you go to
histogram chart histogram will show you
the weights basically here what you have
is
bias and here you what you have is
kernel all the weights
and these are each run read and phi
epoch so this is epoch 1 2
3 4 5 and it is showing the distribution
histogram you all know right histogram
is just a frequency distribution
so just showing the frequency
distribution of all your weight so
for this value you have 49 weights for
example
uh if you look at bias for bias
you will have dense one
and dense four and then five so let's
see so
in the we have dense one so in the dense
one
bias will have ten uh output
values so that's what you'll see here so
these kind of visualization helps you
debug the issues right now our neural
network is very simple
in real life scenarios you'll have much
complex neural network
and at that point using tensorflow
can really become handy and you can
debug various things with it
one of the other thing you can do is you
can
let's say i tried this with sgd now
i want to try it with let's say adam
okay and when i try it with adam i want
to do one thing which is
i want to maybe store it in a separate
directory
so for adam i will just say let's store
that
in logs atom okay
and then for sgd i will just store it in
logs sgd so that way i have like two
separate directories so here
i will say sgd sjd i want to check
performance of both the
optimizer i want to see visualization of
what
these two optimas are doing optimizers
are doing
okay so then what i can do is just say
refresh here
and here it is showing you different
runs okay so i'm just gonna
uncheck this and check the two latest
run that i had
you see i had adam and sgd as a run just
recently
so i'm on clicking this and comparing
these two
and you will notice here how
these two are behaving so my atom which
is blue
is obviously better than sgd see the
accuracy is increasing
and the loss is also decreasing faster
so this way you can compare
two different optimizer or two different
hyper parameters so by the way uh here
optimizer matrix epoch
learning rate these are called hyper
parameters and the parameters
are nothing but the values
of weights basically the weights and
biases are called parameters
and these guys are called hyper
parameters because they control those
parameters
they affect those parameters that's why
they are called hyperparameters
so i hope this video was useful you got
some understanding of tensorboard in the
future video we'll be covering
uh more detailed aspects of tensorboard
all right so if you're liking this
series so far
uh give it a thumbs up share it with
your friends i'm gonna come up with
many more tutorials in this series thank
you for watching