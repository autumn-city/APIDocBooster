OMG, I have realized that actually in my code I was calling the cosine_similarity (sklearn) from a function that I have called 'cosine_similarity'.
cosine_similarity expects 2D np.array, or list of lists.
cosine_similarity returns all-vs-all similarity.
So, let's limit to pairwise comparison, artificially creating second dimension (note the extra square brackets in [row['vector_a']], [row['vector_b']]), and then taking the only element of a 1x1 array (zeros at the end of cosine_similarity(...)[0][0])
You have to change it over the cosine_similarity calculation, just change
sim = cosine_similarity(features_compress)

So here, I reckon features_compress is the set of features for all your images that are contained within x_test, and not a single image.
And if that indeed is the case, then think of the result returned from cosine_similarity() as a matrix that tells you the similarity of each image with every other image.
You can use cosine_similarity function form sklearn.metrics.pairwise [docs (hyper-link)]
Cosine_similarity = 1- (dotproduct of vectors/(product of norm of the vectors)).
[https://en.wikipedia.org/wiki/Cosine_similarity (hyper-link)]
In that case (csr matrix is quite large), it is hard to calculate at once,
My approach was "cosine_similarity(tfidf_matrix[index], tfidf_matrix[:])" * N times.
Actually I performed it with pyspark
def calculate_one_to_all_similarity(index):
...
cosine_similarity(tfidf_matrix[index], tfidf_matrix[:]
rdd.map(lambda r: calculate_one_to_all_similarity(r2index[r]))
Assuming the distance matrix d = cosine_similarity is a such a symmetric distance matrix up to numerical artefacts you can apply
Since cosine_similarity expects a 2d array or sparse matrix, you'll have to use the sparse.vstack to join the matrices.
Using the sklearn cosine_similarity this code segment runs and returns a sensible looking answer.
You can use apply() to use cosine_similarity() on each row:
Are you sure its cosine_similarity() is designed to take such raw strings, and automatically tokenize/combine the words of each example to give sentence-level similarities?
You might want cosine_similarity to check if the result is almost_equal to 0 or 1, in which case return 0 or 1.
In your cosine_similarity function at array_user1 = np.array(item for item in dataset[user1][item])
The only difference is that in your first code, the result is bound to the parameter for cosine_similarity, while in the second, it is bound to the name df currently in scope.
cosine_similarity seems to be handling memory just fine.
If you look closely at your MemoryError traceback, you can see that [cosine_similarity (hyper-link)] tries to use the sparse dot product if possible:
So the problem isn't with cosine_similarity, it's with your matrix.
It fits in memory just fine, but cosine_similarity crashes for whatever unknown reason, probably because they copy the matrix one time too many somewhere.
cosine_similarity is already vectorised.
An ideal solution would therefore simply involve cosine_similarity(A, B) where A and B are your first and second arrays.
We then compute all pairwise distances using the cosine_similarity function, and then reshape to get them into the appropriate shape.
You can convert everything to a 2D numpy array and then simply apply cosine_similarity once and let the shape broadcasting take care of everything
Both your calls to cosine_similarity return the same underlying data.
The docstring for sklearn.metrics.pairwise.cosine_similarity says:
To do this, you simply need to provide a second argument to cosine_similarity containing only the parent_vector
It's not your fault, for some historical reasons, tf.keras.losses.cosine_similarity in tf v1.15 will only return cosine value, but in tf v2.4.1 will return negative cosine value.
[tf v1.15 source code (hyper-link)] of tf.keras.losses.cosine_similarity:
[tf v2.4.1 source code (hyper-link)] of tf.keras.losses.cosine_similarity:
So, if you want using tf.keras.losses.cosine_similarity in tf v1.15 as same way as in tf v2.4.1, just add minus sign before it outputs
Note that cosine_similarity() creates an (n x n) results matrix.
All the diagonal elements of cosine_similarity are same.
Pairwise cosine_similarity is designed for 2D arrays so you'll need to do some reshaping before and after.
Extend the solution of @Psidom to convert the series to numpy arrays before calculating cosine_similarity and also reshape:
I believe cosine_similarity treats the matrices as features on columns and samples on rows (or the other way around).
If you call cosine_similarity with a second argument it will only compute the distance against the second array.
First if the dataframe is not too big (in your case it seems to be), you can do it by using the vectorization of cosine_similarity.
the second iterrows was not taking advantage that cosine_similarity is vetorized for input arrays with several dimensions
There is no need for the for-loops, since cosine_similarity takes as input two arrays of shapes (n_samples_X, n_features) and (n_samples_Y, n_features) and returns an array of shape (n_samples_X, n_samples_Y) by computing cosine similarity between each pair of the two input arrays.
You can use the [TruncatedSVD (hyper-link)] transformer from sklearn 0.14+: you call it with fit_transform on your database of documents and then call the transform method (from the same TruncatedSVD method) on the query document and then can compute the cosine similarity of the transformed query documents with the transformed database with the function: sklearn.metrics.pairwise.cosine_similarity and [numpy.argsort (hyper-link)] the result to find the index of most similar document.
cosine_similarity(doc_term_matrix) returns
So you can use cosine_similarity(doc_term_matrix)[0][1] (or [1][0], it doesn't matter since cosine is symmetric).
According to the [cosine_similarity documentation (hyper-link)], the default axis value is axis=-1
cosine_similarity is in the range of -1 to 1
See [https://en.wikipedia.org/wiki/Cosine_similarity (hyper-link)]
cosine_similarity takes in vectors of numbers, and not strings.
This can be done much more easyly and way faster by passing the whole arrays to cosine_similarity after you move the labels to the index:
you would have to run your program java Cosine_Similarity path_to_text1 path_to_text2
to update because both(linear_kernel and cosine_similarity) are kernel operations.
Also my second comment is that your function cosine_similarity is symmetric (ie, cosine_similarity(x,y) = cosine_similarity(y,x)).
As mentioned in the comments section, I don't think the comparison is fair mainly because the sklearn.metrics.pairwise.cosine_similarity is designed to compare pairwise distance/similarity of the samples in the given input 2-D arrays.
Maybe a more fair comparison is to use scipy.spatial.distance.cdist vs. sklearn.metrics.pairwise.cosine_similarity, where both computes pairwise distance of samples in the given arrays.
cosine_similarity is defined as a dot product of two normalized vectors.
For example cosine_similarity(x.reshape(1,-1),y.reshape(1,-1))
First, we need to remeber that cosine_similarity(tfidf_matrix, tfidf_matrix) return a matrix of similarity using tfidf_matrix indexes, therefore to this case matches[i][j]==matches[j][i] and that value represent the similarity between description[i] and description[j].
