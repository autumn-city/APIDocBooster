In undersampling we are basically losing out
on important information.
how do we tackle that
we tackle that by using something called
as over sampling
In oversampling we kind of increase the number
of samples minority class to match up to the
number of samples in the majority class.
In the previous videos, 
we've looked at under sampling methods.
This video will help you get an idea of how over sampling is done.
The basic fundamental idea of oversampling
is you take the minority class and try to
create new samples that could match up to
the length of my majority class samples.
What I mean by that is
if you have 100 samples in your minority class
and
Ten thousand samples
in your majority class,
then you should increase your minority class sample.
From hundred to say nine thousand or ten thousand
so that the length of both 
your majority & minority class add up.
That's the bare minimum idea of 
how over sampling is achieved
There are various means of doing
it one of them is random over sampling.
& the other is SMOTE wherein you create
artificial samples of your minority class
we'll first look at random over sampling.
So the way random over sampling works is you
have your minority class.
& you have 100 samples
& you have your majority class.
You have thousand samples.
You will pick a row randomly from this minority class
& put it into the data set.
So, I will first pick up say row number 99,
put it up here again.
& I'll do this iteratively so that I match
the total number of points which are there
in my majority class.
That is how random over
sampling works.
Now in comes the next method,
which is called as SMOTE.
So the full form
of SMOTE is
Synthetic
Minority
Oversampling
Technique
Say, for example, you have two
features.
X1
& X2
We have more number of black samples
as compared to blue samples
so there is class imbalance that is there
now rather than decreasing the black samples
I'll be increasing the blue samples, which
is my minority class samples by using SMOTE.
how does this work is what I'll explain
so, I'll magnify the minority class samples in the
next sheet.
So, I had four points in my minority class.
So, I had p1
P2
P3
& P4
& now if I do SMOTE analysis 
on this which is my minority class
if I specify the
K nearest neighbors that I want to consider
to create artificial samples as say three.
Or rather than going into the depth how SMOTE
works is it will find out the nearest neighbors
of every point so for P1
you have P2 which is your nearest neighbor, 
P3 is your nearest neighbor,
& if you keep a
nearest neighbor count of 3
then even P4 is your nearest neighbor.
Similarly for P3
P1 is nearest neighbor P2 is nearest neighbor
and P4 is nearest neighbor
For P4, 
you have P2, P1 & P3
based on the number of samples 
you want SMOTE to create
SMOTE would first
find out these lines which is the line joining
your minority class samples based on how many
nearest neighbors you've considered and it
will plot these instances somewhere on these lines
so
I'll have one minority point new
here, which is artificially generated.
I have one point here.
Which is artificially generated.
I can have one point here.
I can have one point here.
I can have one point here 
& I can have one point here
if I kept the count of more number
of samples between each link that I create
I can have multiple samples on one single
link as well, so it all depends on what you
configure and how many data points you want
after synthesizing the new minority instances
the imbalance that existed in our data set
would go down considerably now if you go back
to the same diagram that we had.
We created good amount of samples again using SMOTE
so one was here, one was here, one was here
I had a sample here.
I had a sample here and I had a sample here
again
now this ratio is almost 50/50 you have
equal number of blue samples and equal number
of black samples,
so you can specify how many
artificial samples you want to grow in SMOTE
so as to match up with your majority class.It's
Lets now write code for implementing 
SMOTE using Python.
Now that we are clear on the concept of
how SMOTE works
let's now write code for implementing
SMOTE for over-sampling.
We're still referring to the same data set
where I had some features and a target variable.
I split that features into Xtrain, X test
Ytrain, Ytest.
In my Y train itself,
I have close to 1 lakh 90,000 samples of class 0
and around 343 samples of class 1.
So there is a clear imbalance that is there
in my data set.
Now the task in hand is to perform over sampling
for which we are using SMOTE
in order to use SMOTE,
I will first import & assign it to a variable
called as sm
Once you have done that we also have to pass
in some parameters.
I pass and random state and ratio wherein
I assign ratio to equal to 1
The significance of assigning ratio to be equal to 1 is it
equates or it creates samples so that my minority
class samples are equal to the majority class
samples.
So in our example, I have close to
1 lakh 90,000 samples for class 0 after using SMOTE
I'll have 1,90,000 samples in class 1 as well.
So I basically fit the samples that are there
in hand and let's see what SMOTE gives me.
So as it was evident by the ratio that we
chose we'll have the same number of samples
for class 0 as well as class 1.
Now, let's go ahead and fit our logistic regression
model and see the results.
So I have Ytest predicted as well.
Now, once I look at the confusion matrix,
as you can see, there are good amount of false
positives in the system, which is what I don't
want in my setup.
So is there a way to tackle it
yes, which will talk in some time but that's
how you do SMOTE at the topmost level.
Now if I generate the model report as well,
I see that it's a fairly accurate model but
with a very low F1 score.
And the AUC values are also very high
now that we've understood that how SMOTE works
and how it's implemented in a sklearn we had
this issue of false positives into the system
or to create a balance between false positives
and false negatives.What we have done in previous
videos is what we will do again.
We'll do good amount of hyperparameter tuning
rather than me having to decide the optimum
weight or ratio for SMOTE.
It's better that I fit models based on those
ratios and see which gives me better results.
One thing that I would like to mention is
the right way of doing SMOTE is not applying
SMOTE directly on your training data as it
is what I mean by that is once you split your
data into train and test that is a good way
of applying SMOTE only on your training data.
So if you apply SMOTE directly on your data
without splitting it
you are essentially creating
new samples that would appear in your validation
or your testing set as well, which would give
out misleading results.
All your artificial
samples should only be used for training and
not for testing purposes.
whatever you apply SMOTE 
on is only going to be used in
Training and don't ever use the samples for
validation or testing because those samples
are artificially generated samples.
so let's come back to the hyperparameter tuning bit.
I generate weights or ratios in the range
of 0.05 to 0.25
there is something called
as make underscore pipeline which helps you
create a pipeline in SMOTE
so you can start
off with SMOTE specifying what all modules
you would require to create a pipeline so
I would require SMOTE & 
then a logistic regression module.
I create a grid search CV object
and save it into a variable called
as gsc and I do a three-fold cross validation
the scoring method that I'm kind of targeting
right now is my f1 score, let's see what the
results are this process will take some time.
Now using hyperparameter tuning I am able
to find out that the best SMOTE ratio is 0.05.
Which is evident from the graph as well so now
I'll be using that same value of 0.005 and
making a logistic regression module fitting
it to my training data again and then validating
it so now when I find out the confusion matrix
now, there is a perfect balance between false
positives and false negatives now when I generate
the model report, it's a highly accurate model
as expected and the AUC ROC values are also
very high
so this was my take on how you can
use SMOTE effectively for over sampling your data
if you do have any questions with what we have covered
in this video then please feel free to ask
in the comments section below and I'll do
my best to answer those if you enjoy these
tutorials and would like to support them then
the easiest way is to simply like the video
and give it a thumbs up and also to use help
to share these videos with anyone
who you think would find them useful
Be sure to subscribe
for future videos 
& thank you so much for watching.