All advanced activations in Keras, including LeakyReLU, are available as [layers (hyper-link)], and not as activations; therefore, you should use it as such:
According to the docs [https://github.com/microsoft/onnxruntime/blob/master/docs/OperatorKernels.md (hyper-link)], LeakyRelu is only implemented for type float (32-bit), while you have double (64 bit).
And maybe create an issue on the ONNX Runtime Github to add double support for LeakyRelu.
To use LeakyReLU in a layer you can do this:
You can implement LeakyReLU like this:
There is a line of the code where we still have discriminator_model.add(Conv2D(64, 5, strides=2, input_shape=(28, 28, 1), padding='same', activation=LeakyReLU(alpha=0.2))) ,  it is the second line of the code.
The difference is that relu is an activation function whereas LeakyReLU is a Layer defined under keras.layers.
For activation functions you need to wrap around or use inside layers such Activation but LeakyReLU gives you a shortcut to that function with an alpha value.
There is no such aliases available in keras, for LeakyRelu activation function.
We have to use [tf.keras.layers.LeakyRelu (hyper-link)] or [tf.nn.leaky_relu (hyper-link)].
From the method mentioned on step 1, this process is done in 2 stages firstly to multiply weights, add biases and then to apply the LeakyRelu activation function (mentioned in 2 lines).
There seem to be some issues when saving & loading models with such "non-standard" activations, as implied also in [this SO thread (hyper-link)]; the safest way would seem to be to re-write your model with the LeakyReLU as a layer, and not as an activation:
This is exactly equivalent to your own model, and more consistent with the design choices of Keras - which, for good or bad, includes LeakyReLU as a layer, and not as a standard activation function.
This test will always be false because 'LeakyReLU' has some capital letters and act never does, therefore it will never add a LeakyRelu layer to your model.
Also, the correct syntax is tf.keras.layers.LeakyReLU() ([Leaky ReLU in tensorflow documentation (hyper-link)]).
Why did you use LeakyReLU in Generator instead of ReLU?
Use LeakyReLU activation in the discriminator for all layers
There is an issue with your code   model.layers[i] = LeakyReLU(name=name).
So the model would look like this (assuming LeakyReLU is applied on the output of LSTM layers):
As for using the LeakyReLU layer: I guess you are right that linear activation should be used as the activation of its previous layer (as also suggested [here (hyper-link)], though aDense layer has been used there).
tanh) and therefore it squashes the outputs to the range [-1,1] which I think may not be efficient when you apply LeakyReLU on it; however, I am not sure about this since I am not completely familiar with leaky relu's practical and recommended usage.
Just divide each pixel with 255. the fact that LeakyRELU
performed better than sigmoid or tanh is because the values are too
large.
No, you forgot to connect the LeakyReLU to the dense layer:
LR = LeakyReLU(alpha=0.1)(dense_1)
Typically, for a LeakyReLU activation, you could adjust the leak parameter through training (and it would, in the documentation's terminology, be referred to as a state of this activation function).
Also, altering the activation function from leakyRelu to relu is similar you randomly alter the shape of your hype-space.
and this add an other activation function whitch is LeakyRelu :
I will advise you to delete the LeakyRelu layers and change the activation functions inside your Conv2D layers by activation='relu'.
What we are doing is a write a wrapper function called my_activation which will return a Leaky ReLu with negative slope of 0.3 if the parameter is LeakyReLU else it will return the normal activation.
In your case the problem is that you are using the LeakyReLU activation.
