 All right, let me now show you how we can implement residual
 networks in pytorch. So I will show you two notebooks. First is
 a naive implementation I have made myself. And then I will
 show you a more sophisticated implementation of resonant 34
 from the pytorch community. So in this first notebook, now I
 will show you what we talked about in the last video, these
 two different types of residual blocks. And then in the next
 notebook, I will show you this residual network with 34 layers.
 So yeah, I'm not going to rerun this notebook, it didn't take
 too long. But yeah, why waiting if, if it's not necessary. So
 we'll show you just the results. So um, yeah, here, it's just a
 boilerplate, importing torch and numpy, the usual stuff for the
 data set. So I'm not using the helper functions here, because
 yeah, it's really just very simple. So I didn't really focus
 on abstracting things, I just coded the residual blocks. And
 this is a self contained notebook in that way. So here,
 I'm using the MNIST data set just for simplicity, because I
 just wanted to have a data set doesn't really matter which one,
 because this is not going to be a good convolution network. It's
 just like more of a proof of concept how the residual block
 works. So the data set really doesn't matter that much here.
 So here, I'm implementing now this residual block, the one
 where the input has the same dimension as here, the output
 from the residual part. So how does it look like? So this one,
 so I implemented it using the torch module class, just a
 regular confinet that I'm implementing here, I have a
 confinet with two residual blocks. And each of those is one
 residual block. So you can see that this convolution here
 represents this one. So here, I'm starting with one channel for
 output channels. Then I have batch norm. Then I have relu. By
 the way, I haven't really explained what that one one
 means. I think I have used that before in some other code. So in
 place equals true. This just means that pytorch doesn't have
 to make a copy of that array internally. So we could do
 something like that. I mean, not here inside, but in general, we
 could do something like
 like this. Just let's write it like this. So this will create a
 new a new tensor x and then overwrite this tensor x. So it's
 essentially overwriting this tensor x. But for a brief moment
 in time, when this gets executed, there are two arrays of
 this is an existing ones, if I have some previous computation
 here. So this previous computation created x. And then
 when I'm calling this, it will take an x and create a new
 version while x is still in memory. So for a brief moment in
 time, I have two arrays in memory, it's not a big deal at
 all. But you have to I mean, under the hood, allocate memory
 and the GPU and stuff like that. So it's kind of a little bit
 more expensive to do that compared to doing an in place
 operation. And in place operation is essentially
 modifying something in place without creating a new array. So
 it's slightly more efficient. It's not always possible to do
 that. But yeah, if you can do that, it's actually nice. So
 it's essentially the difference between, let's say, writing
 this and x plus equals one, in that sense, you are directly
 modifying something, whereas here you are having creating a
 copy and then assigning the copy to it. Anyways, it's a little
 tangent, the results are exactly the same, whether you do this or
 this. And in practice, you probably won't notice any
 difference anyway. So it does not really matter. But yeah, why
 not doing it? All right, so small tangent. So we have
 convolution, batch norm, and then this relu, this is this
 part is really the first three. And then we have another
 convolution and a batch norm, which is this part, notice that
 I'm going from one to four, and then back from four to one seems
 kind of weird. But why am I doing that? Yeah, it's really
 like to match the dimensions. Otherwise, I will have more
 channels here than I have as an input. So I would have four
 channels here, and then one input channel. And that doesn't
 really work if we add them, because then it's not an
 identity anymore. Okay. Yeah. And we also have one fully
 connected layer. This is just to make this classifier. So yes, I
 am implementing this. So here, I was just defining or initializing
 these layers, the blocks, and he I'm calling them. So in the
 forward pass is really where things happen. So I save x as
 the shortcut here. So I'm saving this here, then I'm calling my
 block. So this part I highlighted here, this is really calling
 this whole block, right. So it's this whole block. And actually,
 I could have these are kind of redundant, I could have used the
 same. Okay. But then, of course, the weights are different.
 Anyway, sorry. So I call my block here. And then I have my
 relu function. And the relu function is applied to x plus
 the shortcut. So this part really is this part. So I'm I'm
 adding inside and then I'm applying the relu. So the relu
 that is what is shown here. And here I have this addition,
 right. So this is essentially one residual block. And then I'm
 repeating it. So why am I not using one here? Well, then it
 would be the same layer, it's that wouldn't work really. Okay.
 But the shape is the same, it's just we have different weights,
 right. So it's just like having two convolution layers after
 each other. And then we have this linear here, which is
 turning this into a classifier. Alright, so so the linear layer
 has output the num classes. And here I'm just flattening it. So
 I'm assuming what comes out of this block two has a
 dimensionality 28 times 28 is 784. Yeah, and then I'm running
 this, I'm just pasting my convenience functions that I
 usually have in my helper function, it's a slightly
 simpler version, because I'm not plotting anything, I just want to
 show you that this actually runs doesn't get great performance,
 because of course, it's a very naive implementation is also
 only the training accuracy, the test accuracy is 92%. So what I
 show what I mean is, how do I know that this is the actual
 number? I mean, I can think about it, I can look at this.
 But like I explained in the previous video, what I can also
 do is what most people do is just print x dot size, then you
 can oops, run everything here. Then can run the training and
 then you will see the size. Of course, you don't want to
 complete it, because it's annoying to have it here. So I
 just stopped it, I can see all it's 128 times 28. And that is
 what I can then copy and paste and then go here and put it in
 here. Right. So that's where this number comes from. And it
 is also where this number comes from. Okay. In practice, if you
 don't want to think about it too hard, and you are debugging
 things, I mean, it doesn't hurt to insert a print statement.
 That's what everyone is doing. Okay. Um, so we trained that.
 No, of course, I interrupted it. But suppose it trained, I mean,
 it trained before. So if I fix it, it would train. Now, the
 second part, now focusing on the more interesting part where we
 have this resizing here. So I'm implementing this a little bit
 differently now using a reusable unit, I call that a residual
 block. So I'm implementing my residual block here. And this
 one is implemented the same way. Now, it's a little bit more
 general, I have something called channels here, this is the input
 channels, or let's say the first number of channels, the output
 channels. And then here, I have one and two. So I am going from
 zero to 12. And I can, I mean, I'm not defining what these
 numbers are, I'm defining them later when I'm calling this, I
 can maybe briefly skip ahead. So I'm using this residual block,
 actually, in my convolutional network here. So I'm using it
 here. And here, I'm defining the channels, I'm going from one to
 four to eight. So
 yes, so that's what I'm doing here. So I'm going from zero,
 so one to four, sorry, one to four to eight. And then I have
 my shortcut, which goes also from zero, sorry, from one to
 eight. Otherwise, I wouldn't be able to edit because if let's
 say this is one channel, outcomes, eight channels, then
 this also has to be eight channels. Otherwise, I can't
 edit. So that's what's going on there. So my residual block
 is growing up again. So we can see everything. So my residual
 block 123. This is really on this part, these three first
 blocks. And then like before, the second blocks are this and
 this. Now, the difference is that I have different numbers of
 channels. And I can also reduce the size, right. So yeah, I
 have a stride of two. So that will reduce the size, I have to
 do a stride of two here to to match these dimensions. So yeah,
 I have to be a little bit more careful that the dimensions
 match also. Yeah, and then as before, I have my block, I have
 a shortcut, and both the block plus shortcut, they go into my
 relu function. So this is what I'm showing you here, this
 residual block is really this whole thing here. Yeah, and then
 I'm using my residual block, I'm initializing one residual block
 into another. So I have a network with two residual blocks,
 the first one goes from one to eight, and the second one from
 eight to 32. And the number of the size is here seven times
 seven times 32. So it's because we are also having the
 dimensions here, half and half approximately. So going from 28
 times 28 to 14 times 14, and from 14 times 14 to seven times
 seven. Yeah, this is essentially it. So that's how we implement
 this. So yeah, then we are training it trains here, and it
 performs much better than our previous implementation. But
 again, the goal of residual networks is really to go deep in
 the network in terms of the number of layers. So here, we
 only have two layers. So I mean, this is probably not a great
 network to use for other data sets. Here, we are just using
 MNIST. So if we want to use a more, I would say sophisticated
 data set, I'm actually only using cipher 10, because it's
 simple to lot. But if you want to use a different data set,
 ResNet 34 is a good choice. So this is the one, the deep one
 here, it performs pretty well going back here, it gets
 actually pretty good performance on an image net top one accuracy
 better than VGG, for example. And how does that work? So it's
 the same concept, like shown, sorry, like, shown here, except
 more, I would say more sophisticated implementation of
 that. So I could have implemented it by hand, but
 there's always the chance to make mistakes at some point. So
 why not using what's already implemented? So here, this is
 again, I'm using my helper functions, that's again, the
 same that explained for VGG 16. So everything is the same as
 for VGG 16. So I don't have to discuss everything again, the
 only new part here is really this part, the model. So here, I
 actually copied the code from this website, which is an
 implementation, the official pytorch implementation, which
 has different versions of ResNet, wide ResNet, regular
 ResNet, 18 layers, 34 layers, 100 layers, 152 layers, and so
 forth. I grabbed the code that is used to initialize all of
 these networks. So they have written some code that can be
 reused for different types of residual networks. So here was
 copying it and simplifying a little bit. So it's not that
 long. And then they have something they call the
 bottleneck. It's kind of similar to what I call the residual
 block. But we have here, and then I mean, it's relatively
 complicated, I have to admit, it would take me also a couple of
 hours to really understand how that is implemented. The most
 important thing is that it works. Many people are using it.
 So I'm kind of trusting that this is indeed working. So they
 have like a make layer method here or function here that
 creates these layers, it's a little bit more sophisticated
 than than my version. So and then in the forward method, you
 have these different layers. So each layer has also multiple
 conversion layers. That's how you get the number 34. And we
 can also use the torch flatten function here. That's actually
 something I should also maybe use more often. It's a more
 recent thing. So I could actually technically replace
 replace that one by flatten. So yeah, that could be replaced by
 flatten. But I still need to know this number anyways,
 because I have to put it here for the number of parameters. So
 even though we can put a torch flatten here, it's, it's not
 that much simpler. Yeah. So that is essentially it. So here, I
 would have to know still this number in this linear near the
 500 to 12. So I could technically also write this as
 torch view, minus one, 12, think blocks expansion here is one,
 this is only used for the other types of networks, other
 residual nets. So could technically also write it like
 this. But yeah, we have this nicer flatten thing. What's
 nice about flatten is everyone knows what to flatten, that it
 has a meaning that is more intuitive, maybe than saying
 view, minus one or something. Okay. Yeah, yeah, it's also the
 same code that I used for the VGG. And now it's training.
 Actually, we are using cipher 10 here. Let me open this one
 again. So here, I have 70 by 70 images scrolling up, sorry. Yeah,
 I have made it larger, because otherwise, the performance was
 really poor. I mean, all these types of networks are really
 implemented for bigger data sets, not cipher 10. I'm just
 using cipher 10. Because then we don't have to download a
 separate data set if you want to reproduce these results. And I
 showed you how you can use your own data set too. So in that
 way, shouldn't be an issue for you. But if you have questions,
 you can always ask, I'm happy to help with that. So here, with
 ResNet, we get approximately 48%, which is not much better
 than what we got with VGG 16. Here, it's also kind of the
 same. But notice, even though I use larger images here, it was
 at least at least faster to run 62 minutes versus 90 minutes.
 Okay, so if I if I would have made the images smaller here at
 the same size, it would have probably finished in like 30 or
 40 minutes. Also overfitting. So here might be a case for adding
 more dropout. So here we only have do we have actually dropped
 much? No, not really. We only have batch norm. So maybe could
 be added to could be adding dropout. Okay, so some results
 gets bird and frog wrong. What was the one that this one got
 wrong? Dear and frog interesting. So yeah, animal
 classes are still confusing. You can also project you can see,
 again, the square where it makes misclassifications between
 different animals. Again, the tech cats and dogs. And yeah,
 this is ResNet implemented here. Honestly, if you implement
 networks, you don't have to implement things from scratch
 unless it's for educational purposes, like for learning
 things. Usually when you find a paper, or read a paper with an
 interesting implementation, or thing you want to try, usually
 what people do is they would go on GitHub and search for the
 original authors providing the code for that paper, and then
 adopting this code. So you would technically not run it one to
 one, you have to probably make some modifications so that it
 works for your data set. But usually in practice, once we are
 working with these more complicated data sets, there's
 no doesn't make sense to implement this ResNet 34. Let's
 say completely from scratch, it's only another source of
 making errors. I mean, it's it's useful here as a thought
 exercise to do it with a simple case with two layers, where you
 have simple implementation with two layers, yet maybe makes
 sense to do that. But if you go deeper, ResNet 34, maybe use
 something that is someone has implemented saves you lots of
 time and pain in that way. Alright, so okay, this is
 ResNet, I think we are already at the 75 minutes. So we will
 continue next week with the all convolutional network, I already
 implemented this somewhere here. And then we will also talk about
 transfer learning, I have to still implement it anyway, not I
 have it here already. So we will talk about transfer learning
 also next week. Alright.