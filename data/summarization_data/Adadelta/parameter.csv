"So yes, lr is just starting learning rate. "
"However, note that, by default, decay parameter for Adadelta is zero and is not part of the “standard” arguments, so your learning rate would not be changing its value when using default arguments. "
learning_rate: A Tensor or a floating point value. 
"If you really want to use Adadelta, use the parameters from the paper: learning_rate=1., rho=0.95, epsilon=1e-6. "
"A bigger epsilon will help at the start, but be prepared to wait a bit longer than with other optimizers to see convergence. "
To match the exact form in the original paper use 1.0. 
"To print it after every epoch, as @orabis mentioned, you can make a callback class: "
and then add its instance to the callbacks when calling model.fit() like: 
"On the other hand, rho parameter, which is nonzero by default, doesn’t describe the decay of the learning rate, but corresponds to the fraction of gradient to keep at each time step (according to the [Keras documentation (hyper-link)]). "
Although as you can see in tensorflow [source code (hyper-link)] to achieve the exact results of Adadelta paper you should set it to 1.0: 
"Note that in the paper, they don't even use a learning rate, which is the same as keeping it equal to 1. "
