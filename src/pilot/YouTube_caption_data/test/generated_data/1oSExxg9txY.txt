welcome to my presentation this poster
is about the paper
joint negative learning and positive
learning or noisy labels
uh it was presented in cvpr this year as
a poster session
this paper is about how to train deep
neural networks when training data is
mixed with noisy labeled data
there's a previous work called negative
learning for noisy labels
this paper proposed negative learning
and now indirect learning method using
complementary label as it prevents
overfitting to noisy data
it also causes underfitting in this
paper designed
three-stage pipeline for filtering noisy
data using both
nl and pl loss functions which is quite
inefficient and
impractical for actual usage
so in this paper we present joint
negative and positive learning
we propose advanced version of nl and pl
loss functions
which is nl plus and pl plus
and repropose unified single stage
pipeline for filtering noisy data
okay so we're going to start with nl
plus the reason for under fitting with
nl
comes from the gradient the gradient map
of the anode loss is shown below
along with distribution of training data
you can see the under fitting
when there's a lot of noise the sample
in the red box is the reason why
where the complementary label is
actually the ground truth label
in this case this image receives the
gradient causing it to have probability
of uniform distribution
to solve this issue we propose to
multiply constant weighting factor
to the existing analyse function with
the same noisy sample shown before
you can see the gradient received for
this sample is reduced
this change the gradient is shown in the
upper left corner of the gradient map
so you can see that nlplus shows better
conversions than nl
without any extra step
next one is the pl plus just like the
previous work
positive learning is applied only to
training data with high confidence
in plus candidate samples of high
confidence
are selected followed by bernoulli
sampling to make it stochastic
we also analyzed the gradient coming
from the previous po
you can see on the graph on the left the
sample with higher confidence
receives lower gradient which is ironic
because the purpose of pl
is to accelerate the training of
confidence angles
so we apply another constant weighting
factor to the pl
loss function to give higher gradients
to confidence samples you can see plus
receives
higher accuracies and faster training
speed in the graph on the right
next up we compare jnpl to nlnl
first up jmpl achieves faster and better
convergence
because nl plus and pl plus are done as
a single stage
secondly jnpl generalized to real-world
noise
consider when samples are mixed between
similar classes
you can see that only nlplus distinguish
the data
when neither nl or selective anal fails
it's because the gradient of nlplus is
reduced
in smooth manner from the center point
of the gradient map
thirdly jmpl outperforms in filtering
noisy data
which is shown by average precision on
multiple data sets
know each side voice types and ratios
finally we have the experiment result
comparing with other baseline methods
table on the left shows our method
achieves state-of-the-art performance
we have also done the experiment on
closing 1m
which is the large-scale data set of 1
million closing images
with the noise actually made by the
people which is shown
on the table on the right the table on
the right
shows that our method achieves
comparable results demonstrating
successful application of our method
to the real life situations
as a conclusion we propose joint
negative and positive learning
the next version of nlnl we developed
nlplus and plus
enhancing convergence and training speed
and represent
novel single stage pipeline for
filtering noisy training data
our method is stable and robust in
various types and ratios of noise
and our method does not rely on any
prior knowledge
making it practical to use in real life