Tanh seems maybe slower than ReLU for many of the given examples, but produces more natural looking fits for the data using only linear inputs, as you describe.  For [example a circle (hyper-link)] vs a [square/hexagon thing (hyper-link)].
Most of time tanh is quickly converge than sigmoid and logistic function, and performs better accuracy [[1] (hyper-link)]. 
Update in attempt to appease commenters: based purely on observation, rather than the theory that is covered above, Tanh and ReLU activation functions are more performant than sigmoid. 
For example, try limiting the number of features to force logic into network nodes in XOR and [sigmoid rarely succeeds (hyper-link)] whereas [Tanh (hyper-link)] and [ReLU (hyper-link)] have more success. 
 In the Deep Neural Network, which I mentioned earlier, this affine and tanh process is repeated three times.  This combination of Affine and Tanh can be used once to function as a Neural Network for one layer. 
