um so today we're going to look at
the uh one of very the best uh
rnn hack the tutorial slash blog post i
have ever found
i even don't remember how i found it but
this guy uh
danny jar i'm not sure if i spelled it
correctly
he even have the uh the
uh le the bibtex situation so
he wants himself to be citated actually
you can do a quick check if if
if you can cite him
so if i let's let's go to the
um semantic
scholar
let's see if some because this is so
good and let's see if somebody is
sighting it
so i also saw in the
pytorch blog and are not blatant blog
but
in python forum that they are
recommending to implement those some of
those as defaults
[Music]
okay probably need to look for that
reference specifically it was not
to find is it so easy to find it
i anyway i would not be surprised if the
people are referencing to this
so the idea is that uh he gives some of
the
uh tricks to improve the uh performance
of the lstms and also the
references basically any recurrent
neural networks references how to
improve that
so one of the things is uh the so we can
go step by step
some of them we will not implement right
now
but in general uh all of them are quite
good
so the source code is available on the
left side by the way
so the one is that they recommend to use
the adaptive learning rate and
uh so you already know about them
so this is uh i think just one comment
regarding this because
many uh state of the art models use
these learning grade schedulers
that actually are not really friendly
with the adaptive learning rate
optimizers because you can imagine the
poor guy
adaptive optimizer wants to estimate the
gradient
like acceleration in some
some parts of the learning space and
then if you have the scheduler
you you troll him off all the time so
anyways
so the rule of thumb in general would
not be used to use adam or
these adaptive cool optimizers together
with the learning grade schedulers
the learning reschedulers work good with
the
uh simple sgd with momentum
but at the same time from my experience
always much better to use adaptive
learning right schedule
learning rate rather than those
schedulers
so if somebody have different experience
you can share that that'd be interesting
but my experience is that then the
gradient clipping
that is often used in reinforcement
learning so
that you can easily like clip the as the
max value of the gradient at any stage
of your function graph
but that's not we're not going to use
that now then
normalizing the loss to get losses
similar magnitude across data sets
divide them by the maximum sequence
length all this is
quite fun thing i did not notice before
so to get losses similar magnitude like
your
your data sets so if you have different
uh
sequence lengths they he like
advise to divide the loss by the maximum
sequence length
i guess of the batch probably so
that that that sounds quite cool there
then
turn okay truncated back propagation
that's uh something that
is already by default recurrent neural
networks can hard time uh
noisy ingredients vanishing gradients
right okay yeah so this is also cool
idea
so because the long sequences you can
have vanishing radians
for the uh like some of the uh
earning models like last time we looked
at phase dallas time that
don't have this problem but in general
the one way how to
reduce this problem is by using
overlapping sequences like with some
kind of step
then long training time this is also fun
the uh some of the lstms converge
after longer time period like when you
have the
loss not actually changing much but the
the accuracy and quality still improves
so then multi-step loss
okay so this is a little bit tricky when
you have
the teacher and the learner
so two models this we're not going to
use but now we're starting to uh
do some things that we're going to do
today so about the
network structure so one is
to use a guyrow will not do that today
but
the garu is a little bit smaller
like so so it would be very easy to
convert what we're going to do today to
to go
to green units
then the other thing is layer
normalization so this
is something that uh the fun thing i
think that
layer norm when this is like a layer
norm is is basically the same as group
norm it's just that
the uh the the group
is the one if you look at the paper i
think those guys actually mentioned
in their own paper that this is uh used
with
lstm so when they invented layer norm i
think the layer norm was in general
invented before the
group norm and the instant like i'm not
sure like the order
2016 they they specifically say that if
you add layer norm to all
linear projections in in lstm
you get quite quite cool results
basically
you can see that uh for example here uh
on some of the
steps the convergence is like uh so
so much better with the with the these
layer norms
and also there was a lot of tasks or
they showed that
the accuracy and everything is much much
better if you just add layer norms for
all of the uh linear normalizations
linear uh projections
inside the ls uh the lstm or gaiaru
doesn't matter
so let's do that from
from my experience when we tested all of
these things
with our models of different kind of
problems
the layer norm was a huge impact so you
can get
i think like if like in some car data
sets we got
three to five percent improvement just
by adding the layer norms
and if you read again in the python
forums they they did discuss and i think
even
i think if i remember correctly they
even have it already
so the layer norm
uh lstm pytorch that they have already
implemented it
if i remember correctly inside
the official branch at somewhere
so uh
custom lstm benchmarks
okay i was not able to find it quickly
but if i remember
correctly they had it oh there are some
interesting lstm variants
so anyways so this is what you need to
do anyway so let's let's do that in the
code
i will go to the code
so i can type in chat as well if you if
you want to ask something or
you can also try to jump in in the the
conversation
so let's see
so i added also some to do somewhere
let's let's try to find
did i add to those or not
okay i did not add to those okay so
we can do that anyways so the model
the model so we have multi-layer lstm
here
and we have the cell here the actually
this is a phased
lstm cell pays the last time so
but but it's not actually face the last
time sorry for that
sorry for that it is remember that it
does not fail yeah
so let's let's rename this so
rename this uh rename this to
uh sorta lstm
state of the rtelstem
so i just use the uh pycharm
rename function i also will not need
this function
so it actually doesn't matter you can
leave it as it is but i forgot to change
the name there
that's why i wanted to fix it
so we'll need to add the layer norms for
different for all of the linear
projections
to make this
to make this exactly right as it was in
the paper
we should have the layer norms for
for every linear projection separately
but as you know
some of them are tied together with that
equation
where you try to project linear uh
projection directly on all of the gates
at once
so there is this this first part of the
projection and then the second part
so in a classic implementation you even
have just one part because
you don't have peep holes so in this
presentation we also have p
calls that's why we have two uh these uh
uh projections so you can actually have
uh if you have a simple
lstm you can have just one projection
without the gates at once
because because there is no peepholes
and then
basically you could have the layer known
for each of the sections of that
but in my case i just don't
want to change that too much
so let me look at that so i had
um so one option is just to
write it down now just a second
um
so i'm not sure did i maybe i
accidentally shared
with you some
wrong code
okay i think but okay now that's fine
i'm just checking with my my own code
that i did before
just a second so if i will not be able
to find it
then we'll have to do it from scratch
funny
just a second i thought
i did it so if i if i will not find my
my own
uh notes then i will just code it
live and then probably i will have some
bugs but
uh but but uh just give me a second
that is funny that i somehow
lost it hmm
okay this is not expected so let's do it
live
so i somehow lost my my notes but
still i can do this i believe so
okay so one is that we need those layer
norms so
so the layer norm so this is going to be
the
layer norm
layer norm one
so we'll have two layer norms because we
have
two parts of these calculations and
so i thought i don't want to split this
in three
parts so that's why i will just use the
group norm so it will be
similar idea torch and n
group norm and the groups will be
of course num groups will be
three because we have three parts of
that so
this is not specifically saying that
they will
form the correct layer norms but in
general
the capability is the same basically so
here will be
also with the group norm add the number
of channels
the number of channels now
let me think so we'll have the three
groups
we'll have the
the number of channels i believe it
should be hidden size
but we'll see so if i add the hidden
size here
and let's test this also
in in the bottom part here and then i
will
also pause so you can catch up i will
show you
back this display so the
uh there is the linear projection
so just after the linear projections
you want to add the layer norms to
center them
around the hive this this data space
so gates forward
gates and now let's check
if there is no bugs so just to check
okay
yeah so maybe at some point well you
will catch up
i will look for my note that's funny how
i lost them
so okay running what is happening
i press run let me check
data project data
there is a data so
the max length okay so i will
probably reduce the max length just to
make sure that it's running
and not something is not working also
that
there isn't everything working okay
i'm running this on cpu
on google call up we can run this on gpu
so i added from this
yeah okay
so
yeah i want to see my error i'm not sure
why it's not working
so just a second
what is happening not sure maybe i
updated my
my code somehow sometimes
really not sure okay i'm running the
wrong code sorry
so i'm running the wrong code okay i had
to change this
okay so and i also want to just
make sure that i had not lost my notes
or did i
lose them find
the layer norm
maybe i have them somewhere
so layer norm lstm symbol
simple lstm
layer norm
simple layer known peepholes okay
interesting
i did not see those just a second
i will find my notes and then we'll then
then it will be much faster
so isn't there because of
group nor mostly is used for
three-dimensional that
we have only two dimensions like dutch
and features
should not be should not be the problem
because it kind of matches up we have uh
three times 64 it's it matches with the
192
so i don't know what's the problem just
second
we can debug of course but uh
let me check
it's definitely not what i expected it
doesn't matter
okay um
the group normal number channels number
of channels okay
um
number of channels groups okay division
okay i guess it cannot divide that
probably
okay so okay so let's
let's back up so let's use the layer
so this will not be exact implementation
but still it will be better than nothing
so the idea would be that we can use the
uh we can use the separate layer norms
for each of those or maybe
let's do that let's do that otherwise it
will be not
fair so let's let's use then the four
layer norms
so this will be linear one this will be
hidden size
of the num
so there it should be something what is
the parameter name
the parameter name is normalized shape
normalized shape but it doesn't matter
you can just have the hidden size here
so this will be for the first gate maybe
you can even
call them with the gate names with
the gate names what were the gate names
here
so there will be input gate norm input
for get gate then there will be
the output gate
and then it will be the sea gate or the
cell
update gate so this is going to be then
the
sea gate so you have three uh four
laners
and these four layer norms then we can
use so maybe i will drop them in the
chat
this this part of the chord if by some
reason
somebody don't catch up with that so
i dropped that part in the in the chat
so
now here we can split out them
and use them so
the input the input even we can
the more easier way probably is not not
to
split them but just add before the
sigmoid here
so just like add it like that
so it's funny that i lost my notes and
not now it will be
more improvisation so here is the
for get gate then
there will be the output gate
but i guess you can do this with group
norm as well but
we'll be fine with this one so these
ones
then we need the seagate
the cell state gate stuff here
around this one and
maybe even better i will i will share in
the chat
not the score snippets directly but the
screenshot so it's a little bit easier
to follow because the
the formatting then is not clear so
if we add it like this i will also run
it and check it
i believe it should work but maybe i'm
wrong
so i'm trying to share
this code
so i'm generating a link
so the link is ready in the
chat the link for that bottom part
okay so i'm trying to run it
okay now it works so this this works
so i shared the code also in the chat
and also let's a little bit scroll
up and share also this part in the chat
with with the screenshot
so you have two screenshots and when
you're ready with the layer norms then
a post plus sign in the chat so
like that so our
layer norms okay
plus minus if minus you can ask also
some question
meanwhile i will try to look for those
notes
so i hope you can catch up with this
this
this part of the code
okay i found the notes so
i will also show you uh that part here
so if you would like to have the group
norm there for the whole layer
then my my problem was that the norm
channels was actually not the
output channels paired the pair every
um per every uh this this
um this group
but actually the correct one was this
one
so uh you you should have three groups
and then the hidden size times three
and then it would work so it would be
the it'll be possible to divide
the this one by three and it then it
will be no error
so but this this method with the four
layer norms is much better
is it's like more cl is is exactly like
in that paper
so that's cool so
let's see how you're succeeding i don't
see any pluses
maybe i was a little bit too fast um
yeah is anyone trying to do this or
should i go
okay so are you also the walters is
posting that
he's done with this and i will wait a
few
a little bit and then show the other
other parts
okay so
i hope that uh using those links at
least you can catch up a little bit
later or or yeah if you have some
questions or problems you can also
help so i will share the again the
the blog post so the next thing
is the feed forward layers first
so pre-processing the input with feed
forward layers
allows your model to project data into
space that is easier for temporal
dynamics
so that is also true we tested that that
if you add the linear projection before
the rnn
actually the funny thing is that i
remember that
when we looked at some of the
transformer models they also
used some linear projections before the
feeding in the attention mechanism
and so in this model actually i think we
already have that
so let me check if we have that
so the idea is that
here here we have the input features by
hidden size
that means for every time step we do the
linear projection
feed forward before the lstms
so that is that is already implemented
here
so this is this is really good also way
do not
not to forget not to forget those
uh those uh projections before the rnn
then the next fun thing that i think
this was really cool
is uh uh the stacked uh recurring neural
networks
this is i think really cool there so
it's interesting that
for for this one they don't have the
reference uh
i'm not sure like why there should be
some paper about it
so recurrent neural networks need a
quadratic number of weights uh
uh in their layer size uh so
and it can be more efficient to stack
multiple layers than having one big one
so it means that
it's better to have smaller lstms
with a hidden size let's say 128 or 64
or something like that
than just having one large lstm with
hidden size of thousands so
that is one thing and the other thing
cool is some the
outputs of all layers instead of using
just the last one
similar like resnet or dense net so this
line i think is really cool
so it's in there that you can have the
skip connections between the
lstm layers or any other recurrent
model layers so we can do that uh let's
try to do that
so it's basically
let's put even i don't know how to say
it's just uh
like some kind of a resonant type of
lstm and it also works
really well so let's let's look at that
so we have the layers stacked here with
the sequential
that's fine
and the change will be actually very
small i think
so let's see so we have
lstm called here that was team called
here
so let's see they have z1 z0
then we can have the z1 layers
the um also one one small
new ones if i scroll in the top part now
we have
num layers one please set num layers to
let's say
at least two or maybe three or
four so num layer should be more than
one because otherwise it doesn't make
sense this is there
so num layers one
now ze1 in the forward pass
of the model lstm
z1 all so this is going to be the not
all but let's say layers
layers so we have here we will collect
all of the hidden states in between the
layers
so let's collect those hidden states
so for in range
of lstm layers
and we can access actually sequential
model we can access
oh sorry not like that but e we can
access sequential model by index if i
remember correctly
so z one layers
append uh the um
itself lstm
these will be the layers actually and
then if i add the index here
e forward forward
and we need some kind of that z
z zero and we append those to the layers
and then
replace that by the last value so
z zero is z 1
layers the minus 1 last last value
that's cool so now after
this one you'll have options okay we
don't need this line as well so the
options
are following so we can either do the
dense net or resonant
uh type of actions so one is we can
concatenate them
dance and that would be uh concatenate
concat or we can use the res
net approach of some or we can do both
if we want
so or we can do attention as well so
attention
that is another option you can just
learn some kind of weights with soft max
and add it here
so uh multiply it here
so what here we can do then let's do the
resnet type of thing so the easiest way
for the res net that would be
a layer size independent i think would
be to calculate the mean value
so it's actually interesting that we
already calculate
the mean over the temporal pooling
but we can calculate it again also here
so
first we need to stack them torch
stack stack
let's stack those z's z1
layers over dimension of
one because that is the feature
uh i'm not yeah let me let me think
so is it a feature patch sequence
so that is the sequence dimension so let
me check i will
debug this a little bit so then
z 1 torch
mean so we can calculate the mean value
of all
layers basically and the the division
part is
is is good because we can change the
number of layers and
reduce a little bit the impact of that
or another option is we can use the sum
here and then we need some kind of
normalization after that
let's say layer norm if we add the sum
plus layer norm it will work just as
fine
or we can use the mean so there are a
lot of options that you can use but the
idea is that you have skip connections
so i will add the breakpoints here and
just
double check if this is correct i slowly
scroll
up sorry for that trying to
run it now and see
maybe the lion 184 maybe that's not
correct i'm not
totally sure about that i hopefully you
can select it out like that
um
so i'm running it
hmm takes quite a long
time
another way to get this without the
sequential
i'm mostly using with these cases torch
and a module list
you can use it as a list and for the
modules and you can iterate through the
layers
without indexes oh what else is that's
nice
yeah that's good that could work that's
really good there yeah but it worked
also like this
um so but but yeah walters is right
that is that is maybe better um option
to use
and so here is then we try to stack it
so i just want to double check
about the actual dimension of what we
want to stack
so the the first one will be the layers
basically
okay so and then we'll let me check cell
0
size will be
16 to this batch size or
sequence features and then if i
stack it the z1
shape is 16 but yeah that's cool
so we created the new dimension
basically we created a layer dimension
so okay let's let's write it down so
here
oh man my okay so
here we created the uh this is actually
batch layers
then the sequence
and then the features so
now we have again options what to do we
can sum the layers as in
resonant and that then add the layer
norm or we can use
my approach of having the mean value so
and then after the mean value we will
eliminate
again this dimension we will uh reduce
it as a mean value
of those all of those features
with every sequence time step so it will
be like that
and then the next thing is as you see
here
so it's uh actually not the hidden size
basically features doesn't matter
is that we reduce every sample just the
features within
the next line and this is a data this is
a temporal pulling
tempo rail pulling so one thing that
often is used here you can add also
attention to
look at what exact time step you want to
pay
more attention just learn one weight
matrices
with the and then apply salt max and
then multiply that
that's another option but this will be
the simplest one
so the simplest one i think works
so if i uh now just try to run it again
and then i will give you this this this
snippet of the code
this is really neat a very easy trick
to improve the gradient flow so but the
the one trick
one thing that i think is quite
important is that you
don't just jump over to the to the zero
if you just
have the um like a link over the
lstm at all i think that would not be
beneficial then it would
start learning just the patterns not the
time sequences
so my advice would be not to jump over
the first layer
but maybe even as i say this could be
interesting research topic
how like at what point you should start
jumping you know maybe
uh first couple of layers you should not
have the gradient
uh skips but then at uh let's say the
last
layers maybe if you have this gradient
skips it would be beneficial
that's a small interesting research i
think so and also these pooling methods
they are also
i think quite interesting so uh if you
yeah maybe uh i'm not sure like if you
are really interested in those pooling
methods maybe i can do a small workshop
next time
about the pulling methods about the
attention mechanisms
so let me know in the chat if you want
to have it attention mechanisms in rnns
so okay i see that uh walter's shared
the
module list code that's cool so i will
share also my
my code about the
this part right so this is
this is ready so i shared the code of
that
um yeah maybe let's take a small pause
so
is the i'm writing in the chat is the
um what does it call like rnn
resin style skips
ready skips ready
by the way the skips are there are so
many options what you can do you can
even have
to have it pure isn't it type of skip
you you can have the
like that they could be let's say uh
added here i think as well
so this is z here maybe you can have
something like
this z plus zero
so oh man i didn't want to do this so
z plus uh z zero
so maybe you could do this as well uh so
there are a lot of options but if you do
this i think you should have
then layer norm after this one so
for somebody looking for a small
research there there you have like you
can
you getting you can do a lot of
interesting things with these uh
like skips so you can do even that
okay let's let's uh i see two pluses
that's great
uh let's move back to the uh
great blog post that i really like
so yeah then
the learned initial initial state so
this is really coolly there
so i i bet uh all of you have thought
about this that
when you have lstms uh why why do why do
we need to initialize the
the first hidden state as zeros why
wouldn't
like by the zeros is always the best
value you know
so that is true so maybe that he's
saying that
we can learn the initial state uh from
the basically your training data set
and then we can predict uh like uh the
the best uh
initial state with which to we could
start with any situation
that's i think is really cool there so
it is referencing to this post
let me quickly open that post so
non-zero initial states from 2016
some kind of small research
okay here's a cool cool interesting
stuff so
the zero losses then
there is a noisy loss so that the this
is
initialized as noise and then there is a
noisy variable so basically you
initialize it
as noisy and then learn it
so let's do that that will be fun so
learnable initial state i think that's
that's really good
there so let's let's move back to the
code
so they we already know how to make
things
learnable right because uh because we
already have
uh a lot of parameters right so
using if you're following my these my
tutorials
basically you you already know that you
can have
parameter for everything so we can
easily make the learnable states so
let's do that so the learnable states we
can do
by self h
so i think also the trick is that you
need to initialize them
uh noisy uniform probably so
h 0
if you initialize them as zeros probably
it will not learn
as good uh and and parameter
parameter and now there is a nasty thing
so the nasty thing is that
in order for this to work properly at
least you can in
in inference you can probably like hack
it but
you will need to have the same batch
size because of course
the uh let me think
actually the initial state so it's not
nasty thing actually let's
i this is my mistake you don't need you
don't need batch size
i almost made a mistake almost made the
mistake because here is the batch size
we just need to
extend it over the batch size because
that's the same learnable state for
every sample
so that's really cool so self
hidden i mean hidden state hidden size
hidden size that will be good then we
can
uh make it noisy we can make it noisy
with the same
deviation that before i think uniform
hidden state okay so this will be h0
that we will extend over the batch size
so that i almost told a not
good thing so hc so this is for a hidden
state zero state
zero uh and for the cell state
so we have two learnable parameters
and now we need to apply them so we can
apply them
by there are two methods repeat or
extend
the repeat as i remember was copying
extend is copying pointers
so let's see so here
h will be torch h
extend extend
i hope let me check if this is correct
extend extend extend was it extend or
some other
function let me quickly double check it
with
the pythage documentation
extent because i now see the repeat and
i can do this with repeat but i
i think the extent was better
was it extend tensor
okay let's see if i will have mistake
then we'll fix it
so size by hidden size
and this is the same for c
and also to device i for almost forgot i
know
we don't need to have device because
because it's already the parameter
so h and the c so
we're basically not it's not reshaping
its
copying pointers but i'm not sure about
this
extend so let's see if i add the
breakpoint here and try to run it
if it will work or not or maybe i will
just run the run
so if if i remembered correctly
there there was such a function
uh torch tensor
extend expand i can be okay
expand sorry i think it was expand next
not extend
or yes it was expand
expand size
from documentation you should use repeat
because no expand expand doesn't copy
data yeah we should copy the data now no
i don't
need to copy the data but also but
the funny thing is that uh does it
really matter
if it copies or not because uh
because for example here i copied the
data but we can just replace it quickly
but expand as well
[Music]
i'm not sure it should not affect the
gradient in my opinion but if it
does affect the gradient then you should
use expand uh
if somebody knows the answer that would
be interesting so
if i run it now yeah
if i i believe both of those should not
affect the gradient but
um but yeah i think in most cases you
could use
expand i just forgot about that function
before
so if i run it
yeah cool it works and even with this
super small sample it
also it doesn't matter actually there is
super small sample so anyway
so it works this is great so we have
learnable
uh hidden and the cell state
and as you saw in that blog post and in
that uh
publication that with this one you also
get
quite a big improvement so i think you
start
sensing that if you just combine all of
this stuff together with the
with the face that i'll stand before i
believe you can have
super like really powerful model that
could probably compete with
the transformers and i have seen
actually some
papers where they have all sorts of
variations of
lstms that can approach also the
transformers and transformers also have
some disadvantages
so okay this is good
let's see so let's let's
i posted the the shared uh code also as
a as a screenshot
so let's ask the question is the
learner bo h and
c ready learnable
hnc ready cool we have one plus so we
are almost ready
just a second yeah just bear with me
i have a few things to show you more
and hopefully then you can make your own
powerful models so
the forget gate so this is simple one
and very effective
so especially when you have long
sequences
if you have if you initialize them by
random the forget gates so if you
remember the forget gate will
uh measure like it will have the sigmoid
on top of it
and it will be multiplied by the uh
hidden state and will
limit the amount of information coming
from the previous time steps to the
current time step
so if you have long sequence in the very
beginning
it will be very disadvantageous if you
have
if you lose all of the information in
the very
beginning of the learning uh so
and then later basically the days that
if we
initialize the forget gates to once not
to
a uniform distribution and the same
there is for g rule to rest get rested
gates to the minus one
then we in the beginning of the
training we will encourage to remember
as much as information as possible and
then during the training
lstm will start to forget some
information that is not necessary
this is really a neat trick very easy
and very powerful
so we can do that so i will go back to
the code
i really like that blog post actually so
um the initialization
let's see so
we need to find the forget for get gate
so
so let's see so the forget gate
is the dimension of
of of shear here from multiplied by two
so this is for reget gate
uh huh so and
i actually there is one other thing
i forgot in the the blog post it says
need to initialize as once the be biased
because of course if we initialize the
matrix as one then uh the dot product
will
will make us like wonders but if
we initialize the bias by one then we
ensure that we will have the sigmoid
open
so the bias is here basically okay
that's fine and then we need to select
the
uh for get gate so the forget gate
for get gate is actually self b
uh data so we want to access
the tensor itself so we'll access the
tensor and then we'll have the hidden
size
uh selection out of here hidden size
times two so this is a this small slice
of the forget gate and we will use the
function
of fill and with the pie torches you see
that
if you want to in place operations you
can use this
small like this underscore again and
here we'll set it to once
like that and let me see if this is
working
if it's working then we can run it so
okay i will i will make a screenshot i
believe it's working
so we already finished all of these uh
hacks this is the next hack this is a
forget-gate hack
really good one so but also i must say
that this is not
always appropriate for every problem uh
especially if you have like
short sequences let's say up to 100 time
steps
um though maybe even the less than uh
then
maybe this will be even worse results
so i'm posting the the
uh link to the image and let me
no let me so i will ask the question
is the for get gate
hack ready so let me no
okay cool this is ready so
i'll stop it so we have the next hack
ready
uh-huh so and i think we have left only
one hack
also a very cool hack so
let me go back to the jamboard i'm at
the jamboard but to the
blog post so if your model is
overfitting use regularization methods
for
recurrent networks so there are specific
regularization networks for regulation
methods for acrylic networks
like one is that of course between the
layers you can
have the dropouts that is the classic
method
but i think there is something
interesting here is i remember
zone out was i think interesting one let
me check
um so actually there are and the other
one is also was a little bit
different i think so one is between the
layers dropout but
the the better option is to add dropouts
inside your
lstm cell architecture so let let me
quickly check these two papers so this
is from 2016-17
uh not sure what maybe there's something
newer after this one
but this is actually very similar
equation than it was for the phased lstm
so let me find it so here
basically the z is drop out and then
it's
multiplied by 1 minus z
to the h t minus 1 and then
z times h t basically there is the g
some some kind of i guess some kind of
i'm not sure
exactly where but basically the idea
this equation is actually very similar
to the phase the lstm
but only the only difference is that the
uh
phase the stem disease was was
this uh phase gate remember
phase gate where it leaves the untouched
previous time step or by some defined
phase
let something in the cell in this case
this is just a random variable
so that's this is quite cool there very
very easy to implement
so this is one option and the other
option
is about the zone out zone
out zone out
um so
i think again this was very sim similar
equation
here again there is a minus one
that multiplied by this uh this d
d is is again the random variable
and then we we just leave something from
t minus 1 or the current t
so i'm not sure really what is big
difference between
those two papers probably the other
paper use some kind of
g not sure exactly you like need to look
up what exactly how they calculate the g
here
with this linear projection not sure
anyway so we can do this we can
implement
this zone out approach probably so yeah
this will be the last one
so we'll go back to the code and
implement the uh
regularization method inside this cell
itself so that's also quite exciting i
think
so let's go back so
add the regularization inside the cell
so we can go to the lines in the
cell so the sota lstm cell the forward
pass we don't need to learn
anything regards to the dropouts of
course
dropouts are not learnable right
um so they'll they'll be here
so we need also some interesting stuff
here so one
is that let's say
let's see let's see so d there will be
the as in that paper dh
torch ones like so we will open
the gates basically
let me think so we will
how they compare drop out versus the
zone of idea
zone out is that you don't inject
randomness by like
random variables you some
randomly inject previous time steps
activation
it's not random it's from premium stuff
okay oh yeah that's interesting
um so the others is good at reading
papers very fast
not really good uh so he's telling that
the there is a difference that there is
some
randomness injected from previous time
steps so
here uh let's let's right with the with
the just with simple dropouts
like this so one thing is that we need
to take in account that is it the
training or
tray uh or or the evaluation
so of course the we don't want dropouts
to be enabled
for the evaluation so let me check
the training yes there is the variable
of training
oh man sorry i will move back so like
this
so except if we want to do the language
modeling
where the language modeling sometimes
you want the the dropouts to be
enabled in the uh evolution so so you
uh at least not in any relation but in
in the production so
the outputs would be more natural more
interesting
so th will be
f the dropout the torch functional
dropout
and here we can use basically
so we can use actually i think basically
the same thing of this
and the probability of the dropout let's
add is as
0.5 and also for this one
the h d c
so we have these two vectors
with draw parts like little
gates there so these little gates we
will
make inside this equation so here again
the same way as in the phase the lstm
almost exactly the same way
we can have it here as a prim so we just
uh we will just call these two
as a not as c but the c prime and h
prime because
the final c and h will be combination of
those like dropouts so here
let's see so the this will be the c
is a dc
multiplied uh the pair raised
multiplication
by c prime plus 1
minus d c raise mo
multiplied by previous time step c
and here this the same thing
just with the h h so we can just
replace all of the c's with h h
h h hopefully now
the pi torch like highlighting will
show that i don't have bugs or will i
okay looks good so and if i run it let's
see if it works
and if it works then i will share it and
hopefully then we can just quickly at
the last step i can run
this on the google column and
if you add all of the classes basically
it will be take a lot of time to train
but
you can play out like do your own
ablation study you can just remove all
sorts of
all sorts of the parts of the this this
new code and see
the effect of that so let's see it's
running
running hopefully there will not be
errors
this is really really interesting i
think
oh yeah it works so it works i will
add this part to the to the to the chat
and then we'll wrap up so
this part is also really cool
that we can uh make the
uh these uh hidden states more resilient
to the inputs so the dropouts and the
regulation
is the idea that you can avoid the
situation that it learns
exactly precisely those values
and then if something a little bit
changes then it starts doing
crazy things so r
dropouts okay drop
oh it's okay
cool so that that's that's the last step
and
now if i i'm shared also that code
now just a second i will go back i will
set up the
google column google collab
and just run a quick run to see that if
i don't have some bugs for the gpu
hopefully not and then we'll end this
session
very soon so sorry for the very
beginning that i lost my notes
if i lose my notes i also can do all of
this stuff but a little bit slower
slow so and also more bugs
without notes so
if i let me share the jamboard so the
dogs are
working in the top so here is the
jamboard
i just copied this same code here we
have the
uh condition that if you have the cuda
so if i run change the runtime to the
cuda
oops so then we will use the all of the
code so this is first thing
the second thing is we can add more
classes here
right 2 3 4 and so on to make this more
complex task
also we can change the input size you
can change all of these things as well
so if i run it let's see let's just
check if there is no major bugs oh the
cat is walking with the rose so
top to top
so it's running running running
downloading quickly quickly checking
running
now there is another thing that we not
not looked
with these workshops is that of course
if you have different
sequence lengths then it uh makes then
it's a little bit uh not so nice because
you then you need to
have the collation right collage collate
function where you
collect that uh the um like the
try to have the as much as many as
a similar sequence size as possible in
the batch
and then you need to also have the some
masks
or uh yeah probably masks
and stuff like that so so all of if you
have like
natural language modeling tasks where
you have different sequence lengths it
will not be so nice
but it's also possible to apply all
these things and they should work really
really good okay so it's working at
least there is no
immediate error it was trained really
slow
because this is not torch uh cuda and uh
like this cuda and then library where
they have
pre-compiled all of these things with
the old
approaches so this is quite new approach
so they have not pre-compiled all of
this
and yeah so that's that's i think it for
today
so i hope you enjoyed at least the
second part of our experiments
um next time i will try it not to lose
notes and
some like uh session i think maybe not
next one but the
one after that we will have uh some
people from latvian university are also
joining and showing some
interesting uh lstm variants that they
do
so if you have any comments then please
let me know and uh that's it for today
bye thank you