Let's compare errors being larger than 1.0 in case of MSELoss and SmoothL1Loss.
MSELoss would give it value of 100 (or 50 in case of pytorch implementation), while SmoothL1Loss gives just this value of 10, hence it won't punish the model so much for large errors.
In case of value below 1.0 SmoothL1Loss punishes the model less than L1Loss.
0.5 would become 0.5*0.5 so 0.25 for Huber and 0.5 for L1Loss.
Mean Squared Error - amplifies large errors and downplays the small ones, L1Loss gives errors "equal" weight let's say.
Experiment and check whether any specific loss functions exist for task at hand, though I think it's unlikely those experiments will give you significant boost over L1Loss if any.
If you really want to set custom threshold for MSELoss and L1Loss you could implement it on your own though:
Everything below threshold would get MSELoss while all above would have L1Loss.
