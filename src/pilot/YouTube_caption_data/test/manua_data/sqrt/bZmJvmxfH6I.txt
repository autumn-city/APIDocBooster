Hello!
Welcome!
Can the folks online see and hear okay as
far as we know?
No reply, I'm sure they'll let us know.
This week, obviously quite a bit just to get
set up to get results from reading in all
of ImageNet and getting all that working.
I know a lot of you are still working through
that.
I did want to mention a couple of reminders
just that I've noticed.
One is that, in general, we have that thing
on the Wiki about how to use the notebooks
and we really strongly advise that you don't
open up the notebook we give you and click
[SHIFT][ENTER] through it again and again.
You're not really going to learn much from
that.
Go back to that Wiki page, it's like the first
thing that's mentioned in the first paragraph
on the home page for Wiki, it's like How to
Use Notebooks.
Basically the idea is try to start with a
fresh notebook, think about what you need
to do first, try and do that thing.
If you have no idea, then you can go to the
existing notebook, take a peek, close it again,
try and re-implement what you just saw.
As much as possible, don't just [SHIFT][ENTER]
through the notebooks.
I know some of you are doing it because there
are threads on the forum saying, I was [SHIFT][ENTER]'ing
through the notebook and this thing didn't
work.
Well, that's because that thing's not defined
yet.
Consider yourself busted.
The other thing to remind you of is the goal
of Part 2 is to kind of get you to a point
where you could read papers.
And the reason for that is because you kind
of know the best practices now.
So anytime you want to do something beyond
what we've learnt, you're going to be implementing
things from papers, and probably going beyond
that and implementing new things.
Reading a new paper in an area that you haven't
looked at before is (at least to me) somewhat
terrifying.
On the other hand, reading a paper for the
thing that we already studied last week hopefully
isn't terrifying at all because you already
know what the paper says.
So I always have that in the assignments each
week, read the paper for the thing you just
learnt about.
And go back over it and please ask on the
forums if there's notation or anything that
you don't understand, or if there's something
you heard in class that you can't see in the
paper.
Or particularly interesting, you see something
in the paper that you don't see in class.
So that's the reason that I really encourage
you to read the papers for the topics we studied
in class.
I think for those of you like me who don't
have a technical academic background, it's
a really great way to familiarize yourself
with notation.
And I'm actually looking forward to some of
you asking about notation on the forum, so
I can explain some of it to you.
There's a few key things that kind of keep
coming up in notation, like probability distributions
and stuff like that, so please feel free.
And if you're watching this later in the MOOC,
again feel free to ask on the forum if it's
not clear.
I was kind of interested in following up on
some of last week's experiments myself.
The thing that we were kind of all a bit shocked
about was putting this guy into the DeVISE
model and getting out more pictures of similar
looking fish in nets.
It was kind of curious about how that was
working and how well that was working.
I then completely broke things by training
it for a few more epochs.
And after that I then did an image similarity
search again and I got these 3 guys who were
no longer in nets.
So I'm not quite sure what's going on here.
And the other thing I'll mention is when I
trained it from my starting point of what
we looked at in class, which was just before
the final bottleneck layer, I didn't get very
good results from this thing.
But when I trained it from the starting point
of just after the bottleneck layer, I got
the good results that you saw.
Again, I don't know why that is and I don't
think this has been studied as far as I'm
aware so there's lots of open questions here.
But I'll show you something I did then do.
I thought, well that's interesting, I think
what's happened here is that when you train
it for longer, it knows that the important
thing is the fish and not the net.
And it seems to be now focusing on giving
us the same kind of fish, like these are clearly
the exact same type of fish.
So I started wondering, how could we force
it to combine.
So I tried the most obvious possible thing.
I wanted to get more fish in nets and I typed
word2vec dict net plus word2vec dict fish
divided by 2, the average of the two word
vectors and give me the nearest neighbor,
and that's what I got.
And then just to prove it wasn't a fluke,
I tried the same on tench plus rod, and here's
my nearest neighbor.
Now do you know what's really freaky about
this?
If you google for ImageNet categories, you'll
get a list of the 1000 ImageNet categories.
If you search through them, neither net nor
rod appear at all.
Like I can't begin to imagine why this works,
but it does.
So this DeVISE model is clearly doing some
pretty deep magic in terms of the understanding
of these objects and their relationships.
Not only are we able to combine things like
this, but we're able to combine it with categories
that it's literally never seen before.
It's never seen a rod, we've never told it
what a rod (for a net) looks like.
I tried quite a few of these combinations
and they just kept working.
I understand why this works.
I tried searching for boat.
Boat doesn't appear in ImageNet, but there's
lots of kinds of boats that appear in ImageNet.
Not surprisingly, generally speaking it figures
out how to find boats.
I expected that.
And then I tried boat+engine, and I got back
pictures of powerboats, and then I tried boat+paddle
and I got back pictures of rowing boats.
So there's a lot going on here and I think
there's lots of opportunities for you to explore
and experiment, based on the explorations
and experiments that I've done.
More to the point, perhaps to create some
interesting and valuable tools.
Like I would have thought a tool to do kind
of an image search to say, Show me all the
images that contain these kinds of objects.
Or better still, maybe you could start training
with things that aren't just nouns but are
adjectives, so you could start to search for
pictures of crying babies or flaming houses
or whatever.
I think there's all kinds of stuff you could
do with this which is really interesting,
whether it be in a narrow kind of organizational
setting, or to create some new start-up or
new opensource project or whatever.
So anyway, lots of things to try.
More stuff this week.
I actually missed this, this wasn't this week,
but I was thrilled to see that one of our
students has written this fantastic medium
post, Linear Algebra Cheat Sheet for Deep
Learning.
I think I missed it because it was posted
not to the Part 2 forum, but the main forum.
This is really cool.
Brandon has gone through and really explained
all the stuff that I would have wanted to
know about linear algebra before I got started.
And particularly I really appreciate that
he's taking kind of a code-first approach.
So like how do you actually do this in NumPy
and talking about broadcasting.
So you guys will be very familiar with this
already, but for your friends who are wondering
how to get started in deep learning, what's
the minimal things you should know -- it's
probably the chain rule and some linear algebra.
I think this covers a lot of linear algebra
pretty effectively.
Other things from last week.
Andrea Frome, who wrote that DeVISE paper,
I actually emailed her and asked her else
she thought I should look at.
She suggested this paper, Zero-Shot Learning
by Convex Combination of Semantic Embeddings,
which she's only a later author on but she
said is in some ways a more powerful version
of DeVISE.
It's actually quite different.
I haven't implemented it myself, but it solves
similar problems and anybody who's interested
in exploring this multi-modal images and tech
space might be interested in this.
And we'll put this on the Wiki, of course.
And then one more involving the same author,
in a similar area, a little bit later, Attention
for Fine-Grained Categorization.
A lot of these things, at least the way Andrea
Frome was casting it, was about fine-grained
categorization, which is how do we build something
that can find like very specific kinds of
birds, or very specific kinds of dogs.
I think these kinds of models have very very
wide applicability.
So I mentioned we would kind of wrap up some
final topics around computer-vision-y stuff
this week before we started looking at some
more NLP related stuff.
One of the things I wanted to zip through
was a paper which I think some of you might
enjoy, Systematic Evaluation of CNN Advances
on the ImageNet Dataset.
I've pulled out what I though was some of
the key insights, some of these are things
we haven't really looked at before.
One key insight, which is very much the kind
of thing I appreciate, is that they compared
what the difference between the kind of original
CaffeNet plus AlexNet versus GoogLeNet versus
VGGNet on two different sized images, training
on the original 227 or 128.
And what this chart shows is that the relative
difference between these different architectures
is almost exactly the same, regardless of
what size image you're looking at.
And this really reminds me of in Part 1, when
we looked at data augmentation.
And we said, You can understand which kind
of data augmentation to use and how much on
a small sample of the data, rather than the
whole dataset.
And what this paper is saying is something
similar, which is you can look at different
architectures on small sized images rather
than full size images.
And so they then used this insight to do all
of their experiments from then on using a
smaller 128x128 ImageNet model, which the
said was 10X faster.
So I thought that was the kind of thing which
not enough academic papers do - what are the
hack-y shortcuts we can get away with.
So they tried lot's of different activation
functions.
It does look like maxpooling is way better.
So this is the gain compared to ReLU, but
this one actually has twice the complexity
so it doesn't quite say that.
What it really says is that something we haven't
looked at, which is ELU (which is very simple,
as you can see here - if x>=0, then y=x, otherwise
it's e^x - 1).
ELU basically is just like a ReLU, except
it's smooth.
Whereas ReLU looks like that, ELU looks exactly
the same here, and then here it goes like
that.
So that's one thing you might want to try
using.
Another thing they tried which was interesting
was using ELU for the convolutional layers
and maxout for the fully-connected layers.
I guess nowadays we don't use fully connected
layers very much, so maybe that's not interesting.
The main interesting thing here I think is
the ELU activation function.
2 percentage points, quite a big difference.
They looked at different learning rate annealing
approaches.
And you can use Keras to automatically do
learning rate annealing.
What they showed is that linear annealing
seems to work best.
I tried something else, which was what about
different color transformations.
They found that amongst the normal approaches
to color, RGB actually seems to work the best.
Then they tried something I haven't seen before
which is the added two 1x1 convolutions at
the very start of the network.
So each of those 1x1 convolutions is basically
doing a linear combination of the channels
and with a non-linearity in-between.
They found that that actually gave them quite
a big improvement.
That should be pretty much zero cost.
So there's another thing which I haven't really
seen written about elsewhere, but it's a good
trick.
They looked at the impact of batchnorm, so
here is the impact of batchnorm, positive
or negative.
Actually adding batchnorm to GoogLeNet didn't
help, it actually made it worse.
So it seems with these really complex, carefully
tuned architectures you've got to be pretty
careful, whereas on a simple network it helps
a lot.
And the amount it helps also depends on which
activation function you use.
So, batchnorm, I think we already know that
now.
Be careful when you use it, sometimes its
fantastically helpful, sometimes it's slightly
unhelpful.
Question: Is there any advantage to using
fully connected layers for classification?
Answer: Yes, I think there is.
Like it's terribly out of fashion, but I think
for transfer learning they still seem to be
the best.
The fully connected layers are super-fast
to train and you seem to get a lot of flexibility
there.
I don't think we know one way or another yet,
but I do think that VGG still has a lot to
give us in terms of the last carefully tuned
thing with fully connected layers and that
really seems to be great for transfer learning.
Comment: ELU's advantage is not just that
it's smooth but that it goes a little below
zero.
That's a great point, that you for adding
that.
Anytime you hear me say something slightly
stupid, please feel free to jump in because
otherwise it's on the video forever.
On the other hand, it does give you an improvement
in accuracy if you remove the final maxpooling
layer, replace all the fully connected layers
with convolutional layers and stick an average
pooling at the end, which is basically what
this is doing.
So it does seem that there is definitely an
upside to fully convolutional networks in
terms of accuracy, but there may be a downside
in terms of flexibility around transfer learning.
That's a little unclear still.
I thought this was an interesting picture
I hadn't quite seen before.
Let me explain the picture.
What this shows is, these are different batch
sizes along the bottom, and then we've got
accuracy.
What it's showing is with a learning rate
of .01, this is what happens to accuracy.
So as you go above 256 batch size, it plummets.
On the other hand, if you use a learning rate
of .01 times batch_size/256, it's pretty flat.
So what this suggests to me is that anytime
you change the batch size, this basically
is telling you to change the learning rate
by a proportional amount.
Which I think a lot of us have realized through
experiments, but I don't think I've seen explicitly
mentioned before.
I think this is very helpful to understand
as well is that removing data has a non-linear
effect on accuracy.
Here's this green line here, here's what happens
when you remove images.
With ImageNet, down to about half the size
of ImageNet, there isn't a huge impact on
accuracy.
Maybe if you really want to speed things up,
you go 128x128 sized images and use just 600,000
of them, or even maybe 400,000.
But beneath that, it starts to plummet.
So I think that's an interesting insight.
Another interesting insight, although I'm
going to add something to this in a moment,
is that rather than removing images, if you
instead flip the labels to make them incorrect,
that has a worse effect than not having the
data at all.
But there are things we can do to try and
improve things there.
Specifically, I want to bring your attention
to this paper, Training Deep Neural Networks
on Noisy Labels with Bootstrapping.
What they show is a very simple tweak you
can add to any training method which dramatically
improves their ability to handle noisy labels.
So this here is showing if you add noise from
about .3 to .5 to MNIST, up to half of it,
the baselines are doing nothing at all.
It really collapses the accuracy.
But if you use their approach to bootstrapping
you can actually go up to nearly half the
images, intentionally changing their label
and it still works nearly as well.
I think this is a really important paper to
mention in this like stuff that most of you
will find important and useful area because
most real-world datasets have noise in them.
So maybe this is something you should consider
adding to everything that you've trained,
whether it be Kaggle datasets or your own
datasets or whatever.
Particularly because you don't necessarily
know how noisy the labels are.
Question: So noisy labels means incorrect?
Answer: Yes, noisy just means incorrect.
Question: So bootstrapping improves it?
Answer: Yes, this paper describes a particular
technique which you can read during the week,
if you're interested.
Interestingly, they find that if you take
VGG and then add all of these things together
and do them at once, you can actually get
a pretty big performance hike.
It looks in fact like VGG becomes more accurate
than GoogLeNet after making all these changes.
So that's an interesting point.
Although VGG is very very slow.
There's lots of stuff I notice they didn't
look at.
They didn't look at data augmentation, different
approach to zooming and cropping, adding skip
connections, like in ResNet, or DenseNet or
highway networks, different initialization
methods, different amounts of depth.
And to me the most important is the impact
on transfer learning.
So these to me are all open questions, as
far as I know.
So maybe one of you would like to create the
successor to this, More Observations on Training
CNNs.
There's another interesting paper, although
the main interesting thing about this paper
is this particular picture, so feel free to
check it out.
It's pretty short and simple.
But this paper is looking at the accuracy
versus the size and the speed of different
networks.
So the size of a bubble is how big is the
network, how many parameters does it have?
So you can see, VGG16 and VGG19 are by far
the biggest of any of these networks.
Interestingly the second biggest is the very
old AlexNet.
Interestingly, newer networks seem to have
a lot less parameters which is a good sign.
Then on this axis we have how long does it
take to train.
So again, VGG is big and slow and, without
at least some tweaks, not terribly accurate.
So again, there's definitely reasons not to
use VGG, even if it seems easier for transfer
learning, or we don't necessarily know how
to do a great job of transfer learning on
ResNet or Inception.
But as you can see, the more recent ResNet
and Inception based approaches are more accurate
and faster and smaller.
This is why I was looking last week at trying
to do transfer learning on top of ResNet,
there's good reasons to want to do that.
So I think this is a great picture and these
two papers really do show us that academic
papers are not always just here's some highly
theoretical wacky result.
From time-to-time people write these great
thorough analyses of best practices and everything
that's going on, so there's some really great
stuff out there.
One other paper to mention in this kind of
broad ideas of things that you might find
helpful.
There's a paper by somebody named Leslie Smith,
who I think has got to be about the most overlooked
researcher.
Leslie Smith does a lot of really great papers,
which I really like.
This particular paper came up with a list
of 14 design patterns which seem to be generally
associated with better CNNs.
This is a great paper to read, it's a really
easy read.
You guys won't have any trouble with it at
all, I don't think.
And it's very short.
I looked through all these and I just kind
of thought, Yeah, these all make a lot of
sense.
And so as you, if you're doing something a
bit different and a bit new and you have to
design a new architecture, this would be a
great list of patterns to look through.
One more Leslie Smith paper to mention, and
it's crazy that this is not more well-known.
Something incredibly simple which is a different
approach to learning rates.
Rather than having your learning rate gradually
decrease, I'm sure a lot of you have noticed
that sometimes if you suddenly increase the
learning rate for a bit and then suddenly
decrease the learning rate for a bit, it kind
of goes into a better little area.
What this paper suggests doing is try actually
continually increasing your learning rate
and then decreasing it, increasing it, decreasing
it, increasing it, decreasing it; something
they call cyclical learning rates.
Check out the impact.
Compared to non-cyclical approaches it is
way way faster.
At every point it's much better.
And this is something which you could easily
add.
I haven't seen this added to any library.
So if you created the cyclical learning rate
annealing class for Keras, many people would
thank you.
Actually many people wouldn't know what you're
talking about until you write the blog post,
explain why it's good, show them this picture
and they'll thank you.
Question: Doesn't Keras has lots of call-backs
and loops?
Answer: Right.
And if I was doing this in Keras, I would
start with the existing learning rate annealing
code there and make small changes until it
starts working.
There's already code that does pretty much
everything you want.
The other cool thing about this paper is they
suggest a fairly automated approach to picking
what the minimum and maximum bounds should
be.
Again this idea of roughly what should our
learning rate be is something that we tend
to use a lot of trial and error for.
So check out this paper for a suggestion about
how to do it somewhat automatically.
Okay, so there's a whole bunch of things that
I've zipped over.
Normally I would have dug into each of those
and explained it and shown examples and notebooks
and stuff, but you guys hopefully now have
enough knowledge to take this information
and play with it.
What I'm hoping is that different people will
play with different parts and come back and
tell us what you find.
Hopefully we'll get some good new contributions
to Keras or PyTorch or some blog posts or
some papers or so forth.
Or maybe with that DeVISE stuff, maybe some
new applications.
So the next thing I wanted to look at, again
somewhat briefly, is the Data Science Bowl.
And one the reasons I wanted to particularly
dig into the Data Science Bowl (there's a
million reasons) is there's a $1,000,000 prize,
and there are 23 days to go.
The second is it's an extension of everything
you guys have learned so far about computer
vision.
It uses all the techniques you've learnt so
far about computer vision.
It uses all the techniques you've learnt but
then some.
Rather than 2D images, they're going to be
3D volumes.
Rather than being 300x300 or 500x500, they're
going to be 512x512x200.
So like a couple hundred times bigger than
the stuff you've built before.
The stuff we learnt in Lesson 7 about where
are the fish, you're going to be needing to
use a lot of that.
So I think it's a really interesting problem
to solve.
I personally care a lot about this because
my previous start-up, Enlitic, was the first
organization to use deep learning to tackle
this exact problem, which is trying to find
lung cancer in CT-scans.
The reason I made that Enlitic's first problem
was mainly because I learnt that if you can
find lung cancer earlier, the probability
of survival is 10X higher.
So here is something where you can have a
real impact by doing this well.
Not to say that $1,000,000 is not a big impact
as well.
So let me tell you a little bit about this
problem.
Here is a lung.
This is in a DICOM viewer.
DICOM is the standard that is used for sharing
most kinds of medical imaging, certainly CT
scans.
It is a format which contains 2 main things,
one is a stack of images and another is some
metadata.
That metadata would be how much radiation
were they zapped by, how far away from the
chest was the machine, and what brand of machine
was it, and so forth.
Most DICOM viewers just use your scroll wheel
to zip through them.
So all this is doing is going from top to
bottom or from bottom to top, and so you kind
of see what's going on.
Question: Do you want to orient?
Answer: What I might do which I think is more
interesting is to stay ... let's actually
focus on the bit that's going to matter to
you which is the inside of the lung is this
dark area here.
These little white dots are what's called
the vasculature, the vessels and stuff going
through the lungs.
As I scroll through, have a look at this little
dot.
You'll see that it seems to move, see how
it's moving.
The reason it's moving is because it's not
a dot, it's actually a vessel going through
space, so it actually looks like this.
And so if you take a slice through that it
looks like lots of dots.
So as you go through those slices, it looks
like that.
And then eventually we get to the top of the
lung, and that's why you see it eventually
all goes to white.
So that's the edge basically of the organ.
So you can see, there are edges on each side.
Some of you have been looking at this already
over the last few weeks and have often asked
me how to deal with multiple images.
What I've said each time is, don't think of
it as multiple images.
Think of it in the way your DICOM viewer can,
if you have a 3D button, like this one does.
That's actually what we're just looking at.
So it's not a bunch of flat images, it's a
3D volume.
It just so happens that the default way that
most DICOM viewers show things is by a bunch
of flat images.
But it's really important that you think of
it as a 3D volume because you're looking in
this space.
Now what are you looking for in this space?
What you're looking for is somebody who has
lung cancer.
And what somebody with lung cancer looks like
is that somewhere in this space, there is
a blob.
Could be roughly spherical blob.
Could be pretty small, around 5 mm, is where
people start to get particularly concerned
about a blob.
So what that means is for a radiologist as
they flick through a scan like this is that
they're looking for a dot which doesn't move,
but which appears, gets bigger, and then disappears.
That's what a blob looks like.
So you can see why radiologists very very
often miss nodules, blobs, in lungs.
In all this area, you've got to have extraordinary
vision to be able to see every little blob
appear and then disappear again.
And remember, the sooner you catch it, you
get a 10X improved chance of survival.
And generally speaking, when a radiologist
looks at one of these scans, they're not looking
for nodules, they're looking for something
else.
Lung cancer, at least in the earliest stages,
is asymptomatic, it doesn't cause you to feel
different.
So it's like something that every radiologist
has to be thinking about when they're looking
for pneumonia or whatever else.
So that's the basic idea, we're going to try
to come up with in the next half-hour or so,
some idea about how would you find these blobs,
how would you find these nodules.
So each of these things generally is about
512 by 512 by a couple of hundred.
And the equivalent of a pixel in 3D space
is called a voxel.
So a voxel simply means a pixel in 3D space.
So this here is rendering a bunch of voxels.
Each voxel in a CT scan is a 12-bit integer,
if memory serves me correctly.
And a computer screen can only show 8 bits
of gray-scale.
Furthermore, your eyes can't necessarily distinguish
between all those gray-scale perfectly anyway.
So what every DICOM viewer provides is something
called a windowing adjustment.
So a windowing adjustment, here is the default
window, which is designed to basically map
some subset of that 12-bit space to the screen
so that it highlights certain things.
And so the units a CT scan uses are called
Hounsfield units, and different, certain ranges
of Hounsfield units tell you that something
is some particular part of the body.
And so you can see here that the bone is being
lit up, we selected an image window which
is designed to let us see the bone clearly.
What I did when I opened this was I switched
it to CT lungs, which some kind person has
already figured out what the best
window to see the nodules and vasculature
is.
Now for you, working in deep learning don't
have to care about that because of course
the deep learning algorithm can see 12 bits
perfectly well.
So nothing really to worry about.
So one of the challenges in dealing with this
Data Science Bowl data is that there's a lot
of preprocessing to do.
But the good news is that there's a couple
of fantastic tutorials.
So hopefully you've found out by now that
in Kaggle if you click on the Kernels button
you basically get to see people's iPython
notebooks where they show you how to do certain
things.
In this case, this guy has got a full preprocessing
tutorial, showing how to load DICOM, convert
the values to Hounsfield units, and so forth.
And I'll show you some of these pieces.
So DICOM you will load with some library,
probably pydicom.
So pydicom is a library that kind of is a
bit like pillow, image.open is like a dicom.open,
and you end up with a 3D file and, of course,
the metadata.
You can see here using the metadata, ImagePositionPatient,
SliceLocation.
So the metadata comes through with attributes
of the Python object.
And this person has very kindly provided for
you a list (copied from Wikipedia) of the
Hounsfield units for each of the different
substances.
So it shows how to translate stuff into that
range and so it's great to draw lots of pictures.
So here is a histogram for this particular
picture.
So you can see that most of it is air, and
then you get some bone and some lung; here's
the actual slice.
So the next thing to think about is voxel
spacing, which is as you move across one bit
of x-axis or one bit of y-axis or slice-to-slice,
how far in the real world are you moving.
One of the annoying things about medical imaging
is that different kinds of scanners have different
distances between those slices (it's called
the slice thickness) and different meanings
of the x and y axes.
Luckily that stuff's all in the DICOM metadata,
so the resampling process means taking those
lists of slices and turning it into something
where every step in the x-direction, the y-direction
or the z-direction equals 1 mm in the real
world.
And so it would be very annoying for your
deep learning network if your different lung
images were squished by different amounts,
especially if you can give it the metadata
about how much things squished.
So that's what resampling does.
As you can see, it's using the slice thickness
and the pixel spacing to make it nice and
even.
So there are various ways to do 3D plots and
it's always a good idea to do that.
Then something else that people tend to do
is segmentation, and depending on time we
may or may not get around to looking more
at segmentation in this part of the course.
But effectively, segmentation is just another
generative model where hopefully somebody's
given you things saying, This is lungs, This
is air.
And then you build a model which tries to
predict for something else what is lung and
what is air.
Unfortunately for lung CT scans we don't generally
have that kind of ground truth of which bit
is lung and which bit is air.
So generally speaking, in medical imaging
people use a whole lot of heuristic approaches.
So kind of hacky, rule-based approaches.
And in particular, applications of region
growing and morphological operations.
I find this kind of the boring part of medical
imaging, because it's so clearly a dumb way
to do things.
But deep learning is far too new in this area
yet to develop the datasets that we need to
do this properly.
But the good news is that there's a button
which I don't think many people know exists,
called Tutorial (on the main Data Science
Bowl page), where these folks from Booz Allen
Hamilton actually show you a complete tutorial
approach.
Now it's interesting that they picked U-Net
segmentation.
This is definitely the thing I would be teaching
you guys about segmentation if we have time.
U-Net is one of these things that, outside
of the Kaggle world, I don't think that many
people are familiar with.
But inside the Kaggle world, we know that
anytime segmentation crops up, U-Net wins;
it's the best.
More recently, there's actually been something
called DenseNet, which takes U-Net even a
little bit further, and maybe that would be
the new winner for newer Kaggle competitions,
when they happen.
But the basic idea here of things like U-Net
and DenseNet which is we have a model, maybe
I can draw it.
You know when we do generative models we think
about doing style transfer.
You generally start with this kind of large
image, and then we do some down-sampling operations
to make it a smaller image, and then we do
some computation, and then we make it bigger
again with these up-sampling operations.
What happens in U-Net is that there are additional
neural network connections made directly from
here to here and directly from here to here
and here to here and here to here.
And those connections basically allow it to
almost do like a residual learning approach.
Like it can figure out the key semantic pieces
at lower resolution, but then as it upscales
it, it can learn what was special about the
down-sampled image and the original image
here.
It can learn to add that kind of additional
detail at each point.
So U-Net and DenseNet for segmentation are
really interesting and I hope you find some
time to get to them in this part of the course.
But if we don't, you can get started by looking
at this tutorial in which these folks basically
show you from scratch.
And what they try to do in this tutorial is
something very specific, which is the detection
part.
Think about the Fisheries Competition.
We pretty much decided that in the Fisheries
Competition if you wanted it to do really
well you would first of all find the fish,
then you would zoom in to the fish and then
you would figure out what kind of fish it
is.
Certainly in the Right Whale Competition earlier
that was how that was found.
But in this competition, this is even more
clearly going to be the approach because these
images are just far too big to do a normal
convolutional neural network.
In one step, it's going to find the nodule,
and then in a second step it's going to zoom
in to a possible nodule and figure out is
this a malignant tumor or something else,
a false positive.
The bad news is that the Data Science Bowl
dataset does not give you any information
at all about for the training set, where are
the cancerous nodules.
I actually wrote a post in the Kaggle forums
about this.
I just think this is a terrible, terrible
idea because that information actually exists;
the dataset they got this from is called the
National Lung Screening Trial, which actually
has that information, or something close to
it.
So the fact they didn't provide it, I just
think it's horrible.
For a competition which can save lives, I
can't begin to imagine.
The good news, though, is there is a dataset
which does have this information.
The original dataset was call LIDC-IDRI.
Interestingly, that dataset was recently used
for another competition, a non-Kaggle competition,
called LUNA.
That competition is now finished and one of
the tracks in that competition was actually
specifically a false positive detection track;
the other track was a find-the-nodule track
basically.
So you can actually go back and look at the
papers written by the winners.
They're generally ridiculously short.
Many of them are a single-sentence saying,
Due to the confidentiality agreement we can't
do anything.
But some of them, including the winner of
the false positive track, is a film they actually
provided.
Not surprisingly, they all used deep learning.
And so what you can do (in fact I think what
you have to do to do well in this competition)
is download the LUNA dataset, use that to
build a nodule detection algorithm.
So if the LUNA dataset includes files saying,
this lung has nodules here, here, and here.
So do nodule detection based on that, and
run that nodule detection algorithm on the
Kaggle dataset, find the nodules, and then
use that to do some classification.
They're asking tricky things with that.
The biggest tricky thing is that most of the
CT scans in the LUNA dataset are what's called
contrast studies.
A contrast scan means that the patient had
radioactive dye injected into them so that
the things that they're looking for are easier
to see.
For the national lung screening trial, which
is what they use in the Kaggle dataset, none
of them used contrast.
And the reason why is what we really want
to be able to do is to take anybody who's
over 65 and has been smoking more than a pack
a day for more than 20 years, and give them
all a CT scan and find out which ones have
cancer.
In the process, we don't want to be shooting
them up with radioactive dye and giving them
cancer.
So that's why we try to make sure that when
we're doing this kind of asymptomatic scans
that they're as low radiation dose as possible.
So that means that you're going to have to
think about how the contrast in your image
is going to be different between the thing
you build and the LUNA dataset for nodule
detection and the Kaggle competition dataset.
When I looked at it, I didn't find that was
a terribly difficult problem.
I'm sure you won't find it impossible by any
means.
So to finalize this discussion, I wanted to
refer to this paper, which I'm guessing not
that many people have read yet.
It's a medical imaging paper.
What it is is a non-deep-learning approach
to trying to find nodules, so that's where
they use this thing here, nodule segmentation.
Comment: We have a correction from a radiologist
saying that dye is not radioactive, it's just
dense.
Isovue 370 or Isovue 300 usually.
Response: There's a reason we don't inject
people with contrast dye.
Comment: Issues are contrast induced nepropathy,
allergic reaction.
Response: Yes that's what I meant.
I do know that the NLST studies (again, radiologists
correct me) use a lower amount of radioactivity
than I think the LUNA ones do.
So it's not a difference.
So, this is an interesting idea of how can
you find nodules using more of a heuristic
approach.
And the heuristic approach they suggest here
is to do clustering.
We haven't really done any clustering in class
yet, so we're going to be getting into this
in some detail.
Because I think this is a great idea for the
kind of heuristics you can add on top of deep
learning, to make deep learning work in different
areas.
The basic idea here (as you can see they call
it a 5-dimensional mean), they're going to
try and find groups of voxels which are similar
and they're going to cluster them together.
And hopefully, we're going to particularly
cluster together things that look like nodules.
So the idea is at the end of this segmentation,
there will be one cluster for the whole lung
boundary, one cluster for the whole vasculature
and one cluster for every nodule.
So the 5 dimensions are x, y, z, intensity
(number of Hounsfield units), and then the
5th one is volumetric shape index.
This is the one tricky one.
The basic idea here is it's going to be a
combination of the different curvatures of
the voxel based on the Gaussian and mean curvatures.
Now what the paper goes on to explain is that
you can use for these the first and second
derivatives of the image.
Now all that basically means is you subtract
one voxel from its neighbor.
And then you take that whole thing and you
subtract one voxel's version of that from
its neighbor, to get the first and second
derivatives.
That kind of tells you the direction of the
change in image intensity at that point.
So by getting the first and second derivatives
of the image and then you put it into this
formula, it comes out with something which
basically tells you how sphere-like this voxel
seems to be, how sphere-like a construct.
So that's great.
If we can basically take all the voxels and
combine the ones that are nearby, at a similar
number of Hounsfield units and similar kinds
of shapes, we're going to get what we want.
So I'm not going to worry about this bit here
because it's very specific to medical imaging.
Anybody who's interested in doing this feel
free to talk on the forum about what this
looks like in Python.
But what I did want to talk about was the
mean shift clustering, a particular approach
to clustering which they talk about.
So clustering is something which for a long
time I've been kind of an anti-fan of.
It belongs to this group of unsupervised learning
algorithms which always seem to be kind of
looking for a problem to solve.
But I realized recently that there are some
specific problems that can be solved well
them.
And I'm going to be showing you a couple.
One today and one in Lesson 14.
Clustering algorithms are easiest to describe
by generating some data to show them.
Here's some generated data.
I'm going to create 6 clusters and for each
cluster I'm going to create 250 samples.
So I'm going to basically say, Okay let's
create a bunch of centroids by creating some
random numbers.
Six pairs of random numbers for my centroids.
And then I'll grab some random numbers around
each of those centroids, combine them all
together, and then plot them.
And so here you can see, each of these X's
represents a centroid (a centroid is just
like an average point or a cluster of data)
and each color represents one cluster.
So imagine if this was showing you clusterings
of different kinds of lung tissue.
Ideally, you'd have some voxels that were
colored one thing for nodule and a bunch of
other things colored a different color for
vasculature and so forth.
We can only show this easily in two dimensions,
but there's no reason not to be able to imagine
doing this in 5 dimensions.
So the goal of clustering will be to undo
this.
Given the data, but not the X's, how can you
figure out where the X's were.
And then it's pretty straight-forward once
you know where the X's are to then find the
closest points to that, to assign every data
point to a cluster.
The most popular approach to clustering is
called K-means.
K-means is an approach where you have to decide
up-front how many clusters are there and what
it basically does is there's two steps.
The first one is to guess as to where those
clusters might be, and the really simple way
to do that is basically to randomly pick a
point and then start randomly picking points
which are as far away as possible from all
the previous ones I've picked, and you throw
away the first one.
So if I started here, then probably the furthest
away point would be down here, so this would
be our starting point for Cluster 1.
And then you say, What point is the furthest
away from that?
That's probably this one here, so that's our
starting point for Cluster 2.
What's the furthest point away from both of
these?
And so forth.
So you keep doing that to get your initial
points.
And then you just iteratively move every point.
So you basically then say, Okay let's assume
these are the clusters.
Which cluster does every point belong to?
And then you just iteratively move the points
to different clusters a bunch of times.
Now, K-means it's a shame it's so popular
because it kind of sucks, right.
Sucky Thing #1 is that you have to decide
how many clusters there are, and at all points
we don't know how many nodules there are.
And then Sucky Thing #2 is without some changes
(this thing called Kernel K-means), it only
works if the things are the same shape, they're
all kind of nicely Gaussian shaped.
So we're going to talk about something way
cooler, which I kind of came across quite
recently, much less well-know, which is called
mean-shift clustering.
Now mean-shift clustering is one of these
things which seems to spend all of its time
in kind of serious mathematician land.
Like whenever I tried to look up something
about mean-shift clustering, I kind of started
seeing these kinds of things.
This is like the first tutorial not in a pdf
that I could find.
So this is one way to think about mean-shift
clustering.
Another way is code-first approach, which
is that this is the entire algorithm.
So let's talk about what's going on here,
what are we doing?
At a high level, we're going to do a bunch
of loops, we're going to do 5 steps.
It would be better if I didn't do 5 steps
but I kept doing this until it was stable.
But for now, we're going to do 5 steps.
And each step (our data is X), I'm going to
enumerate through our data.
So a 
small "x" is the current datapoint I'm looking
at.
Now what I want to do is find out how far
away is this datapoint from every other datapoint.
So I'm going to create a vector of distances.
And I'm going to do that with the magic of
broadcasting.
So small "x" is a vector of size 2 (this is
its 2 coordinates), and big "X" is a matrix
is of size n by 2, where n is the number of
points.
And thanks to what we've now learned about
broadcasting, we know that we could subtract
a matrix from a vector and that vector will
be broadcast across the axis of the matrix.
And so this is going to subtract every element
of big "X" from little "x".
And so if we then go ahead and square that
and then sum it up, and then take the square
root, this is going to return a vector of
distances from small "x" to every element
of big "X".
The sum here is just summing up the 2 coordinates.
So that's Step 1.
So we now know for this particular datapoint
how far away it is from all of the other datapoints.
The Final Step will be to take a weighted
average.
In the Final Step, we're going to say, What
cluster do you belong to?
Let's draw this.
So you've got a whole bunch of datapoints
and we're currently looking at this one.
And what we've done is we've now got a list
of how far it is away from all of the other
datapoints.
And the basic idea is now what we want to
do is take the weighted average of all of
those datapoints, weighted by the inverse
of that distance.
So the things that are a long way away we
want to weight very small, and the things
that are very close we want to weight very
big.
So I think this is probably the closest, this
is about the second closest, and then this
is about the third closest.
So assuming these have most of the weight,
the average is going to be somewhere about
here.
So by doing that for every point, we're going
to move every point closer to where its friends
are, closer to where the nearby things are.
And so if we keep doing this again and again,
everything's going to move until it's right
next to its friends.
So how do we take something which initially
is a distance and make it so that the larger
distances have smaller weights.
The answer is we probably want a shape that
looks something like that.
In other words, Gaussian.
This is by no means the only shape you could
choose.
It would be equally valid to choose this shape,
a triangle, at least the top of one.
In general, if we're going to multiply every
point by one of these things and add them
all together, it would be nice if all of our
weights added to 1, because then we're going
to end up with something that's at the same
scale that we start with.
So when you create one of these curves where
it all adds up to 1, generally speaking we
call that a kernel.
And I mention this because you'll see kernels
everywhere, if you haven't already.
Now that you've seen it, you'll see them everywhere.
In fact, kernel methods is a whole area of
machine learning that in the mid-90's basically
took over.
Because it was so theoretically pure and if
you want to get published in conference proceedings,
it's much more important to be theoretically
pure than actually accurate.
So for a long time kernel methods won out
and neural networks (in particular) disappeared.
Eventually people realized that accuracy was
important as well and in more recent times,
kernel methods are largely disappearing.
But you still see the idea of a kernel coming
up very often because they're super-useful
tools to have.
They're basically something that lets you
take a number, in this case a distance, and
turn it into some other number where you can
weight everything by that other number and
add them together to get a nice little weighted
average.
So in our case, we're going to use a Gaussian
kernel.
The particular formula for a Gaussian doesn't
matter.
I remember learning this formula in Grade
10 and it was by far the most terrifying mathematical
formula I'd ever seen.
But it doesn't really matter.
For those of you who remember, or have seen
the Gaussian formula, you'll recognize it,
but if you haven't it doesn't matter.
But this is the function that draws that curve.
So if we take every one of our distances and
put it through the Gaussian, we will then
get back a bunch of weights that add to 1.
So in the final step, we can multiply every
one of our data points by that weight, add
them up and divide by the sum of the weights,
in other words, take the weighted average.
You'll notice that I had to be a bit careful
about broadcasting here because I needed to
add a unit axis at the end of my dimensions,
not at the start.
So by default, it adds unit axes to the beginning
when you do broadcasting, that's why I had
to do an expand_dims.
If you're not clear on why this is, then that's
a sign you definitely need to do some more
playing around with broadcasting.
So have a fiddle with that during the week.
Feel free to ask, if you're not clear after
you've experimented.
So this is just a weighted sum.
This is just doing sum of (weights * x) divided
by sum of weights.
Importantly, there's a nice little thing we
can pass to a Gaussian because the thing that
decides, Does it look like the thing I just
drew, or does it look like this.
All of those things add up to 1, they have
the same area underneath, but they're very
different shapes.
If we make it look like this, then what that's
going to do is it's going to create a lot
more clusters.
Things that are really close to it are going
to have really high weights and everything
else is going to have a tiny weight and be
meaningless.
Whereas if we do something like this, we're
going to have much fewer clusters, because
even stuff that's further away is going to
have a higher weight in the weighted sum.
So the choice that you use for (this is called)
the kernel width (lots of different names
you can use, up here, "bw" being bandwidth).
That number, there's some cool ways to choose
it.
One simple way to choose it is to find out
which size of bandwidth covers 1/3 of the
data in your dataset.
I think that's the approach that scikit-learn
uses.
So anyway, there are some ways that you can
automatically figure out the bandwidth.
It's just one of the very nice things about
meanshift.
So we just go through a bunch of times (5
times) and each time we replace every point
with its weighted average, weighted by this
Gaussian kernel.
And so when we run this 5 times, it takes
a second, and here's the results.
I've offset everything by 1 so we can see
it, otherwise it would be right on top of
the "X".
So you can see for nearly all of them it's
in exactly the right spot.
Whereas for these 2 clusters, at this particular
bandwidth it decided to create 1 cluster for
them, rather than 2.
So this is kind of an example, whereas if
we decreased our bandwidth it would create
2 clusters.
There's no one right answer.
So one challenge with this is that it's kind
of slow.
So I thought let's try and accelerate it with
a GPU.
And because meanshift's not very cool, nobody's
implemented it in a GPU yet, or maybe it's
not a good idea.
So I thought I would use PyTorch.
And the reason I use PyTorch is because it
really feels like writing PyTorch just feels
like writing NumPy, everything happens straight-away.
So I really hoped that I could take my original
code and make it almost the same.
Indeed, here is the entirety of meanshift
in PyTorch.
So that's pretty cool.
You can see anywhere it used to have np, it
now has torch.
"np.array" is now "torch.FloatTensor".
"np.sqrt" is now "torch.sqrt".
Everything else is almost the same.
One issue is that Torch doesn't support broadcasting.
So, we'll talk more about this shortly, in
a couple of weeks, but basically I decided
that's not okay so I wrote my own broadcasting
library for PyTorch.
So rather than saying little "x" minus big
"X", I used "sub" for subtract, that's the
subtract from my broadcasting library.
If you're curious, check out torch_utils and
you can see my broadcasting operations there.
Basically if you use those (you can see, the
same for multiplication) it will do all the
broadcasting for you.
As you can see, this looks basically identical
to the previous code, but it takes longer
so that's not ideal.
One problem here is that I'm not using cuda.
So I could easily fix that by adding .cuda
to my x.
But that made it slower still.
And the reason why is all the work's being
done in this for loop and PyTorch doesn't
accelerate for loops.
Each run through a for loop in PyTorch is
basically calling a new cuda kernel every
time you go through, and it takes a certain
amount of time to even launch a cuda kernel.
When I'm saying cuda kernel, this is a different
usage of the word "kernel".
In cuda, a kernel refers to a little piece
of code that runs on the GPU.
So it's launching a little GPU process every
time through the for loop.
Takes quite a bit of time and it's also having
to copy data all over the place.
So it gets the right result, that's good.
So what I then tried to do was to make it
faster, the trick is to do it by minibatch.
So each time through the loop we don't want
to do just one piece of data, but a minibatch
of data.
So here are the changes I made.
The main one was my "for i" now jumps through
one batch size at a time.
So I'm going to go not 1,2,3 but 0,16,32.
Okay, so I now need to create a slice which
is, from i to i+batch-size.
Unless we've gone past the end of the data,
in which case it's just as far as n.
So this is going to refer to the slice of
data that we're interested in.
So what we can now do is to say X with that
slice, to grab back all of the data in this
minibatch.
And then I have to create a special version
of (I can't just say subtract anymore) and
I have to think carefully about the broadcasting
operations here.
I'm going to return a matrix.
Let's say batch size is 32.
I'm going to have 32 rows, and then let's
say n is 1000, there will be 1000 columns.
And that shows me how far away each thing
in my batch is from every piece of data.
So when we do things a batch at a time, we're
basically adding another axis to all of your
tensors.
Suddenly now you have a batch axis all the
time.
And when we've been doing deep learning, that's
been like something we've got pretty used
to, right.
The first axis in all of our tensors has always
been a batch axis.
So now we're writing our own GPU-accelerated
algorithm.
Can you believe how crazy this is.
Like 2 years ago, if you google for "K-means
cuda" or "K-means GPU", you get back like
research studies.
People write papers about how to put these
algorithms in GPUs.
Like it was hard.
And here's a page of code that does it.
So this is crazy that this is possible.
Here we area, we've built a batch-by-batch
GPU-accelerated meanshift algorithm.
So the basic distance formula is exactly the
same, I just have to be careful about where
I add, unsqueeze is the same as expand_dims
in NumPy, so I have to be careful about where
I add my unit axes.
Add it to a first axis of one bit and a second
axis of the other bit.
That's going to do like a subtract every one
of these from every one of these, return a
matrix.
Again, this is like a really good time to
look at this and think why does this broadcasting
work, because this is getting more and more
complex broadcasting.
Hopefully you can now see the value of broadcasting.
Not only did I get to avoid writing a pair
of nested for loops here, but I also got to
do this all on the GPU in a single operation,
so I've made this thousands of times faster.
So here is a single operation which does that
entire matrix subtraction.
So that's our batch-wise distance function.
We then chuck that into a Gaussian, and because
this is just element-wise, the Gaussian function
hasn't changed at all, so that's nice.
And then I've got my weighted sum, and then
divide that by the sum of weights.
So that's basically the algorithm.
So previously for my NumPy version that took
1 second, now it's 48 milli seconds.
So we just sped that up by 20X.
Question: I get how batching helps with locality
and cache, but I do not quite follow how it
helps otherwise, especially with respect to
accelerating the for loop.
Answer: Yeah.
In PyTorch, the for loop is not run on the
GPU.
The for loop is run on your CPU and your CPU
goes through each step of the for loop and
calls the GPU to say, Do this thing, do this
thing.
So this is not to say you can't accelerate
this in TensorFlow in a similar way.
Like in TensorFlow, there's a tf.while and
stuff like that where you can actually do
GPU-based loops.
Even still, if you do it entirely in a loop
in Python, it's going to be pretty difficult
to get this performance, but particularly
in PyTorch.
Remember in PyTorch your loops are not optimized.
It's what you do inside each loop that's optimized.
Question: Some of the math functions are coming
from Torch and others are coming from the
math Python library.
What is the difference?
When you use the Python math library does
that mean the GPU is not being used?
Answer: Yeah.
So you'll see that I use that math.pi as a
constant and then sqrt of 2*math.pi as a constant,
so I don't need to use the GPU to calculate
a constant.
We only use Torch for things that are running
on a vector or a matrix or a tensor of data.
Okay, let's have a break.
We'll come back in 10 minutes and we'll talk
about some ideas I have for improving meanshift,
which maybe you guys will want to try out
during the week.
So basically the idea here is we figure there
are 2 steps we need to figure out where the
nodules are in something like this.
Step 1 is to find the things that may be kind
of nodule-ish, zoom in to them and create
a little cropped version.
And then Step 2 would be where your deep learning
particularly comes in, which is to figure
out is that cancerous or not.
Once you've found a nodule-ish thing, the
cancerous ones.
By far the biggest driver of whether or not
something is a malignant is how big it is.
The other thing particularly important is
how kind of spider-y it looks.
If it looks like it's evil-y going out to
capture more territory, that's probably a
bad sign as well.
So the size and the shape are the two things
we're going to try and find.
That's a good thing for a neural net to be
able to do.
You probably don't need that many examples
of it.
When you get to that point, there's the question
of how to deal with the 3D aspect here.
You can just create a 3D convolutional neural
net.
So if you had a 10x10x10 space, that's obviously
certainly not going to be too big.
A 20x20x20 you might be okay.
Kind of think about how big of a volume you
can create.
There's plenty of papers around on 3D convolutions,
although I'm not sure you even need one because
it's just a convolution in 3D.
The other approach that you might find interesting
to think about is called tri-planar.
What tri-planar means is that you take a slice
through the x and the y and the z axes and
so you basically end up with 3 images, one
that goes through x, y and z, and so you could
treat those as different channels, if you
like even.
They probably use pretty standard neural net
libraries that expect 3 channels.
So there's a couple of ideas for how you can
deal with the 3D aspect of it.
I think using the LUNA dataset as much as
possible is going to be a good idea because
you really want something that's pretty good
at detecting nodules before you start putting
it onto the Kaggle dataset.
The other problem with the Kaggle dataset
is that it's ridiculously small.
Again, there's no reason for it.
There are far more cases in NLST than they've
provided to Kaggle.
I can't begin to imagine why they went to
all this trouble and $1,000,000 money for
something which has not been set up to succeed.
Anyway, that's not our problem; it makes it
all the most interesting thing to play with.
But after the competition is finished, if
you get interested in it, you'll probably
want to go and download the whole NLST dataset
(or as much as possible) and do it properly.
Question: Last class, you mentioned that you
would explain when and why to use Keras versus
PyTorch.
If you only had brain space for one (in the
same way some have only brain space for vi
or emacs) which would you pick?
Answer: I would pick PyTorch.
It kind of does everything Keras does but
gives you the flexibility to play around a
lot more.
I'm sure you've got brain space for both.
Question: You mentioned there are other datasets
of cancerous images that has labels?
Answer: That was my suggestion and that's
what the tutorial shows how to do.
There's a whole kernel on Kaggle for Candidate
Generation and LUNA16 something something,
which shows how to use LUNA to build a nodule
finder.
This is one of the highest rated Kaggle kernels.
We've now used "kernel" in three totally different
ways in this lesson.
See if we can come up with a 4th.
Kaggle kernels, cuda kernels, and kernel methods.
So it looks very familiar, doesn't it?
So here is a Keras approach to finding lung
nodules based on LUNA.
Question: Is there such a thing VGG in 3D?
Answer: Yes, 3D CNNs.
I mentioned an opportunity to improve this
meanshift algorithm.
And the opportunity for improvement, when
you think about it, is pretty obvious.
The actual amount of data is huge, we've got
datapoints all over the place.
The ones that are a long way away, the weight
is going to be so close to 0 that we may as
well just ignore them.
The question is how do we quickly find the
ones which are a long way away?
And we know the answer to that, we learned
it.
It's approximate nearest neighbors.
So what if we added an extra step here, which
rather than using X to get the distance to
every data point, we instead used approximate
nearest neighbors to grab the closest ones,
the ones that are actually going to matter.
So that would basically turn this linear time
piece into a linear time piece, which would
be pretty fantastic.
So we learnt very briefly about a particular
approach which is Locality Sensitive Hashing.
I think I mentioned also there's another approach
which I'm really fond of, called spill-trees.
I really really want us as a team to take
this algorithm and add approximate nearest
neighbors to it and release it to the community
as the first ever super-fast GPU-accelerated
approximate nearest neighbor accelerated meanshift
clustering algorithm.
That would be a really good deal.
If anybody's interest in doing that, I believe
you're going to have to implement something
like LSH or spill-trees in PyTorch.
Once you've done that, it should be totally
trivial to add the step that then uses that
bit.
So if you do that, then if you're interested
I would invite you to team up with me and
we would then release this piece of software
together and author a paper or a post together.
So that's my hope, that one of you or a group
of you will make that happen.
That will be super exciting.
So I think this would be great.
It would be showing people something pretty
cool about the idea of writing GPU algorithms
today.
In fact, I found just during the break, here's
a whole paper about how to write K-means with
cuda.
It used to be so much work, and this is without
even including any approximate nearest neighbors
piece or whatever.
So I think this would be great.
So hopefully that will happen.
I guess to do it properly we should also be
replacing the Gaussian kernel bandwidth with
something that we figure out dynamically.
Probably don't want to hard-code it.
So big change.
We're going to learn about chatbots.
So we're going to start here with Slate, Facebook
Thinks It Has Found the Secret to Making Bots
Less Dumb.
Okay, so this talks about a new thing called
memory networks, which was demonstrated by
Facebook.
You can feed it sentences that convey key
plot points from Lord of the Rings, then ask
it various questions.
They published a paper on arXiv that generalizes
the approach.
There was another long article about this
in Popular Science, in which they described
it as early progress toward a truly intelligent
AI, now LeCun is excited about work on a memory
network, giving them the ability to retain
information.
You can tell the network a story and have
it answer questions.
It even has this little Gif.
Comment: Someone has found that the popping
noises might be related to overuse with the
LogiTech camera, according to bug reports.
They recommended lowering the camera resolution,
apparently helps with the CPU issues.
Response: No, it's not that.
I've already put it down to 320x240.
Nice guess.
Funny that we haven't had the popping before.
Apologies.
Okay, so in the article they've got this little
example, showing reading the story of Lord
of the Rings, and then asking various questions
about Lord of the Rings, and it all looks
pretty impressive.
So we're going to implement this paper, and
this paper is called end-to-end memory networks.
The paper was actually not shown on Lord of
the Rings, but was actually shown on something
called babi-memnn.
It's a paper describing a synthetic dataset,
Towards AI-Complete Question Answering: A
Set of Prerequisite Toy Tasks.
I saw a cute tweet last week which was describing
the meaning of various different types of
titles of papers, and basically saying that
"towards" means we've actually made no progress
whatsoever, so we'll take this with a grain
of salt.
So these introduced the babi tasks and the
babi tasks are probably best described by
showing an example.
Here's an example.
Each task is basically a story.
A story contains a list of sentences, a sentence
contains a list of words.
At the end of a story is a query, to which
there is an answer.
So the sentences are ordered in time.
So for "Where is Daniel" we have to go backwards.
This says where John is, this says where Daniel
is, and so Daniel is in the bathroom.
So this is what the babi tasks look like.
There's a number of different structures.
This is called a 1-supporting-fact structure,
which is to say you only have to go back and
find one sentence in the story to grab the
answer.
We're also going to look at 2-supporting-facts
stories, which is ones where you're going
to have to look twice.
So reading in these datasets is not remotely
interesting, they're just a text file, we
can parse them out.
There's various different text files for the
various different tasks.
If you're interested in the various different
tasks, you can check out the paper.
We're going to be looking at single_supporting_fact
and two_supporting_facts.
They have some with 10,000 examples and some
with 1,000 examples.
The goal is to be able to solve every one
of their challenges with just 1,000 samples.`
This paper is not successful with that goal,
but it makes some movement towards it.
So basically we're going to put that into
a bunch of different lists of stories, along
with their queries.
And we can start off by having a look at some
statistics about them.
So the first is for each story, what is the
maximum amount of sentences in the story (story_maxsents).
And the answer is 10.
So Lord of the Rings it ain't.
In fact, if you go back and look at the Gif
and it says read story Lord of the Rings,
that's the whole Lord of the Rings.
Frodo journeyed to Mount Doom.
Frodo dropped the ring there.
The total number of different words in this
thing is 32 (vocab_size).
The total length of any sentence in a story
is 8 (story_maxlen).
The maximum amount of words in any query is
4 (query_maxlen).
So okay, we're immediately thinking, What
the hell.
This was presented by the press as the secret
to making bots less dumb, and showed us that
they took a story and summarized Lord of the
Rings.
They made the plot points and asked various
questions.
Clearly that's not entirely true.
Like if you look at even the stories, the
first word is always somebody's name.
The second word here is some synonym for moved.
There's then a bunch of prepositions and then
the last word is always place.
So like these toy tasks are very very toy.
Immediately we're thinking maybe this is not
a step to making bots less dumb or whatever
they said here, A Truly Intelligent AI.
Maybe its Towards a Truly Intelligent AI.
So to get this into Keras we need to get this
into a tensor in which everything is the same
size so we use pad_sequences for that, like
we did in the last part of the course, which
will add 0's to make sure that everything's
the same size.
The other thing we will do is we will create
a dictionary from words to integers, to turn
every word into an index.
So we're going to turn every word into an
index and then pad them so that they're all
the same length.
And then that's going to give us inputs_train,
10,000 stories each one of 10 sentences each
one of 8 words.
Anything that's not 10 sentences long is going
to get sentences of just 0's.
Any sentence that's not 8 words long will
get some 0's again, at the back.
And ditto for the test.
So we just got 1,000.
So how do we do this?
Not surprisingly, we're going to use embeddings.
Now we have to turn a sentence into an embedding,
not just a word into an embedding.
So there's lots of interesting ways of turning
a sentence into an embedding, but when you're
just doing Towards Intelligent AI, you don't
do any of them, you instead just add the embeddings
up.
And that's what happened in this paper.
And if you look at the way it was set up,
you can see why you can add the embeddings
up.
"Mary", "John" and "Sandra" only appear in
one place, they're only the subject of this,
the verb is always the same thing, the prepositions
are always meaningless and the last word is
always place.
To figure out what a whole sentence says,
you can just add up the word concepts, the
order of them doesn't make any difference,
there's no "not"s, there's nothing that makes
the language remotely complicated or interesting.
So what we're going to do is we're going to
create an input for our stories with the number
of sentences and the length of each one.
We're going to take each word and put it through
an embedding.
So that's what TimeDistributed is doing here;
it's putting each word through a separate
embedding.
And then we do a Lambda layer to add them
up.
So here is our very sophisticated approach
to creating sentence embeddings.
So we do that for our story and then we end
up with something rather than being 10x8 (10
sentences by 8 words), it is now 10x20 (10
sentences by length 20 embedding).
So each one of our 10 sentences has been turned
into a length 20 embedding.
And we're just starting with a random embedding.
We're not going to use word2vec or anything
because we don't need the complexity of that
vocabulary model.
We're going to do exactly the same thing for
the query.
We don't need to use time-distributed this
time, we can just take the query, because
this time we have just one query.
So we can do the embedding, sum it up, and
then we use reshape to add a unit axis to
the front.
So now it's the same basic rank.
We now have 1 question, embedded to length
20.
So we have 10 sentences in the story and 1
query.
Okay, so what is the memory network, or more
specifically the end-to-end memory network?
And the answer is, it is this.
As per usual, when you get down to it, it's
less than a hundred code to do these things.
Let's draw this before we look at the code.
So we have a bunch of sentences, let's just
use 4 sentences for now.
So each sentence contained a bunch of words.
And we took each word and we turned them into
an embedding.
And then we summed all of those embeddings
up to get an embedding for that sentence.
So each sentence is turned into an embedding,
and they were of length 20.
And then we took the query.
Same kind of idea.
Bunch of words that we've got embeddings for,
and we added them up to get an embedding for
our question.
So to do a memory network, what we're going
to do is we're going to take each of these
embeddings, and we're going to combine each
one with a question or a query.
And we're just going to take a dot product.
Dot product, dot product, dot product, dot
product.
So we're going to end up with 4 dot products
from each sentence of the story times the
query.
So what does the dot product do?
It basically says how similar 2 things are.
When one thing is big the other thing is big,
if one thing is small the other thing is small,
those things both make the dot product bigger.
So these basically are going to be 4 vectors
describing how similar each of our 4 sentences
is to the query.
So that's Step 1.
Step 2 is to stick them through a softmax.
Remember, the dot product just returns a scalar.
So we now have 4 scalars, and they add up
to 1.
And they each are basically related to how
similar is the query to each of the 4 sentences.
We're now going to create a totally separate
embedding of each of the sentences in our
story by creating a totally separate embedding
for each word.
So we're basically just going to create a
new random embedding matrix for each word
to start with, some of them all together and
that's going to give us a new embedding, this
one they call "C", I believe.
All we're going to do is we're going to multiply
each one of these embeddings by the equivalent
softmax as a weighting and then just add them
all together.
C1*S1 + C2*S2 + ... That's going to be our
final result, which is going to be of length
20.
So 
this thing is a vector of length 20 and then
we're going to take that and we're going to
put it through a single dense layer and we're
going to get back the answer.
And that whole thing is the memory network.
It's incredibly simple.
There's nothing deep, in terms of deep learning,
there's almost no non-linearities, so it doesn't
seem like it's likely to be able to do very
much.
But I guess we haven't given it very much
to do.
So let's take a look at the code.
Question: So in that last step you said "the
answer", was that really "the embedding of
the answer" and then it has to get reverse-lookup?
Answer: Yes, it's the softmax of the answer
and then you have to do a argmax.
So here it is, right.
We've got the story times the query.
The embedding of the story times the embedding
of the query, the dot product.
We do a softmax.
Softmax works on the last dimension, so I
have to Reshape to get rid of the unit axis
and then I Reshape again to put the unit axis
back on again.
The Reshapes aren't doing anything interesting.
It's just dot product followed by softmax
and that gives us the weights.
So now we're going to take each weight and
multiply it by the second set of embeddings.
Here's our second set of embeddings, emb_c.
And in order to do this I just used the dot
product again.
But because of the fact that you've got a
unit axis there, this is just doing a very
simple weighted average.
And again, reshape to get rid of the unit
axis, so that we can stick it through a dense
layer with a softmax and that gives us our
final result.
So what this is effectively doing is it's
basically saying, How similar is the query
to each one of the sentences in the story,
use that to create a bunch of weights.
And then these things here are basically the
answers.
If it's Story #1 was where the answer was,
then we're going to use this one.
Story #2, #3, #4.
Because there's a single linear layer at the
very end so it doesn't really get to do much
computation.
It basically has to learn what the answer
represented by each story is.
And again, this is lucky because from the
original dataset, the answer to every question
is the last word of the sentence.
Where is Frodo's ring, or whatever.
So that's why we can have this incredibly
simple final piece.
So this is an interesting use of Keras, right.
We've created a model which is no possible
way deep learning, but it's bunches of tensors
and layers that are stuck together.
And so it has some inputs, it has an output.
So we can call it a model.
We can compile it, give it an optimizer and
a loss.
And then we can fit it.
So it's kind of interesting how you can use
Keras for things which really don't use any
of the normal layers in any normal way.
And as you can see, it works.
For what it's wort, we solved the problem.
And the particular problem we solved here
is the one-supporting-fact problem.
And in fact, it worked in less than one epoch.
More interesting is two-supporting-facts.
Actually, before I do that, let me point out
one thing.
We could create another model (now that this
is already trained) which is to return not
the final answer but the value of the weights.
And we can now go back and say, Okay for a
particular story, what are the weights.
So let's do f.predict, rather than answer.predict.
So for this story, Where is Sandra ... Daniel,
Mary, Sandra ... Sandra went to the bathroom.
Where is Sandra, bathroom.
So for this particular story, the weights
are here.
And you can see the weight for Sentence #2
is 0.98.
So we can look inside the model and find out
what sentences is it using to answer this
question.
Question: Would it not make more sense to
concat the embeddings rather than sum them?
Answer: Not for this particular problem, because
of the way the vocabulary is structured and
the sentences are structured.
Comment: It would also have to deal with variable
length.
Answer: Well, you have padding to make them
the same length.
If you wanted to use this in real life, you
would need to come up with a better sentence
embedding.
Presumably it might be an RNN, or something
like that.
Because you need to deal with things like
"not", and the location of subject and object,
and so forth.
One thing to point out is that the order of
the sentences matters.
And so what I actually did when I preprocessed
it was I added a "0:", "1:", whatever, at
the start of each sentence, so that it would
actually be able to learn the time order of
sentences.
So this is like another token that I added.
In case you were wondering what that was,
that was something I added in preprocessing.
Okay, so one nice thing with memory networks
is we can kind of look and see if they're
not working, and in particular, why they're
not working.
So Multi-hop.
So let's now look at an example of a 2-supporting-facts
story.
It's mildly more interesting.
We still have only one type of verb with various
synonyms and a small number of subjects, a
small number of objects.
So it's basically the same.
But now to answer a question we have to go
down through two hops.
So, Where is the milk.
Let's find the milk.
Daniel left the milk there.
Where is Daniel?
Daniel traveled to the hallway.
Where is the milk?
Hallway.
So that's what we have to be able to do this
time.
And so what we're going to do is exactly the
same thing we did before, but we're going
to take our whole little model (do the embedding,
reshape, dot, reshape, softmax, reshape, dot,
reshape, dense layer, sum) and we're going
to call this "one hop".
So this whole picture is going to become one
hop.
And what we're then going to do is we're going
to take this and go back and replace the query
with our new output.
So at each step, each hop, we're going to
replace the query with the result of our memory
network.
And so that way, the memory network can learn
to recognize that, Okay the first thing I
need is the milk.
Search back, find milk.
Okay, I now have the milk.
Now you need to update the query to, Where
is Daniel.
Now go back and find Daniel.
So the memory network in multi-hop mode basically
does this whole thing again and again and
again, replacing the query each time.
So that's why this took this whole set of
steps and chucked it into a single function.
So then I just go, Okay, response,emb_story=one_hop;
response,emb_story=one_hop on that.
And you can keep repeating that again and
again and again.
And then at the end, get an output.
That's our model.
Compile, fit.
I had real trouble getting this to fit nicely.
I had to play around with learning rates and
batch sizes and whatever else.
But I did eventually get it up to .999 accuracy.
So this is kind of an unusual class for me
to be teaching, particularly compared to Part
1, where it was like best practices.
Clearly this is anything but.
I'm kind of showing you something which was
maybe the most popular request was teach us
about chatbots.
Let's be honest, who has ever used a chatbot
that's not terrible.
And the reason no one's used a chatbot that's
not terrible is that the current state-of-the-art
is terrible.
So chatbots have their place.
Indeed one of the students in the class has
written a really interesting analysis of this,
which hopefully she'll share on the forum.
But that place is really lots of heuristics
and carefully set up vocabularies and selecting
from small sets of answers and so forth.
It's not kind of general purpose, Here's a
story, ask anything you'd like about it, here
are some answers.
It's not to say we won't get there.
I sure hope we will.
But the incredible hype we had around neural
Turing Machines and memory networks and end-to-end
memory networks.
As you can see when you look at the dataset
they worked on, it's kind of crazy.
So that is not quite the final conclusion
of this though.
Yesterday a paper came out which showed how
to identify buffer overruns in computer source
code using memory networks.
And so it kind of spoiled my whole narrative
that somebody actually seems to have used
this technology for something effectively.
I guess when you think about it, it makes
some sense.
In case you don't know, a buffer overrun is,
that's like if you're writing in an unsafe
language (probably C), you allocate some memory
that's going to store some result or some
input and you try to put into that memory
something bigger than the amount you allocated,
it basically spills out.
In the best case, it crashes.
In the worst case, somebody figures out how
to get exactly the right code to spill out
in exactly the right place, and ends up taking
over your machine.
So buffer overruns are horrible things.
And the idea of being able to find them, I
can actually see it does look like this memory
network.
You kind of have to see, Oh where was that
variable set, and where was it being set from
set and where was the original thing allocated.
It's kind of like just going back through
the source code.
The vocabulary is pretty straightforward,
you know it's just the variables that have
been defined.
You know, that's kind of interesting.
I haven't had a chance to really study the
paper yet but ... it's no chatbot but maybe
there is a room for memory networks already
after all.
Question: Is there a way to visualize what
the neural network has learned for the text?
Answer: There is no neural network.
If you mean the embeddings, yeah you can look
at the embeddings easily enough.
The whole thing is so simple it's very easy
to look at every embedding and as I mentioned,
we looked at visualizing the weights, now
we're looking at a softmax.
But I mean we don't even need to look at it
in order to figure out what it would look
like.
Based on the fact that this is just a small
number of simple linear steps we know that
it basically has to learn what each sentence
answer can be.
If it's Sentence #3, the answer will always
be milk.
If it's Sentence #4, the answer will always
be hallway (or whatever).
So that's what the "C" embedding is going
to have to be.
And then the embeddings for the weights are
basically going to have to learn how to come
up with a similar embedding for the query.
In fact, I think you can even make them the
same embedding so that these dot products
basically give you something that gives you
similarity scores.
So this is really a very simple, largely linear
model, so it doesn't require too much visualizing.
So having said all that, none of this is to
say that memory networks are useless.
They're created by very smart people with
an impressive pedigree in deep learning.
It's just that this is very early.
And this tends to happen in popular press,
they kind of get over-excited about things.
Although in this case, I don't think we can
blame the press, I think we have to blame
Facebook for creating a ridiculous demo like
this.
This is clearly created to give people the
wrong idea, which I find very surprising from
people like , who normally would do the opposite
of that thing.
So this is not really the press' fault in
this case, but this may well turn out to be
a critical component in chatbots and Q&A systems
and whatever else, but we're not there yet.
I had a good chat with Steve Merryday the
other day, who's a researcher I respect a
lot and also somebody I like.
I asked him what he thought was the most exciting
research in this direction at the moment,
and he mentioned something that I was also
very excited about, which is called Recurrent
Entity Networks.
And the Recurrent Entity Network paper is
the first to solve all of the babi tasks with
100% accuracy.
Take of that what you will.
I don't know how much that means; they're
synthetic tasks.
One of the things that Steve Merryday actually
pointed out in the blog post is that even
the basic kind of coding of how they're created
is pretty bad, they have lots of replicas;
the whole thing's a bit of a mess.
But anyway, none the less, this is an interesting
approach, so if you're interested in memory
networks, that's something you can look at.
It's likely to be an important direction.
Having said all that, one of the key reasons
I wanted to look at memory networks was not
only was it the largest request from the forums
for this part of the course, but also because
it introduces something that's going to be
critical for the next couple of lessons, which
is the concept of Attention.
Attention, or Attentional Models, are models
where we have to do exactly what we just looked
at, which is basically find out at each time
which part of a story to look at next, or
which part of an image to look at next, or
which part of a sentence to look at next.
And so the task that we're going to be trying
to get at over the next lesson or two is going
to be to translate French into English.
So this is clearly not a toy task, this is
a very challenging task.
One of the challenges is that in a particular
French sentence which has got some bunch of
words, it's likely to turn into an English
sentence with some different bunch of words,
and maybe these particular words here might
be this translation here, and this one might
be this one.
And so as you go through you need some way
of saying, Which word do I look at next?
So that's going to be the Attentional Model.
And so what we're going to do is we're going
to be trying to come up with a proper RNN,
like an LSTM or a GRU or whatever, where we're
going to change it so that inside the RNN
it's going to actually have some way of figuring
out which part of the input to look at next.
So that's the basic idea of Attentional Models.
And so, interestingly during this time that
memory networks and neural Turing Machines
and stuff were getting all this huge amount
of press attention, very quietly in the background
at exactly the same time, Attentional Models
were appearing as well.
And it's the Attentional Models for language
that have really turned out to be critical.
So you've probably seen all of the press about
Google's new neural translation system, and
that really is everything that it's claimed
to be.
Like it really is basically one giant neural
network that can translate any pair of languages.
The accuracy of these translations is far
beyond anything that's happened before, and
the basic structure of that neural net is
(as we're going to learn) not that different
from what we've already learnt, it's just
going to have this one extra step, which is
Attention.
Depending on how interested you guys are in
the details of this neural translation system,
it turns out that there are also lots of little
tweaks.
The tweaks are kind of around like, Okay you've
got a really big vocabulary, some of the words
appear very rarely.
How do you build a system that can understand
how to translate those really rare words,
for example.
And also just the kind of things like how
do you deal with the memory issues around
having huge embedding matrices of like 160,000
words and things like that.
So there's like lots of details, and the nice
thing is, particularly because Google has
ended up putting this thing in production,
all of those little details have answers now
and those answers are all really interesting.
There aren't really, on the whole, great examples
of all of those things put together.
So one of the things interesting here is that
you'll have opportunities to kind of do that.
Generally speaking, the blog posts about these
kind of neural translation systems tend to
be kind of at a pretty high level.
They describe roughly how these kind of approaches
work, but Google's complete neural translation
system is not out there.
You can't download it and see the code, so
we'll see how we go.
We'll kind of do it piece by piece.
So I guess one other thing to mention about
the memory network is that Keras actually
comes with an end-to-end memory network example
in the Keras github.
Which, weirdly enough when I actually looked
at it, it turns out doesn't implement this
at all.
And so even on the single-supporting-fact
thing, it takes many many generations and
doesn't get to 100% accuracy.
I find this quite surprising to discover that
once you start getting in to some of these
more recent advances, not just a standard
CNN or whatever, it's less and less common
that you actually find code that's correct
and actually works.
And so this memory network example was one
of them.
If you actually go in to the Keras github
and look at examples and go and have a look
and download the memory network, you'll find
that you don't get results anything like this.
And if you look at the code, you'll see that
it really doesn't do this at all.
So I just wanted to mention that as a bit
of a warning.
You're kind of at the point now where you
might want to take it with a grain of salt
blog posts you read or even some papers that
you read.
It's well worth experimenting with them and
you should start with the assumption that
you can do it better.
Maybe even start with the assumption that
you can't necessarily trust all of the conclusions
that you've read, because the vast majority
of the time in my experience putting together
this part of the course, the vast majority
of the time the stuff out there is wrong.
Even in the cases like the deeply respected
Keras authors and Keras source code, even
in that case this is wrong.
I think that's an important point to be aware
of.
I think we're done so I think we're going
to finish 5 minutes early for a change, I
think that's never happened before.
So thanks everybody.
And so this week hopefully we can have a look
at the Data Science Bowl, make $1,000,000,
create a new PyTorch approximate-nearest-neighbors
algorithm, and then when you're done maybe
figure out the next stage for memory networks.
Thanks everybody.