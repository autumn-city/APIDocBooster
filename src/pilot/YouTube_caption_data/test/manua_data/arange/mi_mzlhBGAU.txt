Hey there and welcome to this video! Today, we 
are going to implement a pretty cool reinforcement  
learning paper called "The Sensory Neuron as 
a Transformer: Permutation-Invariant Neural  
Networks for Reinforcement Learning". This is 
the official GitHub repo the authors open sourced  
and it contains both training and evaluation 
scripts together with pre-trained models. The  
implementation I prepared for this video 
is heavily inspired by this repo, however,  
I decided to only focus on a single task: 
the CartPoleSwingUp. The official code is  
way more general since it can handle all 
the tasks described in the paper. Anyway,  
in my opinion implementing the AttentionNeuron 
for the CartPole task should be enough to  
understand the most important concept. As 
always all the credit goes to the authors  
and I just want to stress that I'm not affiliated 
with any of them. Finally, I apologize in  
advance for potential misinterpretations of 
their work and bugs in my code. One thing  
I would like to highlight is this amazing and 
interactive website that the authors created.
One can experiment and play around with the 
pre-trained models and I will use it later  
to explain some important concepts. Let me try to 
explain in my own words what the paper is about. I  
would definitely recommend you to stop the video 
here and go read the paper to have more context.  
First and foremost, the authors want to create a 
permutation invariant policy network. What does  
it mean? Well, it simply means that no matter 
how we shuffle the input features the output  
action is going to stay identical. The second 
goal is related to the first one and it is to  
have a single function that processes each feature 
independently. Here, I name this function f_local  
and it is only after this independent processing 
that we try to combine the local information  
into a global picture using the f_global function. 
In the next couple of slides I'm going to discuss  
the exact architecture of the policy network, 
however, let me just point out that the paper  
actually demonstrates two very interesting things. 
First of all, one can actually successfully train  
agents like this and they work pretty well which 
is in my opinion not obvious. And second of all,  
agents trained like this seem to be immune to 
noise which I find pretty surprising too. Anyway,  
let us now focus on more specific things. As 
mentioned before to make things simple both these  
slides and the code are going to assume that we 
are dealing with the CartPoleSwingUp environment.  
The policy network has two inputs. The first one 
is the observation vector or I will sometimes  
refer to it as the feature vector. It is composed 
of five real numbers: the position of the cart,  
the velocity of the cart, the cosine and 
the sine of the pole angle and finally the  
angular velocity. At each time step we will get 
a new observation vector from the environment.  
The second input is the previous action and it 
represents the force we applied to the cart.  
Our objective is to take these two inputs and 
define a function or more specifically a neural  
network that is going to turn them into a new 
action. Here you can see the entire pipeline of  
the most important steps. To get from the inputs 
to the output first of all we apply element-wise  
functions to the inputs. This is in line with 
the locality requirement that I talked about.  
And then we generate 3 new matrices Q, K, 
V which stands for query, key and value.  
And they are going to be used in an attention 
module. And what comes out of this attention  
module is a latent code which is going to store 
all relevant information about the observations  
and the previous action. And finally we will take 
this latent code and just run it through a linear  
module to get the final prediction which is 
the new action. Regarding the terminology,  
this entire pipeline describes the forward pass 
of our policy network and everything before the  
linear module is called the AttentionNeuron layer. 
So how do we create the attention inputs? Before I  
start explaining them let me just point out that 
in the top right corner you can see the variable  
names that we are going to be using in the code 
plus their actual values that were taken from the  
paper. Anyway, let's start with the easiest one 
which is the value matrix. It is going to be equal  
to the observation vector. Also, I added an extra 
dimension to it so that we can use it in matrix  
multiplications. Then we will have the query 
matrix which is actually not going to contain  
any learnable parameters and it will just be the 
so-called positional encoding. If you are not  
familiar with it it is a way how one can encode 
the order information and it is used heavily  
in natural language processing. The even columns 
are just sine functions with decreasing frequency  
whereas the odd columns are cosine functions with 
decreasing frequency. Lastly, we want to create  
the key matrix. It is basically motivated by two 
ideas. First of all, we want it to depend both on  
the observations but also on the previous action. 
That is why we concatenate these two. Second of  
all, at a fixed time point we would like to not 
only have access to the most recent observations  
and the previous action but ideally also all the 
previous observations and actions that we gathered  
during our current episode. And in a way this can 
be achieved by keeping a hidden state for each of  
our features. Then we have an LSTM cell that 
takes in the previous hidden state and the new  
observation and action and spits out a new hidden 
state. And that is exactly what you see on the  
screen. Let me just stress that the LSTM cell has 
learnable parameters which are shared among all  
the features. Anyway, the K matrix is nothing else 
than the output hidden state of our LSTM cell.  
Here you can see the attention computation 
together with the shapes of the matrices.  
We take the query matrix and the key matrix and 
project them in a linear fashion. And then we just  
multiply them. The way you can interpret this is 
that for each row of the query matrix we want to  
know what the most similar rows in the K matrix 
are. And after we scale these similarities and  
run them through the hyperbolic tangent activation 
we get attention weights. And finally we multiply  
the attention weights and the value matrix to 
get our latent code. Let me just stress two  
things. This attention computation will work out 
for any number of features and the final latent  
code size is dictated by the number of embeddings. 
This means that our model will be able to accept  
feature vectors of arbitrary size. Second of all, 
if you disregard the fact that each feature could  
have a different hidden state this entire pipeline 
of going from the observation to the latent code  
is permutation invariant. And finally, we take 
this latent code and we apply a linear module  
to it and we get our final action. And since 
this latent code was permutation invariant with  
respect to the observation this new action is 
also going to be permutation invariant. Also,  
let me point out that in theory we could replace 
this linear module by something more complicated  
like the multi-layer perceptron, however, a 
linear module should be enough to make a good  
model. And yeah that is basically all you need 
to know about the policy network. So let's start  
coding. First of all, we want to implement our 
new networks in PyTorch and write some utilities.
Before we start implementing the 
fancy permutation invariant network  
let us just create a benchmark network 
that we are going to use for comparison.  
More specifically let's just 
create a multi-layer perceptron.
We specify the number of features and it 
is something we need to know in advance.  
hidden_layer_sizes is just a tuple of integers 
representing how big the different hidden  
layers are. And internally everything will 
be actually stored in the Sequential module.
Here we define a new tuple containing all the 
layer sizes. So we start with the zeroth layer  
which is let's say just the feature vector 
and finally we also append this output layer  
which has just one element and that is 
because our action is just a real number.
And here we iteratively define a bunch of  
linear modules followed by the 
hyperbolic tangent activations.
We unpack all the layers 
into the Sequential module.
This part is definitely not standard because 
what we are doing is taking all the parameters  
that our network has and we are telling Torch 
that we don't care about the gradients. And  
this is actually something proposed in the 
paper because the author suggested that  
for the CartPole task the best approach 
is to do a so-called direct policy search  
and use an evolutionary algorithm that 
doesn't need gradients. So in other words,  
we only use Torch to define conveniently the 
forward pass, however, we don't care about  
its autograd functionality at all because 
we won't use gradients for the optimization.
The input to the forward pass is just the 
observation tensor. Let me just point out that  
one difference to I guess standard deep 
learning code is that we don't necessarily  
work with tensors that have the batch dimension 
and that is because when we roll out the task  
we will only have access to one single step at a 
time. The output is nothing else than the action.
And the actual implementation is just a one-liner. 
What we do is that we temporarily prepend the  
observation tensor with the batch dimension 
so that it's compatible with Torch. We run the  
forward pass of our network. And finally, we only 
extract the first sample in a batch because there  
was just a single sample. Two small comments. In 
theory, we could have also included the previous  
action as one of the inputs but whatever. And 
the second thing is that we are guaranteed that  
this return value is going to be in the interval 
(-1,1) because we apply the hyperbolic tangent  
activation. And now we want to start building all 
the different components of the AttentionNeuron.  
Let's start with the query tensor and as discussed 
before it is going to be the positional encoding.  
So it depends on two parameters. The first one is  
the number of embeddings which is 
just the number of rows of our table.  
Then the hidden size which is just the number 
of columns. What we return is a numpy array  
that represents the positional encodings. 
For your convenience this is the formula.
This is just a helper function 
that is going to compute the angle.
And this helper function is basically 
just constructing a row in our table.
We define the entire table of angles row by row.
We apply the sine function to all the even 
columns and the cosine function to all of the  
odd columns. Let me just stress there is nothing 
learnable about this table and it only consists  
of constants. Anyway, this is exactly how it's 
going to look like we have 16 rows and 8 columns.
Now we would like to implement a module that 
is going to compute the attention matrix  
using the key and the query tensors.
So first of all, we need to provide the projection 
dimension. We have to provide the hidden size  
which is equal to the number of columns of 
both the Q and K tensors. And finally we  
can choose whether to enable or disable scaling. 
Internally, the projections will be done using the  
torch.Linear module and we'll have a separate 
one for both the key and the query tensor.
We instantiate two linear modules 
and we set the bias equal to False.
And we also prepared the scalar.
So the forward pass will accept two inputs. The 
first one is going to be the query tensor that has  
the shape of number of embeddings times hidden 
size and the second input is going to be the  
key tensor of shape number of features times the 
hidden size. We then return the attention weights  
that are of shape number of embeddings 
times number of features and what's a  
little unusual is that they won't sum up 
to one in general because we are going to  
use the hyperbolic tangent activation 
rather than the softmax. By the way,  
this is the exact formula that we're 
trying to implement in the forward pass.
Here we project the query tensor.
Here we project the key tensor/
Here we matrix multiply both of the projections.
We divide by the scalar and this division  
is element-wise so it's not 
going to change the shape.
Finally, we apply the hyperbolic 
tangent activation and again it's  
element-wise and it doesn't change 
the shape. And that's it. Now we can  
finally start putting things together. Let's 
implement the entire AttentionNeuron module.
We provide the number of embeddings which 
is a hyperparameter and this will actually  
end up being the length of our latent code. 
Then we provide more hyperparameters. Namely,  
the projection dimension and the hidden size. 
Internally, we will have this hidden state  
and note that it is going to be specific to the 
LSTM which means that it's actually two tensors.  
Then we will have this LSTM cell module and as 
discussed it's going to be used to create the  
key tensor. Then we will have an instance of the 
AttentionMatrix module that we just implemented  
and finally we will have this buffer 
that is going to hold the query tensor.
Here we store the parameters as attributes.
We initialize the hidden state with None 
but as you will see in the forward pass  
this will basically mean that 
we will populate it with zeros.
Here we instantiate the LSTMcell. The reason why 
the input size is 2 is because we will concatenate  
one element of our observation vector (e.g. the 
position of the cart) with the previous action.
We instantiate the AttentionMatrix. If you're 
wondering why we set scale equal to False (which  
we know means that we will just divide 
by 1) if i'm not mistaken this is what  
the authors did in their code 
so i just used the same logic.
Here we use the utility function that 
we wrote and we generate the position  
encodings, we convert them to a torch.Tensor 
and finally we register it as a buffer.
Similar to the multi-layer perceptron 
forward pass we see that we provide  
an observation and again we are not working 
with tensors that have the batch dimension.  
Additionally, we also provide a number 
representing the previous action. What  
the forward pass returns is latent code 
which has a shape of number of embeddings  
and it also outputs the attention weights in case 
we want to inspect what happened inside of the  
attention. Here you can again see the formula for 
computing the key and it will be nothing else than  
a new hidden state. Anyway, just to avoid 
confusion the h and c hidden states that you  
see in this formula will be grouped together 
in the code into a single tuple called hx.
Here we just concatenate our observation 
tensor with the previous action.
In case the hidden state was None or in 
other words if this is the first time  
we run the forward pass we populate 
both of the hidden states by zeros.
Here we call our LSTM cell.
We create a variable for our query which 
is nothing else than the internal buffer.
The K tensor is nothing else 
than our output hidden state.
The value tensor is nothing 
else than the input observations  
and we just add a new dimension to 
it so that it's easy to do matrix  
multiplication and here you can see the 
exact formula to generate the latent code.
Here we call our attention matrix 
module and we get the attention weights.
Finally, we matrix multiply the attention weights 
and the value to enter. By the way, we again  
apply the hyperbolic tangent activation. It's not 
something I showed in the formulas but again I  
think I saw it in the official implementation 
so I just apply the same activation.
We squeeze out the dummy dimension and 
we end up with a 1D tensor which is  
our latent code and we return it 
together with the attention weight.  
And yeah that's it. Let's just play 
around with the AttentionNeuron module.
We defined all the important hyperparameters 
and we want to instantiate our module.
First of all, let me demonstrate that our 
module doesn't care about the number of  
input features the observation vector has.
We iterate through different number of 
features and we always make sure that  
we set the hidden state equal to None. And 
as you saw during the first forward pass  
it will be actually populated with tensors 
of zeros so we actually want to reset it.
Here we set 0.5 as the previous action 
and we generate a random observation  
vector with the corresponding number of features.
The shape of our latent code is always 16 
which is the number of embeddings. And it  
is independent of the number of input features. 
This is definitely a very cool property of the  
AttentioNeuron. Second of all, let us 
look into the permutation invariance.  
I create an observation tensor with five 
features which is in line with the CartPole task.  
Again, I make sure that we always reset 
the hidden state which is equivalent to  
just starting a new rollout or episode. 
Here I generate a random permutation.
And then we actually permute the observation 
tensor and send it through the AttentionNeuron.
In all three of the cases the latent 
code did not change. So in other words,  
if the hidden state is identical between 
different calls then yes the permutation  
invariance will hold. However, let 
us try to remove the resetting.
And here you can see that if the hidden state 
is not normalized you will get different latent  
codes. And just to kind of illustrate the point 
I'm going to use the interactive website the  
authors created. So let us see what happens when 
we click on the "shuffle observations" button.  
As you can see it clearly disrupted the 
performance of the model, however, after a couple  
of steps the model is back to working perfectly. 
Shuffling the observations actually does have a  
temporary effect on the performance of the model 
because unfortunately the hidden states cannot  
be shuffled or permuted accordingly. Because we 
don't know what the ground truth permutation was.  
However, it seems like after a couple of steps 
the hidden states are correctly updated too.  
And there is one more thing to do and that 
is to create the entire policy network.
Here again we have the 3 hyper parameters exactly 
the same ones as for the AttentionNeuron module  
and internally we will have an 
instance of our AttentionNeuron module  
and also an instance of a linear module.
We instantiate the AttentionNeuron.
And we instantiate our linear layer 
that is going to input the latent code  
and output the new action.
We do exactly the same thing 
as we did with the multi-layer  
perceptron. We disable gradient 
computations for all our parameters.
The inputs to the forward pass are exactly 
the same as for the AttentionNeuron.  
It's the observation vector and it is also the 
previous action and the output is the next action.
We run the forward pass over the AttentionNeuron.
We take the one-dimensional latent code,  
we temporarily add a new dimension to it 
so that it's a batch of a single sample,  
we apply the linear layer and finally we 
apply the hyperbolic tangent activation.
And here we just get rid of the batch dimension.
So we have written our deep learning modules 
and our goal is to turn them into reinforcement  
learning agents and ultimately train them. 
However, this will require some extra code that  
some of you might find boring so I definitely 
encourage you to check out the chapters that  
I provided for this video and skip all the way 
to the end where I discuss the results. Anyway,  
if you're staying let's go back to coding. To 
make things more readable and extendable we  
are going to introduce a concept of a Solution 
which is just a unified high level interface  
and its goal is to hide all the lower 
level details related to Torch modules.
We are going to create an abstract class called  
Solution and the idea is that we 
are going to subclass it twice.  
Once for the PermutationInvariantNetwork 
and the second time for the MLP.
First of all, we make an assumption that 
the solution has an attribute called policy  
that will actually store the Torch module.  
Here we define an abstract method. What it means 
is that the child will actually have to implement  
this. This specific method is called clone 
and its goal is to copy the current solution.  
The goal of this abstract method is to get 
the next action given the current observation.
This method is supposed to return the 
number of features the model expects  
and we will have a convention that if this 
method returns a None then it means that  
it can take in an input of any size. As 
we saw [PermutationInvariantNetwork] it  
doesn't matter how big the feature vector is 
whereas for the MLP it will be a fixed number.
This method is going to reset the 
solution and the specific use case  
is going to be resetting the hidden state 
of the PermutatioInvariantNetwork and later  
on we will make sure that this method is 
called at the beginning of each rollout.
First of all, note that this is not an abstract 
method anymore and the reason for this is that  
we will provide an implementation in 
this parent class. Its goal is to get  
all the learnable parameters our neural 
network has and flatten them into a 1D array.
Here we iterate parameter by parameter, 
we cast the torch tensor to a numpy array  
and finally we flatten this array. 
And as you can see I'm not necessarily  
sending this tensor to a CPU because in this 
video and in this code we're going to assume  
everything takes place on a CPU. Which by 
the way is a simplification and I think the  
original code handles the possibility 
of running the forward pass with GPUs.
Finally, we concatenate all the 1D arrays into 
a single huge 1D array and the reason why this  
flattening needs to take place is because we 
are using let's say a blackbox evolutionary  
strategy that I will discuss later. And it doesn't 
really care about the fact that some parameters  
come from a linear module or some other 
come from the attention module - there  
is no structure. We implemented the getter 
but we also want to implement the setter.
This method is going to take 
in a 1D numpy array of all  
learnable parameters and it's actually going to 
go inside of the neural network and just assign  
all these parameters to the corresponding weights.
We're going to keep track of what the 
starting index is and what the end index is.
And then we're going to take the torch.Tensor 
and we're going to redefine it in-place using  
the user provided parameters. Finally, 
we just update the start index and again  
this method is going to be used in the 
optimization because we will have this  
black box optimizer that is at each iteration 
going to suggest multiple different parameter sets  
and we will just want to evaluate how good our 
network is given these parameters. Finally,  
just a small method that counts up the number 
of parameters. And yeah that's it for our  
abstract class. Let us now subclass the 
solution class and create a MLPSolution.
So we provide the same hyperparameters 
as we saw before and internally we will  
store all the hyper parameters and 
also the actual policy network.
Here we store the hyperparameters and 
we also define the dtype to be float32.
We instantiate our MLP module,  
we cast all the parameters to the float32 
dtype and finally we set it to evaluation mode.
Here we define what cloning means.
Here we take the observation which is a numpy 
array, we cast it to torch tensor, we make sure  
it has the right dtype and then we run the forward 
pass. The number of features is going to be fixed  
and finally the MLPSolution will not have any 
resetting logic since there are no hidden states.  
Now we want to create a second child class and 
it's going to be the PermutationInvariantSolution.
We've seen these hyperparameters before. Again, 
we will store all the hyperparameters in this  
dictionary. We will have our policy network 
which is the permutation invariant network,  
however, what is different to the MLPSolution 
is that we will also store the previous action  
internally. That means that whenever we call the 
get_action the previous action will get updated.
We instantiate our policy network.
Finally, we initialize the 
previous action to zero.
This clone method is actually identical 
to what we did with the MLPSolution.  
I guess we could have put 
it in the abstract class.
So to get the action first of all we convert  
our numpy observation array to torch.Tensor. 
Then we also provide the previous action which  
we are storing internally and then we just run the 
forward pass of the policy network. Here we make  
sure that we update the previous action with what 
we just predicted and we also return this action.
Unlike the MLPSolution here we actually implement 
the reset method. There are two things that  
are happening. First of all, we make sure that 
the hidden state of our policy network or more  
specifically of the AttentionNeuron submodule of 
the policy network is set to None. That means that  
we are kind of deleting all the memories. And 
second of all we set again the previous action  
to 0. Finally, the number of features is going to 
be None this will mean that the number of features  
can be anything we want. Anyway, now we would like 
to actually implement the rollout logic using the  
OpenAI gym and the CartPoleSwingUp environment. 
The authors actually use their own implementation  
of this task which is supposed to be harder since 
the initial state can have more extreme values.
However, I did not really want 
to rewrite it from scratch or  
clone their repo and instead I just tried to look 
through GitHub for similar implementations. I  
found this repo which seems to implement more 
or less the same thing, however, it seems to  
be an easier version of the CartPoleSwingUp 
environment but it is not that big of a deal.
The author put it on PyPI which 
is great and we're going to use  
it. So let us quickly implement the rollout logic.
We just define the number of original features 
constant which is 5. It's mostly for convenience.
We define a custom exception and we're 
going to raise it in case the model  
number of features is not equal to the actual 
number of features inside of the observation.
First of all, we can choose whether we 
want to render the environment or not.  
This shuffle_on_reset flag will give us 
an option to randomly shuffle the features  
at the beginning of each episode and this 
is actually the way how we're going to test  
whether our agent is able to deal with shuffled 
features. Let me also point out that the authors  
investigated the possibility of shuffling during 
the rollout rather than just before we actually  
run the rollout and yeah our code is not going 
to contain this possibility. n_noise_features 
will control how many noise features we will 
inject into the observation vector and the  
main use case for this is to really test 
out whether our permutation invariant agent  
is immune to noise. Then we have two different 
random states. Finally, we have the maximum  
episode steps. By the way, in this video I use 
the words episode and rollout interchangeably.  
Internally, we will store the number of 
overall features which is the original ones  
plus the noise ones. We will have 
this number array that is going to  
hold the permutation indices. Also we 
will have the actual gym environment.
Here we make our environment.
Here we redefine the maximum episode 
steps. For the rest of this video  
the number will be always 1000 because 
that's what the authors used in the paper.
The number of overall features is the 
original features plus the noise features.  
Here we just create a range of integers from 
0 to number of features and this array will be  
actually used for the shuffling. Here we set the 
noise standard deviation to 0.1. I just took it  
from the paper. And when we are actually injecting 
noisy features inside of the observation it will  
always come from a Gaussian distribution 
with mean 0 and this standard deviation.
Now we want to define a method that will 
dictate what happens just before the rollout.
The only thing we actually do before the 
rollout is to make sure we permute the features.
Here we regenerate the permutation indices 
to be again a range of numbers and then  
if the user wanted to do the shuffling before 
each rollout we just shuffle it. And note that  
this shuffle method actually works in-place. First 
of all, we actually want to shuffle the features  
and second of all if the user wanted to have 
noise features we will insert them into this  
array. The input is the raw observation 
feature array that only has five elements.  
This is actually what the gym environment is 
going to give us at each step. What is returned  
is a modified version of the input it will be 
shuffled and it will contain noisy features.  
And this modified observation is exactly the 
one that goes on to our deep learning model.
Here we use the normal distribution 
to generate our noise vector.
Here we concatenate the original 
observation vector together with the noise.
Here we use our permutation 
indices to do the shuffling.  
Now we have everything to actually implement 
the rollout. The input is going to be the  
solution. So it's either the MLPSolution 
or the PermutationInvariantSolution.  
And we return a reward representing how good of 
a job our solution did during this given rollout.
Here we just wrote a check making sure that the 
number of features is compatible. First of all,  
we run the reset for rollout that we just 
implemented and as you saw this will do  
the shuffling of the permutation indices 
assuming the user wanted to shuffle.
Now we will reset the solution and if you don't 
remember it was a noop for the MLPSolution.  
However, for the PermutationInvariantSolution 
this would set the hidden state to None which  
means that we basically forget everything 
that happened. And lastly we also reset the  
gym environment and this will give us the very 
first observation. Optionally, we also render.  
So this episode reward will be the return value 
and we will actually iteratively add to it  
all the per-step rewards. done is a boolean that 
will signal whether our rollout has finished or  
not. And there are two ways we can finish. The 
first one is reaching the maximum episode steps  
that we set to 1000. Or the second option 
is that the cart goes off the screen.
We modified the raw observation which means that 
we will add the noise features to it and we will  
shuffle it. We send this modified observation 
through to our solution to get the next action.  
We call the step method of the gym 
environment which accepts the new  
action and it gives us the new observation, 
a reward and it updates the done variable.
And that's it for our rollout but again 
as I mentioned we are not able to shuffle  
during the episode but only before the episode. 
Before we start writing the training script let  
me quickly talk about the actual optimization 
algorithm we are going to use. It is called the  
Covariance matrix adaptation evolution strategy 
(CMA-ES) and what you see on the screen is a  
Python package that implements it. And I'm 
not sure why the authors decided to go for  
this specific optimizer, however, 
we're going to use the same thing.
More specifically, we will be using the 
ask and tell interface. The idea is that  
at each iteration we will ask the optimizer to 
give us a set of candidate parameter vectors  
and we are then responsible for actually assigning 
a loss to each of them. And once we have it we  
will just tell the optimizer what the losses are. 
Right, so let's implement the training script.
So first of all, let us write a save function 
that we will call during our training.
We simply create a tuple with two 
elements. First of them is going  
to be the solver instance and second 
of them is going to be our solution  
instance. Then we just pickle dump 
it on the disk. Now the goal is to  
write a get_fitness function that is going to 
evaluate how good or bad a given solution is.
So this function is going to input a 
solution and also a couple of other  
task related parameters and it is going to 
return a list of all episode rewards. And we  
will need to run this function for each of the 
proposed parameter vectors from the optimizers.
And the actual implementation is pretty 
straightforward because all the logic is  
actually implemented already inside of the 
task class. Now I'll quickly write a CLI.
Let me discuss a couple of important arguments 
and options. So the first positional argument is  
named "solution" and there are three options: 
linear, MLP, invariant. In the background,  
what we want to do we want to instantiate the 
corresponding solution class. And note that  
linear is nothing else than multi-layer perceptron 
with no hidden layers. And this logging folder is  
where we're going to put all the tensorboard 
related records and also the checkpoints. This  
parameter is very important because it determines 
how many rollouts we will do when evaluating the  
fitness. The higher this number the more precise 
estimate we will have. However, the slower the  
training is going to be. I took 16 because I 
believe that's what the authors did. Also we  
will implement a multi-processing logic and here 
one can specify the number of jobs. Here another  
optimization related parameter which is the 
population size and by default it is set to 256.  
And basically dictates how big a possible solution 
set is going to be at each iteration. I think the  
reason why the authors put it to 256 is because 
they had a cluster that had 256 CPUs. Again,  
we'll just set it to the same number. Finally, we 
can shuffle_on_reset during the training and we'll  
actually use it in some experiments to kind of see 
whether one can improve the multi-layer perceptron  
if at training time we do this shuffling 
so in a way it will be like augmentation.
We instantiate the tensorboard SummaryWriter 
and we dump all the parameters.  
And now we just want to map the string 
solution to an actual solution instance.
First of all, we define the linear solution 
which is nothing else than a multi-layer  
perceptron with no hidden layers. I 
know that here we need to specifically  
mention the number of features at training time.
Here we create a multi-layer perceptron with 
a single hidden layer size with 16 nodes.
Finally, the star of the show - the 
PermutationInvariantSolution solution  
with these hard-coded hyperparameters. But 
as mentioned these are the hyperparameters  
they used in the paper I believe. Now 
we would like to initialize our solver.
There is an option of providing a 
checkpoint that is useful for continuing  
training that was started before.
If there is no checkpoint our initial guess 
is going to be just zeros. Then we just  
simply instantiate the solver providing 
all the relevant solver parameters.
However, if the user does provide a 
checkpoint path we unpickle it. Now,  
we just take our get_fitness function and 
we set all the keyword only parameters.
If the number of jobs is -1 we actually want to 
use all the CPUs and if not it's just whatever the  
user provided. Finally, after all this work we can 
write the training or let's say evolution logic.
We use this multi-processing 
pool context manager to be  
able to do multi-processing in a very simple way.
Then we will iterate all the way 
up to the max number of iterations  
and here we ask the solver to give us a bunch of 
different proposals when it comes to parameter  
vectors and actually the size of this param set 
is going to be equal to the population size.
What we do in this iterable is that we go one 
by one through the parameter vectors and we  
just use our set_params method of the Solution 
class to actually register those parameters.
We basically go through each solution  
in our iterable and we just compute what 
the fitness of that given solution is.
And remember our get_fitness function was 
actually returning a list of per-episode rewards  
and here we just actually consider the mean of 
these as the final fitness of that given solution.
And since the CMA solver actually looks 
for minimums we need to flip the sign.  
And here we'll just concatenate all the 
parameters together for evaluation purposes.
Here we have a bunch of useful metrics.
Here we'll basically checkpoint 
our model and the solver  
every couple of iterations or 
if it's the very last iteration.  
Finally, we actually tell the solver how good 
or bad the proposed parameter vectors were.
If we send the keyboard interrupt signal 
the checkpointing will take place.  
That is our training script! I wrote a 
script that launches multiple experiments.  
First of all, we train a simple linear model and 
there is no shuffling taking place during the  
training. Then we train a linear model again, 
however, this time we're going to randomly  
shuffle the features before the rollout and that 
means that the fitness function should basically  
encode how good the model is when we give it 
randomly shuffled observations. This approach is  
very reminiscent of augmentations and instead of 
encoding a certain prior into the architecture we  
want the model to learn permutation invariance 
from data or at least partially. The third  
and the fourth experiment are the multi-layer 
perceptrons with and without shuffling. Finally,  
we have our permutation invariant solution and 
let me just stress that at training time we are  
not going to do any shuffling whatsoever. Also you 
probably noticed that the number of iterations for  
the permutation invariant model is very high and 
let me just warn you that unless you have dozens  
of CPUs you will not be able to train it. It is 
totally possible that my modifications to their  
code introduced major slowdowns. Anyway, it is 
what it is. I actually ran this entire script  
already and let me just show you the results. Here 
we have the tensorboard and let me show you some  
plots. Let's look at the max generation metric. So 
each iteration or let's say generation the solver  
proposed 256 different solutions and this graph 
basically shows the best solution per generation.  
Let me just stress that the upper bound of this 
metric is 1000 which would mean that we held  
the pole completely straight for 1000 steps. 
However, since the environment starts with the  
pole being down it is virtually impossible to 
actually reach this upper bound. Here you can  
see the five different experiments and clearly 
the multi-layer perceptron model corresponding  
to the red plot without augmentations was the 
model that was able to solve this task and it  
didn't really take that many iterations. The 
linear model in dark blue did not seem to have  
enough capacity to learn the tasks perfectly, 
however, it almost hit the 800 mark. The orange  
plot is the permutation invariant agent and 
as you can see it keeps on going up, however,  
at a very slow rate. I'm sure if I continued the 
training it would get to higher scores, however,  
running these 9000 iterations already took more 
than a week on my machine. Finally, if we look at  
the pink and the bright blue experiments they 
were not really able to hit very high scores.  
Note that these experiments correspond to the 
augmentation setup where we were shuffling before  
rollouts during the training. Anyway, the metrics 
we were tracking with tensorboard are interesting,  
however, the main question we want to answer is 
the following. How good are these models when  
we shuffle the features before each rollout and 
what happens if we inject noise features into  
the observation vector? Let me just stress that 
the models trained without augmentations never  
came across shuffled observations at training 
time and we want to know what would happen to  
their performance if we do shuffle the features. 
And for this I had to write some extra code that  
I'm not gonna present in this video. However, you 
can find it on GitHub. Here are the results. This  
plot investigates how good or bad different 
models are at dealing with feature shuffling.  
On the x-axis we have six different models. A 
small note here. Invariant ours corresponds to the  
model I tried to train from scratch whereas the 
invariant official is the permutation invariant  
model with the weights the authors open sourced on 
GitHub. The y-axis represents the overall reward.  
Finally, the two colors represent whether 
shuffling was taking place or not. And the  
actual distributions were obtained by running 200 
rollouts. So if we start from the left we see the  
MLP model clearly is very good when there is 
no shuffling, however, as soon as we introduce  
shuffling it basically stops working. Then we 
have the MLP model with shuffling augmentations  
and here we see that without shuffling the 
performance decreases compared to the no  
augmentation model ,however, if we start shuffling 
the features the model gets worse but not by much.  
This would suggest that the augmentations did make 
the model immune to shuffling to a certain extent,  
however, not completely. Then we have the 
permutation invariant model and as you can  
see especially the model with the official weights 
is performing really well and its performance is  
not influenced by shuffling the features at all. 
By the way let me stress that these results are  
not comparable to those presented in the paper 
because we have a different environment that is  
supposed to be way easier. Finally, we have the 
linear models and here we see the same pattern  
as with the MLP model. Without augmentations and 
no shuffling the model is pretty good, however,  
as soon as we shuffle it is basically useless. 
However, the augmentations do make the model  
immune to shuffling to a certain extent but 
not completely. Here we have a closer look at  
the permutation invariant model using the official 
weights. On the x-axis we have the number of noise  
features we injected into the observation vector 
and the rest of the figure stays the same as the  
previous one. And quite amazingly you can see that 
the model is basically able to ignore the noise  
features when there are five of them. And when we 
increase the number of noise features there seem  
to be rollouts where the model struggles but still 
overall it seems to be extremely robust to noise.  
And I believe this is one of the main properties 
that the authors try to highlight in the paper. It  
is pretty cool I guess. One could question whether 
their generated noise was really that hard to spot  
since it was sampled independently at each step 
with a fixed mean and standard deviation, however,  
I did not really have time to do more experiments. 
Anyway, this property is definitely impressive and  
in my opinion not obvious. Anyway, that's it for 
today. I just generated a bunch of videos of the  
actual rollouts so feel free to inspect them. Let 
me stress that one thing I did not really cover  
in this video was shuffling the features during 
a rollout which is a more challenging task than  
only doing it once before the rollout. Anyway, 
thank you very much for watching this video.  
And again I would really like to thank the authors 
and give them all the credit. I hope I managed to  
simplify their multi-purpose code, however, maybe 
I didn't. Who knows. I would definitely encourage  
you to check out their code too. It is more 
than likely that my code contains mistakes and  
misses important details. Additionally, I 
would like to point out that the authors  
were really responsive and supportive when 
I asked questions on GitHub so I definitely  
appreciate that. If you enjoyed this video 
don't hesitate to like it leave a comment  
and subscribe. This is probably the best way 
to let me know that you enjoy my content!  
Also, I'm very open to suggestions and making 
these videos better and therefore constructive  
criticism is more than welcome! Anyway, have a 
nice rest of the day and see you next time!!!