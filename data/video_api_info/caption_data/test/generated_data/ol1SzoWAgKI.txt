okay so good evening everyone how are
you guys doing so in general about life
in general about our 576 a weekend so
some students came to an officer and
maybe we can discuss some aspects of the
assignment later on towards the end of
this lecture okay yeah so we are on like
fifth lecture for this course right so
just to kind of recap whatever we did we
started with regular classification
pipeline and that's you know it's
basically a recap of what you have seen
in previous courses but the use is that
you will see that when you use PI tours
it turns the flow you need to think in
terms of optimization of the module
model part of the modules cost function
of the modules and then essentially
you're you know doing a for loop over
folks
or batch sizes different batches of
updating the parameters of your MA
updating the parameters of your model
right so that's why that first lecture
was like that so it's like think
everything in a modular way so that you
know what to change sometimes the cost
function has to change for example if
you're doing not just object
classification the stuff that we saw for
CN NS but if you're doing of the
classification but also you want a
bounding box okay around the object then
your training leader should have
bounding box information but your last
function will also change it will not be
just the cross entropy laws but cross
we lost maybe plus errors in bounding
box coordinate values for example so
this is an example of thinking
everything in terms of modular way cost
functions there is some richness and
understanding you need to have how to
define good cost functions how to define
models models start will see like for
example in touch you would primarily
build on top of n n dot module and then
dot linear and in or continuity or
something like that and optimization we
haven't seen much except for you know
simple gradient descent with fixed
learning rate there variations and there
also other techniques called atom and
momentum and so on but we'll see that
later and maybe in the next lecture or
the lecture after that ok so that's
first lecture was just about setting the
landscape of thing everything in terms
of modules and this is actually true for
many other models so even if you change
this to training your linear class you
know training linear classifiers or
training random forests or training
support vector machines when you have
the similar architecture and different
different I guess ecosystems have
different ways of thinking about these
things for example if you are familiar
with the scikit-learn library this is a
certain way that they have defined most
of their classifiers regressors and the
certain way that they you know create
dress test training validation split and
so on so this is just a convention that
I guess different communities adopt and
we'll see what I guess sense the flow
and fighters communities
you know how the how the regular how
most of the code looks very similar
except some changes in these modules
right so second lecture we saw feed
power networks because those are
essentially I guess traditionally you
would have seen artificial neural
networks or multi-layer perceptrons it's
all the same name but you know trying to
make those networks have higher depth
and also compose them with other types
of layers and the third lecture was
essentially about certain type of layer
called the convolutional layer and we
understood okay it has lesser number of
parameters compared to a fully connected
layer and and it also has some
so properties that is exploiting for
image type of data right spatial
invariance and so on and we also looked
at a couple of other layers I mean one
layer in in this lecture called you know
as a max pool which is like just
reducing the number of parameter so that
the tensor sizes are not huge when you
kind of get closer to the getting with
the scores score vectors okay in the
last lecture we saw the notion of
embeddings right so we saw we actually
just quickly run through some existing
national language processing models like
topic model we saw lady and additional
allocation okay so that's one type of
modeling I saw it has its own you know
field of study actually it's got its in
the field of problem C graphical models
and hopefully we will cover few tidbits
of that you know feel during this doing
this lecture during this course but we
saw one key concept which is the notion
of embeddings and they were nothing but
if you had things which are not numbers
already so if you have features which
are not numbers which essentially means
features which are categorical variables
things like you know movie names or
things like strings or words then you
somehow have to have a numerical
representation for them and for each
word or each such object which is not a
number you are ascribing vector to it
okay and you are essentially learning
that vector in the embedding you know
learning that vector and that vector is
essentially called an embedding okay so
essentially whenever you have objects
like you know graphs or texts you know
even characters are even whole sentences
you can always embed all of them into
all these I guess calming total objects
are things which are richer into into
numbers okay which is essentially
vectors because most of neural networks
is mostly about working with numbers so
you need to do that pre-processing of
getting other objects to numbers or
vectors right so that was last lecture
and today we're going to look at so so
at that point since last lecture we kind
of shifted away from images we went to
textual data
and the next lecture is trying to now
work I guess between text and images
we're going to look at some sort of a
temporal relation so temp text has their
poor relations right so a sentence
proceeding another sentence in a
paragraph has some temporal aspect to it
you wouldn't obviously jumble the
sentences themselves or you wouldn't
even jumble the words because then the
sentence would not make sense so there
is some temporal aspect and our n ends
are just some extensions of regular I
would say feed-forward neural networks
that capture you know that that make us
easy to capture this temporal structure
ok so if you did not know Erin else you
could still you know somehow you know
and I hope they come up with a nice
model where you are able to capture
temporal aspect okay like I guess the
thing that you're shooting
ringabel is time series time series has
temporal aspect you cannot just jumble
the you know stock prices across days
and then still do separate some
prediction right so you want to capture
temporal aspects so that's today's
lecture and what the goals the goals are
first to kind of understand what type of
prediction tasks we can do which have
these temporal structure which you know
which are which I'm calling a sequential
dependencies the second point is what is
an RNN an RNN is just a concatenation of
a bunch of regular feed-forward networks
okay so that concatenation of a bunch of
FIFO networks for a few times like five
times of ten times is essentially what
an RN is and so that's the idea that you
need to get get get from this from
today's lecture okay that's a goal so
you should it should be demystified
completely is just a copy of a Fe power
Network or some variation like that few
times the third third goal is to get a
little bit of more detail into what are
some popular RNN
cells so when you copy you know I said a
fee for network there is some popular
variation called
LS TM so that's one block which you copy
several times so we will get into the
details of some I guess will ascribe
some semantic meaning to how that
network was designed okay so
lsdm is like a small variation of a
power Network and there is some I guess
semantic meaning that people ascribe to
how they design that network and we'll
get there that's just to kind of
demystify when you see L STM's or gr use
or variations like that in scripts that
you see online or you're in your own
exercises then you should know what that
block means and you don't have to coat
the inside of the block you don't have
the code that's architecture from
scratch but you should know when you
call just like you call con 2d you know
eventually you're not going to write a
convolutional layer from scratch where
you kind of do a for loop and move your
filter across similarly you will call
lsdm block in your in your NN dot module
and so you need to know what's going on
inside and the last part is somewhat of
a high-level we will just look at some
little bit more detail into what are
these you know this class of problems
called sequence is a sequence to
sequence problems okay
actually yeah even something like
language modeling that we saw last time
using CNN's actually not using CNN's
last time we saw using Marco hidden
regular Markov models right so to model
language so we'll see maybe that's our
application of a sequence of sequence
problem okay that's a type of sequence
the sequence problem any questions about
the lay of the land so far okay so
okay so they said today I will try to
cover the you know understand ordinance
and lsdm so let's start okay I don't
know how to hide that but so let's start
with you know even and before I get to
what you know what are the block
what are the blocks in an RM n what is
what is the sequence of sequential
sequence to sequence learning a family
of problems okay so the so here in terms
of block diagram you just illustrating
what are the different types of
prediction problems you can have so the
one that should be very familiar with
familiar to you is the one on the
extreme left which is the usual way to
think about classification problem so
you have some pick size feature vector
maybe even think of shallow models right
some feature vector maybe an image or a
tabular data so that feature vector you
pass it through a green block which is
essentially for example and you will
network or you know some other does
segregation model and so on and then you
get some output it's potentially a
single vector optics dimension for
example if you're classifying ten
classes in safar then then your output
is going to be ten dimensional vector
okay so but there are just extensions or
just I guess bigger sub bigger classes
of problems which are as follows so here
it's the second diagram on the on this
on this slide is about the problem of
image captioning okay where the input is
also fixed size it's an image and then
the output is actually a caption for
that image and a caption is going to be
texture so the text the number of words
in the text caption is can change based
on what's in the image okay so let's say
you could hypothetically magically
figure out you had data which had images
captions and somehow you trained a
network which could go from image to
captions you need to kind of think of
that you know every image should not
have the same number of words as a
caption okay so an image can have
different lengths captions right so
that's something where the input is fix
but fixed size but the output is could
change it could be two words could be
ten words and could be 20 words okay the
third one is where the input is now the
input is now variable and this is the
version that we have already seen last
time where we were doing sentence you
know sentence classification or
sentiment analysis where the input is a
sentence and then the output is just one
label whether it's positive or negative
or it could be you know multiple labels
not just positive negative but sarcastic
humorous you know angry different things
for example so so there the input you
know when we're looking at it last time
I made the sentence a matrix right but
the but different sentences will have
different lengths of different number of
rows and the corresponding matrix right
so the CNN architecture that we saw the
block should be such that irrespective
of the size of the matrix in terms of
the number of rows dimensions is fixed
let's say it should still be able to do
its filtering operation and still be
able to compute the same dimensional
output scores okay two dimension for
example if it's post a negative class we
you should check offline whether the
previous architecture we saw conforms to
that dimensionality aspect or not next
is many to many where as you should also
input length is arbitrary output length
is arbitrary so for example machine
translation which is actually going from
one language to the other your language
from English to Lhasa French than the
number of words in English for a
sentence and number of words in French
for a sentence could be different and
you still need to make that prediction
so given the English sentence you want
to give the best French translation of
that okay and the last one is also many
too many but here
since in in machine translation you have
to wait till the end of the sentence to
know what the sentence means right you
can't just looking at each word he just
wanted not you don't want it immediately
translate because then you then you
don't know what the sender's actually
meant but this is a question whether it
is an answer whether it had something
else coming up coming up later later in
the later part of the sentence it's not
here so so this this I this architecture
which I put star and circle
they're just differing in terms of that
consideration okay so so these are I
guess different variations of sequence a
sequence problems and we'll see a few
more concrete examples after we go
through lsdm so the key idea of you know
even forget about RNN the key idea that
we want to move towards now since since
we saw some fixed size classifiers is
the notion of persistence which is a
notion of temporal correlation between
some information and some information
that precedes the current information
okay what do I mean by that you can have
so if you look at the previous slide if
you look at sentiment analysis the
information the information from let's
say the first first word in the sentence
first word in the first few words in the
sentence have a bearing on how you
perceive the next few words in the
sentence so there's all I'm trying to
say is that you don't want to think of
this as a bag of words representation
you wanna think of there is some
temporal aspect to the sentence okay so
for example the initial mode or negation
or negation words like not and other
words like that they influence what's
coming later okay so you can you can say
there's a negative review and then or
this is sarcastic review and phrase it
completely in the next few sentences
that means that that review should be
negative right so there is this temporal
aspect that you wanna exploit and
let me specify that with you know what's
happening with previous models so let's
say think of FIFA on your network then
you're basically doing things like this
right so you're giving a given now image
you're just classifying it as maybe just
doing classification so you find you
know finding the object class in that
image okay now think of a movie then you
have a sequence of images now what you
classified in the first frame may have
bearing on what you would like to
classify in the next frame okay maybe if
it's the same maybe is the same of the
assisting cross across frames across
images of that movie okay so that's a
very simple example of you know that you
want to exploit temporal aspect the
other I'm just doing for per input
classification example okay and there
are different ways to capture temporal
aspects one one thing that you might
have seen in traditional I guess shallow
modeling I guess not shallow modeling
but traditional models is a idea of
autoregressive moving average models so
just think of data being numerical now
then you know how to capture temporal
aspects you can have peeth order terms
and then therefore the peeth number that
you saw or p days ago the stock price
that we saw has an influence on the
return that you will see today for
example right so that's a notion of a
temple temporal ask that you're
capturing now I like to think of you
know you're going from X to Y right so X
to have I had some prediction what I
like to think about what our hands and
what even ARMA models are doing is that
they are trying to tease out the way
think of this X as a sequence of stock
prices let's say price you know ten days
ago all the way to price yesterday okay
and you know you're trying to predict
our returns returns today
so both arma models and RNs are
explicitly trying to break your input in
input observation okay you're not
thinking that as a you know a single dog
now you're literally trying to
explicitly capture that okay price ten
days ago maybe you know not very
relevant the price today you know
yesterday may be more relevant say
you're capturing that you know in a
linear fashion in a for example a normal
model right so the what purchases that
we want is that if it's let's say
instead of prices these are sentences
then we want to know what information
started you know a few words ago so it's
the same observation it's the same
sentence so X is a sentence now maybe it
has twenty words the word that appeared
in the first three you know in the first
three words of the sentence for example
it could be the gender of the subject in
a sentence okay whether it was he or she
you want to remember that okay
when you know when you're processing and
you're putting something ok so if it's
if it's relevant to the output ok so
when I mean temporal temporal structure
I'm not talking about a broad structure
across observations which is like across
sentences or let's say across paragraphs
here I'm talking about temporal
correlation across words that's that's
the resolution empowerment
maybe we can revisit this point if it's
not clear so so what is the RNN
architecture is basically start with a
base network will you know like a
feed-forward network in this case what
we will do is actually start with the
network there will you know and it will
have an input and an output but we'll
break that input and output in two parts
so it's as if as good as saying I have
two inputs and two outputs right I can
just take a vector into two parts and
then say this input one and input two
right I can do that so let's say that's
I call that the base network yes or what
is the starting point and then what I'm
going to do is see they're two inputs
here and two outputs here right I'm
going to take one of the outputs and
plug it back to the input okay
this is one way to represent it
represent this this idea that I have to
1% of outputs one output is actually
being fed back as an input and so this
closes the loop and there is an output
usually offered in the usual input okay
but what this actually means is just
this and this is what I was talking
about at the beginning of the class that
you have some base a starting point or a
building block neural network
you're just making copies of it but
really when I say copies you are sharing
the weights okay and I'll get to that
point in the next slide you make copies
of this building block in this case
let's say I made four copies or in
general it could be more copies and I'm
connecting the output a part of the
output to the next input okay part of
the next input okay so there is an
exogenous input external input at every
copy there's also an input from the
previous copy okay that's the two inputs
and there's a this could be a potential
output in the in each copy and another
output a part of the output is also fed
to the next copy success in succeeding
copy okay that's the that's the idea so
what this does is now this this bottom
blue circles that's your observation
that's your sentence okay so you could
have you could think okay given the
whole sentence I can directly say say
well it's whether it's positive or
negative class or I can go word by word
do some processing that that block will
get into the block a soon we can go word
by word feed some information as we go
move forward as we pass each word and
then finally produce the classify let's
hedge like a hedge capital T is our
class whether it's pasta negative okay
so that's that's the temporal structure
I'm talking about so maybe at the
beginning there's a gender or maybe a
sarcasm sarcastic word there that has
just should have a bearing on what the
sentiment is for example right so and
I'm just saying that this the sequence
could be those those movie frames that
that few slides ago I was giving an
example of
or it could be words and this is this is
called a sequence you know collection in
our ordered collection of things okay in
our case it will be vectors even if it
is words the embeddings are no vectors
okay so let's now dive into this this
base or the building block a okay so two
inputs and two outputs at the beginning
there is let's say some null input okay
doesn't matter now to kind of ease the
notation I'm just showing that I'm gonna
use square blocks to represent some
neural network think of you don't have
to now visualize the internals of the
new network don't worry about the number
of hidden nodes or the number of layers
and all that let's say it's something
that's taking input X and outputting
some output point or input you know z1
outputting some Z tube okay so there's a
block arrows represent vectors this
represents a concatenation operations
you know it could also represent
addition but I'll explicitly represent
addition as with a plus sign so let's
let's think of this as concatenation and
then this is a say copy okay so here is
an example of a vanilla building block
okay so here are three copies of the
same building block so let's just focus
on the one in the center okay
it has two inputs this this guy which is
XT and sum or something output from the
previous copy okay these two are getting
concatenated okay so two vectors are
getting concatenated so for example XT
could be five dimensional and the output
from the previous previous block one of
the outputs one of your previous block
every five dimensions then I get a 10
dimensional vector that vector is input
to this this yellow block think of that
as a as a neural net or some some
transformation that's taking the 10
dimensional actor and outputting
outputting a vector okay that vector I
can in this case it turns out that I'm
let's say this vector is Phi dimensional
this is Phi dimensional this is Phi
dimensional okay I'm just taking that
fundamental output and revealing that to
be the first
as well as well as I'm taking it as a
second outfit as well in this case okay
so this is the copy operation here but
you know this is just I mean you don't
have to do you don't have to copy at the
output you can have potentially a 10
damage selector and then 5 out 5 first
five coordinates is the output and four
second five coordinates is the what you
pass on to the next not okay so you can
in fact I mean so in this example I'm
assuming that the the yellow block is
actually is a simple is a simple
transformation like this which is tan H
of W times previous previous previous HT
minus 1 concatenated with XT okay that's
the vector plus bias okay
so if W is number of rows is five then
the output is going to be some parameter
vector if I cross one darkness what is
Tana Stannis is like sigmoid or raloo so
it's an element-wise operation takes a
number outputs a number and if you
really want to visualize it it's
basically something like this this is 0
& 1 and minus 1 it's like a shift at a
sigmoid so so as I said it's a bunch of
copies so you can always create you know
an RNN you know this is written in numpy
but this is not what you actually use so
you initialize with parameters in this
case w s-- and bias are the are the
parameters of that yellow block that we
saw and here I'm just defining a step
where we are going from you know we're
basically talking about what computation
is happening at each each copy of this
RNN sequence okay you basically
computing the W times HT minus 1
concatenated with XT and you're just
lenient transforming it adding bias
applying this transformation and you
just storing that in H ok and that's
that's the example being done here so
this just say this is a code version of
what I just showed in the previous
slide so you do this you do this for 40
is equal to 1/2
whatever in this case 3 okay any
questions about this first output in a
second input so here I chose first
output so which which out to deduct
Odinson let's say this is the output so
this is financial by cross 1 okay so in
this example the output is Phi
dimensional the first output so there
are two I said there has to be two
outputs and two inputs in this example
the two outputs is the same I mean is
this a copy of each other it's Fida
mentionin so input is so you can see
that that's why this is five and five
it's ten dimensional input and the
Tannis is producing a high dimension
over in this in this example so the
number of rows here should be five and
number of columns should be 10 in this w
matrix okay
any other question okay so given that
I'm gonna kind of run through examples
language model that you can build using
ordinance instead of those Marco
I guess conditional probably is that you
could estimate directly as frequency
counts in your data right so what we'll
do is we'll you know given the text data
set will model the probability of the
next character given a sequence of
previous character so let's say let's
give it in the character level but the
character level is not very critical you
can think of this word moldable language
model as well okay for us it's a
question of there's a sequence of
objects whether the characters are words
it doesn't matter okay and let's say in
fact I'm gonna look at only look at a
very very small example so let's say I
rock every size is four we only have
four characters
okay h-e-l-l-o let's say we only have
one training sequence h-e-l-l-o okay
there could be more but let's say this
is the set then we can create four
training data for training pairs for for
this
language model construction okay so what
do I mean by that if input is just hatch
I should be guessing that E is the next
character okay so language model is all
about guessing what next collect the
character is so and if if the input is H
E I should guess that L is the next most
likely next character okay and similarly
you know all the way to htl l the next
slightly character could be you know in
a different world it could be just face
in this case it's O
and what we're going to do is you know
we have first characters we have to
pre-process them and or or not free
process but just represent them so let's
just represent them as four dimensional
vectors okay because we have four
characters the simplest representation
is a one hard encoding representation
okay but you can also use one embeddings
but then the number of the
dimensionality of the vector is
hopefully smaller than the number of
okay you know how much vocabulary that
you have in this case of cavity size is
the same as a number of number of the
dimension of the vector sample talking
about okay so which means that you know
I have for you know four or four words
never Cali right so that's a H is this
one hard encoded vector E is this 100
code vector L and L you know this 104
vector then a particular version of this
sequence sequence of sequence learning
problem is that given this vector I
compute this hidden this this output
right we saw from the a cell the output
is passed on to the next cell and also
is being measured so what is being what
is happening here the output of the
Tanners okay I'm gonna choose it to be
four-dimensional okay so in the previous
example I just picked five of the number
here the input is four-dimensional
that's about encoding the output now I'm
going to pick as four dimensional where
the it's four because my vocabulary size
is four so at each copy I want to
predict one of the characters okay and
the way I'm going to do it I mean I have
four characters so I'm going to have to
have a essentially a scoring function it
says what would be the
which characters more likely or which
dr. Scholl is the highest so in this
case let's say this these are the four
numbers I want the number or the
coordinate corresponding to the
character e so E is the second
coordinate so think of h-e-l-l-o as like
1 0 0 0 and 0 1 0 0 and so on 0 0 1 0
and 0 0 0 1 so this is the one hard
encoding for HCl oh so this is H this is
e and so on ok so for the first output
head should predict e so the second
coordinate should be higher okay that's
the that's what I want I want to set the
weights of this in this neural network
such that the second coordinate is
highest now I do the concat so I have E
which is a 100 representation here I
passed the hidden hidden vector here
which is in this case the hidden layer
hidden layers output is 3 so it's not
exactly the same as the previous example
so there's two weight matrices here
you're you're going from a for dementia
nectar to a 3 dimensional vector from
three dimension to getting four four
scores so given these two E and this
hidden vector we compute some hidden
numbers I mean these are activations and
then those activations translate into
scores should be such that now the third
coordinate should be one should be
highest because L is the L is the next
you know the next character that I want
given H E and similarly you can see the
rest of the references so here we again
meet need the third third coordinate to
be third coordinate third score
coordinate score to be highest and then
the last one would be the fourth
coordinate scored to be highest yeah
okay this is an illustration so okay
let's tie it back to what we already
know so why am i producing scores which
are of the same length as the vocabulary
do the softmax trans softmax
transformation okay and then that's
going to be numbers between 0 to 1 then
I can actually do evaluate whether this
this transform scores are good or not by
just using cross-entropy laws okay and I
want to have the concept that we lost no
which means that the score should better
be that they are the ones corresponding
to the right character okay so if my
output class so think of this is a for
class classification problem happening
at each copy where the true label is e
and my output scores are these are just
transform versions would be some numbers
and I want the 8th coordinate to be the
highest okay so this cross and it
captures that so these numbers are just
illustrations here yeah will change you
know these numbers which are going from
you know which are positive negative the
softmax will put them into between 0 to
1 and in fact normalize it to normalize
such that the sums the numbers add up to
1 right it's a probability vector then
you compare it with the 100 coded
version of e so that's essentially
cross-entropy loss yeah any other
questions ok and and and that's what I'm
a mean here so you define cross-entropy
loss / / / copy so now the overall loss
function is actually the loss of the
first copy plus the loss of the second
coffee plus the loss of the third copy
plus the loss of the fourth copy
okay so that's the loss function it's a
function of the parameters and the
parameters are in this or in this
computation right so the yellow block in
the previous slides and you need to
update those parameter such that this
loss function is low for this training
sequence and you can have more other
other words so given a text corpus you
can always build a character
language model like this okay so what I
kind of did
not emphasized before and let me get
back there is that the parameter you
know like the weight matrix okay think
of the damage layer for example there's
a weight matrix and device it's the same
parameter across all these copies okay
so this is a different way of saying I'm
doing VAE cheering so this is a
different incarnation of I would say way
cheering okay so I have different copies
but the same parameters okay so when I'm
in the sense so when I'm evaluating
forward you know I'm doing a forward
evaluation so given the current weights
or the current phase I use the same
weights in every copy when I evaluate
what I'm predicting in the first
character what I'm reading the second
character what I'm reading in third now
crossing a velocity function of these
these weights which are the same so I
have to do some gradient when I do the
green update of this base I need to take
into account you know they just get
added because they are now part of two
different loss functions does this we
saw in the first lecture okay when two
radians to do different loss functions
or when from the regularizer and the
loss function that update the gradient
of W you saw that we would add the
gradients right so you have to do
something similar here now just that
copies also have the same parameter so
you have to update across copies that's
that's what's happening
okay so I mean so this isn't just an
example of why you know so what is the
long range dependent so what is the
temporal dependency that we're catching
here is that when the first ill you know
first time you encounter L I mean with
this simple you know one data point when
you know this five five character data
set is that first time I'm called well I
should predict L as the most likely
output and the next time I encounter L I
should predict not L but something else
okay so there's a thing of it is like
something like an autoregressive thing
where if I'm dancing the last two inputs
then I should my answer should change
for example right so so that's the
notion of core system so any questions
at this point this is basic but who are
thinking about and if your other side
the summary is for this part is that RN
is nothing but a cop multiple copies of
some things near a network so whatever
network you've designed previously you
can use that as a one giant modular
block and concatenate it as long as you
have you can take into account inputs
and outputs properly like like the two
outputs same for two inputs
interpretation that I had so it turns
out that if you do this when I say it it
comes out here is already smiling so
turns out if you do this then it is this
because I'm sharing the same base right
so I want to do brain updates it turns
out that the the usual procedure of
learning the weights is very unstable so
people have figured out what are the
best architectures what are the best
blocks to use as as these those building
blocks in the are in an architecture
okay so you can have any exotic you know
even a CNN with ten layers in there but
it turns out that they're simpler you
know well I guess popular in practice
and is you know relay to be easier to
Train blocks that have become popular
and that's what we're going to look at
and so one example of that is the long
short-term memory cell okay my cell I
just mean that base block and the
building block so
and these these next three or four
slides are just talking about why you
know what is the motivation for four for
a specific cell there is this so think
of this odd as a RNN function or a
language model in so advanced prediction
and then say models okay so here you
know in the previous immediate
information is being useful so in that
case maybe with the Vinita the
architecture that we saw where you use
the Satanic activation in Tanis is just
again the choice you could have
potentially change to something else
could would be enough but what if you
had something like multiple sentences
where initially you said I went to UIC
and then later on in some some major on
sentence you're saying I lived in right
there the language model what should it
say whether it's Chicago I mean even if
it knows even if it will stick to
yourself the cities it's not clear which
which what it should say right so here
more context is needed so you really
want to capture some information
semantic information from long time ago
okay
so that's the that's what you are not
able to capture in if you design and
order in from scratch or orbital pick a
base building block okay so and this is
an illustration of that so that like
this output depends on let's say these
two it's still fine but if this output
long time from now depends on for
example the gender or or the name of the
person or some some aspect of that then
is difficult for the architecture that
we saw the one example architecture that
we saw to learn properly okay to be to
learn probably in a sense carry good
language model example so I mean this is
I guess the main point that when Aladdin
and saw seem unable to learn these
long-range information and this is the
one of the reasons why we're looking at
a particular cell and this cell tends to
at least empirically it's been observe
that it can capture certain long-range
dependencies and we will look at you
know how the
so the cell will have some you know
neural network elements like
transformations
you know nonlinearities and so on and
there's a semantic meaning a stripe two
different ways you know different parts
of how the cell was designed and I'm
gonna now spend a few minutes talking
about what is semantic meaning is okay
so just to look at you know the block
diagram the a self riously had one one
yellow block right when I said the
yellow block is like a new network or
some transformation like that one layer
of neural network for example instead of
that we actually have four blocks okay
for neural network style blocks or think
of it as four blocks
I just mean something like this WX plus
B so this is like linear transformation
followed by a non-linearity let's say
this is the simplest block then you have
four such blocks here I mean the yellow
objects and they have some semantic
meaning when they constructed this
disturbin architecture it's from is
actually really old 1980 1990s so let's
let's interpret this okay so so I'm
going to use squares to represent that ñ
ññ ññ operation I'm going to use circles
represent simple operations like add two
vectors or multiply element wise two
vectors okay and let's see so so in lsdm
so previously we had two inputs and two
outputs right so a block a had some
output two outputs on you know top right
and talk talk and two inputs right left
and bottom now LST M on the other hand
is slightly different so the key
component in analyst theorem is the
notion of a cell state so we are now
assuming into just one block this is
just a building block one copy and the
key vector associated with that copy is
called this
state it's a it's a vector so actually
this just focus on this the input one is
called seat I guess just representing a
CT minus one so that's fine so that this
is called cell state this is a vector
and will essentially be manipulating
this vector as we pass information as
inputs and outputs come coming as inputs
coming to this cell and and how and
we're going to describe how we compute
the outputs okay
so cell state is what will modify and by
cell state I just mean some vector okay
it's just designated and you're calling
it cell state so itself has two inputs
and two outputs in the sense that input
is of course some some information right
now for example the current character or
the current word but it also depends on
previous cell state okay it also depends
on part of so instead of two inputs to
outputs it looks like two inputs two
inputs and sorry three inputs and three
outputs but you can imagine that the
cell state is the key ingredient this
the other thing is just a slight
transformation of it so we'll we'll see
what the other output is it's just a
simple transformation of the our cell
state okay so there are some updates
happening in the cell state it's a
vector what I mean by updates the
updates just mean if it's a vector
there's going to be some multiplication
operation on the vector or some
additional operations vector okay that
those are the two updates in particular
which I'm representing our circles so
circles with the cross means an update
is happening where the previous vector
is getting multiplied elementwise
with the new act with another vector
we'll discuss what that is and later on
you know that whatever is the output is
getting added with some other vector
element wise that's that's that's
essentially what's happened okay so
those are two operations that I'm going
to do on the cell this cell state vector
and
the idea is to think of cell state as
capturing the semantic information of
the previous you know whatever X number
of words for example in the sentence
okay so the cell state vector somehow
has to encode that oh maybe I saw a
gender information maybe I saw a place
information maybe I saw you know
positive sentiment related word or
negative unfortunate word previously
okay so that's that's what semantically
that we're gonna interpret this vector
to be and the way we're going to
manipulate the cell state which is a
multiplication and addition is requires
another gadget which we call the gate
okay so we're going to modify the cell
state you know those addition and
multiplication that we saw we're going
to modify it we're gonna use a gadget
called a gate so what is a gate gate is
a construction is a gadget which lets
information through optional ease which
just means that I have a okay actually I
have a yeah so let's say I have a vector
V and I want to modify I want to modify
that vector okay
so a simple gate can be something which
which is like a function which takes an
input u and depending on that input u it
out put some numbers right so that's a V
is a two-dimensional vector so three and
five okay you can be whatever dimension
let's a Sigma you know a transformation
of it U is an ax is is two numbers 0.5
and 0.5 okay then the output is just an
element-wise multiplication of 0.5 0.5
so elementwise multiplication of the
first coordinate here and the first
content here okay so a gate is
essentially for example a terminating
some coordinates of the input or
defining the coordinates of the input so
for example three and five are getting
divided by two in each coordinate so the
output f of U comma V
is let's say 3 3 by 2 and 5 by 2 so this
is the operation of a particular gate so
I'm saying that a gate just attenuates
information that is that is that its
output can attenuate information of a
vector okay it'll it'll do if you do
elementwise multiplication now and
actually this is a more explanation of
the same thing what you know the reason
why we use the Sigma sigmoid
non-linearity so this this this this
part is just this okay given you
linearly transformed it add a bias and
then apply Sigma transformation Sigma I
transformation the nice property is it's
between 0 & 1 right so if it is 0 then
it's basically killing that coordinate
and whatever the number is multiplied
with the number which is 0 which is 0
for that coordinate of the input vector
and if the output is 1 for that
coordinate then it's essentially passed
letting the number pass through okay of
that vector okay so so that's the notion
of a gate and let's pick the first gate
which is and there's going to be several
gates actually in particular three but
the first part of the del s TM is going
to be to forget previous information
forget all information so we have the
cell state c t minus 1 i somehow to
transform such that some intermediate
number of maybe c hat I guess T these
are vectors okay I have to forget some
coordinates semantically I'm saying we
need to forget some coordinates it just
means I need to multiply you know I need
to take a take a gate gadget and
multiply element wise such that
hopefully some coordinates are
multiplied with 0 and those those
coordinates are kind of forgetting some
information is this a semantic
interpretation of what's happening with
the vector and in particular the gate
in this case yeah the particular the
gate is going to be Sigma of W of HT
minus 1 and XT plus some bias okay so
the inputs are all am now specifying is
the the gates input is going to be the
current exogenous input maybe the
current character current word and HT
minus 1 which was which is the
additional input that that we also get
from previous cell so from the previous
cell we are getting CT minus 1 the
previous cell state as well as HT minus
1 which is the just think of it as some
some slight simple transformation of the
cell state previous cell state ok so
given those to do some linear animation
transform and with that I get this
intermediate quantity such that the
previous cell state I'm throwing away
some information that's a semantic
meaning and here's an example cell state
may include the gender of the current
subject and maybe it's useful to predict
example the current pronoun or use the
character now and then when a new
subject is observed then you want to
forget what are the gender of the
previous subject ok that's that's the
idea and what I wrote in that guess the
previous page is essentially the same as
here subscript F here just talking about
like this is like a forget forget gate
operation so the highlighted portion
towards the bottom right sorry bottom
left is basically taking HT minus 1 XT
going through this non-linearity and
multiplying elementwise with the
previous cell state to compute something
in intermediate which is that which is
what I call C hat in the previous slide
ok so that's that part of that circuit
or that part of that base block next
will decide so once I forgotten some
information we're thinking it thinking
about this modular lis we following some
information now we want to add some
information ok of course if you want to
add information the only thing you know
there's some sources of new information
which is the biggest of course the
previous HT minus one as well as some
exogenous word as well so it's basically
the same inputs but I have to manipulate
them such that I can add some
information with the self state or the
other vector
and for that there's going to be one
gate and one simple zero network so this
actually is the is the you know the a
that I was considering the base law that
I was considering at the beginning it
had some 10h operation right so it had
HT minus 1 and XT they were getting
concatenated and there was a tannish
layer and that was being output like
this so this Stannis is the same as that
Hampton it's okay
almost is the same interpretation but
there is an input gate which tells me
what to add to the cell state so so this
stanitch computes something
so this Tanis computes something which
is can be added and input gate modulates
you know what information should
actually be added okay so let's look at
that you know this part of the
architecture which is that I have the
same history minus 1 and XT in the
bottom left I take those two pass it
through a gate the gate is this this
thing over here that's the gate Sigma
that computes in numbers bunch of
numbers between 0 and between 0 and 1 so
it can remember that number that's I'm
calling that as I subscript T ok this is
the teeth copy that I subscript T and
I'm passing the same you know these two
HT minus 1 XT to the tannish that was
the tannest from the first you know
first few slides it gives me another
another thing which is C tilde C T lotty
that's the candidate information that I
want to add to the cell state and what
I'm going to do is use I T and C till
dirty and multiply them element ones and
what is doing is trying to you know see
till dirty is my candidate new
information but I'm going to modulate it
as well I'm gonna set some coordinates
to be 0 if they're irrelevant or keep
you know or keep them if they're
relevant ok and once I do that
then I add it back okay where is this
yeah once I do that I can add it back to
the cell state okay this is the element
wise addition operation so I did some
forgetting operation before and now I
computed I T and C tilde T I multiplied
them together and then whatever was the
intermediate quantity I'm adding it to
the cell state okay so this is like a
processed infant process new information
that I'm giving it to the cell state
okay so I first forgot something and
then I added some new information which
is processed because of this input gate
input gate modulation s modulation
attenuation yeah yeah so this is a
semantic understanding of this
architecture how much decided is
completely redone driven right so the
only things that you know once you have
trained this network the only things are
relevant to the weights of these these
Sigma you know these gates for example
the weights of these gates and the
weights of the tannest function yeah so
you need to have training Nina so think
of the language model example right so
in the language model if the pronoun for
somebody you know like this general
information in the beginning of a
sentence then that information should
influence you know what you decide like
she did something right so that's that
could influence what words come next
okay so you want to predict it let's say
the teeth round or theif coffee you want
to predict maybe a verb or maybe another
norm the general information may have
influence and that's being passed on
through these vectors now I now why
would the how is it happening in a Dana
driven way is is that those weights
those weights here it's like these
weights here for example for the new
information case they are being learned
in your training data such that they can
pick up and drop off
appropriate information in a test
example so that you guys example let's
think of that as a test example the
weights are already tuned such that they
know that they need to given this
particular sequence of words they've
seen in history the weights are such
that they can pick up the appropriate
key it's a little bit I guess
messy but yeah yeah yeah so it's just
think of this this image right so we are
just now digging deep into the building
block so the cell state is just this you
know the little X C 1 is C 2 C 3 and so
on that's what you modifying no there is
no there is no storage here no memory
this is just a computation we are just
interpreting parts of the computation
just like neural network so it does
there's no explained notion of a storage
this is like sequence of computations
that I'm doing but I'm interpreting what
are the intermediate computations which
are happening I don't have to answer so
any any confusion so far or everything
is completely confusing here yes but how
you learn the magista training example
and keep that language model example in
your at the back of your mind yeah so so
we we looked at two things how to forget
and how to add information okay
that completely settles the issue of
modifying the cell state now what we're
going to do next is to figure out what
this additional HT you know additional
HT output is it's just a simple
transformation of the
CEL state okay and what is what is that
okay so HT is the output so we call HT
so cell state is you know just you know
the key vector that we key key vector
corresponding to this copy HT you can
call it as the output like for example
in the language model case the output
should have should exactly be the
vocabulary size okay so for example if
you you know example the character
language character level language model
maybe 26 characters or you know 100
characters whatever is the ASCII
characters everybody ate so HT has to be
exactly that dimension in fact HT has to
hopefully you know get get transform you
know HT hopefully represent score
vectors themselves its co-writer
themselves or maybe one more
transformation to get to probability
that this character is something ok so
that's a fixed dimension thing where the
cell state is not you know is hopefully
capture something else there is a
relation between HT and cell and then it
may not be the same copy okay so that's
why we are talking about output being
slightly different from the cell state
so and we are calling that output to be
HT at this copy teeth copy and and the
way we're gonna transform is again
simple because we are using new networks
we're going to look at whatever the cell
state dimension is maybe it's 50
dimensions and maybe my vocabulary size
is hundred I need to go from 5200
dimensions I'm gonna do pass its
vanished
Tannis linear layer okay so basically go
from some weight matrix which is 100
cross 50 and I have the cell state plus
bias and apply tannish operation so I
will get HT which is 100 100 dimensions
which is the you know let's say which is
my mocha besides X is the current XT
it's the emitting of a character of
example or we saw that we saw we saw the
several slides ago yeah so just think of
bezel XT this is
these red locks at the bottom are ex T's
okay the blue box are the talk are the
HTS but yeah blues are the HTS XT sir
these these goddamn months 100 coded odd
word embeddings of them and in this
example there's only one you know
there's only HT HT minus one being
passed to the next copy in LST M's the
key concept is the cell state which is
also being passed the next copy
there's also output V the next copy as
well as as being output okay that's the
only difference which slide was it okay
so let's say we look at two gates one
gate was modulating for forgetting of
cell state we another gate was figuring
out how what information to you know
there was some information being
generated that was getting modulated and
then element was getting added to the
cell state next the last part is we're
going to define what the output is HT
given the cell state okay and that is
what I was telling so HT is just going
to be some transformation of cell state
but that again will be modulated and so
first we will pass a cell state which is
some vector through the Danish layer
we'll get some intermediate vector and
then we're going to again scale it with
another gate so you know all gates do is
just scale right I mean given in vector
multiply element wise with zero some
number between zero and one right so it
can kind of decide like for example what
parts of the cell state it can be if you
wanna example the simplest thing is
let's say the cell state is ten
dimensional and you know not all the ten
damage is actually actually useful then
you can set some of the dimensions to be
zero through this gate so through this
output gate and that's captured here so
again the gate is a function of HT minus
1 and XT okay that's gonna element-wise
multiplied with the transformation of
the cell state okay so Tanis is taking
something from here which is a cell
state CT
getting some intermediate now
intermediate vector and elementwise
multiplying with some numbers between 0
to 1 coordinate wise and that's the HT
okay so that's this HT and there's a
copy which is I don't know why it's
being passed on top but it's not very
relevant okay so this is essentially the
three parts of forgetting information
adding information and defining what the
output is so I think the next slide
shows the different parts highlighted
and other parts shared it out okay so
forget add something essentially
modifies cell state and I guess they're
modifying remember I just said create me
information and output is just creation
of the output so this we saw it changes
a set some corners to zero this we saw
create some new information okay and
then given the new information modify
write and create HT here output using CT
as you know given the cell state is that
fine okay
so that's that's lsdm actually so any
questions about lsdm architecture it
seems exotic because a lot of at least
four different neural networks in there
four different neural networks inside
there's two sigmoid layer three sigmoid
layer neural networks and sorry but as
the neural networks just think of those
three sigmoid layers and one tannish
layer so like for matrices are there
inside and in fact here this damage is
actually not a layer it's elementwise
damage operation okay so actually I
should oh that means that this is not
correct yeah this is not correct okay it
doesn't matter okay so therefore for
weight matrices and for biases that you
need to estimate okay
and so that's LS TM and it's somehow you
know empirically people have figured out
that this is able to capture you know is
able to generate better language model
Stan if you use a single
in a tannish version that we were
looking at at the beginning of this
lecture
okay now this that's not the only
architecture right that seems like a
very heavily engineered hand design
architecture there are many other
architectures that you can also have but
it's it's basically driven by intuition
how people create these based you know
building blocks for an RNN architecture
the couple which are which are very
popular and probably if you are just
doing applied work then you want to pick
one of these blocks the second off is
called a gated recurrent recurrent unit
okay it's the same idea there's a bunch
of gates and the modify information but
it kind of reduces the complexity so LS
diems have this notion of a cell state
the question that I mentioned has a
notion of cell state and an output this
one gated recurrent unit recurrent unit
reduces the reduce the complex is a
little bit by merging some gates so
there's only so much a cell stayed in
the hidden state and combines the forget
on the remember part so somehow directly
yeah so there were three parts
previously in lsdm there was a
forgetting and adding information so it
basically combines that those two
aspects in the network and let's not go
into the detail because the previous one
was quite mechanical enough I mean this
it's basically just trying to interpret
an architecture and the weights RS
hopefully are data driven and well tuned
such that they do capture the examples
that I was talking about where some
general X you know five words before is
actually helping me predict the next
word for example right so that's gru and
so will not go to the detail of a GRU
but GRU something that for example
available in pi torsion you can use an n
dot lsdm or an n dot GRU to build an RNN
okay so given these building blocks and
RNN was just stretching you know copying
the blocks horizontally you can do many
other things with with with this idea
okay
you can stack ordnance on top of each
other
it just means that you're now copying
spatially okay whatever the block inside
may have whatever complication has
you're somehow tying the outputs to yeah
tying the outputs to inputs okay so the
outputs HT HT at this layer h0 at this
layer is now the x0 for the layer about
okay so that's just tacking but I makes
the makes the whole computation you know
highly nonlinear but you can see that
you know it they all should have the
same parameters whatever the weights are
therefore made for weight weight
matrices they should be the same okay so
when you're training there's a lot of
gradients which get accumulated
essentially and and those two well that
we saw at CMS and GRU our seemed to me
the best seem to be really good choices
and they essentially on the again tying
back the original idea they have this
you know they hopefully can capture
long-range dependencies of of your
sequential data so let's actually yeah
so you see your choice I mean so here
I'm using the same a a block but you can
change this to some other block B and so
on yeah but the thing is I don't think
you will go into such exotic
architectures is an example of something
that you can do right I'm invited by do
only do something sequentially if you
want further nonlinearities in your
model then you can you can do this I
think I'm not sure mainly for speech
applications people have looked at this
stacked layer of RN ins but I'm not
seeing this elsewhere
so yeah straining fairness is the same
as straining for regular fee Power
Networks there's just weight sharing the
weights of the each cell copy is the
same weight so if they have to automate
gradients appropriately so I mean so
I'll skip this slide but this slide is
according for dimension so what is the
dimension of the input and the output
example if HT is two dimensions so you
need to maintain hidden vectors as two
dimensions and cell state is two
dimensions then it is explicitly showing
to the each dimension is being shown a
single line okay so for example the two
lines is input so two numbers as input
both numbers are getting multiplied by
numbers between 0 to 1 to forget both
numbers are getting added with some new
numbers to you know remember some new
information and then both numbers are
getting renormalized essentially between
scale between 0 to 1 again getting
multiplied by numbers will use it 1 2
again for the output filtering and then
you have HT which is I get two
dimensions so any questions so far
ok this this is a little bit of a
laborious and it seems like mechanical
process but this is interpreting the
insides of at least one one one cell
okay so that's the see use and what I'll
do next is should I take a break now ok
ok so let's take a break and resume at 7
for 725
ok so in the next you know 15 20 minutes
less like look at some more applications
of sequence of sequence models sequence
sequence problems in detail a little bit
of detail and then we can call it is
this a computational issues related to
fight or shortage the flow or okay let's
do that so
okay so first example so basically we'll
go through like three years examples so
first example is sentence classification
so we saw how to use the CNN last time
and you can also use anything else I
mean you can just use name based
classifiers for example to classify a
positive and negative sentiment right
that's the bag of words representation
so how to use RN n is basically you
don't really care about the intermediate
head 0 H 1 and so on this hedge T should
be exactly the dimension of your what is
it your output the number of classes
that you have ok if it's an S
classification in two classes then HT
should be two dimensional okay so and X
0 X 1 and X 2 can be the vector
representations of the words that you
have in your sentence okay for example
you can download the globe or fast X
star or is it were two back vectors and
use those vectors as inputs here ok so
inputs are going to be if it's below and
for example this one dimensional version
of the globe vector embeddings so x x0
x1 x2 although it XD will be hundred
dimensional and then h0 h1 you don't
really care so it but you don't need to
decide what the dimensionality is here
that depends on the weight matrix being
in the that depends on how what are the
matrix size that you're assuming the in
the LST m and this has t should be
exactly for example two dimensional you
can feed in zeros I mean something now
it will not be different it will be the
same except you need to pass in
something so that's a very quick example
because I'm not gonna spend too much
time on such as classification actually
there is so okay so on the syllabus page
miss that yeah on the syllabus page
there is actually by Tosh tutorial using
RNN for sentence classification so it's
the same tutorial from the previous from
the previous lecture so in the previous
lecture there was a version of notebook
for doing which implemented essentially
that very popular CNN architecture for
senders classification there's an RNN
version as well in the same code face ok
so have a look at that or maybe you can
look at that towards the end of this
lecture so the next example is
essentially is a very very simple idea
as well straightforward idea which is to
do image captioning now so for image
captioning the output is going to be a
caption which is going to be a sequence
of words right and the input is going to
be an image right a fixed size you know
what about two for two you know two
things or whatever 294 / 74 depends on
the pre train classifier free training
network that you have okay so the input
is gonna be an image the output is going
to be let's say two two words here straw
hat and then end as a token so actually
a lot of NLP details I'm skipping out
but they're like things that you need to
have like a beginning of sentence and
things like end of sentence and so on so
so how do you do this so you've got an
image do the peach extraction which is
what you've been doing for example in
your assignment get a you know delete
the last few last layer for example our
last two layers get a feature vector and
then that's the X not that's gonna be
your X naught and all sorry that's not
gonna that's that's not the X naught but
that's the H naught okay so the initial
initial information so one of the inputs
to your RN n so our n n needed know
means two inputs right so one of the
inputs can be this and then what you do
is and then you are supposed to caption
this image so you now you have the
feature vector for the image itself
that's the full information that's the
information vector that you have right
you can transform it so here we're just
showing it as WI WI times H okay you can
start with the start token to start
generating the caption okay so start on
the image vector together are the inputs
for this copy of the RNN block and the
output is gonna be a vector which is
gonna be the size of the vocabulary that
you have for your words okay if it's
having English word dictionary maybe
it's even 50,000 or whatever is in our
efforts you know key words in your
vocabulary okay so you're trying to
predict so remember the original
character recognition character language
modeling problem where I said the output
has to be the same size of the vocab
size so here also the output size with
your cap size and hopefully you want to
get high score on that coordinate which
captures that right caption so in your
training data you have images and
captions okay
and so at the beginning the start and
the image is a feature vector together
should output scores which is going to
be the vocab size such that the
coordinate corresponding to the first
word of that caption has the highest
score that's how you want to train the
weights in your network okay so let's
say this we are not training this let's
say those are frozen then the weights
that we are training are essentially the
parts of our RN and cell okay the base
block it could be the simple block we
saw at the beginning or the lsdm okay
bunch of weights and I mean here we are
just showing that I mean so I'm just
saying that this this additional this
this initial input is like a different
input it could be it is a different
dimensional thing then the could for
example here let's say we are picking
this layer
in this network there is some old layer
so this is 4096 dimensional you don't
want to maintain a hidden hidden HT
which is going from one RNN once L
copy to the other one one copy to the
other to also be 4000 X dimension so you
can always do a linear transformation
and pass it to the Tanis for the initial
initial cell okay
so back to your question initial cell in
this case is gonna be slightly different
okay and then what you do is you could
do a few things you can actually let's
say you you got your let's think of the
scenario where we have trained the
network okay which means that I've
trained my RN n let's say I fix the
weights okay somehow have trained it
let's come back to this point again if
I've trained it then at at test time I
can pass the test image get the initial
vector give the start as always a
starting token and generate is is
essentially going to be a probability
mass function okay this is gonna be okay
at size and every coordinate is going to
be a number between 0 to 1 hopefully the
right first word would have the highest
score so whatever so this is going to be
work outside right so from there I can
sample and get a word okay so that's
gonna be my first word for the first
word of the caption for this image and
then that sample I'm gonna use it as
input when I pass it to the my next RNN
copy okay so maybe a sample straw I mean
this in this case then I'm gonna pass it
as input maybe the word vector
corresponding to that that by that word
as input and the hidden vector from the
previous copy and similarly so again
again sample something from here and
pass it to as input to the next next ok
so that's what you would do in a test
time and and you'll continue to do this
as long maybe as long as you don't
sample n which may or may not be
realistic or maybe you sample it maybe
you you terminate after a few fix number
of
you know fixed number of rounds it means
that your caption you're getting like 10
words or 20 version and stopping okay so
so so the test insurance time is scale
right so how do you get a caption if you
had trained the network right now train
the network itself actually this one we
come up later as well
so while training the network you can
actually either you can actually train
in a similar way so you can so you can
train in a similar way so in the sense
of remember how we train the character
language model we said you know output
is no vocal resize and we'll have a
cross entropy loss per copy okay that
you know for that for that for maybe the
right word here is straw and there is a
y vector which is you know which will
have a lot of coordinates lot of
different scores or different words but
we prefer we'll have cross entrepreneurs
where we prefer this code to be highest
for the straw coordinate and everything
else okay so you can do that this so we
are defining a loss function and you can
follow the same procedure which is
sample the word from the y 0 vector and
process it through the next copy and get
another vector you can also do something
else which is I know that the true in
training I know what the true caption
word should be I can actually pass that
word as the input to my next copy ok so
I'm not actually so I'm not really I'm
trying to be more you know give more
information to my network in the sense
that let's not depend on what it
predicted for the first copy let me
actually here the training data to my
second copy to learn can this this is
the only helpful when you're starting
from scratch in the RNN part of this
small ok those are the two points I
wanted to make so let's look at a
slightly different application which is
instead of image captioning now I want
to also make input variable length which
just means I'm going to look at a
variation like translation
so we saw a knurled run you know we saw
machine translation where we were going
from this english to french then the
words number of words in one language of
number of words in the other language is
going to be variable and it's vary
across sentences and so on so there are
many many different versions which they
all have the same structure for example
there's a question of certain lengths
then there's answer of certain lengths
for speech and we'll see speech a little
bit later same thing a translation but
here I'm gonna just drown it on a
specific you know another stylized
example let's say auto-reply so you see
not a recline I guess Gmail right so
basically it takes the it passes in the
your input email and given the input
email it generates a string right you it
gives you a few options and you pick one
of them right so the first version is
gonna so how do you how do you generate
this variable length outputs right so
they're different ways to do it it's and
it's going to be a departure from the
RNN architectures we have seen so far in
the sense that there's going to be two
parts to this problem okay
actually not there but that's a lask
what is this
first I'm gonna parse all the input
words okay pass in all the input words
the outputs at that copy I don't care
I'm not doing any cost entropy loss
minimization there I only care about the
last once that sentence ended you know
once that email ended okay I generate
some vector final vector right has
output vector of that last copy
now that vector is the encoded state of
this whole email okay so this part
actually this whole you know whole part
including the last let's say last state
that part is called the encoder it just
encodes the whole input sentence into
some vector
that's that's what essentially has
happened and and this part is
essentially an RNN okay
now given the encoded vector okay I mean
it's terminated by an N token given the
encoder vector now I'm trying to give
that as an input to another RN n okay
where the are an ordinance input is a
single vector like H team or whatever
hundred dimensions but starting from the
higher dimensions vector I need to
generate a variable length output this
we have seen image captioning was
essentially starting from image vector
for the other nine is now vector we have
to create a caption right so here it's
very similar in stuff the input is not
the HT is not directly a feature vector
feature extraction from an image but
it's actually the encoded or the output
of a previous RN answer basically
concatenated to RN ends where one RN n
has variable input but single output and
the second Allanon has one input and
variable output okay and of course you
know like if I'm outputting things these
have to be pocket-sized predictions and
here I can you know here I can in my
training data I can have to do cross
entropy for per copy
okay any questions about this example
what did I do yeah
so okay so the second version is again
copy
you know copying with like inspired by
the previous image captioning example
I'm gonna actually help my RNN so
actually previously even five minutes
captioning I had mentioned that I can
actually pass in previous word as a
input to the next next our next coffee
right actually in the in the first
version I did not do that actually I was
just showing that given the starting
state I'll just estimate I am and given
the hidden state next I'll I will try to
estimate fine and so on but this is the
standard right so we saw it in image
captioning we can actually actually some
training we can give the true true word
itself or given the given a distribution
here sample a word give it to the next
copy and generate so that's that's the
only difference and other side this part
is an encoder and the other part is
stick and the part on the right is
decoder okay so there is one more
advanced
one small the ones I mean this is just a
way to kind of give you an idea that
it's not it's not enough so you have to
do a little bit more which is that let's
say at test time as an inference time
you've already trained you network so
you have an input sequence the some
email okay you first output you know you
start you some email you processed it
you got you know end of the encoder now
you're generating words right response
words now maybe on output some problem
distribution you can sample from it that
could be a first for a high response but
he goes also take the highest
probability you know you can take the
word with the highest probability you
know that word has right one of the
words will have highest probability of
or the highest score just take that word
and then feed it as the input to the
next copy okay that's the highest
priority whereas as the input of next
copy because if you sample then maybe
you are picking a low probability word
with some probability and and then you
may be derailed okay so you can do that
given these two you pick the
deterministic pick the coordinate or the
word with the highest probability then
you got some other output and I'm keep
repeating this picking the highest
probability word at each copy is a
greedy procedure okay and and there is
an I would say it's some sort of an
amplification of mistake so if you make
a mistake early on it kind of fails the
you know the rest of the response is
going to be very suboptimal okay so a
small trick here is what is called beam
search or essentially just it is about
maintaining several candidates as you
keep generating parts of your auto
response automatic reply okay what do I
mean by that so you start with an input
okay and you generate in your first word
what you do is sorry you use you
starting the generator your response so
in the first word of a response maybe
you can generate not you know maybe you
can pick not just the top-maul you know
that word which are the highest
probability you pick the top three words
with the highest probability okay so
let's say like case three okay pick the
top three words okay so these are the
three words there's a high yes and
please and then you actually pass all
three words into the next copy and get
the top three words in the second level
okay so for high maybe there's three
things here three things here and three
things here okay and you can see that
the moment I keep three three words
there is a branching factor of three and
so on fine exploding in terms of the
number of choices but I can also
maintain is that when I pick this word I
can also remember the probability with
which that word was you know the
probability corresponding that word so
for the three words looking to copy the
probabilities for the corresponding
water that that
at that level that index right so what I
do is I multiply this probability I keep
a track of the probabilities for each
path okay so like please I okay
so please maybe you know was supposed to
be output with had a probability value
of lesser point six and I had a
probability value of 0.5 then you
multiply those two okay for each path so
so here you have nine choices right at
this the second one devil you have nine
choices but you also have the product of
the probabilities of the two words okay
so those products you kind of against
arc and drop the ones which are dog the
last six and then you can keep the top
three top three to Phase two phrases
okay and you can so this is just a way
to kind of not be too greedy but also
consider an initially suboptimal word
but hopefully you know as it becomes a
phrase it's actually much better phrase
to respond yeah
yeah so there's some issue I mean so the
version that we saw also and still has
some issue but we're not gonna discuss
this here which is that I'm somehow you
know maybe the email has like three
paragraphs and I'm condensing the three
paragraphs into maybe a honey
dimensional vector or whatever 50
dimension vector or maybe well thousand
dimensional vector that's like it's not
clear whether the Hana dimension vector
is going to capture all the somatic
information in the right way so that I
can generate the right response right so
there is way there are ways to train
networks where where you can do better
than what what is this this particular
encoder decoder architecture that we
have okay and that's through this trick
of attention okay attention just means
that when I'm trying to generate a word
I am NOT it's I'm not only going to look
at the last output last copies output in
the hand order I'm gonna look at all the
outputs of the encoders I have now I'm
gonna keep track of all outputs okay and
selectively focus on those outputs which
are relevant to my current prediction
okay so that's a semantic interpretation
of what attention trick is supposed to
do but the implementation details will
skip today it's just a way to say that
okay why should i only remember
why should I only take the last encoders
last output as input to fire
input to my subsequent decoder
processing I'll look at I'll keep track
of all the outputs and selectively pick
those outputs depending on which part of
the response I am I'm trying to respond
with okay so this is a quite relevant
for machine translation where maybe the
first few words are relevant for the
first few words of your English
available to first viewers of your
French so you would you would prefer the
hidden vectors to be related to the
first few words rather than the fully
processed final word fully processed a
full sentence okay so that's that's it
for the auto reply and you know size
variations of that now I'd also quickly
mention speech speech transcription
because this is all the same as before
except how do you work a speech data so
I'm just giving a couple of I guess
spending a slide or like people who have
not seen speech data it's not really
complicated you can actually work with
the same architectures that we have but
you have to do ones one pre-processing
step okay and okay traditionally let's
just to give you a brief idea so before
using like RN n type of architectures we
saw that we can use a language model we
can build a language model using RN n we
can also build a language model using
just Markov model assumption right we
saw that in the last lecture similarly
with speech you can actually again do
something like a Markov model which is
you know or you can also use RN in here
but traditionally historically I was a
five ten years ago people were
constructing interesting language models
so these were like hidden Markov models
they are called and so basically Markov
models and then the acoustic model would
be some sort of a Gaussian mixture model
okay Gaussian mixture model means some
model we're given the word how would the
output sound it's like a mixture of
gaussians okay so this is basically
given the word how do how is how is it
pronounced okay it's going to be some
there'll be different pronunciations so
there'll be a lot of variation so so
that part people use small it has DMMs
and this would be hmm these are both
graphical models we will see if we can
get to that in giving the course and a
lot of speech engine that's how that's
how people used to do extra speech as
speech to text okay but now you can
actually use the encoder decoder
architecture that we just mentioned
three in the previous slide essentially
is the backbone or is the starting point
for building even speech translation
free speech transcription solutions
right so like Mozilla's deep speech and
Baidu steep speech variation something
like that so how do you how
stuff like speech data is essentially
actually looks like this in time them in
terms of time as a function of time you
actually have some sensor which picks up
magnitudes basically it's if it looks
like this in time dimension so you're
measuring so any symmetric across across
the axis you're basically measuring map
magnitudes across time okay so this is a
speech data in raw in in time and it
corresponds to let's say this is this
text okay hi how's how's it so you want
to go from that speech to text okay
so given that speech that's the sequence
input sequence in this case it looks
pretty messy it looks like now I spend
it's basically if you think of the next
y-axis there's one number per timestamp
and there's these timestamps are very
close to each other and the output is a
sequence of words
okay now the way people generally go
from this one dimensional data time
versus some magnitude numbers is to
actually transform it to what are called
M FCC features okay and these are
essentially features in the in a
different domain like a transformation
just means going from some domain to
another right so in particular here
you're going from a time domain to a
frequency domain okay
so there's some time frequency duality
in play here but we don't you know
that's not very important here what is
important is that you look at parts of
this speech signal okay
sequential parts of the speech signal
maybe 16 sample 16 samples 16 samples
and time those 16 samples you
essentially transform think of like
Fourier transform or variations of that
so M FCC is one such operation so you
get you get a mech term okay so that
that's the vector X x0 let's say there's
a vector X x1
let's say X vector x2 so you keep moving
forward in time and you somehow
transform this this data to frequency
spectrum data vector
and then that's the input to your
encoder okay and then the rest of the
process is similar okay except that here
you see that they have used the two
layer RNN that I was you have to ask the
question before right so that's that's
an example of speech transcription
I guess the extremely like lot of
details missing but this is this is the
core idea but this is not the only idea
that many different ways to do this
speech transcription and so as and it's
also true for language models
there's also question about whether lsdm
there could be other things inside lsdm
like dropout and max pooling and so on
or multiple layers you can have those
things but empirically in practice
people don't really change those pace
building blocks for events they focus on
like stacking you know they don't change
inside then inside the base block but
they kind of stack things on top of each
other and so on so so things like
bi-directional ORM RNN just means you
compute you do computation in this way
you also do a computation the opposite
way and then aggregate and get some
final hedge you know encoded vector and
then pass it on to the decoder okay so
these are all variations that people
look at any questions about so far so
those are the few examples and I wanted
to give about our n ends and how they're
useful for sequence to sequence type of
problems so you can also do I mean you
can definitely do time series analysis
time series models are essentially like
your inputs are numbers and you want to
predict one final number each day right
so that's third sub model that we saw
where essentially the encoder model
right so you can have previous prices
and one final output which is the return
for today right so any questions about
the applications yeah
yeah yes yes yes and in these examples
yeah this is a yeah yes it is it is
so what capsize is the it's a limitation
I think somebody was referring to the
sparse cross and trophy loss maybe that
helps computing you know the loss at
least in training time you know if the
vector is sparse hopefully yeah I'm not
sure so but there is it is of high
dimensions yeah any questions okay so
basically you just summarize this part I
want to have some time for assignment
questions and so on so we motivated when
ordnance can be used understood some
internal workings of the RNN building
block basically lsdm if you're inclined
look at GRU and we looked at some you
know minor details would say high level
details of simple sequence applications
okay so now actually what I want to do
is several of you had questions about
the assignment so I want to spend some
time on the assignment all we can talk
about also basics of PI touch that we
kind of some of the students and me
discuss during the office hours so how
is the state of affairs with the first
of the first and second question office
on and - so we can actually
so so because of because of feedback I'm
not pleased assignment 3 yet anywhere I
mean because the deadline is tomorrow
I'm not released it it's gonna be on
ordnance assignment 3 because I think
the next exam is pretty soon so maybe
the number of questions be smaller but
let's look at assignment 2 so each slide
deck has a bunch of I mean ample exam
questions and each so you can look at
what are the learning goals right so
you'll be quizzed on the learning goals
you need yeah yeah so so any questions
with the first first assignment question
fine tuning and beetle extraction yeah
yeah so okay so fine-tuning yeah first
at 18 also the runtime is high it should
not be idly so what's happening with the
so we had this
so okay so this particular so beats me
we saw this so this is this is given
lecture - right so here the data sets is
small right what is the size hundred
images and here they are able to show
that they can find you
although they are using rest at 18 so
they still have lots of weights right
what is the size of the video name
anybody type and entered parameters and
see how because the yes fully connected
layers are the bottleneck essentially
because there too many weights to train
there yeah yeah so you are playing in
this in this example they replace it
with the two cross Phi 12-page matrix
essentially and in your case you have to
replace it that ten cross five five
matrix that's fine
but I'm trying to understand why the
training time is large is that because
you're batching properly or I mean are
you guys using batching I mean first of
all are you guys using the GPU service
okay you can try increase in the bath
size I mean so you should increase the
batch size enough so that you are you
know your baths I should influence you
know you should keep the bath is high
enough such that the GPU can take that
much of weights and batch into into a
cup into its computation the moment you
increase the batch size you will be on
some level then you will see that you
will be running out because of memory
requirements okay one one I guess
solution is to not fine-tune the initial
layers by which I mean you have to
delete I mean not believe but remove
them from the parameters that require
gradients okay so you need to set
somehow the required gradient to be
false for the initial layer parameters
okay and since all the layers have names
you can
figure out how to set how to freeze them
essentially try that I mean first of all
I will try of course so even before
fine-tuning freeze everything and just
tune the last layer which is to cross by
12 right so you should know what the
timing requirement for that is and then
look at just slightly extending to the
next immediate layer and so on so any
questions about the second assignment
okay first one yes let's go let's finish
the first year that benchmark I don't
know so what is it it's a 10 class
classification problem so I don't know
what the benchmark is so you can so
there was a link right oh maybe I am not
sure if I'll share it with you there is
state of the art there's a web service
tax the tracks the state of the art
numbers classification numbers for many
of these standard data sets but we are
not changing say they are numbers here
we just want a working solution which
you know definitely guess is better than
random so here because the classes are
the same random guesses one by ten okay
with one out of ten trough chance you're
going to cover guesses correctly yeah I
mean because the classes are the same
here okay that's nuts with namely not
doing anything right that's random
guessing so that should definitely be a
benchmark so you cannot be doing worse
than that okay any other question like
even if it strain a subset of the data
you will do better than 10% for sure
okay so if data size is a limitation I
mean if it's if it's influencing your
training yeah so what is he is there any
question any other question for the
first assignment first first question
yeah
for each class subset the data into
subset the data to get the observations
corresponding the class and yeah this is
what he said oh yeah it's just that yeah
if you call one cell per image that it
would be like too many too long you know
in your notebook so yeah I mean just
just use the matplotlib sub plot plot of
subplot so it appropriate like grid size
okay yeah you can do that as well yeah
if you understand the make read syntax
then you can use that it's not very
necessary here any other question about
the first part okay so there were
several questions over the second part
so let's see is there any question for
the second question yeah to point to
optimize yeah so this is where so how
many of you have learned gone through a
tutorial for fighters or tensorflow
you have okay so few of you who has not
gone through a tutorial for by toss our
tensorflow yeah it was there or was it
that was by - there okay I mean so
tensorflow in the sense you just use
chaos right so it's not really you're
using tableau ha did somebody actually
use tensor flow directly okay you did
chaos right so chaos is okay
so first of all the second part so okay
so optimizing this function is is
something which is a little bit
non-standard it's not a neural network I
mean it is a neural network in a sense
you can actually so this problem is
about collaborative filtering item item
collaborative filtering okay so if you
ignore the you know these bulleted
questions it's all about okay I have
data about movies you know who watched
what movies can I make movie
recommendations you know that's a
business question now to do so you can
do all sorts of color filtering
approaches you can also do other regular
feature engineering based predictions of
what would be the most likely movies
recommend so that's you can always do
you know the many many approaches to do
this what we are focusing in the is on
precisely item item collaborative
filtering approach okay so just to add
to that you can actually bring in neural
networks into this I mean as an as an
extension or based on build on top of
this okay but this actually doesn't
require any knowledge of neural networks
okay there is no neural networks
involved but the second question which
is optimize this objective function over
these vectors these are M vectors
requires essentially defined that you
know requires there is no model here the
model is so the model is hidden in this
I guess we I transpose V J okay that's
essentially the model so you have this
last function which is a C function and
you want to find coordinates the numbers
of these vectors such that you know the
class loss function is low so whatever
you computed the X IJ is at the data
okay do you computer the data ones free
process those you know that matrix okay
and that's Fritz that's fixed now you're
not changing that you're not change need
to change the parameters it's like
saying I have train it I'm not sure now
change train you know I'm going to
change the weights off when you okay but
new neural network has a specific form
of going from data and parameters to
loss function here there is a specific
form as well except that the data and
the parameters are not even getting
with each other okay the parameters VI
and VJ are just getting multiplied with
themselves and then being compared to
the data directly whereas in a neural
network you would think of okay there's
some data that are getting mandated by
the parameters you know W times X plus B
and then eventually some scoring
function you know my prediction - y true
or something right so that's not
happening here that's the only I guess
that's actual block okay but this does
require knowing a little bit of the
basics of either may not yeah I mean PI
tours are denser flow okay so the next
five ten minutes let's we'll run through
a few basics basic ideas in PI - okay
for people who have not seen it people
who who know stuff can chime in the B's
are the ways okay weights are the
parameters weights are just another name
for parameters of my function right so
these are the weights here RVs are the
parameters here but these are the B
vectors are the ones you need to
optimize optimize for so you want to
find the grain of the last function with
respect to B's okay and if you can find
that then you can do great and update
and keep changing the V vectors and
eventually we'll get a good set of V
vectors which are you know which
minimizes loss any other question yeah
oh yeah here them you're asking about so
I just said you know in the first step
you compute given the data the raw data
Brooklands data you computed X IJ right
so those are some counts right
so some integers that's fixed now and
that's done now your model for
predicting whether so you're trying to
predict the counts actually so in this
problem all you're trying to predict is
given mu V I and J if you really think
one thing that does prediction given a
movie I am given o VJ how many people
would watch it okay so you know that
number the target is
how many people would watch it input as
which movie in which movie the movie
pair and bi and Vijay are the parameters
of the movies whose inner product is
what is predicting the count okay so the
if you want to think in terms of a model
you know so f of movie I and J is trying
to predict the count right X IJ maybe
this is 10 okay so the input is the
movie indices okay so that's why this
problem is about embedding right so the
input is movie indices movies themselves
are not numerical objects so you're
going from movie names which are non
numerical two things which are numerical
okay so this function is we are saying
is nothing but VI transpose BJ in a
sense that for each movie there is a
corresponding vector V I VI or BJ okay
these can be whatever dimensions you
choose you know there is some nuance in
what damage you want to choose but if
these are the vectors then I'm saying
that this is the model which predicts
ten but you don't need to interpret it
this way as well I mean there's some I
mean if you could think of in terms of
pure optimization V's you know
coordinates
you know coordinates of these are what I
need to change so as to minimize some
loss function looks like a squared loss
function fine just do great an update on
on the parameters here which are piece
okay so these are you know you can for
conceptual you can think of them
belonging to R 2 which means two
dimensions okay two dimensions let's say
there are two dimensional vectors then
for every movie you can have a
two-dimensional vector okay that that
will be in some two dimensions right XY
XY plane and and then you can actually
visualize which movies are close to each
other and so on those are vectors
corresponding to movies but your model
for creating method whether a movie is
liked by sorry model for predicting the
count of accounts for pair of movie is
this
we are transfer VJ is my count for
people would like this pair of movies
okay what is the what is that kind of
people who can who like both these
movies essentially so X IJ is a target
essentially think of X IJ as your no
exercises your truth ground truth and
the model is models input as movie one
movie two and there has to be a way to
say what is the count right there has to
be a mapping from movies movie names
think of like I give you two movie names
tell me what the count is like what
that's the model they're constructing
here so actually I took too much time
there so so I'm gonna share another set
of tutorials and this is going to be a
basic BIOS tutorial but so so this is I
haven't shared this notebook online yet
but I will say this this is from I
forget the name of the author but I'll
share the github link on the syllabus
page so here they're just contrasting
you know what is the what are the key
key modules that you want to import from
Python from Taj and initially they
contrast it with the numpy okay so
tossed out n N which is where you'll get
the N n module n n dot module which is
the base class from which you can extend
extend to create other classes okay any
not functional and Taj vision - vision
is to get those things like data sets
and data transformers so data loaders
okay
maybe even transforms I'm not sure so
what I want to get to so this this this
notebook has a lot of details but we get
directly to
okay let me just spend some time on
numpy versus star so if you actually
torch or tensorflow
you know those are things that you'd get
to once you're comfortable with the
numpy version of your code so for even
small scale you know small scale small
data version of your code you should be
able to write the number version in the
sense that you can you should be able to
manipulate matrices and and and and we
saw a version of that in the first
lecture right literally we did not do
anything we just literally hand
calculated the backdrop equation and the
for loop to get the classifier so here
you know you just combine MP dot I is a
is the identity matrix and Tosh that I
also can see that energy matrix right so
they're extremely similar
so the Tarts object the tensile object
is essentially the numpy and the array
object but with the additional it's kind
of compatible with the numpad ii-era
objects has more features okay
so random is fine like you can call dot
shape and get the shape of your tensors
and in fact this is kind of critical
actually one of the key features for
stars is that you can of course push
your data to GPU seamlessly you can also
debug your code using traces okay you
can set trace like I don't have some of
you have used Java or maybe I'm not sure
you do that but in C and C++ you can set
trace and get into debuggers right so
you can do debugging in in torch as well
you can push here push the tensor from
CPU memory or the regular memory to the
GPU memory okay by using things like to
do to two CPU to GPU and so on so if you
want your computations to happen on the
GPU the neural network training then you
should actually put them on a device
okay there is some function calls you
should do so these are you know this is
just notation how to multiply things I
mean even my numpy you can use X
transpose X you know is this an example
but multiply matrices this is the
notation you use so with numpy you
should be definitely comfortable with
numpy so you this should not be alien to
you dot T is transpose at the rate and
then the
matrix okay so this is fine another
feature to say memory is to do a tensor
dot some operation underscore underscore
you'll see you in place operation so you
can scarce a memory that way
so some in some code bases online you
will see people use you know tensor dot
stencil dot operation ad underscore
something okay underscore is the in
place operation index in broadcasting is
very similar to tense of numpy
conversion so you can convert from
tencel to number a dot number five in
this case are from numpy you can make it
a toss out a torch object you will do
this so that you can push it onto device
for example okay so I push it down at
the GPU example so now let's look at
this notion of gradients right so
instead of hand computing radians how
does you know how or does pie charts
what is the fight off structure to get
the gradient started easily okay
how does PI does what does my dolls do
under the hood so although you will not
do this explicitly like import Auto grad
which is from torch we'll see how that
operates so let's say actually did I
execute the whole thing I don't need all
this but let's say
okay so so W dot required requires grad
is the key you know he flagged that you
need to flag it so if you put any way
any tarts tensor I need or you do dot
requires grad is equal to true then if
you do some other other function calls
you will actually get the gradient of
the final result you know wherever you
do the function call you will get the
grain of the final thing with respect to
this this variable okay that's the idea
so think of the final thing is like a
loss if you do law start backward for
example you will get a gradient with
respect to any variable as long as it's
required grad is true okay
so W is a one-dimensional tensor here in
the screening Z and here you can see
that because required grad is by default
false if you do you know some
computation addition total is the sum of
two tensors you know one one plus ones
answers so that was a computation if I
find if you want to find the gradient of
the output just total with respect to
its input okay
inputs any inputs whose required that is
true because none of the inputs required
are is true I got an error okay and and
so now I'm we're just showing that yeah
you can do that by setting explicitly
requires grad is true and and you can
get the gradient okay you can get the
gradient to this as W dot grad so it's
the same operation I did before except
that I added required blood is requires
the Harrisville true and I can get the
gradient of the output with respect to
its input with respect to parameters
which have requires grad is true and you
can also sometimes you see places where
people don't want people want to compute
with respect to variables but they don't
want the gradient to be to be locally
compute
so behind the hood as you do a forward
computation if you requires GLaDOS
through gradients will be kind of
information that is needed for gradient
commutation is stored okay so we don't
want such Taurus then you can do a
forward computation where you ignore
okay although I have told that these
variables have requires that is true you
can drop it by this torch out no grad
since you know context and in that
context it will not compute gradients so
let's directly go to the example where I
want to show the structure let's see so
linear regression so this is a this is
example that we are discussing in the
office hours so this part is just data
prep okay so this is just saying from
scikit-learn there's a function called
make regression which creates a
regression example in this case I'm
creating a 100 hundred sample 100
observations and 1 dimensional data
fundamental features right so and this
is the data set right not sure but ok so
this is the data set okay X is X versus
Y plot scatter plot now next call is I'm
converting the x and y to tensors okay
and then I'm actually creating a linear
regression module okay so the linear
regression module is a class which
subclasses
nn-not module and basically you just
need to define two functions ok what's
the first function is you need to
initialize what are the what are the
parts of the linear regression model
it's essentially going to be a linear
regression is just going to be one
linear transformation right beta
transpose X ok so here you can do that
by saying here the views actually N and
not linear so you should look up what n
n not linear is but you could have
written here you know
initialize B does to be something and in
the forward pass you could have written
beta transpose X okay so you could have
so you need to write these two functions
where you initialize with some
parameters and update you know if you do
forward computation we are making my
prediction that's gonna be media
transpose X here since it's a layer its
beta off there's a bracket that's the
only difference and then this part
they're just moving you know the feeding
odd method whether the machine has a GPU
if it is then you can you can do two
device as I said two GPU or two CPU you
first I created a class called linear
regression so I create a you know my
model object is just an instantiation of
that and then this is the usual
structure so model is defined that was
the subclass of n n dot module
optimization there's going to be a call
like this where you cast the model dot
parameters the things that you want to
optimize so model R parameters is
explicitly the the things that you want
to optimize okay so for linear
regression it's going to be the linear
parameters let's say if it's ten hours
regression the beta 1 beta do be happy
that I'm okay and in this case you know
is a CD optimizer so it's also taking a
learning rate it fixed learning rate and
the third thing you need to define is
criteria okay here we are so thing is we
are using support criteria the loss
function you can actually write your own
function you can write in an umpire the
usual def function you know loss
function is equal to with some arguments
in as input you can do that even for
model you don't need to actually do any
not module okay but it just makes life
easier instead of calling autocrat and
all that and and here is a this one
executed so here is a training one step
of training okay one step of training so
model dot train so I'm setting gradients
to be zero okay and then I'm computing
the loss okay
I'm passing a particular I mean passing
100 points 100 observations competing a
loss with respect to predictions by
underscores and true values wise and I'm
calling this key step which is last dot
backward okay
last
backward computes the gradients with
respect to current parameters and
optimize the step does that grade an
update step okay alpha or whatever this
parameter beta current parameter beta
new parameter is with our current
parameter minus alpha times our learning
rate times gradient right so and then if
you want to evaluate a model then this
is how you evaluate a model where you
don't get over the gradients that's why
there is a starch no grad context okay
and here's the plot so let me run this
okay so we started with a so this this
orange dots are the bunch of the
original ADA original data and the blue
dots are my predictions okay with my
current model parameter okay so this is
one step of training that I executed
from the cell if you press control enter
you can keep receiving the cell multiple
times so you can actually just so you
can you can see that the line is
rotating right the blue line
I mean very I'm running it several times
but so you can see that the blue line
turn okay that's what's happening so the
key step is lost function not backward
and optimize that step okay that's
getting you the gradients and gradient
updated parameters so the only thing you
have to now worry about is how do I sell
it I set up the model correctly here we
did use an inert module so we extended
that so you need to read up a bit of the
documentation how to set the model
correctly if you could set that up then
last function definition and then loss
function not backward and optimize on
the stack we get to most of the way
there you need to put this in a for loop
right as a for loop over a box and then
take care of batch sizes and all that so
there are functions in the in this
script like which you should familiarize
yourself with like Daedelus formations
data loader okay and data set is this
data set data sets
that's helpful for vision similarly for
text there is darts text and that's that
example you saw in the CNN example from
last time right if you'll open that
notebook they're using toss text based
utilities to deal with text data but
ultimately once you set up the data
whether it's images are text the key
steps are the ones I mentioned said of
the model optimizer dot you know law
start backward and optimizer go step
right that's that's that's the thing so
I'll I'll give the link on the syllabus
page so to go through this and that's it
for today ask me questions if after you
know after this we have lab sessions
what is it yeah so the TA is gonna talk
about assignments and solutions of
assignments in logs right did she not
doing that yeah feature extraction takes
a while yes because that's happening on
the it may be happening on the GPU sorry
on the CPU side so you're reading image
from this passing through your network
and getting the feature vector yeah I
smell it may take more time because the
number of parameter is high even
sorry I can't hear you yeah no no it
doesn't so so this example this
combination your conventional classifier
example is actually taking the same rest
at 18 but it's only training on 100
examples so it's it's very fast now for
C part and it could take two hours so I
can't hear you any common
[Music]