hey guys welcome back to the channel and this new 
tutorial on pie torch in previous video we learned  
about uh like the concepts of self attention 
in this video we will uh solidify that concept  
and implement it using pytorch okay so we could 
uh split the implementation in five steps okay  
so the first step is like we create query key 
and value using vectors like using input vectors  
and then we compute attention scores using query 
and key and then we convert the attention scores  
into probability distribution using softmax 
and then we try to compute the weighted values  
and then we add up the weighted values to compute 
the final output okay last two steps four and five  
are kind of a bit uh tricky so that is something 
that you need to understand properly so here you  
can see uh carefully like uh the score that was 
generated by q1 x k1 is multiplied by v1 okay and  
then again q1 x k2 is multiplied by v2 and qn x kn 
is multiplied by vn so for the same query the key  
for which it was multiplied and the score was 
generated corresponding value is multiplied there  
okay so that's how you get the weighted uh values 
okay in the similar way when we try to add up the  
weighted values uh we need to be careful what 
values we need to add up okay so in this case  
see here we are adding the values corresponding to 
query one okay so all the values that are scored  
with query one will be add up to generate the 
first row in the output okay and the same way all  
the values that are weighted with query two will 
be added up to generate the row two okay and so on  
okay so uh this is something that you need to 
understand and be careful when you are doing the  
final implementation okay so let us dive in 
and do the implementation so i copied the  
weight matrices that we have used in the 
previous video to showcase the various steps  
of self retention so that we will verify like 
our implementation is correct to generate the  
final output okay so let us start implementing 
so here let us call our implementation class as
self
attention okay as usual we need to  
import anon module this one and then dot module 
and as usual we will need to create first init
and we will need to pass embedding 
dimension and let us call  
so we need to pass embedding dimension 
as well as model dimension let us call it
and also we need to call super so that
it won't complain anything and let us define 
self dot embedding dimension is equal to  
embedding dimension and self 
dot model dimension is equal to  
okay and now uh so as we have seen in 
the previous video that we need to create  
three linear layers and we will follow the 
same uh same um concept so we will say self dot
to query okay because we will use it to generate 
the query matrix so let us call it nn dot linear
and input feature will be embedding dimension 
and output feature will be modern dimension  
and for the time being we will disable the bias 
because uh we will use this model to generate the  
output that we have uh seen in the previous video 
uh so that it will be consistent okay so we will  
say the same as bias is equal to 
false because we haven't used bias in  
our previous video and similarly we will say 
to key and similarly we will set two values
and that's all for
this init method so we have three linear 
layers that we will use to generate  
uh query key and value matrices okay and 
now let us implement our forward method
and that's where it will be a little 
important to focus so we will say inputs  
and so how we will generate 
query query is equal to self dot
to query and we pass the input and that's all we 
got the query and similarly we will get the key
and similarly we will get the values
okay so we have created a query key and 
value using input vectors so okay so we  
applied three different linear transformation on 
the input and we got query key and values okay  
so the next step is let us comment 
it so that it will be much easier so  
creating create query key 
and value using input vectors  
okay and the next step is what we will do is 
we will create attention scores okay so compute
is codes and how we will do it we will simply 
uh multiply query with key transpose okay so  
that is we will call it attention scores 
is equal to let us do math multiplication
and we will pass query and e transpose okay and 
we got the attention scores and the next step  
is we need to convert the attention score into 
probability distribution and we will simply use  
softmax function there so we will call it softmax 
r now let us call it softmax tension scopes  
okay and then what we will do 
is we simply pass it through the
software function
and we need it to be on dimension 
minus 1 the last dimension  
okay let us comment this as 
well so we will call it convert
attention is goes into probability
distributions
okay
next is the tricky part now we have the tricky 
part is that we need to compute weighted values  
and add them up okay so this is where we will 
use uh some operations mainly it will be like uh  
indexing and slicing so i already created a video 
on that where you can learn it properly like how  
we will do so what we will do actually is we 
will uh reformat our we will create a new uh  
view of the uh attention scores as well as 
the values so that we should be able to uh  
uh simply multiply them and add them 
up easily okay so we will simply use uh  
uh slicing and then we will simply 
add a new dimension in that okay  
so what we will do is we will call it v 
formatted is equal to v and we will simply use uh
let us print it so that it will be much easier 
to view it okay so let us print we format it okay  
and let us exit it so let us create an instance 
of attention is equal to self-attention  
and the input dimension is 4 and 
hidden dimension we will use 3
okay
and that's it now let us pass input to  
this so that it will run the forward 
pass okay so let us run and see it
okay so it says uh math multiplication is in 
one required argument let us see what is missing
okay so it should not be this it should be
this one okay let us print v as well 
so that it will be much more clear  
if we will print here v we will simply say
print v and let us see both of them v and we 
formatted okay okay so what we have here is the  
actual v and we have the formatted v which is 
actually we change the view of it simply okay  
so here you can see clearly that this uh row is 
converted into a matrix of three cross one so  
the none uh dimension is actually added one 
more axis to the row that it was traversing  
okay and the same thing happens for row 2 which 
is value 2 and row 3 which is value 3 okay so this  
is simply like we created a new matrix for each 
value okay and then let us do a formatting of uh
scores as well so that it will be easier for 
us to multiply them okay so we will use same  
trick with uh attention scores as well so let us 
call it attention scores formatted is equal to  
first we need to transpose it so that it will be 
consistent with multiplication so let us call it
transpose
simply let us call t on it it will be transposed
and next is we will simply do
attention score
formatted is equal to we will 
simply run the same slicing
so this is first one this is 
the second one and this is the
new dimension edit okay so let 
us print this as well let's see
let us rerun and see it okay so now see it 
carefully what is happening here so this  
should be q1 k1 okay this should be q2 k1 and 
this should be q3 k1 okay so what will happen is  
when we do a simple matrix multiplication 
not matrix multiplication it's simple uh  
point wise multiplication so this matrix will be 
broadcasted and this v1 will be multiplied with  
all these three values okay so this is q1 k1 
q2 k1 and q3 k1 okay that's what it should be  
okay so then when we multiply it with v1 
we will get a new matrix which will be  
q1 k1 v1 q1 q2 k1 v1 q3 k1 and v1 this will 
be the multiplied values are weighted values  
the same thing the second matrix 
will be q1 k2 and this one will be  
q2 k2 and this will be q3 k2 and the same 
thing will happen when we will simply do a  
multiplication this v2 will be multiplied with 
these three values because it will be broadcasted  
and we will get here q1 k2 v2 and again q2 k2 
v2 and here we will get q3 k2 2 okay and the  
same thing will happen with this third row are 
the third matrix this is nothing but uh q1 k3  
q2 k3 and q3 k3 and we need to multiply 
this with v3 okay so let us do that as well
so that should be clear if you uh you have 
queries ask in the comment section i will  
try to elaborate it further so let 
us simply do a weighted v call it v  
weighted is equal to we will simply multiply this
with reformatted
okay and the final output would be simply
dot sum at dimension is equal to 0.  
so what we will do is let us print a weighted 
view as well then we will do the final thing print
v waited okay let us exit here and then 
read and see the output so this one is q1 k1  
v1 q2 k1 v1 this is q3 k1 v1 okay and this is q2
k2 v2 sorry this is q1 k2 v2 q2 k2 v2 and this 
is q3 k2 v2 okay so this is like for q1 q2 q3  
and corresponding uh keys and values 
okay so the sum should be this row  
is summed with this row and with this row okay 
that's what uh the scores are multiplied for a  
corresponding query okay so the query one for all 
the keys and all the values are summed up okay  
so that's what we will do in some 
values so we don't need this one
so that's what we did we did it in dimension 
0 and that's what is the output print
let us remove this and run and see it
so this is our final output which is coming 
for the input when we pass it through  
the self attention this is what we 
get now let us verify it uh replacing  
the weight matrices of query key and 
value so that it will be consistent with  
what we were getting uh in the previous video okay 
so what we will simply do is self dot to query
dot weight is equal to and then dot parameter 
and we will simply pass w query dot transpose  
and the same thing we will 
do for square e as well so w
t transpose
and we will get the new weights 
and the same thing we will do for  
values as well yes so let us do that as well
w
values
okay so let us run and see that if we are 
getting the same output as we have been getting  
with the previous computation in the 
last video okay so let us run and see it
so here you can see like we are getting one 
point nine one three point two four three  
five 3.57 and so on and if we look at our ppts and 
the final output was 1.9 3.3 and 3.6 and then if  
we uh if we simply uh do a round of these values 
we will get exactly the same okay so this is two  
four four two four four one point nine three point 
two four three point five seven okay so i hope  
that is clear if you have further questions ask 
in the comment section i will try to address them  
definitely so this is for a single input another 
case would be when we have more than one input
sequences okay like that is uh the case 
when we have batch of input sequences  
then how we apply the self attention that 
we will see in the next video and then  
further on we will learn about multi-headed 
attention definitely in the next next video  
so in this video we will stop here in the next 
video we will continue learning self attention  
for best input so thanks for watching 
bye for now take care see you in the next