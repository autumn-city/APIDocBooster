hi welcome to a new video today we want
to talk about the callback library
divergence
i want to give some intuition for it
then we will calculate a simple example
and in the end we will see how it is
implemented in tensorflow probability
so we're talking about the coolback
library versions so we're talking about
something
which allows us to measure how far two
distributions
are apart this seems a little bit
strange at first
so let me give you some intuition for it
assume you have
numbers on the real line so let's
say for instance you have an a with two
and a b which is
five two variables and your task
is to find the distance between
a and b and what you would probably do
is
you say well the distance measure is a
minus b and then take the absolute and
that's correct
that allows you to infer how far they
are apart
if you go one step further and you are
in a coordinate system with
an x1 and an x2 and you have
two variables here for instance an
a vector which is at position
1 1 and a b vector
which is at position
for two and you're again asked what is
the distance
between a and
b so what you would do is you say well
the distance of
a and b is the
norm of the difference between
two vectors and for instance you choose
the two norm in order to
be the shortest distance in this
euclidean space
now we are looking at distributions
so we look at something that is
associating a value of x or
an instantiation of x with a potential
or with a
probability of how likely this value is
to occur
and let us assume you have two
distributions here
so you have one distribution that looks
like this which
you would call p of x and
you have another distribution so let's
also put this here
which is a little shifted it's a little
sharper
and you call this the q of x
and now the question is what is the
distance
between
p and q or in other terms what is the
distance
between two distributions
well that's kind of hard to say i mean
they
they look different state they have
nothing to do with those kind of points
but the callback library divergence
allows us
to find a measure how far they are apart
or how different they are
or how different the description of
these two is
for our random variable so this is where
the
kl divergence comes in and the kl
divergence
is defined as d index kl
of p double vertical line q
and it is defined as the expectation
over x distributed by p of x
of the logarithm of p of x
divided by q of x
and this obviously depends
on whether x is a discrete or continuous
variable
and so we have to define the expectation
respectively
and this results in either
a summation over x over all possible
states of x
times p of x of p of x times the
logarithm
of p of x divided by q of x
and that is if x is discrete
and it is the integral over all possible
ranges of x or values of x
of p of x times the logarithm of p
of x divided by q of x
if x is continuous
and you might ask yourself okay
this is called the divergence and we
actually wanted to
calculate the distance and
it is called divergence because of a
reason since
this particular expression that we use
here does not have all the properties of
a distance
so the kl divergence
does not have all properties of a
distance
properties of a distance
measure therefore we are not calling it
a distance
but conceptually it is doing something
that this
measure also does it allows us to
have an informed decision on how far two
things are apart or
how far the description of a random
variable is apart from another
okay this might seem a little abstract
at first so
for this let us look at an example
and i want to look at an example where
our x
is discrete and we are not just looking
at an x we look at a particular
problem and we look at
the weather and we say in really simple
terms
the weather can either be bad
or it can be good and we call our revere
at capital w and say
that w takes values
in bad and good
so these are two states and we encode
them as
0 and 1. and this of course rings a bell
great this is the bernoulli distribution
and which means that
w is distributed according to a
bernoulli
with a parameter theta and theta
this was the probability of good weather
so this is a probability
of good weather
and now consider the following scenario
you have two people arguing about the
weather
and one says okay i
think that goodweather appears with
probability
of 80 percent and someone else says
okay i say it's only 70 percent
and then you assume those two have this
kind of theta value
and you predict 365 samples with this
particular theta value
and you then want to know how far those
two data sets that you get out
are apart from each other or like how
different they are
and we can give a measure for this by
evaluating
how high the kl divergence is between
those two distributions
so we have a w and
we have a person one which says
theta a is eighty percent and so he says
that w is distributed with a bernoulli
with a bernoulli
with a theta a which is our
p of w distribution and a person two
is saying that theta is actually 0.7
and he says that w is distributed
according
to a bernoulli of theta b
which is our q and the question is
how far apart
are their distributions
or how far apart are their opinions
okay for this let us look at the kl
divergence
so we want to calculate the kl
divergence so dkl
of p and q
and for this since we said w is a
discrete variable we
take the discrete format and of course
we exchange
x with a w because we now have just a
different name for our random variable
and we have this is w of
its w of p of w times the logarithm
of p of w over q of w
and now we have to evaluate the
summation
and since w can take two states zero and
one
this is just a sum of zero and one so
essentially what we're doing is
we have a small w not a capital w
anymore
going from zero to one and here we plug
in
p of w is our small w
times the logarithm of p of big w is our
small w
divided by q of big w is our small w
and then we evaluate this
so now we said that the two people think
it's
a bernoulli just with a different
parameter so
let us recall what was a bernoulli
so a bernoulli with a feta with a
general theta
was or let's say a bernoulli of
w distributed with theta is
an theta to the w times
one minus theta to the one minus w
so we just plug it in and this is
a little bit tedious so one step after
the other
this is the summation so first p of big
w small w
so we have theta a times
and it is first zero times
one minus theta a to the one minus zero
times the logarithm of theta a
times one minus theta a to
the oh sorry of course this is zero not
a theta
one minus zero and this is two to zero
and here we have theta b two to zero
times one minus
theta b to the one minus zero
plus uh theta a
to the one times one minus
theta a to the one minus one
times the logarithm of theta a
to the one times one minus theta a
to the one minus one and
we have divide we divide by a theta b to
the 1
times the 1 minus theta b to the 1 minus
1.
and this is our summation evaluated
okay let's look at how these terms boil
down
here theta a to the power of zero this
is of course this is one
and so in total this becomes one minus
theta a
the same is true for here this is one
minus theta a
and here we have one minus theta b
and here we have theta a to the one
which is theta a and one minus theta a
to the zero is one so we just get
theta a here this will also be
beta a and this will be
theta with b
okay so we get one minus theta a
times the logarithm of one minus theta a
over one minus theta b plus
theta a times the
logarithm of theta a
over theta b and now we said that we
have particular values for theta a and
theta b
so we can evaluate this expression and
for this
let us bring over a terminal and
open up an interactive python session
then we get ourselves numpy
and then let's plug in so we have 1
minus
and theta a was 0.8
times the logarithm
of 1 minus
0.8
divided by 1 minus 0.7
plus 0.8
times the logarithm of
and here we have 0.8 divided by 0.7
and what we get here is
0.0257
and this is our measure of the callback
library divergence
so in essence this is the distance
between
the two opinions okay lastly let us look
at
tensorflow probability we go back to
python
and first i will all import a package
not suppress tensorflow warnings
then we need tensorflow probability
this time we don't need tensorflow and
then we have to define
um our two weber probabilities or better
distributions
let's call this whether a and
it is a tensorflow probability
distributions bernoulli
with a probability of 0.8 this was what
our first person was saying
and whether b is essentially the same
just with a different probability of 0.7
we can then for example observe the
weather
365 times with a and b
and look how often it's good or bad
weather but we're interested in the kl
divergence
and we do this with tfp.distributions kl
divergence
and then we plug in those two objects so
we plug in
weather a and weather b
and we see the value we get out here is
exactly
the same value as we calculated it with
numpy
and you also see that this is a measure
of distributions so we plug in
objects that are related to
distributions i mean whether a
is a distribution it's a bernoulli
distribution
and i want to also show you one
particular
um or one another of the kl divergence
properties is
that if we change what we have in here
we won't get the same value and this is
the reason
or one of the reasons it is called
divergence and not a distance
because it is not symmetric so take care
which
you put in as the first argument in
which you put in as a second argument