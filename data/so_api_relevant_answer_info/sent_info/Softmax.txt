When computing softmax, the intermediate values may become very large.
"When you’re writing code for computing the Softmax function in practice, the intermediate terms may be very large due to the exponentials.
To see that this is the case, let's try your solution (your_softmax) and one where the only difference is the axis argument:
In fact, it is the recommended way of implementing the softmax function - see [here (hyper-link)] for the justification (numeric stability, also pointed out by some other answers here).
We now expect 3 rows of softmax activations where the first should be the same as the third and also the same as our activation of x1!
Additionally, here is the results of TensorFlows softmax implementation:
The following is the code for softmax function;
def softmax(x):
Now your function softmax returns a vector, whose i-th coordinate is equal to
As of version 1.2.0, scipy includes softmax as a special function:
[https://scipy.github.io/devdocs/generated/scipy.special.softmax.html (hyper-link)]
I wrote a function applying the softmax over any axis:
sklearn also offers implementation of softmax
The purpose of the softmax function is to preserve the ratio of the vectors as opposed to squashing the end-points with a sigmoid as the values saturate (i.e.
For more detail see :
[https://medium.com/@ravish1729/analysis-of-softmax-function-ad058d6a564d (hyper-link)]
This provides similar results as tensorflow's softmax function.
Ref: [Tensorflow softmax (hyper-link)]
Softmax using tensorflow:
Softmax using scipy:
Softmax using numpy ([https://nolanbconaway.github.io/blog/2017/softmax-numpy (hyper-link)]) :
The softmax function is an activation function that turns numbers into probabilities which sum to one.
The softmax function outputs a vector that represents the probability distributions of a list of outcomes.
Softmax function is used when we have multiple classes.
The Softmax function is ideally used in the output layer, where we are actually trying to attain the probabilities to define the class of each input.
Softmax function turns logits [2.0, 1.0, 0.1] into probabilities [0.7, 0.2, 0.1], and the probabilities sum to 1.
To understand the softmax function, we must look at the output of the (n-1)th layer.
The softmax function is, in fact, an arg max function.
Before softmax
After softmax
On the surface the softmax algorithm seems to be a simple non-linear (we are spreading the data with exponential) normalization.
Information Theory - from the perspective of information theory the softmax function can be seen as trying to minimize the cross-entropy between the predictions and the truth.
In this case the softmax equation find the MLE (Maximum Likelihood Estimate)
In summary, even though the softmax equation seems like it could be arbitrary it is NOT.
Suppose we change the softmax function so the output activations are given by
[ (hyper-link)]
Note that c=1 corresponds to the standard softmax function.
But if we use a different value of c we get a different function, which is nonetheless qualitatively rather similar to the softmax.
In particular, show that the output activations form a probability distribution, just as for the usual softmax.
This is the origin of the term "softmax".
We can reconstruct the probability P(k=?|x) using properties of exponential family distributions, it coincides with the softmax formula.
If you believe the problem can be modelled by another distribution, other than multinomial, then you could reach a conclusion that is different from softmax.
For further information and a formal derivation please refer to [CS229 lecture notes (9.3 Softmax Regression) (hyper-link)].
Additionally, a useful trick usually performs to softmax is: softmax(x) = softmax(x+c), softmax is invariant to constant offsets in the input.
There is one nice attribute of Softmax as compared with standard normalisation.
The choice of the softmax function seems somehow arbitrary as there are many other possible normalizing functions.
It is thus unclear why the log-softmax loss would perform better than other loss alternatives.
From "An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family" [https://arxiv.org/abs/1511.05042 (hyper-link)]
The authors explored some other functions among which are Taylor expansion of exp and so called spherical softmax and found out that sometimes they might perform better than usual softmax.
It seems like we just cleverly guessed the softmax as an output function and then interpret the input to the softmax as log-probabilities.
Then the softmax is defined as
The exp in the softmax function roughly cancels out the log in the cross-entropy loss causing the loss to be roughly linear in z_i.
Thus, a wrong saturated softmax does not cause a vanishing gradient.
Now, we only focus on the softmax here with z already given, so we can replace
Now, we see that when we take the logarithm of the softmax, to calculate the sample's log-likelihood, we get:
Also, even if the model is really incorrect, which leads to a saturated softmax, the loss function does not saturate.
If the softmax still seems like an arbitrary choice to you, you can take a look at the justification for using the sigmoid in logistic regression:
The softmax is the generalization of the sigmoid for multi-class problems justified analogously.
For example for a = [-2, -1, 1, 2] the sum will be 0, we can use softmax to avoid division by zero.
While it indeed somewhat [arbitrary (hyper-link)], the softmax has desirable properties such as:
tf.nn.softmax produces just the result of applying the [softmax function (hyper-link)] to an input tensor.
The softmax "squishes" the inputs so that sum(input) = 1:  it's a way of normalizing.
The shape of output of a softmax is the same as the input: it just normalizes the values.
The outputs of softmax can be interpreted as probabilities.
In contrast, tf.nn.softmax_cross_entropy_with_logits computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way).
The output of tf.nn.softmax_cross_entropy_with_logits on a shape [2,5] tensor is of shape [2,1] (the first dimension is treated as the batch).
If you want to do optimization to minimize the cross entropy AND you're softmaxing after your last layer, you should use tf.nn.softmax_cross_entropy_with_logits instead of doing it yourself, because it covers numerically unstable corner cases in the mathematically right way.
Edited 2016-02-07: 
If you have single-class labels, where an object can only belong to one class, you might now  consider using tf.nn.sparse_softmax_cross_entropy_with_logits so that you don't have to convert your labels to a dense one-hot array.
tf.nn.softmax computes the forward propagation through a softmax layer.
tf.nn.softmax_cross_entropy_with_logits computes the cost for a softmax layer.
The logits are the unnormalized log probabilities output the model (the values output before the softmax normalization is applied to them).
is essentially equivalent to the total cross-entropy loss computed with the function softmax_cross_entropy_with_logits():
In order to normalize them, we can apply the softmax function, which interprets the input as unnormalized log probabilities (aka logits) and outputs normalized linear probabilities.
It's important to fully understand what the softmax output is saying.
Is the probability distribution in y_hat_softmax close to the probability distribution in y_true?
This result makes sense because in our example above, y_hat_softmax showed that training instance 1's highest probability was for "Class 2", which matches training instance 1 in y_true; however, the prediction for training instance 2 showed a highest probability for "Class 1", which does not match the true class "Class 3".
Using softmax_cross_entropy_with_logits()
We can instead compute the total cross entropy loss using the tf.nn.softmax_cross_entropy_with_logits() function, as shown below.
However, you might as well use the second approach: it takes one less line of code and accumulates less numerical error because the softmax is done for you inside of softmax_cross_entropy_with_logits().
Hence it is a good practice to use: tf.nn.softmax_cross_entropy() over tf.nn.softmax(); tf.nn.cross_entropy()
tf.nn.softmax
tf.nn.softmax_cross_entropy_with_logits
tf.nn.sparse_softmax_cross_entropy_with_logits
tf.compat.v2.nn.softmax
tf.compat.v2.nn.softmax_cross_entropy_with_logits
tf.compat.v2.nn.sparse_softmax_cross_entropy_with_logits
So using o softmax activation, will overcome this problem.
In binary classification this is typically the logistic function, and in multi-class tasks the multinomial logistic function (a.k.a softmax2).
That is why the arguments to softmax is called logits in Tensorflow - because under the assumption that softmax is the final layer in the model, and the output p is interpreted as a probability, the input x to this layer is interpretable as a logit:
softmax might be more accurately called softargmax, as it is a [smooth approximation of the argmax function (hyper-link)].
There is nothing wrong with calculating the softmax function as it is in your case.
The softmax exp(x)/sum(exp(x)) is actually numerically well-behaved.
But it is easy to guard against that by using the identity softmax(x) = softmax(x + c) which holds for any scalar c: Subtracting max(x) from x leaves a vector that has only non-positive entries, ruling out overflow and at least one element that is zero ruling out a vanishing denominator (underflow in some but not all entries is harmless).
[https://nolanbconaway.github.io/blog/2017/softmax-numpy (hyper-link)]
Softmax function is prone to two issues: overflow and underflow
To combat these issues when doing softmax computation, a common trick is to shift the input vector by subtracting the maximum element in it from all elements.
And then take the softmax of the new (stable) vector z
In the above case, we safely avoided the overflow problem by using stable_softmax()
Extending @kmario23's answer to support 1 or 2 dimensional numpy arrays or lists (common if you're passing a batch of results through the softmax function):
You can choose T to be anything (the higher the T, the 'softer' the distribution will be - if it is 1, the output distribution will be the same as your normal softmax outputs).
a) Sample 'hard' softmax probs : [0.01,0.01,0.98]
b) Sample 'soft' softmax probs : [0.2,0.2,0.6]
When the bigger logits shrink more than your smaller logits, more probability mass (to be computed by the softmax) will be assigned to the smaller logits.
Then you should call the softmax as:
Now if you want the matrix to contain values in each row (axis=0) or column (axis=1) that sum to 1, then, you can simply call the softmax function on the 2d tensor as follows:
[Softmax (hyper-link)] doesn't get a single input value.
So, if your NN layer for example has 5 units/neurons, the softmax function takes as input 5 values and normalizes them into a probability distribution which all 5 output values are between [0, 1] using the following formula:
Here's a sample Java code implementing softmax:
I'm assuming you are familiar with the cs231n Softmax loss function.
A Softmax function is defined as follows:
To avoid the overflow, we can divide the numerator and denominator in the softmax equation with a constant C. Then the softmax function becomes following:
We can compare the results with PyTorch implementation - torch.nn.functional.softmax using below snippet:
[Softmax discussion (hyper-link)]
[Discussion on Softmax implementation at PyTorch forumn (hyper-link)]
[An SO thread on implementation of Softmax in Python (hyper-link)]
Your softmax function's dim parameter determines across which dimension to perform Softmax operation.
Please look at picture below (sorry for horrible drawing) to understand how softmax is performed when you specify dim as 1.
Update: The question which dimension the softmax should be applied depends on what data your tensor store, and what is your goal.
Although that tutorial does not perform Softmax operation, what you need to do is just use [torch.nn.functional.log_softmax (hyper-link)] on output of last fully connected layer.
First, this sets dscores equal to the probabilities computed by the softmax function.
Backpropagation is to reduce the cost J of the entire system (softmax classifier here) and it is a problem to optimize the weight parameter W to minimize the cost.
When there are layers after the Affine layer such as the softmax layer and the log loss layer in the softmax classifier, we can calculate the gradient with the chain rule.
In the diagram, replace sigmoid with softmax.
As stated in [Computing the Analytic Gradient with Backpropagation (hyper-link)] in the cs321 page, the gradient contribution from the softmax layer and the log loss layer is the dscore part.
Then do your customary softmax.
I haven't seen a library implementation of softmax, although that's not proof that it doesn't exist.
For the record, the softmax function on u1, u2, u3 ... is just the tuple (exp(u1)/Z, exp(u2)/Z, exp(u3)/Z, ...) where the normalizing constant Z is just the sum of the exponentials, Z = exp(u1) + exp(u2) + exp(u3) + ....
Softmax is used, row 57:
See [softmax_cross_entropy_with_logits (hyper-link)] for more details.
For the Windows version, /mydesire/neural folder has several softmax classifiers, some with softmax-specific gradient-descent algorithm.
look at the link:
[http://www.youtube.com/watch?v=UOt3M5IuD5s (hyper-link)]
the softmax derivative is: dyi/dzi= yi * (1.0 - yi);
Pop1 and Pop2 give weird results because they are calculating the softmax of a softmax.
In addition, your model does not have the softmax as a separate layer, so pop takes off the entire last Dense layer.
To fix this, add the softmax as a separate layer like so:
You can write your own layer to convert the outputs to unit norm (ie normalise in your case) without applying a softmax.
You don't need to add two softmax.
Yet, if you have more intermediate layers and you want them to behave more moderately, you could use a "tanh" instead of softmax.
Softmax tends to favor only one of the results.
a) CNN with Softmax activation function -> accuracy ~ 0.50, loss ~ 7.60
Now that I also see you are using only 1 output neuron with Softmax, you will not be able to capture the second class in binary classification.
With Softmax you need to define K neurons in the output layer - where K is the number of classes you want to predict.
so in short, this should change in your code when using softmax for 2 classes:
When doing binary classification, a sigmoid function is more suitable as it is simply computationally more effective compared to the more generalized softmax function (which is normally being used for multi-class prediction when you have K>2 classes).
There are many different functions, just to name some: sigmoid, tanh, relu, prelu, elu ,maxout, max, argmax, softmax etc.
Now let's only compare sigmoid, relu/maxout and softmax:
softmax:
Softmax by definition requires more than 1 output neuron to make sense.
1 Softmax neuron will always output 1 (lookup the formula and think about it).
If you want to test softmax, you have to make an output neuron for each class and then "one-hot encode" your ytrain and yval (look up one-hot encoding for more explanations).
I was not able to tell conclusively from the docs, but it seems to me that binary cross entropy expects 1 output neuron that's either 0 or 1 (where Sigmoid is the correct activation to use) whereas the categorical cross entropy expects one output neuron for each class, where Softmax makes sense.
As you have two classes, you need to compute the softmax + categorical_crossentropy on two outputs to pick the most probable one.
Also, please find more info about [Softmax Function (hyper-link)] and [Cross Entropy (hyper-link)]
Not sure about other things, but softmax can't predict multiple labels.
Sum of your softmax layer is 1(by mathematical design of softmax function) which means that with softmax you can have only one answer(only one 1 at output and other zeroes).
So expected output for softmax can be
Sigmoid function gives you an activation probability from 0-1 for each activation node, where softmax will give you an activation probability from 0-1 THROUGH THE SUM OF ALL YOUR OUTPUTS.
So you are mixing both concepts, your shape mistmatch is probably due to softmax shape = 2, (152/2!=78 you also have some indexes problems with last minibatch size)
There are minor differences in multiple logistic regression models and a softmax output.
You can think of logistic regression as a binary classifier and softmax regression is one way(there are other ways) to implement an multi-class classifier.
The number of output layers in softmax regression is equal to the number of class you want to predict.
From the example above it can be seen that the output a softmax function is equal to the number of classes.
link:[http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/ (hyper-link)]
Difference: In the link above it is described in detail that a softmax with only 2 class is same as a logistic regression.
we call it as logistic regression when we are dealing with a 2-class problem and softmax when we are dealing with a multinational(more than 2 classes) problem.
Note: It is worth to remember that softmax regression can also be used in other models like neural networks.
Softmax Regression is a generalization of Logistic Regression that
summarizes a 'k' dimensional vector of arbitrary values to a 'k' 
dimensional vector of values bounded in the range (0, 1).
However, Softmax Regression allows one to handle [image] classes.
Hypothesis function:


LR: [image]
Softmax Regression: [image]
Reference: [http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/ (hyper-link)]
You can relate logistic regression to binary softmax regression when you transfer the  latent model outputs pairs (z1, z2) to z = z1-z2 and apply the logistic function
Iterative version for softmax derivative
Reference: [https://medium.com/@aerinykim/how-to-implement-the-softmax-derivative-independently-from-any-loss-function-ae6d44363a9d (hyper-link)]
So basically you have to change a_i in softmax, not the entirety of a.
As far as I understood the question: 2-way Softmax means, the Neural Network should decide, weather it is this (x)or that.
Check this out:
[https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions (hyper-link)]
Applying the 2-way softmax here, will result in ~(0.2, 0.8).
This expression means: slice an array softmax_output of shape (N, C) extracting from it only values related to the training labels y.
The first index range_num has a length equals to the first dimension of softmax_output (= N).
Thus we are only interested in softmax outputs for datapoint class.
loss = -np.sum(np.log(softmax_output[range(num_train), list(y)]))
The code softmax_output[range(num_train), list(y)] is used to select softmax outputs for respective classes.
Softmax is a parameter free activation function like RELU, Tanh or Sigmoid: it doesn't need to be trained.
As of version 1.2.0, scipy includes softmax as a special function:
[https://scipy.github.io/devdocs/generated/scipy.special.softmax.html (hyper-link)]
For the sake of associating an answer with the question, I'll paste in my general softmax function operating over an arbitrary axis, including a tricky max subtraction bit.
My implementation of softmax function in numpy module is like this:
Then it is possible to use softmax function as a typical numpy built-in function.
TFLite compute softmax in floating point, then multiply it by 256 and store as uint8.
However, the ranking should be the same, if you want to display softmax, you can divide them by 256 to show the top few values that are non-zero.
It looks to me like you have misunderstood the argument dim of LogSoftmax.
dim (int) – A dimension along which Softmax will be computed (so every
  slice along dim will sum to 1).
Now, after you pass your input through your two linear layers, the tensor you get and to which you apply LogSoftmax has dimensions 178 x 3.
If x is matrix, please check the softmax function in [this notebook (hyper-link)].
Can I use that softmax function like an equivalent to hinge function?
a hinge function is a loss function and do not provide well-calibrated probabilities, whereas softmax is a mapping function (one that maps a set of scores into a distribution, one that sums to one).
Each consecutive call of tf.contrib.layers.softmax creates a new node in the graph, even though it's used only once.
If you print the graph definition, you'll see 100 softmax ops, while the first snippet will have just one.
Your second code snippet evaluates only logits, which doesn't depend on the softmax op.
In other words, they want you to compute softmax for each row independently.
Lower versions don't support softmax axis.
Then add a softmax layer in this way:
If you donot want to downdegree keras, tf 1.5.0 is the first version that support softmax(axis=axis).
and model.add(Activation(tf.nn.softmax)) works fine.
Due to the Softmax, the contribution of the L1-Regularization to the total cost is in fact constant.
However, the gradient of the regularization term is non-zero and equals to the number of non-zero activations (gradient of abs is sign, so we have a sum of signs of activations that are positive due to the softmax).
I found this question quite useful when was writing my softmax function: [Softmax derivative in NumPy approaches 0 (implementation) (hyper-link)].
I've been dealing with the same problem and finally figured out a way to vectorize a batch implementation of the softmax Jacobian.
It should be noted that softmax is almost exclusively used as the last layer and commonly with a cross-entrpy loss objective function.
In that case, the deriative of the objective function with respect to the softmax inputs can be more efficiently found as (S - Y)/m, where m is the number of examples in the batch, Y are your batch's labels, and S are your softmax outputs.
Let's review the uses of softmax:
You should use softmax if:

You are training a NN and want to limit the range of output values during training (you could use other activation functions instead).
You should not use (or remove) softmax if:

You are performing inference on a NN and you only care about the top class.
Note that the NN could have been trained with Softmax (for better accuracy, faster convergence, etc..).
In your case, your insights are right: Softmax as an activation function in the last layer is meaningless if your problem only requires you to get the index of the maximum value during the inference phase.
To do : normalize your data before giving it to softmax: could you divide it by 1000 before giving it to softmax, so that, instead of having input in [-20000,20000], you would have an input as floats in [-20, 20].
In case of applying softmax on a large numbers, you can try using max normalization:
As you can see, this does not affect the result of softmax.
Applying this on your softmax:
According to [softmax function (hyper-link)], you need to iterate all elements in the array and compute the exponential for each individual element then divide it by the sum of the exponential of the all elements:
Then compute the softmax.
For example, to compute the softmax of [1, 3, 5] use [1-5, 3-5, 5-5] which is [-4, -2, 0].
Softmax is just a function that takes a vector and outputs a vector of the same size having values within the range [0,1].
But sometimes people use Softmax classifier which refers to a MLP with input and 1 output layer (which makes it a linear classifier like linear SVM) where softmax function is applied to the outputs of output layer.
I think the simplest way is using a Reshape layer and then apply the softmax along the correct axis:
This is an answer on how to calculate the derivative of the softmax function in a more vectorized numpy fashion.
Next, you want to compute the Jacobian matrix of your softmax function.
The answer is softmax layer Do not transforms these N channels to the final image of labels
So the softmax layer outputs a 2 channel object where channel 1 says tumor present and channel 2 says otherwise.
Softmax uses Monte Carlo to estimate the expected log likelihood, so you do always need Adam.
I can never remember what form the labels should be passed in, and it's very possible that there is a difference between RobustMax and SoftMax.
You found the correct method, and I would refer to the TensorFlow documentation of tf.nn.sparse_softmax_cross_entropy_with_logits() to determine what the right format is.
In DNN of softmax, we select max(0, value).
Because softmax(x) = softmax(x-c) for any constant c (exp(-c) factors out of all the exponentials, and cancels between numerator and demoninator), you can apply the softmax in a numerically stable way by subtracting an appropriate constant.
No, PyTorch does not automatically apply softmax, and you can at any point apply torch.nn.Softmax() as you want.
But, softmax has [some issues with numerical stability (hyper-link)], which we want to avoid as much as we can.
One solution is to use log-softmax, but this tends to be slower than a direct computation.
Especially when we are using Negative Log Likelihood as a loss function (in PyTorch, this is [torch.nn.NLLLoss (hyper-link)], we can utilize the fact that the derivative of (log-)softmax+NLLL is actually [mathematically quite nice (hyper-link)] and simple, which is why it makes sense to combine the both into a single function/element.
Clipping the logit would appear to focus on generating smaller values to feed to softmax, but that's probably not why it's helping.
(In fact, softmax can handle a logit with value tf.float32.max no problem, so it's really unlikely the value of the logit is the issue).
apply softmax ( action that has to be unlearned gets a nonzero value.. )
Maybe this answer will have to change slightly based on a potential response to my comment, but I'm just going ahead and throwing in my two cents about Softmax.
Generally, the formula for softmax is explained rather well in the [PyTorch documentation (hyper-link)], where we can see that this is a exponential of the current value, divided by the sum of all classes.
Furthermore, you can also see in the description for CE that it automatically combines two functions, namely a (numerically stable) version of the softmax function, as well as the negative log likelihood loss (NLLL).
It does not matter whether this is squashed to a specific range or not (e.g., by applying some form of activation function), as the softmax will be basically a normalization.
First of all, softmax() ceases to make sense here, since you are computing a probability distribution across multiple outputs, and unless you are fairly certain that their distributions are dependent on one another in a very specific way, I would argue that this is not the case here.
Then, a softmax
It does not make sense to apply a softmax to a convolutional layer.
The softmax activation will be applied to the last axis.
Having only one element at the last axis, your softmax output will always be 1 indeed.
For instance, if you want the softmax to consider the 3 channels, you need to move these channels to the final position:
In this case, the softmax would automatically be applied to channels, without extra work needed.
Is it necessary to use tf.nn.softmax() to get the softmax of logits before using tf.nn.weighted_cross_entropy_with_logits()?
FYI [Softmax vs Sigmoid function in Logistic classifier?
You are passing axis=0 to "your" softmax.
Although I don't know how your data looks, 0 is usually the batch axis and performing softmax along this axis is incorrect.
See the docs of [tf.nn.softmax (hyper-link)]: The default value for axis is -1.
Assuming your softmax implementation is correct
First of all it is not fair to compare tensorflow softmax with handwritten softmax because there is randomness included in your program
You can only compare both the softmax if you have some kind of seed(some kind of starting point)
Now if you have performed the above experiment multiple times and every time the tensorflow softmax beats the handwritten softmax then your question is valid
Anyways if your handwritten softmax is correct then with the seed the tensorflow softmax and your softmax should output the same results
And even I think your axis should be 1 in your case which is the last axis as the softmax should be along the axis where there are classes
Softmax output is supposed to sum to 1.
It should be like this: (x is the input to the softmax layer and dy is the delta coming from the loss above it)
If you work out what does this Jacobian look like for softmax [1], and then multiply it from the left by a vector dy, after a bit of algebra you'll find out that you get something that corresponds to my Python code.
[1] [https://stats.stackexchange.com/questions/79454/softmax-layer-in-a-neural-network (hyper-link)]
Mathematically, the derivative of Softmax σ(j) with respect to the logit Zi (for example, Wi*X) is
In the image below, it is a brief derivation of the backward for softmax.
The operations carried out by "Softmax" layer's forward method are:
Yes, NLLLoss takes log-probabilities (log(softmax(x))) as input.
Because if you add a nn.LogSoftmax (or F.log_softmax) as the final layer of your model's output, you can easily get the probabilities using torch.exp(output), and in order to get cross-entropy loss, you can directly use nn.NLLLoss.
Of course, log-softmax is more stable as you said.
And, there is only one log (it's in nn.LogSoftmax).
nn.CrossEntropyLoss() combines nn.LogSoftmax() (log(softmax(x))) and nn.NLLLoss() in one single class.
Therefore, the output from the network that is passed into nn.CrossEntropyLoss needs to be the raw output of the network (called logits), not the output of the softmax function.
So your understanding for the C++ implementation is correct, and for the question on how softmax handles ND input with N > 1 is that each axis is evaluated separately.
The Softmax regression is a generalization of the Logistic regression.
In Logistic regression, the labels are binary and in Softmax regression, they can take more than two values.
Logistic regression refers to binomial logistic regression and Softmax regression refers to multinomial logistic regression.
Based on the wikipedia page for Softmax, I'm under the impression that
  I might need multiple weight vectors, one for every possible
  classification.
Softmax is applied to class based output (look at the graph, it is not boundig box output!).
Boundig box output does not use softmax but rather normal output + L1 loss.
You can do the following on the output of dynamic_rnn that you called output[0] in order to compute the two softmax and the corresponding losses:
EDIT
To answer your question in comment about what you specifically need to do with the two softmax outputs: you can do the following approximately:
You aren't defining your logits for the size 10 softmax layer in your code, and you would have to do that explicitly.
Once that was done, you could use [tf.nn.softmax (hyper-link)], applying it separately to both of your logit tensors.
For example, for your 20-class softmax tensor:
There is also a [tf.contrib.layers.softmax (hyper-link)] which allows you to apply the softmax on the final axis of a tensor with greater than 2 dimensions, but it doesn't look like you need anything like that.
[tf.nn.softmax (hyper-link)] should work here.
These weights and biases (output_layer, output_bias) also do not represent the output layer of your network (as that will come from whatever you do to your softmax outputs, right?).
The problem arises because you call tf.reduce_sum on the argument of tf.nn.softmax.
As a result, the softmax function fails because a scalar is not a valid input argument.
Add regularization to your loss function for all the weights Ej:
cross_entropy =    tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_conv)+beta*tf.nn.l2_loss(W_conv1) +beta*tf.nn.l2_loss(W_conv2) +beta*tf.nn.l2_loss(W_fc1)+beta*tf.nn.l2_loss(W_fc2))
Strictly speaking, if you were to train word2vec using SG model and ordinary softmax loss, the correct label would be [0, 0.5, 0, 0.5, 0].
In practice, however, ordinary softmax is rarely used, because there are too many classes and strict distribution is too expensive and simply not needed (almost all probabilities are nearly zero all the time).
Instead, the researchers use sampled loss functions for training, which approximate softmax loss, but are much more efficient.
These losses are more complicated than softmax, but if you're using tensorflow, [all of them are implemented (hyper-link)] and can be used just as easily.
That is the behavior of the [softmax function (hyper-link)].
As you see the softmax' applied to the result of softmax does not convey much information about the original values, as the values produced are too close to each other, but the softmax' applied to the original inputs of softmax give information about the proportions of the inputs.
Softmax will always return positive results, but it will keep track of other results:
So, if you have negative results for softmax this is not possible, you may have hit some implementation failure.
For running inference on a trained network, you should use the main classifier, called softmax:0 in the model, and NOT the auxiliary classifier, called auxiliary_softmax:0.
Softmax is a well-defined equation, so you can simply make an inverse equation, and then apply that to the softmax output.
As already mentioned above, you can always reverse the softmax operation that should be straight forward.
It features various output thresholding functions like tanh, softmax, etc.
You should check more about softmax regression.
The axis = 1 is for the row-wise softmax you mentioned in the heading of your answer.
As of version 1.2.0, scipy includes softmax as a special function:
[https://scipy.github.io/devdocs/generated/scipy.special.softmax.html (hyper-link)]
I wrote a very general softmax function operating over an arbitrary axis, including the tricky max subtraction bit.
In image classification, the Top-5 accuracy is also often used (the top 5 maximum values in the softmax layer are treated as guesses of the NN and they are considered for the accuracy).
More generally, when the output layer activation is softmax, we will normally get floating probability predictions, and in very very little chance will we get integer probability predictions like [0, 0, 1].
So we can't use accuracy as a metric when using softmax as activation.
Technically speaking, you will never get integer values for the softmax layer output since the type is float.
Nevertheless, accuracy is a valid metric when using Softmax as the classification layer of a neural network.
With the baseline model (Softmax), in one epoch you should be getting way better than 700.
Also, your evaluation model should report true perplexities by using a Softmax -- are you doing that?
CrossEntropyLoss in PyTorch is already implemented with Softmax:
This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one single class.
Theoretically speaking, since the softmax layer you added can predict the correct answer in a reasonable accuracy, the following layer should be able to do the same by preserving the maximum value with identity between the last two layers.
Although the softmax normalizes those bounded outputs (between 0 and 1) again, it may change the way those are distributed, but still can preserve the maximum and therefore the class that is predicted.
When you have a double softmax in the output layer, you basically change the output function in such way that it changes the gradients that are propagated to your network.
The softmax with cross entropy is a preferred loss function due to the gradients it produces.
You can prove it to yourself by computing the gradients of the cost function, and account for the fact that each "activation" (softmax) is bounded between 0 and 1.
The additional softmax "behind" the original one just multiplies the gradients with values between 0 and 1 and thus reducing the value.
Just have one softmax and you're done.
What am I doing wrong with the softmax output layer in PyTorch?
PyTorch [torch.nn.CrossEntropyLoss (hyper-link)] already includes softmax:
This criterion combines nn.LogSoftmax() and nn.NLLLoss() in one
  single class.
So you can just leave out the softmax activation at the end.
A more concise version of softmax would be:
Your input array (or tensor) has shape (5, 1), and by default, tf.nn.softmax operates on the last dimension.
You might see what goes wrong now, because the last dimension is an individual element, which is then normalized with softmax to 1.0
Specify axis=0 to tf.nn.softmax so the operation is performed on the first and not the last dimension.
Reshape the array to shape (1, 5) which would work with a default call to tf.nn.softmax
Typical implementations of softmax take away the maximum value first to solve this problem:
Please refer to math.stackexchange:
[https://math.stackexchange.com/questions/2085616/rbm-deriving-the-replicated-softmax-model-rsm/2087272#2087272 (hyper-link)]
Softmax itself will not occur any problem.
Thus, SoftMax is merely converting back to linear values and normalizing.
The empirical reason is simple: SoftMax is used where it produces better results.
tf.nn.softmax accepts in input a generic nonempty tensor.
You can decide to apply softmax on every dimension you want to.
Usually, softmax is applied to the last dimension (that's the default behavior) of the input tensor.
This because usually softmax is applied to neural network output that's usually a tensor with a shape of [batch_size, num_classes].
However, you could decide to apply softmax to a tensor with a shape of [batch_size, num_classes, 2, 1] and compute the softmax only over the second dimension of the tensor: tf.nn.softmax(tensor, axis=1)
Softmax indeed assigns a probability for each action, but you are calling .max(1)[1] after you get the results from DQN, which computes max and argmax along axis 1 (.max(1)) and selects argmax ([1]).
Try calling the DQN instance directly, it will return the full softmax output.
Softmax function is used for the output layer only (at least in most cases) to ensure that the sum of the components of output vector is equal to 1 (for clarity see the formula of softmax cost  function).
I haven't found any publications about why using softmax as an activation in a hidden layer is not the best idea (except [Quora (hyper-link)] question which you probably have already read) but I will try to explain why it is not the best idea to use it in this case :
If you use softmax layer as a hidden layer - then you will keep all your nodes (hidden variables) linearly dependent which may result in many problems and poor generalization.
But on the other hand a technique called Batch Normalization has been already proven to work better, whereas it was reported that setting softmax as activation function in hidden layer may decrease the accuracy and the speed of learning.
Softmax layers can be used within neural networks such as in [Neural Turing Machines (NTM) (hyper-link)] and an improvement of those which are [Differentiable Neural Computer (DNC) (hyper-link)].
Quickly explained, the softmax function here enables a normalization of a fetch of the memory and other similar quirks for content-based addressing of the memory.
Moreover, Softmax is used in attention mechanisms for, say, machine translation, such as in [this paper (hyper-link)].
There, the Softmax enables a normalization of the places to where attention is distributed in order to "softly" retain the maximal place to pay attention to: that is, to also pay a little bit of attention to elsewhere in a soft manner.
Therefore, it could be debated whether or not Softmax is used only at the end of neural networks.
Edit - More recently, it's even possible to see Neural Machine Translation (NMT) models where only attention (with softmax) is used, without any RNN nor CNN: [http://nlp.seas.harvard.edu/2018/04/03/attention.html (hyper-link)]
Use a softmax activation wherever you want to model a multinomial distribution.
Any time we wish to represent a probability distribution over a discrete variable with n possible values, we may use the softmax function.
Softmax functions are most often used as the output of a classifier, to represent the probability distribution over n different classes.
More rarely, softmax functions can be used inside the model itself, if we wish the model to choose between one of n different options for some internal variable.
Softmax function is one of the most important output function used in deep learning within the neural networks (see Understanding Softmax in minute by Uniqtech).
The Softmax function is apply where there are three or more classes of outcomes.
The softmax formula takes the e raised to the exponent score of each value score and devide it by the sum of e raised the exponent scores values.
For example, if I know the Logit scores of these four classes to be: [3.00, 2.0, 1.00, 0.10], in order to obtain the probabilities outputs, the softmax function can be apply as follows:
def softmax(x):
print(softmax(scores))
Finally, the softmax output, can help us to understand and interpret Multinomial Logit Model.
The outputs variable contains the raw output of the decoder that you're looking for (that would normally be fed to the softmax).
Hierarchical Softmax (HSM) doesn't reduce the memory requirements - it just speeds up the training.
Since the dscore, which is probability (softmax output), was divided by the num_samples, did not understand that it was normalization for dot and sum part later in the code.
Softmax can be easily applied in parallel except for normalization, which requires a reduction.
That being said, if you only care about the softmax, it sounds to me like you're doing inference.
And perhaps it is a viable choice for your application to move the logits to CPU and run softmax there, while your GPU already processes the next batch?
I have created an in-place version of softmax:
This is ensured by softmax activation which will scale the outputs from your last layer to probabilities.
If you want the outputs of the network to be already normalized, I'd strongly recommend LogSoftmax() as the activation combined with NLLLoss as the criterion.
Training with plain softmax is risky for numerical reasons; the output can be easily postprocessed if probabilities are needed in the downstream task.
The softmax enforces that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes.
So in your case if the model is good the prediction will not differ alot when either using sigmoid or softmax, softmax forces the sum of prediction to be 1 sigmoid doesn't do that.
The first part of your question is probably answered by the comment and boils down to the version of SciPy you're using simply being one that doesn't include softmax.
That softmax does not work with integers is also apparent from [the documentation (hyper-link)] and is [by design (hyper-link)]:
This criterion combines [nn.LogSoftmax() (hyper-link)] and [nn.NLLLoss() (hyper-link)] in one single class.
Therefore, you should not use softmax before.
As you already observed the "softmax loss" is basically a cross entropy loss which computation combines the softmax function and the loss for numerical stability and efficiency.
The softmax function creates a pseudo-probability distribution for multi-dimensional outputs (all values sum up to 1).
This is the reason why the softmax function perfectly fits for classification tasks (predicting probabilities for different classes).
As you want to perform a regression task and your output is one-dimensional, softmax would not work properly because it is always 1 for a one-dimensional input.
Note that you can also interpret both the output of the sigmoid and the softmax function as probabilities.
The 1000 nodes, will output 10 probabilities for EACH example, the softmax is an ACTIVATION function!
Now, softmax function will be applied on x_10 to produce the probability output for 10 classes.
If you need both the softprob output and a class prediction, it is probably best to implement softmax on the softprob output yourself eg.
Pay attention also when using tf.keras.layers.Softmax, it doesn't require to specify the units, it's a simple activation
by default, the softmax is computed on the -1 axis, you can change this if you have tensor outputs > 2D and want to operate softmax on other dimensionalities.
The accepted answer has helped me to understand my misconceptions regarding the Softmax temperature implementation.
In other words, the temperature must be applied to the Softmax layer, not the SoftmaxWithLoss layer.
My confusion was due primarily to my misunderstanding of the difference between SoftmaxWithLoss and Softmax.
This enables softmax loss (i.e.
tf.nn.softmax_cross_entropy_with_logits) to "ignore" all -1 labels.
Define a Lambda layer and use the softmax function from the backend with a desired axis to compute the softmax over that axis:
Now, in your example, the same thing holds for computing softmax function.
You must first determine over which axis you want to compute the softmax and then specify that using axis argument.
Further, note that softmax by default is applied on the last axis (i.e.
Update 2: There is also another way of doing this using Softmax layer:
Which is what it is designed to do:
[http://deeplearning.net/software/theano/library/tensor/nnet/nnet.html#theano.tensor.nnet.nnet.softmax (hyper-link)]
Well it looks like the temperature is something you do to the output of the softmax layer.
the term exp(zi-m) can suffer underflow if m is much greater than other z_i, but that's ok since this means z_i is irrelevant on the softmax output after normalization.
I changed it to sparse_softmax_cross_entropy_with_logits , which doesn't need labels in one hot encoding format.
Note that it assumes that the output layer is a Dense(activation='softmax') and it ignores y_pred.
Note that the SampledSoftmaxLoss imposes that the inputs of the last model Layer must have the same dimensions as the number of classes.
turned out I was using the NNlib softmax on the entire matrix.. which the python book was NOT doing.. and all in needed to do was to modify my softmax() call likeso
Underlying problem is that irrespective of whether an array is masked or not, softmax() still operates on 0.0 values and returns a non-zero value as mathematically expected ([link (hyper-link)]).
The only way to get a zero output from a softmax() is to pass a very small float value.
If you set the masked values to the minimum possible machine limit for float64, Softmax() of this value will be zero.
Here is an implementation for your reference on tf.boolean_mask only, and the correct method of using tf.where for creating the mask and passing it to softmax() -
Finally if you want a separate softmax layer, you ara actually looking for a DNN, which can be initialized by the DBN you are training here.
For 2-class classification, you could either use 2 nodes with 1-of-2 coding softmax layer, or 1 binary node (0 for one class, 1 for the other).
Sometimes people use the softmax and sigmoid interchangeably.
However softmax is a loss function which should be used in order to optimize solution multiclass classifiaction problem.
Softmax "transform" NN outputs into probability of each class occurance.
There is no sense in comparing softmax with SVM, because first one is a loss function second one is a classifier.
Softmax:
When you use a softmax, basically you get a probability of each class, (join distribution and a multinomial likelihood) whose sum is bound to be one.
In the case of softmax, increasing the output value of one class makes the others go down (because sum=1 always).
If you plan to find exactly one value (which is the case in your ethnicity classifier) you should use softmax function.
In general cases, if you are dealing with multi-class clasification problems, you should use a Softmax because you are guaranted that the sum of probabilities of all clases will sum 1, by weighting them individually and computing the join distribution, whereas with a Sigmoid, you'd be predicting the probability of each class individually, but not necesarilly weighted.
You need to look at the input to the softmax as well.
This would result in a softmax output vector containing all zeros and a single one value.
You correctly pointed out that the softmax numerator should never have zero-values due to the exponential.
I'm not sure but maybe this question will help you: [Softmax derivative in NumPy approaches 0 (implementation) (hyper-link)]
Since you are using softmax activation in your last layer, your outputs are indeed normalized, so you have two options:
Remove the softmax activation as you have already tried.
The functional form of this weighted average should look familiar to you if you already know what a SoftMax regression is.
To soften the edges of max(x,y), one can use a variant with softer edges: the softmax function.
Most TF functions, such as [tf.nn.softmax (hyper-link)], assume by default that the batch dimension is the first one - that is a common practice.
), and as a result, tf.nn.softmax is computing the softmax activation along the batch dimension.
Computing the argmax of the softmax along the first axis should yield the desired results (it is equivalent to taking the argmax of the logits):
Assuming in your deploying net, the softmax layer is like below:
An easy solution would be to mask out illegal moves with a large negative value, this will practically force very low (log)softmax values (example below).
The sum of all elements of the softmax actications is supposed to equal to 1.
As pointed out by JosepJoestar in the comments, the definition of the softmax function can be found [here (hyper-link)].
Imagine 3 softmax output values s = [0.5, 0.25, 0.25].
I think the NaN problem that you mention in a comment is due to your Softmax function.
Softmax computes the exponential function, exp(x) which can easily exceed the range of single or double precision floats for moderate values of x.
The mathematical form of Softmax is:
I don't use Python or numpy, but I imagine you could define softmax something like:
Actually, there is a function that more closely does what you intend, [tf.sparse.softmax (hyper-link)].
However, it requires a [SparseTensor (hyper-link)] as input, and I'm not sure it should be faster since it has to figure out which sparse values go together in the softmax.
The good thing about this function is that you could have different number of elements to softmax in each row, but in your case that does not seem to be important.
Compute custom softmax using the mask
The last layer could be softmaxloss
[http://www.vlfeat.org/matconvnet/mfiles/vl_nnsoftmaxloss/ (hyper-link)]
softmax() helps when you want a probability distribution, which sums up to 1. sigmoid is used when you want the output to be ranging from 0 to 1, but need not sum to 1.
I would recommend using softmax() as you will get a probability distribution which you can apply cross entropy loss function on.
Then softmax is good because of its proberty that the whole layer sums up to 1.
In general Softmax is used (Softmax Classifier) when ‘n’ number of classes are there.
Sigmoid or softmax both can be used for binary (n=2) classification.
Softmax:
Softmax is kind of Multi Class Sigmoid, but if you see the function of Softmax, the sum of all softmax units are supposed to be 1.
When you use a softmax, basically you get a probability of each class, (join distribution and a multinomial likelihood) whose sum is bound to be one.
The sigmoid and the softmax function have different purposes.
For a detailed explanation of when to use sigmoid vs. softmax in neural network design, you can look at this article: ["Classification: Sigmoid vs.
Softmax."
If you instead have a multi-class classification problem where there is only one "right answer" (the outputs are mutually exclusive), then use a softmax function.
The softmax will enforce that the sum of the probabilities of your output classes are equal to one, so in order to increase the probability of a particular class, your model must correspondingly decrease the probability of at least one of the other classes.
1) Element-wise power of the softmax values.
2) Importantly, both softmax and one-hot vectors are normalized, but not our "soft_extreme".
If you need a better softmax to one-hot conversion, then you may do steps 1 to 3 two or more times in a row.
For the triplet loss defined in the paper, you need to compute L2 norm for x-x+ and for x-x-, concat these two blobs and feed the concat blob to a "Softmax" layer.
You don't need to worry about the 2d shape, the softmax will work fine.
It's not because of the softmax.
Softmax function (not layer) receives n values and produces n values.
In practice, word2vec hierarchical-softmax implementations create an output layer with exactly as many nodes as vocabulary words.
In the simplest implementation, your last layer (just before softmax) should indeed output a 10-dim vector, which will be squeezed to [0, 1] by the softmax.
Practically if your total number of categories is in the range of hundreds to thousands (less than 50K), you don't need to consider using hierarchical softmax, which is designed to run training faster for classifying into millions of categories (for example, the number of words in a vocabulary).
However, if you are interested to implement Hierarchical Softmax anyway, that's another story.
By default, Keras categorical_crossentropy does not apply softmax to the output (see the [categorical_crossentropy implementation (hyper-link)] and the [Tensorflow backend call (hyper-link)]).
You are completely right, just translating the mathematical definition of softmax might make it unstable, which is why you have to substract the maximum of x before doing any compution.
And the output layer is (probably) something like Dense(V, activation='softmax'), which is a matrix H2xV.
And be aware the problem of nan can also cause by X = X.exp() in the softmax(X), when X is too big then exp() will outputs inf, when this happen you could try to clip the X before using exp()
You have incorrectly specified the dimension for the softmax (across batches instead of across the variables), and hence when given a batch dimension it is computing the softmax across samples instead of within samples:
nn.Softmax requires us to specify the dimension along which the softmax function is applied:
In this case, we have two input vectors in two rows (just like when we work with
batches), so we initialize nn.Softmax to operate along dimension 1.
Change torch.nn.Softmax(dim=0) to torch.nn.Softmax(dim=1) to get appropriate results.
softmax is meant for fitting classification networks with a factor response variable.
With that definition in mind, cross entropy with softmax is not appropriate for multilabel classification.
Cross entropy with softmax is appropriate for multiclass classification.
If on the other hand you are a problem with many multiclass labels, then you can use cross_entropy_with_softmax for each of them and CNTK will automatically sum all these loss values.
Therefore, since different tasks involves learning different Loss function, you can attribute to the first part of your assumption, i.e, Softmax is applied only to the labels of the first task, while learning the first task.
Nitpicking: you should not use softmax for binary classification, but rather a regular sigmoid (which is kind of the 2d reduction of the softmax), followed by a log-loss (same).
You shouldn't feed softmax_cross_entropy_with_logits with the output of the softmax, you did two softmax layer here, try to remove softmax layer, because softmax_cross_entropy_with_logits do softmax and cross_entropy together.
You need to split your logits (tf.slice) into two arrays and softmax them separately.
Your cost function implements a softmax atop of your model output which also has a softmax.
The softmax activation is applied while calculating the loss with tf.losses.softmax_cross_entropy.
It is better to calculate the softmax with the loss.
As for why there is no softmax layer, I think that this is because they use the CrossEntropyLoss loss function in the backend.
This function takes in raw logits and combines nn.LogSoftmax() and nn.NLLLoss() in one computation.
So there is no need to perform an additional softmax function before loss evaluation.
It is called logit to previous value before applying the softmax.
That function performs the softmax and then the cross_entropy.
EDIT:
cross_entropy_with_logits_v2 is a layer that does the following cross_entropy (softmax (x), y).
The problem is that in the backward this combination of cross_entropy and then softmax is not numerically stable.
When both are combined, a simplification is made in the following way: [https://deepnotes.io/softmax-crossentropy (hyper-link)]
Softmax activation function is generally used as a categorical activation.
This is because softmax squashes the outputs between the range (0,1) so that the sum of the outputs is always 1.
Only Softmax is not possible for the reasons mentioned.
There are anti-patterns that one can explain to people, things that never work, like the Softmax in the example above.
Softmax is used for [multi-class classification (hyper-link)].
As you stated one of the reason why one uses Softmax over max function is the softmax function is diffrential over Real Numbers and max function is not.
There are some other properties of softmax function that makes it suitable to use for neural networks compared to max.
Second important property of softmax is the output of sofmax function are in interval [0,1] and the sum of these values is equal to 1.
For this reason the output of softmax function can be interpreted as probability.
Please check last line of my code .. basically your dimension for softmax was wrong.
So if you expect softmax output in the VNCoreMLFeatureValueObservation then you need to make sure your model has a softmax as its final layer.
My understanding of Softmax probability
But if we do the softmax trick, after transformation firstly a+b+c = 1 which makes it interpretable as probability.
Softmax chooses f(x)=exp(x).
There is no easy solution to implementing a softmax output, as this will mean changing the cost function and hence the error function used in the backpropagation routine.
Although not the mathematically elegant solution you are looking for, I would try play around with some cruder approaches before tackling the implementation of a softmax cost function - as one of these might be sufficient for your purposes.
Here is the softmax activation on the last layer:
If your model has an output shape of (None, m, n) and you want to compute the softmax over the second axis, you can simply use the [softmax (hyper-link)] activation method and pass the axis argument to it (in your case it must be axis=1):
Alternatively, if you would like to use it as an independent layer, you can use a [Lambda (hyper-link)] layer and the backend softmax function:
Softmax classification uses cross-entropy loss function to train and classify data among discrete classes.
There are other activation functions used like ReLU (Rectified Linear Units) or Sigmoid that are used in Linear Classification and NN; in this case Softmax is used.
As some point out, softmax cross-entropy is a commonly used term in Classification for convenient notation.
Check [this question (hyper-link)] to know more about softmax_cross_entropy_with_logits and its components.
Actually, the softmax_layer returns (None, 3), because the size of last layer is 3.
So, in order to fix it you need the size of output layer (softmax_layer) should be equal to size of your label array (datay.shape[1]).
You are correct  - with torch.nn.CrossEntropyLoss there is no need to include softmax layer.
If one does include softmax it will still lead to proper classification result, since softmax does not change which element has max score.
In general, there's no point in additional sigmoid activation just before the softmax output layer.
Since the sigmoid function is [a partial case of softmax (hyper-link)], it will just squash the values into [0, 1] interval two times in a row, which would give be a nearly uniform output distribution.
considering your softmax probabilities are stored in a variable named 'output'
session.run(train, feed_dict{ derivative: softmax(output.eval(),deriv=True)})
So you have can calculate your l2_delta simply by subtracting your target from the softmax output from the front propagation, instead of the complex Jacobian matrix method.
Excellent explanation on why softmax derivative is not simple as any other activation --> [Derivative of a softmax function (hyper-link)]
Your softmax forward calculation is correct, but possibly numerically unstable.
Softmax derivative itself is a bit hairy.
It is more efficient (and easier) to compute the backward signal from the softmax layer, that is the derivative of cross-entropy loss wrt the signal.
To do it, you need to pass the correct labels y as well into softmax_function.
Also axis was [introduced as an argument on November 22, 2017 (hyper-link)] in TensorFlow's softmax() so if the TensorFlow version is 1.4.0 or less, that would also cause this error.
It appears that the issue wasn't with the handling of Softmax with a TimeDistributed wrapper, but an error in my predictions function, which was summing over the whole matrix rather than on a row by row basis.
If you want to use softmax, use axis=-1.
As the assert statement suggets, the softmax function must be applied to a 2D array; and so that all rows of the result u / z sum to one.
Finally a softmax function on rows would be
In my hidden layer it does not make sense to me to use the softmax activation function too - is this correct?
Actually the gradient of cross entropy with softmax on a one hot encoding vector is just grad -log(softmax(x)) = (1 - softmax(x)) at the index of the vector of the corresponding class.
([https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/ (hyper-link)]).
If the value passed to the softmax is large, the softmax will produce 1 and therefore produce 0 gradient.
I think that it's important to understand softmax and cross-entropy, at least from a practical point of view.
Again, there are some complicated statistical ways to interpret softmax that we won't discuss here.
The key thing from a practical standpoint is that softmax is a function that takes a list of unbounded values as input, and outputs a valid probability mass function with the relative ordering maintained.
This implies that the maximum element in the input to softmax corresponds to the maximum element in the output of softmax.
Consider a softmax activated model trained to minimize cross-entropy.
In this case, prior to softmax, the model's goal is to produce the highest value possible for the correct label and the lowest value possible for the incorrect label.
The definition of [CrossEntropyLoss (hyper-link)] in PyTorch is a combination of softmax and cross-entropy.
CrossEntropyLoss(x, y) := H(one_hot(y), softmax(x))
Equivalently you can formulate CrossEntropyLoss as a combination of [LogSoftmax (hyper-link)] and negative log-likelihood loss (i.e.
LogSoftmax(x) := ln(softmax(x))
CrossEntropyLoss(x, y) := NLLLoss(LogSoftmax(x), y)
Due to the exponentiation in softmax, there are some computational "tricks" that make directly using CrossEntropyLoss more stable (more accurate, less likely to get NaN), than computing it in stages.
Apply softmax to the output of the network to infer the probabilities per class.
If the goal is to just find the relative ordering or highest probability class then just apply argsort or argmax to the output directly (since softmax maintains relative ordering).
Generally, you don't want to train a network that outputs softmaxed outputs for stability reasons mentioned above.
which is mathematically equivalent to using CrossEntropyLoss with a model that does not use softmax activation.
You are right in that you don't need softmax to predict the most probable class - you can indeed just take the class with the highest score.
Howewer, you need softmax in the training time to calculate the loss function (cross-entropy), because it works well only with probability distributions over classes.
The softmax transform guarantees that the output of your network does indeed look like a distribution: all scores are positive and they sum up to 1.
Moreover, at the prediction time softmax can be useful as well, because when you report probability instead of just score, you can interpret it as confidence: e.g.
In such cases, instead of the most probable class you want to look at the probabilities themselves - and softmax helps to estimate them correctly.
You would use softmax cross-entropy loss, if only one class can be "true".
I believe this is wrong, given the derivative of the softmax [as defined on wikipedia (hyper-link)].
Softmax works on an entire layer of neurons, and must have all their values to compute each of their outputs.
The softmax function looks like softmax_i(v) = exp(v_i)/sum_j(exp(v_j)), where v would be your neuron values (in your image, [0.82, 1.21, 0.74]), and exp is just exp(x) = e^x.
Note that, because of this, you don't usually have another layer after the softmax.
Usually, the softmax is applied as the activation on your output layer, not a middle layer like you show.
A more typical architecture would be something 2 neurons -> 3 neurons (sigmoid) -> 4 neurons (softmax) and now you'll have the probability that your input value falls into one of four classes.
Use the softmax and specify the row as the dimension to operate on
Softmax does not work that way.
Take a look at the formula of softmax
This will exclude all the indices that have a corresponding value of 0, and then perform softmax on only the non-zero indices.
Softmax() still operates on 0.0 values and returns a non-zero value as mathematically expected ([link (hyper-link)]).
The only way to get a zero output from a softmax() is to pass a very small float value.
If you set the masked values to the minimum possible machine limit for float64, Softmax() of this value will be zero.
Apply this after your tf.multiply() and before using softmax to change the zeros to the min machine limit for float64, and softmax will mark them as 0 -
It's because of the [tf.nn.sampled_softmax_loss (hyper-link)] function.
The way sampled_softmax_loss works is by sampling weights belonging to a subset of output nodes that are going to be optimized every iteration (i.e.
Your y2 as a matter of fact is not equivalent to computing softmax.
Softmax is
Generally speaking, having softmax in the hidden layers is not preferred because we want every neuron to be independent from each other.
If you apply softmax then they will be linearly dependent as the activation will force their sum to be equal to one.
But with softmax that will not be possible.
Model is able to generalize this dataset with softmax.
With some data softmax worked as good as relu.
With Softmax:
A short answer to your first question is yes, you need to compute the derivative of softmax.
However, since you are using the softmax for your last layer, it is very likely that you are going to optimize a cross-entropy cost function while training your neural network, namely:
where tj is a target value and aj is a softmax result for class j.
Softmax itself represents a probability distribution over n classes:
where n is the number of layer, i is the number of neuron in the previous layer and j is the number of neuron in our softmax layer.
where second partial derivative ∂ak/∂zj is indeed the softmax derivative and can be computed in the following way:
softmax) together with the partial derivative of the output activation w.r.t.
Softmax is a function that finds the probabilities of all the classes such that they sum to 1.
If we are using a softmax, in order for the probability of one class to increase, the probabilities of at least one of the other classes has to decrease by an equivalent amount.
Due to this property we use softmax function in multi-class classification but not in multi-label classification.
You would typically only want to do negative-sampling or hierarchical-softmax, not both.
If you enable hierarchical-softmax, you should disable negative-sampling, for example: hs=1, negative=0.
In the SVM if the new data point has a score that is out of the margin range from the correct class score the loss wouldn't change but in the Softmax loss if the score of the new added datapoint be close to +infinity it will adversely affect the loss, but definitely the loss of Softmax will change.
Because all neurons in a softmax layer should have a total activation of 1, it means that neurons cannot be higher than a specific value.
You can also use samples softmax with multiple labels, you just have to take the mean of each samples softmax
By the definition of [Softmax (hyper-link)], it "'squashes' a K-dimensional vector of arbitrary real values to a K-dimensional vector of real values in the range (0, 1) that add up to 1"
If there is only 1 output value, then the Categorical Probability Distribution that Softmax outputs is just 1, as opposed to values that add up to 1.
Seems like your softmax function is applied to every distinct value in the output vector.
change tf.nn.softmax(tf.matmul(x, W) + b)) to tf.nn.softmax(tf.transpose(tf.matmul(x, W) + b))).
In this case, you should use tf.nn.sigmoid_cross_entropy_with_logits instead of softmax.
I found this implementation: [https://github.com/olirice/sampled_softmax_loss (hyper-link)]
and solved the problem by reshaping the labels
This criterion combines log_softmax and nll_loss in a single function.
For numerical stability it is better to "absorb" the softmax into the loss function and not to explicitly compute it by the model.
This is quite a common practice having the model outputs "raw" predictions (aka "logits") and then letting the loss (aka criterion) do the softmax internally.
If you really need the probabilities you can add a softmax on top when deploying your model.
If you use tf.reduce_sum() in the upper example, as you did in the lower one, you should be able to achieve similar results with both methods: cost = tf.reduce_mean(tf.reduce_sum( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))).
I increased the number of training epochs to 50 and achieved accuracies of 93.06% (tf.nn.softmax_cross_entropy_with_logits()) and 93.24% (softmax and cross entropy separately), so the results are quite similar.
Whatever, you can find on my [GitHub (hyper-link)] the implementation of numerically stable cross entropy loss function which has the same result as tf.nn.softmax_cross_entropy_with_logits() function.
You can see that tf.nn.softmax_cross_entropy_with_logits() doesn't calculate the large numbers softmax normalization, on only approximate them, more details are in [README (hyper-link)] section.
The output layer will be 5 units, each is a essentially a logistic unit...and the softmax can be thought of as an adaptor that normalizes the final outputs.
But "softmax" is commonly used as an output type.
I'll try to give an example...
say I have a 3 class problem, I train a network with a 3 unit softmax.
this is the softmax output...it makes sure the sum of the output units is one.
Of course, you need to make sure you provide the data in the right format (sigmoid and BCE [0,1,1,1,...] instead of softmax + CCE [[0,1],[1,0],[1,0],[1,0],...].
Unlike sparse_softmax_cross_entropy_with_logits, sigmoid_cross_entropy_with_logits expects the logits tensor and the labels tensor to have the same shape and type.
However, I beleive using softmax will give you better results than sigmoid for multiclass classification.
The softmax function and loss function perform similar to the single input/output model even in the case of multiple input/output model.
In your softmax layer you are multiplying your network predictions, which have dimension (num_classes,) by your w matrix which has dimension (num_classes, num_hidden_1), so you end up trying to compare your target labels of size (num_classes,) to something that is now size (num_hidden_1,).
The shape of weight passed to sampled_softmax is not the the same with the general situation.
For example, logits = xw + b, call sampled_softmax like this:
sampled_softmax(weight=tf.transpose(w), bias=b, inputs=x), NOT sampled_softmax(weight=w, bias=b, inputs=logits)!!
torch.nn.CrossEntropyLoss is  a combination of torch.nn.LogSoftmax and torch.nn.NLLLoss():
[ (hyper-link)]
You would just have to remove the softmax on your model's last layer and convert your targets labels.
You don't have to use a softmax layer.
The difference is that a softmax says with class is the most probable one, while multiple logistic regression outputs can assign a data point to several classes simultaneously.
I have recently forgot to put a softmax layer on top of a convolution neural network and I used a fully connected (inner product) layer as output with quite good results, too!
The function that you have mentioned is tf.nn.softmax_cross_entropy_with_logits.
As the name suggests, it first performs a softmax (i.e scaling) on logits and then calculates the entropy between logits and labels.
Therefore, if you input logits (as result in your code) that already performed the softmax then you perform the softmax twice on your logits, which will produce incorrect results.
Since tf.nn.softmax_cross_entropy_with_logits computes internally the softmax (in a numerically stable way) of its input, you have to define your network in order to use the linear activation function: tf.identity
Moreover, once the network has been trained and you want to use use the model for inference, you have to replace the activation with the softmax.
Here, too, the 'temperature' term in the softmax is reduced, changing it gradually from a 'soft' sigmoid function to a 'sharp' [argmax (hyper-link)] 
Try plotting a softmax function for yourself with different temperatures, you'll see the difference...
Contrary to all the information online, simply changing the derivative of the softmax cross entropy from prediction - label to label - prediction solved the problem.
Also, you do not need to run softmax on your outcomes since softmax is designed to smooth argmax.
Implement your own Softmax layer with a temperature parameter.
It should be quite straight forward to modify the code of [softmax_layer.cpp (hyper-link)] to take into account a "temperature" T. You might need to tweak the [caffe.proto (hyper-link)] as well to allow for parsing Softmax layer with an extra parameter.
"extracting features", then you can simply output as features the "top" of the layer before the softmax layer and do the softmax with temperature outside caffe altogether.
You can add [Scale (hyper-link)] layer before the top Softmax layer:
A softmax layer typically maps the predictions(logits) into a more understandable format where's each value in the tensor can add up to become 1
This way, you need not use softmax in the layer.
It will automatically optimize it as if you used softmax.
But if you still wish to use softmax in the Dense layer then you can use it.
You do not have to create one-hot vectors; you can use tf.nn.sparse_softmax_cross_entropy_with_logits.
[https://www.tensorflow.org/versions/r0.8/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits (hyper-link)]
[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/image/cifar10/cifar10.py (hyper-link)]
it expects that the values come from a layer without a softmax activation, so it performs the softmax operation itself.
If you already have a softmax function in your final layer, you should not set from_logits to True, set it to False.
Your model works well without the softmax function and bad with the softmax function for this reason.
When it says that softmax is the only that depends on other outputs it's because sofmax has a condition that all outputs sum 1.
Softmax activation:
Softmax on the other hand, will make sure that the sum of the 3 output values be 1.
To visualize activation over final dense layer outputs, we need to switch the softmax activation out for linear since gradient of output node will depend on all the other node activations.
Then, when you use softmax, the gradient is not considering only one output value, but all of them together.
For ease we can concatenate the outputs of the 3 softmax layers and use binary crossentropy loss.
Softmax output is calculated using this formula (ref: [https://android.googlesource.com/platform/frameworks/ml/+/android-p-preview-4/nn/runtime/include/NeuralNetworks.h (hyper-link)])
As per your test case, input tensor is defined as {1.0f, 2.0f, 10.0f, 20.0f} ([http://androidxref.com/9.0.0_r3/xref/frameworks/ml/nn/runtime/test/generated/examples/softmax_float_1.example.cpp (hyper-link)])
The actual test case is defined here - [http://androidxref.com/9.0.0_r3/xref/frameworks/ml/nn/runtime/test/generated/models/softmax_float_1.model.cpp (hyper-link)] -
Here is the python code (rough) for softmax function:
Output is [ 0.24550335  0.24611264  0.25104177  0.25734224] - This is the expected output (rounded off) {0.25f, 0.25f, 0.25f, 0.25f} // as per your test data - [http://androidxref.com/9.0.0_r3/xref/frameworks/ml/nn/runtime/test/generated/examples/softmax_float_1.example.cpp (hyper-link)]
Softmax is supposed to tell the probability of every class that how much percent this class belongs to the image we are feeding to the neural net.
[0,1,0] Now when softmax predict this image it becomes.
Thats how softmax give us prediction.
Now You problem is that your y_test in the form of [1,2,3] but softmax required that their will be a 3 element array against every label.i.e [[1,0,0],[0,1,0],[0,0,1]]So that softmax will be able to give probability of every class to you.
In simple softmax requiring an array against one image where all the elements are 0 except the true label.
But when softmax see label its just a 1 element so your error arise that softmax is looking for 10 element array but got 1 element.
Solution:
Add this code of line this will convert your every image label to a 10 element array which your softmax is requiring.
The Softmax function is commonly used to map output weights to a set of corresponding probabilities.
This is a simple example of using the Softmax function.
After that the choice of Loss function is loss_fn=BCEWithLogitsLoss() (which is numerically stable than using the softmax first and then calculating loss) which will apply Softmax function to the output of last layer to give us a probability.
Isn't it better to use the sigmoid once after the last layer within the network rather using a softmax and a sigmoid at 2 different places given it's a binary classification?
BCEWithLogitsLoss applies Sigmoid not Softmax, there is no Softmax involved at all.
If you apply softmax in hidden layers then you are essentially combining multiple neurons and tampering with their independence.
nn.CrossEntropyLoss computes log softmax of the input scores and computes the negative log-likelihood loss.
This criterion combines [nn.LogSoftmax() (hyper-link)] and [nn.NLLLoss() (hyper-link)] in one single class.
However, you can convert the output of your model into probability values by using the [softmax (hyper-link)] function.
But if you still want to use Softmax() in your network, then you can use the NLLLoss() as the loss criterion, only apply [log() (hyper-link)] before feeding model's output to the criterion function.
Similarly, if you use LogSoftmax instead in your network, you can apply [exp() (hyper-link)] to get the probability values.
To use log() on the Softmax output, please do:
But many layers have activation parameter which you can use to apply softmax.
dense = tf.keras.layers.Dense(10, activation = 'softmax')(in_layer)
softmax_out = tf.keras.layers.Activation('softmax')(in_layer)
Taking the argmax of an array of logits should return the same as taking the argmax of the softmax of that array.
This is because the softmax function maps larger logits to be closer to 1 in a strictly increasing way.
The softmax function takes a set of outputs (an array) y and maps it to exp(y)/sum(exp(y)), the larger the y[i] the larger the softmax of y[i] and so it must be that argmax(y[i])==argmax(softmax(y[i]))
Instead of using sparse tensors to make the tensor with "all zeros except softmaxed top-K values", use [tf.scatter_nd (hyper-link)]:
I think that it is wrong to say that any question with the term “Softmax function” is a duplicate copy question.
Diversity of opinions, questions and answers on the term ”softmax function” should be well come and should not be called duplicate copy.
For more information on how to apply softmax in machine learning, please click the link below: 
[How to use softmax output in python for neural-network and machine-learning to interpret Multinomial Logit Model?
103 and the number of columns will be 10 with each row indicating softmax output with probabilities for each input example.
With mentioning column index as (Y), you are only fetching probability value from softmax output(each row), which indicates the model's outcome for the desired number.
For example, if you wanted the model to detect a number to be 3 then you're only checking how model did on softmax output that should have had high probability value on the index representing value 3.
What you'll get in log_likelihood is a vector that is representing the negative log-likelihood cost for softmax units which should have been predicted with high probability.
Softmax function is one of the most important output function used in deep learning within the neural networks (see Understanding Softmax in minute by Uniqtech).
The Softmax function is apply where there are three or more classes of outcomes.
The softmax formula takes the e raised to the exponent score of each value score and devide it by the sum of e raised the exponent scores values.
For example, if I know the Logit scores of these four classes to be: [3.00, 2.0, 1.00, 0.10], in order to obtain the probabilities outputs, the softmax function can be apply as follows:
def softmax(x):
print(softmax(scores))
Finally, the softmax output, can help us to understand and interpret Multinomial Logit Model.
The formula you linked is a standard affine transformation preceding the application of a pointwise nonlinearity, not the softmax activation function itself.
Be careful if you need to train your network with a crossentropyloss as this latter already include a softmax.
The key observation here is that the TensorFlow sampled softmax function returns actual losses, not a set of predictions over the set of possible labels to compare with the ground truth data to then compute losses as a separate step.
This is used for the labels argument of the sampled_softmax_loss function.
Second, we construct a new custom Keras layer that calls the sampled_softmax_loss function with two Keras layers as its inputs: the output of the dense layer that predicts our classes, and then the second input that contains a copy of the training data.
Finally, we have to construct a new "dumb" loss function that ignores the training data and just uses the loss reported by the sampled_softmax_loss function.
Note that because the sampled softmax function returns losses, not class predictions, you can't use this model specification for validation or inference.
You'll need to re-use the trained layers from this "training version" in a new specification that applies a standard softmax function to the original dense layer which has the default activation function applied.
For example, you'd probably want to make the number of classes an argument of the SampledSoftmax layer, or better yet, condense this all into the loss function as in the original question and avoid passing in the training data twice.
I have built a custom GumbelSoftmax layer for usage in Tensorflow 2+.
[https://github.com/gugarosa/nalp/blob/master/nalp/models/layers/gumbel_softmax.py (hyper-link)]
Looks to me you need to provide the dim argument to softmax as 0 so it calculates the column softmax instead of row softmax (by default dim=-1); Since for each row you have only one element (w.shape[1] == 1), whatever the value is, softmax gives 1:
So you are applying softmax on 1 * 1 entry which will return you 1 always.
Change model = tf.nn.softmax(tf.matmul(k,w)+b)
to model = tf.nn.softmax(tf.reshape(tf.matmul(k,w)+b, [-1]))
But the reduce_sum in tf.nn.softmax() is by default applied to the last axis.
NCE or sampled softmax is essential for training LMs with large vocabularies.
If you increase your vocabulary size to say 500k, you will see a significant difference between NCE and full softmax.
No, there is no differentiable solution, that is why we use the softmax activation, because it is a differentiable approximation to the max function.
By construction, the softmax prediction should sum to one.
In short the suggested solution for it in keras is to replace the softmax layer with a sigmoid layer and use binary_crossentropy as your cost function.
Replacing output = nn.Softmax(self.fc3(output)) with output = F.softmax(self.fc3(output)) seems to solve the issue.
nn.Softmax defines a module, nn.Modules are defined as Python classes and have attributes, e.g., a nn.LSTM module will have some internal attributes like self.hidden_size.
On the other hand, F.softmax defines the operation and needs all arguments to be passed (including the weights and bias).
This explains why F.softmax instead of nn.Softmax resolves your issue.
I think you need to change your nn.softmax to F.softmax why?
because the nn.functional contains the softmax operation, and you have imported it already in our code torch.nn.functional as F. So to solve the problem, just make it F.softmax instead of nn.softmax.
However, if you are using a softmax or sigmoid in the final layer in the network, you do not need from_logits=True.
Softmax and sigmoid output normalized values between [0, 1], which are considered probabilities in this context.
Now to fix your 50% accuracy issue with softmax, change the following code from this:
Remember that when you are using softmax, you are outputting the probability of the example belonging to each class.
The softmax is
In functional sense, the [sigmoid is a partial case of the softmax function (hyper-link)], when the number of classes equals 2.
multi-labels), while softmax deals
with exclusive classes (see below).
[tf.nn.softmax_cross_entropy_with_logits (hyper-link)] (DEPRECATED IN 1.5)
[tf.nn.softmax_cross_entropy_with_logits_v2 (hyper-link)]
[tf.losses.softmax_cross_entropy (hyper-link)]
[tf.contrib.losses.softmax_cross_entropy (hyper-link)] (DEPRECATED)
Just like in sigmoid family, tf.losses.softmax_cross_entropy allows
to set the in-batch weights, i.e.
[UPD] In tensorflow 1.5, v2 version [was introduced (hyper-link)] and the original softmax_cross_entropy_with_logits loss got deprecated.
[tf.nn.sparse_softmax_cross_entropy_with_logits (hyper-link)]
[tf.losses.sparse_softmax_cross_entropy (hyper-link)]
[tf.contrib.losses.sparse_softmax_cross_entropy (hyper-link)] (DEPRECATED)
Like ordinary softmax above, these loss functions should be used for
multinomial mutually exclusive classification, i.e.
[tf.nn.sampled_softmax_loss (hyper-link)]
[tf.contrib.nn.rank_sampled_softmax_loss (hyper-link)]
In test time, it's recommended to
use a standard softmax loss (either sparse or one-hot) to get an actual distribution.
I've included this function to the softmax family, because NCE guarantees approximation to softmax in the limit.
However, for version 1.5, softmax_cross_entropy_with_logits_v2 must be used instead, while using its argument with the argument key=..., for example
We have the softmax formula which is an activation for multi-class scenario.
It will be a softmax activation and a (categorical)CE.
So one could use softmax_cross_entropy or more preferably softmax_cross_entropy_with_logits.
As far as I understand, the assignment wants you to implement your own version of the Softmax function.
But, I didn't get what do you mean by and pipe its output with torch.nn.Softmax.
Are they asking you to return the output of your custom Softmax along with torch.nn.Softmax from your custom nn.Module?
You could apply the [tf.split (hyper-link)] function to obtain 91 tensors (one for each class), then apply softmax to each of them.
Method 2 is correct, since softmax is used for multi-class problems.
If your network outputs 2 channels use a softmax function and calculate your loss with your output (continous values) and target (2 channel one-hot-encoded).
The softmax activation function comes in the last layer but the question of when to use it depends of your data, labels, loss function and optimizer.
For example if you have multi-class classification and integer labels like [0,1,2] not having the softmax may make your training faster.
I had an experience where I had to use Adamax without softmax for the best result.
Note that you should replace the softmax activation with a sigmoid, since in the your case the probabilities don't have to sum to 1
tf.exp(s) easily overflows for large s.  That's the main reason that tf.nn.softmax doesn't actually use that equation but does something equilivent to it (according to the docs).
When I rewrote your softmax function to
Here is a fully working python 2.7 implementation that uses a hand-crafted softmax and works (using the reshape function)
I think I've figured out my misunderstanding: The softmax units behave as groups of binary subunits, and each subunit has its own weights to the hidden units.
Using softmax, with your current configuration, is actually forcing it to choose always only one class.
It might be the reason you get always recall equal to zero for one class and one for the other class in your experience using softmax.
The binary_crossentropy is not supposed to be used for softmax.
If you change the loss to categorical cross-entropy and make the DENSE of size 2 for the last layer( since you want to choose between two classes using softmax), you should get almost the same performance; i.e: change this part of the code;
Unfortunately, softmax is not as easy as the other activation functions you have posted.
Luckily, the loss it is something a little bit easier to understand, since you can think about the softmax giving you some probabilities (so it resembles a probability distribution) and you calculate the Cross Entropy as is between the returned values and the target ones.
While Softmax returns the probability of each target class given the model predictions, SoftmaxWithLoss not only applies the softmax operation to the predictions, but also computes the multinomial logistic loss, returned as output.
See
[SoftmaxWithLossLayer (hyper-link)]
and [Caffe Loss (hyper-link)]
for more info.
possible duplicate: [Applying tf.nn.softmax() only to positive elements of a tensor (hyper-link)]
Note that tf.random.categorical takes logits as inputs, so you don't need the softmax activation at the end of the dense layer.
The main issue lies in the fact that Softmax is called twice.
Softmax was called in the forward_propagation part of the code, and that was placed in the Tensorflow cross entropy code, which already contains a softmax, hence causing an anomaly in the outputs.
If you want to add the term w_G^{mn}, I guess "adding a value log(w_G^{mn})" to each output before applying usual softmax should have a same effect.
tf.nn.sparse_softmax_cross_entropy_with_logits accepts sparse labels of integer type.
To use one-hot float labels, consider using tf.nn.softmax_cross_entropy_with_logits instead.
The softmax op produces a vector-valued prediction for each example.
If you want to use a softmax for this problem, you could use [1, 0] as the output target where you are currently using [0] and use [0, 1] where you are currently using [1].
Another option is you could keep using just one number, but change the output layer to sigmoid instead of softmax, and change the cost function to be the sigmoid-based cost function as well.
Softmax doesn't work on a long tensor, so it should be converted to a float or double tensor first
and for simplicity just use inbuilt softmax function:
tf.nn.softmax_cross_entropy_with_logits takes in W(x)+b and efficiently calculates the cross entropy.
Logit is nowadays used in ML community for any non-normalised probability distribution (basically anything that gets mapped to a probability distribution by a parameter-less transformation, like sigmoid function for a binary variable or softmax for multinomial one).
You can add a Global max pooling layer and then a dense layer with a softmax activation.
The global max pooling layer takes the max vector over the steps dimension so there will be no more different shape of data and then you can apply a dense layer ith the softmax activation.
Your guess is correct, the weights parameter in [tf.losses.softmax_cross_entropy (hyper-link)] and [tf.losses.sparse_softmax_cross_entropy (hyper-link)] means the weights across the batch, i.e.
The reason that RelaxedOneHotCategorical is actually differentiable is connected to the fact that it returns a softmax vector of floats instead of the argmax int index.
You can't get what you want in a differentiable manner because argmax isn't differentiable, which is why the Gumbel-Softmax distribution was created in the first place.
As of version 1.2.0, scipy includes softmax as a special function:
[https://scipy.github.io/devdocs/generated/scipy.special.softmax.html (hyper-link)]
You need to pass that raw output through softmax in order to squash it into something that can be interpreted as probabilities.
Your code DOES put it through softmax with the call to tf.nn.softmax_cross_entropy_with_logits, but that ends up in a separate branch of the graph, which is not what you are printing for your own eyes -- TF gets it for the cost calculation, but you don't see it
So, simply run the raw output through softmax separately, and then take argmax of that output.
You have used softmax as your activation in the last layer.
(For more clarity, you can look into how softmax function works)
So it calculates values for each class and then softmax normalizes it.
The output of a softmax layer is not 0 or 1.
[https://en.wikipedia.org/wiki/Softmax_function (hyper-link)] for the definition.
My understanding is, Softmax says the likelihood of the value landing in that bucket out of the 201 buckets.
A decent [description on developers.Google of SoftMax (hyper-link)] here
The softmax loss is per pixel: the output (predicted) probability is a vector of length #classes per-pixel and the loss is "SoftmaxWithLoss" per-pixel.
Now let's look at what you have and see if you need a softmax.
You don't need a softmax because P1+P2+P3+P4 is already normalized.
I don't know exactly what you are doing but if you are just doing a multi class classification model, why not have one output per class and than finally use the softmax?
You don't have to convert your ground truth into the one-hot format, sparse_softmax will dot it for you.
You can vectorize softmax_grad like the following;
Details: sigma(j) * delta(ij) is a diagonal matrix with sigma(j) as diagonal elements which you can create with np.diagflat(s); sigma(j) * sigma(i) is a matrix multiplication (or outer product) of the softmax which can be calculated using np.dot:
Softmax is relatively new (probably earlier this year).
Your version of tensorflow is probably from before Softmax is added, but the version of Keras is from after it was added.
Therefore when you call Softmax, keras tries to call it in tensorflow but it receives an error.
Apply softmax to filter dimension:
[code snippet]
First, I would return the value of the softmax tensor.
You look for a reference to it somewhere (you keep a reference to it when you create it, or you find it back in the appropriate tensor collection) And then evaluate it in a sess.run([softmaxtensor,prediction],feed_dict=..) And then you play with it with python as much as you like.
As described in the comments, it is essential that the output of softmax(scores) be an array, as lists do not have the .T attribute.
softmax produces a list of 3 items; the first is the number 0, the rest are arrays with shape like x.
Were you expecting softmax to produce an array with the same shape as its input, in this case a (3,80).
Softmax should be applied in places where we want [almost] one-hot distribution in trained network.
Output of softmax defines distribution (sum is equal to 1) but there are no restrictions about input of softmax.
If you pass all 0 to softmax you will get uniform distribution as an output.
Secondly, the loss function is as follows, from softmax_loss_layer.cpp:
prob_data is the output from the softmax, as explained in the caffe tutorials, softmax loss layer can be decomposed into a softmax layer followed by multinomial logistic loss
For the first one, yes, softmax always sums to one.
The implementation of tf.nn.softmax_cross_entropy_with_logits further goes to native C++ code, [here (hyper-link)] is XLA implementation.
If you wish, you can clip the logits just before the softmax, but it's not recommended, because it kills the gradient when the output is large.
tf.nn.softmax_cross_etnropy_with_logits_v2 as per [the documentation (hyper-link)] expects unscaled inputs, because it performs a softmax operation on logits internally.
Your second input [0, 0, 0, 0, 1] thus is internally softmaxed to something roughly like [0.15, 0.15, 0.15, 0.15, 0.4] and then, cross entropy for this logit and the true label [0, 0, 0, 0, 1] is computed to be the value you get
The way tf.nn.softmax_cross_entropy_with_logits_v2 works is that it does softmax on your x array to turn the array into probabilities:
Then the output of tf.nn.softmax_cross_entropy_with_logits_v2 will be the dotproduct between -log(p) and the labels:
So in your first sample, the softmax probability of the first index is
tf.nn.softmax will always return an array of sum=1.
Since your output is 1 value (you have one unit on your final/output layer), a softmax operation will transform this value to 1.
The formula that you use is correct and it directly corresponds to [tf.nn.softmax_cross_entropy_with_logits (hyper-link)]:
Note that q is computing [tf.nn.softmax (hyper-link)], i.e.
See [this answer (hyper-link)] that outlines the difference between softmax and sigmoid functions in tensorflow.
you can understand differences between softmax and sigmoid cross entropy in following way:
for softmax cross entropy, it actually has one probability distribution
for softmax cross entropy it looks exactly as above formula，
Note: Also, you don't need to use .Softmax() before nn.CrossEntropyLoss() module as this class has nn.LogSoftmax included in itself.
If your target vocabulary(or in other words amount of classes you want to predict) is really big, it is very hard to use regular softmax, because you have to calculate probability for every word in dictionary.
By Using  sampled_softmax_loss you only take in account subset V of your vocabulary to calculate your loss.
Sampled softmax only makes sense if we sample(our V) less than vocabulary size.
If your vocabulary(amount of labels) is small, there is no point using sampled_softmax_loss.
This is a sampled softmax_cross_entropy_with_logits, so it takes just a few samples before using the cross entropy rather than using the full cross entropy: [https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/nn_impl.py#L1269 (hyper-link)]
How is the softmax computed since there are seq_length x seq_length values per batch element?
The softmax is performed on w.r.t the last axis (torch.nn.Softmax(dim=-1)(tensor) where tensor is of shape batch_size x seq_length x seq_length) to get the probability of attending to every element for each element in the input sequence.
As you know, first scaled-dot-product is performed QK^T/square_root(d_k) and then softmax is computed for each sequence element.
Here, Softmax is performed for the first sequence element "Thinking".
The raw score of 14 and 12 is turned into a probability of 0.88 and 0.12 by doing softmax.
With the softmax, we're computing the Napierian logarithm of the loss.
the values will be rescaled to [0,1] and tf.nn.sparse_softmax_cross_entropy_with_logits will return expected values
The logits are the output of a SoftMax function.
I've built a network following basic MNIST examples, I've used tf.nn.softmax in the final layer and expected to get results from said layer.
It looks like I need to use softmax function again to get the results from a layer such as yPred = tf.nn.softmax(logits) with logits being the name of the output layer.
On top of requiring the input to be not softmax-ed, tensorflow's ctc_loss also expects the dims to be NUM_TIME, BATCHSIZE, FEATURES.
It does log() which gets rid of the softmax scaling and it also shuffles the dims so that its in the right shape.
When I say gets rid of softmax scaling, it obviously does not restore the original tensor, but rather softmax(log(softmax(x))) = softmax(x).
Keras differ between the layer softmax and the activation function softmax.
Instead of mapping the continuous input data to a specific class as it is done using the Softmax function as in your case, you have to make the network use only a single output node.
To come back to your Matlab implementation, it would be incorrect to change the current Softmax output layer to be an activation function such as a Sigmoid or ReLU.
Because SoftmaxOuput computes the gradient in the backward pass, it is not convenient to multiply losses as you want to do.
