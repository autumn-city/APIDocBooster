 In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.  This is numerically more stable. 
 Yeah, conceptually this is a tensor of integers, they can only be 0 or 1, but we, we�re going to be using a cross entropy style loss function, so we're going to actually need to do floating-point calculations on them.  That's going to be faster to just store them as float in the first place rather than converting backwards and forwards, even though they're conceptually an �int� we're not going to be doing kind of �int style calculations� with them. 
We say the loss is minimized because the lower the loss or cost of error, the better the model. 
 If we use negative  log likelihood loss or cross entropy in pytorch.  But  numerically, like stability wise on the computer, the cross  entropy one is more stable.  So and also for this one, really pay  attention to this one, it's taking the logits as input. 
 And the term binary  cross entropy and negative log likelihood are essentially the  same. 
