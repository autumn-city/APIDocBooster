okay so uh today is lecture 11
uh about logistic regression and
this is the um the most basic topics for
uh to introduce you to the deep learning
okay so uh
because we haven't touched our machine
learning nutshell for a little while
so let me just try to uh let's just
review what
is the nutshell we try to um we try to
use this
more like a framework to chunk or to
analyze
all the possible machine learning method
so we can
segment the possible machine learning
tasks or
like tools according to the data they
can handle
and we have touched uh tabular data
and we're going to touch imaging data
and then we can also separate the
machine learning method according to the
task
according to representation according to
to a scoring function
according to optimization and according
to their model
uh characteristics um
so roughly uh by now i actually can show
you this figure
this slides so if we use
the task variables to decompose our
whole class
we can segue them into like
roughly uh six different uh segments
the the biggest segment um the basic
segment
uh we just finished is the regression uh
section and that's really about when
your why is continuous
but in that section we covered uh
all the possible basics and the
most basic loss from formulation the
most basic representation formulation
the most basic observation strategies
and then we spent actually quite some
sections about learning theorems so uh
the most important we covered is
the workflow so what type of workflow
can
guarantee you can help you to to get a
model generalized well
and uh the fundamental theorems we
introduce you is this bias variance
trade-off and
last section we uh try to explain
why learning about biosphere trade-off
and
understanding uh like where you are
it's important because that's more of a
guidance
for you to understand what is your model
why your model works and how can you
improve
upon your current uh based on those
sanity curves we ask you to draw in
homework too
so now we actually move uh totally move
to classification
so the kenya's neighbor method we
covered it's more like
a boundary case it can be used for
regression it can
also be used for classification and
today the logistic regression we're
going to cover
is just for classification and then
uh later we're going to cover uh like um
supervised method
which essentially there's no why and
classification is about
why it's discrete and then uh if
possible we're
also going to choreographical models so
the
uh important at the essence there is
we try to model dependency proposed
dependence among the variables
and we treat everything as a random
variable and
i'm going to touch it a little bit today
and then if
time possible we're going to touch a
little bit about reinforcement learning
so everything we have i mean the first
five
and the vanilla version the basic
version
always assume your samples are
independent and identically distributed
so
that's the basic iid distribution
id assumption but when you um
move from the move into the
reinforcement
learning regime then that id assumption
just doesn't hold anymore
and so it's it's really a trajectory
dependent of some
the samples are dependent with each
other
and um it's there's a almost everything
you learned
needs to be revised learning the first
five section needs to be revised and by
consider this
non-iid property and then to get a
better
reinforcement learning all right so uh
that's the
uh chunks of class according with
regarding to the tasks
uh we can also think about our class i
mean um
chunk them with regard to the data
so uh basically everything we have
covered
i have focused on the tabular data
so we can think about tabular data i
just those kind of matrix we show you
string tests and then different features
as columns and different samples as
rows so that's the what we meant tabular
data
so we actually don't
uh explode the relationships among x1 to
xp
while you view your data from this angle
right
so you have your samples as well as rho
onto sn and you have x1 to xp
but when we move to the imaging data and
there's a clear relationship
among x1 xp just the most simple
assuming you have this really really
tiny image
has a maybe two times three number of
pixels
so this is not impossible like for the
current state or there's
it's much larger than this but just give
you an example
so and this is in fact your x
so your x is x1 x2
x3 x4 x5
and x6 and they locate
in this kind of grade two dimensional
grade structure
and there are certain really important
properties we need to model
to get better predictions but explore
this type of structure
so that's the this sections focus
and after we finish the
imaging data or to degrade data we're
going to move to one dimensional
sequential data
and if time allows i will also cover a
little bit
about graph data so in essence
is so you only knows like
the elements um
you actually know elements of x1 to xp
and you also know um some like
relationship among x that just given
and you need to model those type of
relationships um
but you actually do not know if they're
very nicely
uh stayed in uh euclidian um like
geometric space what you only know is
the relationship among nodes
so that's the graph structured data and
there's
actually also more like set structured
you actually know just those they are in
a set
but you don't know their relationships
or
it's not just this two-dimensional it's
actually three-dimensional
so assuming you're if you need to
maybe measure or predict the um
virtual reality measurements right so
there's x and y
and z right that's a clear a natural
three-dimensional measurements
so how do you perform that kind of
prediction there's another
group of method that fits there so we so
that's
just a variable we can categorize all
the possible data
possible method so according to the data
so before we move on i actually want to
show you uh
something about your ex your project
projects like project plan
so uh this is the
yeah so before i show you this uh
ambition share i want to show you
um so if so we we care to write the
projects into three types right
so their uh research type you can re
produce a paper a state of the art paper
and also if you like the
library type i mean you can just try to
benchmarking
some of the maybe pytorch maybe
tensorflow libraries
and those are very engineering type and
the third type is
actually what i really really like too i
i think a lot of students will like this
type
especially if you are interested in help
do some help things um maybe for example
kobe 19
which really troubles earth i mean makes
such a difficult year
all right so uh there are many places
you can find such a kind of data
and there's one hub place called kovi
data i put it as first link
in our project page and the second page
is the scado
site and it's actually extremely easy to
fund this type of data so i just
search a data actually i did not even
use
cody data i just searched data and
then this gave me a lot of tags
really good tags so tagging here i just
tag
the image data so i tag uh
okay i just cleaned it the tag so let me
add this tag
so i tag it with image data uh medicine
data
um it's health i mean this combination
of filters give me this top rank
this data for example just uh being
shared four months before
and this is extremely actual very nice
data so this is from brazil
and i think it's about like yes this is
a number
about roughly 1200 ct scans of positive
cova 19 patients
and about the equal number of ct scans
from non-infected
cd scans and then they share this data
it's super easy
to download and this is how they
organize the data
so actually in fact uh next actually on
thursday i'm gonna show you
um i showed you before right so
uh i told you i don't like
benchmark data i like to collect my own
data
and if that's something you like to look
over your phone and think about some
tasks
so this is the task i try to make up
from my own data
so i just classify all this
uh the food from my phone into
not sweet uh and sweet and
and the coffee and mostly it's what i
i made so i saw my bot
of course so and then i just use this
fast ai library
i try to directly like load the library
i predict
so if you directly load this library
called rest night 34
and this is the performance so
performance i have only seven
images uh in this in my validation
data and this almost nothing got
predicted correctly i could only one
only one get predicted correctly just
but
and uh the majority of the sweet got
predicted at coffee and no sweet
and coffee got predicted as not sweet
and sweet so
so which means the off the shelf uh it
doesn't work on my
on my own data all right so
then i retrain it uh so this is actually
the prediction you can tell i mean
they're really bad this got predicted
as sweet and this get predicted at the
suite
but the the label i did i actually
labeled it as
is not sweet and the it's extremely easy
to organize this type of data
i will have a read settle section tell
you how to organize this later i mean i
think maybe next friday three to four
during my office hour so and then you
just
run around you fine-tune it um
and i think this is the
yeah i just somehow got kicked out a lot
i think i already have this
yeah so this is the retrain re uh re
fine-tuned model uh after this
uh let's see after fine-tuning if
it predicts better yeah okay if you see
you just fine-tune
eight epochs after on my training data
and on this validation data it's
actually gonna be really really good
readout it's just
get everything predicted correctly and
there's actually no loss i mean
everything is predicted correct so no
sweet as sweet
so okay so this is all good and this
last code i hope
it i hope it works because i just
download some images from web
about ice cream uh
and this is nothing related to my my own
phone
because there's let me see if the
production is correct
oh actually no yeah
so if you're out of the domain i mean
this is all my own like
kind of food data right and then in my
test i just
grab uh crowd the images from the web
and this this image is
a ice cream i think this is predicted
as not sweet yeah
rum production and i think this is also
pretty s
no sweet all right so uh clearly
my training data doesn't cover nicely
with the ice cream distributions
uh because the sweetness in my data is
mostly looked like this
or like a pancake style and
uh i don't know i don't remember yeah i
don't think anything in my training data
is like a ice cream so this actually
tells you something right it can
fine-tune but it cannot go out of um
what your training data represent and i
will show you something very similar in
discovery data on thursday
all right so enough just to give you a
little bit of fun things too
so now we are going to come back to
the gesture regression and
uh there's quite some uh
theorems i actually want to cover
because this is a very
important basic module for deep learning
all right so enough talk about 2d
imaging data you are on the standard
and they're just so fun right there so
it can be
biomedical imaging data it can also be
maybe
fashion data it can be also some cooking
data like i
had or maybe a waste i mean in this
if you go to the kaggle image search uh
this this portal there are a lot of
imaging based applications i mean
more than you can imagine so which means
it's really really a powerful type of
data
all right so uh now let's
see one you uh
want to do a classification um
so so let's just look at this
nurse shell first thing is i mean
how do you decide how to classify and
second is how do you represent
and then later is how do you score when
you're why
it's a discrete variable uh and then
how do you optimize and what are the
parameters
all right so let's keep being the
question in mind is
when you have binary classification uh
in this case
logistic regression is for binary
classification
it's not formatted class classification
there are other
you need to have other revisions
to go to to make it possible for my
t-class classification
so this session is about binary
classification
all right so what we're going to cover
is how do you classify
and then is how do you represent
the function f and then how do you score
it
okay so before i talk about
logistic regression classification i
want to talk to you
a more general class called base
classifier
and this class of classifiers
they're using the rules called map rules
to perform classification
based on a probabilistic formulation
all right so the most important is
related to the last section we try to
review the maximal likelihood estimation
right
so in there i told you uh we
treat each feature variables and class
variables
as a random as all as a random variable
so for example a y so in because now i i
started
so for one talk about base classifier
just to a little bit distinguished
uh distinguishing to the regression let
me just use the variable as
c sorry as c variable
all right so c variable and it's a
discrete
random variable there may be c1 on 2cl
number of possible class and um
that's a discrete random variable
so why you have a discrete random
variable
and um as to model the output
and how what is the appropriate loss
function
uh to uh to
for that evaluate the cause or loss of
classification
so this is actually pretty intuitive
right we always just
um if it's predicted that specific class
correctly
we think um there's no loss
if it doesn't predict that specific
class correctly
we think there's one loss so which means
what i mean this is
actually called zero one loss uh
formulation
and then essentially uh the confusion
matrix
so if you treat um yeah sorry let me try
to
write so so assuming like
you just try to write class c
one c c2 and 2cl
and then c1 c2
onto cl so this is
the y predicted so i think
the y dimensional is y predicted
and this dimension that x is
y truth y truth
then if c one equals to c uh
it if it's y predictor is say one and y
chooses c one
the cost is zero and the same thing for
all the diagonal terms and then for at
whatever off diagonal you're going to
have a cost
1. so if c1
true c1 got predicted as c2 c3 or
whatever cl
they are all going to contribute a loss
of 1 or cos of 1. so that's called
one loss um so
um yeah can you optimize this type of
loss
actually it's very difficult and this
loss
is you cannot calculate gradient
of this type of loss the zero one loss
so people just try to
make uh serious and people try to
instead of using this directly calculate
the zero one list loss
type of formulation people trying to
make use
of a probability formulation and then to
estimate and to model the loss
is a surrogate of the zero one loss mean
okay so uh what is it so
essentially i try to model the
conditional probability of pc variable
given the x
so if you still remember we are saying
when you talk about
our probability their journal
probability
marginal probability and in this case is
this called
this is conditional probability so
and i try to model so essentially this
is my representation
i'm trying to model pc condition on x
input as my f function
this is actually what my f really is my
model
is this this is my model f x
um then after i have this
um how do i decide so it's actually very
intuitive right so i
can so because c it's discrete random
variable
and then you actually can for
you can have i mean because see it's a
discrete right then
essentially you can write out c1 given x
uh c2 given x
onto cl given x
yeah i should actually write smaller
because that's a
initiation right and then whoever
it's ag max is whoever is the max
among all this probability
should be the class as the predicted
c star all right so this is very
intuitive and this
actually have a name uh in the
literature called
maximal a posterior probability rules
um so this is exactly what i
wrote like uh what i have this detailed
description in the previous page right
so here you just
take a max and you get the best
c star whoever give me the max means
that
if that's the predicted class this is
called
c-map and the classifier using this rule
is called base classifiers
they all follow this uh so in fact
you actually can prove let me see if i
have that extra slides
um yeah i will talk about why this makes
sense but
intuitively intuitively this makes a lot
of sense right i mean
because probability wise that's the most
probable class
so that's why i use that class as the
predictor class
uh so there's a one thing you actually
need to
consider there be very careful is
you need to be to guarantee this
p c equal to c j j from one to l
and summation equals needs to guarantee
to be one right because they are the
probability so be
be careful about this and this
classifier using this type of map rules
are called base classifier
this actually has a lot of relationship
to
what we try to explain you with the
expected prediction areas
so same thing here so in the expected
prediction areas
we consider all of the possible
population
x and y and we try to take
integral of a loss function and then
times
that probability of consider both x
and y and then um
in there uh in when we talk about
uh ep like bias variance tradeoff
we only cover the one type of loss is
this type of loss
y minus f x and take a square and
in fact there are many kind of laws you
can use in the epe
so on the whole population i try to
minimize this error to loss
if that's your goal the best predictor
at that specific act this is called the
pointwise estimation so this is totally
extra
so if you can understand this is great
if you you cannot don't
don't worry about it so
on the whole population if you try to
minimize r2
the best predictor is in fact is this
predictor
is the mean predictor at that specific
point
so if you think about it what type of
uh method after follow this we already
covered right
regression regression models
it's exactly this kind of mean
estimation
and kenya's neighbor kenya's neighbor
for classification they're using their
neighbors
to approximate this
point wise and then using the mean of
the neighbor's production as their
prediction
and which exactly actually followed this
the mean estimator all right so
if your loss is zero one loss uh like
the loss i just showed you i mean um if
correct
no loss if not correct for whatever not
correct class it's a one
it's plus one cost there you can prove
the best predictor at that point is the
base classifier
using the map rules yeah so this is like
you know
it's more like uh my intuition really
grounded
by some um by theorems yeah that's the
reason why i
want to show you so if you are
interested i have the full proof factor
in extra slice
all right so now we know base classifier
is good
so whenever i want to make a
classification
i want to have a possible way
to model p c conditional x
if i have that and i can just directly
predict on every possible x
samples whoever give me the largest
among
all the classes is the predicted label
and this is a good prediction
uh so now the question is how do you get
this pc conditional x right
so if you again still remember basic
probability if you can directly
estimate from data you just directly
estimate from data
and it's the same thing applied from
here
if you directly estimate pc condition x
from data
and this group of classifier is called
discriminator classifier
if it's difficult to directly
estimate pc condition x and
i'm going to use base rules instead and
to
estimate pc condition x and this group
of classifier is called
generative classifiers and this
actually gives us a very clear
explanation
why um in the literature you can people
roughly group the classification method
into three groups
the first group is called discriminated
classifiers
you directly estimate pc condition x
from data second group is generator
classifiers
you actually in fact estimate
pc conditional x from uh a conditional
actually see it's the base rule so let
me
put base through using the base rule
instead all right so
majority of what we are going to learn
is belongs to discriminator classifiers
like a support vector machine uh
boosting tree decision tree and today we
are going to cover logistic regression
and the neural networks majority of the
deep learning is in the discriminative
regime and we are also going to cover
a generative classifiers uh the most
classic is the naive bayes classifier
and the third class is this instant
based classifier kenya's neighbor base
so which i already told you before so
essentially
kenya's based classifiers they're using
a different loss function and a
completely different formulation
they are not based classifier families
all right i hope this gives you a very
good overall views about
classifiers and how you think about them
and today uh and the whole actually the
whole section two
will uh focus on discriminative
classifier
which essentially is so this is my x
vector
i'm directly predicting i'm directly
modeling
p c condition x
and essentially and this is my model
right so my model is pc condition
x all right okay enough
of the intro so summarize base
classifier is a family of classifiers
and the most important is they're using
this
map rules to perform classification
and their parameters is just the
parameters you use
to model the pc condition on x that's
your model parameter
okay keep that question in mind
discriminator classifier the most
important is to
how to model pc conditional x
and the logistic regression almost the
simplest
discriminative classifier and the whole
point
is the pc condition x is
in this kind of form and i will explain
to you
it's actually a very very simple form
even though it looks complicated now
okay so first is about representation
so how do i represent pc conditional x
in this logistic regression so the
essence of logistic regression
is uh
my locker ratio log odds
so let's let's still keep in mind the
fundamental of discriminator classifiers
is pc condition x
right um so now
pc conditional x but um it's a
now now let me switch to a easier view
to introduce your logistic question
first because
you might kind of
question or have confused why this is
called a regression
even the whole reason is
so the first line is a linear regression
right so this is a classic
linear regression and logistic
regression
really is uh so the
kind of special function here this
of the y it's a linear regression form
and and this the left side essentially
if you uh remember probability a little
bit it just it's the log odds function
of p y conditional x so let me again
clarify something
so i told you pc conditional x is the
most important for discriminative
classifier right
so logistic regression is for binary
classification
so when you are target
it's just a binary y
so which means the y so now i'm
switching back to y because it's easier
in this case
so why
in fact so it's a y
variable y is a boolean variable
yes and no or you can write it as
one or zero or you can write it as head
and tail
right this is really how the binary
variable really is binary variable can
be represented
uh as the most classic distribution to
represent
binary binary variable
is bernoulli distribution or hand and
tail
yeah let me write a binary variable
that's easier
so i'm modeling my output y
as a binary variable
and then here so the upper the
denominator
is essentially p y equal to 1
and the denominator is p y equal to 0.
so i think i have a more clear in the
next
uh slides yes so essentially um
what this really is is um
log odds right it's the
log p yes
conditional x over p
no condition x
is a linear regression form that's
all logistic regression really is and we
normally cut this part as
a logit function and this is why this is
called the logistic regression
because the logit is a regression form
i hope this explains yeah
all right so this logic form um
can be converted into like i said right
any discriminative classifier or base
classifier
the most important is pc conditional
x-ray
and that's my fundamentals without it
i cannot make a prediction because i
need to do augments
of this i mean to give me a prediction
right so because this is a boolean
variable right
i mean if you still remember whenever we
talk about boolean rep variables we did
we do a short form in this time in this
case i just do a short form if
p yes i just write it at p y for p no
i write it as one minus p y so just to
show you whenever this is p
y conditional in largest regression the
full
meaning in fact is py
is the head or that's maybe even
more closer to the bernoulli view of
bernoulli
variable we p
hat and p tail
or p positive of negative or p
s or p no so here this is the p hat
p hat is in this form and this is
actually my probability representation
so remember that so i can also write
this as
p hat
of a bernoulli distribution is having
this
form all right so
i hope this is clear clear and i'm going
to show you a geometric
view of this so if we plot the logit
because it's a linear regression
function right so this one it's actually
x
is one d x is one d so that's why i can
plot it
uh so when x is one d i just got linear
regression
line and that's my logic function and
for
but i can also plot this p y kind of p
hat right p hat um
as x function so because i told you
before p
y equal to 1 condition x essentially
it's a p
hat if you think about this is the y
my output variable is a bernoulli
variable
and this is
this probability and x relationships and
this
is has a s shape so remember that
this is a s shape function
and like i have emphasized many times y
is modeled as bernoulli variables
p y equals one condition x is the head
part
p head and then the other side is a p
tail all right and the summation is one
right
so that's why you don't need to have two
variables represented
you only need one because the summation
is one all right
okay so let's now look at the logistic
regression from all different kinds of
possible views
the first view of logistic regression is
a logic
of that p y p that p hat is a linear
function
so this is exactly how we introduce here
right all right
logit of
p hat is registered it's a linear
regression function
this is the first view and the second
view is
uh we model um
our y as bernoulli distribution
and everything here is p hat right
all right so if you still remember
bernoulli distribution
let's look at the basic binomial
distribution we actually reviewed last
time
but only distribution is you flip a coin
and it represents like uh the
observation represents its head and
tail they're all in two outcomes and
this is a binary
random variable and there's only one
parameter
is the p hat and to define this binomial
distribution
so i only need one one parameter
is the p hat and
normally we think that p hat is constant
right so in the most basic bernoulli
we think that p hat is constant and we
can use maximal likelihood estimation if
you observed if you still remember the
mre we reviewed
we observed many heads and tail like
flips
assuming z z1 on to zn
and that really transferred maybe this
head head
tail whatever head and this using
mre
this can tell you the p head right
just by maximize the likelihood and this
is that parameter
all right all right let's see um
what's the differences of logistic
regression and the basic bernoulli
because both is just modeling the y
as a random variable right
so y is model as the bernoulli p
but differently in the basic bernoulli
distribution
that p is a constant by in logistic
regression
p is a function of x
so logistic regression is for binary
production right
we observe zero and one for the target
variables
but we can think about target variable
conceptually as
a probability that class one will occur
but no distribution the pro that p prop
p hat
parameter is having this form
p hat equals to p
y equals to 1 condition
x equals to that really weird form
exponential beta transposition
x exponential beta transposition
x plus one and you can tell
a basic bernoulli has assumed a constant
p
but logistic regression is a
parametrized
bernoulli what i meant parameters is
my p head is a function controlled by x
all right i hope this explains all right
so and then the third view uh okay let's
do the second view
so we can think about every position
here
is uh bernoulli and that
p head is generated
by this specific s-shape
function so maybe this is a little
difficult so this is the x
and that's the y so at this specific
location
i got a bernoulli distribution that
double node distribution
the p hat is this
and versus the p tail the p hat is p
y given x and which have this form for
that specifics
oh sorry all right so i hope this
gave you a good understanding of
the probability meaning of a
classification and
it's actually very important to
understand it almost all the deep
learning classifiers
are rely upon this type of reasoning
i'm modeling the y as some kind of
discrete random variables
and that discrete random variable
can be modeled by bernoulli or martinoli
or maybe some other
discrete random variable distribution
for whatever distribution just a
parameter
there's maybe multiple parameters that
parameter
in the basic case is normally constant
but in this
deep learning based case it's a
function controlled by the x
that's all it really is i mean
everything can be explained from this
this kind of angle all right so this is
the second view
the third view is also important it's
going to help us later to understand the
composition of deep learning models
all right so we can think about um
the um you know this
x and y is a squash function so
so we know this um i think i have that
here yes again so if you think about
this part
it i'm doing the one dimension now
alpha plus beta x i mean x
is from minus infinity to positive
infinity
and then which means what alpha plus
beta
x it's also in the range of minus
infinity to positive infinity but
after this e to this specific number
let's just see maybe this is
i can represent it as maybe h let me use
h to represent it
h is alpha plus beta x then
it's really the e h over
uh and you write a little slowly
so eh over
1 plus eh is what it really does right
it really does what it kind of
compressed
from the minus infinity to positive
infinity of this
linear regression's output range into a
range of
zero one and this is very important
so this is actually exactly the sigmoid
sigmoid layer in the deep learning
e h e to the h over
1 plus e to the h is the sigmoid
squashing layer all right
so this is the view three you can think
about logistic regression
is a composition of a linear regression
plus a sigmoid squashing yeah
all right so let me write it here so
let's just check
regression equals to
a a basic linear regression
plus a sigmoid so that's the third view
to think about a logistic regression
it's a composition of basic linear
regression plus sigmoid
compression all right so we have
actually the fourth field i know this is
so much
but it just looks just regression it's
so important
it has it's simple but it's the basic
module to many complex um
algorithms so we better understand it to
the extreme that i we can
okay so the last few it's also extremely
important
is i'm thinking i try to like explain to
you
uh from this kind of classification or
separation perspective
so all right so what do we mean
classification if we draw them
in the geometrical space so let's just
draw now i'm i'm in a two-dimensional
space now
so there's x1 there's x2
all right so i have a class one it's all
uh
positive and i have my class
zero so this is my class one and then i
have my class zero
it's all maybe circle
so this is my class 0 case and then the
others class plus case
so in the end classification
geometrically you can think about it's
just
finding something separate class right
and then
it's really interesting to understand
those points you cannot separate
at those points you you cannot make a
decision
build up your decision boundaries
and so let's see what do logistic
regression decision boundary look like
so let's think about what do decision
boundary mean
i already told you decision boundary
means i cannot make a decision
i just there's no way so in our binary
classification
case is so our mechanism to represent
classification
is p y equals one condition p hat
and p tail right
so what type of points i cannot make a
decision
feel free to type in the chat if you can
directly
tell me so we are using map
rules right map rule is to take a max
over the two
the case that i cannot make a decision
is
the max is the same as there's no way to
take a max
which means what p y equal to 1 equals
to
p y equal to 0. so those cases
are the boundary points
and just no decision
all right so one 1 equals to 1
the p hat equals to p tail
let's see this part what is this part
gives me
hat is on the nominator tail is on the
denominator
they equal to each other which means
this part
is one then log one is what
log one is zero which means
those points that i cannot make a
decision
follows this function which is alpha
plus beta transposition x equal to zero
which means what that's exactly
representing line right because
lines exactly follow this type of form
so in this case alpha plus beta x
equal to zero actually there's two x
beta here
so yeah let me just write that shooter
form it's easier maybe
maybe w transposition x
vector plus the b equals zero
and those the weights part there's also
buyers part
and this is just a re writing
and this exactly tells us logistic
regressions
decision boundaries is a linear line
and this is the reason why we call
logistic regression this type of model
when your decision boundary is a linear
line this
linear classification method
all right i hope this is clear
um yeah i i did all this derivation
um like it's just a repetition so
svn actually it's another really big
group of
linear classification algorithms
okay so this is the first module