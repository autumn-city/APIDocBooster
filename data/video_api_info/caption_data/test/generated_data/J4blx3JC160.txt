all right hello everyone um so let's get
let's get started off with introductory
workshop today which is introduction to
nlp with deep learning
with tensorflow so today we'll be
discussing what deep learning is and
what the uh and how to how tense
approach and syntax also works
all right um so just to introduce our
speakers so and to andrew's here i also
understand her speakers as well so
andrew was a great element student i had
seen her had seen her secondary soon
school and he's the co-director of
ignition hacks he's a deep learning
practitioner who's proficient in
tensorflow by keras and by dodge he was
placed 29th in the world on the kaggle
kidney segmentation challenge and he's
also started his own summer company
called shawbotix you guys can check out
shabbat x and you can also connect with
andrew on instagram
uh with this instagram link
all right here so moving on so hi guys
my name is gavin bhartia i'm a fourth
year company engineering student at ubc
currently i'm into currently i'm
interning at aws cic
and in the past i was an ml research
assistant at qcri which is the
research institute and in my free time i
also work as a mental health advocate um
you can go ahead and scan the qr code
from my link here from my linkedin and
you can also follow you can also go to
visit this website which is
going through what linkedin.nbc what
deep learning is what nlp is and what
and how the tensorflow for how the
tensorflow format works as well and
along with that i'll also be explaining
what transfer learning is and how the
transformers work as well and at the end
we have a very we have a fantastic model
built by me and andrew and then the demo
will be actually showing how ner works
or any ar or natural entity recognition
works as well
um so
looking at the agenda for today so today
first of all we'll be going through what
deep learning and neural networks are
and secondly we'll also be going through
what the how the tensorflow syntax works
and after that we'll be going through
transformers and transformers and how
hugging face works along with that we'll
also be discussing what pre-trained and
transfer learning is and finally we'll
be going through to collab notebook
which is which focuses on ner so this is
so it's going to be a really interesting
journey and i hope i really hope
i really hope that everyone can learn
something from it
and if you guys have any questions you
can always put it in the now you can
always go to this little chat and you
can put the comments there and i'll be
monitoring that and if you have any
other comments you can also use the
youtube and if you're watching on
youtube you can also use the youtube
comment section as well
all right so if you have everyone's
ready let's get started
so let's get started so let's talk about
what name whole neural networks and team
and what deep learning is with
tensorflow
so what the so how deeply before we
actually talk about what deep learning
is let's actually first look at the
interesting part of the applications of
deep learning so there are multiple
applications of deep learning and the
most and the most common ones are in
computer vision and natural language
processing and audio and audio and sound
manipulation
so firstly
firstly how it works for computer vision
is that we actually allow the computers
to see on which is basically we're using
image classification image segmentation
and object detection a lot of now a lot
of you know a lot of islam might have
actually seen those images with boxes
that boxes around the person and that
are being used to train a different
different kind of computer vision models
computer mission is actually one of the
most powerful
uses of deep learning
along with that today we'll be focusing
on nlp which is natural language
processing nlp personally is my forte
and i've worked a lot on a lot of
different on a lot of different nlp
models and done a lot of different nlp
research things as well
and nlp basically allow basically
converts language or text to computer to
machine language as well it basically
helps machine to understand and create
engine create and manipulate language so
basically like how we can actually make
how we can understand english or any
other different languages
so nfp allows the machines to also
understand these different languages and
actually perform
different ways of different tasks based
on them like a like good example the
class would be like text classification
or which is an example of central
analysis
and token token classification and so on
we also do have question answering so
you can ask you can give it a paragraph
and ask the question and then and the
model will generate an answer for it
and finally the topic that we'll also be
covering in our demo today which is
natural entity recognition i'll be going
through in more in detail what ner is in
becoming like in the coming slides but
for now you work for now it's really
important know that ner is also one of
the end also one of the most very
powerful np resources
and finally coming to audio data audio
and audio classification and sound event
detection
or action manipulating audio and sound
actually can take up a lot of resources
a lot of intense resources as well
and deep learning is quite resource
intensive in itself as well because all
these tasks like computer vision nlp and
audio usually require a large data set
to train a model as well and whenever we
do actually use more whenever we
actually do use a trained model as well
just predicting from these models can be
very important computer so it can be
very intensive to help from compute as
well
so it's quite so it's actually quite
important that we mean that we make sure
that we are that our text classification
and our audio and all of our models are
working working well
looking at this actually focusing on
what the artificial intelligence
landscape looks like so deep learning is
so if you actually just look at
look at the diagram here so deep
learning is actually a smaller part of
what machine learning is and in itself
machine learning is a small part what
artificial intelligence it is in itself
so we can as we can as we can see here
that there are three different layers
there's machine learning deep learning
and we also have artificial intelligence
so as soon as we can observe here these
are these are three different things so
as we can see so deep learning basic
deep learning builds on what machine
learning has been done and art and so on
and so on the relationship with large
film learning as well
so all of these are different ai mod the
algorithms that we've been using for
different different tasks that we can
actually fine-tune and we can retrain
our models for
and one really interesting thing that
we'll be covering today is unsupervised
learning so one of the most common
unsupervised learning methods is
clustering as we can see here so as we
can see here there are different
clusters of data
so all of these data have been clustered
according to a certain rule that we that
has been applied in this case
so all using using this rule here we can
actually see what the cluster data is
and the how data how like how clustering
works as well in unsupervised learning
there is no other there is no variety
but there is validation and of course
but there is no supervision from a
person who's actually monitoring the one
from these data sets
and finally coming to what is deep
learning deep learning is complex is
complexified and powerful machine
learning or basically when basically or
whenever whenever we have three or more
layers that can be called deep learning
so let's actually take a look at the
deep learning process and how deep
learning actually works
so deep learning is actually quite
interesting and i find deep learning
really interesting and i've worked a lot
with these deep learning algorithms and
it's always it's quite interesting to
understand how these things actually
work here
so how the how the deep learning process
works is that we actually first of all
we make a guess
and huh and actually evaluate like using
a high parameter model and we trying to
make a guess on what the prediction
could be
and secondly so we are using the using
the deep learning process once we have
first now that we have actually
evaluated it
now that we actually use the prediction
or we have actually made a guess using a
very high parameter model so we actually
first of all make the prediction
now secondly then we actually evaluate
that guess that how can i how how
correct that guess is using a cost
function
or is it like and use and also we can
also use a cost function to like to
reward how poor the gas is and we can
use a reward function to clear like how
good the guess is and it's quite some
and it's quite similar to how
reinforcement learning works as well and
so if you guys have actually worked with
the reinforcement learning it's quite
similar to that as well
and now now that we actually see so
since now that we actually have a cost
function reward function or basically
now we understand that how uh like how
good our guess was or how bad our guess
was we can use that data to actually
improve the model and so improving using
so there are two different ways to
improve the model here firstly we can
use a gradient descent and there are
multiple types of gradient descents that
can actually be used in this case here
secondly we also can use an optimizer
that will reduce the loss or increase
the award and so and so on we can
actually increase the law and so on we
can actually tune the model to make sure
that our deep learning model is working
correctly and our deep learning pipeline
is working as smoothly as possible
it's quite important that we actually
follow this process and make sure that
our model is performing accurately
and this deep learning models usually
end up having a higher accuracy level
than any other to any other machine
learning models and it is because of
this process that actually makes the
deep learning models more and more
accurate
and so let's actually look at
let's actually look at what basic
machine learning
what basic natural language processing
looks like so as you can see this is a
quite this is quite similar to what is
the water converter neural network looks
like
and this is just looking at an mlp and
like and it'll be classified we actually
so firstly whatever we do
so we actually so one of the major
starting points of any national language
processing task is the language part or
basically it's just text
so we actually so first thing that we
need to do so and again machines cannot
really understand text like the way we
do we have to break it down we have to
convert them convert it into embedding
so that we so that the machine can
actually understand what like what is
the text about and what what are we
talking about here
so we have so the first thing that we
need to do
is to take in the taking the input cell
and convert it to an embedding
and every new and every neuron here is
connected to a different hidden hidden
cell
as we can see we have just three inputs
and we have four out four on him and
four hidden layers here that connect to
two output scores so what so how it
actually works is so we actually take
each neuron suppose now that we've
converted all the text to embeddings and
then we're not and we connected to file
plus every neuron is connected to the
neuron before here
and every neuron also connected to a
hidden layer in this case so finally we
finally using a soft max function we
actually get an output score of what our
output scored on our narcissistic score
depending on which class it is so
looking at so if you actually look and
look at a multi-class classifier
using this using this algorithm we can
actually see that with class would it
belong to so maybe if you're actually
looking maybe let's say if you're
looking at sentiment analysis
and we have like three classes we have
like a positive new positive negative
and neutral we can actually use the
softmax function to predict an output
score of what class would this be what
class would the text belong to and
involve with what probability as well
and that is and that is how an mlp
classifier would work work would work
here
but in most of the deep learning cloud
deep learning once we actually use lstm
or which is long-term short-term memory
processing
or
which
so we review how it actually works is so
this is the this is a really good
example of how an lstm works
here we have actually have an lstm unit
which takes an input which is xt
and process it processes it one by one
so here we take in xd and then we you we
go through our lstm unit
and using it using it keeps a long term
and it also keeps a short-term memory
state so it can remember what the
previous words were it can also remember
what the
long-term like the long-term short
memory work as well so that actually
gives us an advantage here
and
then using that we can also so here we
have multiple hidden layers inside an
lstm cell
so using those different layers and we
and you and one lstm unit has attached
different lstm units as well which are
performing processing for x t minus one
and x d plus one as well
and so and so on we actually get these
different value and so using here and
now that we actually start processing it
here
using the different algorithms that we
have applied in this case we would
actually focus and we would get our
results in form of an ot which is
basically our output here
and so and so on we can and this is
basically how an ls theme cell would
work in this case
now that we actually got gone through
the theory let's actually move on and i
mean i've actually been going through
this very quickly because i'm sure
everyone's more interested in how how
can we actually implement tensorflow and
how can we actually use tensorflow in
this case here
so let's actually look into what the
tensorflow basics are and how can we
actually build an lstm model and one of
the really cool things is that we'll be
using our same lstm lstm model to be so
that we can train it and we can use this
here you can use our model here in this
case to
train it to use an ner system as well
so let's look at well let's actually go
ahead and look at what the tensorflow
model looks like
let's first actually use first let me
explain what tensorflow is so tensorflow
is a deep learning library which is
built by google and which is based in
python that makes new make creating
neural networks really simple
so now that we're actually going to look
at the tensorflow syntax it's really
easy and really simple to understand as
well
and we can also see that the tensorflow
so let's actually go ahead and look at
the tensorflow data processing part
and tensorflow is actually really really
simple to use here and it becomes really
easy to use and the data processing and
the data files like the data cleaning up
becomes really easy to use by tensorflow
and now that we just explaining the
syntax we can if you will also get a
chance to actually explain how the code
works as well in our actual in our
actual workshop demo
so just uh so everyone just stay tuned
that we'll get there as well
um so firstly as we are starting off for
data processing the first thing that we
actually use is tf data or which is
basically the main model for tensorflow
data processing
and and as in every data and as in every
nlp pipeline we also need to use a
tokenizer as i mentioned before we need
to and we need to actually convert from
text from text to actually learn
language the machine can also understand
so for that we need to use a tokenizer
and we'll see once we've actually done
using a tokenizer which basically
converts text to indices
and once you're actually done converting
the documents we can also use data so
there multiple ways how we can actually
get the data set so we can use the data
from tensor slices and which is which is
also the easiest way to create a data so
it's the easiest way to create a
tensorflow data set here
and now once we actually do have a
really large data set and we want to and
we want to apply like the same batch
processing thing so now let's say that
we want to clean up the data and we want
to apply the same function to every day
every row on the data set
and one so how can we do that so it's
quite actually so i mean for anyone
who's done programming and everyone
might be thinking we can use like large
for loop and stuff
but again that's very intensive and that
that can be very computer intensive as
well so what we can actually do is that
we can use dataset.map which maps a
function to every element of the data
set and that makes a job really easy and
adults and also uses sticky gm so we can
also monitor our progress and we can
also see how far like what how far we
are how far along are we
in the data set
and finally now that we are actually
applied and cleaned our data set we just
need to back we just need to batch our
data set and create different batches
out of it so that we are not just
feeding our data we just so that we're
not feeding a huge data set into our
model and it's going in it's going in
slowly slowly as well
so that we can also accommodate for any
of the changes that we want to do
and
so now that we actually completed the
data link the data preprocessing we will
actually want to create the different
neural network models so the neural
network models in tensorflow actually is
done using keras class is a tensorflow
extension which makes making neural
network models really simple
so let's look at how can we actually
build and build
so all of these ones are actually
different and different lstm lstm and
different lstm users
and we can actually let's go ahead and
look at the syntax in this case so the
first thing that we you actually use is
that as a tf tf or kerosene layers which
contains the which contains the mole of
in tensor fluorides there are multiple
different types of layers there there's
like there's oxygen the soft max the
different lstm1 is gru there are
different layers that we can actually
use in this case as well
and but here we'd specifically be
focusing on the kerosene sequential
neural network method in which case we
basically create a neural network which
goes sequentially or which basically
follows the same order of the same
sequence that we defined it to be
and using that exact list of layers is
basically how our model is being created
and this is a really good tool guys that
we are actually sharing so the first
thing that we actually just use is the
layers and then and just moving on from
the left we actually want to use the
sequential model here so using the
sequential model framework we'd actually
the first thing as i've said before is
that we have already tokenized the data
using using in our data processing part
now not from that text we already
receive the indexes from that but we
can't really process indices because
indices are can be difficult to
understand as well so we would want to
embed those in
indices and convert those indices into
vectors and now vectors are really
vectors are really easy to process as
we've seen
in the lstm cell here each x here each
of these are vectors and using an n in
our lstm cell we can easily process and
any of the lst any of the vectors that
we have provided here so using this
using these vectors we can actually
process it and take it through our lstm1
cell to get different outputs in the in
the same vector format
and so now and now that we actually see
so coming uh now that we have embedded
our layers we take it through our lstm1
so using lstm
into in keras is really simple and it's
just tf.tf which is tensorflow
tensorflow.js.s
and it's that easy to actually just go
go ahead and create an lst in there
now that you've actually created a ls
team there you would actually also want
to get like the final output scores and
to get the final output scores we can
just use a time distributed layer
which is which will be based on the same
which will be based on it to get our
stealth dense there and we can use that
layer actually to create an output score
that will allow that won't allow us to
actually join that will allow us to
output a final score as well
and find and finally so now that's how
and that's how just putting it all
together we can get and build an lst
model
and finally so now that we actually
created a model so here let's so here we
have actually used a segment a
sequential model and we've converted
from
from text to indices to vectors and then
we process the
and then we then we process the index
from the vectors using an lst model into
a to get a different vector and now
now using a time like a time distributed
lab you finally got the output scores as
well
and now that we have actually created
our model you would want to compile the
model so now that we have created like
so if you actually look at it like a
black box in this case they just for
example we can see that now we actually
assemble the box inside and now we will
wrap it up and announce that we wrap it
around using a using a loss and a cost
function so that we can actually run and
train our model as well
so now let's see so now actually we can
use model dot compile model doll compile
basically compiles the model with the
lost in a cause function and using an
optimization function but there are
multiple different types of log loss
cost and optimization functions that we
can implement here and all of them can
give us different outputs and different
feedbacks as well
and fi and finally here we and finally
we also do have model.fit the model dot
fit actually takes in so we've already
processed the data we have our data sets
ready we have a model ready and we've
already already compiled the model as
well so more what model dot fit is
basically the last stage in training any
of the models that we have here
what how model dot fit works is that
model of it actually allows us to train
the model with the
with the particular data set so one so
let's say we already have an x and a y
data set that has already been given we
have already split it into training and
test data sets and we can and all we
have to do is just call the model and
just call it called the data set here
and we and we are ready and that's all
we need to do to train a model in this
case
so it's quite simple and it's quite it's
quite easy to use and easy to use here
as well
and it's just and it's just because all
that the keras makes it that makes our
life really easy to actually train and
build these lstm models and lst models
as we'll be actually explaining
it in the in the ner workshop as well
all right um so before before i actually
move on to the second part of the
workshop um i just want to ask you are
there any questions or any comments that
anyone would like to share
all right um yes i'm actually i'm gonna
move on um since there are no questions
but if you guys have any questions you
can always put in them in the chat
and it can help you i can help you
through it
so let's now actually look at a really
interesting topic as well about what is
transfer learning and what are
transformers as well
and transfer learning is actually a
really is actually really cool and i
find it really impressive as well
because transfer learning has taken lnp
to a completely different completely
different level
because the models that actually use
transforms and transfer landings usually
have a very high accuracy compared to
the models that use lstm or basic lstm
cells
so let's actually look at what trump
before we actually go on and actually
talk about what transfer learning is i
think it's really important to explore
and understand the topic of attention
and so let's see we suppose we have
these four different entities
which is like a hugging face as a hard
eyes emoji a technology and a star
so now if you actually just want to
actually get the attention of
and let's so before we actually talk
about what is attention here so
attention is basically given
and each entity is represented by three
vectors here which is a query vector a
key vector and a value vector
so all of these as you can see here in
all of these vectors are actually
provided here
and v k so we have the query vector
which is q we have the key vector which
is k and we also have the value vector
which is v now all three of these ones
are actually provided into a
multi-header tension
so how
without trans so how attention actually
works is the attention actually allows
us so in an rnn cell or in an lstm cell
we basically can just look back one well
you can since we're just using lstm cell
we can actually look back
um we can actually look we can actually
look back and actually go ahead and
actually understand what how how the
sellers actually working but in this
case we are actually gonna focus on a
larger text and actually get more data
and give it more data as well
so that is why we actually can use it
and we can use attention here and
attention is actually given by the
formula which is attention of your
attention using the query vector key
vector in the val and the value vector
is given by the soft max of the query
vector and the transpose of the qr
transpose of the query of the key vector
divided by divided by a constant
multiplied by the value vector and as we
can as and as we can just visualize it
from here so we basically also we
basically apply them like matrix method
like matrix multiplication for q and k
then we scale it across scale it to a
constant and apply the soft max function
and then we finally finally multiply it
with the matrix matrix multiplication um
so why is action so why is actually why
do we care about retention
uh because attention has actually
changed and actually implemented the
transformers model
and
and one of the most popular one of the
most popular and most important papers
in nlp is paper which is my attention is
all you need um you can all you can find
so you can just google attention all you
need and and get all these citations and
stuff but attention is all you need has
basically revolutionized how the way we
actually interact and then the way we
actually use nlp nowadays
and attention on uni actually actually
presented this model which is
which is basically the modern
transformers model that we use nowadays
so in the in the model transformer model
we actually use and we actually have
inputs and outputs as well so
here we have different for different
kinds of severe problems we have
positional encodings we have multi-hair
retention model we have a feed forward
model and then we finally do have mia
and then we finally do have a mass
multi-head attention model as well
so all of these different cells are
actually are actually some rise from the
formula from just using attention at the
basic level
and as we can see each end so each
entity and the attention of each entity
would just be given by the same formula
just using the attention with your
attention which we've actually been
applying here we actually use a
multi-head attention model a cell here
so now that you've actually obtained the
so now that we actually obtain the
attention of one of these entities here
we have we want we would want to like
obtain the attention of all of these
ones on each of them so let's so if you
want to so here that we have calculated
the attention of phase on the
again on the three different ones what
if you are gonna calculate the attention
of heart eyes emoji on the other three
ones so then so to do that we can
actually so we actually calculate them
separately and get them and we get the
scale dot product attention here
which is basically the dot product of
attention of all of these ones
so and and the next so in the multi head
one we would
concat all of these different all these
different layers to create one linear
layer
take it through or take it through a
linear layer and finally get a multihead
retention output
which is basically being used in all of
these all these different cells here and
this small and this type of model has
actually been really really powerful and
can actually make a model really
powerful and can be used in multiple
different ways as well
so there are different concepts that
have actually been applied here so other
than attention as well we actually do
see that not just attention we also do
have
what we also do our positional encoding
and we also do have in like input
embeddings as well
and so finally what we actually and here
one thing that one thing that's really
interesting to note here is that we also
take an output embeddings so we do have
the inputs and we do have the outputs
which are basically the output
probabilities here so we take in the
output and the inputs as well but the
outputs are shifted right and so and so
basically the vector has been changed or
the output output probability in the
output vectors has been modified and has
been shifted right
and so on once we do that we take in the
output and the input embeddings
and we take it through additional like a
position encoding to make sure that all
of the models and all the model is
running smoothly
and our architecture is satisfied so
here since we have two different layers
here we can actually use these different
layers and just use the multi head
attention model here
and as you can see as i've explained
before using the scale dot product of
the attention here we can we can concat
that layer and get one again a
particular multi attention value here
and using the same value this is
basically how the transformers model
basically works this is the trans so the
transform models actually allow us to
use multi-hand retention and which
basically allows us to operate more
parallelly and get actually inject more
data into the model here as well
and because of that and since because of
this because of all these details um
transformers are actually very heavy and
can be actually can take up a lot of
resources as well but transformers
actually implement allow us to also use
parallelism without using transformers
like with using transforms we can
actually really convert and we can also
so this is a really simple example of
how translation would work
so let's see let's say we actually have
a text in french and then we convert and
then we take it through our encoder and
a decoder model
and the encoder model also uses or
encoder model would use other same same
similar transformer model as well and so
the decoder model and using a scene
using a simple encoder anti-code model
we can just implement the same model and
get an output which is i am assumed
and again i'm not really good with
french but uh i think so it's quite it's
quite simple to understand in this case
though
and the i think the biggest advantage
here is that we there's no data loss and
there's no data vanishing here as well
which actually allows us to use more and
more data and train our train train a
bigger model as well
and as i've said before all of these
models which are trans which are
transformers based on transfer learning
based usually end up having an accuracy
which is far greater than any other
model which is just you which is based
on lstm or any of the different kinds of
cells but and but one of the biggest
disadvantages i've said before as well
that they require that transformers
require a lot of memory as well and so
and transformers are typically are
pretty big models as well so a user
transformer like a bird or any other
like bird or gpd or any any of these
ones are usually
around 200 to 300 megabytes of data
which can be pretty intensive on a local
system
and one and to train any of these models
we also doing a gpu instance
it's quite hard to train a transformers
on a cpu and because they do and i think
that's also
and i think one of the biggest
advantages again coming back to it is
that transformers can actually take use
of gpu instances so that using a gpu
instance or a tpu instance can really
speed up how the transformer training
works and it can actually really super
degree superpower as well and make a
training go really fast as well
and so looking at what is transfer
learning so transfer learning is really
interesting here because transfer
learning is a machine learning method
where a model is developed for a task
one of which has been already been
developed is being used as a starting
point for a model for on a second task
so let's see we actually have i think
this is a really cool example but let's
see we actually have like a lego tower
and we are we already built it for we
already built it for a particular design
but now we actually want to reuse it and
build something on top of it or build
something similar like similar to it
so all we need to do is we need to train
it on a we need to add a
data set or basically more lego blocks
and we can just build on the same like
on the same lego base that we built
before
and that is what transfer learning is
basically the sound fun learning is
basically was a machine learning method
when a model developed for a task is
being reduced as a starting point for a
model on a second task
so if we look at an example or if we
actually look at a more detailed example
here so how traditional machine learning
works is that we have a focus task and
we use the data set we use a data set
to train our model to get and to get our
model to actually just do that
particular task
and that has been that is basically how
so maybe let's actually just look at if
we just are actually going to focus on a
sentiment analysis model here
and we can just actually look at the
learning system in this one in this case
and just looking at the running system
here so we're just going to be using
in the find the focus one here
now once you actually train the model
here
and we can we can just go ahead and just
use the learning you can just use go
ahead and just use it for prediction
here but in this case the model will not
be so accurate because we just train on
a very limited data set here
but if you actually look at what if we
move away and look at chance how
transfer learning works is that even we
have a huge data set and we've already
trained the data set on our model and
now and now just using a smaller task or
a target data set
and we can just use this drug we can
just fine tune our model using the
target data set to get even better
results so instead of having to train
our model every single time we can we
can also use and we can also just
continue and just training our model as
well
and that and that is how transfer
learning would work here
and let's let's actually look at why
transfer learning is really important in
nlp
and so for actually
how we actually use transfer learning in
nlp is that we focus and we learn on one
general task and then we transfer it to
a more specific task in this case
so let's actually look at so
initially when we have a very large data
set and datasets that are usually being
trained are usually like 100 100
gigabytes or even one terabyte as well
so these are really large error but
these are these are really large draw
models data sets as well
and finally we and you also developed a
model so like we have like word to wear
club bird gpd5 bar all of these are
really all these are really really big
models and they have been trained on the
large data set
so
so firstly what we need to do is that we
need to perform pre-training on these
models and what pre-training is is that
basically we just take a model which is
which is basically a base model and we
train it on a large data set
so now this training on a large data set
is a very it is an extremely
computationally intensive step it
usually requires a very high
very high gpu instance and it often
takes a really really long protein as
well
and that is partially because all of
these models like gpt per d5 usually
have usually have a very high very large
number of parameter
uh particularly so if you guys actually
go and look at gpd3 has almost 175
billion from billion parameters that it
actually uses and because because of
which gp3 is one of the most powerful ml
most powerful mn and nlp models at the
moment and it is really really intense
it is really really fast as well
but using these models out of the box
aren't enough actually for us so since
we were since just using these models is
not quite easy for everyone as well but
we actually you do want to use these
models for particular tasks that you
want to adopt them for
so let's say we actually just want to
use a little bit like a question
answering system where we provide them
where we provide the model of a
paragraph and we also give it a question
and we want to um we actually want to
get the answer from the paragraph
and now how can we do it so now let's
see that we have already built a model
and we already have a pre-drained model
that has been trained on a really large
data set
but but the catch here is that the model
has not been actually been trained for a
question answering data set this is just
being trained on a very generic data set
so it so like a pre-trained model would
not perform really well on us on a
specific task like question answering or
anything like that
but if you but we can actually use the
pre-trained model and so so now that we
take the pre-trained model we can and
now we now also let's say we also have a
smaller data set which is more focused
on a question answering data set
so using the question answering data set
we can just you know we can just take up
a pre-trained model and we can fine-tune
our model on the new data set
so just using the find user and find we
can find unit and we can adapt it to a
different data set and to get to get and
so we can also use so depending on what
data set we have we can create different
wire different tasks and we can use it
in different ways as well
this actually makes pre-drained models
really really powerful because we can
adapt it for different tasks and use it
in different ways as well
and this is and that is why we have a
lot of different tasks and we can create
new models very very quickly as well
and let's actually look at how model
training like how language model
pre-training looks looks like and how
can we actually do it as well so let's
say that we have like a small snippet of
text here
and which is just a really simple one or
which is basically my house that small
so there can be like different things
that can come in this case and let's
look at the second one as well
and which is i love my dash now these
are now these are different masks and
these are different dashes that we
actually want to predict the predict the
word for
in this case
so how prediction would actually work is
that we would take the probability of
this text and we would take the people
you take the key of the probability in
this text or we would also take the
probability of the text or the other
text and give and you actually combine
them to get one to get and predict the
rest and to do actual value here
the biggest advantage of actually using
language model retaining is that it does
not require human annotation for any of
these large scale tasks it usually
requires the human to annotate all of
these values and actually get the answer
as well
but since we are actually using
unsupervised the deep learning here we
don't really need to like uh it does not
really require any human annotation
and
on the only thing that we do need and
like for freedom is to have a large
large purpose of data set so whenever we
do have like a really large data set
that we can use and that can that we can
implement here
we can just go ahead and build a very
generic task in a very generic model
that we can that we can actually
directly performs do pretty well on any
task that we can use but is not good
enough and that needs to be fine-tuned
or pre-trained or fine-tuned for any
other other tasks that we would want to
implement
so basically a pre-trained model is like
a box that is good at everything but
isn't actually really good at anything
in particular but we can find unit using
a different data set to actually make it
better at a particular task as well
so let's look at
birth of basically bi-directional
transformers but the one of the most
powerful models grammar models out there
and and you can also also check out the
citations that have been added in the
notes in the presentation as well
and i'll also add them in the comments
if anyone's interested
and how bird actually works is that we
are here let's actually just look at an
example here so here's our example is
quite simple so our example my sheep
mars my sheep mask are good
so here we just been given a random
sentence here which is a trend which is
a training input
now that we've been given us so just
using the same model has the same the
same pipeline and the same thing that we
actually did in the year in the
tensorflow syntax as well we have to
take it through our input embeddings we
actually have a hidden states involved
in as well and then we finally have a
transform model and then we also do our
hidden output states and finally we
could to get the trans like use just
using the output embeddings we get our
answer but what the what is the major
difference here is that here we just
using a very generic transformers model
and as
i've already explained how the
transformers actually work
just using the same transformer model we
can also implement the similar design
that we already shown here
and one of the biggest advantages of
using bird is that we can adapt it to
different downstream stars as i've
already mentioned so here's some here we
just have the same word so this is the
same model that can be used for sentence
band classification using different data
sets so just using cell so if you have
two data sets and you want to actually
identify
what are the what are these ones
and so we can actually just focus on
these things we can just actually focus
on this and we can go ahead as well
and looking at so if you actually look
on look at single sentence
classification tasks as well so there
are different ways just using a
different data set we can actually find
you both to actually work as a single
single sentence classification tasker
similarly if we change our data set in
vm and you would want to implement the
question answering tasks we can use the
score data set and get an output of that
and finally if you just want to if you
just want to implement like the sentence
single sentence tag and task or which is
basically which is ner
and we can also we can use a different
we can use the data set here as well and
we can also on we can also get the
details here as well
so basically adoption of new tasks is
like building down new towers with lego
as a man as i mentioned before transfer
learning allows us to use a pre-trained
based model and build a new model out of
it as well
and that is and that is how you can
actually use adoption and that is how we
can actually build new models and build
a pre-trained model as well
so now let's actually look at the
transformer hugging face transformer
library
uh i'm actually just gonna go through
i'm actually gonna go through all these
details first and then and then we get a
chance to answer all the questions that
have come up
and just looking at the hugging face
transformers library hanging face is
actually has built a really really nice
interface to use all the different
transformers like all the different
transformers and all the different data
sets as well
so hugging face is actually being and
then the hunger face is the size of the
startup they have actually built with
all these different open source tools
their loan their slogan is that they're
democratizing nlp
they've built and they've built and
developed different tools for transfer
learning and nlp as well
and the one of the biggest advantages of
using car using the transformer library
is that it's just one line comma it's
just one line command code that allows
us to tame that allows us to use any
model that that any model that we would
need and all these models that have been
actually built by like research science
like research scientists have actually
taken time on like huge models but
hugging face actually allows us to use
these models in a really simple manner
um like transfer transformers also do
support by dodge tensorflow and jacks
though which are basically all the big
or all the big machine learning models
as well
and so trans using transfer using
transforms makes her life really easy to
actually implement nlp here as well
and looking at the looking focus at the
transformers library so transformer
library is a library which is dedicated
to support the transformer based
architecture and it also fell states the
distribution of pre-trained models
so
how actually how the transformer library
actually works is that it provides and
it provides us
multiple pre-trained models that we can
actually use in a centralized model hub
so as we can see here just by using the
transformers library we just have to
import our tokenizer and we import the
model as well
and all we need to do is that we need to
define the model name
and all and we can on we can also we
just need to define the model name and
just we can get the model down when we
can download the model in that case
and that is and that is basically how
the transformer library works and works
here just using the transformer library
is quite simple and it's also part of
the transformer it's also part of the
transformer library as well
and we do also also do have a
transformers pipeline which features
which uses the transformers library the
data sets the metrics and everything
that is
involved
now that we also now that we already
downloaded the data set the next thing
that like sorry but now that we've
already downloaded the model we also
would need to actually first tokenize
the data set as i've said before we were
also as i've said before we need to
convert the text to an empty text in
embedding so that the machine can
understand it and in using you usually
the tokens are actually tokenization
actually takes a while because it's a
really it's a really fast way it's
really long process but using the
hugging face tokens library it makes it
really easy and ultra fast as well and
we basically can encode 1gb in less than
20 seconds and it supports the vpp byte
level vpp word piece sentence piece and
all the different kinds of embeddings
and you can you can always go in and
check out hong kong on github the
hunting face transformers link as well
and finally looking at one of the most
impressive data centers library which is
on face data sets library so the honey
phase data sets library actually allows
us to actually use go beyond
organization and actually use the model
and use the model and find in the model
just using using any data set that you
would want to
so increase data sets library is very is
extremely lightweight and
it just downloads the data set that we
would want to use here
and as you can see here downloading the
data set is quite simple it's as simple
as just as simple as just calling the
data set and then just load just using
function load data set and with the data
set name
and that's it and that that allows us
and that completely downloads the data
cell like the data set onto a local
system one of the biggest advantages of
use actually using the datasets library
is that it also caches the dataset onto
your local system which makes it really
easy to actually use the data set again
and again
and so finally looking at the final full
hugging phase pipeline we firstly we
have the data set
which is basically the data and now then
we take it through a tokenizer and we
complete the tokenization process
and then we also take and then we also
have a prediction model when we actually
where we do the prediction and then we
using using the model which has been
downloaded from transformers
and then we can finally evaluate our
prediction using the different metrics
that are also available in the datasets
library
one of the easiest ways of using any
using a downstream task on a pipeline
would just be to use a pipeline function
here so here let's see that we already
so here we go and the pipeline function
also comes with the transformers library
so here we just want to use a downstream
task which is summarization
so so we just want to summarize about a
really smart really short sentence which
is an apple a day keeps the doctor away
and we just want like a fire and we
would want to specify the lens as well
so just using the entire so this would
actually just call the entire pipeline
so this would take the data set tokenize
it download the model and then actually
take the model and evaluate it as well
so it's quite easy it's quite easy and
it's quite intuitive to use these models
these models and these libraries as well
and i think and as i mentioned before we
also will be going before going into and
talk about what ner is as well so with i
think it's really important to like
actually talk about what ner is and how
how india works before we get into the
demo
and before i let my like my workshop
cody take over the demo
so what ner is that an anya is actually
called entity and entity identification
or entity extraction is a nlp technique
that allow that can automatically
identify named entities in the text and
classify them in treating three
different categories so here as we can
see that we have a sentence which is
officer diva founder adam newman listed
the manhattan penthouse for 37.5 million
here we can identify that we work as an
organization i don't know when as a
person manhattan is a location and we
also do have a momentary value so this
is basically how ner works and as we see
another in our different demo in our
demos we can also we'll be using in er
and we'll also be using we'll be using
lst models and we're also using
transformers to build two different ner
models and then we can evaluate them on
our different metrics as well
all right um so i'm going to let michael
mcco work shortly and you take over for
the for building our ner system with
tensorflow hanging face and transformers
yep so just before we dive right into
the demo let's go ahead and answer a few
of the questions that you guys have
posted inside the chat so the first one
was what is an lstm
so when you're going with introducing
kind of the idea of an lstm you have to
understand that humans like to build
deep learning systems based on how we
act as let's just say a human and so
how the lstm was created was really that
humans saw that we like to read from
left to right and basically they took
that idea and ran with the wind so
how an lstm basically works is that just
like how a human might read the sentence
let's just say
basic natural language processing
you might first read the word basic and
knowing that context you would then read
natural language and processing and
therefore you'd understand the entire
sentence and lstm works kind of like
that by taking in let's just say first
the word basic as x t minus one
processing it and then storing that
inside of some sort of kind of like
history and so by doing that you can
keep kind of some of the ideas of
context as it moves on to the next words
including natural language and
processing effectively allowing it to
slowly read in sentences just like how a
human would left to right
now
we've kind of modified that classic idea
of how humans operate with reading with
something called the bi-directional lcm
and the idea is is that
you kind of need context that is both
left to right and right to left in order
to read a sentence effectively just like
how a human might read in small windows
and so how we've achieved that with
bi-directional lstms is that we have one
lscm just running left to right so for
instance they'd first read the word
basic natural language processing and
then it would make a decision
versus a second one that would also grab
some information from right to left and
so would first read the word processing
and go backwards and so by combining
those two we can kind of gain some
intuition from both sides of context so
the idea behind the lstm or it's called
long short-term memory is that we're
able to process in words sort of like
how a human would actually go about
reading them reading them left to right
and storing in kind of the values as
you're going along
does that make sense
all right
uh
the next question was the idea of
what was generalized attention
and kind of what was the math behind it
so
similar to the lstm attention was based
on the idea that humans would focus on
certain words and focus on and put more
weight on certain words compared to
others when they're reading a sentence
for instance
um let's just say let's take the title
let's begin with generalized attention
when you go about reading this sentence
you would probably focus on a few key
words including generalized attention
and maybe the word begin other words
that are kind of not as important like
let's and with you might kind of ignore
you might hide and so we kind of use
these three vectors that might have
caused a little bit of confusion they're
called the query key and value vector
and so they allow us to basically
compute what is called the tension
allowing us to focus on certain words
over the other so the simplest way to
explain these three vectors and what
they really mean is that the query
vector is kind of what are we looking up
and so
now we kind of give the meaning behind
the query key and value vectors based on
their names but in reality we're kind of
training the model hoping that you'll
learn these things so the query vector
was intended to kind of look up certain
things that the model would need the key
value would kind of match a query to a
key and then the value vector would give
it some information along with it and so
by leveraging those three kinds of
aspects of informational retrieval we
can focus on certain words let's just
say the word begin generalize and
attention and allow our model to focus
on those keywords compared to reading in
everything and keeping in everything as
an lstm would
and so that has allowed us to get
higher than lstm performance by forcing
our models to kind of focus on certain
words that have a lot more weight and
when it comes to classification
leveraging those specific words to make
it decision
i'm hoping that makes sense
okay
so in terms of are we implementing
anything with code yes for sure so once
we get done explaining uh the rest of
these questions i will dive right into
our demo which is a google collab
notebook on building lstms and
transformers using tensorflow so that'll
be at the end
and so the final question is
uh how can i use bert for information
retrieval and so by leveraging kind of
this is called kind of like um
extracting features so if you notice our
burt model is really just a bunch of
layers of these kinds of transformer
blocks and attention and so as we slowly
refine down let's just say from our raw
words and we stack on more and more
layers we can extract better and better
features from the words and so if we can
only keep the most salient of features
or the most important of features using
our burp model we can kind of train our
model to focus on
only the most important details for
let's just say classification or natural
language or natural entity recognition
or stuff like that or i think it's sorry
it's named entity recognition but
basically just pre-training on a task
and only grabbing out the base layers
because those will learn general
information so just like how gagon was
explaining a bit about transfer learning
and pre-training we can kind of use
for extracting out features and grabbing
out more information by leveraging the
fact that it will focus on the most
important words for let's just say
classification and that will kind of
slowly refine down the information
as it's getting processed down
so by leveraging our burp model we can
extract the most important of details
allowing us to learn more about our data
and ultimately grab the most salient
features using our model by pre-training
it
does that make sense
all right perfect so let me go ahead and
change the screen and then we can dive
right into the demo
so our rough demo will look like uh
we'll first train an lstm using
tensorflow just to demonstrate kind of
like the most basics of syntax and get a
rough introduction into kind of raw
tensorflow and then we'll dive right
into kind of processing everything with
hugging face transformers and really
leveraging that kind of powerful
pre-training
so for everybody who needs it
um the link i will post inside the hop
in shot unless it's already there
oh it's already there okay perfect so
everybody has access to this notebook
and uh this is called a jupiter
environment so for anybody who is
unaware uh jupiter is a python basically
notebook allowing us to run code cells
rather than your classic script and so
the reason why we're leveraging collab
today is because google collab allows us
to gain access to let's just say free
gpus and free resources in general so
i'm sure everybody at this point has
heard of like gpus and cpus basically
gpus are canonically used for gaming but
they're also very very important for
deep learning because they can process
kind of rapid let's just say computation
really really quickly so it allows us to
train our models a lot faster so if you
need to activate a gpu you would click
on this ram button and you can simply
press change runtime type
and go ahead and click gpu compared to
none and after you've done that you will
gain access to a new gpu runtime
now in terms of processing all of this
data in i've already done a little bit
of it for you because it takes a little
bit of time as you can see it took two
minutes but basically i've extracted the
data from a google drive link and i've
loaded in some basic tensorflow like
just some basic data science imports so
our first major code cell is right here
this is how we go ahead and actually
load in tensorflow and some other handy
data science imports that we're going to
need in basically every data science
process project so first things first
you want to import tensorflow so import
tensorflow is tf and import keras and
we're going to be grabbing tensorflow's
version
now in terms of just some classic data
science imports you've probably seen
this in just your work with machine
learning in the past but we'll be
importing uh numpy and pandas just to
kind of pre-process some of our data and
a really really handy library called
nltk or the natural language toolkit
which allows us to process in text data
really really easily
and then we'll dive into scikit-learn so
we'll be using just basically the
splitting functions of scikit-learn in
order to split up our data into train
and test just to prevent a bit of
overfitting and finally just so we get
some progress bar and some graphs we are
going to be uh importing tqdm which
allows us to get process bars or like uh
progress bars so that it doesn't look
like it's eternity so you can see a
progress bar right here
and um
graphs so that's just with classic
matplotlib
all right
so first things i was doing is i just
simply grabbed this from the google
drive i loaded in the basic data and
copied it to the main data path so
you'll see inside of um inside of the
google co-lab kind of file storage
you'll see archive.zip which has all of
the files that we need and specifically
ner dataset reference
then i load it in using pandas so
pandas.readcsv
and let's go ahead and actually
visualize the csv file that we got
so
effectively we've grabbed this data from
a free public kaggle data set so it's
completely publicly accessible
and
basically every single sentence you'll
notice that every word is annotated with
a part of speech so if you've taken a
foreign language class typically this is
one of the first line first like lessons
that they teach you is like what type of
word is each of the words inside of a
sentence we're going to be training a
machine learning model to do this for us
so you'll notice that each of this each
of the words inside of each sentence
let's just say thousands of
demonstrators have marched it will have
a different part of speech for each of
them
so
just with some basic kind of a python
algorithm in order to load in all the
data what i've done here is i've
essentially just grabbed out whatever
the ground truth was which was the part
of speech tag that we needed
so in total i think there is 32
and i've also extracted out what the
specific word was too so this is kind of
like the x to y
and whatever sentence number that we
specifically needed so through
processing it just making sure that uh
every start of the sentence which is
annotated with uh if you have a notice
inside of the csv file which is
annotated with a new kind of like
sentence one tag
inside of the sentence number field
um essentially that'll tell you when a
new sentence needs to be made and i've
just split it up
by words so after two minutes of
processing each of these examples
you can find them all inside of um
inside of the variables i've created in
this case so i'm going to load them all
in and then we're going to take this
kind of rough data and we're going to
process it down slowly using these data
science libraries
okay
so right away uh before even doing
anything the first thing you want to
make sure you do is you split the data
just make sure that you're not
overfitting so you have to make sure
that you're splitting the data in this
case we're just going to do to keep it
simple a 90 10 split between train and
test
so by using scikit-learn's shuffle split
function we're going to basically just
split it out 90 to 10 and we're going to
be just grabbing out train x and train y
then
in terms of tokenizing our data which
involves taking our raw words and
processing into numbers as machines love
numbers compared to like just working
with basic text
uh we're going to be using a tokenizer
in this case
so what a tokenizer is
is um
essentially it is
a
function inside of tensorflow or
specific object that they give you that
allows you to take in
um in this case
raw text and process it into numbers and
for anybody who's wondering in terms of
archive.zip that is loaded on my google
drive right now but it is all publicly
accessible and i will post the kaggle
dataset link to where you can download
it the reason is just because to reduce
the like kind of like pain of dealing
with kaggle apis live during a workshop
i didn't want to load in all the cattle
details right away like just live during
the workshop but i'll send the link uh
after
the end of my portion uh to where you
can access all this data publicly so
it's all publicly accessible
um
then after using in this case our
tokenizer we're going to fit it on our
train x
and go ahead and process in each of
these words so you'll notice that word
index is basically a map from every
single word to a given index and we're
going to tokenize each of these words
one by one removing all punctuation to
make it more clean and any word that
doesn't exist inside of our current
vocabulary which is limited to 35 000
words just to make it a little bit
easier on the computer is going to be
given just a rough um out of vocabulary
token which basically just says
um hey this word was too unique we're
not going to um
in this case this word was way too
unique we're not going to go ahead and
use this for now we're going to just
replace it with an outer vocabulary
token
okay so after processing in each of
these words
uh the progress versus fill and we'll
have all of our tokenized sequences and
so if you want to take a look at them
you will see trained tokenized sentences
and you'll see each of these sentences
have been transformed from a series of
words to just their respective numbers
and you can actually go ahead and decode
them
let's just say i grab one sentence i can
go ahead and actually decode what the
original result was so i can just say
tokenizer dot decode in this case
and i can go ahead and decode whatever i
had before so
let me just double check what it was
called but basically you can just
essentially just decode whatever existed
before let me just double check the
exact function name and i'll get back to
you on that
so for anybody who didn't know you can
look at all the different methods that
exist
uh by using
in this case um you can leverage like uh
dir to figure out whatever methods exist
and so
in our case what i've done is i've
grabbed all the word indexes so i can
actually go ahead and decode these one
by one it might take a little bit more
code you'd have to reverse the
dictionary and look up each of these
words one by one but um i'll do that at
the end basically just showing off what
the raw data was but effectively what
we've taken is we've grabbed whatever
our data was so train x
so this is just a series of words right
and we've turned that all into a mapping
of numbers something that's a little bit
more suited for the computer
uh we're doing the exact same thing but
for the y values so in this case we have
all these different parts of speech
right we have maybe 32 different kinds
of tags we're going to do the exact same
thing just because again computers
really really hate working with text
they'd rather work with numbers and so
we can translate that using a tokenizer
so we're going to go ahead and grab
something called the y tokenizer and
we're just going to fit it on all of our
train data to make sure that we can just
map every single part of speech token to
a given index so we're going to process
it in the exact same way so you'll
notice the code is literally the same
so finally we're just going to remove
any inconsistent data to make sure that
everything is processed perfectly and
we'll have about a million examples at
the end to work with
so now that we have everything basically
just standardized everything is all
inconsistent examples are removed we're
going to just finally just say
we're going to pad it all so we're going
to make sure that it is all a uniform
length because computers don't like
working with variable size lengths just
because it's harder to run all at once
obviously running 32 examples at once is
a lot faster than just pushing one
example at a time and so by padding all
our sequences or truncating making sure
that every single sentence is 30 length
max we're going to just train this model
off faster
so we can just leverage
tf.cares.preprocessing.sequence.pad
sequence in order to make sure that
everything is of the same length
so now
let's go on ahead and train our lstm
so just like how gagan was explaining
before we can go ahead and actually
create an lstm or we can create an
entire tensorflow model using just a
list and this is by leveraging our keras
that's a dot sequential module and so we
can just basically from this python list
i can isolate it a bit
is warning me that i'm not using my gpu
because i haven't started training yet
but um basically just from this python
list of layers i can actually create an
entire neural network
that is customized to my liking
so in particular what i've done is i've
used an embedding layer which says um
given our indices let's just say the
numbers like maybe 13 24 etc etc i'm
going to grab out a vector that
represents its meanings and so what an
embedding layer does is rather than
forcing the computer to memorize 35 000
different words but just based on one
number which is hard for any human and
it's also hard for computers we're going
to say that each of these kind of
numbers like let's just say the number
is zero gets turned into a vector of
length x
and it points in some direction so if
you've taken vectors inside of high
school uh it will point in some
direction with some magnitude and
direction and so
we are going to do the exact same thing
but for every single word
and words that point in the similar
direction with a similar magnitude and
direction
will be
similar in meaning meaning that let's
just say i have an arrow and it's
pointing to the top left
that might represent the num the the
word dog and the number or i guess the
word canine would be pointing in a very
very similar direction because they're
closely related while a word like chair
might be in a complete opposite
direction and so by doing that we can
represent meaning inside of the words by
a value of numbers compared to just
saying hey computer memorize 35 000
different words have fun
so in our case i'm going to say that
each of our vectors are going to be 64
dimensions just to give it a little bit
of leeway
so every single word can be mapped into
the 64 dimensional vector and this is
thrown into two l two lstm models just
going back to what i was talking about
before with the bidirectional one where
it can process both left to right and
left and right to left
and we're just going to say that in
every single layer we're going to grab
an lstm we're going to process it with
32
so 32 dimensions and we're going to
return to sequences meaning that rather
than collapsing everything into one
vector at the end which means that we
can no longer run another lstml and we
only get one output for the entire
sentence which doesn't exactly make
sense for named entity recognition
we're going to return every single word
back just to make sure that for every
single word we can get a classification
result we're going to do this twice just
to give the model a bit more power kind
of just stacking the layers
kind of the premise of deep learning by
stacking layers together
and then we finally say
keras.layers.dense which basically just
takes every single one of our outputs
and processes in them one uh basically
just saying this word gets processed
with a dense layer using the time
distributed layer so gigon explained
that a little bit before but a time
distributed just says for every single
word run the same operation on it to get
a classification result on every word so
every word will now have
what part of speech does it represent
so
uh one really powerful thing is that
after you've built your model and you've
decided that you're happy with it you
can actually recall something called
model.summary which basically just says
okay
uh given our
current model i want a summary of every
single layer so my embedding layer my
bidirectional layer
and in this case it would be two
bi-directional layers stacked on top of
each other and a dense layer at the end
to say this is our classification
results and in the end we have 1.7
million parameters so that sounds like a
lot but i'm uh almost 100 sure that
gigan's transformer model will be a lot
larger so this is kind of just
scratching the surface with deep
learning 1.7 million parameters you can
definitely go a lot deeper a lot larger
with parameters because deep learning is
all based on those deep layers with
millions or billions of parameters
so pretty simple model uh just an
embedding layer with a bunch of
different lstm layers stacked on top
and then finally we're going to use
model.compile which allows us to
basically just say um
uh basically the idea is is that um
[Music]
we're going to take in this case
model.compile which just says uh given
our model we're going to give it an
optimizer in this case atom which is
just a classic optimizer that we're
using
and
uh we're going to um
sorry i see a few questions in the shot
okay
uh yes so i will send you all of the um
okay yes so in terms of all of this code
that i was talking about before it needs
an author uh authorization code and all
that like archive.zip stuff um after i'm
done with all this um
while the model's training i will go
ahead and send you guys all of the data
that i use so i stored it on google
drive which is why there's an
authentication step um you guys can
download it directly from kaggle and
upload it just via colabs interface the
reason why i didn't want to do that is
just the risk of technical errors google
drive is a lot more reliable than let's
just say
kaggle's api in case i went down or
something so i just use google drive but
i will send you guys the kaggle link
which is a popular data science platform
and it has the data set so
i'll send you guys that publicly
accessible link and you guys can just
press download zip okay
um
is there any other questions
can you open up yes yes so i will send
you guys that in like maybe
five or six minutes uh i'm just going to
go ahead and explain the training and
then i'll send you guys all the
resources and i might even uh here i'll
also show you guys how you go about
downloading inside of cola inside of
kaggle just to make this a bit easier
okay so
um
next i'm going to go ahead and compile
this with an optimizer so we're going to
explain that a little bit before but
optimizers is how we go about actually
training the model how we go about like
improving it and so in this case we're
going to grab
let's just say
um our bad guess and we're going to
slowly improve it and adam's just the
classic one that we always use so if
you're worried about which optimizer to
use adam's pretty much the safe bet
but obviously there might be a bit
better if you're tuning in a bit more
but just as a safe bet just start with
atom and then we're going to be using as
our loss function it's called
categorical cross entropy which
basically just means
um
uh given our distribution let's just say
there's 32 different kinds of parts of
speech tags that we're going to be using
uh for every single one we want to push
the probability
like the output layer the probability
that a given like the answer is the
highest probability of our model output
so if it's lower we're going to punish
the model more and adam is going to do
its absolute best to make sure that that
is going the correct word is going to be
pushed to 100 while the rest are going
to push down to zero so kind of just
slowly improving on the actual like
confidence of the model and we punish it
based on how unconfident it is in the
given word and finally we just say as a
metric we say accuracy or ack basically
just saying um
if we just take pure accuracy which
isn't actually the best metric you might
want to use something like f1 or f beta
or something like that just any other
score but just for simplicity we'll use
accuracy um
how spot on is the model when i just
rout when i just basically round
everything up and down
so after compiling it you'll notice that
everything is now ready uh it won't
actually give you any messages or
anything and then we can go ahead and
actually form our tf data sets
so
basically in order to create this data
set the easiest way is from the like the
numpy arrays that we already have right
the numpy arrays that i created up above
so in this case it would be padded train
x uh you'll notice that they're all
numpy arrays you can actually create a
tensorflow data set uh gigan touched on
this before but um
you can basically create a
tensorflow data set from these tensor
slices and so by using tf.data.dataset
from tensorslices we can go ahead and
process all this data in
uh just using that module and so it
allows us to create a tf data set with
really no struggle whatsoever
and so we're going to do the same with
train x train y test x and test y
and then we're going to just zip them
together meaning that for every x
there's going to be a y we're going to
try to find out that relationship so
it's basically just we're training a
very very overly complex function to map
from x to y so we can leverage that kind
of function in the end for our
downstream tasks so for instance if i
want to deploy this model for my
hackathon project i would just use my
blackbox function from it's basically my
deep learning model you want to treat it
like a function where there's just
basically a black box i give my x i get
a y that's kind of the idea so i want to
map every single one of my x values to a
given y for me to learn
and some of this is just to make it
faster um this code down here is really
just to prefetch some stuff it's
basically just saying
shuffle the train data set uh give it a
batch size of 32 so we can run 32
examples all at once instead of running
it one by one
um and then we're going to just prefetch
so while one is processing i'm already
going to grab the next one just out of
the buffer just to make sure it's as
fast as possible and so that's one of
the really benefits of tensorflow is
that there is that kind of ability to
customize and make it a lot faster so pi
torch might uh pi torch is around the
same speed but it's a little bit slower
just because it doesn't have those
nitty-gritty kind of optimization
details that tensorflow might provide uh
natively
and so finally after running this and we
have our tensorflow data sets we can
actually go ahead and take a look at
them using train data set and test data
set and it actually just gives you all
the information it says okay every
single one of my x's are going to be a
list of 30 words and same for the y
because every single one of them is
going to be mapped to every word inside
that sentence is going to be mapped to a
part of speech token and both of them
are integers because um they're just
tokens for now
and so you'll see that test and train
are both the same
and we're ready to actually go about
training our models so always just make
sure to do those sanity checks to check
that your data is not the problem uh you
don't you won't believe how many times
my projects are messed up just because
there's like a tiny mistake in the data
that i just didn't bother catching
because i'm like oh it's probably fine
uh just make sure to always do those
sanity checks before you go about
training because a lot of the time if
you have bad data it's the classic same
garbage and garbage out if you have bad
data your model will never be good
okay so let's go ahead and train this
and it'll actually take a bit of time so
in the meanwhile i will show you guys
the kaggle platform and where you can
specifically find it and then i'll hand
it over to gagan who can explain his um
uh gigan who will go ahead and explain
the tensorflow uh i guess it would be
the transformers portion so let me just
go ahead and transform uh just change
over to kaggle platform and you can go
ahead and download and i'll show you how
to download the dataset
um hey um so now we're actually going to
be focusing on how to use transformers
so all the all the models that call the
libraries that andrew was talking about
are some of them are actually already
been downloaded to cola but the mall but
the libraries that we'll be using here
like which is basically data sets
transformers and sql are not do not come
with cola because they are part they are
a bit heavy libraries so we'll just be
downloading them uh from
like so we'll just be using pip install
for them
and as you can see i've already done
i've already done the cell so that you
know it has already been downloaded
and the next thing that we need to do is
maybe to login into the hub and face cli
system
and just to do that we just need to do
hug and paste cli login and once we do
that we just will be prompted for a
username and a password so my username
is gargan3012
and i've already entered my password so
you guys can go ahead and you can run
the same you can run the same one to get
the same results as well
and so this is brill and so this is
pretty easy so now the one the major
differences in what andre was doing with
tensorflow
and what i'll be doing here is that i
will all i will be using the data set
that has already been using the data set
library so this this notebook so you can
actually reproduce it without having to
download the data set on your on your
own
so all you need to do is you just need
to like import the data set function and
then download the data set
from the new from just just by giving
the data set name so once we run this
function we can see that the data set is
going to be downloaded so it takes a
while
because it's it's not a very big data
set but it still takes a while it still
takes a while for the data set to get
downloaded
so as we can see the data set date has
been downloaded so it doesn't feature
strain validation and test data sets
and now also now let's actually go ahead
and start processing the data so here i
will before i'll be using the the
distilled word basically
based on case model here so if you would
want to actually check out the
distillate birthday base model you can
also check it out here it is one of the
most downloaded models on the hugging
face hub
and now as we can see so i'll be using
three i'll be using three bottles but
maybe to simplify one i will just be
using one importer and with a bad size
of 16
and then we also have and just using the
data set that we can also use the data
sets along that's since you've already
downloaded let's actually go ahead and
now let's tokenize the data set so
firstly we've gone with the tokenizer
which is just using an auto tokenizer
here since it's the since it's a simple
block model we're just using the same
order document auto tokenizer here
now once the tokenizer has been
downloaded so as we can see that all
that all the tokens files are being
downloaded here
and be sure and always be sure to use
like the gpu instance here because
otherwise your training might take way
too long
and now let's actually go in and start
tokenizing the data set so before we
actually go and we tokenize the entire
data set let me show you how let me show
you guys how does the tokenization
actually work
so for tokenization we actually see that
first thing of course we just have a
term we just have a data set here so i'm
just going to use the first one
first row of the data set and
see so uh long curve you actually see
the first one data set is just eu
rejects german called by a country lamb
again which is a normal sentence that we
have here but once we tokenize it it's
all in lowercase it's all and we also
added the different we also added the
different cell separations here as well
and now let's actually go ahead and
maybe just unlock the align the words
and make sure that everything is in
order as well
so now we just align so well now
everything that i've done here we need
to combine this into one function and
run datasets.now
as well um so now that we so now we're
just going to be going ahead and we just
need to run down we just need to
tokenize and align all of the tables
here
and so now now that we align and
optimize all the labels we just need to
go we just need to apply it and map it
to the entire data set
so now and before we actually use it
before you do that let's actually take a
look at what the different entities that
we have here are
so firstly we have o and obc means that
it has not been identified then we have
the person or which is the person has
been identified
a person has been identified the one
which is at the first end which is the
first token of a person
and i person is which is the second term
which is the second or the end part of
the auto person and similarly we have
organization we have location and we
have miscellaneous
so let's now let's map our entire data
set and we are let's apply the same
function to our entire data set
and we have done it for we have done it
for trained tests and validation data
sets as well
and now let's go ahead since since we
are using digital bird as our base so
now let's go ahead and fine tune the
model to get a better model out of it
i've already explained what pre-trained
and fine-tuning of a model is
and so i think you can always go ahead
and refer to that
so now that knowledge now that we're
actually going to be downloading the
model it's really important to pay
attention if you want to actually just
take a look at what the different layers
of the models are
and it just takes a while so the model
is not a really big model it's just 268
mb but it just takes a while for it to
be downloaded
and now do we also we also do get a few
of these warnings
and i've already added so if you just go
into the notebook you can already always
go and see what the details are and why
the warnings have been coming
and if you just want to maybe read
monitor uh i will not be explaining why
all these things you can always go ahead
and take a look there as well
now let's go ahead and define our
training arguments so firstly we just
actually just want to define that our
folder to be test in yarn
and then we also need and then we also
need to define our learning rate our
evaluation strategy upper device
available size our num training pods
complete decay and whether we want to
push it to hub or not and with what name
do we want to push it to the hub
so let's go so let's go ahead and also
define our training training arguments
oh just to just say no i'm i'm not
actually using i'm not actually using
tensorflow or python like all the
computer all the compute that is being
done here is actually is using a pi
dodge back end but here i'm just using
the i'm just using the like the
transformers api which makes it
extremely easy to uh for find unit for
any of any task
all right now now that we are now that
we are ready for actually our training
we just need to collate our data and
make sure that our tokenizer is actually
working well and for token
classification task
again i've already added all the clues
and all that all the details why do we
need like a data panel data collector
and so you can always go ahead and
revisit all the revisitor file as well
but finally let's actually go ahead and
test out our metric and then we can also
download a metric so here we'll be using
sequence eval which is which comes in
the form of c curl package using load
metric
and let's go ahead so we have already
written a function here which will
calculate the precision recall f1 and
accuracy all of these different things
and finally we come to the trainer part
so in trainer we define a model
we define the training arguments we
define the train data set eval data set
the tokenizer and the compute metrics
method and now that you've defined the
trainer all we need to do to fine-tune
our model is that we just need to call
train door trainer.train dataset
so the first thing this will actually do
is this is actually going to create a
data set so um let me if i go to my
profile on the hugging face hub
you can see that i can see that i do
have multiple models here so what this
is actually going to do is this is going
to actually clone um this is actually
going to create a distilled word base
and case model into an empty directory
so now this is so i've already since
i've already run this file before and
you can also see that i've already done
this before as well so we already do
have a model which has been pre-trained
before so to be actually going to use a
pre-trained model
along with that and we'll be using the
same model to be to point unit again and
again
so just so just so you just we wait and
then the next step would be just to push
then just change the model and just be
using trainer.train
and once we actually done training model
we just have we just be going ahead so
let me just run the train files
once it's done downloading the model
again so and
we just need to evaluate the model and
see where see how good it's performing
let's go ahead and evaluate the model as
well
and finally if you want to predict and
get all the get all the particular
predictions for each of them we think we
can do that as well
and so and now now that we've already
downloaded that
download the data set it's just going to
take a few seconds before you can
actually use it and start training as
well um
in the meanwhile are there any questions
that that i can help answer or anything
that about the talk about the
transformers library or any other code
that we'll be working with
um
uh andrew do you want to jump in right
now and maybe you could take one
explaining a few yeah
so um before i'm going to show both the
lstm results and kind of analyze it but
before that just to show you guys how
you would actually go about using the
kaggle platform if you take a look at my
screen right now uh you will notice that
i am on the kaggle platform that i've
linked inside the chat basically i am
already signed into my account and
specifically mine right now but um
the idea is is that you will load in uh
just a page similar to this you might
have to create an account but all the
data if you actually look is all on this
platform so you'll see the exact same
data that i was using is inside of here
and once you've signed into your account
you can just simply press the download
button and so it will just download that
archive.zip file onto your local
computer
and so when you actually go onto your
let's just say after it's done
downloading it's 3.2 megabytes so it's
not too large it's just uh i didn't want
any technical difficulties uh once
that's done downloading all you have to
go about doing is you would go onto your
google co-lab link let me just go ahead
and share
so that
compared to just running the google
drive link you would uh you'd go into
google collab
and they have a really nice drag and
drop interface so you can literally just
drop the file
in this case it's the file that i
downloaded from kaggle it's called
archive.zip and you can just drag and
drop it right into the file section
and in this case
uh you can just start writing code cells
directly from here so from this fourth
cell you just run cell
2 in this case for tensorflow
and you would run
uh the unzip file and then everything
else should be good okay
so that's how we gain access to the data
specifically it's all public domain
uh specifically for my lstm training
you'll notice something that's really
nice about keras and model that fits
specifically is that they will also give
you automatic progress bars so in this
case the data actually isn't too large
uh one million examples for text isn't
actually that problematic um and so it
all trained within a few minutes
but um
typically the longest that i typically
train models for epoc is maybe like an
hour or two for epoch and so just having
that tren uh that kind of progress bar
to kind of slowly track your project
progress like as it's training it's a
really nice sanity check just to make
sure that after like the hour that you
spent training on epoch you can at least
see that your model slowly improving
throughout the process and so while you
actually go about training the model
these values will automatically update
and you can see first they'll have a
training phase so on let's just say uh
it was like about a million example so
it's about uh 12 it's 1200 times our
batch size and then you have your
testing phase which is 144 times our
batch size and this is just how we get
our metrics and you'll notice that
slowly as we train more and more and
more you'll notice that the model is
getting better and better and better
accuracy is improving while loss is
decreasing and that's what we want to
see
and by the end of it
i've only trained it for 10 epochs but
in theory you could train it for a lot
longer um by the end of it you will see
that um
i i've collected all the data inside of
a loss
list so you'll see all the losses as
epochs go on
and you can see something inside of
accuracy on how accuracy is improving
slowly um as the model is being trained
so 62 up to 74 79 90 94 and it converges
all the way up to 94.5
and so this is on the test data set by
the way so
this is on us evaluating on the test
data set so it's actually doing a pretty
good job at not overfitting
and so after doing all this i can
actually go ahead and actually plot my
training data it's probably the most
valuable thing you can do with it after
you're done training just to do a quick
visualization you'll just use matplotlib
and plt.plot to kind of visualize and
you want to see these kinds of smooth
curves going down you'll see that slowly
starting to converge so as the loss is
getting better and better and better so
it's decreasing you'll notice that it
becomes a little bit more unstable at
the bottom just because obviously it
can't always just keep improving until
it gets into like the negative range it
will slowly plateau and so you'll notice
that the start of it is happening
however um you can definitely train it
for a lot longer you can see that it
hasn't really plateaued too too much
and in terms of the accuracy you can see
a slight more of a plateau but it's
still rising at a pretty decent rate and
so in theory you should maybe train that
for maybe double the time like 20 epochs
but you can see some really good
progress from my lstm model now
you can definitely boost this score
using the transformers library just
because all of that data that google has
trained already on their past corpuses
and like core pi and like basically just
training all those data like
pre-training their models on bert and
stuff like that and now you can leverage
all that power all those powerful models
on any r uh you can definitely achieve a
lot better results so in this case 95 is
actually mediocre you can definitely
reach around 99 maybe even like close to
100 accuracy with the really really
powerful burp model but um just for a
simple demonstration i would definitely
recommend starting off always simple
make sure that you just have a baseline
to just double check that your data is
fine just because remember garbage in
garbage out if you have bad data you
will have a bad model no matter how good
how powerful your model is and so
uh how do you do the drag and drop
um
you would just uh so do you see like the
file
you would like press that it opens up
your file system and whenever you
download something i don't know if it's
the same on like chrome but um
it's at the bottom but you can just grab
like in finder like windows or wherever
you download it on your local computer
you can just drag the file directly in
and it'll say something like drop files
to upload them to session storage if you
just let go you'll see that there's like
a progress bar at the bottom like if you
just drag and drop it you'll see that
there's like a progress bar at the
bottom saying that's slowly being
uploaded so if you just press this file
system you grab whatever was downloaded
to your local computer you can drop drag
and drop it and then you can start
working with the data
but um
in terms of um
what's it called back on track in terms
of just improving models obviously using
larger and larger models will bring
better performance um the only warning
is that it will take a lot longer to
train the reason why i kept it so simple
with like a 32 dimensional lstm is just
so it can train in these two minutes so
uh model is likely going to take a bit
longer but it will be more powerful
so hopefully uh as i've analyzed a bit
uh i'm going to introduce one last
concept and hopefully by then keegan's
model sun training so um in terms of
saving models and saving weights uh
obviously when you're using a model for
a hackathon purpose you don't want to
just see this graph and call it a day uh
you need to save these models so you can
maybe deploy it to a web application or
a mobile application and by the way
tensorflow has some really good app like
it has a really good api for both mobile
and web development uh if you want to
deploy your tensorflow model to the web
i would recommend using tensorflow.js
which allows you to basically just stick
a tensorflow model onto javascript
directly um in terms of deploying to a
mobile application there is something
called tensorflow.light which allows you
to take your tensorflow model that
you've already trained and send it to
your mobile application where it will
run a bit faster so that maybe low power
devices like your cell phone can still
run these powerful models
and so
those are just two apis you might want
to look into especially as we're
approaching hack the north
um
and yeah so in terms of just saving base
weights for maybe just like a kaggle
competition or you just want to load it
back onto a regular computer again you
can use model.save
this will load everything onto a file
called model.h5
and you'll notice that it is like a 20
megabyte file so it's not too large at
all but it basically stores everything
about the model including the structure
all the weights and you can just
directly load that in
in terms of um
loading in let's just say um weights
specifically so this is what you can do
with pi torch and pipe which doesn't
actually have the option to save an
entire model
if you just want to save the weights to
save a bit of memory um you can go ahead
and use model.save weights which will
only save the specific numbers that we
need
and so
the only issue with that is that when
you have to go about loading it in again
uh just make sure that you create the
exact same model and use model.load
weights so that'll allow you to load in
all that back into your tensorflow model
and restore the progress but it's
definitely more error prone so if you
have the memory you might as well use
monologue save but there are certain um
situations like if you're using a
inherited model but you might dive into
that a little bit more uh in the future
in terms of just more complicated
tensorflow models if you're using a
subclass model by creating like object
oriented programming and you're using
like a subclass layers like using like
uh
tf.keras.layer and stuff like that which
was introduced a bit before um
you won't be able to actually save
using the canonical method of model.save
you'll only be able to save the weights
so just keep that in mind you'll have to
recreate your model completely the same
and load it back in using one of the
load weights
okay
so hopefully by now against model's
training so i'll pass that back to him
um so yeah now we can actually see like
the training is actually completed and
we've trained for three patches and our
accuracy is 98.3
which is very good
and one of the biggest advantages that
we also evaluated it and like the
different laws
and so one of the biggest advantages is
that we can also test it instead of
actually using so we already uploaded it
to the facehub now instead of testing it
on a local system we can always go ahead
and test it on the hosting influence api
by having face
so that allows us to just report so you
suppose we have like a demo sentence
here like my name is i live in berkeley
california
we can just go ahead and just run the
commute
so this actually saves us a lot of time
and a lot of money as well because these
compute can be pretty expensive
so that is why that's why it's always
better to use the hugging face hub and
the hugging face api as well
and it just takes a while for the model
to be loaded and once the model is done
loading we can all we can just go ahead
and we just see the results
and that is and that is basically how we
cannot we can actually just go ahead and
evaluate our models
and all the really good things is that
all of our metrics that we reported have
also been added here so all that all the
time and all these details have also
been added here
and here and finally as you can see here
we can like we can see label one you can
and berkeley so if we go back to see
what label one and stuff uh
at the end the same order here so you
can see like the label one would be b
person and label five would just be
b which is
which is working california which is
which is pretty accurate
here in this
case and finally uh and finally we do
have all these resources here so
resources for tensorflow and stencil
flow find a source for tensorflow and
for hugging face in display and you can
get all the details of the workshop as
well
and thanks so much everyone thanks so
much for joining us today and it was
really a pleasure to talk to and
explaining all these details to all of
you guys thank you
yep so do we have any last minute
questions in terms of just like uh yeah
so for sure run into the feedback poll
and then if you guys have any other
questions just drop them into the
um hop in chat
or on slido both work yeah so
please just fill out the general
feedback survey and um
yeah if you have any other questions
don't hesitate to ask via hop in or uh
just through the slido and we'd be happy
to answer
yep so in terms of adding models into
hogging face gone you might want to take
that over
yeah just adding that model and pushing
that directly to hogging face
i can also help if
needed okay
uh can you guys see my screen
all right uh
okay that's fine i'll just show you guys
specifically how you push the hub so um
i think my screen shows disabled for now
but uh oh there it is okay so
specifically on how we go about pushing
models to uh hugging fees specifically a
hugging face is known for being very
very all in one so you can actually a
lot of this is very very simplified to
make it as easy as possible for you to
work with deep learning and so in order
to push the data to your hub after
you've trained it
you can legitimately just
type in the code trainer.push2hub and it
will save everything under your account
so if you look under gagon's public
feedback or public like um
public like thing inside of hugging face
like his public profile you will see the
specific model that he has pushed
because it allows you to just push it
directly under your username under the
allocated storage that you have and so
just by using trainer that pushed a hub
all of your data about training and
history and all that stuff about like
running the model it'll all
automatically just be pushed to the
hugging face library so you can just go
ahead and look on the public domain for
all these different like
all these different models that people
have trained and you can actually use
them instead of your own projects so
kind of leveraging their work and then
building on top of that to build an even
better system
so yeah just using that simple line
called trainer dot um
push to hub you're able to actually push
all that data to uh hugging face
is there anything else
all right uh
if there isn't any other question let me
just double check the slido
um
yeah it was a real pleasure to be uh
hosting this workshop and yeah thanks
everyone for coming uh again uh feel
free to reach out um i'll give you guys
my discord if you need it uh yeah if you
have any other questions about
tensorflow in general just uh feel free
to dm me i don't know if you don't want
to post his discord too but um yeah feel
free to reach out to me on linkedin or
through get uh through uh discord and
i'd be happy to help you out during like
uh maybe not during the hackathon
because i will also be hacking so that
might be a little bit not allowed but
i'd be happy to help out in general just
about tensorflow and syntax and stuff
like that and so yeah just feel free to
reach out through discord or through
linkedin and i'd be happy to help with
any technical questions or any further
resources about tensorflow um again we
kind of rushed over the workshop slides
in terms of resources like a little bit
quick but yeah uh just check out
tensorflow.org and yeah thanks everyone
for coming have a great day