Welcome, again, everybody!
Some really fun stuff appearing on the forums
this week.
One of the really great projects which was
created by I believe our sole Bulgarian participant
in the course, Slav Ivanov, wrote a great
post about picking an optimizer for style
transfer.
This post came from a forum discussion in
which I made an off-hand remark, about how
I know that in theory BFGS (Broyden-Fletcher-Goldfarb-Shanno)
is a deterministic optimizer, uses a line
search, it approximates the Hessian, it ought
to work on this kind of deterministic problem
better, but I hadn't tried it myself and I
hadn't seen anybody try it, so maybe somebody
should try it.
I don't know if you've noticed, but pretty
much every week I say something like that
a number of times and every time I do I'm
always hoping that somebody might go, I wonder
that as well.
So Slav did wonder and he posted a really
interesting blog post about that exact question.
I was thrilled to see that the blog post got
a lot of pickup.
On the machine learning Reddit, it got 55
upvotes, which for that subreddit put it in
2nd place on the front page.
It also got picked up by the WildML mailing
list weekly summary of interesting things
in AI, as the 2nd post that was listed, so
that was great.
For those of you that have looked at it and
kind of wondered what is it about this post
that causes it to get noticed whereas other
ones don't ... I'm not sure I know the secret
but as soon as I read it I thought, Okay I
think a lot of people are going to read this.
It gives some background, it assumes an intelligent
reader, it assumes an intelligent reader who
doesn't necessarily know all about this.
Something like you guys 6 months ago.
It describes this is what it is, this is where
this kind of thing is used, gives some examples,
and then goes ahead and sets up the question
of different optimization algorithms and then
shows lots of examples of learning curves
as well as pictures that come out of these
different experiments.
Hopefully it's been a great experience for
Slav as well because in the Reddit thread
there's all kinds of folks pointing out other
things he could try, questions that weren't
quite clear, and summarized in that thread
are a list of things that could be done next.
It kind of opened up a whole interesting question.
Another post (I'm not even sure if it's officially
posted yet, I got the early-bird version from
Brad), is this crazy thing.
Here is Kanye drawn using a brush of Jean-Luc
Picard.
In case you're wondering if that's really
him, I will show you a zoomed in version - it
really is Jean-Luc Picard.
This is a really interesting idea because
he points out that generally speaking when
you try to use a non-artwork as your style
image, it doesn't actually give very good
results.
Here's another example of non-artwork, doesn't
give good results.
It's kind of interesting, but it's not quite
what he was looking for.
But if you tile it, you totally get it.
So here's Kanye using a Nintendo N-controller
brush.
So then he tried out this Jean-Luc Picard
and got okay results and kind of realized
that actually the size of the texture is pretty
critical.
I've never seen anybody do this before.
I think when this image gets shared on Twitter,
it's going to go everywhere.
It's just the freakiest thing.
Freaky is good.
I think I warned you about your projects when
I first mentioned them as being very easy
to overshoot a little bit and spend weeks
and weeks talking about what you're eventually
going to do.
You've had a couple of weeks.
Really it would have been nice to have something
done by now, rather than spending a couple
of weeks wondering about what to do.
So if your team's being a bit slow agreeing
upon something, just start working on something
yourself.
Or as a team, pick something you can do by
next Monday and write up something brief about
it.
So for example, if you're thinking, We might
do the $1 million dollar Data Science Bowl.
That's fine, you're not going to finish it
by Monday.
But maybe by Monday you can have written a
blog post introducing what you can learn in
a week about medical imaging.
Oh, it turns out it uses something called
Daikon.
Here are the Python Daikon libraries, and
we tried to use them and these were the things
that got us kind of confused, and these are
the ways we solved them, and here's a Python
notebook which shows you some of the main
ways you can look at these, for instance.
Split up your project into little tiny pieces.
It's like when you enter a Kaggle competition.
I always tell people submit every single day
and try and put in at least half an hour a
day to make it slightly better than yesterday.
So how do you put in the first day's submission?
What I always do on the first day is to submit
the benchmark script, just generally like
all 0's.
And then the next day I try to improve it,
so I put in all 0.5's.
And the next day I try and improve it, be
like Okay what's the average for Cats, what's
the average for Dogs, something like that.
And if you do that every day for 90 days,
you'll be amazed at how much you can achieve.
Whereas if you wait 2 months and spend all
that time reading papers and theorizing and
thinking about the best possible approach,
you'll discover that you don't get any submissions
in.
Or you finally get your perfect submission
in and it goes terribly and now you don't
have time to make it better.
I think those tips are equally useful for
Kaggle competitions as well as for making
sure that at the end of this part of the course,
you have something that you're proud of.
Something that you think you did a good job
in a small amount of time.
If you try and publish something every week
on the same kind of topic, you'll be able
to keep going further and further on that.
I don't know what Slav's plans are, but maybe
next week he'll follow up on some of the interesting
research angles that came up on the Reddit.
Or maybe Brad will follow up on additional
ideas from his post.
Okay, there's a Lesson 10 wiki up already
which has the notebooks; just do a git-pull
on the github repo to get the most up-to-date.
I don't know if any of you saw a few months
ago there was a deep learning paper that was
summarized as having trouble with training
your deep learning algorithm off-again on-again.
Basically the paper said that if you restart
gradient descent, turning everything off and
then on again.
Another thing I wanted to point out is that
in study group (we've been having study groups
each Friday here, and I know some of you have
had study groups elsewhere around the Bay
Area), one of you asked me, I don't understand
this Gram matrix stuff, I don't get it, what's
going on?
I understand the symbols, I understand the
math, but I don't understand what's going
on.
And I said, Maybe if you had a spreadsheet
it would all make sense.
He just kind of went, Oh, I'm doing it in
Python, I think it's nice...
I said, Maybe if you had a spreadsheet, it
would all make sense.
He said, Maybe I'll create a spreadsheet.
I said, Yes, do that.
And 20 minutes later, I went to him and said,
Well how do you feel about Gram matrices now?
And he goes, I totally understand them.
And I looked over, and he had created a spreadsheet.
This is the spreadsheet he created.
It's a very simple spreadsheet where here's
the pixels, just like 1, -1, and 0.
It has two filters that are either 1 or -1.
He has the flattened convolutions next to
each other.
And then he's created the little dot product
matrix.
And so I haven't been doing so much Excel
stuff myself, but I think you learn a lot
more by trying it yourself.
Particularly if you try it yourself and you
can't figure out how to do it in Excel.
I love Excel, so if you ask me questions about
Excel we will have a great time.
So I'm not going to put that one on the forum
for now because I think it's so easy to create
and you get so much more out of doing it yourself,
for anybody who's still not quite understanding
what Gram matrices are.
So last week, we talked about the idea of
learning with larger datasets.
Our goal was to try and replicate the DeVISE
paper.
And to remind you, the DeVISE paper is the
one where we do a regular CNN, but the thing
that we're trying to predict is not a one-hot
encoding of the category, but it's the word
vector for the category.
So it's an interesting problem.
One of the things that's interesting about
it is we have to use all of ImageNet, which
has its own challenges.
So last week we got to the point where we
had created the word vectors.
Remember, the word vectors we then had to
map to ImageNet categories.
There are 1,000 ImageNet categories, so we
have to create the word vector for each one.
We didn't quite get all of them to match,
we got something like 2/3 of them to match,
so we went through 2/3 of ImageNet.
We've got as far as reading the filenames
for ImageNet.
Then we're going to resize our images, so
we're going to resize all our images to 224x224.
I think it's a good ideal to do some of this
preprocessing up front.
Something that TensorFlow and PyTorch both
do (and Keras recently started doing) is that
if you use a generator, it actually does the
image preprocessing in a number of separate
threads in parallel behind the scenes.
So some of this is a little less important
than it was 6 months ago, when Keras didn't
do that.
It used to be we had to spend a long time
waiting for our data to get processed before
it could get into the CNN.
Having said that, particularly image resizing,
when you've got large jpg's, just reading
them off the harddisk and resizing them can
take quite a bit of time.
So I always like to do all that resizing up
front and end up with something in a nice
convenient bcolz array.
Amongst other things it means that, unless
you have enough money to have a huge NVMe
or SSD drive which you can put the entirety
of ImageNet on, you probably have your big
datasets on some pretty slow spinning disk
or slow RAID array.
So one of the nice things about doing the
resizing first is that it makes it a lot smaller
and you probably can then fit it on your SSD.
There's lots of reasons that I think this
is good.
So I'm going to resize all of the ImageNet
images, put them in a bcolz array on my SSD.
So here's the path and dpath is the path to
my fast SSD mount point.
We talked briefly about there are the things
that actually do the resizing, and we're going
to do a different kind of resizing.
In the past, we've done the same kind of resizing
that Keras does, which is to add a black border.
Like if you start with something that's not
square and you make it square, you resize
the largest axis to be the size of your square,
which means you're left with a black border.
I was concerned that any model where you have
that A) is going to have learn to model the
black border, and B) that you're kind of throwing
away information, you're not using the full
size of the image.
Indeed, pretty much every other library or
paper I've seen uses a different approach,
which is to resize the smaller side of the
image to the square.
Now the larger size is too big for your square,
so you crop off the top and bottom, or crop
off the left and right.
This is called "center cropping" approach.
Question: I understand your point about needing
to learn the border.
Then you said after that you're throwing away
data, but it seems you're not throwing away
data.
Answer: That's true.
What you're doing is throwing away compute.
Like with the one where you do center crop,
you have a complete 224 thing full of meaningful
pixels.
Whereas with a black border, you have like
a 180x224 bit with meaningful pixels and a
whole bunch of black pixels
Question:
Answer: That can be a problem.
It works well for ImageNet because ImageNet
things are generally somewhat centered.
These things are all compromises.
But I've got to say, since I switched to using
this approach I feel like my models are trained
a lot faster and am getting better results,
certainly on super-resolution.
I said that week that we're going to start
looking at parallel processing.
If you're wondering about last week's homework,
we're going to get there, but some of the
techniques we're about to learn, we're going
to use to do last week's homework even better.
So don't worry.
I've got a CPU with something like 10 cores
on it and each of those cores have hyperthreading,
so it means each of those cores can do kind
of two things at once.
I really want to be able to have a couple
of processes going on each one resizing images,
that's called parallel processing.
Just to remind you, this is as opposed to
vectorization, or SIMD, which is where a single
thread operates on a bunch of things at a
time.
We learnt that to get SIMD working, you just
have to install pillow-simd.
It just happens, 600% speedup, I tried it,
it works.
As well as a 600% speedup, we're also going
to get another 10X or 20X speedup by doing
parallel processing.
The basic approach to parallel processing
in Python3 is to set up something called either
a process pool or a thread pool.
The idea here is that we've got a number of
little programs running threads, or processes,
and when we set up that pool we say how many
of those little programs do we want to fire
up.
Then what we do is we say, Okay now I want
you to use all of those workers to do some
thing.
And the easiest way to do a thing in Python3
is to use map.
How many of you have used map?
So for those of you who haven't, map is a
very common functional programming construct
that's found its way into lots of other languages,
which simply says, Loop through a collection
and call a function on every thing in that
collection and return a new collection which
is the result of calling that function on
that thing.
In our case, the function is resize, and the
collection is ImageNet images.
In fact the collection is a bunch of numbers,
0,1,2,3,4 and so forth.
And what the resize image is going to do is
open that image off disk.
It's turning the number "3" into the third
image resized to 224x224, and it will return
that.
So the general approach here, this is what
it looks like to do parallel processing in
Python.
It may look a bit weird, we're going result
equals exec.map, resize_img is the function
I want and range(i, min(i+step, n)) is the
thing I want to map over.
Then I'm saying for each thing in that list
do something.
Wait, does that make you think this list has
to have enough memory for every single resized
image?
And the answer is, No, no it doesn't.
One of the things that Python3 uses a lot
more is using these things they call generators,
which is something that looks like a list
but it's lazy, it only creates that thing
when you ask for it.
So as I append each image, it's going to give
me that image.
And if this mapping is not yet finished creating
it, it will wait.
So this approach looks like it's going to
use heaps of memory, but it doesn't.
It uses only the minimum amount of memory
necessary and it does everything in parallel.
So resize_img is something that is going to
open up the image, it's going to turn it into
a Numpy array and then it's going to resize
it.
So then the resize does the center cropping
that we just mentioned, and then after it's
resized, it's going to get appended.
What does append image (app_img) do?
So this is a bit weird.
What's going on here?
What it does is it's going to stick it into
what we call a pre-allocated array.
So we're learning a lot of computer science
here.
Anybody that's done computer science before
will be familiar with all of this already,
if you haven't you probably won't.
It's important to know that the slowest thing
in your computer, generally speaking, is allocating
memory.
It's finding some memory, it's reading stuff
from that memory, it's writing to that memory,
unless of course, it's like cache or something.
Generally speaking, if you create lots and
lots of arrays and then throw them away again,
that's likely to be really really slow.
So what I wanted to do was to create a single
224x224 array which is going to contain my
resized image, and then I'm going to append
that to my bcolz tensor.
So the way you do that in Python is wonderfully
easy.
You can create a variable from this thing
called threading.local [tl=threading.local()].
tl is something that looks a bit like a dictionary,
but it's a very special kind of dictionary.
It's going to create a separate copy of it
for every thread or process.
Normally when you've got lots of things happening
at once, it's going to be a real pain because
if two things try to use it at the same time
you get bad results or even crashes.
But if you allocate a variable like this,
it automatically creates a separate copy in
every thread.
And you don't have to worry about locks, you
don't have to worry about race conditions,
whatever.
So once I've created this special threading.local
variable, I then create a placeholder inside
it, which is just an array of 0's of size
224x224x3.
And later on, I create my big bcolz array
(which is where I'm going to put everything
eventually) and to append the image, I grab
the bit of the image that I want and I put
it into that preallocated threaded local variable
(tl) and then I append that to my bcolz array.
So there's lots of detail here in terms of
like using parallel processing effectively.
I wanted to kind of briefly mention it, not
because I think somebody who hasn't studied
computer science is now going to go, Okay
I totally understood all that, but to give
you some of the things to search for and learn
about over the next week if you haven't done
any parallel programming before.
You're going to need to understand local storage
and race conditions and ...
In Python, there's something called a global
interpreter lock (which is one of the many
awful things about Python, possibly its most
awful thing), which is in theory two things
cannot happen at the same time, because Python
wasn't really written in a thread-safe way.
The good news is that lots of libraries are
written in a thread-safe way.
So if you're using a library where most of
its work is being done in C, as is the case
with pillow-simd, actually then you don't
have to worry about that.
I can prove it to you even because I drew
a little picture.
Here is the result of serial vs parallel.
The serial without simd version is 6X bigger
than this, 2000 images.
With SIMD, it's 25 seconds.
With the process pool it's 8 seconds for 3
workers, for 6 workers it's 5 seconds, so
on and so forth.
For the thread pool, it's even better, 3.6
seconds for 12 workers, 3.2 seconds for 16
workers.
Now your mileage will vary, depending on what
CPU you have.
Given that quite a lot of you are using the
P2 still (unless you've got your deep learning
box up and running), you'll have the same
performance as other people using the P2,
but you should try something like this which
is to try different numbers of workers and
see what's the optimal for that particular
CPU.
Once you've done that, you know.
Once I got beyond 16, I didn't get any improvements.
So I know that on that computer a thread pool
of size 16 is a pretty good choice.
As you can see, once you get into the right
general vicinity, it doesn't vary too much.
Question: ... two threads per core ...
Answer: Roughly that, yeah.
So that's the general approach here.
Run through something in parallel.
Each time append it to my bcolz array.
At the end of that, I've got a bcolz array
that I can use again and again.
So I don't rerun that code very often anymore.
I've got all of ImageNet resized into each
of 72x72, 224 and 288.
I give them different names and I just use
them.
Question:
Answer: In fact I think that's what Keras
does, now that I think about it.
I think it squishes.
Here's one of these things.
I'm not quite sure.
My guess was that I don't think it's a good
idea because you're now going to have dogs
of various different squish levels and your
CNN is going to have to learn that thing.
It's got another type of symmetry to learn
about, level of squishiness.
Whereas if we keep everything at the same
kind of aspect ratio, I think it's going to
be easier to learn so we'll get better results
with less epochs of training.
That's my theory and I'd be fascinated to
see somebody do a really in-depth analysis
of like black borders vs center cropping vs
squishing in ImageNet.
From now on, we can just open the bcolz array
and there we go.
We're now ready to create our model, and I'll
run through this pretty quickly because most
of this is pretty boring.
The basic idea here is we need to create an
array of labels, called vecs, which contains
for every image in my bcolz array needs to
contain the target word vector for that image.
Just to remind you, last week we randomly
ordered the filenames so this bcolz array
is in random order.
So we've got our labels, which is the word
vectors for every image.
We need to do our normal preprocessing.
This is a handy way to preprocess in the new
version of Keras.
We're using the normal Keras ResNet model
(ResNet50), the one that comes in Keras.applications.
It doesn't do the preprocessing for you, but
if you create a lambda layer that does the
preprocessing, then you can use that lambda
layer as the input tensor.
So this whole thing now will do the preprocessing
automatically, without you having to worry
about it.
So that's a good little trick.
I'm not sure it's quite as neat as what we
did in Part 1, where we put it in the model
itself, but at least this way we don't have
to maintain a whole separate version of all
of the models, so that's kind of what I'm
doing nowadays.
When you're working on really big datasets,
you don't want to process things any more
than necessary, any more times than necessary.
I know ahead of time that I'm going to want
to do some fine-tuning.
So what I decided to do was I decided this
is the particular layer where I'm going to
do my fine-tuning, so I decided to first create
a model which started at input and went as
far as this layer (res5a_branch2a).
So my first step was to create that model
and save the results of that.
The next step will be to take that intermediate
step and to take it to the next stage I want
to fine-tune to and then say that.
So it's a little shortcut.
There's a couple of really important intricacies
to be aware of though.
The first one is you'll notice that ResNet
and Inception are not used very often in transfer
learning.
Again this is something which I've not seen
studied, and I think this is a really important
thing to study, is which of these things work
best for transfer learning.
But I think one of the difficulties is just
ResNet and Inception are harder.
And the reason they're harder is that if you
look at ResNet you've got lots and lots of
layers which make no sense on their own.
They keep on splitting into 2 bits and merge
again, splitting into 2 bits
So what I did was I looked at the Keras source
code to kind of find out how is each block
named.
Because what I want to do is say, Okay, we've
got a ResNet block, we've just had a merge,
it goes out and does a couple of convolutions,
and it comes back and does an addition.
And basically, I want to get, I want to grab
one of these.
Unfortunately, for some reason, Keras does
not name these merged cells.
So what I had to do was get the next cell
and then go back by 1.
It kind of shows you how little people have
been working with ResNet and transfer learning;
literally the only bits to transfer learn
from are nameless in one of the most popular,
probably the most popular, thing in transfer
learning.
There's a second complexity when working with
ResNet.
We haven't discussed this much but ResNet
actually has 2 kinds of ResNet blocks.
One is this kind, which is an identity block.
And the second kind is a ResNet convolution
block, which they also call a bottleneck block.
And what this is is it's pretty similar (one
thing that goes up through a couple of convolutions,
then it gets added together), but the other
side is not an identity, the other side is
a single convolution.
In ResNet, they throw in one of these every
half dozen blocks or so.
Why is that?
The reason is that if you only have identity
blocks then all it can really do is to continually
fine-tune where it's at so far.
We've learned quite a few times now that these
identity blocks, basically they map to the
residual.
They continually fine-tune the types of features
that we have so far.
Whereas these bottleneck blocks actually force
it from time to time to kind of create a whole
different type of features, because there
is no identity path through here, the shortest
path still goes through a single convolution.
So when you think about transfer learning
from ResNet, you kind of need to think about,
Okay, Should I transfer learn from an identity
block, or after, or from a bottleneck block,
or after?
Again I don't think anybody's studied this,
or at least I haven't seen anybody write it
down.
I've played around with it a bit and I'm not
sure I have a totally decisive suggestion
for you.
My guess is that the best point to grab in
ResNet is the end of the block immediately
before the bottleneck block.
And the reason for that is that at that level
of receptive field (obviously because each
bottleneck block is changing the receptive
field) and at that level of semantic complexity
this is the most sophisticated version of
it.
It has been through a whole bunch of identity
blocks to get there, and gradually fine-tune,
fine-tune, fine-tune, bottleneck ... fine-tune,
fine-tune, fine-tune, bottleneck.
My belief is you want to get just before that
bottleneck is the best place to transfer learn
from.
So that's what this is.
This is the spot just before the last bottleneck
layer in ResNet.
So it's pretty late and so as we know very
well from Part 1 with transfer learning when
you're doing something that's not too different,
and in this case we're switching from one-hot
encoding to word vectors, which is not too
different.
You probably don't want to transfer learn
from too early, so that's why I picked this
fairly late stage, just before the final bottleneck.
The second complexity here is that this bottleneck
block has these dimensions (14,14,1024), the
output is 14 by 14 by 1024.
So we have about a million images.
So a million by 14 by 14 by 1024 is more than
I want to deal with.
So I did something very simple, which was
I popped in one more layer after this, which
is an average pooling layer, 7 by 7.
So that's going to take my 14 by 14 output
and turn it into a 2 by 2 output.
So let's say one of those activations was
looking for birds' eyeballs, then it's saying
in each of the 14 by 14 spots how likely is
it that this is a bird's eyeball.
After this, it's now saying in each of these
4 spots, on average how much do those cells
look like birds eyeballs.
So this is losing information.
If I had a bigger SSD and more time, I wouldn't
have done this.
But it's a good trick, right, when you're
working with these fully convolutional architectures
you can pop an average pooling layer anywhere
and decrease the resolution to something that
you feel you could deal with.
So in this case, this was my decision - to
go to 2 by 2 by 1024.
Answer: We have quite a few times.
Which is basically the merge was the thing
that does the "+" here.
That's the trick to making it into a ResNet
block, is having the addition of the identity
with the result of going through a couple
convolutions.
Answer: There's not exactly a best-practice
for that.
In a sense, every single successful architecture
gives you some insights about that because
every one of them eventually has to end up
with 1000 categories if it's ImageNet or 3
channels of 0,255 continuous if it's generative.
So the best thing you can really do is two
things.
One is to kind of look at the successful architectures.
Another thing is, although this week is the
last week we're going to be looking at images,
I am going to briefly open next week with
a quick run-through of some of the things
you can look at to learn more.
One of them is going to be a paper.
I've got two different papers which have best
practices really nice descriptions like we've
did 100 different things and here are the
100 different results.
All this stuff is pretty artisanal.
Answer: Yes, and that's compressed.
Uncompressed it's like a couple of hundred
Gig.
Answer: I'm not going to load it into memory,
you'll see.
That's exactly the right segue I way looking
for, so thank you.
So what we're going to do now is we're going
to run this model that we've just built, just
call basically .predict on it and save the
predictions.
The problem is that the size of those predictions
is going to be bigger than the amount of RAM
I have, so I need to do it a batch at a time
and save it a batch at a time.
I've got a million things, each one with this
many activations.
This is going to happen quite often.
You'll either be working on a smaller computer,
or you're working with a bigger dataset, or
you're working with a dataset where you're
using a larger number of activations.
This is actually very easy to handle.
You just create your bcolz array where you're
going to store it and then all I do is I go
through from 0 to the length of my array (my
source array) a batch at a time.
So this is creating numbers 0, 0+128, 128+128,
and so on and so forth.
And then I take the slice of my source array
from 0 to 128, 128 to 256 an so forth.
So this is going to create a slice of my bcolz
array.
Well actually this is going to create a generator
which is going to have all of those slices.
And being a generator it's going to be lazy.
So I can then enumerate through each of these
slices and I can append to my bcolz array
the result of predicting just on that one
batch.
So you've seen predict, and evaluate and fit
and so forth and the generator versions.
Also in Keras, there's generally an on_batch
version, so there's a train_on_batch, and
a predict_on_batch.
What these do is they basically have no smarts
to them at all, it's just like the most basic
thing.
It's basically just going to take whatever
you give it and call predict on this thing.
It won't shuffle it, or batch it, it's just
going to throw it directly into the computation.
So this is going to take our model and it's
going to call predict on just this batch of
data.
And then from time-to-time, I'll print out
how far I've got, just so that I'll know how
I'm going.
Also, from time-to-time I call .flush, that's
the thing in bcolz that actually writes it
to disk, that's to make sure it's continuously
written to disk.
One of the nice things I can do here is I
can do some data augmentation as well.
So I've added a direction parameter and what
I'm going to do is I'm going to have a second
copy of all of my images which is flipped
horizontally.
I think I screwed this up ... To flip things
horizontally, if we pass in a -1 here then
it's going to flip it horizontally.
That explains why some of my results haven't
been quite as good as I hoped.
So when you run this, we're going to end up
with a big bcolz array that's going to contain
2 copies of every resized ImageNet image,
the activations at the layer one layer before
this.
So I call it once with direction forwards
and once with direction backwards.
A the end of that, I've now got nearly 2 million
images for activations of 2 by 2 by 1024.
So 
that's pretty close to the end of ResNet.
I've then just copied and pasted from the
Keras code the last few steps of ResNet.
So this is the last few blocks.
I added in one extra identity block just because
I had a feeling it might help things along
a little bit.
Again, people have not really studied this
yet.
I haven't had a chance to properly experiment,
but it seemed to work quite well.
This is basically copied and pasted from Keras'
code.
I then need to copy the weights in Keras for
those last few layers of ResNet.
So now I'm going to repeat the same process
again, which is to call predict on these last
few layers, and the input will be the output
from the previous one.
We went like 2/3 of the way into ResNet and
got those activations, then put those activations
in to the last few stages of ResNet to get
those activations.
The outputs from this are actually just a
vector of length 2048 which does fit in my
RAM, so I didn't bother with calling predict_on_batch,
I just call .predict.
If you do this at home and you don't have
enough memory, you can just use predict_on_batch
trick again.
Anytime you run out of memory when calling
predict, you can always just use this pattern.
So at the end of all that, I've got the activations
from the penultimate layer of ResNet, and
so I can do our usual transfer learning trick
of creating a linear model.
And my linear model is now going to try to
use the number of dimensions in my word vectors
as its output.
And you'll see it doesn't have any activation
function, that's because I'm not doing one-hot-encoding.
My word vectors can be any size numbers, so
I just leave it as linear.
And then I compile it.
And then I fit it.
And so this linear model is now my very first
... you know, this is almost the same as what
we did in Lesson 1, in Dogs vs Cats.
We're fine-tuning a model to a slightly different
target to what it was originally trained with.
It's just that we're doing it with a lot more
data, so we have to do it more thoughtfully.
There's one other difference here which is
I'm using a custom loss function.
And the loss function I'm using is cosine
distance (cos_distance).
You can look that up at home if you're not
familiar with it, but basically cos_distance
says for these two points in space, what's
the angle between them, rather than how far
away are they.
The reason we're doing that is we're about
to start using K-nearest neighbors.
In K-nearest neighbors we're going to basically
say, Here's the word vector we predicted,
which is the word vector which is closest
to it.
It turns out that in really really high dimensional
space, the concept of how far away something
is is nearly meaningless.
The reason why is that in really really high
dimensional space, everything sits on the
edge of that space.
Basically because you can imagine like as
you add each additional dimension, the probability
that something's on the edge in that dimension.
Let's say that the probability it sits right
on the edge is 1/10, then if you go 1 dimension,
you've got a probability of 1/10 that it's
on the edge in 1 dimension.
In 3 dimensions, it's basically multiplicatively
decreasing the probability that that happens.
So in a few hundred dimensional space, everything
is on the edge.
And when everything's on the edge everything
is kind of an equal distance away from each
other, more or less.
So distances aren't very helpful.
But the angle between things will vary.
So when you're doing anything with trying
to find nearest neighbors, it's a really good
idea to train things using cosine distance
(cos_distance), and this is the formula for
cos_distance.
Again, this is one of those things where I'm
skipping over something where you could probably
spend a week in undergrad studying.
There's heaps of information about cosine
distance on the web.
So for those of you who are very familiar
with it, I won't waste your time.
For those of you not, it's a very very good
idea to become familiar with this.
Feel free to ask on the forums if you can't
find any material that makes sense.
So we've fitted our linear model.
As per usual, we've saved our weights.
And we can see how we're going.
So what we've got now is something where we
can feed in an image and it will spit out
a word vector.
But it's something that looks like a word
vector, it has the same dimensionality as
a word vector but it's very unlikely that
it's going to be the exact same vector as
one of our 1000 target word vectors.
If the word vector for a Pug is this list
of 200 floats, even if we have a perfectly
Pug-y Pug, we're not going to get that exact
list of 2000 floats.
We'll have something that is similar.
And when we say "similar" we probably mean
that the cosine distance between the perfect
iconic Pug and our Pug is pretty small.
So that's why after we get our predictions,
we then have to use nearest neighbors as a
second step, to basically say for each of
those predictions, what are the 3 word vectors
that are the closest to that prediction.
We can now take those nearest neighbors and
find out for a bunch of our images, what are
the 3 things it might be.
For example, for this image here its best
guess was trombone.
Next was flute and third was cello.
So this gives us some hope that this approach
seems to be working okay.
It's not great yet, but it's recognized these
things as musical instruments.
And its third guess was in fact the correct
musical instrument.
So we know what to do next, what we do next
is to fine-tune more layers.
And because we have already saved the intermediate
results from an earlier layer, that fine-tuning
is going to be much faster to do.
Two more things I'll briefly mention.
There's a couple of different ways to do nearest
neighbors.
One is called the brute-force approach, which
is literally, Okay what is the nearest word
vector for this word vector, go through every
one and see how far away it is.
There's another approach, which is approximate
nearest neighbors.
And when you've got lots and lots of things
you're trying to look for nearest neighbors,
the brute-force approach is going to be n-squared
time, it's going to be super-slow.
Whereas approximate nearest neighbors are
generally approximately log-n time.
So orders of magnitude faster, if you've got
a large dataset.
The particular approach I'm using here is
called Locality Sensitive Hashing (LSH).
It's a fascinating and wonderful algorithm.
Anybody who's interested in algorithms, I
strongly recommend you go read about it, let
me know if you need a hand with it.
My favorite kind of algorithms are these approximate
algorithms.
In data science, you almost never need to
know something exactly.
Yet nearly every algorithm that people learn
at university (and certainly high school)
are exact.
We learn exact nearest neighbor algorithms,
exact indexing algorithms, exact median algorithms.
Pretty much for every algorithm out there,
there's an approximate version that runs an
order of n or log-n/n faster.
And one of the cool things is that once you
start realizing that you suddenly discover
that all of the libraries you've been using
for ages were written by people who didn't
know this.
And then you realize that for every sub-algorithm
they've written they could have used an approximate
version, the next thing you know you've got
something that runs 1000 times faster.
The other cool thing about approximate algorithms
is that they're generally written to provably
be accurate to within "so close".
And it can tell you with your parameters how
close is "so close".
Which means that if you want to make it more
accurate, you run it more times, with like
different random seeds.
And so this thing called LSHForest is a Locality
Sensitive Hashing Forest, which means it creates
a bunch of these locality sensitive hashes.
And the amazingly great thing about approximate
algorithms is that each time you create another
version of it, you're exponentially increasing
or multiplicatively increasing the accuracy,
but only linearly increasing the time.
So if the error on one call of LSH was (1-b),
then the error on two calls is (1-b) squared,
and the error on three calls is (1-b) cubed.
And the time you're taking is now 2*n or 3*n.
So when you've got something that you can
make as accurate as you like with only linear
increase in time, it's incredibly powerful.
So this is a great approximation algorithm.
I wish we had more time, I'd love to tell
you all about it.
So I generally use LSHForest when I'm doing
nearest neighbors, because it's arbitrarily
close and much faster when you've got word
vectors.
The reason that, the time that becomes important
is when I move beyond ImageNet, which I'm
going to do now.
So let's say I've got a picture and I don't
just want to say which one of the 1000 ImageNet
categories is it, but which one of the 100,000
WordNet nouns is it.
That's a much harder thing to do.
And that's something that no previous model
could do.
When you trained an ImageNet model, the only
thing it could do was recognize pictures of
things that were in ImageNet.
But now, we've got a word vector model and
so we can put in an image that spits out a
word vector and that word vector could be
closer to things that are not in ImageNet
at all.
Or it could be some higher level of the hierarchy,
so you could look for a dog rather than a
Pug.
Or a plane, rather than a 747.
So here we bring in the entire set of word
vectors.
I'll have to remember to share these with
you because these are actually quite hard
to create.
This is where I definitely want LSHForest
because this would be pretty slow.
And we can now do the same thing.
Not surprisingly, it's got worse.
The thing that was actually cello, cello's
not even in the top 3.
So this is a harder problem.
So let's try fine-tuning.
Fine-tuning is the final trick I'm going to
show you.
Answer: You might remember last week we looked
at creating our word vectors, and what we
did was actually I created a list.
I went to WordNet and I downloaded the whole
of WordNet, and then I figured out which things
are nouns and I used a regex to parse out
those and then I saved that.
So, yes, we actually have the entirety of
WordNet nouns.
Answer: Because it's not a good enough model
yet.
Now that there's 80,000 nouns, there's a lot
more ways to be wrong.
So when it only has to say, Which of these
1000 things is it?
That's pretty easy.
Which of these 80,000 things is it?
That's pretty hard.
So to fine-tune it, it looks very similar
to our usual way of fine-tuning things.
Which is that we take our two models and stick
them back to back, and we're now going to
train the whole thing rather than just a linear
model.
Now the problem is that the input to this
model is too big to fit in RAM.
So how are we going to call fit or fit_generator
when we have an array that's too big to fit
into RAM?
Well, one obvious thing to do would be to
pass in the bcolz array.
Because to most things in Python a bcolz array
looks just like a regular array, doesn't really
look any different.
But here's the problem -- the way a bcolz
array is actually stored is it's actually
stored in a directory (as I'm sure you've
noticed), and in that directory it's got something
called chunk length, "chunklen" actually.
I set it to 32 when I created these bcolz
arrays.
What it does is it takes every 32 images and
it puts them in to a separate file.
So each one of these has 32 images in it,
or 32 of the leading axis of the array.
Now, if you then try to take this whole array
and pass it .fit in Keras with shuffle, it's
going to try and grab one thing from here,
and one thing from here, one thing from here,
and one thing from here.
And here's the bad news, for bcolz to get
one thing out of a chunk it has to read and
decompress the whole thing.
It has to read and decompress 32 images in
order to give you the one image you asked
for.
That would be a disaster.
That would be ridiculously horribly slow.
We didn't have to worry about that when we
called predict_on_batch, because we were not
shuffling, but we were going in order, it
was never grabbing a single image out of the
chunk.
But now that we want to shuffle, it would.
So what we've done is, somebody actually on
the Kaggle forum, provided something called
a bcolz array iterator.
And the bcolz array iterator, which was kindly
discovered on the forums actually by somebody
named MPJansen, originally written by this
fellow (Thiago Ramon Gonsalves Montoya).
What it does is provides a Keras-compatible
generator which grabs an entire chunk at a
time.
So it's a little bit less random.
Given that this has 2,000,000 images and the
chunk length is 32, then it's going to basically
create a batch of chunks, rather than a batch
of images.
And so that means we have none of the performance
problems.
And particularly because, remember originally
we randomly shuffled our files, so this thing
is randomly shuffled anyway.
So this is a good trick.
So you'll find the bcolz array iterator on
github.
Feel free to take a look at the code, it's
pretty straightforward.
There was a few issues with the original version
so MPJansen and I tried to fix it up and I've
written some tests for it and he's written
some documentation for it.
But if you just want to use it, it's as simple
as writing this.
iterator-blah equals BcolzArrayIterator(data,
labels, shuffle=True, batch_size=whatever),
and you can just call fit_generator as per
usual, passing in that iterator, and that
iterator's number of items.
So to all of you guys who've been asking how
do I deal with data that's bigger than memory,
this is how you do it.
Hopefully that will make life easier for a
lot of people.
So we fine-tune it for a while, we do some
annealing for a while, and this basically
runs overnight, for me.
Takes about 6 hours to run.
So I come back the next morning and I just
copy and paste my K nearest neighbors, and
I call predict, get my predictive word vectors.
For each word vector I then pass it in to
nearest neighbors, this is my 1000 categories.
Lo and behold, we now have cello in the top
spot, as we hoped.
How did it go in the harder problem of looking
at the 100,000 nouns or so in English?
Pretty good.
It got this one right.
And just pick another one at random, it said
throne and that sort of looks like a throne.
So it's looking pretty good.
So here's something interesting.
Now that we have brought images and words
into the same space, let's play with it some
more.
So why don't we use nearest neighbors with
those predictions.
Answer: To the word2vec vector, which Google
created, but the subset of those which are
nouns according to WordNet, mapped to their
synset IDs.
Answer: The word vectors are just the word2vec
word vectors that we can download off the
Internet.
They were pre-trained by Google.
Answer: They're embeddings.
They are not really weights, they're embeddings.
Answer: Yes, exactly.
So we're saying here is this image, spits
out a vector from the thing we just trained.
We have 100,000 word2vec word vectors for
all the nouns in English.
Which one of those is the closest to the thing
that came out of our model.
And the answer was "throne".
Let's do something interesting.
Let's create a nearest neighbors, not for
all the word2vec vectors, but for all of our
image predicted vectors.
And now, we can do the opposite.
Let's take a word we pick at random.
Let's look it up in our word2vec dictionary
and let's find the nearest neighbors to that
in our images.
There it is.
So this is pretty interesting.
There are the top 3.
You can now find the images that are the most
like whatever word you come up with.
So that's crazy, but we can do crazier.
Here is a random thing I picked.
Notice I picked it from the validation set
in ImageNet, so we've never seen this image
before.
Honestly when I opened it up my heart sank
because I don't know what it is.
So this is a problem.
What is that?
Manta Ray.
So I didn't pick it, but what we can do is
we can call .predict on that image and then
we can do a nearest neighbors on all of our
other images.
There's the first, there's the second.
And the third one is even somebody putting
their hand on it.
Which is slightly crazy, but that is what
the original one looked like.
In fact, I ran it again on a different image.
Here, I took this one.
I actually looked around for something pretty
weird, and this is pretty weird, right.
Is this a net or is this a fish?
So when we then ask for nearest neighbors,
we get fish in nets.
So it's like sometimes deep learning is so
magic, you go, How can that possibly work?
Answer: Yeah, yeah.
Only a little bit.
Maybe in a future course we might look at
that.
Maybe even in your numerical linear algebra
course we might be looking at that.
I don't think we'll cover it in this course.
But do look at Dask, it's super-cool.
Answer: No, not at all.
These are actually labeled as this particular
kind of fish.
That's the other thing, it's not only found
fish in nets, it's found more or less the
same breed of fish in the nets.
But when we call .predict on those it created
a word vector which was probably halfway between
that kind of fish and a net.
Because it doesn't know what to do.
Sometimes when it sees things like that it
would have been marked in ImageNet as a net
and sometimes as a fish.
And so the best way to minimize the loss function
would be to kind of hedge.
So it hedged and as a result the images that
were closest were the ones which actually
were halfway between the two.
So it's kind of a convenient accident.
Answer: You absolutely can and
I have.
But for nearest neighbors I haven't found
anything nearly as good as cosine, and that's
true in all of the things I looked up as well.
By the way, I should mention that when you
use locality sensitive hashing in Python by
default it uses something that's equivalent
to cosine metrics, that's why nearest neighbors
works.
Question: ... more than one word?
Answer: Yes.
Exactly.
Starting next week we're going to be learning
about sequence-to-sequence models and memory
and attention methods.
They're going to show us how we can take an
input such as a sentence in English and spit
out an output such as a sentence in French.
This particular case study we're going to
be spending 2 or 3 weeks on.
When you combine that with this, you get image
captioning.
I'm not sure if we're going to have time to
do it ourselves, but it will literally be
trivial for you guys to take the two things
and combine them and do image captioning.
It's just those two things mixed together.
Before we take a break I want to show you
the homework.
The homework, hopefully you guys noticed I
gave you some tips because it's a really challenging
one.
Even though in a sense it's kind of straight-forward,
which is take everything that we've already
learnt about super-resolution and slightly
change the loss function so that it does perceptual
losses for style transfer instead, the details
are tricky.
I'm going to quickly show you two things.
First of all, I'm going to show you how I
did the homework, because I actually hadn't
done it last week.
Luckily I have enough RAM that I could read
the two things into memory.
Once again, you can just do that to a bcolz
array to turn it into a Numpy array in memory.
So one thing I did was I created my upsampling
block to get rid of the checkerboard patterns.
That was literally as simple as saying, UpSampling2D
and then a 1x1 conv.
So that got rid of my checkerboard patterns.
The next thing I did was I changed my loss
function and I decided before I tried to do
style transfer with the perceptual losses,
let's try and do super-resolution with multiple
content loss layers, because that's one thing
that I'm going to have to do for style transfer
is to be able to use multiple layers.
I always like to start with something that
works and make small little changes so it
keeps working at every point.
So in this case I thought, let's first of
all slightly change the loss function for
super-resolution so that it uses multiple
layers.
So here's how I did that.
I changed my vgg content so that it created
a list of outputs (conv1 the first, second
and third blocks), and then I changed my loss
function so it went through and added the
mean square difference for each of those three
layers.
I also decided to add a weight, just for fun.
I decided to go [0.1,0.8,0.1] because this
is the layer that they used in the paper,
but let's have a little bit of more precise
super-resolution and a little bit of more
semantic super-resolution and see how it goes.
I created this function for a more general
sort of mean-square-error (mean_sqr_b).
And that was basically it.
So other than that line and that line everything
else was the same.
So that gave me super-resolution working on
multiple layers.
One of the things I found fascinating is that
this was the original low-res.
It's done a good job of upscaling it, but
it's also fixed up the weird white balance,
which really surprised me.
It's taken this obviously over-yellow shot
and this is what ceramic should look like,
it should be white.
And somehow it's kind of adjusted everything
so that the serviette in the background has
gone from a yellow-brown here to a nice white,
as well as these cups here.
It's figured out that these slightly pixelated
things are actually meant to be upside-down
handles.
This is on only 20,000 images.
I'm very surprised that it's fixing the color
because we never asked it to.
But I guess it knows what a cup's meant to
look like and so this is what it decided to
do, to make a cup look the way it's meant
to look.
So that was pretty cool.
So then to go from there to style transfer
was pretty straight-forward.
I had to read in my style, as before.
This is the code to do the special kind of
ResNet block where we use valid convolutions,
which means we lose 2 pixels each time and
so therefore we have to do a center crop.
Don['t forget, lambda layers are great for
this kind of thing.
Whatever code you can write, tuck it into
a lambda layer and suddenly it's a Keras layer.
So I do my center crop (res_crop_block), this
is now a ResNet block which does valid convs.
This is basically all exactly the same.
We have to do a few downsamplings, then our
computations and our upsamplings, just like
the supplemental paper.
So the loss function looks a lot like the
loss function did before but we've got two
extra things.
One is the Gram matrix.
So here is a version of the Gram matrix which
works a batch at a time (gram_matrix_b).
If any of you tried to do this a single image
at a time you would have gone crazy with how
slow it took.
I saw a few of you trying to do that.
So here's the batchwise version of Gram matrix.
And the second thing I needed to do was somehow
feed in my style target.
Another thing I saw some of you do was feed
in the style target every time, feed in that
array into your loss function.
Now you can obviously calculate your style
target by just calling .predict with the thing
which gives you all of your different style
target layers.
But the problem is this thing here returns
a Numpy array.
It's a pretty big Numpy array.
Which means that then when you want to use
it as a style target in training, it has to
copy that back to the GPU.
And copying to the GPU is very, very, very
slow.
And this is a really big thing to copy to
the GPU.
So if any of you tried this and I saw some
of you try it, it took forever.
So here's the trick - call .variable on it.
Turning something into a variable sticks it
on the GPU for you.
So once you've done that you can now treat
this as a list of symbolic entities which
are the GPU versions of this.
So I can now use this inside my GPU code.
So here my style targets I can use inside
my loss function and it doesn't have to do
any copying backwards and forwards.
So there's a subtlety, but if you don't get
that subtlety right you're going to be waiting
for a week or so for your code to finish.
So those were the little subtleties which
were necessary to get this to work.
Once you get it to work, it does exactly the
same thing basically as before.
Where this gets combined with DeVISE is I
wanted to try something interesting, which
is in the original perceptual losses paper
they trained it on the MSCOCO dataset, which
has 80,000 images.
I wanted to know what would happen if we trained
it on all of ImageNet, so I did.
So I decided to train a super-resolution network
on all of ImageNet.
The code is all identical so I'm not going
to explain it other than you'll notice that
we don't have the [:] here anymore because
we don't want to try to read in the entirety
of ImageNet into RAM, but these are still
bcolz arrays.
All the other code is identical until we get
to here.
So I used a BcolzArrayIterator.
I can't just call .fit because .fit (or .fit_generator)
assumes that your iterator is returning your
data and your labels.
In our case we don't have data and labels.
We have two things that both get fed in as
two inputs.
And our labels are a list of 0's.
So here's the trick, and this answers your
earlier question about how do you do multi-input
models on large datasets.
And the answer is create your own training
loop.
Create your own training loop which loops
through a bunch of iterations and then you
can grab as many batches of data from as many
different iterators as you like and then call
train on batch.
So in my case, my BcolzArrayIterator is going
to return my high-resolution and low-resolution
batch of images.
So I go through a bunch of iterations, grab
one batch of high-res and low-res images and
pass them as my two inputs to train on batch.
So this is the only code I changed, other
than changing .fit_generator to actually call
in train.
So as you can actually see this took me 4.5
hours to train.
Then I decreased the learning rate and I trained
for another 4.5 hours.
Actually I did it overnight last night and
I only had enough time to do about half of
ImageNet, this isn't even the whole thing.
But check this out.
Take that model and we're going to call .predict
on this is the original high-res image, here's
the low-res version and here's the version
that we've created.
As you can see, it's done a pretty extraordinarily
good job.
When you look at the original ball, there
was this kind of vague yellow thing here,
it's kind of turned it into a nice kind of
inscription.
You can see that the eyes were like two gray
blobs and it's kind of turned it into some
eyes.
You could just tell that that's an "A", maybe
if you look carefully, now it's very clearly
an "A".
So you can see it does an amazing job of upscaling
it.
Cooler still this is a fully convolutional
net and not is specific to any particular
to any input resolution.
So what I can do is I can create another version
of the model using our high-res as the input.
So now we're going to call .predict with the
high-res input and that's what we get back.
So like look at that, we can now see all of
this detail on the basketball where none of
that really existed here.
Like it was there, but it was pretty hard
to see what it was.
Look at before, look at her hair, it was kind
of this gray blob here.
Here you can see it knows, it's like little
bits of pulled-back hair.
Like we can take any sized image and we can
make it bigger.
This to me is one of the most amazing results
I've seen in deep learning.
We trained something on nearly all of ImageNet,
it's a single epoch, so there's definitely
no overfitting and it's able to able to recognize
what hair is meant to look like when it's
pulled back into a bun.
Pretty extraordinary results I think.
Something else which I only realized later
is that it's all a bit fuzzy, and there's
an arm in the background that's fuzzy, and
the model knows that that's meant to stay
fuzzy, right.
It knows what out-of-focus things look like.
Equally cool is not just how like that "A"
is now incredibly precise and accurate but
the fact that it knows that blurry things
stay blurry.
I don't know if you're as amazed at this as
I am but I thought that this is a pretty cool
result.
We could run this over a 24 hour period or
maybe two epochs over all of the Internet
and get even better still.
Before I continue, I did want to mention one
thing in the homework that I changed.
I realized in my manually created loss function
I was already doing a mean square error in
the loss function, but then when I told Keras
to make that thing as close to 0 as possible,
I had to also give it a loss function and
I was giving it MSE, and effectively that
was like squaring my squared errors, it seemed
wrong.
So I've changed it to 'mae' (mean absolute
error), so when you look back over the notebooks,
that's why.
Because this is just to say, Hey get the loss
as close to 0 as possible, I didn't really
want to re-square it, that didn't make any
sense.
So that's why you'll see that minor change.
The other thing to mention is that I did notice
that when I retrained my super-resolution
on my new images that didn't have the black
border, it gave good results much much faster.
So I really think that thing of like learning
to put the black border back in seemed to
take quite a lot of effort for it.
So again, hopefully some of you are going
to look into that in more detail.
So we're going to learn about adversarial
networks, generative adversarial networks.
And this will kind of close off our deep dive
into generative models as applied to images.
Just to remind you, the purpose of this has
been to learn about generative models, not
just to specifically learn about super-resolution
or artistic style.
But remember, these things can be used to
create all kinds of images.
So one of the groups is interested in taking
a 2D photo and trying to turn it into something
that you can rotate in 3D, or at least show
a different angle of that 2D photo.
That's a great example of something that this
should totally work for.
It's just a mapping from one image to some
different image.
What would this image look like from above
versus from the front.
Keep in mind the purpose of this, just like
in Part 1 we learned about classification
which you could use for a thousand things,
now we're learning about generative models
that you can use for a different thousand
things.
Now any generative model you can build you
can make it better by adding on top of it
a GAN, a Generative Adversarial Network.
This is something I don't really feel like
it's been fully appreciated.
People I've seen generally treat GANs as a
different way of fitting a generative model.
But I think of this more as like why not create
your generative model using the kind of techniques
we've been talking about then ... Think of
it this way, think about all the artistic
style stuff we're doing, like my terrible
attempt at a Simpson's cartoon version of
a picture, it looked nothing like The Simpsons.
So what would be one way to improve that?
One way to improve that would be to create
2 networks.
There would be one network that takes our
picture, which is actually not The Simpsons,
and takes another picture that is The Simpsons.
And maybe we can train a neural network that
takes those 2 images and spits out something
saying, Is that a real Simpsons image or not?
This thing we'll call the discriminator.
So we could easily train a discriminator right
now.
It's just a classification network, just use
the same techniques we used in Part 1.
We feed it the 2 images and it's going to
spit out a "1" if it's a real Simpsons cartoon
and a "0" if it's Jeremy's crappy generative
model of The Simpsons.
That's easy, we know how to do that right
now.
So, okay, go ahead and build that.
Now, go and build another model ...
Answer: There's two images as inputs, yeah.
No, you would feed it one thing that's The
Simpsons and one thing that's a generative
output.
It's up to you to feed it one of each.
Alternatively, you could feed it one thing.
In fact, probably easier is to just feed it
one thing and it spits out, Is it The Simpsons,
or Isn't it The Simpsons.
And you could just mix and match them.
Actually it's the later that we're going to
do.
That's probably easier.
So let's redraw this.
We're going to have one thing which is either
Not a Simpsons or Is a Simpsons.
And we're going to kind of have a mix of 50-50
of those two.
And we're going to have something come out
saying, What do you think, is it real or not.
So this thing, this discriminator, from now
on we'll probably just generally be calling
it "D".
So it's a thing called "D".
We can think of that as a function.
D is a function that takes some input x (which
is an image, right) and spits out a "1" or
a "0", or maybe a probability.
So what we could now do is create another
neural network.
And what this neural network is going to do
is it's going to take as input some random
noise, just like all of our generators have
so far, and it's going to spit out an image.
And the loss function is going to be if you
take that image and stick it through D, did
you manage to fool it.
Could you create something where in fact we
wanted to say, Oh yeah.
Totally.
That's a real Simpsons.
So if that was our loss function ... The generator,
we'll call it "G".
It's just something exactly like our perceptual
losses style transfer model.
Could be exactly the same model.
The loss function is now going to be, take
the output of that and stick it through D
(the discriminator) and try to trick it.
So the generator is doing well if the discriminator
is getting in wrong.
So one way to do this would be to take our
discriminator and train it as best as we can
to recognize the difference between our crappy
Simpsons and real Simpsons.
And then get a generator and train it to trick
that discriminator.
But now at the end of that, it's probably
still not very good because you realize that
actually your discriminator didn't have to
be very good before because my Simpsons generators
were so bad.
So I could now go back and retrain the discriminator
based on my better generated images.
And then I could go back and retrain the generator.
And back and forth I go.
That is the general approach of a GAN, is
to keep going back between two things, which
is training a discriminator and training a
generator using a discriminator as a loss
function.
So we've got one thing which is a discriminator
on some image and another thing which is a
discriminator on a generator on some noise.
Answer: In practice, these things are going
to spit out probabilities.
Okay, that's the general idea.
In practice they found it very difficult to
do this like train a discriminator as best
as we can.
Stop.
Train the generator as best as we can.
Stop.
And so on and so forth.
So instead what the original GAN paper, called
"Generative Adversarial Nets" and here you
can see they've actually specified this loss
function, so here it is in notation.
They call it minimizing the generator whilst
maximizing the discriminator, this is what
minmax is referring to.
What they do in practice is they do it a batch
at a time.
So they have a loop.
We're going to go through a loop and do a
single batch, go through the discriminator,
that same batch stick it through the generator.
So we're going to do it a batch at a time.
So let's look at that.
So here's the original GAN from that paper.
And we're gong to do it on MNIST.
And what we're going to do is we're going
to start from scratch to create something
which can create images which the discriminator
cannot tell whether they're real or fake.
And it's a discriminator which has learnt
to be good at discriminating real from fake
pictures of MNIST images.
So we load it into MNIST and the first thing
they do in the paper is they just use a standard
multi-layer percepteron.
I'm just going to skip over that and let's
get to the percepteron.
So here's our generator.
It's just a standard multi-layer percepteron.
And here's our discriminator, which is also
a standard multi-layer percepteron.
The generator has a sigmoid activation, so
in other words we're going to spit out an
image where all of the pixels are between
0 and 1.
So if you want to print it out, just multiply
it by 255, I guess.
So there's our generator, there's our discriminator.
There's then the combination of the two.
So take the generator and stick it into the
discriminator, we can just use sequential
for that.
This is actually therefore the loss function
that I want on my generator.
So generate something and see if you can fool
the discriminator.
So there's all my architectures set up.
So the next thing I need to do is set up this
thing called "train" which is going to do
this adversarial training.
Let's go back and have a look at train.
So what train is going to do is it's going
to go through a bunch of epochs.
Notice here I wrap it in this tqdm, this is
the thing that creates the progress bar.
Doesn't do anything else, just creates the
progress bar.
We learned about that last week.
The first thing I need to do is to generate
some data to feed the discriminator.
So I create a little function for that, and
here's my little function (data_D), and it's
going to create a little bit of data that's
real and a little bit of data that's fake.
So my real data.
Okay, let's go into my actual training set
and grab some randomly selected MNIST digits.
That's my real bit.
And then, let's create some fake.
So "noise" is a function that I've just created
up here which gets 100 random numbers.
So let's create some noise, call G.predict.
And then so I concatenate the two together.
So now I've got some real data and some fake
data.
And so this is going to try and predict whether
or not something is fake.
So "1" means fake and "0" means real.
So I'm going to return my data and my labels,
which is a bunch of 0's to say they're all
real and a bunch of 1's to say they're all
fake.
So there's my discriminator data.
So go ahead and create a set of data for the
discriminator and do one batch of training.
Now I'm going to do the same thing for the
generator.
But when I train the generator, I don't want
to change the discriminator's weights.
So make_trainable simply goes through each
layer and says it's not trainable, to make
my discriminator non-trainable and do one
batch of training where I'm taking noise as
my inputs and my goal is to get the discriminator
to think that they are actually real.
So that's why I'm passing in a bunch of 0s,
because remember 0 means real.
And that's it.
And then make the discriminator trainable
again.
So keep working through this.
Train the discriminator on a batch of half
real half fake.
Then train the generator to try and trick
the discriminator using all fake.
Repeat.
So that's the training loop.
That's the basic GAN.
Because we use tqdm we get a nice progress
bar.
We can plot out.
I kept track of the loss at each step so there's
our loss for the discriminator.
And there's our loss for the generator.
So the question is what do these loss curves
mean?
Are they good or bad?
How do we know.
The answer is for this kind of GAN, they mean
nothing at all.
The generator could get fantastic but it could
be because the discriminator is terrible.
They don't really know whether each one is
good or not, so like even the order of magnitude
of both of them is meaningless.
So these curves mean nothing.
The direction of the curves mean nothing.
And this is one of the real difficulties with
training GANs.
Here's what happens when I plot 12 randomly
selected random noise vectors stuck through
there.
We have not got things to look terribly like
MNIST digits and they also don't look terribly
much like they have a lot of variety.
This is called "mode collapse".
Very common problem when training GANs.
And what it means is that the generator and
the discriminator have kind of reached a stalemate.
Neither of them basically knows how to go
from here.
And in terms of optimization, we've basically
found a local minimum.
So, okay, that was not very successful.
Can we do better?
So the next major paper that came along was
this one, "Unsupervised Representation Learning
with Deep Convolutional Generative Adversarial
Networks".
So this created something they call DCGANs
and the main page that we have to look at
here is page 3, where they say "Core to our
approach is" doing these three things.
And basically what they do is they do exactly
the same thing as GANs, but they do 3 things.
One is to use the kind of, in fact all of
them is to learn the tricks that we've been
learning for generative models.
Use an all convolutional net, get rid of maxpooling
and use strided convolutions instead, get
rid of fully connected layers and use lots
of convolutional features instead and add
in batch norm.
And then use a CNN rather than NLP, so here
is that.
This will look very familiar, it looks just
like lesson's stuff.
So the generator is going to take in a random
grid of inputs, it's going to do a BatchNorm,
upsample (you'll notice I'm doing even newer
than this paper, I'm doing the UpSampling
approach because we know that's better), upsample,
1x1 conv, BatchNorm, upsample, 1x1 conv, BatchNorm,
and then a final conv layer.
Discriminator basically does the opposite.
Which is some 2x2 subsamplings, so no downsampling
in the discriminator.
Another trick that is mentioned (I think it
is mentioned in the paper) is before you do
the back-and-forth batch for the discriminator,
and a batch for generator, is to train the
discriminator for a fraction of an epoch.
Like do a few batches through the discriminator
so at least it knows how to recognize the
difference between a random image and a real
image a little bit.
So you can see here, I actually just start
by calling discriminator .fit (CNN_D.fit)
with just a very small amount of data.
So this is just kind of like bootstrapping
the discriminator.
And then I just go ahead and call the same
train as we had before with my better architectures.
And again these curves are kind of totally
meaningless.
But we have something which if you squint
you could almost convince yourself that's
a "5".
So until a week or two before this course
started, this was kind of about as good as
we had.
People were much better at the artisanal details
of this than I was, and indeed there's a whole
page called ganhacks which had lots of tips.
But then a couple weeks before this class
started (as I mentioned in the first class),
along came the Wasserstein GAN.
And the Wasserstein GAN got rid of all these
problems.
And here is the Wasserstein GAN paper.
And this paper is quite an extraordinary paper.
It's particularly extraordinary because (and
I think I mentioned this in the first class
of this part), most papers either tend to
be math theory which goes nowhere or kind
of nice experiments and engineering, where
the theory bit is kind of hacked on at the
end and kind of meaningless.
This paper is entirely driven by theory.
And then the theory, they go on to show this
is what the theory means, this is what we
do, and suddenly all the problems go away.
The loss curves are going to actually mean
something and we're going to be able to do
what I said we wanted to do right at the start
of this GAN section which is to train the
discriminator a whole bunch of steps, and
then do a generator, and then the discriminator
a whole bunch of steps, and then do a generator.
And all that is going to suddenly start working.
How do we get it to work?
So in fact, despite the fact that this paper
is both long and full of equations and theorems
and proofs, and there's a whole bunch of appendices
at the back with more theorems and proofs,
there's actually only two things you need
to do.
One is remove the log from the loss file.
Rather than using cross-entropy loss, we're
just going to use mean square error.
That's one change.
And the second change is we're going to constrain
the weights so that they lie between -.01
and +.01.
We're going to constrain the weights to make
them small.
Now in the process of saying that's all we're
going to do is to not give credit to this
paper.
Because what this paper is they figured out
that's what we need to do.
And on the forums, some of you have been reading
through this paper.
I've already given you some tips, there's
a really great walk-through, I'll put it on
our wiki, that explains all the math from
scratch, but basically what the math says
is this.
The loss function for a GAN is not really
the loss function you put into Keras.
Like we thought we were just putting in a
cross-entropy loss function, but in fact for
a GAN what we really care about is the difference
between two distributions.
The difference between the discriminator and
the generator.
And the difference between two loss functions
has a very different shape than the loss function
on its own.
So it turns out that the difference between
the two loss functions, the two cross-entropy
loss functions is something called the Jensen-Shannon
distance.
And this paper shows that that loss function
is hideous.
It is not differentiable and it does not have
a nice smooth shape at all.
So this kind of explains why it is that we
keep getting this mode collapse and failing
to find nice minimums.
Basically, mathematically this loss function
does not behave the way a good loss function
should.
Previously we've not come across anything
like this because we've been training a single
function at a time.
We really understand those loss functions,
mean-square-error, cross-entropy.
Even though we haven't always derived the
math in detail, plenty of people have.
We know that they're nice and smooth and they
have pretty nice shapes and they do what you
want them to do.
In this case, by training two things adversarily
to each other we're actually doing something
quite different and this paper just absolutely
fantastically shows, with both examples and
with theory, why that's just never going to
work.
Answer: Yes but even the cosine distance ... So
the cosine distance is the difference between
two things whereas these distances that we're
talking about here are the distances between
two distributions, which is a much more tricky
problem to deal with.
The cosine distance, actually if you look
at the notebook during the week you'll see
it's basically the same as the Euclidean distance
but you normalize the data first, so it has
all the same nice properties as the Euclidean
distance.
So one thing that's fun is that the authors
of this paper released their code in PyTorch.
Luckily, PyTorch the first kind of pre-release
came out in mid-January.
You won't be surprised to hear that one of
the authors of the paper is the main author
of PyTorch, so he was writing this before
he even released the code.
There's lots of reasons we want to learn PyTorch
anyway so here's a good reason.
So let's look at the Wasserstein GAN in PyTorch.
Most of the code, in fact other than this,
pretty much all of the code I'm showing you
in this part of the course is very loosely
based on lots of bits of other code which
I had to massively rewrite because all of
it was wrong and hideous.
This code I actually only did some minor refactoring
to simplify things, so this is actually very
close to their code.
So it was a very nice paper with very nice
code.
So before we look at the Wasserstein GAN in
PyTorch, let's look briefly at PyTorch.
Basically what you're going to see is that
PyTorch looks a lot like Numpy, which is nice.
We don't have to create a computational graph
using variables and placeholders and later
on and launch in a session and blah blah blah.
I'm sure you've seen by now with Keras with
TensorFlow you try to print something out,
maybe an output, it just prints out like "Tensor"
and tells you home many dimensions it has.
That's because all that thing is is the symbolic
part of a computational graph.
PyTorch doesn't work that way.
PyTorch has what's called a define-by-run
framework.
It's basically designed to be so fast to take
your code and compile it that you don't have
to create that graph in advance.
Like every time you run a piece of code, it
puts it on the GPU, runs it, sends it back,
all in one go.
So it makes things look very simple.
This is a slightly cut down version of the
PyTorch tutorial that PyTorch provides on
their website so you can grab that from there.
So rather than creating np.array, you create
torch.Tensor.
But other than that, it's identical.
So here's a random torch.Tensor.
Rather than .shape it's got .size.
But you can see it looks very similar, right.
And so unlike in TensorFlow or Theano, we
can just say "x+y" and there it is.
We don't have to say "z=x+y", "f=function(x,y)",
takes x and y as inputs and there is its output...
No, just go "x+y" and there it is.
So you can see why it's called define-by-run.
We just provide the code and it just runs
it.
Generally speaking most operations in Torch
as well as having this infix version, there's
also a prefix version.
These two are exactly the same thing.
You can often add an "out=" and that puts
the results in this pre-allocated memory.
We've already talked about why it's really
important to pre-allocate memory.
It's particularly important on GPUs.
So if you write your own algorithms in PyTorch,
you'll need to be very careful of this.
Perhaps the best trick is that you can stick
an underscore on the end of most things and
it causes it to do it in-place.
This is basically y plus equals x.
So there's some good little tricks.
You can do slicing just like Numpy.
You can turn Numpy stuff into Torch tensors
(and vice versa) by simply going .numpy.
One thing to be very aware of is that "a"
and "b" are now referring to the same thing.
So if I now add underscore to in-place a plus
equals 1, it also changes b.
Vice versa you can turn Numpy into Torch by
calling Torch from Numpy.
And again, same thing.
If you change the Numpy, it changes the Torch.
All of that so far has been running on the
CPU.
To turn anything into something that runs
on the GPU you chuck .cuda at the end of it.
So this "x+y" just ran on the GPU.
Where things get cool is that something like
this knows not just how to do that piece of
arithmetic, but it also knows how to take
a gradient of that.
So to make anything into something which calculates
gradients, you just take your Torch tensor,
wrap it in Variable and add this parameter
to it.
From now on, anything I do to x, it's going
to remember what I did so that it can take
the gradient of it.
So for example, x+2.
I get back 3, just like a normal tensor.
So a variable and a tensor have the same API,
except that I can keep doing things to it.
Squared times 3, .mean.
Later on, I can go .backward and .grad and
I can get the gradient.
So that's the critical difference between
a tensor and a variable.
They have exactly the same API, except variable
also has .backward and .grad gets you the
gradient.
So when I say .grad, the reason that this
is "dout/dx" is because I typed "out.backward()".
So this is the thing that the derivative is
with respect to.
So this is kind of crazy.
You can do things like while loops and get
the gradients of them.
So this kind of thing, pretty tricky to do
with TensorFlow or Theano, these kind of computation
graph approaches.
So it gives you a whole lot of flexibility
to define things in much more natural ways.
So you can write PyTorch just like you're
writing regular old Numpy stuff.
It has plenty of libraries so if you want
to create a neural network, here's how you
do a CNN.
I warned you early on that if you don't know
about OO and Python you need to learn it and
so here's why.
Because in PyTorch, everything's kind of done
using OO.
I really like this.
Like in TensorFlow, they kind of invent their
own weird way of programming, rather than
use Python OO.
Whereas PyTorch just goes, Oh we already have
these features in a language, let's just use
them.
It's way easier, in my opinion.
So to create a neural net, you create a new
class.
You derive from Module and then in the constructor,
you create all of the things that have weights.
So conv1 is now something that has some weights,
it's a 2D conv. conv2 is something with some
weights.
fc1 is something with some weights.
So there's all of your layers.
And then you get to say exactly what happens
in your forward pass.
Now because max_pool2d doesn't have any weights
and relu doesn't have any weights, there's
no need to define them in the initializer.
You can just call them as functions.
But these things have weights, so they need
to be stateful in the system.
So in my forward pass, you literally just
define what are the things that happen.
.view is the same as .reshape.
So like the whole API has different names
for everything.
It was mildly annoying for the first week,
but you kind of get used to it.
So .reshape is called .view.
During the week, if you try to use PyTorch
and you like, How do you say blah in PyTorch,
and you can't find it, feel free to post on
the forum.
Having said that, PyTorch has its own discourse
based forum (https://discuss.pytorch.org).
As you can see, it is just as busy and friendly
as our forums.
People are posting on these all the time.
So I find it a really great, helpful community.
So feel free to ask over there or over here.
So you can then put all of that computation
onto the GPU by calling .cuda.
You can then take some input, put that on
the GPU with .cuda.
You can then calculate your derivatives, calculate
your loss.
And then later on you can optimize it.
This is just one step of the optimizer, so
we have to kind of put that in a loop.
So there's the basic pieces.
At the end here there's a process, but I think
more fun will be to see the process in a Wasserstein
GAN.
So here it is.
I've kind of got this torch_utils thing that
you'll find in github which has the basic
stuff you'll want for Torch all there, so
you can just import that.
So let's get the Wasserstein GAN working.
So we set up the batch size, the size of our
image, the size of our noise vector.
Look how cool it is, I really like this.
This is how you import datasets.
It has a datasets module already in the torch-vision
library.
Here's the CIFAR10 dataset, it will automatically
download it to this path for you if you say
"download=True".
And rather than having to figure out how to
do the preprocessing, you can create a list
of transforms.
So I think this is a really lovely API.
The reason that this is so new yet has such
a nice API is because this comes from a Lua
library called Torch that's been around for
many years.
So these guys started off by copying what
they already had and what already worked well.
So I think this is very elegant.
I've got two different things you can look
at here.
They're both from the paper.
One is CIFAR10 with these tiny little images.
Another is something we haven't seen before
which is called LSUN, which is a really nice
dataset.
It's a huge dataset with millions of images,
3 million bedroom images, for example.
So we can use either one.
This is pretty cool.
We can then create a DataLoader, say how many
workers to use (we already know what workers
are) and this is all built in to the framework.
Now that you know how many workers your GPU
or CPU likes to use, you can just go ahead
and put that number in here.
You can use your CPU to load in this data
in parallel in the background.
We're going to start with CIFAR10, so we've
got 47 thousand of those images.
So I have just put the definitions of the
discriminator and generator architectures
into a separate Python file, dcgan.py.
We're going to skip over very quickly because
it's really straightforward.
Here's a conv block that consists of a Conv2d,
a BatchNorm2d and a leakyReLU.
So in my initializer I can go ahead and say,
Okay we'll start with a conv block.
Optionally we have a few extra conv blocks.
This is really nice.
Here's a while loop that says keep adding
more upsampling blocks
until you've got as many as you need.
So that's a nice kind of use of a while loop
to simplify creating our architecture.
And then a final conv block at the end to
actually create the thing we want.
And then this is pretty nifty.
If you pass in ngpu>1, then it will call parallel.data.parallel
passing in those gpu_ids and it will do automatic
multi-GPU training.
So this is by far the easiest multi-GPU training
I've ever seen.
So that's it, that's the forward pass.
Answer: We'll learn more about this over the
next couple of weeks.
In fact, given that we're a little short of
time, let's discuss that next week and let
me know if you don't think we cover it.
Okay, here's the generator.
Looks very similar.
Again there's a while loop to make sure we've
gone through the right number of deconv blocks.
So this is actually interesting.
This would probably be better off with an
upsampling block followed by a 1x1 convolution.
So maybe at home you could try this and see
if you get better results because this has
probably got the checkerboard pattern problem.
Okay, so there's our generator and our discriminator.
It's only 75 lines of code.
Nice and easy.
You know, everything's a little bit different
in PyTorch.
If we want to say what initializer to use,
again we're going to kind of use more decoupled.
Maybe at first it's a little more complex,
but there's less things you have to learn.
In this case we can call something called
.apply, which takes some function and passes
it to everything in our architecture.
So this function is something that says, Is
this a Conv2d or a ConvTranspose2d, if so
use this initialization function.
Or if it's a BatchNorm, use this initialization
function.
So again, everything's a little bit different.
There isn't a separate initializer parameter;
this is in my opinion much more flexible,
I really like it.
So as before we need something that creates
some noise.
So let's go ahead and create some fixed noise.
We're going to have an optimizer for the discriminator,
we've got an optimizer for the generator.
Here is something that does one step of the
discriminator, right.
So we're going to call the forward pass and
then we're going to call the backward pass,
and then we return the error.
Just like before, we've got something called
make_trainable.
This is how we make something trainable or
not trainable in PyTorch.
And just like before, we have a train loop.
The train loop has got a little more going
on, partly because of the Wasserstein GAN,
partly because of PyTorch.
But the basic ideal is the same.
For each epoch, for each batch, make the discriminator
trainable.
And then this is the number of iterations
to train the discriminator for.
So remember I told you one of the nice things
about the Wasserstein GAN is that we don't
have to do 1 batch discriminator, 1 batch
generator, 1 batch discriminator, 1 batch
generator, but we can actually train the discriminator
properly with a bunch of batches.
And in the paper, they suggest using 5 iterations,
or 5 batches of discriminator training each
time through the loop, unless you're still
on the first 25 iterations.
If you're still in the first 25 iterations,
do 100 batches.
And they also say, from time-to-time do 100
batches.
So it's kind of nice.
By having the flexibility here to change things,
we can do exactly what the paper wants us
to do.
So basically, first we're going to train the
discriminator carefully.
And we will also from time-to-time train the
discriminator very carefully.
Otherwise, we'll just do 5 batches.
So this is where we go ahead and train the
discriminator.
And you'll see here we clamp (this is the
same as clip) the weights in the discriminator
to fall in this range.
And if you're interested in reading the paper,
the paper explains that basically the reason
for this is that their assumptions are only
true in this kind of small area.
So that's why we have to make sure the weights
stay in this small area.
So then we go ahead and do a single step with
the discriminator.
Then we create some noise.
And then we do a single step with the generator.
We get our fake data for the discriminator
and we can subtract the fake from the real
to get our error with the discriminator.
So there's 1 step of the discriminator.
We do that either 5 or 100 times.
Make our discriminator not trainable and then
do one step of the generator.
You can see here we call generator with some
noise and then pass it into the discriminator
to see if we tricked it or not.
So during the week you can look at these two
different versions and you're going to see
the PyTorch and the Keras version are basically
the same thing.
The only difference is ... two things.
One is the presence of this clamping and the
second is that the loss function is mean-square-error,
rather than cross-entropy.
So let's see what happens.
Here is some examples from CIFAR10.
And so they're certainly a lot better than
our crappy DCGAN MNIST examples, but they're
not great.
Why are they not great?
Probably the reason they're not great is because
CIFAR10 has quite a few different kinds of
categories of different kinds of things.
And so it doesn't really know what it's going
to be drawing a picture of.
Sometimes I guess it kind of figures it out.
Like I guess this must be a plane, I think.
But a lot of the time it hedges and kind of
draws a picture of something that kind of
looks like it might be a reasonable picture
but it's not a picture of anything in particular.
On the other hand, the LSUN dataset has 3
million bedrooms.
So we would hope that when we train the Wasserstein
GAN on LSUN bedrooms we might get better results.
Here's the real CIFAR10, by the way.
So here are our fake bedrooms.
And they are pretty freaking awesome.
So literally these started out as random noise,
and every one has been turned into ... like
that's definitely a bedroom.
And they're all different.
And then here is the real bedrooms, to compare.
So we can see here, imagine if we took this
and stuck it on the end of any kind of generator.
You could really use this to make your generator
much more believable.
Any time you look at it and say, Oh that doesn't
look like the real X, maybe you could try
using a WGAN to try and make it look more
like a real X.
So this paper is so new and like so important
...
So here's the other thing, the loss function
for these actually makes sense.
The discriminator and the generator loss functions
actually decrease as they get better.
So you can actually tell if your thing is
training properly.
You can't exactly compare two different architectures
to each other still, but you can certainly
see that the training curves are working.
So now that we have (in my opinion) a GAN
that actually really works reliably works
for the first time ever, I feel that this
changes the equation for what generators can
and cannot do.
And this has not been applied to anything
yet.
So you can take any old paper that produces
3D outputs or segmentations or vector outputs
or colorization and add this and it would
be great to see what happens because none
of that's been done before.
It's not been done before because we haven't
had a good way to train GANs before.
You know I think this is kind of something
where anybody who's interested in a project,
yeah, this would be a great project.
And something that maybe you can do reasonably
quickly.
Another thing you could do as a project is
to convert this into Keras.
You can take the Keras DCGAN notebook that
we've already got and change the loss function
and the weight clipping and try training on
this LSUN bedroom dataset and you should get
the same results.
And then you can add this on top of any of
your Keras stuff.
There's so much that you could do this week.
I don't feel I want to give you an assignment
per se because there's 1000 assignments you
could do.
As per usual, you should go back and look
at the papers.
The original GAN paper is a fairly easy read.
There's a section called Theoretical Results
which is kind of like the pointless math bit,
like here's some theoretical stuff.
It's actually interesting to read this now
because you go back and you look at this stuff
where they prove various nice things about
their GAN.
So they talk about how the generative model
perfectly replicates the data generating process.
It's interesting to go back and look and say,
They've proved these things but it turned
out to be pointless, it still didn't work,
it didn't really work.
It's kind of interesting to look back and
say how is it ... That's not to say this isn't
a good paper.
It's a good paper, but it is interesting to
see like when is the theoretical stuff useful
and when not.
Then you look at the Wasserstein GAN theoretical
sections and it spends a lot of time talking
about why their theory actually matters.
So they have this really cool example where
they say, Let's create something really simple.
What if you wanted to learn parallel lines.
And they show why it is that the old way of
doing GANs can't learn parallel lines.
Then they show how their different objective
function can learn parallel lines.
So I think anybody who's kind of interested
in getting into the theory a little bit, it's
very interesting to look at why the Proof
of Convergence showed something, but it didn't
show something that turned out to matter.
Whereas in this paper, the theory turned out
to be super-important and basically created
something that allowed GANs to work for the
first time.
So there's lots of stuff you can get out of
these papers if you're interested.
In terms of the notation, we might look at
some of the notation a little bit more next
week.
But if we look for example at the, let's see
if we can find the algorithms sections.
I think in general, the bit I find most useful,
not being much of a math guy, is the bit when
they actually write the pseudo-code.
Even that is useful to learn some kind of
nomenclature.
So for each iteration, for each step ... What
does this mean, sample noise samples from
noise prior?
So there's a lot of probability nomenclature
which you can very quickly translate.
A prior simply means np.random.something.
So in this case, probably like np.random.normal.
This just means some random number generator
that you get to pick.
This one here, sample from a data generating
distribution.
That means randomly pick some stuff in your
array.
So these are the two steps, generate some
random numbers and then randomly select things
from your array.
And the bit where it talks about the gradient
you can kind of largely ignore, except the
bit in the middle is your loss function.
So you can see here, these things here is
your noise, so noise, generator on noise,
discriminator on generator on noise, so there's
the bit where we're trying to fool the discriminator.
And we're trying to make that trigger, that's
why we do 1 minus.
And then here's getting the discriminator
to be accurate, because these x's is the real
data.
So that's the math version of what we just
learned.
The Wasserstein GAN also has an algorithm
section and so it's kind of interesting to
compare the two.
So here we go, here's the Wasserstein GAN,
here's the algorithm and basically this says
exactly the same thing as the last one said,
but I actually find this one a bit clearer.
Sample from the real data, sample from your
priors.
Hopefully that's enough to get going.
Look forward to talking on the forums and
see how everybody gets along.