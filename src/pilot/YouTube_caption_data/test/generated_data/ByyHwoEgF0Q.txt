all right all right let's get live
are we going live
looks like we are
okay
so we are going to be working on
some neural network classification
problems with pytorch
now if you're not sure about pi torch or
about neural network classification that
is fine we're going to code this all
from scratch and all of the materials
that you need
at learnpytorch.io
so i'll put this in the chat otherwise
just go to learn pytorch.oh we are
working on notebook number two and if
you have zero experience with pi torch
and mono i would suggest going through
zero zero zero one zero two but let's
start with uh
zero two
now this is all the materials the
backing materials that
we're going to be working through
but down the bottom of this so this is a
course that i'm working on to learn pie
torch
it's still a work in progress but right
now i've done three modules almost to
completion you'll see the tk's so far
online pytorch.oh if you're in the
future you're probably not going to see
the tk's because it's finished tk just
means that i need to finish it
um
but yeah there's a github for it as well
pi torch deep learning
mr deburk slash pi talk
deep learning
there's gonna be about eight notebooks
by the time that it's finished to start
right from the beginning all the way up
to
a few milestone projects with pytorch
but now let's uh i'm gonna put this
stuff over here
so that i don't mess anything up
if you're joining in
say hello in the chat and let me know if
you can't hear or can't see anything
but without any further ado
let's get started
so
i've got all the code here
so this notebook gets built into this
like online book
then we've got all the headings here
so we're going to be working through a
pie torch workflow
um
so step one get data ready turn it into
tenses step two build or pick a trade
paying model to suit your problem 2.1
pick a loss function optimizer a loss
function measures how wrong your model
is so if you're trying to predict
something remember classification i
haven't actually said this so why am i
saying remember classification is
predicting whether something is one
thing or another say for example you had
a photo of a dog and a photo of a cat
you want to classify which photo is
which which one's the dog which one's a
cat or if you had an email and you want
to classify which one's spam and not
spam
so
a loss function measures how wrong your
model is so if a loss is high it means
your model is quite wrong if it's low it
means that your model is doing quite
well
build a training loop so that means a
training loop takes steps over your data
we're going to see all this in code in a
second so as in you start with your data
set here say it's photos of cats and
dogs your training loop will iterate
over your data so go through each of
those photos and allow your model to
make predictions on whether those images
are a cat of dog the loss function
measures how wrong those predictions are
and then the optimizer does what its
name says it optimizes your model so it
pushes your model in the right direction
of predicting uh the right prediction of
cat or dog we fit the model to make a
prediction so go over the training data
we evaluate it we can improve through
experimentation lots of things you could
do here and then we save and reload the
train model
but now let's get into the exercises so
down here you can if you're looking at
learn pytorch.oh you can go to the
exercises and
this is what we're going to be working
through
all exercises should be completed using
device agnostic code what does that mean
doesn't matter let's just start going
through them
i've got a google colab notebook here
which i'm assuming that you can see
but if not
i'll move my head
i'm just watching the stream to see if
you can see this i just have to adjust
my pants as well
but let's get started i'll zoom right in
it should be enough
okay so google collab is the easiest way
to get started with this notebook you
can run them locally however
there's a button right at the top of
these that you can just go open in
google colab
so
that's what i've done here
you can click open in google collab
start a new notebook and google colab is
a notebook that's going to give us
access to
uh writing python code in an interactive
way
alex what's going on
nice to see you if you're watching this
in the future uh this originally is a
live stream
so what i'm going to do is i'm going to
use a gpu
so hardware accelerator gpu that's going
to make all my code run faster
if you don't have a gpu on your local
machine google collab does that for you
so
exercise solutions
so all exercises should be completed
using device agnostic code what does
that mean
so in pi torch
you have to decide where your data and
model live so
by default everything
starts on the cpu the cpu is computer
processing unit the cpu is not as fast
as the gpu for performing certain
numerical calculations at the end of the
day that's what a new
neural network does it performs a series
of numerical calculations on data which
is in the form of tensors and if you'd
like that
those calculations to be done faster you
should use
a gpu
so we have access to a gpu
let's check for a gpu
so nvidia smi
you've been slacking on the db streams
hey that's fine i've been slacking on
doing the streams but i'm going to get
straight back into it so here's i've got
access to a nvidia gpu powered by cuda
which is excellent
and then we're going to now
do
base imports
so we're going to import torch
because this is the exercise we're going
to work through
uh make a binary classification data set
with scikit loans make moon function so
let's see what that is i actually don't
want that there
i want it over here
import import torch
and we're going to set up
device agnostic code here
so device equals cuda
if torch
cuda dot is available
now
if you're not sure what cuda is else cpu
so then we can check what the device is
invalid syntax oh i spelt else wrong
um
what is cuda
sakura is
uh parallel programming for gpus i think
that's a good enough summary for me
what is cuda so nvidia is basically the
number one gpu company
and their programming system is called
cuda and if you want to use their gpus
you basically have to write cuda code
pytorch takes care of all that for us so
we can write python or pytorch pi in pi
torch sensor python
and we can leverage cuda so this is
going to set up device agnostic code
device equals cuda
which is nvidia's programming language
for gpus
if torch.cuda which is pi torch's cuda
module is available else
cpu
and see because we have activated a gpu
by going runtime change runtime type gpu
we have cuda available now let's go on
to exercise number one
make a binary classification data set
with scikit learn's make moon function
i like this
one we'll turn this into markdown
and
then i'll go here
i might copy the exercises in here from
the original notebook i think this is a
good idea
that way i'll have the links and the
code formatting and everything like that
that'll be
that'll look much better
there we go
okay
so now let's jump into the make moons
function and see what we can do
is there example usage not really
[Music]
the data set should have a thousand
samples so we should write here
num samples equals a thousand uh random
seed equals 42. we can use these all
later on
and
let's create it so x y
equals oh we need we'll need psychic
loan
so import sklearn
uh
and we could just just import
from sklearn.datasets
um we'll import make moons
excellent and then we have a simple
interface to make some classification
data here we're going to see what this
looks like in a minute
make moons and
n samples equals num samples
and then we can go
random state equals
random seed we're using 42. so the
random seed what's that going to do the
random seed
so
computers use pseudo randomness
to generate numbers so if you tell a
computer to generate random numbers
because a computer is deterministic
it's actually pseudo randomness and i'll
let you look that up
the random seed flavors our computer's
randomness so that means if we were to
run this same function again we would
get the same amount
the same different random points and
this is going to be confusing unless we
look at it so let's do that i'm going to
introduce a little bit of noise
i think
0.03 might be enough but we'll find out
now let's just have a look at what x and
y look like maybe the first 10 samples
of each because we have a thousand of
each due to the end samples parameter
okay wonderful so we have an array of x
values here an array of y values here so
x has two features
so this is
one
two
now what happens if we turn this into a
data frame so
let's go
df
data df
turn data into a data frame
we can visualize it this way
import pandas as pd
and data df is going to be pd data frame
and we want a dictionary we'll put x1
or x0
is equal to
x and everything on the zeroth x
zeroth column
and then
we'll go
x one which is x feature one is going to
be everything
on the first index column
and then we'll go y
for label
equals y
what's this look like
okay beautiful so we have x y x zero x
one y
uh these are two features which lead to
this label so we have a supervised
learning problem and we're working with
binary classification how do we know
that it's binary classification
so this is make a binary classification
we only have zeros and ones so that
means our x features could be zero or
they could be one
binary classification means one or the
other two options
and so now let's
visualize visualize visualize visualize
visualize visualize the data
on a plot
so we want
from
no import map plot lib
dot pi plot
as plt plt dot plot uh what happens if
we just go x y what will that do
okay all over the place
i think we need to go
x zero plot that first and then plt dot
plot x um
one first i don't
actually know if this will work
no that's just the exact same thing that
we got before okay i think i know what
to do we need to go
x01 then we need to color it with y
rbga should have a length three or four
now this is where i need to go back to
my own code and reference it
so all of these exercises are based off
what we did here
what have we got wrong here
oh scatter
we want to scatter plot
there we go we've got two moons and
let's color these to be
red and blue dots so this is just taking
from
this is where the exercises are based
off right that's the point of code is
the point of writing code of course is
not to repeat the same code that you
write over and over again is to write it
once and then reference back to it if
you need to
so we're referencing back to the code
that we previously wrote
cmap equals plt.cm
red yellow blue
that just means okay
uh maybe we increase the noise a little
bit i think i'd like more noise
so
noise equals
seven
yeah there we go okay so now this is
what we're trying to do we're trying to
build a model that's going to separate
the blue and red dots and you might
think that this is very trivial daniel
we're they're already separate and yes
you would be 100 correct in saying that
they are already separate but how about
we build a model that can draw a line
through here
so separating them like the curves that
they are
well let's give it a shot we'll go back
to the exercises what are we up to
i want you to have a thing about this
actually before we build a model is can
this be separated by a straight line
this is a very important point in neural
networks actually by the way
can you draw a straight line to separate
these two these two red and blue dots
just think about that we'll come back to
that in a second
so number two is
let's go into here
build a model
build a model by subclassing nn module
that incorporates nonlinear activation
functions and is capable of fitting data
to the cr to capable of fitting the data
you created in one
feel free to use any combination of pie
torch layers linear and non-linear
ah there's a little hint there
okay first of all we need to discuss
what linear is
so linear is a straight line let's
create this have we got torch if not
we'll just get it again
um
linear line
equals
torch
dot
a range
0 to 100 and the step is one
and then plt
dot plot
linear line
right that's linear linear means
straight and then non-linear means
not straight so let's do a little
we'll use an activation function a
nonlinear activation function or maybe
we go
this can be a bit better
negative 100 to 1.
close that's right we don't need that
i'm just discussing the difference
between linear and non-linear here so
could this straight line if we built a
model to to to predict
uh
to separate a model to separate these
two data sources can it can it do it
with a straight
line mean
it would do most of them
but not all of them
so
that's a problem you might have seen
this uh line of best fit used somewhere
before like in high school mathematics
but we need something that has
some curves in it don't we now how can
we get some curves well that's when we
bring in non-linearity so if we go
use a non-linear activation function
like torch sigmoid
on the linear line what does this look
like
oh yeah we got some curves look at that
now what if we used relu
now
i've just thrown a lot of information at
you to begin with but essentially the
whole
crux of neural networks
is combining
linear
straight line functions and non-linear
non-straight line functions
together
to model some kind of data now our data
set is quite simple there's only two
features x0 and x1 but what if you had x
0 to 10 000.
how do you model that in the same
process you combine lots of straight and
lots of
non-linear lines
the question is what could you draw if
you had an unlimited amount of straight
and non-straight lines
you could draw a pretty intricate
pattern couldn't you
well let's see it in action hey
so
what do we got here build a model by
subclassing and then module that
incorporates
nonlinear activation functions is
capable of fitting the data you create
at once so actually we've missed two
steps up here
um
split the data into pi torch tensors
or turn the data
so let's do that and then we'll split it
into
so
turn data into tensors
um
so we have x
equals torch tensor
x and we're going to set d type equals
torch.float torch.float is float32 which
is the default data type in pi torch
which is more than enough for us now
there are other data types but i'll let
you dig into them
and let's split the data
into train and test sets
why do we do this well think of it like
you're taking an exam if we want our
model to be able to learn things in the
real world
uh we don't want it to we need some
portion of the data that we can evaluate
it so instead of just building a model
in a notebook that works really well on
these blue and red dots what if we had
someone who was
just wanted to randomly give us some
blue and red dots
how could we evaluate how our model
would go on those
while we kind of
pseudo-evaluate it i say pseudo what's a
better word
i can't think of the right word but a
training set is like the course
materials that you take before you take
the final exam
you wouldn't really be testing your
knowledge if you could see the final
exam before you took it so when we build
our model we train it on the training
data and then we evaluate it on the
testing data just like when you learn
the course materials and then you test
your knowledge on the final exam
um let's go train split we'll use 80 20.
so 80 of the data will be used for a
training and 20 of the data will be used
for testing
equals
um
int 0.8 so 80
times length of x
and i think that should be enough what
does that look like
800 okay beautiful
so then we want to go
oh we could actually just do random
let's do that
so from sklearn dot model selection
import train test split and you're like
daniel why are we just doing blue and
red dots well because we're just
learning the principles of a pie torch
workflow
so let's go back up here
this is what we're learning
just the workflow you can adjust this
workflow to any data set you're using
later on
xtrain x test that's actually a very
good way to start with machine learning
concepts is to
start with a small data set first and
then use a larger data set later on so
test size is 20 percent and the random
state here is going to be equal to the
random seed
wonderful
so this should give us
an 80 20 split
random split that is but
using a random seed
x test
um let's go y train and lent y test it
should be 800 800 200 200
okay
clone
okay
oh there were already tenses so the only
reason that happened is because
we got a clone warning
but that's because they were already
tenses
okay beautiful yahweh chic what's going
on my friend
there we go so we have
800 training samples 200 testing samples
800 training labels and
200 testing labels
heya chic
thank you for the subscription my friend
um now we have a data set let's get rid
of this
we have training and test data let's go
back to the exercises
so let's build a model
i'm just going to start right from the
top import torch every time we're going
to just import things even though we
don't need to write this every time but
i'm going to do it so you get some
familiarity with doing it so we're going
to create a class
because we want to model our moon data
let's call it
moon model
maybe moon model v0 because it's the
first one
we might build a few models here i don't
know
and then module
so when you're creating a class in
python you can
create it
just without inheriting from anything
so class moon model and started from
there
but you can also create it by inheriting
from something so this means hey my
class moon model is going to get all the
functionality of nn.module and
nn.module in pi torch
basically
everything in neural networks or
everything in pi torch neural networks
inherits from an nn module so you can
think of nn module as lego bricks for
neural networks
if we're constructing a neural network
we want all the functionality that and
module provides hence why our moon model
our classification neural network
inherits from it
and then this is a bit confusing if
you've never written much pi to uh
python classes before you have to write
like a def uh init function
and you have to
um
write like super init like that
i'll let you look into like what that
actually means but just get used to
writing
if you're inheriting from nn module
you're going to have like this
boilerplate code to begin with
so
let's go
you can also put
the name of the class here but you don't
actually have to
self
so you don't you don't need that there
you can if you want you don't need that
there so this just means hey initialize
moon model v0 class and inherit from nn
module
and then initialize these upcoming
variables
so we're not going to copy what's going
on here
let's just go back there
and do we need any
i think we can get away with
just creating
some layers so self dot
linear
layers oh let's go self.linear1
equals
nn.linear so this is going to be a
linear
layer
in pi torch i'll show you what that is
in a second in features how many in
features do we want
that could be a parameter
in features
uh we want out features as well
and let's go hidden units
so in features can be in features
out features
can be out
features
and we want self.linear2
can be nn.linear
in features now what is this going to
have to be
in features equals
so we need to line these up
so the in features here
actually the out feature is going to be
a little bit confusing
i need a demonstration of this what this
looks like
tensorflow playground
when you're constructing neural networks
uh baseline neural networks if this
if this one outputs two features
two here
the input features it's not pictured
very well here to these four neurons has
to be two as well then this is going to
output four features the input features
to this layer have to be four then this
can output whatever it wants
so
let me just code this up and i'll
explain it in a second
so in features equals hidden units
because y we're taking the output
features the size of the output features
which is hidden units as the in features
to the next layer
and then we can go out features
this one can be hidden units
and then we've got let's do another
third linear layer
self.linear3
and then
linear
uh in features
equals hidden units
and then
out features
equals
out features
okay so we've got three linear layers
now what is a linear layer let's go um
torch
and then linear
again all of this is covered in material
materials here we're just going to quick
briefly overview so the linear applies
the linear transformation to the
incoming data y equals x
times a which is the weight tensor which
is transpose plus a bias
this is the same as the line of best fit
y equals um mx plus c
very very look at that it's basically
the same
where c is the bias
you ever done a line of best fit there
we go
y equals mx plus
c
this is the linear function
in pi torch
so question is if we combine three
straight line functions
can we model our data
what have we discussed before
this is curvy data
so will three straight line functions
linear
be able to model the data
i don't know let's find out then we're
going to uh create a forward method here
which inherits from self a little bit
confusing you've never done pie charts
classes but just means hey this forward
method belongs to this class
and we're going to take x as an input
which is
you can actually put whatever you want
here you can put input
but x is kind of universal across
different pie charts examples that
you're going to find and then the
forward what does the forward method do
when you define a subclass of nn module
the forward method
let's look it up
defines the computation performed at
every call should be overwritten by all
subclasses that means if you subclass
from nn module from pytorch remember
nnmodule is like lego bricks for pi
torch models
you combine everything from different nn
modules to create your pi touch neural
networks
plus a few other things but nn modules
your number one
the forward method defines the forward
computation of your neural network so if
we just look this up
neural network
right a forward computation just means
you put your inputs
through these two nodes and then they
get calculated on these hidden layers
and then they get outputted here so look
what we've got here we've got linear one
which is the input we've got linear 2
which you could classify as the hidden
hence why i've got hidden units and then
we've got linear 3 which is
the out feature and in features and out
features are just parameters of
the linear layer a lot to cover here but
let's see what happens
so we want we could do this
i'll show you two ways of doing it
x equals
dot linear one we could do this all
sequentially just like this
linear oh did i do linear one no linear
two x and then x equals self dot linear
three
and then return x so that's
progressively putting x through
all three layers at once or you can do
it like this self.linear3
on the inside of self.linear2
on the inside or outside sorry of linear
one
and then x is in there so generally
you're going to see
it might seem like they're doing the
exact computation
but
you're going to see speed ups if you can
do your computations like this so these
are like linking them all together
in one hit rather than reassigning x for
three different steps it shouldn't
matter too much with
our small model here but depending on
what type of computations you're
performing this is generally a good
speed up
so
let's see here return x
invalid syntax
where's my mistake
oh i need a extra bracket there we go
so now if we look up
uh
let's set it to moon model
and we'll call it model 0
just so we're writing less code model 0
equals
moon model
v1
v0 and the in features is going to be
the number of features our data has
which is 2 y2 because we have x0 and x1
so these are the two features one two
that would change if we had x all the
way up to 10 we would put in features as
10
hidden units
this is for our neural network
if we want seven
hidden units it will be like that
and if we want four hidden units
it will be like that
so let's do that
out features equals four if anyone knows
a good pie torch visualization
software i'd like to use that
and then
oh sorry that was hidden units
hidden units equals four and then what
is our out features
well our out features equals one why
because we want our model to predict
zero or one
or could it be two
it could be 2 but i'm going to go with 1
for the time being we'll see we'll see
later on we'll find out
and then we're going to write a little
bit of code here to device
what does this do
well what did we set the device as
before
so remember we're writing device
agnostic code
so our device is cuda so what this is
going to do is create moon model v0
using what we've just created here and
then it's going to send it to the target
device you'll see this throughout
pytorch a lot 0.2 device or something
similar to this device is pretty
universal around all the examples you'll
find
and this just means hey put our model on
the gpu which is cuda
or if we just left it like that
by default our model would be on the cpu
but
because we've written device agnostic
code if a gpu is available it will be
set to the device if it's not by default
gpu sorry this device will be set to cpu
and it will go to the cpu
let's have a look
in featurers of course we've got a typo
this is just sending it to the device
yeah a sheik is saying in the chat
um
when we stack linear layers on top of
each other all the weights from each
layer combines into a single weight
variable yeah
well that's what we've done here
we're going to see in a second
what are you going to what do you think
is going to happen if you combine three
straight line functions
three linear functions on top of each
other
so now we've got our model on the device
what do we what was the next exercise
okay
we've got that right to exercise three
here we go
now
this exercise is termed
pretty straight up
uh set up a binary classification
compatible loss function and optimize it
to use when training the model the model
is
model built into i'm going to write this
built
into
so when it comes to loss functions
let's have a look at the
let's go
oops
loss function
uh we'll go to the architecture
so remember these exercises are based
off
the
learn pie torch dot io number two so you
would have already gone through this
section here architecture of a
classification neural network because
we're using binary classification
binary classification our loss function
is going to be binary cross entropy now
there are many different types of loss
functions but for binary classification
generally you're going to find torch.n
slash bc loss if you want to look up the
function for that binary cross entropy
there we go you'll find articles galore
on how to use that
um because we're focused on code we're
just going to focus on coding it out and
then the optimizer a very common
optimizer to use
is stochastic gradient descent or atom
but torch dot opt in
has a bunch of different optimizers
ready to go as well
but but again by far the most common
ones are stochastic gradient descent and
atom
a few variants of adam that are quite
popular recently but those two are the
most popular so let's see
uh how we can do this and by the way
this is where we're up to
we're still going through the workflow
get the data ready well we've done that
we've got the data ready it's in
training and test sets build a model
well we've we've built a model here
and again this model will differ
depending on what problem you're working
on
and now we're up to 2.1 pick a loss
function and an optimizer
so let's go down here our loss function
torch and then bca loss let's write that
in
torch loss function equals torch and
then
bce
loss does this work
what's our loss function
beautiful
okay well we've got a loss function
ready
and you don't actually need torch here
you could just do nn.bce loss
because n n just stands for neural
network and torch pi torch
well at least i think it does and then
optimizer is going to be torch.optim
dot sgd
and then we have to pass in params
so params equals
model 0 dot parameters
what is this well we're going to have a
look at that in a second and lr is going
to be
0.01 so this is
learning rate
and this is
uh parameters
of model to optimize
so let's just set that up
what is model parameters though model 0
dot parameters
generator
let's get the state deck how about that
okay beautiful
so
this is
what happens when we create
oh remember
linear function
y equals x so the x is the input data
so here
our input data
the data that we created up here
this is x
and then we have we multiply that by a
weight matrix
the weight matrix is transposed because
the way uh matrix multiplication works
you need things to be in the right shape
and then we add a bias so look at what
nn linear has created for us
it has created a weight matrix
and what is the size here well we could
count how many elements there are one
two three four five six seven eight why
is it that size
well we've because we've set out
features
to be
uh
do we set it to be four or eight
i know four sorry four times two
and then on here
the output of this
is 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
16. okay beautiful
so you can go through the different
shapes of how these are calculated but
the point being here is that
our model starts off with
a random collection of weight numbers
and a random collection of bias numbers
now the weight is the a matrix here
and the bias number is here
and when we train a model
these random numbers will get updated by
the optimizer
right
by a certain because this is the model
zero parameters these are parameters of
the model
these random numbers will get
updated over time
with
every time our model looks at this data
and goes hmm that first time that i
predicted that line that went over there
didn't do pretty well
the next time i i changed a few of these
internal patterns
to be a different combination of
internal patterns and then
the line fit a bit better and so i kept
following that way
until
my loss function went down
that's a very brief overview of what it
does but essentially
start with random numbers look at the
data try to model it update the random
numbers in some way shape or form try to
model it again if it doesn't work change
it if it does work keep going
so
the loss function measures how wrong
your model is so how
wrong these random numbers are at
predicting the patterns in this data and
then the optimizer tells hey you know
what model you should try changing this
0.089 to 0.135
and see if that works better and then it
goes okay i'll try and then again the
loss function will measure how wrong it
is it goes oh no 135 wasn't wasn't too
good now change it to 255.
oh that's much better let's keep going
with that
and
even now
there is almost too many numbers here
for humans to keep track of
so this is why we wrote code to do that
for us
because you can imagine if we're just
working with a toy problem here and
we've got all these numbers on the
screen going on could you try updating
all those by hand
sure you could but it would take you
quite a long time now imagine if you had
a bigger data set that was say
millions of data points or even billions
these days with the scale of internet
data there is no way you could do that
all by hand so that is why we are
writing code to do that updates for us
so from the top the model that we just
created starts with random numbers
it will look at the data
it will try to model it by drawing a
line through it
the loss function will measure how
wrong that line is
and then based on the values from the
loss function
the optimizer will update the model's
internal random numbers
to better fit this data
and in an ideal world we move towards a
loss value of zero which means that
our model models the data perfectly
let's keep pushing forward
so now we've got a loss function and an
optimizer
let's what are we up to we're up to the
next exercise number four
create a training and testing loop to
fit the model you created into to the
data you created and one
okay let's see what this so this is
where the magic is going to happen this
is the magic of machine learning the
training loop
um let's go here and this is probably
one of the things that's a little bit
harder to
recognize when you first come to pytorch
if you've come from another
machine learning framework such as
tensorflow or scikit-learn in
scikit-learn and tensorflow you have a
model.fit function which takes care of
all of the code that we're about to
write right now
so
we want to
create a training and testing loop to
fit the money created into credit
so let's have a look at the pie torch
training steps
training loop steps
so one forward pass the model goes
through all of the training data once
performing its forward calculation
calculate the loss the model's outputs
predictions are compared to the ground
truth an evaluator to see how wrong they
are remember a loss function measures
how wrong your model's predictions are
zero the gradients oh we haven't heard
about gradients yet
the optimizer's gradients are set to
zero
they are
accumulated by default so they can be
recalculated for the specific training
step
perform back propagation and loss
computes the gradient of the loss with
respect for every model parameter to be
updated
this is known as backpropagation hence
backwards
step the optimizer gradient descent
update the parameters which requires
grad equals true with respect to the
loss gradients in order to improve them
optimizer.step
so
the loss function
there are two main functions here that
we're doing which is backpropagation
i'll let you look up what that is
back propagation short for back
propagation of errors is an algorithm
for supervised learning of artificial
neural networks using gradient descent
the next one is gradient descent
gradient descent is an iterative first
order optimization algorithm used to
find a local minimum maximum of a given
function
so the loss function
is what we're trying to find the minimum
of so if you imagine let's look at these
images
um
yeah right so imagine if we plotted our
loss function
to draw some sort of pattern like this
what we want to do is find the minimum
point
of the loss function a curve's probably
easier
here we go so cost
loss can be used interchangeably
um
could go like a curve like that why do
we use a curve well because we can
measure the gradients to go down to find
a curve oh sorry we measure the
gradients of a function that's why it's
a function
so there's a few steps we actually you
know what we're talking too much let's
just write some code and i'll show you
what i mean
so we'll talk about it while we go
through so we need to go
measure some accuracy now we don't need
that yet so torch
dot manual seed
do we use weights and biases in the
course well you're just going to have to
wait and see
random seed
now we're going to go epochs
epochs can be a hundred
so epochs is a term
so one epoch is look at all the data at
one hit so if we do a hundred we're
going to go through a hundred looks of
the data so
actually we need to
send data to the device i'll show you
what this what happens if we don't do
this
x train
y train equals
x train.2 device
ytrain.2 device
x test y test equals
x test
to device
so in pi torch
computations won't work if you've got
half your data on a cpu and half your
data on gpu and vice versa all of your
model and data should be on the same
device
so let's just comment this out
coding comma welcome back
oh thank you i appreciate that good to
see you back
and then we're going to go
loop through the data
so for
epoch in range epochs so that means
we're just doing a for loop
so we're stepping through 100 the data
100 times
let's now do the training
what we're going to do here is set our
model to train mode
so internally
or when you set up a model by default by
subclassing nn.module like we did up
here
it's in training mode to begin with
however there's anothe another mode
called eval mode
eval
which turns off a few things that you
don't need for training
like gradients
why would you do that
why would you have two modes isn't that
just confusing again this is another
thing that you might not be aware of if
you've come from
tensorflow or something like that with a
fit function does this for you
is to save save
compute time
so if you're just evaluating your model
you don't need to calculate gradients so
you can turn them off so it uses less
less data and in turn can hopefully
compute faster
so the first step
now this is a song
pie torch optimization
loop song daniel burke
here we go
i need to make a video on this song
let's sing the pie torch optimization
song together you ready
it's train time do the forward pass
calculate the loss optimize the zero
grad lost backwards optimize the step
step step
let's test now with torch no grad do the
forward pass calculate the loss watch it
go down down
down oh that's good fun i'm going to
stand up we need to we need to get some
grooving going with this
but this is how you're going to remember
the pie torch optimization loop which is
a training loop or a testing lube
hold on
what up
i think my desk just got caught on
something but that's all right so here
we go we do the forward pass
we're going to write this up forward
pass
so this means we pass everything through
our model
y logit
why logit equals model
0
and then we pass it the training data
and then we go why pred equals model
0
we need to do an uh
an activation here oh now let's just
let's just see what it looks like
print
y
let's just do that actually
before we even do a training loop let's
see what comes out of our model model 0
x train
maybe we do it
first 10.
ah have we got the classic not
implemented error
look at this
this is what absolutely astounds me
sometimes when
model zero
for some reason
it doesn't capture the forward method
because the spacing isn't correct
has anyone ever had this to them in
collab
look if i copy i don't want to copy but
i'm going to do it anyway
i'm just going to copy this
for some reason
the spacing of
you might run into this
yeah see
ah okay i think my return function is
outside the function oh no
yeah
look at that tab
tab have i got all my spaces correct
not implemented error but then if i pass
it this which is basically the exact
same model
with different different features
that is bizarre
can anyone see what's going on
maybe i need the same amount of spacing
i never get this if i create a model
from scratch in collab this happens so
often
super
ah
self
no i shouldn't need that
has anyone ever seen this
i'm so confused i'm always confused when
i write this in google collab the
spacing just seems to be off it says not
implemented error so this is this is
actually a good thing you're going to
get one of the most common errors in pi
torch is to have a not implemented error
aside from shape errors
and that means that the forward method
is not implemented
this is this is very good error catching
actually
so if you ever run into a not
implemented error
go back to your forward method and see
what's going on here
return this
x equals self.linear
for some reason the indentation is is he
is not oh
maybe i need to do that
okay this is a good note if you're using
collab
let me just see
what does this do not implemented error
this is bizarro
so right now it's saying that this
method is not implemented but if i
uncomment this
this is just what i want to show you
what's very confusing and this is like a
a big
troubleshooting
step
what if i just try to copy this here
i wrote all of this from scratch you all
saw me write this from scratch so i just
want you to like realize that if you're
running into these similar errors i've
run into them as well so we're going to
go through and try to fix this
not implemented error this is bizarre
but if i uncomment this
sorry comment that and uncomment this
basically the same code
except two layers instead of three
it works
this one's got no
oh excuse me
i need to comment out this as well
okay so now it works this is bizarro
okay so
let me just talk through what the error
is
so i'm not
i'm going to adjust this
it's the spacings here however
even if you create uh a model with the
similar spacings
if you're writing a forward method on
google colab just keep this in mind you
may run into the not implemented error
which is what we saw before but now
we're getting a runtime error that our
tensors and models need to be on the
same device
so
this is circle model this is from module
o2 we're going to change this back to
moon model and i'm going to readjust
this to be similar to here now you might
be thinking daniel your code above is
the exact same as the code below
or it's going to be very similar you
watch
why is it not working well i don't
really have a good answer for that
because this i've come across this error
multiple times in google colab
when i've created a pytorch model using
a ford method and using nn module
that even though i define the forward
method pi torch raises the not
implemented error and i've searched far
and wide to try and figure that out
but i haven't been very successful i
usually go about it figuring it out by
this roundabout process that we're kind
of seeing live here
so
if you're running into errors non-stop
like not implemented error please just
know that you're not alone
if anyone has a fix for that
i would love to know
let's see if all we did all we did was
copy the spacing from
another model
so we just copied this code here or
something very similar to this over here
not an ideal solution
but
we're going to see if it works and so
far it looks like it has
layer 3 i'm even going to remove the
underscores
i'm just going to cancel all of this
this is actually very good error finding
because you're seeing me
run into the exact same error
that you may run into
and then all we need to do here is add
self.layer three
and we can get rid of this
then i'm going to change this to moon
model
and let's see if this works
oops
in features
equals two
out features
equals one
hidden
units equals four
spelling mistake
okay there we go
and then we look at device
cuda model 0 state deck exactly how we
had it before
now is this going to do a forward pass
okay beautiful so now we've got a good
runtime error so just just be aware that
if you run into an error here when you
try to do a dummy forward pass with your
model and it says not implemented error
usually that's to do with the forward
method not being implemented and for
some reason if you're using google colab
the spacing may be off when creating
your ford method
and then pytorch will think that it's
not implemented so i've run into this
error many times i don't have i would
love to have a great fix but i don't
have one at the moment and i'm going to
get rid of this oh no we need the pie
torch optimization song in a second i
need to go to the bathroom i'll be back
in a sec
okay so why are we getting this error
well it's because all the tensors should
be on the same device
so found at least two devices
cuda and cpu so right now our data we
haven't put that on the gpu but we've
put the model on the gpu
and we write device agnostic code by
adding 0.2 device to a lot of our
data and models
because we set up at the beginning
the device parameter to be cuda if
um the gpu is available and to be cpu if
which is the default
if cuda is not available but pytorch
doesn't automatically recognize where
you want your model slash
data
so
you use the to function or the two
method to send it to the target device
so now we do a forward pass look at that
oh that is beautiful it works now
so what is happening here
well
we're putting some data through our
model and what it is doing the outputs
here
are what we defined in the forward
method
so the forward method takes x and passes
it through layer 1 which is
nn linear which is if we go back up here
do we have nn linear
an n linear
which is a linear computation
does this
and then
it goes through layer two which is the
same thing but with different
shapes and then layer three which again
is the same thing and in linear with
different shapes to finally it goes to
the out features which we set as one so
now we're getting one output after it's
been computed through three different
layers
for each input sample so if we go
lang
x train
and
length model 0
x train to device
800 800 beautiful so our model
essentially is making one prediction
per input sample and if we just wanted
to go
model 0
x train
to device
or let's just do the first sample of x
train that's going to be its prediction
and ideally we would have wanted it to
predict
the label for it which is y train 0
1.
gero hello have i ever tried data spell
no i haven't i'm not sure what that is
um so our model is predicting negative
one for
a value that we wanted to predict one
so
not very good now why is it doing that
well because
our model is still only predicting with
random parameters so what we have to do
is oh and by the way a model's raw
outputs are called logits so what we
have to do is convert these logits into
prediction probabilities so if we go
torch dot sigmoid
torch sigmoid is an activation function
beautiful and if we go to sigmoid we
actually recreated that sigmoid function
before
but
doesn't matter we can do it again let me
show you what sigmoid does
where did this come from it came from
the architecture which is again we cover
this in section 0.2 this is just the
exercises uh solutions to the exercises
that is
architecture of a classification neural
network
you want the output activation is the
sigmoid function so let's look up
sigmoid function for binary
classification it doesn't have anything
there okay some poor documentation but
that's all right we can practice the
sigmoid function so torch a range
negative 100 to 100 with a step of one
create a range we'll say this is a
let's plot a
here we go okay straight line
now if we go plot plt dot plot
torch this is what sigmoid does
sigmoid a
oh it makes a curve like that so then
we've got a decision boundary so
sigmoid everything
below this decision boundary this
vertical line here gets the value of
zero and everything
beyond this decision boundary gets a
value of one
now there's a great website i like for
this ml cheat sheet
sigmoid
activation functions here we go
sigmoid
sigmoid takes a real value as input and
outputs another value between 0 and 1.
it's easy to work with because it has
all the nice properties of activation
functions it's non-linear continuously
differentiable that means you can use it
for back propagation
what do we got
it's good for a classifier
yeah the output of the activation
function is always going to be range
zero to one
so
this is
what we're going to use to turn our
models raw logit outputs from
uh
how they looked like this into
prediction probabilities and then
because of the decision boundary
i wonder if we can plot this
plt dot plot
we're just going to say everything
ah it's on cuda that's why
can we turn this to
cpu uh because it needs gradient okay
this is
detach
dot numpy
okay
or maybe cpu
one more try and then we'll just show
you anyway
whoa that's all over the place okay so
what we're going to do
is instead of that jargon
is say
oh my goodness daniel
i hope you guys are seeing what happens
when you live code there's a lot of
errors you don't get everything right
the first go
so what we're going to say with this is
that everything
everything below 0.5 which we're going
to pretend that this is 0.5 this line
gets a value of zero
and everything above 0.5 gets a value of
1.
so let's do this imagine if we had
what if this was between 0 and negative
1 and 1.
or zero on one
and it was
0.01
okay
this is cool
okay there we go so let's see this what
if we do torch round we need to get them
what does our y
train look like up to
we need to get our
what we're trying to do is get our
models predictions in the same format as
our model's labels or as our data as
labels so we need to convert these
so y logits
so logits
print
is the raw outputs of the model
raw outputs of model
whoa alex you're at your first job
machine learning congrats my friend and
then we go prediction
probabilities
pred probs is
torch
dot sigmoid we use an activation
function on the logits
and then finally
we go from prediction probabilities to
pred labels
by rounding
the outputs of the sigmoid function
torch.round there we go okay beautiful
so these are all crap at the moment
because our model hasn't learned
anything
so our model raw outputs let's do this
model
raw outputs uh
logits
and our loss function
we actually want
we used bce loss but we could also use
this is another confusing thing about pi
torch loss function equals nn
bce
uh with
logits loss
so if we look at the documentation for
these two
bce with logic's loss
which one should you use well it
actually says they do the same thing
except uh the bce with logits loss
require has a sigmoid layer built in so
we don't need to pass it uh
logits that have gone through a sigmoid
layer we could just pass it raw logits
hence with logit's loss remember logits
are the raw outputs of our model
it says it's more numerically stable
by combining the operations into one
layer we take advantage of the log sum x
trick i'm not sure what that is for
numerical
stability so generally pytorch recommend
using this for more numerical stability
so what does this look like let's do
that
y logix equals the forward pass
and then let's go y pred probs
equals
torch dot
sigmoid
on y pred probs
sorry on y logics
and then y pred equals
torch dot round
on y pred props
so that's the forward pass
two
calculate the loss
i'm going to just get rid of this
get rid of this
and then the loss equals loss
fn and we're going to pass it in y
logits compared to
what y train
so let's write this down loss
equals compare
uh model
raw outputs to
desired
model outputs
then we can also calculate the accuracy
here let's see if we can do that
so we can go pip
q
install torch metrics
and then from
torch metrics
import accuracy
and then
ack fn
equals accuracy
dot 2 device
so torch metrics is
we could code our own accuracy function
by scratch
but let's look this up
torch matrix is a great
library that's based on pi torch
yeah
there we go
which calculates uh different metrics
in a very pi torch or python
pi torch e
pi torchy way
so we could code our own accuracy
function from scratch or we could just
import that
so let's write that down
hey siri stop the timer
my timer's going off
because i've done two hours of coding
this morning that's exciting let's
calculate
the accuracy
i try to start every morning with two
hours of coding
so we're just gonna we have to install
torch metrics because
um
colab doesn't
come with torch metrics
uh so we've got accuracy and then we can
calculate the accuracy by going fn but
we need to put in y pred and y train
so
the accuracy function
needs
to compare
uh
pred labels with um actual
labels
and we'll put here
not load yet
so then we do what do we do next
what's the pie torch optimization loop
song well
zero the gradient
optimizer zero grad
and then we have next
we need to do lost backwards
so this is back propagation over the
loss
loss backwards
and then we're going to
go
step the optimizer
so perform back propagation
so if you want to look into what's
happening here
i'll discuss this in reverse order so
i'll go lost backwards first so this is
back propagation
backward propagation of errors is an
algorithm for supervised learning of
artificial neural networks using
gradient descent it's going to uh
calculate the gradient function
over our
uh
over all the parameters in our model so
with respect to the loss and then the
optimizer is going to perform gradient
descent
on those gradients
so that's where these two things come in
so i'm going to go here
back propagation
there
and then if you want gradient descent
gradient descent
there
beautiful
and so this is where the optimization
loop song comes in we've done the
training
so
pi torch optimization song you ready for
it it's train time we do the forward
pass calculate the loss
optimize the zero grad lost backwards
step the optimizer no optimize a step
step step i don't know the full song yet
i promise by the time the course comes
out i'll know the song off by heart i
should make a video just about this song
why do we zero the gradients well when
we do this step
because every loop it's going to add
those uh steps up
but we want it to reset itself every
time so it only does a little bit of
adjustment why does it do this well
there's a lot of internal things that pi
torch does behind the scenes to optimize
our code for us perform automatic
gradient calculation which is called
auto grad
so that's
the backwards propagation so calculate
the gradients with respect to loss it
adds them up every step in the loop so
we want to or every time it steps so we
want to instead of it accumulating them
instead of like say zero after iteration
zero it's they're ten after iteration
one they're 20 after iteration one
they're 30. we wanted every iteration to
start
at zero so after iteration one it's up
here and then it gets zeroed and then it
goes back to here and then zeroed back
to here so that's what happens with
optimizer zero grad
but just remember the pi touch
optimization song
it's train time do the forward pass
calculate the loss optimize the zero
grad
loss backwards optimizer step step step
and now let's test i need to update the
song because there's a better way with
torch dot inference mode so when we're
doing testing we're getting our model to
make predictions using the patterns that
it's learned in the training loop on the
testing data
and so
that's why we use model
model.eval and with torch.inference mode
both of these together kind of goes
tells pytorch hey a lot of the things
that you
turned on for training a model such as
automatic gradient calculation we don't
need that during prediction during
inference we're not calculating
gradients when we're evaluating our
model we just want to do a forward pass
aka the song let's test now with torch
no grad you might see this as well
torch no grad in some places which means
no gradients no gradient calculations uh
which is used in gradient descent and
back propagation
uh the new one is inference mode so this
is
uh the faster way of doing things i
would recommend using this this is what
pi touch recommends at the time of
recording this and to do this we just
want to do a forward pass with our model
forward pass so test logits remember the
raw outputs from our model are just the
logits
so we go model zero
equals model zero but this time we're
going to do the forward pass on the test
data
and then
we can turn it into
test spreads
by going
torch dot round torch dot sigmoid and
then we will use
um
test logits there
and then we can go
to calculate
calculate the loss slash accuracy and
then we go test loss equals
loss fn remember our loss function which
is binary or bce with logits loss wants
the logits as input plus the
testing labels this time and then for
the test accuracy we can use our
accuracy function which is going to take
in test spread
and our y test labels and then our
exercise says
we have to
what do we want
print out the progress every 10 epochs
so let's do so
print out
what's happening
hey abdullah you recently took my ml
course and udemy here to say hi well
thank you for joining in
um so if epoc
because we're looping through epochs we
can go
let's count the epoch here
f epoch
divided by
10 equals zero
let's do uh a print statement of
f
epoch
is
epoch
and then we'll go
loss
equals loss
and then we'll go ack
equals
accuracy
and then we can go test loss
is test loss
and
test ack is
test ack
but we probably want these as 0 2 f
just the two decimal points that should
be enough
let's see if this works we definitely
have an error somewhere
expected all tenses ah so this is i said
before we're going to run into an error
why because
we haven't got the data on the same
device as our model
so
what happens let's let's just
familiarize ourselves with this error
expected all tenses to be on the same
device but found at least two devices
cuda
so our model is on the on the gpu
and our data is on the cpu but what
happens when we put our data to the gpu
as well
oh we got something wrong here using a
target size torch 800 that is different
to the input size torch
oh
please ensure they have the same size so
we've got something going on
we need to
i just want to show you print y
logits we need to squeeze our outputs
uh i want the shape
so see how we have this extra dimension
here
oh my gosh that was just 800 outputs
i could have just gone up to here we
want to remove this extra dimension so
watch this
if i go dot squeeze
oh did it remove anything
oh no i need to squeeze the end of it
dot squeeze
watch this it's got a bracket here
this is what we have to do we have to
remove that extra dimension there we go
and then if we do the same here
watch this bracket there
bracket there we're going to get rid of
that by squeezing it
squeeze just remains to remove all the
one dimensions there we go beautiful and
then if we put squeeze here
magic okay
so
what we have to do is just squeeze the
output of this
and remember what it looked like before
squeeze
we might only print out the first
five every time
that's what it looked like before
we run it
again cuda error that's all right but
look at this
beautiful
we squeezed it
uh device side assert
triggered
cuda kernel errors might have
asynchrously reported at some other api
call
okay so i feel like this is because
we have
already put
our
we've done it we've done a few different
things here so we're going to just
reinstantiate everything
because our code is a bit jumbled what
i'll do is i'll just delete a few extra
things
and write this
write that
oh we've got a cuda we've got an actual
cuda error
this is excellent
i'm going to just restart the runtime
here
rather than troubleshoot a cuda error in
this video
let's just run
all before
see if this works oh we probably need to
squeeze this as well
the target has to be an integer tensor
are you kidding me
oh torch metrics
why pred
i'm just gonna turn this into an end
oh this is lost backward instead of
backwards okay we're getting typo errors
galore here but this is this is
excellent this is excellent this is the
things you're going to run into when you
do this stuff
oh my goodness what have we got wrong
here
oh we need test logits not y logits
is this going to work
it's working
but it's not learning
oh no that's okay our model is
predicting at about 50 percent
but let's plot the predictions and see
what happens we've got a plot
predictions function in here
uh
no plot decision boundary
here we go
let's bring this in
plot the model
predictions
bring that function in
and then we can go in here
plot the decision boundary
look what's happened this is why our
model is getting 50 percent because it
is
trying to divide our
training and test data into red and blue
using straight lines so how do we fix
this
well we have to go back up to our model
definition
and we have to add in
some non-linearity
so if we do relu here
because nn.relu
so remember what the relu function does
relu
there we go
so the relu function it turns everything
that's negative into
zero and leaves everything as positive
as positive
so if we just add in this simple
nonlinear function
to in between different layers of our
model self.relu here and then we're
going to go self.reload here
let's see what happens
oh we still don't get too good but we
got a little bit better than what we did
before
okay how about we increase
i think we're going to decrease this
decrease the learning rate
and we're going to up the hidden units
to 10.
sorry increase the learning rate
to try and get our model to learn faster
and
increase the number of hidden units
oh there we go beautiful so what did we
do just then we we increased the
learning rate to get our model to learn
faster and we increased the number of
hidden units in our model
to
learn
more features of the data and now what
what do you think will happen if we
train it for longer so a thousand epochs
instead of 100 epochs so now not only
does it have do we want our model to
learn faster by
increasing the learning rate from 0.01
to 0.1
we want to train for more epochs so
giving our model more chances to look at
the data so an epoch remember is one
forward past the data and we're
increasing the number of hidden units
where's the tensorflow playground
tensorflow playground
inside our model
to go from four hidden units per layer
well this doesn't even go up to ten now
it goes to ten
so let's see what happens
we'll train it for a thousand epochs oh
my goodness
oh my goodness look at that
now we have a test accuracy of 1.0 and a
training accuracy of 1.0 so what happens
if we plot our predictions again
what does it look like a
beautiful
look at that
so that is we introduced the power of
non-linearity and we upgraded our
model's potential to learn and so we
went from drawing just a straight line
and we added just one function to our
model which was the relu function the
non-linear activation the relu function
we put that
in here and then our model went from
only being able to draw straight lines
to being able to draw
non-straight lines as well and we can
replicate that on here
so if we go linear
let's turn this down to four
so we've got two circles here if we just
use a linear activation function with a
learning rate of 0.1
what's going to happen
well the loss is about 50 percent
so 0.5 roughly about 50 percent why is
that well if you're working with the
binary classification problem and your
accuracy or your loss is around 50
percent it means your model is basically
guessing because you could randomly go
okay this point is a blue point this
point's an orange point or just predict
them all to be orange points and you
would get about 50 50 if your class
samples are balanced but what happens if
we stop this what happens if we just
change this from linear we add in a relu
function just like we did here all we
did was we added added in a relu
function you ready to watch this
oh my goodness
have a go at that
our test loss
just went from 0.5
to 100 times less
more than 100 times less it's modeling
the data perfectly here
all right we've got two features very
similar to what we're working with four
hidden neurons
learning rate of 0.01 i'm not sure what
optimizer they're using here but we've
also used the same radioactivation
function
how beautiful is that that's the power
of neural networks
a combination of linear and non-linear
activation functions so that took us a
while to build a training loop but the
what you may have noticed is we've got a
lot of boilerplate code here
so you could functionalize this into a
function such as def train model
and then it takes as a model as input a
loss function as input an evaluation
function is input and an optimizer and
then it just does these steps then you
could have a a def test function which
does these steps and then you can have a
def
train function which combines the train
step and the test step functions which
then outputs how the model is doing
so there's a lot of code here that we
could fix up
but what i might do is just go from the
top i think if we train from a thousand
epochs from scratch
it should just work
we'll print out only every 100 epochs
there we go okay so by changing a few
parameters we go from after 100 epochs
50 accuracy to 76 and then all the way
up to 100 accuracy and now we can plot
our model predictions how good is that
so
we're working through the exercises here
we're now up to
oh we've done number five
oops
and now we're up to number six
so replicate the tan h
function
in a pure pie torch
okay
so we saw how to do
so we've got some different activation
functions here we use the relu function
uh but what does relu do behind the
scenes remember relu
for your model's parameters
turns the negative parameters into zero
and the positive parameters it leaves
them as is
so it creates non-linearity in that
sense the 10h is another form
of
nonlinear activation function
oh we got a bit wonky here
but
it still works pretty good
so let's replicate that what it looks
like in code but if we go torch 10 h
so pi torch
have a lot of inbuilt
non-linear activation functions so what
we're going to do is we're going to
replicate this
instead of using torch 10h
so
uh
let's just create this we'll go
uh what can we call our tensor we'll go
g
i'll go tensor a
equals torch
a range
0 to 100 or no we've got negative 100 to
100 and one and then let's go torch or
we'll go plt
dot plot um
tensor a
just to visualize it
and then we'll go torch i mean sorry
plot
plt dot plot
torch
dot tan h
tensor a
so we get a similar output to the
sigmoid function
but now we're going to replicate this
with
without using torch.10h so let's write
this def 10h
it's going to take as input x
and it's going to return
something
so we want to create a function that
replicates this functionality so now
let's go ml cheat sheet
10 h
let's look up the formula for this
tan hz which is logit
z is another thing for logits or input
equals
exponential to the power of z minus
exponential to the power of negative z
on top of
exponential
z plus exponential to the power of
negative z so let's replicate this
we want torch exp
which is stands for exponential
uh exp on x
minus
torch dot exp
let's write this down actually source
exp on negative x
and then this is going to be over
torch dot exp
over
x
plus
torch dot exp or exponential
to the power of negative x and they use
z in this example but i'm just using x
as the input now let's see if this
works how we want it to work
tan h
tensor a
beautiful
how cool is that we just replicated a
non-linear activation function and you
can do the same for many more in if you
just go torch
activation functions
over here
non-linear activation so some of the
most simple and most common ones are
going to be relu
there's different forms of reload like
cellu silu
jello
sigmoid we've seen sigmoid
[Music]
there's lots of them tanh
glue
so go check these out you're going to
see a lot of them in practice relu is
probably the most common because it's
the most simplest that's another thing
you've got to keep in mind is when
you're
implementing different things is
simplicity usually wins if you want to
deploy your models into applications so
just keep that in mind
now we're going to put everything
together
by creating a multi-class data set
so let's put everything
together
so construct a model capable of fitting
the data
can create a multi-class data set using
the spirals data creation function from
c
cs231n this is this code here
so this is going to create a spirals
data set
just using very similar code to what
we've used before
i need to write down here as well
let me just say
split the data
into
training
and test sets
80 train
20 test
as well as turn it
into tensors
by torch
tenses
okay
let's now
copy this
we've got a few steps here but basically
all we're going to do is turn everything
that we've done so far
put together everything we've done so
far into a multi-class classification
problem there's a fair few steps here
but that's all right so create a
multi-class data set let's just see if
we can do this
on our own
so this is using the spiral data set
so i wonder if
i want to change the colors of this
red yellow blue i want to change these
to red yellow blue
is it bu for blue
okay wonderful so we need to
now we have
or before let's check what we had before
we had two classes so we had binary
classification
red dots and blue dots now we have
three classes
blue yellow and red dots
so this is using some code from cs
i've linked the reference here
cs231n that's a great course on the the
in depth behind the scene stuff what's
going on on neural networks we're
focusing on code here
so let's
see what the length of x is
okay beautiful
and
length of y let's create training test
splits
so x train
x test
y train
y test equals train
test split we'll just import
we'll get i'll show you where we get
train test split from
trying to split on x y test size equals
0.2 and the random state can be
random seed
wonderful i'm just going to set up
a random seed here
42
mp
random seed
wonderful and now we should have
length
x train
x test
and we'll check length of y train
beautiful
okay
uh so now we have 240 training samples
to 200 and
[Music]
sorry and 60 testing samples so now we
want to
i think we can turn the data into
tensors first actually
or we can just do this
turn the data into tensors
and we're going to pass in x train the d
type can be torch
dot float
data is into tensors okay and then what
was the next step so we split it in
to training and test construct a model
capable of fitting the data you may need
a combination of linear and non-linear
layers so let's create a model
why don't we use the sequential api this
time
fitting the data
so model one
equals
now the sequential api is quite a bit
simpler than the
nn module
but sequential is subclass of nn module
so before we created uh a model
we
used this
and then module i'm just going to delete
this so it's it's a little bit less
code
we did all this stuff
but in the end our model just went
through a straightforward stack of
layers like there wasn't nothing there
wasn't anything too complex
in our forward function it just went hey
go through layer one then a reload go
through layer two then a reload go
through layer three and then that's
you're done
how about we do
a little bit different
this time
when we go
nn.linear
in features hard code at this time
the the benefit of creating a model
class is that it's more flexible
such as before we could set the in
features the benefit of an n sequential
is that it's just more straightforward
so you can use them interchangeably like
you could use an n sequential inside a
subclass of nnmodule
so let's go this out features equals 10
so that means our first layer of our
model is going to take in
two features
which is what x has
0 0
and then it's going to output 10
features
and then our next layer
has to take in 10 features because our
first layer outputs 10 features
and then we're going to get it to out
10 as well
and then and then linear our final layer
is going to take in
10 features from the previous layer and
then it's going to output how many
well let's have a look at why
first ten of y
we have zero zero zero zero oh wow how
about we look at y train
ah so we have zero two or 201 so we have
three different features so three
different labels
or in this case three different classes
for three different colors we have
yellow blue red oh sorry yellow red blue
and that is why we need our model to
take to output three features one for
each class that we have
and so if we send this to device
oh wait what are we missing here
we've only got three linear layers
so
will our model using just straight lines
just linear layers being able to model
this data i don't think so we need some
lot non-linearity
so let's bring in
relu we're going to intersperse relu
in between
each of the two middle layers
now there's actually really no set in
stone way of how you can
interlace these layers however you'll
see quite often that uh
if you do a linear transformation so a
linear layer you'll quite often see
straight after that some kind of
non-linear transformation so a
non-linear activation function
so linear layer non-linear layer linear
layer non-linear layer linear layer is
the output could have an activation
layer here too
such as in the case we might want our
model to output prediction probabilities
instead of logits so why and then
softmax because if we look at the
architecture for a classification model
if you have more than one class
where is it
architecture
if you're working on multi-class
classification which we are
the output activation function we saw
that before to go from logits for binary
classification to prediction
probabilities we use torch sigmoid
but now for multi-class classification
to go from logits to prediction
probabilities we want to use softmax
so we've just created our model let's
get rid of that
and let's see what happens when we do
model one on x train what happens
let's get the first 10.
oh again
all tensors need to be on the same
device goodness gracious me
we need some
device agnostic code
set up
data to be
device
agnostic so uh
so we need let's just set this up here
we don't need to do this again but we're
going to do it anyway prepare device
agnostic code just to remind ourselves
device equals
cuda
if torch dot cuda is available
else
cpu
so the model is going to go to the
whatever device is available then we
want the same for our data
x train
y train equals
x train to device
y train to device we'll put our training
data there then we can do the same for x
test x test y test
equals x test two device
uh y test to device
and then we're gonna see what our
forward pass looks like
wonderful we get one logit value per
class that we have so this is a sample
here
and we get one value for
class zero we get one value for class
one we get one value for
class two
so let's do this
so
print out model outputs
so this is print
logits
and then we'll go print
but what if we squeeze these as well
oh we don't need to squeeze those
actually
or do we
same thing
and then we want print
fred probs if we want to go from logits
to pred probs for a multi-class
classification we need an output
activation of
sigma uh softmax so torch softmax
on x on model
one x train and let's have a look at the
first 10.
oh goodness gracious me we have to tell
it sigmoid that we want the soft max to
go
on the first
dimension
wonderful now we have prediction
probabilities for each class now with
the prediction probability after using
the softmax function because
each one of these samples is now going
to add to one
so if you add it up
0.3976 and .2210 and 0.3813 you will get
one or very close two and
the
one that the model the class that the
model thinks is most likely that the
particular sample is associated to in
our case
blue yellow or red or 0 1 2
is going to be the indexed value here
with the highest value so
in this case index 0 has the highest
value compared to the rest index 2 is
quite close so for this particular
sample the model thinks that the most
likely class
red yellow or blue is the first one
index zero and then i think it's a
similar pattern for
the rest of these samples but again
remember this is using an untrained
model
print out untrained model outputs so
they're basically gibberish right now
our model doesn't have very much
predictive power
so
now we want pred labels
to get the pred labels we can take the
arg max across the first dimension as
well so let's see what that does
wonderful so now our pred labels are in
the same format as
our
ground truth labels
so y train
and of course it's going to get
the first 10
basically wrong probably going to
operate around 30 accuracy because it's
just guessing and if the model just
guesses zero for every class because we
have three balanced classes we have
equal points between blue yellow and red
it's going to default to around 30 33
accuracy
but now let's construct a training loop
what's our song
build a training loop
for the model what does it say here what
was the instructions to do
oh we we forgot one thing we need to
build the loss function and the
optimizer capable of handling
multi-class data
optimal extension use the atom optimizer
instead of sgd okay beautiful
let's do that
so we want to
set up
loss function
and optimizer
so loss function is going to be
nn dot cross entropy
loss why is that well for
here
if we're doing binary classification we
use binary cross entropy
and if we are doing multi-class
classification we use cross-entropy so
cross-entropy without the binary and
that is torch dot nn dot cross entropy
loss
and for the optimizer this time we're
going to we use sgd before stochastic
gradient descent this time we're going
to use the atom optimizer which is
possibly the most common aside from
and we need it to optimize our model one
parameters
and we'll set the learning rate to maybe
0.001
atom is is pretty good with a lower
learning rate so again the two most
common optimizers are sgd and atom
but there are many other variants of
optimizers just as there are many other
variants of loss functions
which one you use will be depending on
the problem
most often for binary multi-class
classification you'll look at a setup
similar to this
so now let's build the training loop for
the model epochs equals
how many
how many did we say
for the exercise
uh it needs to reach over 95 testing
accuracy so let's do it for a thousand
and here's where we can do
our training loop song
so
uh
loop over data
so for epoch
epoch in range epochs
we're going to do our training loop song
so do you remember it's
do the forward pass
calculate loss
optimizer
zero grad
and then we're gonna do last backwards
and then we're gonna go
optimizer step step step
so there's our song
we do the forward pass calculate the
loss optimize the zero grad last
backwards optimize a step step step and
that's the training loop optimization
song
so training
we're going to go
model one dot train
and then we're going to do the forward
pass so we're going to go y logits
because remember the raw outputs of our
model without any activation function
applied are called the logits
you can look at what the logits mean i
don't actually really no i just know
that that's the name of them and i like
to uh interact with the code rather than
sort of dig into the the names of
different things i think it stands for
log something log
log probability or something like that
and
we're going to pass it the training data
and then we're going to because we've
already set we could set the data again
to the device but we've already done
that up here
and then we can calculate the y pred
equals what does that equal well we need
to go torch
soft max
across the logits
and across the first dimension and then
we need to take the arg max also across
the first dimension because
where
we turn the logits into prediction
probabilities using softmax and then we
find the prediction classes the
prediction labels by taking the arg max
so the highest prediction probability
value or the index where that prediction
probability value occurs
so then number two we need to calculate
the loss so the loss equals
loss function we're going to pass it in
the logits and we are also going to pass
it in y train the labels and we can do
the accuracy as well
so let's go
we'll get the accuracy function that we
got up before
i'll bring this down here
accuracy function
back eagles
and then we're going to do optimizer
zero grad
zero the gradients because by default
they accumulate
we'll take the loss backwards we'll form
perform back propagation over the loss
and then we're going to go optimizer
we're going to step the optimizer and
figure out how to improve our models
parameters with respect to uh how we
best improve the loss
so there's a training loop
oh have we instantiated that
and then we can do testing
testing is model one dot eval
and then we can do uh torch dot
inference mode
for making predictions remember when
you're testing you can just
uh
you don't need to calculate gradients or
anything like that so these three steps
aren't required for testing so testing
is also just making inference and
predictions we're evaluating our model
hence why we turn it
to a vowel mode model one and val we
turn on inference mode which means we're
making inference with our model and then
we go test logits
equals
model one we'll pass it the test data
and then we'll go test
pred equals torch dot softmax and we're
going to get the softmax across the test
logits
the first dimension
and then we're going to take the arg max
also across the first dimension to get
the test labels
and that's the forward pass
forward pass
and then two we're going to calculate
loss and accuracy
so test loss equals
loss fn
on test logits
and
test pred and then finally we're going
to get the test ack
which equals
rcfn
on
test pred
oops we want that
test pred and y test
i think it'll be the same regardless of
the order that we put that and then
we'll print out
so if epoch
divide by 10 equal or number every 100
epochs let's do that what's going on
here why is this wrong
uh if epoch divided by a hundred equals
zero
let's print out what's happening
so
print
and then we want f
epoch
and then we want
loss
equals loss and we're going to take that
to two decimal places
and then we want ack
i'm gonna go back to two decimal places
and then we're going to go
test loss
which is
test loss to two decimal places
and we're going to go test
ack which is
test ack
to two decimal places
wonderful now you reckon this is going
to work let's find out if in doubt run
the code oh
we got something wrong here loss
function
not implemented for float okay um what
if we just pass in the straight up
threads here
expect it to be in range of negative one
to zero but got one
oh let's print out our preds
print
why pred
troubleshooting on the fly here team
zero zero zero
zero huh
what's why train look like
do we need this is floats
and do we have
cross entropy loss
let's look up this
torch cross entropy loss
what's happening here
input target
yeah what gives here
why you're not working on the logits
not implemented for float
huh
so that comes out correctly
what if i turn this into torch
tensor
um
d type equals torch
not implemented for int
but also not implemented for float now
this is another error that you're going
to run into quite often
runtime error nn loss forward not
implemented for float
but if we go here
they're all floats yes
and then if we go like this
they come out as
integers
so what if we put in y pred here
my pred dot unsqueeze
host softmax not implemented for
long
this is good troubleshooting right here
what if we unsqueeze on dimension one
let's google this and see what comes up
i've never got these errors
that means i need to have the dimension
of the outputs tensor one two
from the docs i'm assuming your
c equals two and n equals two
hmm
pseudomaze what's happening
why is this not
we're doing some exercises for my
uh upcoming pie torch course but we're
running into some errors which is
actually quite good
so the model is outputting
why on earth
what d type is this
float32 okay
this is not making sense to me
you're telling me
torch flight 32 so they're in they're
both in float 32 so this is the the way
we should be looking at
oh that's that one of those
i don't get i've never seen this error
before
and then last ford reduce cuda kernel 2d
index not implemented for float
you
what about
why pred
and we got the same so this is another
strange error i haven't really faced
before
we're looking at
our
shapes are the same size but now we're
getting dimension out of range expected
to be in range of negative one to zero
but got one
pulling out all the tricks in the book
here
host softmax not implemented for
you
oh my goodness
this is an absolute crack up
well lucky i went through uh
troubleshooting all of this before
putting it out
as some exercises
live demos you're right
but i'm i'm so the shapes are right
the function looks like it's being used
correctly
cross entropy loss
oh
how does this look any different
so this is this is what you can do you
can come to the if you're having trouble
with things come to the this is a great
lesson actually come to the pi torch
documentation see what the
example does and copy it
so let's have a look at the input
and the target
what if i did that
not implemented for float
these are floats
i don't get it
they're float 32s
let's see if i can solve it using my own
code
loss function
atom
this is exactly what this looks like
i wonder if i change this too
i wonder if torch
ah
nope
see this works
i wonder if this code still works
we're going to try and fix this you know
because this is what people are going to
run into as errors
this is a really recent era
in my case the custom target i was
comparing to the model
d type errors friends
you're going to run into them all the
time they're both float flirty too
not implemented for float
am i resetting y train somewhere up here
um
all float32
see this is code exact
same code i wrote
that works for a slightly different data
set
output
so then why logics equals model
model one x train
and then if i change this to pred
you
you
i have i'm seriously a bit stuck here i
have no idea why this to me it should be
working
convert pie torch d-type
torch.tensor
went back to where we started not
implemented for float
not implemented for float maybe maybe
i'm using a version of pie torch that's
got an error
no
i'm going to try to rebuild the model
with nn module and see what happens
you
what is it why don't you do that color
you don't need to delete my code my
friend
come on come on
now this is where some reason
if it doesn't work on the forward method
we want
what's what's it got going on here
why is it output to n64
absolutely perplexed
i need to restart the runtime
what if i turn this into an end
now it's not implemented for int
not implemented for float
what form should the labels be in this
is
friends i actually don't know
oh
is it a long tensor that's not working
oh my goodness gracious me
you
so now i've changed it to a long 64.
oh my gosh it worked okay
so
we needed our labels as a long tensor
so this is the example what did that
take like half an hour maybe longer of
troubleshooting of
labels
need to be
of type long
but
i'm glad i recorded this video because
now you can see how long it took me to
actually troubleshoot an error like that
i'll never forget that again will i
oh lost backward not backwards
oh my gosh it's working
stop stop stop
stop man stop
after heroin
effort
troubleshooting we got there in the end
friends
it was one of the errors
that
i actually would have not thought for a
long time would it have been
but
that's all right the error is always the
last place you look
how's our model going oh slowly getting
better
oh i think if we up the learning rate a
bit it'll get better quicker
there we go okay
our model is performing excellently
yep with the atom learning rate up now
let's go plot
we can get our code from above here
hey
there we go
would you look at that
our model is predicting pretty darn well
so
with that being said we are now through
all of the exercises
that come with notebook number two it
took us nearly three hours so that's
actually pretty good
with a fair bit of troubleshooting
we got over 95 accuracy and we pl
plotted the decision boundaries so if
you'd like to see all of the code for
this uh it'll be
on loan pie torch dot io
and the exercises
and solutions will be on the github as
well so it's mr deburg slash pie torch
deep learning the default notebooks will
be
uh here in the main repo and then the
solutions and exercises so we just went
through number two not there yet but
i'll add them there in a second i might
just add them there
i'll tidy them up and i'll add them into
the solutions folder as well
but
if you want more check out
pytorch learn python io
and i'll be back in an upcoming stream
we'll go through more pie torch but this
is a pretty good overall introduction to
a pie torch workflow
for classification on a couple of toy
data sets and we had the bonus of
running into
a fair few errors that you'll often come
into empire torch namely the forward not
implemented error so if you get not
implemented error check out your forward
method and a whole bunch of
data type slash shape calculations
errors when calculating the loss
often are end devices
so often times when you get errors in
calculations they'll either be shape
errors data type errors or device errors
and we saw all of those in here so
uh thank you so much for tuning in
i will see you in an upcoming stream
where we will tackle more errors
peace
thanks for tuning in ajax on i
appreciate it
see you later my friend