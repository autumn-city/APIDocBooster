hey guys welcome back to the channel and
this new tutorial on python so in
previous few videos we learned about uh
attention masking like how we do it in
decoder and how we do it encoder
in this video we will talk about another
very important concept known as dropout
so dropout is mainly
useful to do the regularization so
regularization mainly helps to prevent
the overfitting like the model
generalizes well if the
weights are regularized so there are
other techniques as well but dropout is
shown to be very effective compared to
others okay
so internally dropout uses a bernoulli
distribution we will look at it very
basics like
how
we use it in pie torch okay
and in a patch each sample will be
zeroed out uh independently so the
dropout is applied on each and every
sample in a patch separately and at the
end the output are scaled by a factor of
1 upon 1 minus p if the probability is
uh 0.3 so it will be scaled like 1 upon
1 minus 0.3 which is 1 upon 0.7 okay
and during evaluation the model simply
computes an identity function so the
dropout is only applied during the
training so this is very important to
remember so when we have a layer in our
model so it will be only active when we
run the training during the evaluation
it should be deactivated and we will see
like how we can deactivate it using eval
function okay so let us dive in and
start with
simply learning about basics of
bernoulli okay
so
to learn about bernoulli let us create a
simple
sampler okay so let us say
bernoulli sampler
is equal to we will simply create a
uh
instance of bernoulli
class
and it takes probability so we will say
probability is equal to p and let us
define p also here p is equal to let us
say
0.3
okay so what does this mean that when
the probability is said to be 0.3 it
means
that
30 percent
it will be one
and seventy percent
it will be zeros okay
okay so next is to sample from this okay
so
we can simply call
samples is equal to
bernoulli
sampler dot sample
what it accept is the same okay and we
can simply pass let us say we want to
have a shape of 20
and it should be a shape so we will pass
it as a tuple
let us now print the sample
so it should be
zeros and once so it will be 30 percent
of time it should be one and seventy
percent times it would be zero so it is
not like it will be uh accurate so it
will be more towards that uh probability
distribution when we sample a lot of
samples from it okay so let us say uh we
have 20 samples then it should not be
necessarily to be 30 1 and 70 percent of
zeros it would be slightly uh here and
there okay so let us run and see it
so now you see like we have uh ones and
zeros and to compute like how many of
them are zero and ones we can simply
uh call print
so let us call
sample start
count
zeros
let us say count
non-zeros
so it will count all non zeros
and if you divide it by length of
samples
uh so it will be a percentage of once
okay so let us run and see it
okay so it should be zero not zero so
let us read an and see it
so now you see it is saying 0.45 whereas
the probability we passed is
0.3 so if we keep increasing
the tensor size let us say 2000 it will
uh reach our it will uh 10 towards 0.3
okay so i'll just run it again
so now you see it is 298 and if we keep
increasing let us say it more
now it should be very close to 0.3
percent
now you see it is uh very close like
it's 0.31 if we keep increasing so uh to
to get the probability actually what we
provided we should sample a lot to to be
exact so that's why it is not
necessarily that always we get 30
percent of the
of the units are 30 of the items uh are
zeroed in dropout event okay we will see
that in next
so let us create a dropout model and
learn that how
it
works
so let us say dropout
model is equal to nn so the dropout
module is uh in nn
package so we will simply import an n
and then
we'll simply say drop out so the default
probability is 0.5
if you don't pass anything so if you
pass in no probability it will be 50
percent of
once and 50 of zeros
and if we pass the probability it will
be like that and there is another uh
argument which is in place which is by
default false so in places like if you
pass that uh
argument so it will do the
tensor uh drop out in place we don't
need to get any output from that okay
but it is uh it is okay to have it uh
false
so
let us create a tensor of
let us say same
20 samples so let us say
tensor is equal to
torch dot
let us create a random tensor
and let us say it's a 20 uh
items in it okay
now let us apply uh let us print it
actually
print tensor
and another thing that we will do is we
will
print the scale as well like as we know
that after applying dropout the final
output is scaled with one upon one minus
p okay so what we will do is we will
also print the scaled output
so what we will do is we will multiply
tensor into
one upon
one minus p
okay
and now what we will do is we will
pass through
the tensor with dropout layer
okay it is simply like
we call the model and then input it
to
the model and we will get the output
okay so now let us run and see the
output
so now if you look at carefully this is
our original tensor
and this is our scaled tensor okay where
uh the output would be so at you can see
this is the output after the dropout
layer okay so in this you will see like
some of them are
zeros okay
that's what it does and be careful here
like uh
number of once we get those many would
be zero okay
so
let us look at it again so here we are
saying that
30 percent of time it will uh produce
the bernoulli distribution will produce
once and those ones will result into uh
dropping out are zeroing out the input
tensor okay so in that sense 30
of
the input tensor would be zero
okay
so here we have one and only two zeros
which is like
10 not 30 okay so if the size of the
tensor keep increasing uh it is more
likely that it will be exactly same as
much we said as a probability
okay and next let us look at like how we
how it behaves when we uh do the
evaluation okay so to convert a model or
any pi torch model to uh
into eval mod by default when we create
it it is in trend mode okay and to
convert convert it into a valve what we
simply call
dropout model is equal to dropout dot
eval and that's all so when the model is
in evaluation mode the dropout layer is
actually disabled so in other words the
dropout
sampling would be always a identity
function okay so it will always uh
always sample all zeros in bernoulli
distribution okay so nothing will be
masked in that sense or nothing will be
zeroed
so now if we will pass through the
tensor through the dropout model it
won't change anything it will be exactly
same as
the input tensor okay so let us run and
see it
so now if you look at carefully here it
is exactly same as what we pass as the
original tensor okay
now to convert it back into a trend mode
we simply
run dot trend on the
model okay so to convert it into train
we simply say
train
okay now the dropout model is converted
into train mode
and now it will do the dropout again
okay so let's run and see it again
so you can see here now it is again
started uh doing the zeroing okay in the
in this case you can see it is much more
like uh earlier
and uh as we increase the
number of
samples in a batch for example let us
say now we have two batch two samples so
each sample would be zeroed
independently so thirty percent of the
first sample would be zero and thirty
percent of the second sample would be
zeroed okay
so it is done on sample by sample not
the whole input that you pass it through
okay
so now understand that uh how it works
so now if you look at carefully this is
the first input
so it is uh masked independently and the
other one the second one is also masked
independently okay so i hope that is
clear like how dropout works
in general with uh by torch model
and uh in next video we will learn
another very interesting uh
regularization technique as well which
is called layer norm and that will
complete our all the
basic components required to implement
the transformer layer okay so thanks for
watching bye for now take care see you
in the next