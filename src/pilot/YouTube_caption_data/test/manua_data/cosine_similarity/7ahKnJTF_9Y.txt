Let’s discuss about another word embedding
technique: Glove
i.e.
Global vectors for word representation
In this technique, we map each word to some
high dimension vector.
Suppose if man is to woman, then king is to
………
Fill in the blank.
such type of problem can be solved by Glove.
It will return answer as queen.
We will use pre-trained model here.
You can download pre-trained glove from this
site.
I will paste this link in the description
below.
Click on this glove.6B.zip
Extract the file and keep it in the folder
where your python file will be located.
I have already extracted the file
This is the file or folder.
If I open this folder, there are total of
4 files with different dimension.
I will use 50 dimensions glove in this video.
You can check others file by yourself.
Now, let’s do code.
First you have to import several packages
like numpy, pandas and os.
Then, check the current working directory
I am inside this folder right now
whereas my all python file is located at this
path.
So, I have to change the directory.
I have to add this path more.
and I am doing the same thing.
Let me check my current working directory
again.
Now, my current working directory got match
to the directory where my this glove.py file
is located.
I can list the directory that are inside my
glove.6B folder
There are so many glove files with different
dimension
As I have mentioned before, I will use glove
file of 50 dimensions.
Now, I have to read my file.
For this, I have made one function that take
my file_name as a parameter.
Then, inside this function, I have to open
my file in read mode.
I have one empty set.
Do you know about set?
In set, there will be no duplicate element.
All the elements are unique to each other.
In this set, I will store my all vocabulary.
Next, I have one empty dictionary where I
will store all the vectors of a particular
word.
I will iterate over each line of my file.
Remove the white space at the beginning or
at the end of that line.
Split the line and put them into the list
so that I can iterate over that line.
Then, I will add my vocabulary in the set.
Let me show you, what my file looks like.
You can see, in the index 0, I have vocabulary
and after index 0, I have word vector of that
vocabulary.
Same for other line also,
this is my vocabulary
and this is corresponding word vector
I want to add only vocabulary in my set and
all vocabulary with their corresponding word
vector in my dictionary.
So, here I have to pass 0 index
that is, add vocabulary in my set
and in my dictionary, I have to add vocabulary
as my key and word vector as my values
I hope you know; dictionary has key-value
pair.
and I have to add my word vector as an array
and also, I have to slice index from 1 to
the end of that line.
Finally, print the length of word vocabulary
and return word vocabulary and their corresponding
word vector.
Let me execute this function.
Next, I have to call my function.
I have to pass my file name as an argument.
This step may take 30 second or 1 minute to
execute.
Next, I have made one function to calculate
cosine similarity.
In my previous video, that is in my project
1,
I have imported cosine_similarity from scikit-learn.
But this time, let me implement this function.
You know the formula to calculate cosine_similarity.
That is, numerator will be dot product between
vectors
and denominator will be the product of their
magnitude.
Pause the video and look this function by
yourself.
This is how you can calculate cosine similarity.
That is, if you calculate Euclidian distance
between two similar vectors and if the size
of two similar vectors vary a lot
then, their Euclidian distance will be high
and they are considered unsimilar vector which
is false.
So, we use cosine similarity where the angle
between them will be measured.
So, size doesn’t matter this time.
If you want to learn more about cosine similarity,
feel free to pause the video and search about
it.
Now, let’s predict the output.
We have already obtained word to vector from
our pre-trained glove file.
Using that file and using cosine similarity,
we can find the similarity between two vocabulary.
Let’s see the similarity score between king
and queen.
It returns 78%
that means, king and queen are similar to
each other.
Let me pass two unsimilar words: Father and
apple.
It returns 26%
that means, father and apple are not similar
to each other.
Let me pass Man and Women which are very related
to each other.
It returns 88%.
that means, they are very similar word.
Next, for clothes and shoes
it returns 86% which is also true
because both of these we wear.
Next, let me pass unsimilar words: Water and
USA
It returns 14%
that means, these words are not similar to
each other.
You can see, how accurate these results are.
You can also find some word which you may
get wrong prediction
but that’s okay
It is working pretty well
You can check some other words by yourself
We have used pre-trained glove vectors for
word representation.
If you have any doubt, ask me in the comment
section below
If you like this video, hit the like button,
subscribe my channel IG Tech Team
I will meet you soon.
Stay safe and keep learning.