I ran a quick experiment.
I trained three models
on the same dataset.
All three models
used the same architecture,
optimizer, learning rate.
Pretty much everything.
But I did use a different batch size.
This here are the plots showing
the losses of two of those models.
Look at how different they are.
What is going on here?
Let's find out.
We are going to do this
by training the same model
with three different batch sizes.
First, using a single sample
from our training dataset.
Then we're going to try
the entire training set,
all the data at once.
And  finally, we're going to use a few samples,
more than one,
but fewer than the entire training set.
Here's the code,
and you can find a link to this notebook
in the description below.
 Here I'm using a combination
of Scikit Learn and Keras
to create a fake dataset, 
train it, and evaluate the three models.
The first thing I have to do here
is create that fake dataset.
And I'm using the make blobs function
from Scikit Learn.
You're going to see that I'm using
a thousand samples,
from which 80% are going to go to the training set,
and the remaining 20%
is going to go to the test set.
After creating that dataset,
I'm defining here a couple of functions
that I'm going to use on all three experiments.
The first function, fit model,
receives the batch size
that I'm going to be experimenting with.
It creates a very simple neural network
with a couple of layers:
one hidden layer with 32 neurons
and the output layer.
Then I'm compiling the model with gradient descent.
And finally, I'm fitting that model.
Very simple fitting on the training set.
Then I'm going to be validating this model
on the test set.
And notice that for the batch size,
I'm using the argument
that I'm passing to the function.
Evaluate is very simple.
First, I'm evaluating the model on both the
training and the testing set
so we can print the accuracy of the model.
And finally, I'm plotting the losses
from the training and testing processes.
With all of these in place,
we can start looking at the first experiment.
This experiment uses a single sample.
And whenever we use a single sample
from the training dataset,
we call it Stochastic Gradient Descent.
In case you don't know what this line does,
it allows me to compute
how long this cell takes to run.
Here I want you to focus on a few things.
First, notice this 800 here.
That's the number of times
the algorithm computes the loss
and updates the model's weights
during backpropagation: 800 times.
And it's 800 because
we have 800 training samples
and we're using one sample per batch.
And this, of course, means
that this experiment
takes a long time to complete.
Three minutes and 22 seconds.
Having to compute the weights of the model
so many times
is very, very computational intensive.
Now, I want you to look at the chart.
It's crazy.
You definitely don't want
that much noise in your losses.
The reason this is happening is
because we're updating the loss
for every single training sample.
Finally, the accuracy of this model
is in the high 80s.
It's not great, to be honest.
But there is something even more interesting.
If I run this experiment again,
the results could look very, very different.
Much better accuracy. Or worse.
I just don't know.
And this happens
because all of that noise.
You see, the loss keeps jumping,
and it's very hard for the algorithm
to settle on a good solution.
Even if it reaches the global minimum,
it might just jump right out of it.
It's just too noisy.
Let's look at the second experiment.
And this time
I'm going to use the entire training set
as the batch size.
Using all of the data at once
it's called Batch Gradient Descent.
Notice that here it says one instead of 800.
We are now updating the model's weights
once during every epoch.
So we went from 800 updates
down to a single update.
So of course,
this model is going to run
really, really fast.
4.58 seconds.
Compare that with the three minutes
and 22 seconds from before.
And look at the noise on this chart.
It's none.
It's zero noise.
The accuracy: 93%, 94%.
That's pretty good compared
to the high 80s that we had before.
So obviously, this looks better.
But there is a big problem
that doesn't show here.
Actually, two problems.
First, this is a toy dataset,
so we can easily fit
the entire dataset in memory,
so we can compute the loss during every iteration.
But in most real applications, forget about it.
You won't be able to do that.
And second, that lack of noise
can get us stuck in local minima.
Just like too much noise
will get the model jumping around,
no noise will get you stuck.
We need something better,
so let's look at the third experiment.
Here I'm using 32 samples...
32 samples for my batch size.
Using a few samples,
more than one,
but fewer than the entire training set,
is called Mini-Batch Gradient Descent.
Now, this 25 here
tells us that we are updating the model's weights
25 times during every epoch.
Now, this, of course, runs pretty fast as well.
10.9 seconds.
And the plot here is beautiful.
You see some noise, but it's very smooth overall.
Training and testing accuracies: the best.
They are excellent.
So we can conclude this model is right on point.
So let me summarize what we learned here.
First, Stochastic Gradient Descent,
which is when we use a single sample
as our batch size.
Really slow, a ton of noise,
and it might not settle in a good solution.
Not recommended.
Second, Batch Gradient Descent,
which is when we use the entire training set
as our batch size.
Very fast, zero noise.
But it requires a ton of memory
and it might get us stuck in local minima.
Not recommended.
Third, Mini-Batch Gradient Descent,
which is when we use
a few samples in every batch.
Biggest problem, by far,
we need to worry about another hyperparameter:
the batch size.
Now, we need to tune it.
We need to experiment with it.
But that gives us the best shot
at finding a good solution
without too much memory or too much training time.
This method is what everyone uses.
Four, plenty of people have run plenty of experiments
and have concluded that smaller batches are better.
For example, 32 is a great default value.
And finally, these are Learning Curves.
They give us a ton of information about our model.
And that's why you want to watch this video here
where I show you how to use Learning Curves
to identify two of the most common problems
in Machine Learning.
And... write some code,
go nuts, build something cool,
and I'll see you in the next one.