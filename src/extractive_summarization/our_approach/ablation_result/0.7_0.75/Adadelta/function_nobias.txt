 If you have gone side-track just remember one thing we are going into so much hassle because normal gradient descent has its learning rate constant for the entire training phase... which is so LAME!  So the optimizers like AdaDelta have some techniques to vary the learning rate with every iteration that's it. 
As the first update is very small, the running average of the updates will be very small at the beginning, which is kind of a vicious circle at the beginning 
 To overcome this issue AdaDelta was born.  In the AdaDelta paper by Matthew Zeiler, he talked about this drawback of AdaGrad saying "due to the continual accumulation of squared gradients in the denominator, the learning rate will continue to decrease throughout training, eventually decreasing to zero and stopping training completely. 
 This was made possible due to this rho hyperparameter.  It is also known as 'Decay Constant'.  Instead of writing it with this equation, I have written it separately because we will need delta theta in this equation.  So, this term over here is the learning rate calculated by AdaDelta. 
 We are dividing square root of delta x with the square root of alpha... again this epsilon is a very small number to avoid zero division error.  Then we can sum it with our theta term... normal gradient descent stuff!  At last, we will update our initialized delta x term by rho multiplied by delta x of the previous iteration plus one minus rho into update square. 
