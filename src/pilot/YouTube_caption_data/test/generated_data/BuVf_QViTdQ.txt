okay can you can you see my screen I
think so oh yes yes okay great okay so
uh let's begin uh today's uh lecture so
I'm I'm going to give you a lecture on
variational auto encoders and I was
planning to give you a brief
introduction about myself and what I did
but Mahesh already did that uh so I
would be skipping most of the points
here
uh so before I finish my PSD I was at
ioe pulso campus doing my electronics
and communication engineering uh over
there I uh with some of my friends we
did this thesis about vehicle over speed
detection and recognition
and this was one of the starting point
for myself to get into this amazing
world of machine learning or AI so my
general research interest that was also
explained by Mahesh so I'm interested in
machine learning and within this I am
interested in topics like
disentanglement semi-supervised learning
and generalization and in terms of
application I am interested in how we
can use AI in biomedical domain
especially around medical amazing and
genomics and recently with some of my
colleagues at Stanford we have started
this newsletter to cover the recent
research around application of AI in
biomedicine so if you are interested
please sign up to this newsletter
okay now let's move on towards our uh
session today so uh the these are the
outlines of our today's lecture so I'll
be first discussing about Auto encoders
uh then I would like to briefly explain
about the genetic modeling but I guess
you already have received a very
in-depth lecture on generative modeling
so this should be fairly straightforward
for you
and then with the introduction of
generative modeling we would try to look
into the rational inference and these
topics would then naturally lead us to
uh variational Auto encoders and in
terms of applications I will try to
briefly explain uh some of the
application and among them I would try
to give not detail but still a good
explanation of disentanglement
and in between along the way if time
allows we would also try to see uh the
demo and for the demo I have written
some quotes in Python and pytorch uh if
even if you are not familiar with these
uh languages or Frameworks I hope uh you
pretty much get the ideas of what we
want to achieve
okay so let's move on towards our uh
topic first uh first we will discuss our
encoders so um since you already have
received one whole week of
schooling about machine learning I guess
Auto encoders already came somewhere
along the way so I would like to point
out that auto encoder first of all is
simply a type of neural architecture
um you might already have a CM CNN which
is also a type of architecture or RNN or
lstm which are also another type of
architecture
and within the types of machine learning
we usually try to divide machine
learning into different learning
Paradigm one is supervised another is
unsupervised and another is uh
reinforcement learning so I would like
to place Auto encoders within the
unsupervised learning Paradigm because
here we do not require the annotations
of the data set but still there are
cases or research that are happening
where Auto encoders are also being used
for uh supervised learning or even
reinforcement learning
so different components of Auto encoders
uh basically include encoder and the
decoder so here as you can see in this
pictorial
representation of Auto encoder that we
start with some input and then we pass
it through a stack of neural networks to
get into this code and then use this
very same code to reconstruct back our
input
so there are two major function one is
the encoder which uh tries to transform
our data uh into some low dimensional
latent code and then we have another
function uh which is known as decoder
that tries to transform this latent code
back to the data space
now here since we do not have our X and
Y or our data and the labels you might
be curious what's the use of Auto
encoders so some of the reasons on why
our encoders were initially considered
and how it has been used in machine
learning research includes first of all
compression so
similar to how we compress audio files
to MP3 format or images into jpeg format
Auto encoders can very well be used to
compress our data set so in into a small
representation so that it can be used
through Transportation or sharing or to
understand different aspects of the data
set
and then uh another concept that has
been quite popular these days especially
with the rise of deep learning is the
representation learning where we try to
represent or where we try to learn some
abstract representation of the data set
so Auto encoders are extensively
considered when we want to learn such
representation from the data set and
third which uh would already be quite
obvious is to reduce the dimensions of
the data set so we are we often work in
a setting where we have this High
dimensional data set and by the way this
demand dimensionality of the data set is
growing every day because we are now
considering uh even
data set like a satellite imagery so in
such situation having an algorithm or a
technique that can reduce our data set
without losing much of the information
would be very much useful and auto
encoders can be used in that setting as
well
by the way um Mahesh already told us so
if you have any questions along the way
please feel uh feel free to stop me and
we'll try to address if there are some
confusions along the way
okay now this um
introduction about Auto encoders if we
have to represent it in some
mathematical notation then we can
consider that our data set BX and then
with some function let's say Q which
have a parameters Phi we can encode it
to some latent code Z so that means our
encoder is represented with this
notation that is z equals to Q of x
similarly we have another function B
with the parameters Theta to decode our
latent code back to the data Space X hat
so the expression of the decoder would
be this x hat is equals to A P of Z
now
um we have these two set of parameters
Phi and Theta and the auto encoders in
Auto encoders we try to learn these
parameters simultaneously by minimizing
the Reconstruction loss LR here the
Reconstruction loss LR
is basically a measure of discrepancy
between uh certain between our input X
and its reconstruction X hat over the
training samples now you might be
curious uh how would we measure the
discrepancy between the X and the
reconstructive sample X and it largely
depends on uh the type of the data set
for instance if your data set is bounded
you might even use loss functions like
cross entropy or in general situation if
your data set is unbounded then you
might consider it a mean squared error
as shown here
now with this introduction uh I would
like to bring up an another topic that
is uh the concept of regularization
so although we discussed about Auto
encoders and how we want to train our
encoders was by minimizing the
construction error but in general uh
different research they often include
extra regularization
so the reason to use this extra
regularization is that if we do not have
any extra regularization the auto
encoder can learn some some optimal
solution and they can simply duplicate
our input features with perfect
reconstruction without having extracted
any meaningful features
now
to address this particular problem if
you notice in our earlier slide in this
figure let's say we have reduced our
input uh
this High dimensional input all the way
to this low dimensional bottleneck so by
just having this architectural system
where we are reducing high dimensional
data set into a low dimensional latent
code we already have this regularization
in hand
without having such bottleneck the auto
encoders could have learned uh simply an
identity function but by having
regularization we have stopped that but
here I am actually referring to some
extra regularization and some examples
include for instance if you want to
learn sparse features then you might
um you might consider sparse or encoders
or let's say if you want to learn a
latent code which are insensitive to
changes in the input then you might
consider the auto encoders like
contractive Auto encoder and denoising
autoencoder since today's we are going
to talk more about variational Auto
encoders I am not going into the details
of these specific Auto encoders but the
main message that I wanted to give you
through this uh these few slides was
that whenever we are considering an auto
encoder I would encourage you to think
about a loss function like this where
you first try to minimize a
reconstruction error and then there is
another regularization term that you
could consider depending on what sort of
a problem you want to solve
so the main takeaway uh from this
particular portion of our discussion is
that we have autoencoder which is often
trained by minimizing a reconstruction
loss between our original input X and
the
reconstructed input X which which we get
by first passing the X through some
function Q of I which would give us a
latent code and then passing that latent
code to another function P Theta that
would give us reconstructed input
and
and in as I said and in most of the
cases we use some extra regularization
beside this reconstruction loss so this
is the main takeaway that I would like
you to
um like you to take from this portion of
the of our session
okay now let's move forward
um so now I would uh I would like to
move on to genetic modeling so it would
we are completely changing the tracks
here so
um uh so bear with me on this one and I
I know that you already have received a
very extensive lecture on generative
modeling so I would not
um bore you again with the same details
I hope uh and I would like to only start
from the intuition here so Ingenuity
modeling
um the basic intuition is that we want
to generate some data
um if we want to denote it with X so
let's say we want to generate some data
X now the data set could be a collection
of images of mountains like here or the
collection of some signals like Electric
electrocardiograms in this group so if I
were to ask you to generate some images
of mountains or to
um
or or in other words if I were to ask
you to draw something then I how would
you have done it uh if if I were to
imagine that I would first start with
some some shapes or the colors
and then draw some background and maybe
then only I would start to fill in the
details so in the same way we can think
of those attributes as the attributes of
imagination
so for the image those attributes could
be as I just said shape colors uh size
uh the background and so on for the
signals it could be quite difficult to
imagine because uh there it has certain
physical uh meaning behind these signals
for instance these signals are what we
get when we attach the leads in the body
and the shape of heart or shape of our
body and the underlying disease would
give rise to these signals but in any
case even these signals have certain
attributes now these attributes they are
known as latent variables
so the idea is we have these attributes
latent variables and then there is
certain function that give rise to our
data set X now to represent this in a
mathematical way we have a data X and
then we have a latent latent variable Z
and we have this generative process
where we first would sample our Z from
some uh prior distribution P of Z and
then use that sample
uh to our conditional distribution where
we have P of x given Z
now since we have involved latent
variables this would naturally lead us
to a latent variables model which is a
joint distribution of our data X and a
latent variable Z and it can be
factorized in this way
now here I would like you to maybe take
a pause and think about it
um so when when I introduced about
generative modeling I only demonstrated
you or I only put the pictures of the
mountains but I do not and I and I asked
you to imagine the latent variables so
what I want to say is
we have a data X that is observed but we
have the latent variables that is
unobserved so and then we we said that
our generative model involves generating
data from Z so the question the natural
question here is where is where is this
latent variables coming from or where
can we get that and the answer to that
question is in order to get access to
the latent variables we have to rely on
the process called inference so in
inference uh the goal is that we want to
infer good values of the latent
variables given this observed data set
because eventually we can use those
latent variables to do the generation
process
there are different ways to uh looking
look at this inference uh process but I
would like to take a very simpler route
uh and for that I am uh putting the base
rule in front of you I hope this is a
very understandable since we have been
looking at the base rule uh Even in our
stat courses so in base rule we have
this expression where we try to
calculate the posterior using uh using
this expression and
and over here
this PX is the most important for us
this would uh be evident uh in in few
slides ahead but
in this slide what I would like you to
grabs or take is that we have this
likelihood function P of X which is this
expression of integral p x given Z and p
z of DZ so what we are essentially
trying to do here is we are
marginalizing out or sum out the effect
of our latent variable Z
so using this page rule we have P of X
which is like this and then we have uh
we have our prior pz and then the
conditional of X Cube and Z so it's it
sounds nice and simple with this we we
obtain our latent variable Z
but the problem here is we do not know
how to solve this integral function why
we do not know is the the relation
between the Z and X often becomes often
has this non-linear relationship and
even sometimes when this is uh this is
solvable we often deal with the
situation where we want to work on a
large data set so this would become
computationally expensive as well so in
general this integral function that we
require to get our latent latent
variables is intractable so when this p
x is intractable this would make this
whole thing intractable
that would now
we require something that can help us
solve this intractability and for that I
would like you to take into another nice
concept known as variational inference
okay so what exactly is variational
inference so the variational inference
is or the idea of the rational inference
is that we want to model the true
posterior distribution p z given X which
uh which why we want to model is I guess
has become quite obvious because we want
to have this generic model and then
since we were unable to uh get p z given
X we kind of accepted the the Crux of
the problem and then we are okay to use
some simpler distribution let's consider
it as Q Z given X which is often known
as a proposal distribution or also
variational distribution
and we are using this Q Z given x uh
with some simpler that is very easy to
evaluate for us and whenever you think
about some distribution that is quite
easy to evaluate I am pretty sure that
the gaussian would come into your mind
because it has this mean and variance we
have a lot of analytical expression
associated with the gaussians
so we
accepted that there is a problem and
then we accepted some some approximated
solution to use this Q Z given X to
solve the problem that is in our hand
now at this point
um you might be curious
how we do not know pz given X and we
decided to use this Q Z given X but how
do we know how well we are approximating
this unknown pz given X and for that
um there is this concept of measuring
the Divergence or in other words we want
to measure how well our variational
posterior qz given X is approximating
the true posterior and for that we want
to use uh this Divergence known as
callback libler or KL diversions
it basically measures the information
lost uh using um using Q when we are uh
when we want to approximate the p and
the the mathematical expression of KL
Divergence uh looks something like this
where it it is the integral of Q and the
product with log P by Q
I I guess you have another lecture after
watch on genitive adversarial networks
over there you would uh probably learn
about another Divergence known as Jensen
Divergence which has some association
with the kld versions as well
so the the
takeaway message from this particular
portion of the of the lecture is
um we were stuck in a problem where we
were unable to solve this uh problem of
uh inferring the true uh posterior of
the latent variables and then we decided
to use some uh proposal distribution to
solve that and in order to measure how
far we are from that uh that true
distribution we decided to use the
measure of KL Divergence now since we
are here I would like to maybe give you
a very brief uh demo on uh the kld
versions because it has some nice
properties
and for that uh let me go here okay uh
is my uh this collab visible
okay
[Music]
okay great so
um so yeah as I said in the beginning I
am using Python and all of these quotes
I will put that in the in the assigned
Google drive folder so you don't you do
not have to try to note it down uh or or
do oh you can use zoom it a little bit
okay
okay I hope this is more visible now
okay great
um so so here I want to uh I want to
give you the demo of uh the KL
Divergence and for that I am using this
uh sci-fi stat python Library uh it's
it's fairly straightforward so I'm not
going into the details of what each of
these uh expression mean
so these this cell I just loaded some
libraries in this cell I am defining my
data space and then the most important
part is this cell where I am defining a
distribution and I Define it as a
gaussian distribution the notation for
the parameters is different uh because
they have not referred it as a mean and
variance but they use the location and
skill but let's not
um get into the details um let's just
understand that I have defined a
distribution P which is a bell set
gaussian as you can already see in this
figure
so this is my true distribution now
um in this slide what I want to do is I
want to Define another distribution Q
um and I just want to change the
location or I just want to change the
mean so when I do that I am I am
assigning this index which is coming
from these two for Loops so even this is
not very important but basically I am
saying that this queue would be changing
over time and in the first uh first
instance of my Loop since my index would
be 0 the location would be exactly same
as my true distribution the one that I
defined before that is 0 but then
afterwards this location is going to
change that's why I want to see how my
kle changing so I calculated the KL
between p and Q and I see that in my
first case of this for loop I have a
result as 0 because both the
distribution p and Q were exactly
matching but then it start to get
increase so this shows that how KL
measures the distance or Divergence
between the two uh between the two
distribution
now um
another important thing about the kles
uh although I just said it measures the
distance I would encourage you to not
use the term distance because whenever
we think of a concept as a distance we
usually associate that if I measured a
distance from X to Y versus if I measure
a distance from y to X both of them are
equal right but in KL the versions this
is not true because the KL is
asymmetrical that is if we measure from
P to q and if we measure from Q to P it
would result in a different thing and to
see that I I saw that I have I have
defined p and then I have defined q and
then you can see that the result for the
for these two things from P to q and Q
to P would come different
okay I I hope this asymmetrical part is
also clear
now uh I have another I have written
this code for the visualization so
basically I am placing uh the green as
our true distribution and I'm only
moving this red that I consider to be q
and we see that as I move this thread
towards the green the KL diversions uh
get reduced and eventually when both red
and green matches the kale uh is equals
to zero
okay uh I hope this short demo was uh
helpful
okay let's go back to our uh to our
slide
[Music]
okay
okay so uh so yeah uh we Now understand
that KL we we will use or we want to use
KL diversions to measure the Divergence
or the gap between our
um
uh true posterior and approximated
posterior Cube
okay now
we had this problem uh
uh where we had this likelihood function
we were unable to solve this because it
has a lot of issues that lead us to
intractability and that affected our
ability to uh to uh to capture the true
latent variables that's why we went to
the variational inference now I would
like you to like to show you how we can
use the variational inference to solve
the problems at hand
so we start with this integral problem
and then
um we append or add this proposal
distribution both on denominator and
numerator so this should be fairly uh
easy or fairly easy to understand
then
um I would like to take log on both
sides on both left hand uh side on the
right hand side and I have doing a bit
of advancement here and there so as you
can see I just moved the Q Z given X
under pz but this is fairly
straightforward
then
this might be a bit confusing then what
I did here is I took this log inside the
integral and to do that I am relying on
this inequality which is known as a
Jensen's inequality which
um which explains that the log of
integral is greater than equals to the
integral of the log so that's why my
equals to sine has now become the
greater than equal to sign
okay now
once I use this inequality I would like
to do bit of uh like uh changes inside
the log so what I did here is uh uh I I
guess this property of log uh is uh
clear that is when we have a product
inside the log we can split it uh into
two groups as the sum of logs and when
we have this division we can split it as
a differences between the logs so I am
basically uh changing uh dividing this
expression into two groups based on uh
the log
and then
um then we now have these two integral
expression
uh
and then if you notice uh this second
expression suddenly becomes familiar to
us it actually now resembles the
expression that we sewed before for the
KL diversions here it's an integral of q
log Q by P so now we can replace this by
the KL term so in total our final
expression becomes uh the evidence the
log of evidence is greater than equals
to to this first expression minus the KL
term and I have changed this first
expression that was the integral to the
expectation which is also which should
not be very confusing because we use
expect we use integral for the
continuous and expectation to usually
denote
the broad spectrum
okay so uh this final expression uh
which I have referred as a variational
lower bound is going to be very
important in the the consequent slides
that we are we would be looking at
and this derivation
um should be uh if if there are any
confusions uh in this television I would
be happy to take uh your questions if
not we can move forward
okay I hope that this was uh pretty
clear
okay so using variational inference uh
we were able to solve this uh the
problem the integral problem that was uh
lying in front of us
okay I see a chat message here
okay
um
so this variational lower bound uh is
also sometimes referred to as a elbow or
evidence lower bound so whenever you see
these different terms uh don't get
confused or try to associate them to be
similar thing
so this expression is basically what uh
we were able to derive in the earlier
slide now I would like to give you some
notes on this particular expression so
oops so the the two notes uh that I
would like to draw your attention is
that the first node uh you might uh be
curious or even if you are not we
started with a problem where we wanted
to use scale Divergence to measure the
uh the gap between the true posterior
and the proximated posterior but in this
lower bound this scale term is not not
that particular
um not that particular thing because
here as you can see that this scale is
actually measuring the gap between the
approximated posterior and the prior
so you might be confused on where that
kale term went so to answer that uh if
you if you would have taken a different
route and then derive it in a different
way then you would reach to this
conclusion where your log P of X is
equals to the Elbow term plus the KL
term that was the initial motivation for
us to begin with
this is also something
um I would like you to take a pause and
try to absorb this
Okay so
and and when when you have understand
this point this uh this point a then
this would naturally lead us to point B
because the KL which measures the
Divergence is always non-negative so
that means that when when you maximize
this elbow term this would naturally
mean that you are minimizing the KL term
because the the evidence is is is not
going to change right you are only uh
only modeling your uh latent variables
or you are you are approximating your
latent variables so the better you
approximate it using elbow that means
you uh you are trying to minimize that
gap between the true posterior and the
proximate posterior
okay and to uh to take this concept uh
in a in a more schematic way I would
like you to leave with this with this uh
like a scribble or the plot scribble of
the plot uh that should uh give you the
concept of what I just said so we had
this um this green curve which was our
uh our original log P of X and then we
approximate this very complex
um Phenomenon with some simpler
distribution and then we uh we model it
and but still we see that there is this
Gap that we were unable to
unable to fill in or the better we model
our elbow this Gap would be as close as
possible
so this is very important Concept in
variational inference and there there
has been a lot of research or even when
uh when you want to dig dig more deeper
into this topic you would definitely
come across a situation where you would
better want to minimize this Gap as as
as much as possible
okay now uh we have introduced these
topics now this would uh lead us to uh
the main algorithm that we wanted to
discuss in today's lecture
and um
the the background of variational
autoencoder in terms of
um in terms of how we went there we
already covered but in terms of who
introduced it it's a very interesting
story because the Western Auto encoder
actually came out
um in parallel from these two papers uh
pretty much about the same time so uh
this Auto encoding variational bees came
from a completely different group and
then the the the the and other people
known as a stochastic back propagation
and approximate inference in deep
genetic models came from entirely
different groups but both of them uh
were explaining the same algorithm
and the first one was published in iclr
and the second one I yeah I forgot to
mention here but it was published in
icml
and I am using this uh this could this
quote uh because I really like how the
authors write this code and this is
actually from the second paper so they
say that variational autoencoder marry
the ideas from Deep neural networks and
uh proximate Bayesian inference to
derive a generalized class of deep
directed genetic models and here
um I would like you to notice at these
two terms so the first one is deep
neural network and the second one is
approximate BGN inference so from for
the Deep neural network you can
understand the concept of Auto encoders
that we discussed in the earlier portion
of our lecture and the and approximate
Bayesian inference would be the ideas
from the variational inference
so so as I just said the perspective of
variational Auto encoder I would like
you to try to get it from both both
sides that is the probabilistic and deep
neural networks so from the
probabilistic perspective it's a
generative model where we want to
generate X from some latent variable Z
and at the same time we do inference
from X to Z and from a deep neural
networks perspective we have this data X
and then we pass it through some encoder
which are the layers of neural network
to get our Z and then use the same Z to
decode it back to the input Space X hat
okay as I said in the beginning when we
derive this expression that this is
going to be very important so I am
bringing it again
so this uh this evidence lower bound or
the variational lower bound is basically
the objective that we want to solve for
the variational auto encoder so here
this Q Z given X is our encoder that we
that would encode our data X into a
latent representation Z or in other
words or in the the jargon of the
probabilistic language we are doing the
inference of Z given X
and the the
schematic from the deep neural network
perspective is uh what we saw before as
well
and then there is another term uh that
is p x given z uh which is a function to
decode the latent representation Z into
the data Space X or in other words it
generates our data x given Z
so in in in in in the holistic viewer in
totality uh variational autoencoder is
the combination of encoder and the
decoder and here the objective is to
minimize the Reconstruction error and
some regularization so if you remember
uh when I was discussing about Auto
encoder I was giving you this this
General takeaway where Auto encoders are
trained
first minimize the Reconstruction error
and then depending on the problems at
hand we would add certain regularization
so from that point of view you can say
that or encoder animation or autoencoder
are the same but it's only the
perspective because we just see that
these two we arrive at these two
algorithms in a completely different way
so here
the minimization of reconstruction error
or our LR is represented by this first
term of our evidence lower bound and the
regularization is represented with the
second KL term that we have here now I
will try to go into uh details of of
these things
okay
so investor encoder we have these uh
different components
so the first component that I would like
to introduce is uh this Q5 Z given X
which was our uh way to approximate the
true posterior so we are using a
multivariate gaussian with a diagonal
covariance to approximate our true
posterior at this point you might you
might ask or you might argue that no I
don't want to use a multivariate
gaussian I would like to use something
else some other distributions then the
answer would be yes you can do that
there are so many research that are
actually doing that but when version
autoencoder was first introduced they
started with this simpler distribution
and here the MU and the sigma Square the
parameters of our gaussian distribution
they are basically the outputs of the
deep neural networks
and then I would like you to draw your
attention to this um
this generative model that is the x
given Z
which we use uh uh which we try to model
using the Bernoulli or multivariate
gaussian or again depending on what sort
of data set that you are using for
instance if you are using
um amnest data set a data set like mnist
which is bounded then you might want to
use a maybe Bernoulli or if you want to
explore the the whole variance from the
data set then probably you would want to
use multivariate gaussian so it all
depends it all depends on the data set
in this case
and then for the pz which is the prior
for our latent variables uh we are using
the multivariate gaussian via the
standard isotropic gaussian which has a
mean zero and the covariance as an
identity function
now the interesting thing about uh the
use of isotropic gaussian as a priority
is that the KL Divergence This this term
that we have here in our elbow
between a multivariate gaussian and
isotropic gaussian would become very
straightforward a very easy analytical
solution so here as you can see I am
taking a KL between uh gaussian which
has a parameters mu and sigma square
with the isotropic gaussian so the the
solution as you can see is is very easy
we can if if you think from
implementation point of view uh you you
should be able to write a code pretty
easily and this is not just about
writing code but also in terms of
optimization it becomes very easy to
solve so uh so we are we are trying to
use everything that is that is very easy
and and we can model the algorithm
um uh so that we have a very less
difficulty
Okay so
so uh in this slide
um we um We Now understand what is uh
how we are modeling our queue how we are
modeling our p and what is our pz now I
would like to give you uh this uh the
structure that would naturally lead from
our previous slide so here we started
with x
we have some stack of neural network it
could be
um non-linear like a multi-layer
perceptron or it could be CNN if you are
using an image data set it could be RNA
anything it depends on the problem
but then after the encoder you
approximate or you uh you you you try to
capture the mean and the covariance of
your date of your Q Z given X once you
have the parameters of that distribution
you sample
um your Z which is your latent variables
and then you pass this sample this
sample through the decoders which again
uh could be a multi-layer perceptron or
CNN and then you arrive at your
reconstructed uh sample X then you
you would use a loss function to
minimize the Reconstruction between this
x hat and this X and then you would use
your KL term to minimize the Divergence
between the Q Z given X and pz
okay everything looks uh looks nice
um
but the but
everything is not nice I actually wanted
to uh say that but by first introducing
about this concept and then bringing
then wanted to say you that it's it's
not it's not fine
why it's not fine is in this flow of uh
VA model the the random node this this
one the one that I have colored as gray
this random node or Randomness this is
happening
um inside the model which means that for
the back propagation which we need uh in
deep neural network training we need to
differentiate with respect to this
random sampling and in general uh when
we deal with the Deep neural network
this is not possible
because we cannot back back propagate
through this sampling node instead while
proposing the variational autoencoder
the authors they proposed the solution
which is famously known as a
um re-parameterization trick
uh
where the authors they try to make this
Randomness that was that was presented
in this way via the Epsilon and this is
happening not within the model but this
is happening actually outside the model
so so with this we do not have to worry
about learning and hence calculating the
gradients for the Epsilon because we
basically treat it as a fixed stochastic
node it's still stochastic that is we
are still getting the randomness but
it's fixed now we would only learn the
mu and covariance and then we have this
nice analytical equation to get our
latent variables so in this way we have
this two perspective or two part uh to
do the optimization one where we need
learning or where we need to calculate
the gradients and over there we do not
have any sampling
node so that the gradients can pass
through the model and then second there
is this stochastic node where which is
fixed because we do not want it to
change over the time and we can we are
happy to keep this as a constant
okay so
um I I would like you to probably absorb
this a bit because this is really this
was actually the major Innovation when
the authors propose variational
autoencoder because beside that most of
the things was already lying there they
just added the pieces and then this
particular trick that give rise to the
variation Auto encoder
okay since uh I did not do not see any
question I am assuming that this is
fairly uh straightforward to you
okay now this would lead us to some sort
of a pseudo code if you want to think
how would one Implement uh implement
this idea if they have to write a code
so
so you I would like to draw your
attention to this train function so as
you can see I first start with uh
capturing the ax or I basically sample
the batch of data set from my training
loader depending on if you use pythons
or Keras the the notation may be
different then I pass this x to some
encoder module which would give me two
things uh the mean of Z and the log of
variance of Z
and then I sample the noise that is that
was the Epsilon in our earlier slide and
then using these approximated or these
uh parameters of approximated
distribution I calculated the Z of a
standard deviation and then I have this
analytical form for my Z then I pass
this Z to A decoder function to get the
X hat and then I have a loss function to
calculate the loss for the variational
auto encoder so in this way using these
few lines we arrive at a uh we we can
Implement a variational autoencoder now
if you would like to see what this loss
function looks like then I have passed
these four different terms as an
argument uh the X was the original input
X hat was the reconstructed input and
then this z mean and zlog where where
the parameters of our approximated uh
posterior and you see that I first
calculate the Reconstruction loss using
some measure of discrepancy here I am
considering the mean squared error and
then I am using the KL loss which I
pointed out few slides back that because
we are using the scale between the
multivariate gaussian and isotropic
gaussian it's it's very simple to
calculate that
and then my total loss is the summation
of the Reconstruction loss and Care loss
um I hope this is uh this is clear if
not I would be happy to take questions
here
okay I I hope uh I hope this is this
means it's clear but I would really like
you to um
yeah when when I was first studying
about variational Auto encoder there
were there was one uh important question
that I had when I would look into this
pseudocode was that why would someone
try to get the log of that why they are
not getting just aware
or why since we started if you remember
whenever I am drawing you the picture of
this Felicia autoencurer this or that I
always use the mean and this variance
because these were the parameters of
my approximated
posterior but then every time when I
look at the code the people used to
write a code that would give them log of
air so this was very confusing to me so
if you did not notice I I will try to
explain that or if you already know it
would just be a reason to you
so we often try to capture log of where
because you see that the variance this
would be always uh positive right
because variance is a is a square term
and we cannot have negatives in the
Square term so by capturing the log of
where we are allowing our neural
networks to model however it would like
to model our data and then with log log
where we can simply reverse it to get
the variance so that was that was quite
uh interesting to me anyway I hope this
is now uh clear to you as well now with
this I would like to go into another
demo
um that I have written
okay so this is now as before I would
try to zoom in
okay so uh the framework that I have
used uh to write uh this very basic uh
implementation of variation autoencoder
is uh through pythons so please accept
my apology if you do not use pytots and
use something else
okay let me try to see if okay it's not
giving me but anyway I have the result
so here in the first uh cell we are
loading the libraries
uh and in this seconds cell I am loading
the data set here I am I want to
demonstrate the
uh the algorithm through uh the mnist
data set
so I'm basically loading them
then
I have Define my variational autoencoder
as a class in this function so this
would be this would be more interesting
uh to us
so here as you can see these are the
layers in the encoder and these are the
layers in the decoder so what's
happening here is I start with my input
Dimension 784 because I
I would widen my Ms signal which is 28
cross 28 I would flatten them into 784
even this part is not important so uh
forget about it but anyway we start with
the input signal that was of 784
Dimension and then reached to a lower
dimensional input space
sorry low defense style hidden space of
400 and then we are using two parallel
layers that would give us our 400
dimensional hidden representation to
some Z Dimension where I use 20 uh where
I can use 20 or whatever you would like
to pass into this class
so this is the layers that are involved
in the encoders now in the similar
manner you can you can expand your Z
Dimension to 400 dimensional vector and
then from 400 to your final out input
space Dimension so these are the layers
that are involved in the decoder
function so your forward function would
look like
when whenever you pass your data into a
model model as X you would first get mu
and log where
then you do some re-parameterization to
get your latent samples or latent
variables Z and then you simply pass
that latent variables to the decoder to
get your reconstructed input so this uh
this series of codes should be fairly
straightforward now if you would like to
understand what each of these functions
are doing it's also simple I'm basically
just using those layers that I defined
earlier to get my
uh parameters of my approximated uh
posterior that was mu and log where and
my reparameterization also looked like
what we just discussed in the pseudo
code
okay so I hope this code is very easy to
understand and then in this cell I am
defining
my model and my Optimizer now in terms
of Optimizer you might have seen or
maybe already learned different
optimizers like stochastic gradient
descent or the variance of SCD so the
atom is also one such variance and is
often considered with the variation Auto
encoder I'm not going into too much
details but if you are curious and want
to use
um stochastic gradient descent here I
would you are welcome to do that and I
bet your result is not going to change a
lot
at least in in these uh demonstration uh
purpose
okay then we have a training function
which is again uh the extension of what
we just saw in our pseudo code and you
see that uh I have this loss function
where uh since we are using the amnest I
am using the binary cross entropy and
not the mean square error because it's
bounded and I can get better result with
the binary cross entropy and then the KL
loss term which again I I have probably
I have said it too many times that since
we were using a kill between the
approximated posterior which was from a
multivariate gaussian with the isotropic
gaussian it's really easy and comes into
this nice analytical formation
and then our total loss function is
basically the combination of this
reconstruction error times plus beta
times the regularization which is the KL
loss
okay then this is the train function
uh which as you can see I sample the
data then I get then I pass this data to
a model to to give me reconstructed
samples and the parameters of my
approximated distribution and then I
calculate the loss and then back
propagate them
okay I think this would also be easy to
understand then we have this test
function
where I'm just analyzing how my test
loss would look like now unfortunately I
was unable to load them right now but I
I did
pass the this collab to a quick training
and I have trained it for about 100
Epoch but you can play around it play
around with it I'm not going to train
and this is this may not be very
important okay now consider that we have
trained our model and as you can see
that the loss the lost the loss values
uh if you see are the test set loss it
was decreasing so we can assume that the
model did train nice and fine I can
definitely make it better but it's it
should be good enough for us to look at
the result
okay now here I would like to visualize
how well I did the Reconstruction so
um so here I basically sample the data
and then pass it through my model to do
the Reconstruction and then I
concatenate the tensors and then uh made
a grid and then doing the I am using Im
so I am visualizing them so these part
are just uh technical details that I I
assume that all of you have a very good
coding skill and it should be very easy
so you see that the on the top row these
are my input signals and then on the
bottom row I was able to do the
Reconstruction by passing all the
signals through this small bottleneck of
uh what did I use yeah through the small
bottleneck of just 10 Dimensions so this
is if you think it this is really
interesting right because by just
utilizing this 10 dimensional latent
Vector you were able to reconstruct your
input almost perfectly uh because this
the the dimension of these input signals
they are very huge compared to that 10
dimensional signal
so so this is very interesting
properties of Auto encoders or vegetable
Auto encoders now
um
if I were to give you a demo of
variational autoencoder sorry just an
auto encoder then I would probably stop
myself on this cell but since we are
looking at the variational autoencoder
which is also a generative model we can
look at the generative property of the
variational autoencoder so for that I
get the Epsilon which is basically a
standard noise
and then I pass this noise to do the
decoding so so I am just starting at a
noise and then uh then passing it
through my decoder and I can think this
process as a generative process and then
when I do that I see uh that the model
has is is generating for this particular
noise Vector the model is generating
something like this uh you might say
this this looks almost like a noise this
this is uh
or someone may see this as a digit eight
uh feel free to put your imagination but
but the point here is that the model is
trying to learn something it's it's
trying to generate something here
okay we have some other things as well
but I would come at them again after we
discuss some other topics as well
okay
let's go back to the slide
okay so um I hope the the pseudocode and
the subsequent uh the Python's code was
uh
was helpful for you to understand and
even if you want to later on go and try
to implement this yourself
um I would definitely leave the code but
I I think that even without the code if
you have the concept uh well understood
then this should be very easy the
variational auto encoders is really easy
to implement
the the complexity may arrive depending
on your data set for instance if you
want to model some high dimensional
image data set and maybe the uh the the
architecture that we draw in that code
may not be enough to to capture the
complexity of the data set but those
things are are something that you can
start to add or play around but the
general concepts is what we just covered
okay now let's try to look at the uh
application of valuation Auto encoders
so
whenever we used to talk about deep
generative models that is the generative
models
with the help of deep neural networks uh
the Gans or generative adversarial
networks that you that all of you are
going to have a very nice lecture in a
while
the Gans always used to come before in
terms of the ability to generate very
quality samples
and the vae always used to lag behind
only recently uh the VA are also being
used to generate these high quality
samples like Gan and as you can see I I
took a snip from this particular paper
which came recently in 2021 and as you
can see the pictures over here they are
they are photorealistic and I actually
reduce the size even if I would expand
the size you can still see the details
in the facial expression so this is
really nice
um achievement that variational auto
encoders are starting to gain in recent
times
another application is a semi-supervised
learning
[Music]
since the since the very initial years
when VA was proposed it was subsequently
used to learn uh algorithms like
semi-supervised learning where we have
uh labels for a very limited samples and
then we have this large group of
unlabeled samples and we want to use the
latent structure learned by variational
autoencoder to to generalize my
supervised or semi-supervised model to
the Unseen test data set and then
another nice application and my personal
favorite is disentanglement
uh so uh in our today's session I would
like to go a bit more into this topic of
disentanglement
and there are um and there are a lot of
people's
um that has been out in terms of
disentanglement I would also leave or
leave out uh with some recommendations
as well
Okay so
now the
um you might be curious or like
naturally you might be curious to know
what exactly is a disentanglement or in
other words how do we Define
disentanglement and the unfortunate
answer to this is that even now even
till date uh the researchers in this in
this topic still have not arrived at a
very
um well accepted definition for
disentanglement the well-accepted
mathematical definition for
disentanglement but for a starter like
our today's lecture I would like to
describe this entanglement as the
ability to learn a factorial latent
representation that have this
statistical independent variables
so
in our previous demo slide where we have
this 10 where we Define our latent
variables as a 10 dimensional latent
Vector those 10 dimensional vectors
would be my Z naught to Z10 or sorry z9
so at the latent unit that I want
to be independent are among these uh
these particular units so if if we have
to go back to our earlier discussion on
uh genetic modeling where we started
with this collection of mountain the
images of mountains then uh our uh our
ability to learn a disentangle
representation would mean that the
attributes that would give rise to this
mountain are captured in each latent in
each latent unit
so if I uh wanted to have a generative
model with let's say five dimensional
latent Vector then my first latent unit
would be capturing the shape then size
color and background color now
you might be curious what what's the use
case of this or what does this mean for
the general AI right and then the answer
is
if if we really want to achieve that
let's say the easy eye or commonly known
as artificial general intelligence then
we need to have ability to understand
these Concepts from the data set and
disentanglement learning is actually
giving us the ability to do that
and and in terms of application what
does this mean is for instance if you
were to
find out that your fourth latent unit
have captured the background color for
the mountains that means if you if you
would change only this background color
then I could probably get an image of
this of this Mount Everest with uh with
the black background instead of this uh
light sky background so that ability is
really phenomenal in the in the AI
system
now now the obvious question is why we
are discussing uh disentanglement in
this variational autoencoder session
so I would like to uh give you a
probably a very brief
um
[Music]
introduction of why VA promotes
disentanglement we can go really deep
into this topic but I'm not going to do
that
so
if you remember when we were writing a
code or when we were thinking about
implementing our relational Auto encodra
we approximated the parameters of our
approximated latent variables or the Q Z
given X using the mean and the
covariance right so uh we if you imagine
the two dimensional latent variables we
had this Z and then this Z is coming
from a sample of a distribution whose
parameters are the MU and this uh
diagonal covariance
here the covariance Matrix they
generalize the notion of variance to
multiple Dimensions right and by having
this diagonal covariance we are
essentially learning a structure with
the variance of our each unit to be
independent to one another
further
um in this setup since we associate our
approximated posterior with this
isotropic gaussian whose covariance
again is an identity function
this further encourages all the
dimensions of our latent variables to be
independent so this structure that we
are using in VA is already facilating us
to learn independent features or in
other words disentangle features
however
different researchers are even if you
later go on and try to play with the
code that I have you would notice that
it's not that really easy because your
vae does not have this scale term alone
as your loss function you have this
giant reconstruction loss in your first
term that means for any vanilla VA or
standard VA
the learning disentangle representation
is not good enough because the the other
factors primarily from the
Reconstruction errors would be so
powerful that would that it would
hinders our algorithm to learn
independent features
as such one of the pioneer work in
connecting variational autoencoder with
disentangle representation came in 2016
uh about two or three years after the VA
was first introduced and this work is uh
known as a beta VA
the nice thing about this work is they
actually only added this beta in front
of the scale drum and if you remember
all our discussion about autoencoders we
always discussed that it has this
reconstruction error and then this it
has this regularization loss this
regularization loss can be weighted
through some beta parameters it was all
associated with this beta term but
nobody added the beta term in front of
in front of this KL drum before this
particular paper so what this paper
um proposed was that if we use some beta
hyper parameters greater than one this
would increase the the strength of the
scale term hence giving us more freedom
to learn independent features now it's
very obvious that if you increase the
the strength of this term it would
naturally mean that you would be
decreasing the strength of this term and
hence this paper what they found out
that if you want to learn a disentangle
representation then you might have to
pay the price of reduced reconstruction
capacity
now there are papers that have tried to
mitigate this reduced reconstruction
error despite learning good
disentanglement but we are discussing
about this paper at the point in 2016
where uh if you want to learn a good
disentanglement you have to pay the
price of reduced reconstruction error
okay I I hope this uh this this General
introduction of disentanglement was
somewhat clear even if not we will try
to look at this disentanglement from uh
the pictorial representation so for that
I would like to bring out this gif
this GIF is of a data set known as a d
Sprites
so what's happening in this ZIP is it's
it's combining a lot of different images
so you already see or already noticed
that this data set is a collection of
these small figures within this pack
black background where you see the
changes happening in terms of shape you
see the changes happening in terms of
rotation angle you see the changes
happening in terms of position of this
little figure in
um in your X Direction sorry in your X
direction or in your y direction
yeah as you see that it's moving uh down
uh in in y direction and X Direction so
basically this data set has uh all these
uh factors of variations in the data set
well grounded up so people who do
research in disentanglement they often
consider this sort of a data set because
they would now know how well they can do
the disentanglement
so if you if you think what
disentangling this particular data set
would mean is that I would reach at a
latent variable where each unit in the
latent variable would capture a
different attributes it should capture
the shape independently it should
capture the size independently and so on
the position of X position of Y and so
on and so forth so the result I am I am
picking out the result from this Pita VA
paper
so here
um for a particular model that they
train uh they used I guess 10
dimensional latent uh Vector so you see
that uh for their Z2 the third latent
unit has captured the position y latent
variable so you see that this little dot
is when you Traverse a single latent
um values from 3 to -3 you see the
changes happening only in your y
direction so these little dots it's
moving upward only in this in this
position y
now in the same way their Z6 the sixth
latent unit has captured the variation
across the position X so you see that
this little dot is moving in the X
Direction and its y position is
basically fixed
and in the same way we have this Z1 that
has captured the scale and then this Z5
and z7 they have captured the rotation
and then there are other latent factors
which are unused
so the main takeaway that I would like
you to give is that whenever we think
about this entanglement we are thinking
about the latent vectors
where each latent unit is capturing some
independent features
and to see that we can do this sort of a
traversal experiment
and and find out which latent unit is
capturing what feature
okay some other
application within this entanglement uh
include
um chain like a fish a picture like
image of a face where the this
particular paper has a lot of different
examples and I picked one such example
where when you change a single latent
variable you can introduce a glasses in
a person's face that means that
particular latent unit has captured the
variation across the classes
in the similar way this and other people
uh demonstrated how a single latent unit
captures the rotation across the digit
or how a single latent unit can have the
ability to to remove the colors from the
color remove the color from this
colorful digits
and similarly in other people where they
have this data set where uh they capture
the wall color floor color and so on and
so forth
so depending on the problems depending
on the data set
um you can have a disentangle models
where you can capture the the basic
concepts in your data set
okay um but uh here I I would like you
to give another quick demo that I have
paused when we were discussing uh about
our VA codes before
okay so here
um what I would like to do is
um although I have trained only a
variational autoencoder and not uh the
beta variational autoencoder I could
simply have done that by if you remember
uh the loss function that we wrote here
yeah so here
I use the argument of beta equals to 1
so you probably want to play around by
changing this beta value to maybe 5 10
or however you would like so
so by doing that you would probably
notice a more disentangled features but
in any case even though I have trained
only variational Auto encoder I would
like to show you the code skip it that
would help us to analyze how we can do
such traversal experiment
so here I am doing this traversal
experiment which is also sometimes
referred to as a latent walk because I'm
trying to walk in a latent space so here
uh I am doing that for all uh the Z
Dimensions that is we have 10
dimensional vectors so I want to do the
latent work for each Dimensions because
only by doing that I would know which
latent unit has captured which uh factor
of variations
and then
um I Define a vector which is similar to
my uh to my
yeah to my Epsilon which was my noise
and then here uh
up until here this is just defining that
latent traversal that I want to do and
and at the end I pass this uh
latent Vector to my decoder or to my
generative model to get my uh
reconstructed sorry here we are not
talking about reconstruction because we
do not start from original X since we
started from the noise Vector this is my
generated signal so for different
changes in the latent representation I
would try to capture how it would
generate the images back
and then here is one such example where
by changing only one latent unit I see
uh it has captured something I'm not
sure what is this it's not as clear as
what we saw in the result of the papers
that I put in the slides but we know
that it has it is changing on something
by only changing that one latent unit
you might argue it's trying to form a
digit uh from some blob or you might
think that we started with a noise in
this latent space then we walk into a
space of the latent structure where it
has now started to give a more clear
structure of digit 8. now I could do
that for my whole
um Z Dimensions right we have a 10
dimensional uh let me reduce the size
yeah so I can do that for the whole uh
10 dimensional vectors so each row here
represents a chains happening in only
one unit and there were since 10 units I
can draw 10 such flows now if I pick
let's say one random example so here
let's say I'm looking at this this row
so here I see that for some reason my
general model has learned for this
particular latent you need to to swing
from digit 8 to digit 4 by changing that
only one latent unit and similarly if I
look into this one
uh I see that there were some fool and
then it went to eat and then again back
to one so in in general is this
disentanglement I would say no it's
actually entangled because we see that
uh in different latent units my
um the factors of variation across digit
class that is the ability to represent
different digits is being entangled with
other latent factors so uh this is not
good model to uh understand
um disentanglement but it still
demonstrate how we would be able to
analyze this entanglement
okay I hope uh this would uh make uh the
concept of disentanglement bit clear to
all of you
okay so uh with this I leave I would
like to leave out with some
recommendations if you want to deep uh
dig down deep into this topic of
disentanglement
um if you remember when I was bringing
out the topic of variational lower bound
I said that there are so many ways to
derive those equations you would if even
if you randomly search uh in in some
search engine you might find out
different ways to solve that and this
was one such paper
that further dig down and dismantle the
elbow that we derived earlier and
demonstrated how the elbow has so many
different terms where if you pick one
terms versus another term you might
reach at a completely different models
so these are some of the papers that
could be very important if you want to
learn more about this entanglement
um or if you want to discuss uh if if
you get conf if you went to this paper
and get confused want to discuss I would
be happy to chat about them offline
later on even after this school
okay now I would like to summarize uh
what we just learned in this lecture so
in the beginning we started with the
discussion about Auto encoders where uh
we find out that autoencoders basically
tries to reconstruct my original input
by passing through some latent
bottleneck
but depending on the problems I could
use different regularization to that
and we took a pause over there and then
we jump and did a quick introduction of
genetic models and that led us to the
problems of how we do not have access to
the latent variables and we want to
approximate what we do not have and to
approximate we discuss about the concept
of variational inference which is
basically to use a separate simpler
distribution to approximate the true
unknown latent variables the
distribution of latent variables then
these two concepts that is the concept
of autoencoders and the concept of
variational inference they lead us to
variational autoencoder where we saw
in that demo uh we draw the
reconstructed samples and how it's not
so difficult to write a code about
variational autoencoder as well
and then we also discussed a bit about
disentanglement uh where we try to see
how we can analyze how well we have
disentangled uh the latent space
with this I would like to conclude this
session I would uh thank all of you for
your attention if you ever want to
discuss any of these topics or any other
research topics that interest that are
relevant to my own interest you please
feel free to contact me in any of these
mediums
with this I would be happy to take any
questions if you have thank you
okay I think I saw uh
uh I saw one question here so uh
so I would just rephrase the question
here so Mahesh asked me that could you
please mention ways in which uh
vaishnuvardhan gurram might help
supervise learning problem in limited uh
data setting such as Medical Imaging
where you might have healthy and
abnormal subclasses and there are
relatively fewer abnormal samples wow
that's that's in itself is a is a whole
research topic here but anyway so I
would like to split uh this question
into uh maybe two or three different
groups so
one of the ways in which you can think
variational autoencoder in a supervised
setting is that for that I would
actually like you to go back
uh
um
yeah maybe here so
so you see that in this structure I
I started with X and then I tried to
reconstruct my
um my original X now what if
I have a problem of my ex and then I
have some labels let's say I have this
um images of X-rays and then I have a
labels of it being either healthy or or
normal or either it being unhealthy or
abnormal that is I have a access to the
labels why then maybe I could use
another structure where where I start
with X use the very same structure and
then instead of doing the Reconstruction
I would try to learn the label y right
we can see that it can very well be done
from X to Y instead of from X to X hat
so we just made our variational Auto
encoder to some supervised learning
algorithm and if you would like to know
more about it you might uh you might
search about uh the algorithms like uh
variational information bottleneck so
they try to propose an algorithm in the
same variational Auto encoder structure
where we go from X to Y now uh now going
back to the question of Mahesh so since
we have this ability to go from X to Y
uh and then if I want to have a
semi-supervised structure that means I
would for all the labeled cases I would
go from X to Y and then I have Z and
then for all the unlabeled cases
although I can go from X to Y but since
I do not have the labels for those
unlabeled samples to to do the
supervision I could use some techniques
in semi-supervised learning like
ensuring that my hidden or latent
representations are consistent
regardless of the data set so if I if I
use such techniques then probably the
answer to your question would be yes I
can use the VA in this setting of how
you you explain that uh in the in the
problem of medical imaging
I'm not sure if my answer was helpful uh
okay yeah I see the thank you so I hope
it was helpful
okay
um so I would
go here if just in case if you have not
use this
feel free to ask me if there is a mic
that's been passed around yeah
awkward
foreign
we do not have much questions here and
that's that's completely fine
um some of the topics over here uh would
require some more uh readings
um even when I first started learning
about variational art encoders I had to
probably
go down like read the papers read the
tutorials couple of times and that's
true for not just a VA but any
um
any of the topics that you have been
learning in the school so I would uh
keep this slide in our in our shared
folders you can go through it maybe this
evening or maybe after the school and if
you find out any confusions in some
places of the slides I would be more
than happy to answer them
um later on whenever you have time okay
I think I saw another question
is there automatic way to find out which
latent you need is describing which
features or we always do late in work
wow that's that's very
um very good question especially for
someone like me who have been working on
disentanglement so this question is
something we try to tackle ourselves me
and my PhD advisor a long way back and
unfortunately
um we were not able to do that in in
terms of automatic way but but
latent walk is not just one of the let's
say manual way we now have so many
metrics which would at least tell us
um how well we are doing the
disentangling or how well my let's say
latent unit 1 is independent to my
latent unit two so there are ways to
tell me overall how good I am doing the
disentanglement and in in specific how
well I am learning the independent
features but
what this particular feature is
capturing uh in terms of our latent
attributes is something it's still uh we
we do not have uh the clear ability to
do that
um I hope that was helpful Mahendra
Singh tapa
okay yeah
okay
um I I guess then uh that's uh that's it
for our today's session
um I wish all of you the very best for
this learning experience uh you uh you
still have so much great topics from
amazing researchers
um so yeah I would wish you all the best
and uh as as I mentioned before as well
if there are any questions about the VA
and the entanglement or anything related
to my research that you would like to
know please feel free to email me and we
can discuss more again thank you so much