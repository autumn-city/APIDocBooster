All right.
So using the correct activation
initialization with the correct activation
function can actually significantly
lower the chances that you're going to
have an unstable network with unstable
weights, but sometimes that only stops
the unstable gradient problem from
happening in the beginning and towards
the middle parts of training or towards
the end, you might actually get the.
Unstable gradients problem again.
So what we would do then
is batch normalization.
And I feel like batch normalization
is kind of like the superhero
that the whole deep learning
community is a very happy about.
So let's talk about what that is.
Basically bashed normalization
is like normal normalization is
trying to make your data, compress
it to be in a certain range.
So either you want to put it between
zero and one, or you want to make
the mean zero and standard deviation.
One, um, for this one for
batch normalization, that's
the one we're going for.
What we're trying to do is be what we
are trying to center the input on zero.
So the mean, or their average
number is going to be zero.
And we are trying to make
the center deviation.
One, how we can do that is basically
normally, you know, we have to do it
before you put your inputs, right?
If your inputs range from like, one
of them goes from zero to a hundred,
the other one goes from zero to 255.
Like we did in our hands-on example, if
you remember, we are trying to normalize
them and put them in between zero and one.
So they're all, they
all have the same scale.
Uh, but before.
Based normalization.
We don't only do it for the input layer.
We do it for every layer that we
house or in-between every layer.
We put a batch normalization, a layer,
and then we normalize the output
that came from the previous layer.
And only at only after we
normalize this value, then we
pass it on to the next layer.
Let's talk about, uh, how
this thing actually works.
So again, normalization, it means that
we're going to set the mean to zero
and standard deviation to one, uh, one
little detail before I go into how it
works is, as I said, if you want, you can
also do the virus normalization before.
The input.
So normally we do this
by kind of manually.
We either normalize the values or
standardized values, um, before
we give the input to the network.
But if you don't want to deal with
that, if you want to keep everything
in one place, you can also have a
batch normalization layer, uh, right
before you give the inputs to your net.
I'll show you more in the exercise,
hands on exercise anyways, but
just so you know, you can also
have best normalization layer here.
Then you wouldn't have to, um,
localize or standardized your
inputs by yourself or manually.
Okay.
So for the scary part, this is how best
normalization works, but this is much,
much, much simpler than how it looks.
So let's start what happens is the
first thing that we want to do.
Of course.
Is to find a mean, right?
So let's say we have
this group of numbers.
These are our inputs, 3, 5, 8, 9, 11 24.
What I want to do is
find their mean, okay.
Very simple.
Basically what this means
is what I wrote down here.
Uh, I find there mean I sum all
of them and I divide them by the
number of values that I have.
One quick note here, though, is.
You might realize while training
you are giving your input
to the network in batches.
Most of the time, if you, if you're doing
stochastic, gradient descent, you do it.
You give them to the network one by one.
If you're doing minimize gradient descent,
you give the, give it to the network in
batches of whatever number you decided.
So what's going to happen is
of course, this is the number.
Uh, of values in the batch.
And again, then here this mean is going
to be calculated for the specific batch.
So it might not completely
represent the whole data set.
Right.
Uh, so one solution they found to
this and how it works on by default
with carers right now is that batch
normalization uses a, uh, rolling.
Average.
So what it does is it calculates
the mean for this first batch
it's 10 for the next patch.
It calculates again, if it's 12, then
it finds the average between those two.
So basically every time you give
it a new batch, it is able to
more correctly estimate the,
um, mean of the whole dataset.
So this is just something
that you should keep in mind.
And it's not going to affect
how we do things on the code.
The next step is to calculate
the variance of this.
And how do you calculate
the variance again?
Very simple.
Don't look at this really scary
looking, um, mathematics equation.
If you just look at you guys, just get
the number and then you subtract the
mean and you take the, uh, Oh, of all
of those, sum them up and then divided
by the number of values that you have.
So here it looks like I forgot
two brackets here, but of course
it should be two brackets here.
Let me fix that in one sec.
All right.
That's better.
We don't want any incorrect calculations.
The next one.
Is calculating what the number
should be now that we know
the mean and the variance.
Right.
So I'm only going to show
you the calculation for the
first one, if you want to.
I mean, the four, three,
basically the first element.
If you want to try it out for the, all
the other ones, please be my guest.
I think it will be nice to see how it
works, but basically very simple equation.
Again, you'll have to calculate it.
This guy be over to calculate it.
This guy.
This is the first value that we have.
And Epsilon is just a very, very,
very small value that we add
here so that we will not divide
whatever is up here with zero.
So in case the variance is zero.
We don't want to do division by
zero because that will break the.
Oh, they ever give us infinity
and we don't want infinity.
We cannot work with infinity.
So that's why just in case, if this
number is ever zero, we add this very
tiny number to avoid division by zero.
And then it says, okay, now that
I normalize this value three
actually needs to be minus 1.03.
If I do it for all of them,
I'm going to get this list.
We for three, we have
this number for five.
We have this number for 24.
We have 2.06.
And if you calculate the mean and
standard deviation, you will see
that their mean is zero and their
standard deviation is nearly one
or variance is nearly once a week.
If, you know, variance is the
square of center deviation.
So if one of them is one, the
other one needs to be one, two.
Right.
All right.
Last step.
So we're not done here yet, unfortunately,
that this, this has just only, this
has just been, uh, I'll show you again.
Okay.
Normalization.
Right.
But we have this one last step here.
And what it does is it's rescale these
values that we have and offsets them.
And this is a part that the
network actually learns.
So these little guys, this one,
multiple is something that we
multiply the inputs with these
values, this, uh, normalized inputs.
And also there's something
that we add to them.
So basically when we asked
something, we kind of shifted.
Either to the right or the left, or if
you're subtracting something and if you
rescale them, you kind of increase them.
But these are values that we
learn ambassador mobilization to
kind of transform the inputs or
outputs a little bit in hopes that
our model will perform better.
Basically.
This has all been training.
So we have been training our, uh,
network and we found this values.
And then, you know, we did
it again and again and again,
but this has been training.
So what happens while we're testing?
So these two valleys.
Rescale and offsets value.
I learned their best optimal
values are learned during training.
So they are basically parameters
that need to be learned.
Right?
Uh, they give one value to them.
The, the model doesn't perform super well.
We changed their values and see
when the model performs best.
Whereas these two guys.
I think, as I said, we use a
rolling average to calculate the
average or the mean and standard
deviation or variance, uh, often.
But during test time, we will not be
able to calculate the average, right.
Or the standard deviation or the
variance, because we will be giving
the input of network one by one.
So what happens is you can not just
know the mean of what, I mean, you
cannot find the mean of one input.
It is that value, but it's
not going to be correct.
It's not going to, um, conform it while
we have been calculating the, these
two rescale and offset values with so.
It is done to avoid this problem is
basically these two values are also
learned, but they are learned in a way
of, um, rolling average, basically.
So we get the mean and
variance for the first batch.
We get the mean and variance
for the second batch.
And then we, the.
Uh, we combine those two to find
an estimate for the whole mean and
the mean for the whole dataset.
I mean, for the whole variance for
the whole data set, and then the
third batch comes in again, we use it
for those calculations, but then we
also calculate a rolling average for
the general average and the general
variance of the whole data set.
So basically those are all are.
On these two are trainable.
They are updated based on the training
process, but these ones are just
learned from the dataset that we have.
So this is basically how it works.
Okay.
Now that we know that's
too, you might be wondering.
Okay.
But like, why.
Do you use to normalization?
So as we said, it is what it is.
It's a center.
It's a way of centering the inputs around
zero and then normalizing the inputs and
then moving it around in a way that will
help a network be the most, uh, effective
or efficient, or how the best performance.
But why would we.
Well to avoid on civil gradients problem,
but turns out actually batch normalization
has some extra benefits on top of
being a way of avoiding unstable grade.
First one is that we realized
in time that it helps us achieve
the same accuracy faster.
So if you're not using bashed
normalization, you will have to train
your network for a much longer time.
For more, many more E-box to get the
same accuracy that you would get with
batch starting, maybe with fewer.
And it can also sometimes
lead to better performance.
Uh, there are some research
out there about this.
Of course, this will, again, might depend
on type of problem that you're solving.
But I know for example, for, um, a
network that has been worked on the
image net classification problem,
it has shown better performance
than the one than the network that
did not have batch normalization.
And when you, one other benefit
is that when you are using bashed
normalization, Before you give the input
to the network, then you don't even
have to use this 10 desertion layer.
You don't have to do it manually.
As we talked about, you can just use
so everything can be in one place.
You will have batch normalization.
Um, one thing they found out is also that
you do not need to use regularization.
Sometimes.
So you can say it reduces the
need for using regularization.
Just to recall.
Regularization is something we do
to avoid or lessen over-fitting.
So then we know that batch
normalization also helps with overfit.
And light lastly, kind of connected
to the previous two points.
Um, then you have, then you
train your network in less time.
So specifically each epoch.
So each epoch or less, remember what
epoch is, is having one whole dataset.
Run through the whole network.
Even if you're giving in little batches,
one epoch is us giving the network, having
already seen all of the data points.
So those E-box, those things might
last longer because of all the extra
calculations that we're doing in between
the layers, but in total, the number of.
Minutes that the network would need to
run to train on to converge or to learn.
Um, the, the data, the patterns in
the data is going to be less when
you're using batch normalization.