Adadelta optimizer has a way to adaptively change learning rate but still, it needs an initial value of learning rate. 
 If you have gone side-track just remember one thing we are going into so much hassle because normal gradient descent has its learning rate constant for the entire training phase... which is so LAME!  So the optimizers like AdaDelta have some techniques to vary the learning rate with every iteration that's it. 
It just multiplies the variable updates (see [the update op implementation (hyper-link)]). 
 This was made possible due to this rho hyperparameter.  It is also known as 'Decay Constant'.  Instead of writing it with this equation, I have written it separately because we will need delta theta in this equation.  So, this term over here is the learning rate calculated by AdaDelta. 
The thing you need to know about AdaDelta is the general context of online machine learning.  Online machine learning is where you get your data one-at-a-time (and thus have to update your model's parameters as the data comes in), as opposed to batch machine learning where you can generate your machine learning model with access to the entire dataset all at once.
