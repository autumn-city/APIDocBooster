 In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.  This is numerically more stable. 
 Yeah, conceptually this is a tensor of integers, they can only be 0 or 1, but we, we�re going to be using a cross entropy style loss function, so we're going to actually need to do floating-point calculations on them.  That's going to be faster to just store them as float in the first place rather than converting backwards and forwards, even though they're conceptually an �int� we're not going to be doing kind of �int style calculations� with them. 
We say the loss is minimized because the lower the loss or cost of error, the better the model. 
The combination of nn.LogSoftmax and nn.NLLLoss is equivalent to using nn.CrossEntropyLoss.  This terminology is a particularity of PyTorch, as the nn.NLLoss [sic] computes, in fact, the cross entropy but with log probability predictions as inputs where nn.CrossEntropyLoss takes scores (sometimes called logits).
For a special case of a binary classification, this loss is called binary CE (note that the formula does not change) and for non-binary or multi-class situations the same is called categorical CE (CCE). 
