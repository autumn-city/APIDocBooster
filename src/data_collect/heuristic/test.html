"<p>Let me show some specific examples:</p>
<pre class=""lang-py prettyprint-override""><code># LSTM example:
&gt;&gt;&gt; rnn = nn.LSTM(10, 20, 2)
&gt;&gt;&gt; input = torch.randn(5, 3, 10)
&gt;&gt;&gt; h0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; c0 = torch.randn(2, 3, 20)
&gt;&gt;&gt; output, (hn, cn) = rnn(input, (h0, c0))
# LSTMCell example:
&gt;&gt;&gt; rnn = nn.LSTMCell(10, 20)
&gt;&gt;&gt; input = torch.randn(3, 10)
&gt;&gt;&gt; hx = torch.randn(3, 20)
&gt;&gt;&gt; cx = torch.randn(3, 20)
&gt;&gt;&gt; output = []
&gt;&gt;&gt; for i in range(6):
        hx, cx = rnn(input[i], (hx, cx))
        output.append(hx)
</code></pre>
<p>The key difference:</p>
<ol>
<li>LSTM: the argument <code>2</code>, stands <code>num_layers</code>, number of recurrent layers. There are <code>seq_len * num_layers=5 * 2</code> cells. <strong>No loop but more cells.</strong></li>
<li>LSTMCell: in <code>for</code> loop (<code>seq_len=5</code> times), each output of <code>ith</code> instance will be input of <code>(i+1)th</code> instance. There is only one cell, <strong>Truly Recurrent</strong></li>
</ol>
<p>If we set <code>num_layers=1</code> in LSTM or add one more LSTMCell, the codes above will be the same.</p>
<p>Obviously, It is easier to apply parallel computing in LSTM.</p>
"