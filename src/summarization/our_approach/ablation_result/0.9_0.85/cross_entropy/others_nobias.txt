 In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.  This is numerically more stable. 
 What happens   if we're not predicting which of five things  it is but we're just predicting “is it a cat?”   So in that case if you look at this approach  you end up with this formula, which it's   exactly… this is identical to this formula but  in for just two cases, which is you've either:   you either are a cat; or you're not a cat,  right, and so if you're not-a-cat, it's one minus   you-are-a-cat, and same with the probability  you've got the probability you-are-a-cat,   and then not-a-cat is one minus that. 
 And so then these are the predictions  that came out of the model,   again we can use soft max or it's  binary equivalent, and so that will   give you a prediction that you're-a-cat, and the  prediction that it's not-a-cat is one minus that. 
 If we use negative  log likelihood loss or cross entropy in pytorch.  But  numerically, like stability wise on the computer, the cross  entropy one is more stable.  So and also for this one, really pay  attention to this one, it's taking the logits as input. 
 And the term binary  cross entropy and negative log likelihood are essentially the  same. 
