 This Tanh is a hyperbolic tangent itself, but in the world of Neural Network, Tanh converts the input value into a non-linear one and keeps it in the range of -1~1.  It is used to serve as an activation function. 
 The Tanh function is then used to fit the value between -1 and 1. 
 While Tanh fits the input value between -1 and 1, this Sigmoid fits the input value between 0 and 1. 
 This is nothing but your tan H so I can write the this as 1 minus tan H Square h of Z especially so that is nothing but 1 minus here output Square.  So this is differentiative of your tanh function.  So if you want to see the geometrical interpretation, so how it basically looks like is you have this particular scale minus Z 2 Z you have one then you have 0.5 and then you have your minus 1. 
 Alright, guys let's discuss hyperbolic tangent activation function which is also known as tanh activation function.  Unlike the Sigmoid function, the range for tanh function is -1 to 1.  So it is similar to Sigmoid function, where we were catering the output in a range of 0 to 1 & setting up the threshold at 0.5. so in tanh we extend the lower side of the curve till negative 1 & our mid will be at 0.  So guys by definition tanh function is derived from the trigonometric function that's why it's formula is written as tanh(x) = sinh(x) divided by cosh(x).  A general problem with both the sigmoid and tanh functions is that they saturate.  This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively. 
