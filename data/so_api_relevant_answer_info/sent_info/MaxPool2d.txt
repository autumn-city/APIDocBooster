I was originally making the mistake of considering the required length to be the width of the grayscale images that act as input for the whole network (CNN + MaxPool2D + RNN), but because the MaxPool2D layer creates a tensor of different dimensions for the RNN's input, the ctc loss function crashes.
when I use the MaxPool2D that is commented out right now, than I get a
  "IndexError: tuple index out of range
Conv2D/MaxPool2D: This is the exact same as the above, but now you have an n X m X 2 matrix.
classifier.add(MaxPool2D(pool_size=(2,2), name='pool1'))):
You have to use conv1D and MaxPool1D instead of conv2D and MaxPool2D cause your dataset is a single-channel image instead of 3 channel image.
The only things I changed were: Changed MaxPool2D to MaxPooling2D and also there was a ) missing after one of the layers.
There are two MaxPool2d layers which reduce the spatial dimensions from (H, W) to (H/2, W/2).
Even more so, BatchNorm is considered a layer universally and for ease I would consider others (ReLU, MaxPool2d, Dropout) as ones as well.
You didn't show us how exactly you perform the convolution and pooling, but most probably strides=[1, 1, 1, 1] in conv2d and strides=[1, 2, 2, 1] in maxpool2d, and padding='SAME' in both of them.
It should be because the padding used for maxpool2d is 'VALID' instead of 'SAME'.
from: pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)
