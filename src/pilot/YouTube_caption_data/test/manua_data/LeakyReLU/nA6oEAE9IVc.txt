 Okay, let's now finish up this video by talking about how pytorch
 handles weight initialization, and how we can change it if we
 wanted to. So I looked at the current source code of pytorch,
 because the other weight initialization that they do by
 default, changed over the different version over time. So
 currently in version 1.8, they use the climbing her uniform
 weight initialization. So I found the source code a relevant
 one here, if you want to check it out yourself. And this
 corresponds to the fully connected to the linear layer.
 For the convolutional layer, I saw it's the same though. So
 they use the same weight initialization now for both the
 fully connected and the convolution layer. In previous
 versions, they had a slightly different weight initialization.
 Yeah, and now looking at this here, you may wonder, besides
 climbing uniform, what these different things are. So here,
 this math square root five, this is I think, for the leaky relu.
 So that is if you if you use a leaky relu, you would add that
 number. So they are assuming that if you use a fully
 connected layer, that you also use a leaky relu activation. In
 practice, personally, I tried also using the zero, which is
 usually what people would put in here for the regular relu. If I
 use the regular relu, and I didn't really notice any
 difference. So it might be better to use this for the leaky
 relu and put set it to zero for the normal relu. I will show you
 in the next slide how we can do that more conveniently than
 modifying the source code. But in practice, personally, I didn't
 notice a big difference. So yeah, here's an example of how
 we could manually override the initialization that is done
 automatically. Because if we initialize these layers, it will
 basically execute this one to do the timing initialization. But
 if we wanted to, we can override it. So how I like to do this, I
 mean, in multiple ways, you can do it. But how I like to do it
 is I like to put this for loop after my sequential loop. So
 here, what I'm doing is I'm iterating through all the
 modules in the network. However, here, I'm only interested in the
 other fully connected modules, not in, let's say, relu, and
 things like that. So here, I'm pulling out essentially only the
 fully connected ones. And then I'm using torch and in it,
 chiming uniform, and provided with the corresponding weight.
 So the weight corresponding to m, the linear layer, and I'm
 setting it to fan in and non linearity relu. So this is as
 essentially the default, by the way, too. So this is what is
 done by default, or not actually, that's not true. By
 default, it should be leaky relu. But fan in, that's the
 default. And usually, that's by default, leaky relu, I'm
 changing it to relu. Personally, I don't don't notice any big
 difference. Another thing, if you wanted to, you can set the
 bias to zero, or you can use the initialization scheme they use
 here. Also, for that, personally, I didn't notice any
 difference. But I just trust that the pytorch developers did
 a lot of testing and also considered community feedback.
 So I think this one is probably better than this one. Except, of
 course, yeah, the other one on the previous slide is more
 geared towards leaky relu, and I have a regular relu here. But
 anyways, your mileage may vary. And there's another thing I
 also wanted to say at the end of the video why this may matter
 less than you might think. So yeah, another example here is
 the normal initialization. So using Gaussian distribution. So
 same concept. Now I'm setting the weights here to Yeah,
 values from a Gaussian distribution. So with mean zero
 and standard deviation 0.001. So why am I using the detach here?
 I also already used that in the previous slide, if you have
 noticed here. So what is the detach? So that is because when
 we initialize something that has weight parameters, so linear has
 parameters. So let me go back to slides. So in pytorch, there is
 a thing called parameter. And these are all the learnable
 parameters. And they have the gradient on attribute activated
 by default. And so what happens is that for that part for weight
 and bias, pytorch will track the gradients when we do
 computations. However, we don't want to have the weight
 initialization be part of let's say, the computation graph or
 the gradient computation. So here, by detaching it from the
 computation graph, I kind of prevent that for this part, a
 gradient is computed. So it's just like for convenience, not
 convenience, I think otherwise, it might not even work. Because
 it's kind of a weird operation to include in the computation
 graph, because it's just, this should be essentially our
 starting point and not part of the computation graph. Okay, so
 yeah, here's the how we could then also initialize it from a
 Gaussian distribution. And actually, when I run this, I get
 get like some really funny results. So on the left hand
 side, if you want to replicate that, by the way, here's all
 the GitHub code. So there are also the other ones if you go
 here. So when I was training this network, I noticed that
 nothing happened for a while. So for about like, almost 20
 epochs. And again, then suddenly, I don't know, it got
 a push or something. And then the loss started to go down. So
 it started training after epoch 20 or so, which was funny, I
 have never seen something that weird, but usually also, I would
 stop the training if it doesn't stop, start learning after a
 few epochs. So anyways, so that was kind of funny, it eventually
 learned to perform pretty well. But that was still interesting
 for the timing initialization that trained pretty well. Yeah,
 to begin with, I was not such an issue. So that looks actually
 much better. And this is why we would, for example, use timing
 initialization for network with relu activations. I should say
 though, what I mentioned earlier, is that we use if we
 use batch norm, then actually, this initial feature weight, or
 weight choice is less important than you might think. So because
 the other batch normalization normalizes the activations
 anyways, so in that way, it's less important what type of
 weight initialization we choose, at least in my opinion. So when
 we try or when I tried this in practice, so we can see now that
 on the left hand side, I'm showing you the Gaussian
 initialization with batch norm, that things train also well for
 this case. So actually, yeah, with batch norm, it trains even
 faster than timing her without batch norm. Alright, so yeah,
 this is just a brief overview of different weight initialization
 schemes. It's maybe something you might want to consider. But
 the pytorch defaults are reasonable. If you use the relu
 activation, if you use different types of activation functions,
 you may want to see whether there's a better weight
 initialization scheme. But also, again, with batch norm, things
 are quite robust. Alright, so this is the lecture on batch
 norm and weight initialization, then and next week, we will
 cover optimization algorithms.