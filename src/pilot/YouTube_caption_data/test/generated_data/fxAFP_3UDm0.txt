hi my name is bo yoon
and i'm a master's student studying data
science at smu
today i'm going to walk you through long
short-term memory
which is a type of recurring neural
network that has been gaining traction
for sequential data
first of all what is recurring your
network in a traditional for
feeding neural network we assume that
all inputs are independent of each other
but what if the data is dependent and
sequential
rnn is a type of neural net to recognize
patterns for sequential data
such as text genome and handwriting
due to its internal memory it is
perfectly suited for
machine learning problems that involve
sequential data that are dependent with
each other
let's work through a very simple example
let's say
that david works out in the sequence of
chest
shoulder and abs one work one workout a
day
regardless of what it is so monday he
does chest
tuesday shoulder wednesday abs and
thursday back to chest
for a typical feed forward net we will
use a different features
such as day of the week month of the
week and the health condition of the day
as inputs and a complicated voting
system
to predict what the next workout will be
but the result
won't be as accurate as what we can
simply get from the predefined
sequence of workouts
here when we are using rnn we simply
input what workout we did yesterday
and the prediction naturally goes to the
next
predefined workout sequence
however let's say something came up and
you couldn't work out
and had to skip a day then we go one
time back
and make a prediction based off the
workouts on the day before
yesterday and make subsequent
predictions after
this is a simplified version new
information is fed into the prediction
along
with predictions made for the previous
time step
and this process gets repeated
okay now we understand the big picture
let's take a look at what's happening
under the hood
let's talk about math at time equals 0
x 0 is fed into the first equation to
get
h0 here in the middle term
equals 0 because the time becomes
negative
once we get ho
then we plug it into the second equation
multiply
by the weight w y and plus the bias term
we now move on to the next x1 node
and we can use h0 from the previous run
to calculate
h1 this entire process is repeated until
we reach the final
yi prediction
okay sounds great but
what's the problem with training rnn rnn
is mostly used in sequential data that
has time steps
let's say we have n number of layers
then we need to back propagate so many
layers
and this leads to vanishing gradient
problem okay
so let's see how the problem arises
each layer comes with the cross entropy
loss and its gradient
is represented by e as shown on top of
each layer here
in order to calculate these gradients we
use back propagation through time
and we need to sum up gradients at each
time step as
represented by wr that are located
between layers
the term in the red box is continuously
multiplied across multiple time steps
since we are using sigmoid function here
which means the term in the red box is
less than one
and when this term being multiplied so
many times the value
will get cl get close to zero
then rho e2 over rho w gradient
will quickly vanish over time and long
dependencies
in the earlier times will not contribute
to later times at all
but what if you use a linear function
instead of sigmoid function
well in this case the weights are high
which then leads to exploding gradient
problem
in that case the resulting vector will
show up as
nan during implementation
one solution is to use gradient clipping
which is capping the gradient at a
certain number
so that it doesn't go over it
yet there is a better solution solution
to mitigate vanishing
or exploding gradient is to use long
short-term memory
we replace each hidden layer with lstm
cell
we also add cell state as represented by
c
that connects to the next sequence of
the network
so now let's take a look at this high
level view
of how each lstm cell works each lstm
cell maintains a cell state vector also
known as
long memory this purple box
is the cell state and the lsem cell
that is in the next time step can read
from it write to it
and reset it using explicit gate
mechanisms
cell state runs from one layer to layers
down the sequence
with very minor linear interactions
there are three gate mechanisms that
consist of short-term memory
input gate controls whether the memory
cell is updated
forget gate controls whether the memory
cell is reset
the output gate controls whether the
current information is visible
let's start with the forget gate it's
important to identify which information
is not needed
and need to be thrown away the 4k gate
is a sigmoid layer that implements this
task
and the output of this will be either 0
which means discarded information
or 1 which means kept the information
in the previous step at the 4k gate we
decided which information to keep
or discard here at the input gate we're
going to decide
which information will restore we'll
store
we have two parts sigmoid layer which is
the input gate
that decides which values will be
updated
and we also have 10h layer that creates
a vector of new candidate values
that could be added to the state and
then we multiply these two outputs and
pass it to the next state
here in the cell state we multiply the
old cell state by
ft in order to forget things that we
need to forget
then we we need to um input
gate i t multiplied by ct which is the
new candidate values
and they are scaled by how much we
decided to update
each state value
now we run into another sigmoid layer
to determine what part of the cell state
we're going to output
then we push the new cell state through
10h
and multiply it by the output of the
sigmoid gate
okay so here we're going to go over how
lstm stores and updates keywords
through an example sentence that has
long dependencies
david loves to play around data and draw
insights
from data every day no wonder he's such
an amazing data scientist
his brother kevin loves to paint every
day kevin will be a great painter
one day okay so here we have
um two clearly separate sentence here
the black arrow that passes through the
sentence here identifies and stores
keywords in the cell state
otherwise known as the long-term memory
in the first sentence it will pick up
the word data
which leads to david being a data
scientist
and store it until it runs into the word
paint which leads to his brother kevin
being a painter
so ct1 -1 is the memory of the word
data and the current word which is paint
is passed on to the sigmoid function
which restricts the output vector to
either 0 or
1 depending on whether it decides to
discard the information or not
so here when the output vector of 0
meets the vector
that has the word data the
multiplication will result in 0
and in turn forget the word data
so not only we forget the previous
memory we also have to update the input
and we'll use the sigmoid and 10h
function
for the 10h function we are assigning
certain weights
to h t minus 1 and xt
add the bias term and then push them
through the 10h function
next step we multiply the output from
the input gate
and from 10h and that's how we add the
new memory for
paint
okay so upper gate here again
we apply weight weights to hidden layer
of htm1 and xt and then push them
through the third sigmoid function
we take the newly added memory from the
cell state
apply 10h and then multiply with the
output from the sigmoid function
then we finally get the hidden layer ht
and yt of the current state
okay now let's walk through an example
on how to set up lsdn network on python
notebook
the data is movie review with a mixture
of positive and negative sentiments
first we're going to import keras
packages
and then we see here this
different parameters we embed theme
lstm out batch size dropout x
these are the variables for
hyperparameters and their values are
somehow intuitive
but should be played with in order to
achieve good results
please also note that i'm using softmax
as the activation function
the reason being that our network is
using categorical cross entropy and
softmax is just the right activation
method for that
here we're going to separate the train
and test data set
and then here we are training the neural
network
we should run much more than seven seven
epoch
but um we'd have to wait forever for
this
to run so let's do seven for now
um also here we are extracting a
validation set
and a measuring score and accuracy
once that's done we're finally measuring
the number of correct guesses
it is clear that finding positive
sentiment goes very well for the
for the network but deciding whether it
is negative
is not really as seen here
my educated guess here is that the
negative training set
is dramatically smaller than the
positive so the bad results
uh for the negative reviews
okay to sum it up when dealing with
sequential data
long short term memory is robust against
the problems of long-term
dependencies there are three key
gate mechanisms in each ls stem cell
that play important roles
in solving long-term dependencies which
are forget input and obligates
that concludes my presentation thank you
for watching the video
if you have any question feel free to
email me or connect with me on discord