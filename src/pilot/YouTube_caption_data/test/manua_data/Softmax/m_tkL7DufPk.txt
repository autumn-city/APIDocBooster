Hello and welcome to deep
learning with Python Zero to GANs.
Over the next six weeks, you will learn
deep learning using the PyTorch framework,
and I will be your instructor Akash.
So by the end of this course,
you will be able to train a model
which goes from producing random
noise to fairly good images of
handwritten digits and animal faces.
And what's more interesting is
that both of these are produced
by the exact same model.
The only difference is that data used
to train these models is different.
These are Gans or generative adversarial
networks, and they are quite powerful
and we will learn all about them
starting from the very basics.
Not only that you will be able to
build a real world project using deep
learning and own a verified certificate
of accomplishment from Jovian.
I'm really excited to kick off
this course, so let's get started.
The first thing you need to do is
go to zerotogans.com, which will
bring you to this course page.
Now, on the course page, you can click
the enroll button to enroll for the course
and share button to invite your friends.
And you can see all the lessons
here and the assignments.
So let's open up lesson one.
You will be able to catch a recording
of this lesson on this page.
Now on lesson one, you will find links
to certain Jupyter notebooks, which
we will be using to execute code life.
So let's open up the first link.
This is called . This is a Jupyter
notebook hosted on the Jovian platform.
Let's go through it.
Planning with Pytorch is a
hands-on and beginner friendly
introduction to deep learning.
And this course has a view
of few small prerequisites.
So if you're just getting started
with data science and deep learning,
you can take this tutorial series.
You just need to know a little
bit of programming with Peyton.
And if you do not, you can follow
these links here to learn via some
tutorials, and you need to know a
little bit of high school mathematics.
Once again, you just need to know about
vectors, matrices, derivatives, and
probabilities, and you can follow these
links to learn about these topics.
So in a couple of hours, you should be
able to cover all of the prerequisites
that you need for this course.
And any additional mathematical
or theoretical concepts that we
need, we will cover as we go along.
There is no prior knowledge of
data science or deep learning
required for taking this course.
So what we are looking at here right
now, this is a Jupyter notebook
hosted on the Jovian platform.
And this is a read only view of
the platform of the notebook.
This is used for sharing the notebook.
But if you want to run the code,
and if you scroll down, you will
see that there is some code here.
If you want to run the code, the
easiest way to do that is using free
online resources, but you can also
run it on your computer locally, and
you will find some instructions here.
So what you need to do is
scroll up, find the run button
here and click run on co-lab.
So this will give you an option to
authorize your Google drive access
and run this notebook on co-lab.
So just click on authorize.
And you will be asked to select a
Google account here where you will
authorize Jovian to run notebooks
onto Google CoLab and online platform
for running Jupyter notebooks.
So please select the first account
that is listed in this list.
And, click allow so that we can
create the files within your
Google drive to run this notebook.
And once you do that, this
notebook will open up on.
Colab.research.google.com.
So now this Jupyter notebook, the same
thing that we saw earlier here is now
running here on the cloud, and we will
be using this interface to run the
notebooks throughout this, lecture series.
So the first thing I'm
going to do is co edit.
And clear all outputs.
What this does is the code that is there
in this notebook already has some outputs.
You may want to remove those outputs so
that you can run the code and discover
the outputs and study them on your own.
Then let me just go full screen here.
And I'm also going to
hide menu bar for now.
Okay.
So let's get started.
So what you're looking
at here is a code cell.
So there is a S this is a cell
within a Jupyter notebook,
where there is some code.
If you click the run button
here or press shift plus enter,
this will run the code for you.
So make sure you run the first cell.
Otherwise your notebook
may not function properly.
And the first time you run it, it may
take a minute or two just to initialize.
So let's give it that time.
All right.
So the first cell has now executed.
And then below you can see, this is what
is called a text or a markdown cell.
So a cell can either contain some code
or it can contain some explanations.
And if you want to create a new cell, you
just click plus code or plus text, and
that will create text or code and for you.
So now we are on the first
tutorial by torch basics.
And in this tutorial, we will
cover the following topics
we will learn about  tensors.
We will learn about tensors
operations ingredients.
We will learn about the interoperability
between Pytorch and numpy.
And we will learn how to use the
Python documentation website.
And we've already seen how to run
the code so we can skip ahead.
There is, there are some instructions
here to install the required libraries.
Now, if you are running on Google CoLab,
you do not need to install anything.
Everything is already installed,
but if you're running on.
Your own computer, or if you're picking
one of the other options to run.
So you have an option to run
on a platform called binder or
on a platform called Kaggle.
Then you might need to run
some of these commands.
So I'm just going to skip
ahead because we don't need it.
is already installed here.
So let's import pytorch and the way  you
importpytorche is by writing import torch.
So import torch imports, the
torch module, which contains all
the functionality of Pytorch.
So now we have access to the torch module.
An accurate score.
is a library for processing tensors
at tensor is a number of vector, a
matrix or any end dimensional airy.
So let's create a tensor
with a single number.
This is how you do it.
You just say torch dot tensor,
and then you put in a number here.
Yeah.
And that we've put that result into T1.
And then we've typed even
here to display the result.
And you can see here that
a tensor has been created.
So four dot or four point
is a short time for 4.0.
It is used to indicate to Peyton
and two Pytorch that you want to
create a floating point number.
And we can verify this by checking
the D type attribute of our tensor.
So if you type T1 dot D
type and run the cell.
So once again, I'm running the cell
using shift, plus enter, you can see here
that it has a type torch got float 32.
Now, as I said, a tensor can be a
vector or a matrix, so let's try
creating some more complex tensors.
So here we have Torstar tensor and
we're passing it a list of numbers.
One, two, three, and four,
and we have one.here.
So that is going to convert this
into a floating point number and
let's see what happens to the others.
So what you will notice is that.
A tensor got created, but all
of the values got converted
into floating point numbers.
So this is an important property
that all the elements of a
tensor have the same type.
So if you check the data type now,
and I can do that by adding a code
cell here, plus code and typing
T2 dot D type, you can see that it
has a type float 32 because all the
numbers have been converted to floats.
So that's what is called a vector
or a one-dimensional tensor.
Then we have a matrix here.
So here what we're doing is we're
passing a list of lists of numbers.
So there are multiple lists of
numbers inside a bigger list, and
it forms a matrix, which you might
remember from linear algebra.
It has certain rows and columns.
So it has three rows and two columns.
And once again, we've put in
five.here because we want to convert
this into a floating point number.
Now the reason we use floating point
numbers and floating point tensors for
deep learning is because a lot of the
operations that we will be performing
will not lead in teacher results.
For example, we will be doing
matrix multiplications and divisions
and inversions and things like
that and gradients and so on.
And all of these will not
produce in TJ results.
So you might face issues if you work
with dancers that are in teacher.
So that is why always make sure
you're creating floating point.
Tensors.
So now we've created a two
dimensional tensor or a matrix.
So here we have the same
three rows and two columns.
Next, we have a three dimensional tensor.
So what we're doing here is while
you can think of it a little bit
like this, you take a matrix.
So here we have a matrix and then you take
another matrix, which has the exact same
number of rows and columns as this matrix,
and then put it behind this matrix.
So just think of it in three dimensions.
Now, what that will give you is
that will give you a cuboid kind
of a structure where you have one
matrix and another matrix behind it.
Now, obviously we cannot show that
on this screen while typing code.
So what we do is we write the
matrices one below the other.
So here we take one matrix.
This is one matrix, and this
is the second matrix that is
supposed to be behind this matrix.
And then we put the two of
them together into a list.
And what that gives us is
a three dimensional tensor.
Okay, so there you go.
That's a three-dimensional tenser and
tenser can have any number of dimensions
and different lens along each dimensions.
So we can expect the lent along
each dimension of a tensor
using the dot shape property.
Let's see the pencils we've
created so far and their shapes.
So the T1 tensor was simply a number four.
So it did not really have any shape.
In fact, it has zero dimensions.
It's just a number.
So that is why you get back
an empty list here, then let's
check T2, which was a vector.
So T2 has one dimension.
So that's why you get
one element in the shape.
And that element has the value
for indicating that there are four
elements along that direction.
That is the lent of that
direction of that dimension.
Next up, we have T3, which was a matrix.
So let's check that now.
You probably know the drill by now
you have three rows and two columns.
So that is why you have two elements here.
The first element is three
and the second element is two.
Now what about T4?
T4 was a three dimensional tensor.
Before we run this, let's just try to
figure out what the shape should be.
So this is a thumb rule that you use
for figuring out the shapes of pencils.
Just start with the outermost
bracket and first count the number of
elements in the automotive bracket.
So the automotive bracket has two
elements, both of which are matrices.
So the first element in the shape
is to then go one bracket in.
And when you go one bracket in,
then you see that inside this list.
So this is a list which.
Forms the matrix.
There are once again, two elements.
So the second element in the shape
will also be to then go one bracket in.
So now we're at the animal's bracket
and count the number of elements.
So now you have three elements here.
So the third element is three, right?
So you go outside in, and that is
how you get the shape of a tensor.
And there are three brackets here.
So the shape will have three elements.
So let's print T4 and
let's print the shape.
So as we expected, it has the S shape too.
Two and three indicating three
dimensions and the lens two, two,
and three along each dimension.
But not that it is not possible to
create tensors with an improper shape.
So if you try to create a tensor like
this, where you have three rows and
two columns, but the first row has a
third column, this will cause an issue.
This is an important difference
between a list of lists and a tensor,
has to have a regular, proper shape.
Otherwise you will get back in error.
The other difference being that
all the elements within a tensor
should have the same data type.
If they do not have the same data
type, they will be given the same
data type while creating the tensor.
Okay.
So that's about tensors
now we can combine tensors.
Using the usual arithmetic
operations that we use for numbers.
So let's look at an example.
Here we have X, w and B.
We are creating three dancers.
They have the values, three,
four, and five respectively.
Now we've added this special argument
here called requires grad equals true.
What's that going to do?
We'll see in just a moment.
So let's run this right now.
It's now we have extra BNB with the values
three, four, and five, as we expected.
Now, if we want to combine these tenses
to create a new tensor, Y all we need
to do is use the basic arithmetic
operations that we already know.
So just w multiplied by X.
So the star indicates multiplication plus
B, and you might expect, this will give
you three times four, 12 plus five 17.
And it gives us 17 as we expect now,
what makes by touch unique is that right?
We can automatically
compute the derivative of Y.
Now, if you look at Y is a
function of w X and B, right?
Why is w X plus B?
So you can take the derivative of Y with
respect to w and let's do that mentally.
So w explicitly the derivative of
that would be the derivative of
WX, plus the derivative of B since.
With respect to WB is a constant.
That term goes away.
We get back zero there and with
respect to w X as a constant.
So what we get back the derivative
of, or deed, the derivative of
WX with respect to w is simply X.
So the derivative of Y with
respect to w is simply X.
Okay.
And this is basic derivative calculation.
If you.
Are not finding this familiar,
then just go back and review the
resources related to derivatives.
But diva by DW has the value X
and D by DX will have the value w
in the same way and divide by DB.
We'll have the value one.
Okay.
So now, suppose we wanted to calculate
all of these derivatives and why
would we want these derivatives?
Because the.
Technique that we used to train the
machine learning models, the deep learning
models, the technique that we used to
train the model to produce those images
that we looked at the beginning that
requires computation of derivatives.
It involves derivatives in some way.
And we'll see how today by
the end of this lecture.
So that's why decorators are important.
And what  provides is if you want a
derivative of Y with respect to w.
All you need to do is you
need to call Y dot backward.
So just call wide-eyed backward here.
And when you call wide-eyed backward,
what happens is the derivatives of Y
with respect to each of the inputs,
which has WX and B get stored in the dot
grad property of the respective tensors.
So DUI by DX should go into extort
grad Viber, DW into w dot grad
and diva DB into B-Dog grad.
So this is that, is it DUI by D
w H should have had the value X
and X itself has the value three.
So we got back three here.
That's right.
And diva by DB had the value
one as we calculate it.
So that is something that
we get back here as well.
But DVI by DX has the value.
None.
Now, why is that?
Now, if we go back and look at the
definition of X w and B, we can see
here that we have not specified requires
grad equals two through four X, but
we have specified eight for WNB.
Now, what this tells Pytorch is that we
are not interested in the derivatives of
any future outputs with respect to X, but
we are interested in the degratives of
future outputs with respect to w and B.
And this is just an
optimization right now.
We're just working with three numbers.
But if we were working with 3 million
or 30 million, we would not want to do
millions or tens of millions of useless
operations that will cost us time.
That will cost us energy as well.
So that's where you use
the request grad property.
If you said that requires grad property
to true as we have done for w and B, we
get back these tensors, which are the
derivatives of Y with respect to w and B.
Okay.
And other dot grad here, the grad
indoors grad is short for gradient,
which is another term for derivative.
And the term gradient is primarily
used while dealing with vectors
and matrices and their derivatives
and partial derivatives and so on.
So that was basic tensor operations.
And instead arithmetic.
Apart from arithmetic operations, the
torch module also contains many functions
for creating and manipulating tensors.
So let's look at some examples here.
Let's look at this example
of a function called full.
So you say torch dot full, and you'll give
it a shape and then you give it a value.
And then it creates a tensor with
that value with the given shape.
So that value is repeated everywhere.
Similarly, we have this,
another tensor called cat.
So what this is going to do is
this is going to join the tensors.
So it concatenates or joins
to tensors, which compatible
shapes into a single tensor.
So let's see what T3 was once again.
So this is T3.
T3 has three rows and two columns.
This is  three rows and
two columns as well.
And if we do torch, not cat T3
committee six, we get back T seven.
Where now we have six
rows and two columns.
So can you figure out how this happened?
What happened exactly here?
Try going through the
documentation to figure this out.
Then we have the sign here.
So if we take piece seven, this
is sign or this is D seven.
And if we call it a tortured
sign, this could be signed costs.
There are a bunch of
mathematical functions.
In fact, there are.
Close to a thousand tensor operations
that are available in the library.
You can compute on these stances.
So here you get back to you, two door
TA torch dot sign of , which is tiered.
So the sign of these numbers is this.
And what you can also do is you can
take a tensor and you can retain the
values, the elements in the exact
same order, but you can simply reshape
or change the shape of the tensor.
So here we going here, we are
going from a tensor with the shape.
One, two, three, four, five, six, so six.
Comma to two, we are going to a sir
with the shape three commit to Okay.
And this is now we get back
a three dimensional tensor
from a two dimensional tensor.
So this is also a really useful function.
So this is the documentation,
page for tensor operations.
You can open this up and read about
some tensor operations and over
the course of the assignment, you
will be implementing and showcasing
at least five, 10 set operations.
So more on that later, but right now what
you can do is you can just experiment
with some more tensor functions and
operations using these empty says,
okay, so we've looked
at tensors right now.
We have looked at tensor operations now.
If you are familiar with Peyton and
a little bit of data science, you may
have heard about this library called
numpy and numpy is a popular open
source library used for mathematical
and scientific computing in Python.
The reason it is used is because it
has a lot of functions implemented for
large multi-dimensional Aires, very
efficiently in C plus and it has a
vast ecosystem of supporting libraries.
So it has this library called pandas
for file IO and data analysis.
It has a library called Mac plot lab
for plotting and visualization and
open CV for image and video processing.
Now, this is a huge topic in itself.
Data science with Python.
And if you are interested in learning
more about  and other data science
libraries in Python, then you can
check out this tutorial series.
We also have a full video course that
you can take or over six weeks, and
you can take that course side-by-side
by going to zerotopandas.com.
That is zerotopandas.com
Now, because by, because Python already
has this huge data science ecosystem,
instead of reinventing the wheel
Pytorch interoperates really well
with numpy to leverage its existing
ecosystem of tools and libraries.
So this is how you create, so let's
see how that interoperability works.
So this is how you create an era in numpy
lets, create a numb by area to begin with.
We import numpy as NP.
That's a convention.
And then we do NP dot array and
give it the data that we want.
So just as we have created tensors
we can also create numb by AirAsia.
So there you go.
And Ampyra has been created a two
by two matrix and we can convert
a numpy area into a Pytorch
tenser using torch taught from
So we call tall, short from numpy pass
in the answer X, and then we get back.
Why.
Now let's verify that the numb piety and
the torch tenser have similar data types.
So the number area has a datatype
well, , we've not seen the datatype,
but you can check it using X DOR D
type, and Y dot D type is going to
give you the data type of the tensor.
So they do not have the
same data type float.
64 in numpy is slightly
different and tore short float.
64, both are implemented
internally differently, but.
They correspond to
similar kinds of numbers.
And that's the important thing here.
So what each data type and numpy,
there's a corresponding data
type in Pytorch and vice versa.
Now what we can do is now we
take our tensor and then we do
some machine learning with it,
with some deep learning with it.
We train some models, we put it
on a GPU, we perform a lot of
computations and then we want to get
back some results out into numpy.
So what we can do is we simply
call the dot number by method or
potential, and that gives us back.
interoperability between  is
essential because most of the datasets
that you work with will likely.
Be red and processed using non-buyer
is, and any data analysis course or
tutorial you take will be using numpy.
So you might wonder at this point,
why we need a library like Python at
all, since numpy provides all the data
structures and utilities for working
with multidimensional data, right?
So there are two main reasons here.
One is autograph, which is the
ability of  to automatically compute
gradients for tensor operations.
This is essential for deep learning.
And the second is GPU support.
So when we are working with massive
datasets and large models, which is
GBS and GBS of data by Tosh, tensor
operations can be performed very
efficiently using graphics, processing
units or GPU's and computations that
might typically take hours on a CPU.
Despite using the efficient
functions from numpy, they can be
completed within minutes using GPU's.
And what we will do over the course
of this series is leveraged each
of these things quite extensively.
So that completes our
discussion of tensors.
And at this point, what you should
be doing is to save this notebook.
So what you're looking at right now,
Google CoLab, this is going to shut
down after some time, this is running
on the cloud, and this is stored
privately in your Google account.
Now, what you can do is you can take
this notebook and put it onto your Jovian
account, the same account that you used,
that you created while enrolling for the
course, or while running this notebook.
So all you need to do to put
this notebook onto your Jovian
account is run PIP install Jovian.
So this is going to install the Jovian
Python library, import the Jovian
library, and then say Jovian dot
commit and give it a project name.
So here, I'm going to give it the
project name zero one pytorch basics
live because I don't want to affect
the notebook that I already had.
And I feel already this project
already exist and it will simply
create a new version in the project.
So I would just want to create a new
project here by touch basics life.
And when you create this project or
when you run Jovan not commit, you
will be asked to enter an API key.
So just go to your Jovian account,
which is jovan.ai and click on
the copy API key button, come
back here and paste the API key.
And this will take your
notebook from Google CoLab and
it will put it onto Jovian.
Now you see here.
Now this is a read only
version of the notebook that
you cannot run the code here.
Nobody can modify your code here, but
what you can do is you can take this
link and share it with anyone and you
can, this was also part of your profile.
So you can come back and
run this code later on.
Whenever you need to, if you've done
half your work, keep running Jovan dot,
not commit from time to time in your
notebooks, and you can come back and
you can read on your code and continue.
Every time you run Jovi,
not commit in CoLab.
It will save a new version of
the notebook on the same project.
So just to summarize what we've
learned here, this tutorial
covers introduction to Python.
Tensors tensor operations, ingredients and
interoperability between Python and numpy.
Now, this is all fairly simple stuff,
but we are learning this because
this is giving us the foundation to
pick up and learn the next topic.
Which is gradient descent
and linear regression.
So let me open that up.
Now, one thing you can we do
here is there are a bunch of
questions, almost 30, 32 questions
here at the end of this notebook.
So if you want to test your understanding,
just try answering these questions
or maybe start a new blank notebook
and copy paste each question, and
then type the answer out or write
some code, which produces the answer.
That is a great way to
test your understanding.
So let's move on.
Now.
We are on the second notebook,
zero tolinear regression.
This is linked from the first notebook,
or you can also come back to the
course page and find the link here.
once again, this is a Jupyter
notebook hosted on Jovian.
So what we need to do now is to
run the code, to run the code.
We are going to click run,
and then select run on co-lab.
Now once you've already authorized CoLab,
you will not need to authorize it again.
It will simply run the notebook for you.
so here we have it.
The notebook is now running.
I am just going to go to edit
and clear all the outputs.
Now, a quick tip before we
get into this notebook is.
If you're watching us live, then
you can just sit through and
watch the tutorial right now.
And then you can go back and
watch the recording and posit
wherever you need while you're
experimenting with the notebook.
It's also a good idea to just skip ahead
to the next cell and just run the code.
See what happens.
And if you don't understand, just
come back to the video and watch it.
Now, if you have questions at
any point, then what you can do
is go back to the course page.
So you have go to zerotogans.com,
open up the lecture lesson one.
And on lesson one, you will find
the link to a cost discussion forum.
So just click the cost
discussion forum here.
This will bring you to the discussion
forum where you can ask questions.
So there is a topic here
for this live stream.
So you can just open up this
discussion topic, this thread
and press the reply button.
There's a reply button at the bottom.
Just press the button and you will be
able to ask a question and somebody from
the community or from the course team
is going to answer the question for you.
Participating in the community is
actually a big part of this course.
We've had thousands of people asking
and answering questions and you
can learn a lot simply by reading
the questions and the answers.
So do check out the community forum
and do ask questions and answer them.
Okay.
So getting back to a linear regression
notebook, we have cleared all the outputs.
And now we are ready to
start running the code.
Once again, I will run the first cell
to make sure everything works properly.
This is going to start the
server for us Jupyter in the
background in start initialize
the server and run the first cell.
No, we are on our second topic already.
Gradient descent and linear
regression with Pytorch.
And this tutorial, we are
going to cover these topics.
We are going to understand what linear
regression and gradient descent mean.
We are going to implement a linear
regression model using Pytorch tensors
and we're going to train a linear
regression model using the gradient
descent algorithm, and we will implement
the gradient descent algorithm.
Not just from basic tensor operations,
but also using high touch built-ins.
So first we learn all the nitty gritties,
all the different things involved
in implementing things from scratch.
And then we see how easy  makes
us, makes it for us to perform
all of these competitions
with just a few lines of code.
Okay.
And linear regression is in
fact, one of the foundational
algorithms and machine learning.
If you take any machine learning
course, they will almost always
begin with linear regression.
And so is the case for deep learning.
In fact, linear regression is
very closely related to what
you're doing, deep learning.
Make sure to understand it
properly, ask questions, rewatch
the video, run the notebook, but
make sure that you understand linear
regression and gradient descent.
And that will set you up really
well for the rest of the course.
Now, what we'll do is we will take an
example problem and work through it to
understand what linear regression is.
So we will create a model that predicts
crop yields for apples and oranges.
So these are called the target variables.
And we will, this model will
predict these crop yields by
looking at the average temperature,
rainfall and humidity in a region.
And these are called the input variables.
So here are some training data let's
suppose we've gone to five regions
where you've done surveys over the past
few years, and we've come up with this
information that in the Cantor region,
the temperature was 73 degrees Fahrenheit.
The average temperature, the rainfall
was 67 and the humidity was 43%.
Over a year.
And with these average values, we hired
56 tons per hectare of apples produced and
70 tons per hectare of oranges produced.
So that was a yield of apples and oranges.
And similarly, we've done this
exercise for four other regions.
So now we have what is
called this training data.
So in a linear regression model,
each target variable is estimated
to be a weighted sum of the input
variables offset by some constant.
This is what that looks like.
So you have the yield of apples is
some number, some weight w one, one
multiplied by temperature w one, two
multiplied by rainfall w one three
multiplied by humidity plus B one.
So I understand these weighted, why we
have these weighted averages, because in
a sense you are saying how important each
of temperature, rainfall, and humidity
are in determining the yield of apples.
But what about . This
is a very simple trick.
Suppose temperature, rainfall and humidity
are all zero, which may not happen in
real life, but suppose they were, even
if they did get to that point, the
yield of apples still would not be zero.
That would still be a non-zero yield
audit may be negative, or we may need
some kind of an adjustment factor.
And that is why we have this constant
and this concerned us called the bias.
Okay.
So the yield of apples is
is a linear combination.
A temperature in fallen humidity
using some beats and a bias.
Similarly, the yield of oranges is another
linear combination of temperature in
fallen humidity but this time, we are
using different weights and visually
what this means is if you plotted the
temperature and rainfall on two axes
and then the apples on the third axis.
And we are not looking at humidity
here because it is not possible
to plot in four dimensions.
So we can only plot three dimensions here.
So we have temperature and rainfall.
And what this is saying is as the rainfall
increases, the yield of apples increases,
and as the temperature increases,
the yield of Apple also increases.
Now, this is obviously not strictly true.
If the temperature gets really high, then.
There will be no crop.
Everything will be so our dry,
everything, all plants will die.
And if the rainfall gets really high,
that is going to affect the crop as well.
So there will be a crop failure in
both those conditions, but what we
save, and that is why this is called a
model, and this is not the truth or the
real relationship or a physical law.
What we say is that within reasonable
values of temperature, rainfall, and
humidity, there is roughly speaking.
A linear relationship between the yield
of apples and these input variables.
That's what we mean here and now the
learning part of linear regression is
to figure out a good set of weights.
W +1 121321222, three.
And biases.
. Sometimes weights and biases
together are just called weights.
So you will see them being
used interchangeably.
So the learning part of linear
regression is to figure out a good
set of weights using the training
data so that we can make accurate
predictions for the new data.
So once we have figured out a good set of
weights, then we can go to a sixth region.
And if we know the temperature,
rainfall, and humidity for that
region, let's say we've estimated
that for the next year using some.
Weather analysis.
Then we can predict what the
yield of apples and oranges
in that region is going to be.
And that might be a very useful thing
because depending on the demand,
you can then decide what to plant
more of how much area you should
allocate for apples and oranges,
what the prices should be and so on.
And this is a real world example
that happens all the time.
Prediction of crop yields
using weather, especially.
So that's what the learning
process involves figuring
out a good set of weights.
And the way we will do this
is we will train our model.
We'll train our model by adjusting
the weights slightly many times.
So we started with random weights.
Our model will do very badly.
It will make bad predictions, but
we will improve the weight slowly
using an optimization technique
called gradient descent, which is
at the heart of not just linear
regression, but all of deep learning.
So let's begin by
importing numpy and pytorch
it's not here.
We have the training data, the table that
you saw earlier that can be represented
using two matrices inputs and targets.
So here we have the input matrix.
The input matrix is we're
representing it using a numpy area,
a number area, because this is.
Somewhat closer to the real world
scenario, where somebody will send
you an Excel file or a CSV, and you
will read that Excel file in using
the numpy pandas libraries and you
will get back and umpiring ultimately.
And we learn about those things over
the next few lessons and assignments,
but for now we've assumed that somehow
we have put together a number by Eric
containing all the input variables.
So we have one row for each region.
And then we have one column
for temperature, one for
rainfall and one for humidity.
And these are the exact same
values that you saw earlier.
Now, one way to convert this into floating
point numbers is to just put a.here.
And the other way is to just
specify a data type explicitly here.
Okay.
And we use floating point numbers
because we're going to be doing a lot
of matrix operations, which will involve
decimals, which will not all be integers.
So let's create a non-buyer right here.
And then we can create another
number area for the targets.
So once again, for the five regions,
we have the yield of apples and the
yield of oranges in towns per Hector.
So that is what is captured here
using this five by two matrix.
Once again, this is a floor 32 matrix.
Now we have treated, we have separated
out the inputs and the targets because
we will be operating on them separately.
Now we will be doing some
matrix operations and so on.
So let's convert these areas into Python.
Tensors we say tours start
from numpy inputs and torch
taught from numpy targets.
No, these are
next week.
We need to create our linear regression
model to create a linear regression model.
The first thing we need are
the weights and the biases.
And the weights and the biases
can be represented as matrices.
So let's see how, if you look
at this expression here you
have w one one w (121) 321-2223.
If you simply hide the temperature
rainfall humidity, just put them away,
put all the operators away for a moment.
You can see that this kind of
forms a matrix where it has.
Two rows and it has three elements, right?
So this is a two row by three
element by three column matrix.
And then the biases B one
B2, they form a vector.
So what we'll do is we will initialize
the weights as a matrix, and we will
initialize the biases as a vector.
And we will initialize them with
random values because we don't
know what good weights are.
What are the right weights
for this relationship?
So we can say torch dot rant, N.
Drying enter simply going to create
a torch tenser with the given shape.
So the shape that we are passing
is two common, three, two rows,
and three columns are matrix and we
will set, requires grad set to true.
And we'll see why this is useful later.
Then we also have torched or grinding too.
This is going to create a bias
vector with  and this will set
requires gratitude through as well.
And let's print w and B.
So tall, short rind and creates
the tensor with the given shape.
And the elements are big, randomly
from a normal distribution.
If you don't know what a normal
distribution is, don't worry about it.
All that means is the numbers will
roughly come from the range minus
one to one or minus two to two,
and they will be randomly chosen.
Now that we have the
wheels, our model is simply.
A function that performs a matrix
multiplication of the inputs
and the weights transposed.
So let's see what that means.
So this is one row from our input matrix.
This is the whole input matrix and
look at the first row, 73, 67 43,
temperature, rainfall, and humidity.
Now, if we take the weights matrix, and
then we transpose the weights matrix.
So the first row now becomes the first
column and let's just concentrate
on the first column for now.
When we perform a matrix multiplication.
This element gets multiplied with this
67 gets multiplied with w one, two and
43 gets multiplied with w one three
and together, then they get added up.
So we get 73 w one, one plus 67
w one, two plus 43 w one three.
And that's exactly what we had defined
as our linear regression model.
And then if we just add a plus
here and then add a bias, so
then B one also gets added to it.
So this expression.
Which is, which takes a five
by three matrix, multiplies
it with a three by two matrix.
So that gives us a five by
two matrix, and then it adds
another five by two matrix to it.
So overall, all of this put together
gives us a five row by two column matrix.
This expression will give us
the predictions of the model.
So this expression let's,
see what that gives us.
We take the inputs and then we say,
we want to do a matrix multiplication.
So for the matrix multiplication,
we use the expression at the
add character represents matrix
multiplication in Pytorch.
Then we call w dot T.
So that is going to
transpose the weights matrix.
Remember we needed, I need the rows to
become columns and columns to become rows.
And then we add the bias tones.
So let's see what that gives us this
tensor and what this tensor represents.
Is, if you look at the inputs, once
again, it represents for this input for
this temperature, rainfall, and humidity.
What is temperature times?
W one, one plus rainfall times w one,
two plus humidity times w plus B one.
Which is the prediction of the model for
the yield of apples in the first region.
Similarly, this becomes the
prediction of the model for the
yield of apples in the second region.
Sorry for the yield of
oranges in the first Legion.
So this is this is for each row or
for each row of input for each region.
We get back one row of output, which
is the prediction of the model for
that region for apples and oranges.
And similarly, these other
predictions for the other regions.
So five regions, we get back
five times to 10 predictions.
Okay.
And that's all it is that is
our machine learning model.
It takes the input, performs a
matrix, multiplication adds some
bias and churns out an output of this
is how our model makes predictions.
Obviously it's going to be pretty bad
because our weights are pretty bad,
but we'll see what to do about that.
So we're just going to
define it as a function.
We are going to define a function
model so that we can give
it different sets of inputs.
We can give it one input
to input five inputs.
New inputs and so on.
So it takes the input matrix and it
performs a matrix multiplication with the
weeds transposed, and it adds the bias.
So we've defined a function here.
Now let's pass the inputs as an argument
to model, and that is going to give
us the predictions of the model.
So here we get back this same predictions
as before we've just done the same thing.
We've just used a function right now.
And let's compare these predictions
with the actual targets.
So remember that we've used the
inputs here to make the predictions,
but we've not really used the
targets because our model simply
takes inputs and gives predictions.
So let's take the
targets and compare them.
And it seems like these are off
the, here we get a value of 20,
but the prediction should be fixed.
56 here we get 33, the
predictions would be 70.
Here we get minus eight.
Not sure what that even means.
And here we get minus 30.
So a lot of the predictions
of the model don't make sense.
And there is a big difference
between our models, predictions
and the actual targets.
And this is expected because
we've initialized our model
with random weights and biases.
So obviously we can't expect a randomly
initialized model to just work,
but we have a good starting point.
Now we have a model and the
next thing that we need to do.
Is to improve the model, but to improve
the model, we need a way to evaluate
how well our model is performing.
And we can do that by comparing
the models, predictions with the
actual targets, and you can compare
them visually and say that they are
bad, but that's not good enough.
You need a way to compare
them mathematically.
Ideally, it would be nice to reduce
the whole thing to a single number.
So how are we going to do that?
So here's an idea.
Let's take the predictions and let's
subtract the targets from the predictions.
So what that does when you have
two meters, is that it's going to
perform an element by subtraction.
So you get 20 minus 56, 33 minus 70.
So for each element of this matrix
now how far away that corresponding
prediction is from the target.
Each of these individual
predictions, let's put that into
a diff or difference matrix.
And now you have some, we could take
an average or something at this point,
but the trouble is that there are some
negatives and there are some positives
in the average may come out to zero, but
that does not mean the model is good.
So to get rid of the negative numbers,
we can simply say diff star diff to
take a square or an element by square.
Now re remember this time, when
we say star, we are doing an
element wise multiplication,
not a matrix multiplication.
So we said diff star diff.
And now what that has given us is.
All of these numbers have become
positive, and this is just a
scientific notation E plus zero
means 10 to the power minus three.
So this is 1.288 multiplied by 10
to the minus three, or sorry, 10
to the 10 to the power of three.
If it was minus, it would be
10 to the power of minus three.
And that simply means 1,288, 1288.
All right.
So that is your difference
in the difference matrix.
Now that's great.
Now we have a pretty good idea, and now
we can probably take an average of these.
So to take an average, the first
thing you might want to do is take the
sum and to take the sum of a matrix.
We simply called torch dot sum,
and that is going to take the sum.
And then we divided by
the number of elements.
So the number of elements is 10.
But you should never hardcore this
information because now let's say
you go back and you're running
this notebook again for a different
number of regions with a different
number of target or input variables.
So what do you want to do is you
want to get the list of the number of
elements you want to get the number
of elements in this Def matrix.
And the way to do that is use the
new mill method, def.new mill.
And that's going to give
you the number of elements.
And torch tensors in this way, just as
we see tall short, some, this is a torch
is a function for operating on tensors.
There are hundreds of these.
Similarly tensors also have these methods.
So for each specific tensor, you can
invoke all of these methods to get
information about the tensor, for
example, the total number of elements.
And this is a very important part of
learning deep learning that you have
to know, what are all the different
operations you need to perform and
what are the functions to do them?
Often anything that you want to do
will be possible with just one or two
lines of code, but you will have to
search through the documentation for it.
Or you can just ask on the forum.
Okay.
So now when we do the sum of the
squares, and then we take an average
of that, that gives us a single number.
So now the single number tells us
how badly the model is performing.
So this number is called
the mean squared error.
So we do, we calculate the loss
between a difference between the two
matrices predictions and targets.
Then we square all the elements
of the difference matrix
to remove negative values.
And then we calculate the average of
the elements in the resulting matrix.
And this number is called
the mean squared error.
So now we take the what we're
going to do is define a function
for mean squared error, because
we need to compute it a few times.
And it does the exact same thing
that we did line by line earlier.
So we take T1 and we take T2.
We, these will be predictions
and targets respectively.
So we take a difference between the two
and then we do tall, short, some deaf
start Dave, so square of the elements.
And then we divide that the total sum
of elements of squares, of elements by
the number of elements in the matrix.
Okay.
So it's always a good
idea to write functions.
And whenever you see a function
that you don't understand.
Take it apart.
So use some example, values, some
example inputs and run it line by line.
And that is a great part about Jupyter.
You can type the code and you can
experiment with it right here.
You don't have to ask somebody
what's going to happen, or what
happens inside this function.
You just type the code
and see what happens.
So let's define the MSE loss function
and we can now use the MSE loss function
with the predictions and the targets.
To get back what is called our loss.
Now, what does this number represent
since this is the mean squared error.
Now, if you work backwards, what that
means is on average, each element in
the prediction differs from the actual
target by the square root of this number.
So the square root of 3,600 is about 60.
And if that is the average difference,
that is pretty bad considering that the
numbers that we're trying to predict are
themselves in the range of 50 to 200.
So the expected yield or the real
yield is 60, and you're predicting 120.
That's pretty bad that you're going to
lose a lot of money with that model.
Okay.
And that is why this result
is called a loss because it
indicates how bad the model is at
predicting the target variables.
It indicates the eight represents
the information loss in the model.
So the lower, the loss,
the better the model.
Okay.
It's now we have a loss and now
we need to improve the model.
We need to reduce the loss.
And as I've said, right from
the beginning, gradients
play an important role here.
And that is why if we scroll back up.
To the point where we defined
our weights and biases.
Remember, these are the things that
we've randomly initialized, and these
are the things that we need to change.
So that is why we need to say it
requires grad equals to true here.
So now we have set requires grad to true.
And if you recall what this
means is that you can go back.
Is that you can now run loss dot backward
because loss is obtained by doing a mean
squared error on predictions and targets.
Predictions themselves are obtained
by multiplying the weights matrix
with the inputs and adding the bias.
So the loss is ultimately a function
of weights and biases, and of course
the inputs and targets as well.
But the inputs and targets are fixed.
We don't really want to change them.
So the weights are what's important,
what we're going to change.
So the loss is a function of
the weights and the biases.
So when we run loss dot backward, because
we have set requires Greg to throw in the
weights and in the biases, if I simply
print out w and w dot grad, you can see
here that this is the weights matrix,
and w dot grad now contains a matrix
of the same shape, but with different
values what do these values represent?
This value represents the
derivative of the loss with
respect to this weight element.
Now keep this in mind.
A matrix is something
that we have defined.
It's just a human construct.
We could simply have used numbers.
W one two w one, one, one, two, and
so on, and we could still have done
the same computation instead of using
matrix multiplication, we would have had
to define all of these relationships.
So ultimately it is.
Loss is a function of each
of these individual weights.
And what this represents is the
derivative of the loss with respect
to this specific weight element.
And this is also called the
partial derivative and so on,
but let's not worry about that.
Similarly, the, this element represents
the derivative of the loss with respect
to this specific weight element and so on.
Okay.
Similarly, if we print B.
And beat dot grad.
You will see a similar
relationship here, B and B dot
grad have the exact same shape.
So this represents the derivative
of the loss with respect to
this, with respect to beat now.
Okay.
Why have we done these
derivative calculations?
How is it useful?
We are going to use the derivatives
to adjust the weights and biases.
Remember they were initialized
randomly, and now we are
going to adjust them slightly.
To reduce the loss.
And this is where we have to now apply
a little bit of algebra and calculus.
The loss is a quadratic function of
our weights and biases because mean
squared error performs a square.
So anything that comes inside
that formula gets squared and
multiplied with other terms.
So loss is the quadratic
function of weights and biases.
And this is what a quadratic
function looks like.
Roughly speaking.
Now this is not exactly a quadratic
function because accord retic function
only has one minima, one global minimum.
In this case, there are multiple.
So this is more of a generalized
polynomial, but I guess you get
the point that the loss curve looks
something like this with respect
to any specific weight element.
So what that means is if you take the
element . And you change its value.
If you make it really large.
Now, what that is going to do
is that is going to just make
the law loss really large.
You set the weight to 10,000,
the loss will become huge.
Or if you make it really small,
use it to minus 10,000, it will get
squared because there is squaring
in world and the loss will, once
again become really, large, right?
So as.
The weights go from minus infinity
to infinity you on both sides.
The loss increases and somewhere in
between, there is a nice region where
the losses are low and there is a
point where the loss is the lowest.
Now with this picture in mind, let's
think about what the gradient indicates.
So the gradient of the loss or
the derivative of the loss with
respect to w one, one indicates
the rate of change of loss.
Or geometrically speaking, it indicates
the slope of the loss function.
So if you plot the loss changes in
loss of the value of loss, keeping
all the weights constant and simply
moving  around simply changing that
this is the curve that you'll get.
And at a specific point, the derivative
indicates the rate of change or the slope.
Now, what does that mean?
If the derivative or the
gradient element is positive?
What that means is that the
rate of change is positive.
What that means is if you increase
w one months slightly, then the
loss will increase slightly.
And if you decreased w one months
slightly, then the losses decree
is going to decrease slightly.
And that's really all there is no
matter what values you started out with.
When you compute the gradient and you
look at the value of the gradient,
whether it is positive or negative
whether to increase or decrease
the weight, each individual weight.
So if the gradient is positive to increase
the loss, you increase the weight to
decrease a loss or decrease the weight.
Now, obviously we want
to decrease the weight.
So we are going to decrease the
we want to decrease the loss.
So we are going to decrease
the weight slightly.
And similarly, if the
gradient element is negative.
So that means the rate
of change is negative.
This means that the slope is
downward, the slope is decreasing.
So in this case, we have the
opposite relationship that
increasing the weight slightly.
For example, moving W2 one to the
right is going to reduce the loss
and decreasing the weight slightly
is going to increase the loss.
Okay.
Now this is really the only complex
part of this entire exercise.
So to go back and read this carefully
and try to draw graphs on a piece
of paper, just to figure this out.
Maybe review a lecture on
calculus if you need to.
But what if you get this, you get
the essence of gradient descent
taking random weights and using the
gradients to identify how to adjust
them slightly, to reduce the loss.
So we are going to use this and we
are going to use it in this way.
So now we have the weight.
And then we have the, let us just print
the weight and the gradient of the weight.
So here is the, weights and here
are the gradients of the weights,
and now let's apply our logic.
So we have point minus 0.271.
This is w one one, and it has the
derivative, of the loss with respect to w
one, one is minus 4,252 that's negative.
That means the rate of changes.
Decreasing.
So if what we need to do is we need to
slightly increase the weight element.
Similarly, if the directive was
positive, we would need to slightly
decrease the weight element.
So there's a simple trick of doing this.
What we do is we simply subtract
the gradient from the actual weight.
So we take 0.27, six one.
And from that, we simply subtract this.
So what happens is if the.
If the gradient is positive, then
the weight element decreases.
And if the gradient is negative, then
the weight element increases because
negative of negative becomes positive
and you are actually adding when
you're subtracting a negative number.
Okay.
So now we know that we subtract the
gradient to increase or to decrease
or increase the weight depending on
whether it is positive or negative.
So subtracting the gradient will give
us a new weight, which will have, which
will lead to a lower loss because we
are going downhill along the slope.
We are descending along the gradient and
that's where it's called gradient descent.
Bye.
There's a problem here.
The weight is 0.2 and
we're subtracting food.
Or in this case, we would be adding 4,252.
So we are looking at this case.
Now the weight is 0.2 and you add 4,252.
You're going to end up somewhere
here, way on the right.
And the loss is going to be
so large that you will have no
way of recovering from that.
What you need to do is you need
to take small steps, right?
You need to take a really small
step, maybe just about 10% of the
weight's value or something like that.
And that is where we can use this.
We can use this factor that
when we are subtracting.
So what we're saying is we're saying
w minus equals w dot grad, and this is
going to do an element by subtraction.
This is the same as saying
w equals w minus w dot grad.
So element by subtraction.
So when we're subtracting, we
multiply four to five, two with.
10 to the minus five, one minus five.
And what's the benefit of doing that.
Now you can see that this is also
in a similar range as these weights,
so that when we do w minus w dot
grad times one minus five, these
are the new weights that we get.
Now we have gone from these
weights to this new set of weights,
wherever gradient was negative,
we have increased the weight.
Slightly and wherever gradient was
positive, we have decreased the
weight slightly, and each of these
individual weights has contributed
independently to decreasing the loss.
So now we use the model again with the
new weights, then the loss will be lower.
Why?
Because of this.
So this is how we do it.
We say with torch dot, no grad.
Why do we say  short?
No grad now remember.
Pytorch automatically keeps track of
all of the calculations that you're
doing so that whenever you call
backward, gradients are computed.
That is an expensive operation.
And sometimes it can have
unintended consequences.
Now, what we want to do here is we
want to actually use the gradients
that are already computed to
change the values of the weights.
So we do not want to affect the gradients
while the calculation is happening.
We do not want to track this calculation.
So via simply telling Pytorch do
not track this calculation or this
computation for the purpose of
automatic gradient computation.
So we're done with all that.
We just want to use
the gradient right now.
So we say pit torched or no grant.
And then we subtract from
w w.com multiplied by a
small number one minus five.
And this is also called a
learning rate because this.
Is the number that determines how
big your steps are going to be,
how fast your model is learning.
And by fast, we don't mean a
higher learning rate will mean
that your model will train faster.
In fact, a higher learning rate might
mean that you will jump so far away
that you never recover from it, right?
So it's just a learning rate and you
need to keep it in the right range
so that you take small enough steps
to get closer to the minimum point.
Similarly, we are going
to do B minus B dot grant.
And that's done.
No, let's look at the weight,
the new weights right now.
Let's see.
WNB so these are the new weights,
slightly changed, not too much.
And these are the new biases and
let's know, call let's know make
predictions using the model once again.
So let's call the model.
Give it an input.
So give it the inputs.
So remember the model takes the
weights and we've, since we've
changed the weights, now it is
going to use the new weights.
So it takes the inputs, multiplies
that with the weights, matrix,
transposed, and adds a bias.
So once again, it performs a
competition gives us some predictions.
Let's take those predictions here, put
them in a variable just to make it clear
and let's take those predictions and put
them along with the targets, into the
mean squared, error, and print the loss.
And there you go.
The losses, not two eight, one nine.
So we went from a loss of, let's
see, here, we went from a loss
of three, six, four, five, 3,645
of 2,819.
And that's pretty good.
We've already reduced it by 40% or so.
And that's gradient descent for you.
What we do is we simply take the
gradients and descend along the
gradients by subtracting a small
quantity, proportionate to the gradient.
And we'll just say formalize it a
little bit and do it once again.
But before we proceed, one last
thing that we need to do is we need
to reset the gradients to zero.
So remember w dot grad and beat
or grad have the gradients of the
loss with respect to these weights.
Now, whenever we are done with all
this gradient calculation, we simply
need to tell Pytorch that we need
to, we are done with the gradients.
Now we can remove the gradients,
reset them back to zero.
Otherwise what happens is the next
time you calculate a loss and then the
next time you call law, start backward.
It is going to keep adding to the
gradients that are already there.
This is just how I touch works.
So whenever you're done with your
gradient computation, simply call
this zero method on the gradient.
So w.grad.zero.
And.grad.zero.
What that will do is now it will
set the gradients back to zero.
Okay.
So that's, how you improve the weights.
That's how you adjust
the weights of the model.
Now, as we've seen, this is going to
reduce the loss and improve our model.
So this is what is called
training, the model using gradient
descent, and it has these steps.
Step one, generate predictions.
You take the inputs,
put them into the model.
That gives you predictions step to
calculate the loss, take them predictions
and the targets and put them into
the mean squared error function.
And there are other options for
error functions or loss functions.
But we are going to use mean squared
here, and that gives you the loss
step three, computer the gradients.
So we say lost dot backward, and that
gives us w dot grad and B dot grad.
And step four and five is to adjust
the weights and reset the gradients.
So we adjust the weights by subtracting
a small quantity, proportional to the
gradient w minus equals w dot grad
multiplied by one E to the minus five.
And then we also said the gradients
back to zero, and we do all of
this while informing fighters
that we did not track all of these
computations for the purpose of, for
the purpose of gradient calculation.
We are done with that.
So there we update the weights slightly,
and then we perform, we reset the
gradients back to zero, and that gives
us yet another new set of weights.
So now I think you get the idea.
Now each time for each weight, you
are making a small downward descent
along the stope, along the gradient.
And what that does is
that reduces the loss.
So now you can see the
losses become even lower.
If we went from 2,800 to 2,200, Now
it's straightforward, what you need to
do to get to the best possible model.
So to reduce the loss further, we can
simply repeat the process of adjusting
the weights and biases multiple times.
And each iteration is called an ebook.
So let's train the model 400 deep box.
So we go for in the range, 101,
200 for the exact same process.
Pass the inputs into the
model, put the predictions and
targets into a loss function.
Calculate the gradients using backward.
And then with torch dot, no grad set,
the gradients subtract a small quantity,
proportional to the gradient from each
rate and then set the gradients back
to zero and that will improve the loss.
And really this is all that we are
going to keep doing again and again,
over the course of six weeks with
different models and different inputs
and different data and different
different loss functions, primarily.
The model will get more complex.
The loss function will change.
The data will change, but
this will remain constant.
And that is why it's so
important to understand gradient
descent and linear regression.
Okay.
Once again, let's verify
that the loss is now lower.
This time.
We've done the process a hundred times.
So now the loss is far lower.
It's only about four and four
zero two four zero two squat
squared of four zero two.
Let's see.
Yeah,
so squared or four, zero two is about 20.
That means our predictions are
a lot closer to the targets.
Now
there you go.
The predictions are 60 to 67,
which is quite close to 56 and 70.
It's not perfect.
Maybe you can go for another a
hundred deep box and see what happens,
but it's getting pretty close.
And try running this notebook, try running
for as many boxes you can and see what
is the lowest loss that you can get.
So that was linear regression
and gradient descent from
scratch using matrix operations.
We have understood every single
operation that went in there.
We understood the matrix
multiplications that were happening.
We saw how great in
descent is implemented.
We did not calculate the
derivatives ourselves, but if you.
Tried out the formula on paper,
I'm sure you can work out
the derivative of the loss.
You just expand the loss in terms of
all the weights numbers, the entire
formula, and then take a derivative
and then use those derivatives
to create the gradient matrix.
You can do that.
But pytorch does that for us,
but we know how it works, right?
So we know everything right now from
the very basics, which is tensor
operations, matrix operations on how
to build gradient descent together.
So let's save our notebook at this point.
Once again, you install the Jovian library
and run Jovian dot commit, and that
is going to save the notebook for us.
Let me grab my API key here.
Okay.
Take my API key and put it
into this input that takes us
notebook and portrait on Jovian.
And now you can go back and on this
notebook from Jovian when you need to.
So Jovi not commit, upload your
notebook, and you can view it later
and you can continue your work.
And this will also be
part of your profile.
So other people will be
able to view it as well.
So it becomes a portfolio of
your data science projects.
There's one question here.
Why not do this using numpy?
As we saw earlier in the first
tutorial, the reason for doing this
is because the gradient computation
can be done easily in Python.
We just call last dot backward.
We did not have to implement it
with . And the other reason is
when we deal with larger datasets,
we will have to move them to GPS.
And numpy does not support
GPS, but by touch does.
Okay.
So moving right along.
We've implemented linear regression and
grading descent using the basic pencil
operations, primarily to understand
it, but because this is such a common
pattern in deep learning, VITAS provides
several built-in functions and classes
to make it easy to create and train
models with just a few lines of code.
And we'll see, we'll do the
exact same thing that we did.
But now we are going to use the
built-in functions in Pytorch.
So let's begin by importing the torch
toward an end package from Pytorch.
And this contains all the utility
classes for building neural networks.
And that's a surprise.
We're talking about linear
regression, but it so happens that
linear regression is actually the
simplest form of a neural network.
Okay.
So as before we are going to
represent the inputs and targets as
matrices, so here we have an input.
Matrix this time, instead of
five regions, we've gone the 15
and similarly we have targets.
These, again, we have 15 targets
for each of the 15 regions
yield of apples and oranges.
And then we have the inputs and
targets are converted from matrices,
which is  to pie, torch dancers.
So there you go.
And you can look at the inputs here.
Now we have input says pytorch tensors.
Now we are using 15 training examples,
because I want to illustrate how to work
with large datasets in small batches.
What you will often find in real
world, datasets is you will not
have five or 15, but you will have
maybe thousands or tens of thousands
or even millions of data points.
And when you're working with millions
of rows of data, it will not be
possible to print all of the attain, a
model with the entire dataset at once.
It may not fit in memory, or even
if it does, it may be really slow
and it may actually just slow.
So what we do instead is we take the
dataset and break it into batches.
So we look at maybe five regions at once
and we create three batches and we perform
gradient descent with these batches.
And that helps us clean train our
models faster and fit our model
training within the Ram that we have.
So to do that, there are a couple of
utilities we are going to use first.
We need to create a tensor data set.
So we will import from tall short,
your store data, tensor dataset,
and then we will pass in the inputs
and targets into tenser dataset.
And we'll put that into a train DS
variable and intenser dataset allows
us to access inputs and targets rows
from the inputs and targets as tuples.
So we have 15 inputs and 15 targets.
And if we just pass in the range
zero to three into tensor data and to
train DS, what that's going to give
us is the first three rows of inputs
and the first three rows of targets.
So this is a very simple class that
simply lets you pick a slice of the data.
It doesn't have to be zero to three.
You can also pass a array of indices
and get back a specific, get back
a tuple containing some specific.
Rose from the data and the first,
your return is the input variable.
And the second element are the
targets for these input rules.
Okay.
Next, we will create a
data loader and a data.
Loader is what is going to split our
data into batches off a predefined size.
So we set the batch size to five.
We think that should
fit in our ramp even 15.
But just for demonstration, we are
going to create batches of five.
And then we put in the training data
set into it, which is a tensor dataset.
We provide the batch size into data
loader and we set shuffled to true.
And what is shuffle here when you said
shuffle to true, then the data loader
before creating batches is going to
create a random shuffle of the data.
And let's see how that, is used.
Sure.
I'm going to say for XB
comma vibey in train DL.
And this is how you use a data loader.
This is again, a nice thing about Python
that the classes and the objects that
VITAS provides are very pie tonic in
the sense that they fit in very well
with the kind of Python code you're
already probably used to writing.
So just as you iterate over a list
or iterate over a dictionary or any
other Iterable object in Python,
you can iterate over a data loader
and the data loader gives you.
Not individual elements or individual
rows, but it gives you batches of data.
It gives you a batch of
inputs and a batch of outputs.
So let's see that let's say for
XP, by being trained deal, print
the batch of inputs and print the
batch of outputs and breakouts.
So we are just going to
look at the first batch.
If you do not have this break, all
three batches would get printed.
So here is the first batch and
let's compare that with the inputs.
You can see that the first batch.
Does not exactly use the
first five rows of inputs.
In fact, it has picked a random sample
and that is where shuffle equals
true comes into picture that before
creating batches, it's going to pick
it's going to shuffle the rows and
then it is going to create batches.
And each time you use it in
a four loop, it's going to
create a different set of rows.
So if you just observe the first
row here, it's going to change
each time we call train DL.
There you go.
Now this shuffling helps
randomize the input so that
the loss can be reduced faster.
It has been found empirically.
And even if you reason
about it, it makes sense.
The more randomization that you include,
the more, the faster your model trains.
Okay?
So that's our data loader.
Now we know how to get batches of data.
Next.
We need to create the model.
Now we had initialized our weights
matrix and a bias vector manually.
With randomized values and then
we had defined a model function.
But what we can do instead is use
the  linear classroom Pytorch.
So the antidote linear, what is called a
linear layer of a neural network, a teaser
for what is going to come afterwards.
A linear layer is nothing but
a weights and a bias matrix.
Bundled into this object, which can
also be used as a function, right?
So we create an indict linear, and
we give it the number of inputs
so that we have three inputs,
temperature, rainfall, humidity.
So we give it the number of inputs
and we give it two outputs of, we are
going to get two outputs out of it,
which is yield of apples and oranges.
So that's going to create
our model object for us.
And when we pass in this these numbers, it
automatically creates a weight and a bias.
So weights, matrix, and a bias matrix.
So let's check it out.
So model a lot.
Weight has the exact same shape.
As the weights matrix, we had
created two rows and three columns
and model dot bias has two elements.
So it is a vector and both of these
have requires great set to true.
So that's convenient in this one
line, we have instantiated the weights
and biases with random, values.
And they have required
scratch, set to true.
So that, so now you're not
going to forget any of this.
You just need to set an indoor
linear and an indoor cleaner is
just one form of a high touch model.
There are many other modules available.
You have an indoor convolutional,
which is what, something is something
that we're going to see later.
You can combine, you can
have a layered structure.
You're going to have a layered model,
which has multiple models inside it.
So that's why.
The model also has a parameters method,
model dot parameters, and this parameter
method is going to, it can be used for any
model to get the list of all the weights
and biases, matrices present inside it.
Now in our linear and linear
model, we just have one weights
matrix, and one bias matrix.
The same thing that we saw here.
But later on, we're going
to see how there are.
Multiple possible parameters are there.
They can be huge list of
parameters inside the a model.
Okay.
So this is going to be useful
for us to remember the model
dot parameters function.
And this model can be used to
generate predictions in the exact
same way as we had done before.
So earlier we had defined a model
function, which takes the inputs and
it multiplies it with the weight.
Transposed and add to the bias.
That's the exact same
thing we can do here.
Pass the inputs into the
model, use it as a function.
And that will give you predictions.
So here you get 16 predictions
or 15 predictions from the model.
We know everything that's going on so far.
Next, we are here at the loss function
now instead of defining the mean
squared error loss manually, we can
use the built-in function, MSE loss,
and this is present inside the torch
taught an in functional package.
The antidote functioning package
contains a lot of functions,
especially loss functions,
activation functions, and so on.
So this is another important package
and we normally imported as F so
you have define a loss function,
F dot MSE loss, and we are simply
going to use this loss function.
We are going to pass in the
predictions, which we get from
passing the inputs into the models,
and then we pass in the targets and
that is going to give us the loss.
So now this is the loss of this model.
We know what the loss is, except that
this time we've used all the inbuilt
things to create the model, create
the loss, and also represent the data.
Next.
We can now improve the model
by performing gradient descent.
And we had performed gradient descent
manually, but we can use what is
called an optimizer in Pytorch
to perform this, to perform the
update of the weights and biases.
So we are going to use
the optimizer Thomas dot.
Optym not SGD.
SGD stands for stochastic gradient
descent, which indicates that the
samples are selected in random
matters instead of a group.
So that's just the name of the algorithm,
stochastic, gradient descent, and
inside internally, it performs the exact
same thing that we have done, which is
subtracting from the weights and biases,
a small number proportional to the.
Gradient of those, a gradient of the loss
with respect to those weights and biases.
Okay.
And what we need to pass it
is our list of parameters.
So the list of the weights and
bias matrices that need to be
updated and the learning rate.
And remember this learning rate is what
we applied to reduce the gradient value,
to do something reasonable so that we
take small steps and not very big steps.
So there you go.
Now we've created the optimizer.
And now we are ready to train.
The model will follow the exact
same process as we had done before.
We need to generate new predictions,
calculate the loss, compute
gradients of the laws with
respect to the weights and biases.
It just the way it's by
subtracting a small quantity,
proportional to the gradient.
I hope you're getting bored of
this by now because I, this is, it
gets repetitive after a point, but
this is really what, all it is,
deep learning or machine learning.
And then we reset the
gradients back to zero.
And to do this, we will define a
function this time because rather
than typing it again and again, it
just got to define a utility function.
And we'll keep using this function
throughout the entire series.
So once we define it, we'll just keep
improving it, adding small things here
and there to make it better and better.
So that by the end of the same function
will be used to train the models
that you saw at the very beginning
to produce images of handwritten
digits and images of anime faces.
So we have this fit function, which
takes a certain number of eBooks.
It takes the model, it
takes the loss function.
It takes the optimizer and it breaks
the training, data loaders, all the
things that we've created so far.
And then.
For a given number of each box.
So it repeats the process again and
again, it gets the batches of data and
then it performs gradient descent, get
the predictions, calculate the loss,
compute gradients, update the parameters.
Now here is an important thing,
instead of doing tortured, no grad
opt step, et cetera, et cetera, we
can we can simply call OPD dot step
and that is going to automatically
update all of the weights and biases.
Similarly, we can say opt opt.zero
grad opt is the optimizer.
So opt.zero grad is going to update the is
going to reset the gradients back to zero.
And one last thing at
the end of every book.
So for, we go through the three batches
for each ebook and at the end of each
ebook, we are simply printing the loss
or actually at the end of every 10th
ebook, we're going to print the loss.
So let's run it.
Now let's take the fit function,
give it a number of epoxy, the model,
the loss function, the optimizer
and the training data loader.
And you can see here, we've gone
down to a very small loss of 23.
That's not bad.
Let's generate some
predictions using the model.
So this is the, these are the predictions
of the model and these are the targets.
You can see, we have 56, 57, 70,
71 81 and 81 zero one and 99.
That's pretty close.
And I don't know about you,
but I find this really amazing.
All we have done is said that there
might be a linear relationship
between temperature, rainfall,
humidity, and crop yields.
Then we said that it's going
to be a weighted average,
but we don't know the weight.
So we initialize them randomly.
Then we said, okay, let's figure
out how badly the model is doing by
calculating the mean squared error.
And then we said, okay, what can
we do to improve the weight so
that the error becomes lower.
The loss becomes lower gradients.
We can use gradients for each weight.
We can simply move down along the
gradient to reduce the loss slightly.
So we did that for each weight
and together it had a big effect.
And then we said, maybe let's repeat
that a hundred times and see what we get.
So we took a hundred small steps.
Along each of those loss curves.
And that led us to us low
loss of what is that 23.
And that led us to this model, which
gives us such accurate results.
And indeed these predictions are close
to the, targets and we've trained
a reasonably good model to predict
crop yields for apples and oranges.
So now we can go to a sixth region and
let's say we know for the sixth region
that the temperature average temperature
is going to be 75 degrees Fahrenheit.
The average rainfall is
going to be 63 millimeters.
And the average humidity is going
to be 44% relative humidity.
Then we can take that, create
an input route of it, and
then put that into a batch.
So we just create a fake batch of just one
in patrol and create a tensor out of it.
Now we know what to do
with this batch of inputs.
We simply put that batch of
inputs into the model and we
get back a batch of outputs.
So our model now predicts
that in the sixth region.
We are going to have 53.6 tons per hectare
of apples being produced and 68.5 tons
of Hector of oranges being produced.
And this is exactly what
happens in the real world.
People take data, people, train
models, and then they use those
models to make predictions.
Now here, I want to talk about
the difference between machine
learning and classical programming.
So the approach that we've taken in
this tutorial is very different from
programming, as you might know it.
And that's why it's a
different field of study.
Even though we are writing
code something is different.
Something's different.
So usually we write programs
that take some inputs.
For example, you're calculating
the square of a number.
You take a number as input, then
you perform some operations.
So the operation you performed
for square of a number is
multiply the element by itself.
And then you return the result.
And that said, that's
classical programming.
So what you do is you take, some
rules that is your program, and
then you give some input to those
rules and you get back some answers.
So the rules are to multiply
the number with itself.
The input is a number and you get
back the square of the number.
But in this notebook, what we've
done is we've defined a model.
All the model does is rather
than D tell you exactly how to
get the output from the input.
It says that the outputs and the inputs
may have a certain relationship, the yield
of apples, maybe I'm guessing the yield
of apples has some linear relationship
with the temperature in fallen humidity,
but I'm not sure what the weights for
that linear relationship should be.
So there, there is a model which defines
a certain structure or a relationship.
And some unknown parameters,
which are the weights and biases.
What we then do is we show
the model, some known inputs.
So we show the model that, Hey, when
this is the, these are when these
other temperature involve humidity,
this should be the output and so on.
So we show it the same number
of inputs, and then we train the
model to come up with good values
for these unknown parameters.
And once the model is trained, it
can be used to compute outputs for
new inputs, just just Normal program.
So in some sense, what we've done is
we've taken the data and we have taken
the answers to the data so that we
have taken the inputs and the outputs.
And we've shown that to a model.
Now model simply has a
structure or some relationship.
We've shown that to a model
using which the model has
learned some parameters and use.
And those parameters combined
with the structure of the model
is now what gives us a rule.
So in some sense, the
model is writing a program.
Now that program is not expressed using.
Statements as we know it, but
that program is expressed in terms
of numbers, in terms of weights.
And that's really all it is.
All of machine learning is about
showing some data and some answers
about those data to a model.
And there are different kinds of
models and coming up with parameters
or rules, which can then be used
to make predictions on new data.
Okay.
And this is paradigm of programming
is this word is machine learning.
And deep learning is a branch of machine
learning that takes that users matrix
operations and nonlinear activation
functions and gradient descent, and all
of these things that we are learning
about to build and train models.
And it's a really powerful technique
that has such widespread applicability
in so many different areas.
So Andre, Carpathy the
director of AI at Tesla motors.
He's written a great blog post
on this topic called software 2.2
and how artificial intelligence
machine learning and deep learning
are completely transforming how we
build software and what are all the
new possibilities that they enable.
So do check out this blog post for sure.
So keep this picture in mind.
Just let me hold onto it for a second.
Keep this picture in mind while you
go through this entire tutorial series
while you're building your own models
and see how it is different from the
programming that you've already done.
Okay.
So as a final step, we can record
a new version of the notebook
using the Jovian library.
We just call Jovi, not commit once again
on zero to linear regression, and that is
recorded a new version of the notebook.
And you can open up the notebook
on Jovian on your Jovian
profile and access it there.
One thing that you can also see as
your record multiple versions is
you can see visually what are the
differences between your versions?
Let me quickly show you
that what you can do is.
Go to the notebook page and you
can see the versions here and
there's an option to view a Def.
So you can just click view deaf.
You can see what has changed
between specific versions and
this even shows visual changes.
So if there's a change within the
graphs, you will see that as well.
All right.
We've learned linear regression
ingredient descent in this tutorial.
Now, if you stay till the end, I will
show you a couple more interesting
things at the very end of this tutorial.
So we are going to talk about the
assignment next, but at the end, I'm
going to just show you a small trick to go
from linear regression to deep learning.
And here are some resources
that you can check out.
So there's a visual and
animated explanation of gradient
descent that you can watch.
So you can click through and watch that.
There's also some, if you prefer reading
notes and mat, then there are also
some notes from a, your Udacity course.
You can watch for an
animated visualization of
how linear regression works.
You can check out this post and then
depending on how, deep you want to
get, you can also check out some
course notes from the course started
Stanford university on the exact same
topic of matrix calculus and linear
regression and gradient descent.
But to practice your skills,
you can try participating in
competitions on Kaggle website that
holds data science competitions,
but don't sweat it right now.
We will be giving you
assignments to work on.
You will be building your own models.
Don't worry about it.
And at the end, here are some
questions where if you do go through
these questions, once you've worked
through the notebook, just spend five
to 10 minutes on these questions.
There will certainly be something
where you feel a little unsure.
And these questions are for
you to test your understanding.
These are not graded, but this will help
you fill the gaps in your understanding.
If you see a question that does not
make sense, try to come up with a good
answer or try to rewatch the video,
try to scroll up and find the right
place within the notebook where you
can get the answer to this question.
So that's linear regression.
Next.
Let us come back to the lesson page here.
And if you go back to the course
page, you will see assignment one.
So I mentioned to you, you can earn
a certificate of accomplishment
by completing the scores.
So what you need to do to complete
this course is work on three
assignments and build a course project.
These three weekly assignments.
The first of these is all
about torch dot tensor.
And the objective of this assignment
is to build is for you to build a solid
understanding of  Pytorch tensors.
So what you will do in this assignment
is you will pick five interesting
functions related to pytorch.
You need to go through, you need
to go through this page here.
This talks about all the functions
available in the torch module.
A lot of these functions
apply directly to tensor.
So just pick five functions,
please don't pick the first five.
pick five interesting ones pick maybe five
related to trigonometry or linear algebra
or statistics or anything that you like.
And then there's a starter notebook here.
You can click through and
view the starter notebook.
Now, you know how to run this notebook.
You go here and click run on co-lab and
that's going to run the notebook for you.
Yeah, you run the notebook here and then
you pick five interesting functions.
For example, let's say I am picking
torch.cat So what you need to do is give
your notebook and interesting title.
If you're picking a topic, say
five interesting trigonometry
functions in Pytorch, or let's say
all about pytorch tensor operations
fight by touch nature operations.
You didn't know you needed to pick
something unique, something interesting.
Give it a nice title, give it a short,
write a short introduction about
Pytorch and the chosen functions.
The list of the functions and
then worked through this notebook.
So the first function
you talk about change.
You need to change this.
This is an example.
So change starstruck, tensor
to an actual function, add some
explanation about the function.
Then you need to give three
examples of the function.
So example one shows a simple
use case of the function.
A simple usage may be given, give
it one or two parameters, and then
show another example, includes some
explanation about the example as well.
Then show another example.
Example two.
Example two can be a little more complex,
try to introduce some variety, try to
change some of the arguments and share
some explanation about that example.
And then example three
should be a breaking example.
So here you should illustrate
when a function breaks.
So maybe give it some kind of an improper
input and that will lead to an error.
Try to produce an edit.
That's a very interesting way of learning
to produce and then figure it out.
If the editor is what you expected, then
what you should be doing is explaining.
What went wrong here.
Okay.
So two working examples and one
feeling example, and then finally
some closing comments on when
this function might be used.
So just use the way to edit it as just
double click on a cell and you can
change the explanation you can use
markdown to Mark down is very simple.
All you need to do.
If you want to create a heading is
just put a hash and start typing.
You can see here on the side.
Just Google Mark down and
you'll learn the syntax.
So that's function one.
And then after each function just
keep, committing your notebook.
Commit two zero one tensor operations.
Then go with function
two and function three.
And so on.
Keep going function five.
So you need to write five functions
and three examples for each function,
and then write a small conclusion about
what you covered in this notebook,
and then share some reference links.
Like the official documentation
for tensor operations.
If you found some other useful tutorials
online, you can share those links as well.
Feel free to share the
link of this course.
So what we're doing here is one
you're learning about  tensor
operations, which we will be using
extensively over the next few weeks.
So by just, by going through the
documentation, you will not feel lost
when we use some of these advanced
operations second year learning how to
present something that you have learned.
And presentation is a very important
part of data science and machine learning
because unlike software, where you build
a website and somebody uses the website
in machine learning, what you have to
do is present your findings to a team,
or often you have to present it to
stakeholders who may not have the same
kind of technical knowledge that you do,
product managers, executives, and users.
So it's very important for you to be able
to explain what you have done or, and
explain it in simple words, to try to
keep the word simple, try to explain it.
To somebody who does not know any of it,
make it as beginner friendly as possible.
Okay.
And at the end, you need to
run Jovian.commit And when
you run Jovian.commit that
we'll give you this link.
So now this will get saved
on your profile here.
This is my profile.
Similarly, you will see it
on your profile, then grab
the link from your profile.
And this part is important.
You need to grab the link, come
back here and paste the link here.
So you need to paste the link
here into this, make a submission
form and then click submit.
And when you click submit, a
submission will be recorded.
You'll be able to see your
submission history here.
And then over the course of the
next few days, we will be performing
the evaluations and to receive
a pass grade in this assignment.
So you will either receive
a pass or a fail grade.
Those submitted link should be a jovian
notebook that is publicly accessible.
The notebook should demonstrate
at least five tensor functions.
There should be at least two
working examples and one feeling
example for each function.
And then the Jupyter notebook should
also contain proper explanations.
So not just code and all the
cells of the notebook should be
executed showing proper outputs.
Okay.
So just make sure, just review this
evaluation criteria, especially if you
receive a field grade and then there are
some more links here that you can follow.
So that's the assignment one.
It is due in two weeks from now.
So you have some time for that, but I will
recommend doing it as soon as possible.
This should not take more than a
couple of hours, maybe a little
longer, if you want to spend some time
browsing through the documentation.
But after that, we will
also highly recommend.
And this part is optional.
This is not part of the
grading, but we highly recommend
writing a blog post on medium.
If you have not written a blog post ever,
this is a great time to write a blog post.
The reason for that is writing a
blog post gives you a nice way to.
Summarize, what you've learned into a
small, publicly accessible write up.
This is something that you can share
with you can share with your friends.
This is something that you can share
with potential employers and they will
be able to see your presentation skills.
And this is something that
will be useful for you.
When you need to revise these topics.
And a lot of other people who
are just one step behind you.
So we recommend going to medium.com.
So you can go to medium.com and sign
in to medium.com and create a post.
No, you went, when you need to
include code within your blog post,
you can actually include it by
using the embed option from Jovian.
So if you go back to a Jovian
notebook, you can simply click.
There is a embed option here.
Embed cell And you can simply copy
this link of an embed cell and
paste it within medium, and that
is going to embed the cell for you.
So within your blog post, you will be
able to see, you'll be able to see a
notebook, a piece of a notebook, one
cell from a notebook hosted on Jovian.
And this is a nice way to include
inputs and outputs within your code.
And you can click view file.
Anybody who is reading your
blog post can click view file.
To go and find the entire file,
go and see the entire code.
Okay.
So do write a blog post.
So that's one part of it.
And the next thing is to go and
share your work with the community.
So we have given this link here to
this forum page, come to this forum
page function by touch functions and
tensor operations hit the reply button
and just share the link to your blog.
Post is say this is, your blog post.
This is what it is about.
Or you can share your blog posts.
If you have written one, or you can
just share your Jupyter notebook.
And over time, we will see that
hundreds of people have shared
their work on this thread.
And what you will be able to do is look
through the blog posts and notebooks
written by other people so that you learn
not just the five pencil functions that
you have created, but you get to learn
maybe a hundred other tens of functions.
And that's a great way to learn.
It's a crowdsourced way of learning.
Somebody is doing all the hard work
of coming up with great examples and
simple explanations, and you can just
spend a minute and learn about it.
Okay.
So please make use of the community here.
Make use of all the
resources available here.
If you ask, if you have any questions,
you can ask questions on the forum.
So there is a discussion thread here you
can click on, continue discussion and
ask a question here about the assignment.
No.
So that's the assignment.
Now, one other thing I want to tell you
about is the Jovian mentorship program.
Now you can own the certificate of
accomplishment that you want for this
course that is completely free of cost.
When you do the three assignments and
the course project, you will get it.
But if you are looking for more
one-to-one guidance, then we have.
What is called the Jovian mentorship
program, where you can get
access to a private Slack group.
This is a very limited Slack
group with the course team.
And we also have weekly
office hours on zoom.
This is also part of the mentorship
program, and you can get one-on-one
guidance for your project, all
this as part of this program.
And this is a limited and paid program,
just to help you get the most out of this
course, and you can apply for it here, and
we will be reviewing some applications.
You can apply for it on Joby
not AI slash mentorship.
And we will be reviewing of
applications and getting back to people.
This is just something to keep in
mind, if you need more guidance.
So we've looked at Python basics and
gradient descent, linear regression.
In lesson two, we will be looking at
how to work with images and how to
perform logistic regression on images.
That's going to be our next topic.
Very interesting in many ways, very
similar to what we've done right now.
But the results we will produce in the
Datavail use will be very different.
Next, we will go from linear
and logistic regression.
We'll go one step forward and
create feed forward neural networks.
And as I told you in the in-between,
I will show you a quick trick to
convert your linear regression model
into a feed forward neural network,
right at the very end of this.
So stick around for that.
Then for lesson four, we will have
convolutional neural networks.
So we will learn about special types
of neural networks, which are no
longer these linear layers or these
weights multiplications of matrices.
They are slightly more special
and they are well suited
for working with image data.
So they provide many benefits
for working with image data.
We learn about convolutional
neural networks.
Next we will learn about advanced
convolutional networks called residual
networks, which perform certain.
Improvements to convolutional
networks, improve the network
architecture a little bit.
We will also use tricks like
regularization and data augmentation
to create a state of the art model.
So we will train a state of the art deep
learning model on a huge dataset with tens
of thousands of images in less than five
minutes of this lesson is pretty exciting.
And finally, lesson six is where
we will put everything that we
have learned together to create Jen
generative adversarial networks.
Where we will be training models that
can generate images of handwritten
digits or generate images of faces,
and you will be able to apply the
things you've learned in any of these
lessons to create your course project.
And you will be able to apply them
on a whole variety of datasets,
probably tens of thousands of data
sets that you can find online.
So we will actually share a list of data
sets with you from where you can find
inspiration for working on projects.
And we saw thousands of great
projects last time, and we're hoping
to see many, more this time as well.
And that is how it will go.
So what should you do next?
You need to review the lecture, video
and run the Jupyter notebook on CoLab.
You need to complete the assignment and
share your work and exchange feedback and
participate in the forum discussions and
join a study group near you if possible.
So you can just get together with five
friends and join a study group with them.
Or you can find a study group
on the community as well.
Now, as I promise, I'm going to go
back to the linear regression notebook
and show you how to take this linear
regression model that we have defined
so far and turn it into a machine
learning into a deep learning model
or a feed forward neural network.
So let me just add a few code cells.
All I'm going to say here is
I'm going to create model two
and this time, instead of.
Instead of using an indoor linear,
I'm going to use an indoor sequential
and inside an indoor sequential.
I'm going to put in an indoor linear
where we take three inputs, the
temperature, rainfall humidity, and
then we convert it into four outputs.
Why four?
Because we only have two outputs,
apples and oranges yields.
Let's convert it into four, four outputs.
Then we're going to use what is
called an activation function.
And then dot rail you
or an end or sigmoid.
And we learn about this next
week, the activation, what
is an activation function?
What does it do?
And then we are going to put in an
indoor linear once again, and this time
we're going to take these four outputs
and convert them into two out of four.
So we're going to take the
outputs of the first linear layer.
Now we're calling these layers and
we're taking the outputs, the four
outputs of the first linear layer.
And converting them into two outputs.
Okay.
And you can change this number.
This can be four and four,
five and five, three and three.
It doesn't matter.
So what this is called is this, one
layer is called a hidden layer now,
and this is called an activation
function or a non-linearity.
So now we've have matrix multiplication.
Then we have this.
Non-linearity not sure what that is.
We learn about it.
And then we have this
anecdote linear layer.
Which is the watches.
We just call the output layer.
So that's model to let's create
an optimizer for this model.
So Taj Lord or PIM dot SGD,
and we are going to give it
the parameters of the model.
So modern tool dot parameters and let's
experiment with the learning rate.
Let's say one E minus four.
Okay, so we've just
changed two lines here.
We've gone from a single and indoor
linear model to an indoor sequential,
which is saying that take the linear
output posit into segment, take the
segment output and into another linear.
And now we can use the exact same fit
function that we have used before.
So now I am going to train
it for a hundred each box.
I'm going to posit this new model to
I'm going to pass in the loss function,
the same loss function, F dot MSC loss.
I'm going to pass in the optimizer,
the new optimizer that we just
defined and the same training data.
So nothing has changed to
go from linear regression to
training a deep learning model.
It's just these three
additional lines of code.
And then we, it, and you can see
here that the model is not training.
And it seems like the model
is training slightly slowly.
So maybe we can increase the
learning rate a little bit.
And it continues to train
and get better and better.
Okay.
So now that is that's that's
your machine learning.
That's your deep learning model,
roughly speaking what that does.
And we'll discuss this in a lot more
detail over the next few weeks, but
roughly speaking, what that does.
Is it takes these three
inputs, temperature, rainfall,
humidity, and converts it into
three intermediate inputs.
So now we have performed a matrix
multiplication and gotten three
outputs, intermediate outputs out of it.
Then we take these intermediate outputs
and run them through the sigmoid function.
What does the sigmoid function do?
It's very simple function.
It takes the values that are
given to it each individual value.
So let's say you have three outputs,
so you have three intermediate outputs.
You pass them into segment and you'll get
back three more outputs, only differences,
sigmoid squishes the outputs from a
vide range into a zero to one range.
So it does that by applying
this non-linear function, if
your output is a large negative
value, it becomes close to zero.
And if your output is a large positive
value, it becomes close to one.
Now sigmoid is not the only way to do it.
You can also apply a nonlinear activation
function to do this as another loan
Inder activation is really now, it
really is a lot simpler than sigmoid.
What we do is we simply say that, okay, if
an output is positive, then we retain it.
If an output is negative, or if one of
the inputs given to you is negative.
It simply replaces it with a zero.
Okay.
So we take these intermediate outputs and
apply this non-linearity and then we would
multiply with another wage matrix and
add the biases and give, all the outputs.
Now, what has changed?
The thing that has changed is instead
of having a linear relationship between
the inputs and the outputs we have
now assumed that there is some amount
of non-linearity in the relationship.
So we have made the model more powerful.
Now our model has more parameters
and our model has these ways of
learning a non-linear relationships.
Again, we will talk about this in
more detail later, but just by doing
this, may making, this model more
powerful, introducing nonlinearity.
We have made it more likely to fit the
data better because now it can capture
not just linear relationship, but also
slightly non-linear relationships.
And as relationships get more and
more nonlinear and more and more
complex, we may need more layers
in the model and we may need more.
We may need bigger layers as well.
So we may need, instead of getting three
intermediate outputs, we may need to get
a hundred intermediate outputs and so on.
Okay.
But the way we train it as the exact
same way, which is great in descent.
And that is really what is the essence
of deep learning how to train models.
That's neural networks for you.
With that?
I will see you in the forums.
You can follow us on free code camp,
Jovian ML, or you can follow me on
The next lesson is working with
images and logistic regression.
Do subscribe to the channel
so that you get notified.
It shows up on your feed.
And if you like this tutorial do hit
that like button and leave a comment.
Thank you.
And have a good day or good night.