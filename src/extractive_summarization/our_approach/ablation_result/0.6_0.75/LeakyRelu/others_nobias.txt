You may try to convert to 32-bit float right before LeakyRely in your PyTorch code.  And maybe create an issue on the ONNX Runtime Github to add double support for LeakyRelu.
As for the idea of "maintaining a state", it refers to activation functions that would not behave independently on each and every fed-in sample, but would instead retain some learnable information (the so-called state).  Typically, for a LeakyReLU activation, you could adjust the leak parameter through training (and it would, in the documentation's terminology, be referred to as a state of this activation function).
I would suggest removing the sigmoid activation from last layer, and replace relu with LeakyRelu to improve the model's robustness. 
