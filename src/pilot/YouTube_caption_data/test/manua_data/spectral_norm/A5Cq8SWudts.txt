Sanyam Bhutani: Hey, this is
Sanyam Bhutani and you're
listening to "Chai Time Data
Science", a podcast for data
science enthusiasts, where I
interview practitioners,
researchers, and Kagglers about
their journey, experience, and
talk all things about data
science.
Hello, and welcome to another
episode of the "Chai Time Data
Science" show. In this episode,
I interviewed Jason Antic, the
creator and researcher at
DeOldify. This is the second
time I am lucky enough to be
interviewing Jason. So if you
haven't read my previous blog
interview with him, please do
check it out in the description
of this podcast. This interview
is really a continuation of
that. So I highly recommend that
you give it a few minutes, that
will definitely be worth of your
time. In this interview, we talk
all about Jason's software
engineering experience, his
software experience applied to
deep learning, his journey with
DeOldify, how he came up with
the idea of DeOldify and how he
kept on improving it. We talk
also broadly speaking a lot
about software engineering
practices and how those were
helpful to Jason while working
on DeOldify, fast.ai, and even
the top down way of learning. We
talk a lot about that as well in
this interview. We also talk a
lot about Jason's attitude which
I believe led him to inventing
DeOldify, if I may have there
always there is always a
solution to every problem, you
just need to put in the efforts
to will to find it sooner or
later. For now, here's my
conversation with Jason all
about DeOldify, software
engineering and the top down way
of learning. With a quick note
to the listeners. If you're
watching this video on YouTube,
please remember to enable the
subtitles. These aren't the auto
generated subtitles, I have
manually fixed and uploaded
them. So I hope that improves
your watching experience. The
blog version of this interview
will be published later. So if
you're excited about that, you
can find the link to the website
where it will be published in
the description of this podcast.
For now, here's the
conversation. Please enjoy the
show.
Jason Antic: Cool.
Sanyam Bhutani: Hi, everyone. I
have Jason on the show, actually
on the series for the second
time. Jason, thank you so much
for joining me on the interview
series again.
Jason Antic: Yeah, thank you. My
pleasure.
Sanyam Bhutani: It's again, a
great honor to have you on the
show. For the audience that
might have been living in an
alternate reality, I'd recommend
that you check out the previous
interview. And for the lazy
people who didn't go check that
link in the description, could
you please give a 50 feet
overview of what DeOldify is?
Jason Antic: Oh, yeah, sure. So
DeOldify was this project that
started last year, about one and
a half years ago, actually. That
was basically supposed to be
just as capstone project for
fast.aI. It was going to be
like, a few weeks now it's just
going to go ahead with this idea
I had randomly in the shower
that I could use games to make
colorization of images.
Sanyam Bhutani: Was it quite
literally a shower thought for
you?
Jason Antic: Yeah, I mean, it
was like so so I have this habit
of like, if I have a thought
like that, that I write it down
right away. So it's actually
like one out of like 15 or 20
different project ideas that I
had on list. And I chose that
one specifically because I was
like, oh, you know, if I get
this to work, it'd be really
cool. And, and, you know, I
think people would really like
it. So that's pretty much it.
And it took about like, six
weeks of, you know, so so so
that the core idea was right,
but it's just like the actually
getting it to work was the
heart, of course. And it is kind
of, it's kind of funny, I was
documenting my work on Facebook
posting images to family and
they just thought I was crazy
because the image is just awful!
Sanyam Bhutani: Hahahahah.
Jason Antic: Hehehe they look
just funny I just it's actually
like, picture after picture of
these fish over Carla colored
with all these rainbow colors,
and it just looked so awful. But
then just one day it might end
up working. And then I got
excited and posted it on GitHub
and Reddit. And that's where
went viral.
Sanyam Bhutani: Yeah. Were there
any projects for context that
were at least similar even in a
Percentage to this when you had
this idea?
Jason Antic: Yeah. I mean,
there's, there was a previous
State Of the Art. Now that been
done back in 2017. I think it
was or 2016. It was, I think it
was called colorful colorization
or something like that. But
yeah, there was a previous one,
and it was actually hosted on
algorithm. Yeah, I think it
still is the website. And so you
would, you know, upload your
pictures on there and it would
come back with something
colorful, but it wasn't nearly
as good as DeOldify. They
acually, but you know, for the
time it was pretty impressive.
Sanyam Bhutani: Got it. And I
believe it's been around an ear
and four months since DeOldify
went public so to speak, did you
anticipated getting to this
stage or like this still some
lag in your vision like it's
behind as per as your vision?
Jason Antic: Okay, well, first
of all, when I put it up on
GitHub, I really didn't expect
it to go viral to the degree it
did. I mean, the thing that made
me realize I was kind of in a
whole new role that wasn't
accustomed to, was the the
morning after I put it on
Reddit, I saw that Cory Doctoro
of boing boing, put an article
up about it. And I seen all
these people starting to talk
about it was like, oh, crap.
Sanyam Bhutani: Hehehehehe.
I recommend to the audience go
follow Jason on Twitter. He's:
@citnaj on Twitter. And it's
been a surprise for me like you
tweet these out every day. And
it might become normal
eventually on Twitter, but I'm
still surprised like how good it
has been even since when it came
out. And now it's been getting
better.
Jason Antic: Yeah, no, thanks.
Yeah, it has been getting
better. Even since the last open
source release in May. Literally
little by little I just, you
know, keep on knocking out the
different problems. It's like, I
basically characterize it as
finding the next bottleneck.
Sanyam Bhutani: Yeah.
Jason Antic: Yeah.
Sanyam Bhutani: So can we talk
about the evolution of the
project? Was there any point
which was like a huge
improvement, any major stages of
evolution that you want to point
out?
Jason Antic: Um, yeah, uh, so
the initial release was, like
the GAN based colorization,
which was a pretty big
improvement over State Of the
Art previously chiefly because
it's very colorful and it was
pretty but you know, it had a
lot of problems. They had a
whole bunch of glitches and you
had to like you really had to
cherry pick like one out of 100
pictures would actually look,
maybe decent. I've been looking
at these you know lately at the
old old model I'm like oh my god
that looks terrible now. So the
next release in between that and
next release is the, the big
priority in my mind was to work
on this glitches and stuff like
that so, so that relates to the
NoGan work that Jeremy and
Sylvain and I worked on for this
course. So what so so we worked
on that that was based on like
my DeOldify, we kind of spawned
from there and basically, what
Jeremy wanted to do for that
course is to introduce GAN work
that I had done into lesson
seven. And we're trying to make
it you know, in a fast.ai sort
of flavor which is that it, you
can train it fairly fast. Right?
And it's practical.
Sanyam Bhutani: Yep.
Jason Antic: Because, you know,
GANs in particular, we're using
up a lot of hardware.
Sanyam Bhutani: Yeah.
Jason Antic: You know, at that
time. And so typically, it
seemed like they were pretty out
of reach for, you know,
individual practitioners, that
stay on their desktop. And so,
you know, I was thinking about
Jeremy's ambition here, and I
was like, well, you know, if we
really want to make this
practical, we can get Transfer
Learning to work. This, that
would make this a lot more
practical and easier to train
and train a lot faster. And it
might even look better.
Sanyam Bhutani: Yep.
Jason Antic: So that's where
NoGAN was hatched. And Jeremy
loved the idea. And for the next
two weeks, two weeks prior to
this course, we just bounced,
and him and I and Sylvain were
bouncing these ideas back and
forth working rapid fire. It was
really exhilarating. And we want
to making it work. You know, I
mean, I wouldn't say look
perfect, but what we presented
to the course was pretty cool.
From that, I saw the potential
of taking the GAN as it did and
getting a handle on the glitches
and the quality. Because this
guy seeing that and saying,
well, you know, I think it seems
like the longer you train with
GANs, the worse these glitches
get. So maybe the answer is to
avoid GAN Training as much.
Sanyam Bhutani: And this is
counter intuitive when you start
out because you'd expect the
opposite based on what at least
you read in books or what you
read in these blogs that out
there.
Jason Antic: Yeah, yeah. Because
up until that point, my thought
and I think this is what most
people assumed was that GANs
were the answer, you know, to
getting more realistic and
higher quality images. But yeah,
my thinking with this evolved to
GANs are a tool to get there,
but they also are a double edged
sword.
Sanyam Bhutani: Yeah.
Jason Antic: You know, I mean,
people were recognizing that
they were difficult to train and
stuff like that, but to, you
know, figure out a way to
actually make that work for you,
to get the benefits without the
the difficulties was real
challenge. You know, I think
most most people at the time
were trying to tackle that
problem from the standpoint of
like, stability, and GAN
training, which is still a, you
know, a big challenge. So like
that. So I was trying to take
it, you know from the other
another angle, which is well,
how do you use what we have
already in a more effective way?
You know, I really like thinking
about problems in a way.
Sanyam Bhutani: Yeah, we'll talk
more about NoGAN in a bit, but I
want to focus zoom out and focus
on another theme, which is
learning to learn now, in your
previous interview, you
mentioned that after a few
follow ups with a few online
courses, you decided to invest
your time into first.ai, you
took some time off from work.
Can you tell us what approaches
and methods did you learn and
experiment while you were like
doing this course by yourself? I
remember you took the course
online and not in person.
Jason Antic: Yeah, online. Okay.
So I want to give you a little
bit of a background on this
because I think this is really
probably the most important
aspect of it. I was interested
in deep learning for many years,
like ever since, like 2012, when
AlexNet, took over image net and
just blew away the competition.
And I knew from that point on, I
was like, you know, this, this
is going to be a big deal. So,
you know, I wanted to get in on
that ever since. And I started
to try and taking online courses
around 2015 and, you know, I
tried some of the really
reputable courses taught by
reputable people and I just
couldn't hack it.
Sanyam Bhutani: Hehe.
Jason Antic: Haha. So I tell
people I you know, I rage quite
a few times on these for various
reasons. I won't go you know,
I'm not going to trash any one
course here, but I'll just say
that they were not very
effective, for me.
Sanyam Bhutani: Not very
practical.
Jason Antic: Yeah, yeah. Yeah,
exactly. And you know, a lot of
them suffer from the same thing
that fast.ai tends to stand
against, which is things like
elementitis, where you're
learning, you know, all these
prerequisites before you get to
the actual thing you want to do,
for example. So I finally came
across fast.ai few years ago,
the version one course.
Sanyam Bhutani: The first
version? Original one?
Jason Antic: Yeah, the first
version.
Sanyam Bhutani: Okay.
Jason Antic: And I loved it. And
I got quite a ways through but
the only problem I had at that
point was is that you still
needed to take a lot of time
with it to really do it, right.
Yes, I'm taking a bunch of
vacation days to do it from
work, you know, and I and after
I ran out of those, I was like,
okay, this is not sustainable. I
need to I need to figure out a
better way to do this. So that
was my only beef with it. I
mean, I was like, wow, this is
so great. I just wish I could
spend more time with it.
Sanyam Bhutani: Yeah.
Jason Antic: So that's, that's
what I did the following year. I
saved up, saved up a bit of
money and then negotiated a part
time, hours at work, and then
really sunk in a lot of time
into fast.ai version two, and
did that over the course of
three months in the summer. And
so, you know, when you asked
about learning to learn, that
was a huge component of it, just
simply setting aside the proper
amount of time to do it.
Sanyam Bhutani: Yeah.
Jason Antic: You know, and to
allow for experimentation and,
you know, learning the things
the right way before you move
on. You know, that I think
that's something that it's a
little less convenient, but
that's really what needs to be
done a lot of time-boils down to
that. Yeah. You know, As far, as
far as everything else about
learning, I mean, that fast.ai
really lead you on that like,
like, they really have a great
philosophy of, you know, what it
would, what it means to learn
effectively. So that you know,
as you know, they do the top
down approach where you do that,
you know, the very first thing
you do. Day one is a classifier
of cats and well used to be a
cats and dogs now. It's like dog
breeds Yeah. So that's just
really motivating. You know,
from you know, personal
experiences, just, you're like,
wow, and you already feel
confident from day one that you
could expand upon these ideas,
and do many, many other things.
It's just a great feeling. It's
just so different from previous
experiences.
Sanyam Bhutani: Yeah, the thing
I feel like and I've been
involved in a few study groups,
I feel like all of the fast.ai
students get confused of how
much depth to actually go in,
because top down is something we
totally unfamiliar with. So you
might see something cool
happening on Twitter, then
you've just done lecture one,
but you want to go off course
and start following that. So how
do you avoid that? Or how do you
focus on, how to decide on what
to focus on?
Jason Antic: Well, okay, so
that's a very interesting
question, because I feel like
that's the real advantage of
setting aside a whole bunch of
extra time is that you can go
down those rabbit holes, because
I think you should, honestly, if
it's, if it's interesting to
you, that is probably going to
be the best motivator for you to
learn. Because, you know, this
stuff's a very broad field. I
mean, there's, I mean, fast.ai
covers a whole big range of, you
know, what's in deep learning,
you know.
Sanyam Bhutani: Yes.
Jason Antic: You know, the
vision, the tabular, the NLP
stuff. And I would contend that
you probably can't become an
expert on everything. It
probably pays to know, that
stuff, you know, to a certain
extent for you know, I like to
term it cross pollination of
ideas, you know, to take ideas
from computer vision and
applying to NLP like transform
learning.
Sanyam Bhutani: Then to GANs.
Hehe.
Jason Antic: Yeah. So but
ultimately, ultimately, you're
going to have to pick and find
something that you love. And if
that the first thing you see in
the course, I'd say go for it.
So again, that's why I think
setting aside a lot of time for
it's very important to not rush
it, but rather explore it.
Sanyam Bhutani: Yeah, I think
that answers the project aspect
of it. I believe that's how
DeOldify start, but what about
the other student, half of the
students of fast.ai that want to
learn another thing that's not
being yet taught in the lecture
that's not covered, how do we
like decide how much time to
spend on it because that's, that
takes you to one course from the
other and it becomes an infinite
learning loop.
Jason Antic: Yeah, you can go
crazy. I mean, like, yeah, if
you're not if you're totally not
disciplined about it, then that
could be a problem too for sure.
Yeah.
Sanyam Bhutani: Do you have any
tricks? Or like, was it a
natural flow for you? You didn't
get stuck in any rabbit holes
there?
Jason Antic: Now, luckily, I
didn't I didn't get stuck in a
rabbit hole until I found
DeOldify, hehehe.
Sanyam Bhutani: Hehehe.
Jason Antic: Which I'm okay with
me. It paid off, right?
Sanyam Bhutani: Yeah. Certainly.
Jason Antic: Yeah.
Sanyam Bhutani: Okay, now coming
back to DeOldify, for little
context again, can you tell
what's challenging about
DeOldify. You've been continuing
on working on it for quite a
while. How has the problem not
been, quote unquote, solved yet?
Jason Antic: Oh, yeah. I mean,
the way I see the problem,
phrased in papers, as they call
it, it unconstrained problem.
Meaning that, you know, for
example, an article of clothing
can be many different colors in,
you know, if you think about it,
you're not you're not going to
be able to train a model no
matter how big and sophisticated
it is, to memorize and know,
every single thing in the world,
like what color supposed to be,
especially, like, articles of
clothing and cars, and, you
know, all that stuff, so;
Sanyam Bhutani: There's no right
ground truth.
Jason Antic: Yeah, yeah. I mean,
to a certain extent there is,
you know, depends on what you're
talking about, like grass and
sky and;
Sanyam Bhutani: Skin.
Jason Antic: And skin, skin and
the specific types of flowers
and stuff like that. So, not
completely true. That's, it's
100% unconstrained, but it's,
there's a lot of wiggle room.
And so it's challenging,
particularly challenging for a
model in terms of like, not only
is it impossible, to get all
that data, but also to get a
model that will still
confidently choose one or the
other not, like, give you a
brown for everything because
it's average, which is, what was
one of the major problems I had
to tackle with this stuff is,
you know, the tendency would be
to cheat, you know, and just
say, okay, well, I give up.
Everything's brown or gray.
That's basically what your
network will do if you're not
careful.
Sanyam Bhutani: Yeah. Now, can
you tell us how did you find
ideas for improving like once
the first version which was
relatively stable, how did you
find ideas to improve? Did you
go through research papers?
Because I remember you mentioned
that you're not very good with
math.
Jason Antic: I just talked about
that yesterday. Yeah. So yeah,
what I was saying yesterday that
in that Twitter thread;
Sanyam Bhutani: Tweet thread.
Yep, I'll have that linked in.
Jason Antic: It's funny. So, you
know, basically what we're
saying there is that I'm not
particularly gifted with math,
you know, it's, it's a struggle
like I, you know, I can read
this stuff I and I don't want to
be I don't want, you know, I
don't wanna give the wrong
impression I was math minor. I
took the courses like linear
algebra and all the calculus
courses that you can take. But,
you know, I kind of more
survived the courses than
actually really, truly
understood a lot of it, you
know, so you, you remember a
whole bunch for the test, and
then you forget it, that sort of
thing.
Sanyam Bhutani: Yeah.
Jason Antic: As opposed to the
coding said the coding stuff was
like, you know, a lot more
intuitive to me. So, still, to
this day, it's just, you know, I
think of the math and papers as
like a bad programming language.
It's really hard to read.
Sanyam Bhutani: Hehehehehe.
Jason Antic: I just had to, you
know, read it on my breath, one,
you know, one character at a
time and then go again. And
hopefully, my hope is that I get
at least some intuitive sense of
what's going on there. And then
I can run with that little
intuitive building block in my
head, and then play with it from
there, you know. So that's
basically how I approach papers
is, you know, I really hope that
they have the code cause that's
really what I want.
Sanyam Bhutani: Yeah.
Jason Antic: I try to get the
core idea of what they're doing.
And I tried to simplify it from
there. That's one of the first
things I try to do. And I've had
a lot of luck with that.
Sanyam Bhutani: For approaching
research papers, do you just go
through the equations real quick
and approach the code and try to
pick, cherry pick ideas from
there?
Jason Antic: Yeah, cherry pick.
Yeah, I think that's a good way
to put it. Um, you know, I do
think a lot of times initial
efforts tend to overcomplicate,
so I don't try to read too much
into what a particular paper
does even if it is
groundbreaking. I'm like, hmm,
there's, you know, probably a
way to do this in a simpler way.
So I'm trying to get those core
concepts from the paper, I'm not
necessarily trying to replicate
what they're doing. So I never
never really am what I'm really
trying to do is like you know,
cherry pick and see is this
useful is this useful, is this
useful and then Frankenstein
into my end work.
Sanyam Bhutani: Again, the top
down approach where you take an
idea, see if it works and not
really look behind the curtain,
but if it works, you try to
integrate into.
Jason Antic: Yeah, yeah. That I
feel like that's a much more
efficient way to operate,
honestly. Yeah. Like, there's a
number of reasons why I like
that approach better. You know,
for one, I think you have to
leave by experiment and and, and
empirical evidence, because
that's your ultimate truth. And
that's, if you don't do that
you'll be led astray by clever
ideas that don't actually pan
out in reality, right. And that
happens all the time, honestly.
I mean, it's really kind of
funny if you think about it,
like, batch norm, right, you
know, batch norm is used all
over the place everywhere, if
learning when it came out, just
like any other deep learning
paper, add a whole bunch of math
that looked really impressive
and laid out why and how it
worked, right?
Sanyam Bhutani: Yeah.
Jason Antic: But then you may
know what there's another paper
that came out and said, oh, wait
a minute. You know, batch form
works. You know, we know that
but it doesn't work the way you
thought it did. right.
Sanyam Bhutani: Yeah.
Jason Antic: Kind of funny,
right? Like, because he had all
this, you know, really
impressive looking math,
spelling out how it works, you
know, but yet the core concept
was wrong. Right?
Sanyam Bhutani: Yep.
Jason Antic: So that's how I
look at this stuff. It's like, I
find that, I find that to be a
particularly striking example
because it's like so many people
are looking at the same thing.
Sanyam Bhutani: Yep.
Jason Antic: And, you know, and
using it, right. Yeah, so, that
blew me away, honestly. Um, but
I think that happens quite a
bit. Um, so I, I'm not trying to
be an intellectual here, per se,
but she has, I feel like if you
get too wrapped up in clever
ideas, it can actually be a
trap. Because you get convinced
of the correctness by how
impressive the theorem is or you
know, something like that as
opposed to, again, your ultimate
truth is does it work or not?
Sanyam Bhutani: Yeah.
Jason Antic: Right.
Sanyam Bhutani: Yeah.
So again, like going back, you
had this decade long experience
of software engineering. Did you
find that helpful? Again, while
doing fast.si and while
approaching the project, this
question to credit is by Vrinda
Prabhu from the AMA section.
Jason Antic: Yeah, software,
software engineering, I think is
really helpful for the shield
actually. And, but I have a bit
of a different perspective on
it, I think. So I, as a software
engineer, I I've used software
engineering as problem solving,
distilled in a complex problem
solving distilled like, like, if
you really want to get good at
it, that's what you focus on.
Because I think there's a lot of
people that want to focus on
like, you know, the syntax of
particular language or like, you
know, what is this framework do
or this framework do? And don't
get me wrong, it, that stuff is
helpful, right? You can, you
know, it is helpful to have that
stuff on on the top of your
mental stack to be able to be
efficient at putting together
new things. But you know, the
the real, the thing that I think
truly distinguishes people that
are effective at software
development and people who are
not, are are the people that
don't bury themselves in the
messes that they make, like,
because you can, you can make
things you can make just about
anything. Way overly complex
very quickly.
Sanyam Bhutani: Yeah.
Jason Antic: And the problem
with that, even, it doesn't
matter how smart you are. We all
have very limited mental
capacity. We have very limited
short term memory for example,
you know, you know, like they, I
think it's like, what 6 to 9
digits or something like that,
that you can remember that
that's why phone numbers are
like that length. So we have
very severe mental limitations
that you have to work with, and
acknowledge, you know, and if
you if you learn the techniques
to work with those limitations
effectively, then you can do a
lot more than you would
otherwise. And that's;
Sanyam Bhutani: The challenge
also comes in because making it
complex might sound cool on
Reddit or in your own hacker
circle where you have these
nerds, but it probably won't
sail outside of that.
Jason Antic: Yeah. Yeah, I mean,
you're kind of dead ending
yourself. Something that is
clever, but it doesn't actually,
like you know, turn into
something very useful. Yeah, so
that's basically what the job of
a software engineer is, in my
mind, if you really look at it
is conquering mount complexity.
And in fact, that's, that's the
I, I help devise our software
engineering tests at work. And
that's how we that's how I
designed it was that it would
try to distinguish you, who
could tackle complexity
successfully, and who couldn't,
as opposed to like fizzbuzz or
you know, something like that
the simple tests that really
just, you know, like my CTO
likes to say is testing whether
or not you can fog a spoon
Hehehe.
Sanyam Bhutani: Okay. Hehe.
Jason Antic: It's kind of funny
that way. So, um, I think that
translates directly to ML,
right. And machine learning with
these deep learning models, we
just keep getting more and more
complexity piled on. And I think
it's very easy to get lost in
that. And to not seeing the
forest for the trees, right like
that to, to not get the big
picture of what you're doing. So
introducing that discipline into
what I've been doing and just,
you know, ruthlessly trying to
simplify over and over and over
again and taking papers that I
read and tried to simplify over
and over again and just keep on
working at that iteration. That
has been the thing that I think
has been a real advantage for me
as a software engineer coming
into this field.
Sanyam Bhutani: That's what I
think Jeremy also tries to
propagate through especially the
second part of the course we
actually gives you a walkthrough
of how they iterate over the
code. How do we re factor it
constantly, and he's;
Jason Antic: Oh, yeah, he's a
big fan of refactoring. Yeah.
Sanyam Bhutani: Now, coming to
the code base, I was going, I
went to the code base, and I was
expecting to have a huge code
base since the last time we
checked during the previous
interview, but I counted the
lines, it's close to 1.5
thousand lines, which is not a
lot. Could you give us a
walkthrough of the code base?
And how is it source more even
given the like, amazing results?
Jason Antic: What I can tell you
there's actually some
superfluous code in there too. I
haven't deleted yet. Actually, I
yeah, that's a funny thing. I
kind of feel bad about that
code. Because like, I didn't go
through, go through like I
normally do and refactor it like
ruthlessly. Yeah, it's kind of
yeah, I kind of I was kind of
sloppy about it because I didn't
like a little too excited to get
to the next thing on DeOldify.
Sanyam Bhutani: Yeah.
Jason Antic: So I haven't taken
the time to slow down which is
which I do think is a mistake
actually. So yeah, I mean, the
code base is very simple. I
mean, it's just, you know, so
first of all is built on
fast.ai, that's doing a lot of
the heavy lifting, you know,
fast.ai in Pytorch, fast.ai
provides, you know, a lot of the
plumbing to, you know, do all
the training and you know, as
the code to read the data, and
to, augmentations are a big
deal. So, you know, fast.ai
provides all that plumbing. That
being said, you know, what I
added was, you know, a
customized or dynamic unit. So,
that's, it's a slight, slightly
different variation of that and
now now keep mind at to remember
what I did for that versus what
I'm working on now. Okay, so I'm
going to try to be accurate
here. So there's some constant
code around the unit, which is
the generator and the critic.
But it doesn't deviate too much.
And then the last function is a
Feature loss I added that. And
let's see what else, augment, I
added some augmentations to like
simulate film green, you know,
with basic geology and noise and
poison noise.
Sanyam Bhutani: Are you familiar
with these "Mathy Terms" before
you start the project, or did
you find them out after working
on it?
Jason Antic: The mathy sorry,
the math [?]
Sanyam Bhutani: The Mathy-Terms,
Gaussian noise etc. Again.
Jason Antic: Oh, sorry. Oh,
yeah, yeah, yeah. Nice familiar
with that stuff before.
Abhishek Thakur: Okay.
Jason Antic: Um, yeah. Actually,
that was that was actually
something interesting. Like I
that using Gaussian noise. I
wasn't sure about because that's
like, yeah, it seems like a very
loose approximate approximation
to what you know, Phil noises
and film green. So I wasn't sure
if it was gonna work. But you
know, it's one of those things,
it's like, you just don't know
until you try. And so I
experimented with it and
actually did work pretty well.
Again, I just like to say that
because like, that's a really
good example in my mind of like,
leading, leading by experiment,
as opposed to getting trapped in
the over and over analysis as to
why, why something should or
should not work.
Sanyam Bhutani: I think, again,
to quote an example, you might
get stuck in the loop of going
through transformers, for
example, if you're working on
NLP, and if you have an
intuitive idea that might get
buried in case you again stuck
in that loop, for example.
Jason Antic: Yeah, right. Yeah.
So I think it's actually a
really good discipline in
general to kind of assume that
you're probably wrong in some
way. And like, assume that
you're missing something you
know, I mean, I don't get me
wrong I I'm saying, what I'm
saying is, is that it's two
error is human(?), he is human
basically. I hope I said that
right.
Sanyam Bhutani: Hehehehe.
Jason Antic: And chances are
especially when you're dealing
with new things is that you're
going to be missing something
big. So that's another reason
why I like to lead with
experiments because you know, no
matter how clever an idea is
that or how airtight you think
it is. There is a very real
possibility this is flushed out
by a ton of evidence out there
throughout history, that we are
going to be wrong.
Sanyam Bhutani: Yeah.
Jason Antic: Hehe. And again,
it's no matter how clever you
are. You know, I think it very
easy mistake to make where you
can make like a sort of airtight
proof in your head about why
something should work. But the
key fatal fatal flaw in it is
the underlying assumptions
behind it.
Sanyam Bhutani: Yeah.
Jason Antic: And that those are
actually the things that we
usually get wrong. Yeah, and
again, that's, you know, the
underlying assumptions are the
things that you find out by
actually running it.
Sanyam Bhutani: Now, I want to
again, point out this fact that
I'd like to mention you as,
quote you as the Edison of GANs
where you failed all of these
thousand plus experiments, I
believe, how did you document
your approach for me, like if,
if I fail an experiment, just rm
rf, I'm not doing this anymore.
I'm done with this. I probably
just shut down my computer. How
do not give up?
Jason Antic: Well okay, the way
I document it, and I don't
recommend this is, I have a file
called to dot text. I have it in
Dropbox that that part is good.
So at least it's backed up. And
so to do dot text was originally
my to do list, but then I just
started piling a whole bunch of
notes and who knows what
doesn't, and it's really long
now and it has like, I don't
even know what's in there
anymore. I just keep putting it
in. Hehehe. And, and so I keep
procrastinating and thinking,
okay, one day I'm going to
actually organize this into
something useful. Now, I laugh
but like the thing is, is that
that is actually kind of hurting
me because I know I've at least
a few times I've repeated an
experiment, like in some way
like I did actually figure out
something already and then
actually dry run in and I might
go Yeah, I already knew that I'm
really sloppy. Hehehhehe.
Sanyam Bhutani: Okay.
Jason Antic: So a lot of it. I
mean, I am writing it down, you
have to, but I've been really
keeping it in my head.
Sanyam Bhutani: Also like now,
it's amazing, DeOldify's
amazing. We're all fans of it,
but how did you not give up?
Like, I don't think of any
examples that were as good. So
you, I think you might have
question if, at all it's
possible or not how did you not
give up when the experiments
were not working at all?
Jason Antic: Um, so this is
actually where my experiences
software engineer really came in
handy. So over the years, and
this this is something I
gradually developed, is not
something that I always had. But
I gradually develop this
conviction that there, there's a
solution out there waiting to be
found. And that problem solving
a lot of times really boils down
to being a search problem. So
like one of the very, very first
things I tell people, new
engineers coming in, at work is
that, you know, before you come
up with a theory of why
something isn't working, when
you're debugging, for example,
narrow down the time and space
first. Just do something I said,
don't, don't even begin worrying
about coming up with a theory
yet. Collect the evidence first.
So that's, that's how I approach
a lot of this stuff is like, I
really do think the solution is
out there. And, you know, it's
not it's not unreasonable to
think that means like, you know,
even before I started to work on
this stuff, like, you know, as
you mentioned, there were there
were rather deep learning models
for colorization they weren't as
good. Right? So, you know, the
proof of concept was out there.
It's like, you know, that's
where things were going. Right.
And GANs were doing awesome
stuff outside of that, you know,
already, you know, already at
that point progressive GANs from
NVIDIA were out making really
impressive looking faces.
Sanyam Bhutani: Yeah.
Jason Antic: So it wasn't really
a big mental leap. You know,
the, the challenges is, is yeah,
fighting, giving up, you know,
and, and I would even go as far
as to say, fighting self doubt,
you know, like, thinking that
you aren't smart enough to find
it. I think really, that's the
real thing that you had to fight
more anything else.
Sanyam Bhutani: Yeah. And I
think it's, again, a learning
experience as well. Like you
have to keep getting better.
While example for you, solving
any problem. So you have to also
fight the self doubt while also
improving your skills, trying to
solve that problem.
Jason Antic: Yeah, for sure.
It's about learning along the
way. And you don't know what
you're gonna have to learn until
you get there.
Sanyam Bhutani: Yeah.
Jason Antic: That's the other
thing,
Sanyam Bhutani: You also got to
collaborate with Jeremy and
Sylvain, on working on NoGANs.
Can you tell us about that
research experience? And how was
the collaboration experience?
Jason Antic: So yeah, to give a
little context on that. Jeremy
and Sylvain and Rachel, were my
three heroes in the field, you
know, like, so when Jeremy
contacted me, after I put out my
DeOldify work, I was ecstatic.
You know, and there's like a
dream come true. I mean, it
really was. So yeah, he
contacted me about wanting to
incorporate some of the work in
DeOldify somehow into his lesson
seven, part two. Part One,
sorry. And we were to do;
Sanyam Bhutani: For the, sorry
for the audience. There's
multiple versions of the course
and;
Jason Antic: Oh yeah.
Sanyam Bhutani: Multi version of
the library. Now, what Jason
just referred to is version one
of the library and version three
of the course.
Jason Antic: Yeah, sorry. Oh,
yeah, you're right. I'm sorry.
I'm so sorry. Yeah, it's
confusing. Anyway, yeah. So we
were to do this work and like
two weeks it was kind of is
really crazy ambitious in
hindsight, because you know,
like, what, you know, like I
said earlier, like, what we're
trying to do is make GANs
practical for the fast.ai
audience you know, where they
can run it on their desktop and
train it in a few hours and get
really cool results on in this
case, super resolution right? So
yeah, that's naturally, you
know, that's where I went back
to what Jeremy and I had been
teaching this entire time, which
is, you know, the virtues of
transfer learning, like that's,
you know, almost almost
everything seemed to be improved
by that wherever you you added
that. So that's where I
suggested them, like, you know,
I think that make this effort
practical, you know, to make a
train in a few hours as opposed
to a few days, we should try to
do transfer learning for GANs.
Sanyam Bhutani: Yeah.
Jason Antic: And mind you, and
that really wasn't being done at
that point. For whatever reason,
I don't know. I don't know why.
So that's what we tried to
create in two weeks and we
actually succeeded and it was
really fun. Like, I'd never had
this experience where just, you
know, you're working with
somebody you just, and all these
ideas are just flying and they
just click and just, it just
flowed so well. It just
describes wonderful feeling. And
it was crazy, too. I like I
don't know how many people know
this. But the last bit of code
that was committed for that
notebook that Jeremy used that
night for that, that portion. I
committed that partly because I
made a really dumb mistake, like
four hours before he taught the
course. Hehehhehe.
Sanyam Bhutani: Hehehhe.
Jason Antic: Which I thought was
really funny. But it all worked
out in the end.
Sanyam Bhutani: I do remember,
we so the people who were live
audience, we used to our chart
and I saw your icon in the
commit thread and I mentioned
hey Jason's on the lesson.
Jason Antic: Hehehe, so you saw
yeah hehe.
Sanyam Bhutani: Hehehe, yeah.
Could you tell us more about how
you collaborated about the ideas
or experiments, how were those
discussed tried tracked or
worked upon during that
duration?
Jason Antic: Oh man you know
honestly it was such a blur,
like we, yeah we just we
basically did what you know I'm
used to doing which is leading
by experiment does this work
does this work? No no no you
know you know one of the things
that we want to do is go with
WeightNorm instead of spectral
norm. But again, that was just
experiment after experiment.
That's all boil down to just and
yeah, I don't know what else to
say to that. Sorry.
Sanyam Bhutani: Now coming to
the video results of  again,
DeOldify, you hide, you were
kind enough to share with them
with me way before it went
public. But can you tell us how
did DeOldify get better enough
to with continuous images,
videos, as we call it.
Jason Antic: Yeah, that was
really interesting problem.
Because I think that's one of
those things is one of those
problems that people thought was
a lot harder than it actually
was. I think this is a really
good case of here, I don't know
for lack of a better term
thinking outside the box really
comes in handy with problem
solving. And that you question
basic assumptions and the basic
assumption there was that you
needed temporal modeling for, to
do effective videos, right? That
get rid of the flicker and stuff
like that. Now granted, DeOldify
still had some problems with
with, you know, some flicker to
a certain extent, but I would
argue it's watchable. Right,
like, like, it's a pretty good
effect, right? And I'm pretty
happy with it. But the key
insight there was that, so this
guy Robert Bell from Arizona
randomly contacted me with some,
back in January with some videos
you did with the original
DeOldify.
Sanyam Bhutani: Okay.
Jason Antic: Where where he
colorize him frame by frame and
they look really bad, hehe, but
he was happy with them. He's
like I cuz I cuz he saw you know
the potential in them I think
and I was like and I did share
his excitement with that to a
certain extent but I was like,
yeah, it looks pretty bad like
Sanyam Bhutani: In hindsight I
think it might look bad now?
what I;
Jason Antic: Yeah, is it because
because it had these glitches,
right like the glitches that I
was very used to, you know where
things get these weird red
highlights and stuff and but
when I was looking at it closer
so this is like a really good
example actually of like just,
you know, paying attention to
the evidence right, and, and
paying attention to the
unexpected. So I was looking at
what was actually going on a
little bit closer. I was like,
well, you know, the underlying
images, if you get rid of the
glitches are actually really
stable. Right? It's just these,
this mess that I gotta clean up.
And I'm pretty sure this is
coming 100% from the GANs
training portion and just turned
out a month previously, you
know, we had worked on the, what
became known as NoGANs , right,
the pre training stuff. So, I
just put two and two together
there and I was like, ha, yeah,
you know, you could probably get
rid of these glitches and make
decent video and DeOldify, by
using this GANs pre training
process. So basically it is your
boy games as much as possible
and you're in, you're getting
the benefits without problems.
So that's where it came from. So
that's those line of thinking
that that started it. Now,
figure out how to actually make
it work took a while longer. But
again, it was a, it was a matter
of paying attention to the
evidence and experimenting, and
in this case, so so I, in
hindsight, I was kind of doing
something a little bit crazy,
which is I was doing the GANs
training on pretty small batch
sizes. You know, for those
running it on a desktop, a very
humble normal desktop with this,
you know, NVIDIA.
Sanyam Bhutani: Single GPU?
Jason Antic: Yeah. So you know,
that introduces additional
problems, but I noticed over
time that the the results I was
getting to this trading were a
little bit crazy in that
sometimes, you know, if you ran
it to one point it was good. And
then if you're around a little
bit longer is bad. And if you
ran a little longer it's good
again, like the sine wave,
right?
Sanyam Bhutani: The loss doesn't
make sense for a GANs most of
the time so you don't have any
early stopping you can't
manually inspect the loss curve
so to speak.
Jason Antic: Well, that's what
I've seen. Yeah, that and I was
driving that that's what was
driving me nuts. I was like, I
can't make heads or tails in
this. And you know, it's very
tempting when you see stuff like
that, that the think oh, I this
is beyond my understanding. So I
hope someone else figures this
out eventually. But I decided to
one day just look at it really
closely at fine, fine detail
like okay, where exactly is the
point where glitches are
actually introduced.
Sanyam Bhutani: Okay.
Jason Antic: So it's actually
taking the model snapshots at
like, the nearest like 55th
interval or something like that.
Sanyam Bhutani: Makes sense.
Jason Antic: And like very
small. And I was doing it early
on. And I was looking at the
images and and lo and behold,
there was a specific point where
all sudden it got way worse, but
before that, it was just getting
better and better and better. So
that's;
Sanyam Bhutani: That's where it
"overfit"
Jason Antic: Yeah, so that's,
that's where I was like, ah,
there's something useful here.
There's a small window of
opportunity where you are
getting benefits and then all
sudden it just starts occurring
all this garbage. Now, I didn't
have, I didn't have the
resources to confirm if, you
know, increasing the batch size
would mitigate it and that it
does. Which, which makes sense,
right. But you know what i found
exciting was that even in this
very limited resource regime,
you can still find a way to make
this stuff work. Right?
Sanyam Bhutani: Makes sense.
Jason Antic: Yeah, that's where
I came from.
Sanyam Bhutani: Got it. Do you
have any suggestions for people
who are looking to improve this?
Do you think there are any
aspects of the top of your head
that can be improved in the near
future?
Jason Antic: Oh, sure. It's not
perfect. So as long as it's not
perfect, it can be improved. But
the lack of perfection
specifically. I always want them
to be more colorful, you know, I
really want to get away from
everything being blue, and pink
and purple, and red. I've gotten
away from that, to a certain
extent. I'm really happy with
the latest results I have with
this new model working on but
you know, it's still there. So,
that's the, that's going to be a
challenge for quite a while
because like I said, it's a you
know, very unconstrained
problem, especially with
clothing and stuff. Something
that I pretty much believe or
not, I feel like I've solved is
making it reliable in terms of
like, you don't have to fiddle
around with render settings. I
called it the render factor for
DeOldify where, you know, you'd
have to render a different
resolution to get the best
result. That's something I've
been working really hard to make
obsolete. And I've actually
succeeded in that. I was, I
would say, that's what's going
into the commercial work,
looking and doing. Yeah.
Sanyam Bhutani: Got it. I'd also
like to, just to mention it for
the audience. Can you tell us
the hardware where you created
DeOldify? And has it changed
since then?
Jason Antic: Same hardware.
Sanyam Bhutani: Yeah, I;
Jason Antic: Just to be clear,
you can try this stuff on a
1080Ti And that's generally,
even though I actually have four
GPUs by use them to run four
different experiments.
Generally, I like to keep it
that way.
Sanyam Bhutani: But all of this
was created on a single GPU. So
for a guy who's worried about
DGX, for example, they can still
do this.
Jason Antic: Oh, yeah, yeah.
Yeah, that's kind of the point
of until I yeah, I really,
again, I really liked Jeremy's
vision of fast.ai that you're
democratizing deep learning. So
I like to keep this stuff, you
know, running on a gaming PC
level of hardware.
Sanyam Bhutani: Yeah. Now for
someone who hasn't had their app
idea yet, if I may, what advice
or any suggestions if you could
maybe say a few from your;
Jason Antic: It's kind of funny.
So so so, the preface is, like,
ideas are definitely cheap.
Right? Like, like, you know, I
used to say, as a software
engineer was if you have any
given problem, chances are not
only is somebody else run into
it, but they've also written
about it on StackOverflow. It's
really good heuristic because it
saves you a lot of time. And it
was never wrong to is like, I
always found somebody else ran
into the same problem. So it's
kind of a humbling experience to
because you're like, wow, I
cannot come up with anything
original here. But that's not so
you know, chances are any, any
idea you have somebody at least
thought of, so that's not really
what's going to set you apart.
Right. What's going to set you
apart is whether or not you
actually pursue that idea
doggedly. And, you know, I feel
like you know, you're on the
right path in terms of like,
being competitive on an idea,
not if it's novel, right?
Novelty is not so important.
Like, it's rather if you keep
hitting these pain points that
you're struggling with, you
eventually succeed, but you have
the tenacity to do so. Because
then you know, that other people
will have to run into that same
thing.
Sanyam Bhutani: Yeah.
Jason Antic: And there's a very
good chance that they're going
to give up somewhere on the way.
So I've accrued a lot of this
over the past year with this
stuff. So that's enough for that
reason. I'm like, oh, you know,
we probably have a pretty good
competitive advantage with this,
you know, this mobile app for
some time, even though I know
Google's working on this
problem, too. You know, like
it's kind of intimidating to
think about that. But you know,
same time, you know, why these
these problems are things that
can only really be solved by
having a dedicated person sit
down and just really love it and
and embrace it and, you know,
live and live and breathe the
problem for quite a while.
Sanyam Bhutani: Yeah.
Jason Antic: So that's my long
winded way of saying, don't
focus on the ideas so much as
the execution.
Sanyam Bhutani: Got it? Yeah.
I'm sorry, I don't have a real
physical gift to hand to you.
But I'd like to hand you a
virtual "Chai Time Data Science"
award because Jeremy mentioned
on this interview that DeOldify
is his favorite project. And if
you'd like to, if you'd like to
do an acceptance speech for;
Jason Antic: Oh boy. My
acceptance speech is, thank you
Jeremy. Thank you, my
supporters. Because I do have to
say that's been the coolest part
about doing this is the support
I've been getting and the people
I've been able to talk to and
get to know. So yeah, I'm truly
thankful for that. I'm not just
saying that because I had to do
an acceptance.
Sanyam Bhutani: Yeah, but I
think it's, it's really been
great. We get to see a lot of
this on Twitter again, doing a
plug for you, but there's old
Ireland in color. I think
there's also a few other Twitter
handles that are you doing this
very extensively?
Jason Antic: Yeah, that's, has
been probably the coolest thing
I've seen so far, is, yeah,
Pakistan in color. There's old
Argentina in color, New Zealand.
Well, we took America over
Prague, Prague, and I start
putting out stuff for that.
Yeah, it's such a cool concept.
John Breslin doing the Ireland
one in particular is really
executed quite well. I am, I
love that guy. He's he's been
great. He's like, he's pretty
much like the the power user of
DeOldify.
Sanyam Bhutani: So this this has
been an amazing conversation. My
last question to you would be
what do you have, what best
advice do you have for anyone
just starting their machine
learning or machine learning and
software experience?
Jason Antic: Wow. Yeah. So I
would say for either for machine
learning or software
development, if you're trying to
get into this. Going back to
what I said earlier, is just, it
takes time. And you know, so you
need to set aside time. And, you
know, if you find something that
you're interested in, you know,
allow yourself to dig into that.
Don't feel guilty about it.
Right. I, it's very broad
advice. But I would also say I
really am a big believer and how
fast.ai approaches the teaching
proces, you can also you can
take what they do and apply it
to a whole bunch of other things
in your life with, you know,
whether you're learning,
software development, and so on.
Which is, again, the, you know,
the top down approach where you
don't have to build up this huge
library upfront in your head of
prerequisites. You know;
Sanyam Bhutani: Yeah.
Jason Antic: It's really funny.
I mean, like, people keep
saying, oh, you need to know all
this linear algebra and
calculus, that and they just do
away with that. They said, no
algebra. That's it. And it's
true. I mean, you know, the only
thing you need to know from
calculus to I mean, the only
concept you need to sort of get
with deep learning in terms of
like, calculus is like, you
know, what a derivative is and
what the chain role is, right?
That's, those are the two really
big things and they're really
easy concepts. Honestly. I mean,
there's something you don't have
to have that huge context of
years of calculus to get I don't
think. Yeah, right.
Sanyam Bhutani: Awesome. Now
before we end the interview, I'd
like to mention your social
handles, we'll have your Twitter
linked. And for the audience, if
you just search for DeOldify,
you'll probably find the
repository as well. But any
other platforms?
Jason Antic: Yeah, there's I
mean, there's the Twitter. So my
Twitter handle: @citnaj, we do
have DeOldify one, we just
haven't been putting anything on
there yet. We just, so much work
to do.
Sanyam Bhutani: Yeah.
Jason Antic: Yeah. It's really
hard with just two people. Yeah,
we have an Instagram, actually.
And then no, I'm sorry. It's
under Dana Dana's Instagram.
That's where she's. I'm sorry.
This is disorganized.
Sanyam Bhutani: But we'll have a
link in case the audience wants
to go check it out.
Jason Antic: Yeah. Yeah, let me,
I had to send them to you. Hehe.
So, yeah, yeah, cuz I don't
remember them off the top of my
head. Yeah, so that it gets the
GANs pretty much and my and my
Twitter handle that those are
the two places to go that are
actively maintained for this
stuff right now. It's going to
change fairly soon.
Sanyam Bhutani: Jason, thank you
so much for doing this
interview. And huge thanks to
you for all of your
contributions to if I may, a new
field that you've created with
GANs.
Jason Antic: I, thank you, this
interview's been a huge
pleasure.
Sanyam Bhutani: Thank you so
much for listening to this
episode. If you enjoyed the
show, please be sure to give it
a review or feel free to shoot
me a message. You can find all
of the social media links in the
description. If you like the
show, please subscribe and tune
in each week, to "Chai Time Data
Science."