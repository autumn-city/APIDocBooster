Let me just stress that the LSTM cell has learnable parameters which are shared among all the features. Anyway, the K matrix is nothing else than the output hidden state of our LSTM cell.
So it only receives one character as the input, then for the one layer of the hidden, the initial hidden and cell state, and it only outputs one output here, and then the hidden and cell set for the next state. So why that is more useful is in a way for computing the loss  essentially that you can get one thing at a time, essentially  instead of running the whole thing.
Specify the options state_is_tuple=False will concat the two state variables into one, therefore double the size of what you have specified in the init_state declaration. To avoid this, one can use the built-in zero_state method for an LSTMCell to initialize the state in the correct way without worrying about size differences.
The input gets passed one after the other, for each particular LSTM cell.
So these can be any variants of far, just like I mentioned, either LSTM or GRUs, if we're considering LSTM, then each of the intermediate hidden layers, each of the intermediate hidden cells will have two outputs.
