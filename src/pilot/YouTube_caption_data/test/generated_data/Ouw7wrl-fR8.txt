so hi and welcome back to another video
all the source code and everything else
in this video can be found in an article
published online
check the link in description if you
want to read or just copy the source
code
now let's get started
python ai how to build a neural network
and make predictions if you're just
starting out in the artificial
intelligence af world then python is a
great language to learn since most of
the tools are built using it
deep learning is a technique used to
make predictions using data and it
heavily relies on neural networks
today you'll learn how to build a neural
network from scratch
in a production setting you would use a
deep learning framework like tensorflow
or pytorch instead of building your own
neural network
that said having some knowledge of how
neural networks work is helpful because
you can use it to better architect your
deep learning models
in this tutorial you'll learn what
artificial intelligence is how both
machine learning and deep learning play
a role in ai how a neural network
functions internally how to build a
neural network from scratch using python
let's get started
artificial intelligence overview in
basic terms the goal of using ai is to
make computers think as humans do
this may seem like something new but the
field was born in the 1950s
imagine that you need to write a python
program that uses ai to solve a sudoku
problem
a way to accomplish that is to write
conditional statements and check the
constraints to see if you can place a
number in each position
well this python script is already an
application of ai because you programmed
a computer to solve a problem
machine learning ml and deep learning dl
are also approaches to solving problems
the difference between these techniques
and a python script is that ml and dl
use training data instead of hard-coded
rules but all of them can be used to
solve problems using ai
in the next sections you'll learn more
about what differentiates these two
techniques
machine learning machine learning is a
technique in which you train the system
to solve a problem instead of explicitly
programming the rules
getting back to the sudoku example in
the previous section to solve the
problem using machine learning you would
gather data from solved sudoku games and
train a statistical model
statistical models are mathematically
formalized ways to approximate the
behavior of a phenomenon
a common machine learning task is
supervised learning in which you have a
data set with inputs and known outputs
the task is to use this data set to
train a model that predicts the correct
outputs based on the inputs
the image below presents the workflow to
train a model using supervised learning
workflow to train a machine learning
model the combination of the training
data with the machine learning algorithm
creates the model
then with this model you can make
predictions for new data
the goal of supervised learning tasks is
to make predictions for new unseen data
to do that you assume that this unseen
data follows a probability distribution
similar to the distribution of the
training data set
if in the future this distribution
changes then you need to train your
model again using the new training data
set
feature engineering prediction problems
become harder when you use different
kinds of data as inputs
the sudoku problem is relatively
straightforward because you're dealing
directly with numbers
what if you want to train a model to
predict the sentiment in a sentence
or what if you have an image and you
want to know whether it depicts a cat
another name for input data is feature
and feature engineering is the process
of extracting features from raw data
when dealing with different kinds of
data you need to figure out ways to
represent this data in order to extract
meaningful information from it
an example of a feature engineering
technique is limitization in which you
remove the inflection from words in a
sentence
for example inflected forms of the verb
watch like watches watching and watched
would be reduced to their lemma or base
form watch
if you're using arrays to store each
word of a corpus then by applying
limitization you end up with a less
sparse matrix
this can increase the performance of
some machine learning algorithms
the following image presents the process
of limitization and representation using
a bag of words model
creating features using a bag of words
model first the inflected form of every
word is reduced to its lemma
then the number of occurrences of that
word is computed
the result is an array containing the
number of occurrences of every word in
the text
deep learning deep learning is a
technique in which you let the neural
network figure out by itself which
features are important instead of
applying feature engineering techniques
this means that with deep learning you
can bypass the feature engineering
process
not having to deal with feature
engineering is good because the process
gets harder as the data sets become more
complex
for example how would you extract the
data to predict the mood of a person
given a picture of her face
with neural networks you don't need to
worry about it because the networks can
learn the features by themselves
in the next sections you'll dive deep
into neural networks to better
understand how they work
neural networks main concepts a neural
network is a system that learns how to
make predictions by following these
steps
one taking the input data too
making a prediction three
comparing the prediction to the desired
output for
adjusting its internal state to predict
correctly the next time vectors layers
and linear regression are some of the
building blocks of neural networks
the data is stored as vectors and with
python you store these vectors in arrays
each layer transforms the data that
comes from the previous layer
you can think of each layer as a feature
engineering step because each layer
extracts some representation of the data
that came previously
one cool thing about neural network
layers is that the same computations can
extract information from any kind of
data
this means that it doesn't matter if
you're using image data or text data
the process to extract meaningful
information and train the deep learning
model is the same for both scenarios
in the image below you can see an
example of a network architecture with
two layers
a neural network with two layers each
layer transforms the data that came from
the previous layer by applying some
mathematical operations
the process to train a neural network
training a neural network is similar to
the process of trial and error
imagine you're playing darts for the
first time
in your first throw you try to hit the
central point of the dartboard
usually the first shot is just to get a
sense of how the height and speed of
your hand affect the result
if you see the dart is higher than the
central point then you adjust your hand
to throw it a little lower and so on
these are the steps for trying to hit
the center of a dartboard
steps to hit the center of a dartboard
notice that you keep assessing the error
by observing where the dart landed step
two
you go on until you finally hit the
center of the dart board
with neural networks the process is very
similar you start with some random
weights and bias vectors make a
prediction compare it to the desired
output and adjust the vectors to predict
more accurately the next time
the process continues until the
difference between the prediction and
the correct targets is minimal
knowing when to stop the training and
what accuracy target to set is an
important aspect of training neural
networks mainly because of overfitting
and underfitting scenarios
vectors and weights working with neural
networks consists of doing operations
with vectors
you represent the vectors as
multi-dimensional arrays
vectors are useful in deep learning
mainly because of one particular
operation the dot product
the dot product of two vectors tells you
how similar they are in terms of
direction and is scaled by the magnitude
of the two vectors
the main vectors inside a neural network
are the weights and bias vectors loosely
what you want your neural network to do
is to check if an input is similar to
other inputs it's already seen
if the new input is similar to
previously seen inputs then the outputs
will also be similar
that's how you get the result of a
prediction
the linear regression model regression
is used when you need to estimate the
relationship between a dependent
variable and two or more independent
variables
linear regression is a method applied
when you approximate the relationship
between the variables as linear
the method dates back to the 19th
century and is the most popular
regression method
note a linear relationship is one where
there's a direct relationship between an
independent variable and a dependent
variable
by modeling the relationship between the
variables as linear you can express the
dependent variable as a weighted sum of
the independent variables
so each independent variable will be
multiplied by a vector called weight
besides the weights and the independent
variables you also add another vector
the bias
it sets the result when all the other
independent variables are equal to zero
as a real world example of how to build
a linear regression model imagine you
want to train a model to predict the
price of houses based on the area and
how old the house is
you decide to model this relationship
using linear regression
the following code block shows how you
can write a linear regression model for
the stated problem in pseudocode
price equals weights underscore area
asterisk area plus weights underscore
age asterisk age plus bias in the above
example there are two weights weights
underscore area and weights under score
h
the training process consists of
adjusting the weights and the bias so
the model can predict the correct price
value
to accomplish that you'll need to
compute the prediction error and update
the weights accordingly
these are the basics of how the neural
network mechanism works
now it's time to see how to apply these
concepts using python
python a starting to build your first
neural network the first step in
building a neural network is generating
an output from input data
you'll do that by creating a weighted
sum of the variables
the first thing you'll need to do is
represent the inputs with python and
number py
wrapping the inputs of the neural
network with number py you'll use number
py to represent the input vectors of the
network as arrays
but before you use number py it's a good
idea to play with the vectors in pure
python to better understand what's going
on
in this first example you have an input
vector and the other two weight vectors
the goal is to find which of the weights
is more similar to the input taking into
account the direction and the magnitude
this is how the vectors look if you plot
them
three vectors in a cartesian coordinate
plane weights underscore 2 is more
similar to the input vector since it's
pointing in the same direction and the
magnitude is also similar
so how do you figure out which vectors
are similar using python
first you define the three vectors one
for the input and the other two for the
weights
then you compute how similar input
underscore vector and weights underscore
one are
to do that you'll apply the dot product
since all the vectors are two
two-dimensional vectors these are the
steps to do it one multiply the first
index of input underscore vector by the
first index of weights underscore one
two multiply the second index of input
underscore vector by the second index of
weights underscore two three sum the
results of both multiplications
you can use an ipython console or a
jupiter notebook to follow along
it's a good practice to create a new
virtual environment every time you start
a new python project so you should do
that first
then ships with python versions 3.3 and
above and it's handy for creating a
virtual environment using the above
commands you first create the virtual
environment then you activate it
now it's time to install the ipython
console using pip
since you'll also need number py and
matplotlib it's a good idea install them
too now you're ready to start coding
this is the code for computing the dot
product of input underscore vector and
weights underscore 1.
the result of the dot product is 2.1672
now that you know how to compute the dot
product it's time to use np dot from
number py
here's how to compute start underscore
product underscore one using np dot dot
np data does the same thing you did
before but now you just need to specify
the two arrays as arguments
now let's compute the dot product of
input underscore vector and weights
under score 2. this time the result is
4.1259
as a different way of thinking about the
dot product you can treat the similarity
between the vector coordinates as an on
off switch
if the multiplication result is zero
then you'll say that the coordinates are
not similar
if the result is something other than
zero then you'll say that they are
similar
this way you can view the dot product as
a loose measurement of similarity
between the vectors
every time the multiplication result is
0 the final dot product will have a
lower result
getting back to the vectors of the
example since the dot product of input
underscore vector and weights underscore
2 is 4.1259
and
4.1259 is greater than 2.1672
it means that input underscore vector is
more similar to weights underscore 2.
you'll use this same mechanism in your
neural network
note click the prompt at the top right
of each code block if you need to copy
and paste it
in this tutorial you'll train a model to
make predictions that have only two
possible outcomes
the output result can be either zero or
one
this is a classification problem a
subset of supervised learning problems
in which you have a data set with the
inputs and the known targets
these are the inputs and the outputs of
the data set
the target is the variable you want to
predict
in this example you're dealing with a
data set that consists of numbers
this isn't common in a real production
scenario
usually when there's a need for a deep
learning model the data is presented in
files such as images or text
making your first prediction since this
is your very first neural network you'll
keep things straightforward and build a
network with only two layers
so far you've seen that the only two
operations used inside the neural
network with a dot product and a sum
both are linear operations
if you add more layers but keep using
only linear operations then adding more
layers would have no effect because each
layer will always have some correlation
with the input of the previous layer
this implies that for a network with
multiple layers there would always be a
network with fewer layers that predicts
the same results
what you want is to find an operation
that makes the middle layers sometimes
correlate with an input and sometimes
not correlate
you can achieve this behavior by using
non-linear functions
these non-linear functions are called
activation functions
there are many types of activation
functions
the real u rectified linear unit for
example is a function that converts all
negative numbers to zero
this means that the network can turn off
a weight if it's negative adding
non-linearity
the network you're building will use the
sigmoid activation function
you'll use it in the last layer layer
underscore 2.
the only two possible outputs in the
data set are 0 and 1 and the sigmoid
function limits the output to a range
between 0 and 1.
this is the formula to express the
sigmoid function
sigmoid function formula of the e is a
mathematical constant called euler's
number and you can use np.exp x to
calculate e
probability functions give you the
probability of occurrence for possible
outcomes of an event
the only two possible outputs of the
data set are zero and one and the
bernoulli distribution is a distribution
that has two possible outcomes as well
the sigmoid function is a good choice if
your problem follows the bernoulli
distribution so that's why you're using
item the last layer of your neural
network
since the function limits the output to
a range of 0 to 1 you'll use it to
predict probabilities
if the output is greater than 0.5 then
you'll say the prediction is 1.
if it's below 0.5 then you'll say the
prediction is 0.
this is the flow of the computations
inside the network you're building
the flow of computations inside your
neural network
the yellow hexagons represent the
functions and the blue rectangles
represent the intermediate results
now it's time to turn all this knowledge
into code
you'll also need to wrap the vectors
with number py arrays
this is the code that applies the
functions presented in the image above
the raw prediction result is 0.79 which
is higher than 0.5 so the output is 1.
the network made a correct prediction
now try it with another input vector np
array to 1.5
the correct result for this input is 0.
you'll only need to change the input
underscore vector variable since all the
other parameters remain the same this
time the network made a wrong prediction
the result should be less than 0.5 since
the target for this input is 0 but the
result was 0.87
it made a wrong guess but how bad was
the mistake
the next step is to find a way to assess
that
train your first neural network in the
process of training the neural network
you first assess the error and then
adjust the weights accordingly
to adjust the weights you'll use the
gradient descent and back propagation
algorithms
gradient descent is applied to find the
direction and the rate to update the
parameters
before making any changes in the network
you need to compute the error
that's what you'll do in the next
section
computing the prediction error
to understand the magnitude of the error
you need to choose a way to measure it
the function used to measure the error
is called the cost function or loss
function
in this tutorial you'll use the mean
squared error mse as your cost function
you compute the mse in two steps one
compute the difference between the
prediction and the target
two multiply the result by itself
the network can make a mistake by
outputting a value that's higher or
lower than the correct value
since the mse is the squared difference
between the prediction and the correct
result with this metric you'll always
end up with a positive value
this is the complete expression to
compute the error for the last previous
prediction in the example above the
error is 0.75
one implication of multiplying the
difference by itself is that bigger
errors have an even larger impact and
smaller errors keep getting smaller as
they decrease
understanding how to reduce the error
the goal is to change the weights and
bias variables so you can reduce the
error
to understand how this works you'll
change only the weights variable and
leave the bias fixed for now
you can also get rid of the sigmoid
function and use only the result of
layer underscore one
all that's left is to figure out how you
can modify the weights so that the error
goes down
you compute the mse by doing error
equals and dot square prediction target
if you treat prediction target as a
single variable x then you have error
equals n dot square x which is a
quadratic function
here's how the function looks if you
plot it
plot of a quadratic function the error
is given by the y-axis
if you're in point a and want to reduce
the error towards zero then you need to
bring the x-value down
on the other hand if you're in point b
and want to reduce the error then you
need to bring
x value up
to know which direction you should go to
reduce the error you'll use the
derivative
a derivative explains exactly how a
pattern will change
another word for the derivative is
gradient
gradient descent is the name of the
algorithm used to find the direction and
the rate to update the network
parameters
note to learn more about the math behind
gradient descent check out stochastic
gradient descent algorithm with python
and number py
in this tutorial you won't focus on the
theory behind derivatives so you'll
simply apply the derivative rules for
each function you encounter
the power rule states that the
derivative of x is nx superscript 1.
so the derivative of n dot square x is
two asterisk x and the derivative of x
is one
remember that the error expression is
error equals n dot square prediction
target
when you treat prediction target as a
single variable x the derivative of the
error is two asterisk x
by taking the derivative of this
function you want to know in what
direction should you change x to bring
the result of error to zero thereby
reducing the error
when it comes to your neural network the
derivative will tell you the direction
you should take to update the weights
variable
if it's a positive number then you
predict it too high and you need to
decrease the weights
if it's a negative number then you
predict it too low and you need to
increase the weights
now it's time to write the code to
figure out how to update weights
underscore one for the previous wrong
prediction
if the mean squared error is 0.75 then
should you increase or decrease the
weights
since the derivative is to asterisk x
you just need to multiply the difference
between the prediction and the target by
2.
the result is 1.74 a positive number so
you need to decrease the weights
you do that by subtracting the
derivative result of the weights vector
now you can update weights underscore
one accordingly and predict again to see
how it affects the prediction result the
error dropped down to almost zero
beautiful right
in this example the derivative result
was small but there are some cases where
the derivative result is too high
take the image of the quadratic function
as an example
hi
increments aren't ideal because you
could keep going from point a straight
to point b never getting close to zero
to cope with that you update the weights
with a fraction of the derivative result
to define a fraction for updating the
weights you use the alpha parameter also
called the learning rate if you decrease
the learning rate then the increments
are smaller
if you increase it then the steps are
higher
how do you know what's the best learning
rate value
by making a guess and experimenting with
it
note traditional default learning rate
values are 0.1
0.01 and 0.001
if you take the new weights and make a
prediction with the first input vector
then you'll see that now it makes a
wrong prediction for that one if your
neural network makes a correct
prediction for every instance in your
training set then you probably have an
overfitted model where the model simply
remembers how to classify the examples
instead of learning to notice features
in the data
there are techniques to avoid that
including regularization the stochastic
gradient descent
in this tutorial you'll use the online
stochastic gradient descent
now that you know how to compute the
error and how to adjust the weights
accordingly it's time to get back
continue building your neural network
applying the chain rule in your neural
network you need to update both the
weights and the bias vectors
the function you're using to measure the
error depends on two independent
variables the weights and the bias
since the weights and the bias are
independent variables you can change and
adjust them to get the result you want
the network you're building has two
layers and since each layer has its own
functions you're dealing with a function
composition
this means that the error function is
still np dot square x but now x is the
result of another function
to restate the problem now you want to
know how to change weights underscore 1
and bias to reduce the error
you already saw that you can use
derivatives for this but instead of a
function with only a sum inside now you
have a function that produces its result
using other functions
since now you have this function
composition to take the derivative of
the error concerning the parameters
you'll need to use the chain rule from
calculus
with the chain rule you take the partial
derivatives of each function evaluate
them and multiply all the partial
derivatives to get the derivative you
want
now you can start updating the weights
you want to know how to change the
weights to decrease the error
this implies that you need to compute
the derivative of the error with respect
to weights
since the error is computed by combining
different functions you need to take the
partial derivatives of these functions
here's a visual representation of how
you apply the chain rule to find the
derivative of the error with respect to
the weights
a diagram showing the partial
derivatives inside the neural
network the bold red arrow shows the
derivative you want their underscored
weights
you'll start from the red hexagon taking
the inverse path of making a prediction
and computing the partial derivatives at
each function
in the image above each function is
represented by the yellow hexagons and
the partial derivatives are represented
by the gray arrows on the left
applying the chain rule the value of der
underscored weights will be the
following to calculate the derivative
you multiply all the partial derivatives
that follow the path from the error
hexagon the red one to the hexagon where
you find the weights the leftmost green
one
you can say that the derivative of y
equals f x is the derivative of f with
respect to x
using this nomenclature for do
underscored prediction you want to know
the derivative of the function that
computes the error with respect to the
prediction value
this reverse path is called a backward
pass
in each backward pass you compute the
partial derivatives of each function
substitute the variables by their values
and finally multiply everything
this take the partial derivatives
evaluate and multiply part is how you
apply the chain rule
this algorithm to update the neural
network parameters is called
backpropagation
adjusting the parameters with
backpropagation
in this section you'll walk through the
backpropagation process step by step
starting with how you update the bias
you want to take the derivative of the
error function with respect to the bias
to underscore bias
then you'll keep going backward taking
the partial derivatives until you find
the bias variable
since you're starting from the end and
going backward you first need to take
the partial derivative of the error with
respect to the prediction
that's the der underscored prediction in
the image below
a diagram showing the partial
derivatives to compute the bias
gradient the function that produces the
error is a square function and the
derivative of this function is two
asterisk x as you saw earlier
you applied the first partial derivative
der underscored prediction and still
didn't get to the bias so you need to
take another step back and take the
derivative of the prediction with
respect to the previous layer prediction
underscored layer 1.
the prediction is the result of the
sigmoid function
you can take the derivative of the
sigmoid function by multiplying sigmoid
x and one sigmoid x
this derivative formula is very handy
because you can use the sigmoid result
that has already been computed to
compute the derivative of it
you then take this partial derivative
and continue going backward
now you'll take the derivative of layer
underscore one with respect to the bias
there it is you finally got to it
the bias variable is an independent
variable so the result after applying
the power rule is 1.
cool now that you've completed this
backward pass you can put everything
together and compute der underscored
bias
to update the weights you follow the
same process going backward and taking
the partial derivatives until you get to
the weights variable
since you've already computed some of
the partial derivatives you'll just need
to compute layer one underscored weights
the derivative of the dot product is the
derivative of the first vector
multiplied by the second vector plus the
derivative of the second vector
multiplied by the first vector
creating the neural network class now
you know how to write the expressions to
update both the weights and the bias
it's time to create a class for the
neural network classes are the main
building blocks of object oriented
programming oop
the neural network class generates
random start values for the weights and
bias variables
when instantiating a neural network
object you need to pass the learning
underscore at a parameter
you'll use predict to make a prediction
the methods underscore compute
underscore derivatives and underscore
update underscore parameters have the
computations you learned in this section
this is the final neural network class
image
there you have it that's the code of
your first neural network
congratulations
this code just puts together all the
pieces you've seen so far
if you want to make a prediction first
you create an instance of neural network
and then you call.predict the above code
makes a prediction but now you need to
learn how to train the network
the goal is to make the network
generalize over the training data set
this means that you wanted to adapt to
new unseen data that follow the same
probability distribution as the training
data set
that's what you'll do in the next
section
training the network with more data
you've already adjusted the weights and
the bias for one data instance but the
goal is to make the network generalize
over an entire data set
stochastic gradient descent is a
technique in which at every iteration
the model makes a prediction based on a
randomly selected piece of training data
calculates the error and updates the
parameters
now it's time to create the train method
of your neural network class
you'll save the error over all data
points every 100 iterations because you
want to plot a chart showing how this
metric changes as the number of
iterations increases
this is the final train method of your
neural network
image
there's a lot going on in the above code
block so here's a line by line breakdown
line 8 picks a random instance from the
data set
lines 14 to 16 calculate the partial
derivatives and return the derivatives
for the bias and the weights
they use underscore compute underscore
gradients which you defined earlier
line 18 updates the bias and the weights
using underscore update underscore
parameters which you defined in the
previous code block
line 21 checks if the current iteration
index is a multiple of one hundred
you do this to observe how the error
changes every 100 iterations
line 24 starts the loop that goes
through all the data instances
line 28 computes the prediction result
line 29 computes the error for every
instance
line 31 is where you accumulate the sum
of the errors using the cumulative
underscore or variable
you do this because you want to plot a
point with the error for all the data
instances then on line 32 you append the
error to cumulative underscore errors
the array that stores the errors
you'll use this array to plot the graph
in short you pick a random instance from
the data set compute the gradients and
update the weights and the bias
you also compute the cumulative error
every 100 iterations and save those
results in an array
you'll plot this array to visualize how
the error changes during the training
process
note if you're running the code in a
jupyter notebook then you need to
restart the kernel after adding train to
the neural network class
to keep things less complicated you'll
use a data set with just eight instances
the input underscore vectors array
now you can call train and use
matplotlib to plot the cumulative error
for each iteration
image
you instantiate the neural network class
again and call train using the input
underscore vectors and the target values
you specify that it should run 10 000
times
this is the graph showing the error for
an instance of a neural network
graph image
graph showing the cumulative training
error the overall error is decreasing
which is what you want
the image is generated in the same
directory where you're running ipython
after the largest decrease the error
keeps going up and down quickly from one
interaction to another
that's because the data set is random
and very small so it's hard for the
neural network to extract any features
but it's not a good idea to evaluate the
performance using this metric because
you're evaluating it using data
instances that the network already saw
this can lead to overfitting when the
model fits the training data set so well
that it doesn't generalize to new data
adding more layers to the neural network
the data set in this tutorial was kept
small for learning purposes
usually deep learning models need a
large amount of data because the data
sets are more complex and have a lot of
nuances
since these data sets have more complex
information using only one or two layers
isn't enough
that's why deep learning models are
called deep
they usually have a large number of
layers
by adding more layers and using
activation functions you increase the
network's expressive power and can make
very high level predictions
an example of these types of predictions
is face recognition such as when you
take a photo of your face with your
phone and the phone unlocks if it
recognizes the image as you