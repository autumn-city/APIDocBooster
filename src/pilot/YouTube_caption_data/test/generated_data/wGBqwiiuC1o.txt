so
let's continue with the language models
discussion
so previously uh so let's have a recap
of things which we have
discussed previously uh
so the language model is like a some
system from which you can get the
probability of a sequence of words or
if you give the sentence it gives some
probability value that
saying that this particular sentence is
um what is
belongs to the sentences of the language
or a valid sentence in that language
or the language model can be uh
the problem can be posed as a next word
prediction
uh problem like you are given some
uh words still t minus one and you
predict the next word and what's the
probability of that
conditional i mean what's that
conditional probability
so it's like the say here you can see
the students often the
there can be any number of possibilities
like the it can be a book
it can be laptops exams or mice and it
the language model will give a
redistribution saying uh books has
uh uh this much
uh probability like i said that would be
a uh
number between zero and one and our
laptop has this
much uh probability and uh for the
entire
uh words in the vocabulary there will be
some value associated with it
and all the value will add up to one
and to calculate uh for this particular
conditional probability you can uh
find the probability of that n grams and
divide by the
n minus one gram and these probability
can be estimated
uh by counting the uh number of these
n grams or uh n minus one grams
in the uh in the corpus in a large
purpose and you estimate these counts
and then
during your calculation you use these
counts to
uh create the probability
values
so the same thing like you have a
sequence of words and you are
predicting the next word
and this thing which we have missed and
we directly
jump because r and then like uh same as
the n grams you can also have
uh neural language models with
fixed uh window like here you can see
a window for four words
x1 x2 x3 and x4
uh am i audible
yes yes and uh
you somewhere you convert these to some
vector
like using one fourth representation or
some other
uh vector transformation process and you
concatenate and feed it to a
regular feed forward neural network like
that you
multiply this whole vector with a weight
matrix and add bias
and pass it through a
some non-linear function and then you
convert that
uh resultant vector to a probability
distribution
using soft mass
so you can uh train this neural network
uh through back propagation and at
the end you will you have the model
which is
your weight uh weight vectors as well as
the bias vectors
and you don't have to store the endear
count off
or this
and in the rnn we have seen that rna is
like a
uh what you call like a
mixer or a juicer in which you add
different ingredients and it keeps on
churning the things and you get
an output at every step
and in the sequence of any lung can be
fed into this
rnn and it's kind of a recursive process
like
uh there will be a hidden state which
will be updated at every time step
and input will be going in uh
at each time step like the words will be
going in in
each of these time steps and the hidden
state will be
updated and uh in during the training
the weights so one thing
here is like even if the there is an
uh very large sequence the weight matrix
used is the same for
every uh stage and
during each uh during the training
at each step these weight mattresses are
adjusted
by propagating the error functions
so uh it's like how how do you update
these weights is like
you calculate you have an error function
and on the i mean the cumulative error
and uh and you wanted to update the
weights uh with respect to your
uh this particular hidden state like
this particular step
and this particular step some error has
occurred and i wanted to update that
wait but uh you have you had this
four times steps and at the uh
at the end of the step four you want to
update the
weight for uh adjusting the error
that might have happened at uh the the
first stage
so it's like you calculate the
gradient with respect to a gradient of
the error with respect to the
that hidden state it's like uh
in gradient descent it's like gradient
descent is
just like you explore the
uh gradient um of the error in such a
way that you
your aim is to reach the value of that
error like the minimizing the error
and you you have the you have the notion
of like which direction you have to
follow
uh to reach that value quickly like it's
the direction of
uh negative of the gradient or the
opposite direction of your
gradient like if it's a negative slope
uh at one point uh it's a negative slope
means like the
if you increase your uh
the parameter like this in state h1
then uh you are going towards
the value or if you if your slope is
positive at that particular point and if
you increase
h1 you are you are reaching a point
where
uh the error is more than the current
stage
so you know like which direction you
have to follow
and uh to uh
practically uh do the calculations you
can apply the chain rule and see like
it's like the whole gradient is like
uh
multiplication of different uh gradients
of each of the states and you see like
there are four uh different things
getting multiplied together
and with these things like the
the sequence can be of any length like
in a sentence like
a sentence can be of any length like you
can
always create bigger and bigger
sentences
in any of the languages and
so what happens if one of these
gradients are small
like the whole product is going to be
very low
and you you won't make uh much changes
in weight with uh for one of the i mean
the
earliest earlier uh sequence uh
stages like that's the
problem here in the vanishing gradient
when one of these gradients are smaller
like the whole product gets smaller and
updates to the
weights are going to be small and it's
like
you don't uh learn anything
for long sequences or this long
long term dependencies are not captured
in the system
so any any questions so far
uh no like i was gonna ask like the
solution for
the sunday
so same thing like the for long
sentences
like if there is some dependency
for the next word prediction coming from
the start of the sentence or
start of any other sentence which are
long
back in the sequence are not captured
in this system due to this most of the
time due to this vanishing gradient
problem
[Music]
so it's like if the the dependencies
is not captured means uh if the gradient
is if
you consider just the gradient like the
gradient is becoming very small over the
long distance it
means like two things like one one is
like there is no dependency at all
uh second thing is like it's uh the
model is not able to
capture uh the dependency
so if it's in vanishing gradient problem
the thing is like the
model is not able to capture this
long-term dependency
like the say here you have a uh you want
to predict an extra like
there is this big uh group of sentences
like when she tried to print her tickets
she found that the printer was out of uh
toner and she went to the stationary
store to buy more toner and it was
very overpriced after installing the
toner into the printer she finally
printed her what
uh
the tickets are coming from the the id
of tickets
is coming from the first sentence like
it's a long term dependency
like if you uh ignore this part and see
all the other sentences you won't be
able to predict this part
so it's like uh the uh that's the
problem with the vanishing gradient like
in application how it uh comes out
like and now we have to see like how to
uh deal with this problem
and also like the so in other words like
another
if you are if you look at another issue
like
the uh consider this sentence like the
writer of the books
it could be ease or r but the correct
thing is
uh peace because it's uh dependent on
this writer not on the books but
uh due to the vanishing gradient problem
the model may
uh predict like uh r is the
r with the more probability so that's
the
one of the uh this i mean same problem
but
uh like the syntactic recency
uh i mean the sequential reasons you get
more
predominance over the syntactic recency
in this
iron models
and the same way there is another thing
like if the one of the gradients
is too big uh then
the problem there is a problem with a
problem called exploding gradient
means like the that that overall product
becomes very large
and it won't be the system won't be able
to
keep that number in uh
with limit like if you are you are
seeing a 32-bit
uh unsigned integer then it may go
beyond that
and then the uh system will represent it
as an
uh not a number or high enough
and then you have to
go back and see what happens
something
clipping is one of the exploding
gradient can be
quickly solved by gradient clipping like
if it if the
uh the gradient is going beyond a
threshold
limit that within the threshold but keep
the direction
like you are divided by the
um what you call a norm of that vector
and keep the same direction
and how to fix the vanishing gradient
problem and
uh so we have seen that in vanishing
gradient like
the rnn is not learning uh long-term
dependencies
you can see like the hidden state is uh
getting updated
at every step and there is no control
over how much it needs to be updated and
uh can i stop the updation for one step
or can i
have the updation only for certain stuff
so there is no control over
hidden state updation like it's
automatically getting updated
with this recursive formula like you
multiply
the previous hidden state with some
weights and then
add it to the weighted inputs
plus some bias and pass it through a
non-linear function and update the
credential this is automatically
happening so
one way to solve the vanishing gradient
problem is like people thought about
like having a
memory like memory which can be uh
activated and deactivated like you have
so from that came the idea of lstm
long short term uh memories
so you have in uh this is like uh it has
a built-in memory cell and it
it and store two states one is a hidden
state and another is the cell state
hd and ct and these are these
these both of these are vectors of a
fixed length
and
there are gates with which lstm can
erase write and read information
in this both of these states
and the gates are also vectors of length
n
like it's like for each of the say cell
state you have n
uh elements and the gate will also have
an elements
and if the gate is like the uh
gate is going to be multiplied with this
uh state and if it's uh if the element
is 0
it's like that element that particular
element of the gate is zero that means
that that particular element
that is uh the corresponding element in
the uh
that state vector is not going to be
passed or uh it's going to be it's like
the gate is closed for that particular
element
and the value can be between zero and
one
and so that uh you can pass uh
only certain uh um
what do you like what you call like this
it's like they're weighting
each of the giving weight to each of
these elements in the cell state or
uh for the input or the
uh hidden state so the these different
gates uh
can uh act as
a door like uh if it's
one it's completely uh the
information is completely passed if it's
zero is like completely closed and in
between like it's partially bust
and these gates are like uh
neural networks on its own and it can be
this
the weights and biases of this can be
trained
through back provocation
and this is a holistic complete picture
of
an lstm cell so mainly
like it may look like very complex thing
but it's very simple
like if you like
it's the input gate and the third one is
the output gate
and you have two states like the cities
the cell state
hd is the hidden state and
uh this all these gates
are a kind of neural network like it has
bias uh weight vectors as well as
uh the biases
and what
what does this forget
so you have this uh this is your forget
gate like you have the previous cell
state and
uh it determines how much of this
previous cell state needs to be passed
uh forward
to the next uh cell state and
the input gate uh you can see the
input gate here which controls this
uh the how much input needs to be
passed to the sales team
like uh this thing is the
uh concatenation like the hidden state
previous hidden state
and the input is
passed
needs to be passed to uh the cell state
and there is an output state output gate
here which uh
determines how much of this uh cell
state needs to be
passed to the hidden state
and
ta
and let's see like how much uh this
lstms
got impacted the nlp uh
domain like uh in 2013 and 15 the
lstm where the state of our state of the
art uh
methods and uh so it's been successfully
included in
handwriting condition speech recognition
machine translation
pass passing image captioning and all
those things
but um
today like transformers have
uh come up and those being the
dominant approaches uh in the current
situation like though there are
different transmog transformer
architectures which being widely used
and giving good accuracy
and good performance
uh we'll uh we'll talk about transformer
after this
rn stuff is finished and
uh here looked at the lstm like it had
uh it has the three
different gates and someone came up with
a
simpler model uh with only two gates
one is the update gate and the reset
gate so you can see the comparison of
this
uh three different rnns like the vanilla
rnn lstm and
gre here and
what has changed in gru's like
uh there is one gate which is doing uh
two functions like uh
uh there is this update gate update gate
is used here uh
uh you can see like the one minus update
gate and uh
update your value into hd so it's being
used uh at two places and only one
state is there like the only the hidden
state is that there is no separate
uh cell state and
so reset gate is somewhat like the
uh somewhat like the forget gate so it's
uh
the same way it's operating here and in
the update gate
uh does like how much the
how much the uh current
uh current uh ht needs to be
the the calculated st needs to be there
in the
actual hidden state as well as how much
the
previously done state needs to be passed
to this ht
so both being done in one with the one
uh gate and
in comparison like uh like it's only
not much bigger difference between the
gru
and the lstm but only thing is like gra
fewer
and the weights and biases are also i
mean
only for the two gates then the
parameters are
fewer than lstm and
performance wise no conclusive evidence
like the one performance better than
other like the
better ways like you can start with lstm
and if it works and you can
change it to gru and see it's working
again and then
go with great
there are other solutions to vanishing
gradient problems also like the
in the uh neural network architecture
itself like
uh there are things called resonance so
it's not
very specific to the vanishing gradient
problems are not very specific to the
uh rnns alone like it's
uh they're in any neural network
which has multiple uh layers or
which are kind of a deep network so in
the cnn and all you have to have seen
like
expressing it this residual connections
residual connections are
uh the identity function or the
previous uh inputs uh directly being
fed to the that particular layer instead
of
all being coming through the different
uh
different previous layers so it's been
seen like the
uh with these connections the some uh
dependency long term dependencies can be
like the
yes uh there is always one more
input coming directly to this particular
layer
and the dense net also has similar uh
idea like
in this every layer is connected to
every other
uh upcoming layers like it's the dense
connection
and there is another idea like the hive
in it where
it's like a rest net but uh with a
dynamic gate
like it can determine like whether this
information needs to be passed or
don needs to be doesn't need to be
passed like uh there'll be some
trainable gate just like
lstms and
so it's a combination of this lstm idea
and the respite idea
so that's about vanishing gradient
problem and it's solutions
and in uh our next lecture we'll
have uh
uh the guru architecture
just is it like uh just because like
there are
less number of gates i mean like uh
like it's quicker you know
one thing is like during the training
you only have need to update uh fewer
parameters and
also during your inference you only need
to
do lesser computations
and it's been seen like in practical
situations
the lstm and gru are not
differing much in their performance even
if
lstm has these three different gates
and like uh how does it know like what
to
forget and what to keep in the memory
the like ls theme for elliston's case so
each of these gates
are uh neural networks on its own
like it gives a probability distribution
with the n number of uh what you call n
number
uh it's an
it outputs an number vector with all or
the
entire thing as a probability
distribution and
uh you are learn learning these gates
like the
these weights and biases are learned
through
experience like that these sequences are
fed and
your problem is predicting the next word
and
if the since the during the training if
the in
first row it's predicting wrongly the
error is click then you propagate that
error and update this
gate weights and biases so
with a with a very
large purpose and you are training it on
for a longer time you
your weights and biases will get
adjusted in such a way that it
uh always gives a good accuracy like uh
like for example in that previous ticket
case
uh it it would uh know like the
uh what should be the uh gate value like
when
when it the first uh
when the first ticket small example
comes like it updates
the uh updates the uh
sales state or the hidden state in such
a way that the i mean
are gates are get more importance to
these
that first tickets so that it's uh
captured in the system
so the uh gates needs to be uh
gates will converge or train in that way
with a good number of training examples
it's like you where you want which which
information is
uh more important for you that is
captured
with that uh case
okay and that is decided based on like
the
training corporation yeah it's all
automatically learned it's like you
you are uh you are creating a system
which uh
for us humans like we can detect some uh
rules and things but in the machine
learning thing
you need to the system needs to learn
or generalize on training example and
uh have these rules uh
set up automatically like we cannot go
and tell like this is the rule
that's not the way the this uh
machine learning work like it's solely
depend on
data examples so you give enough data
examples and the system
learns or generalizes on those examples
anything else
no that's it from us
nothing from my side
yeah and uh so the next lectures we will
see the multi-layer
rnas and uh bi-directional rna so
you have notice like in this uh rns the
sequences are loaded from one
in one direction like uh say
uh if it's the sentence it's like how
are you like
that's being fed like how fast our first
and you first you then so it will be
that sequence and you only look in one
direction
or next word prediction so
in bi-directional it's like it's able to
look in both direction
and it goes backwards yeah
multi layer is like the same way as the
multi-layer perceptrons or other neural
networks you can
add uh multiple layers to this rna
source so like
uh and we'll see like what are the
practical use cases
and how like how do you train an rnn
model
and all those things in the uh next
uh lecture
okay
thank you so much for your time have a
great day
bye thank you thank you bye