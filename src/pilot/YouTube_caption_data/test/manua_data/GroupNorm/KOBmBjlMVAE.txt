 Yeah, like I mentioned in the previous video, I wanted to
 share some techniques with you for improving the generalization
 performance of a model. So I did a very spontaneous brainstorming
 session where I took a mind map software and just wrote down
 everything I know that came to mind. This list might not be
 very exhaustive. There might be other techniques I'm not
 listening here. Here was really just focusing on the most
 popular ones that came to mind that are useful in practice. And
 yeah, of course, we can't cover all of them in detail in this
 course, because there are so many other topics to talk about.
 But at least you get the big picture and you may want to look
 into some of these techniques that you could find useful for
 your class project. So like I said, this was pretty
 spontaneous. So there might be different ways to organize this
 better, but I chose a few categories to have some
 structure to this. So one category is I call it data set.
 So here I'm referring to techniques that really modify
 either the features or the labels in the data set, or even
 yeah, creating new data sets or using different data sets. So I
 would say personally, one of the best bangs for the buck, if you
 want to improve model performance is collecting more
 data, if you can. This is not always the case, of course, but
 there are many scenarios where more data can generally be
 helpful. And this will also be something I will discuss a
 little bit more in detail in the next lecture. Sorry, not next
 lecture. In the next video, I will show you a graphic or plot
 that can help you finding out whether more data could be
 useful. Another technique is data augmentation. So this is
 for modifying the input features. So by for instance,
 rotating an image and things like that. And that is also
 something I will talk more about in the next video. So yeah,
 another technique is label smoothing. Personally, I
 haven't really worked with that extensively, I only have used
 that in the context of generative adversarial
 networks. But I think it can be also useful in a general
 classification context. It's basically preventing the
 classifier to become too confident. And that can be
 achieved by instead of using, let's say, zero one labels by
 having softer versions of that, for instance, set of, let's say
 zero and one, we can use point one and point nine. And this has
 been shown to be helpful in the context of generative adversarial
 networks. And I can think of also classification of being
 positively affected by that in certain cases. Then our big
 topic is yeah, leveraging unlabeled data. We talked about
 this a little bit in the introduction to this course. So
 one approach is semi supervised learning, which is essentially
 about leveraging unlabeled data by looking at how confident your
 classifier is. So you fit your classifier on the subset of
 labeled data, and then you apply it to unlabeled data. For
 example, if you have a larger data set, where you have data
 points that are not labeled yet. And if the classifier is very
 confident for some of those, then you may consider them, or
 you can consider the predicted labels as the true labels to
 make your training set larger. Self supervised learning is a
 little bit different. So self supervised learning is also
 leveraging unlabeled data. But here, you create a so called
 pretext task, where you make up a different classification task
 for which you can create the data yourself. So I showed you
 an introduction to this course, like an example of solving a
 jigsaw puzzle where you take an image, and then you divide that
 image into smaller sub images, and then you train a network to
 predict the order of these images that would be, for
 example, self supervised learning. But yeah, these are
 topics on both semi supervised and self supervised learning that
 go a little bit beyond the scope of this course, that might be a
 future topic for a different course at some point for more
 like advanced topics. Yeah, so also related to what I just said
 there's also a set of techniques for leveraging related data. So
 self supervised learning is really like leveraging unlabeled
 data that could be from the same domain or the same data set that
 you're working with for which you have labels. So let's say
 you have a data set for which you have labels, but you can
 design this pretext task on the same data set. So here, you may
 also consider related different data sets. So for instance, one
 technique is called meta learning, where you essentially
 learn how to learn like from different data sets, let's say
 multiple small data sets, that's actually very common in the
 context of future learning. And another is a bit unfortunate.
 Another definition of meta learning is also really
 learning from metadata, that is another thing. So you have
 multiple data sets, you can create metadata and then train a
 classifier on that metadata. And yeah, another technique is
 transfer learning. So actually, this came up when I was just
 grading class projects. So some students are working on COVID
 19 prediction from chest x ray data. So but the data sets are
 very small. So one thing one could do is to collect a
 different data set of lung x ray images, for instance, for
 diagnosing a different disease. And then you train a classifier
 on this large data set for let's say, I don't know some other
 lung disease. And then after you train the model, you take that
 model and fine tune it to the COVID 19 chest x ray database.
 So we will talk about transfer learning also briefly later in
 this course. So it's actually a very useful technique to. So
 okay, this was all considering data sets. And so there's also
 there are some techniques related to architecture set up
 how you structure your architecture, the deep neural
 network architecture. So there will be weight initialization
 strategies, we will discuss that in this course. So I don't want
 to talk too much about it at this point. Yeah, choosing
 activation functions. We talked about this already on last week
 when we talked about, for example, the relu activation
 function, residual layers, they are so called, I consider them
 as skip connections, I think they are sometimes called skip
 connections. So we are skipping or we're adding a connection by
 skipping certain layers that can also be helpful to avoid
 vanishing and exploding gradient problems. And this is also
 something we will talk later about this in this course. So
 there's also a knowledge distillation. So this is beyond
 the scope of scope of this course, but it's kind of an
 interesting setup, where you train a large neural network,
 and then you call it the teacher. And then you have a
 smaller neural network, you call it the student network, and the
 student network learns to predict based on the
 predictions from the teacher. So you train the teacher on this
 data set, and then you run this teacher to make or create the
 predictions and you train the student usually a smaller
 network on the predictions of the teacher. And what's kind of
 interesting about that is, yeah, for the teacher, you can run it
 infinitely on also larger data sets. So you can actually have
 on the infinite in a way infinite predictions for the
 student. Anyways, another set of techniques concerns
 normalization, we talked already about input standardization, I
 will mention it again in the next video when I show you the
 data augmentation. There is also a set of techniques related to
 batch normalization, this is related to input standardization,
 but here, it's internal in the network. So instead of only
 looking at the inputs to the network, we also look at all the
 inputs to the hidden layers. So the hidden layer activations, and
 we normalize those two, there also are more flavors of that
 called group norm and instance norm and layer norm. And also
 those are a topic for a future lecture in this course. Yeah,
 weight standardization is also there are also techniques for
 standardizing weights, it's kind of related to the weight
 initialization on topic, but there are also additional topics
 for that, and also gradient centralization. So gradient
 centralization is similar to input standardization, except
 that you normalize the gradients so that they have zero mean and
 unit variance. So yeah, and next, there are techniques from
 yeah, I would say modifying the training loop. So the for loop
 over the epochs and the mini batches. And so what we can do
 there in terms of the optimizer and things like that should have
 maybe added something like different optimizers here,
 because they are also optimizers that go beyond adaptive learning
 rates. We will talk about this also in more detail in later
 lectures. So for example, there are adaptive learning rates,
 which are super helpful. They're also auxiliary auxiliary losses.
 So we can add additional loss functions, intermediate layers,
 and then also modify the training. Intermediately, we can
 modify the network training by having these on additional loss
 functions. So one common thing that comes to mind right now
 would be the inception network. We will talk about that in the
 convolution network section where we have multiple loss
 functions that we combine from different places in the network.
 And that helps also training the network. So just briefly, if you
 have a network like that, usually you have the output
 output here. And then you have a loss function and you have the
 label. So you have the true label. So you have your input
 image, the x goes into the network, oops. And the y goes
 into the loss function together with y hat, the predicted label
 or probability. So you get y y hat as the loss function that
 you compute for backpropagation. But yeah, what you can also do
 is you can also have intermediate value here,
 intermediate prediction from the intermediate layer, and also add
 that to the loss function, that is essentially how inception
 works. And with that, you can make sure that if you have a
 very big, big long network, that also the intermediate layers
 are trained well. So Jung Ji and I, we also worked on a method
 related to this topic called looking back, looking back to
 earlier layers for designing better loss functions. Yeah,
 then there's also gradient clipping. So avoiding, like very
 large gradients. So if they go too large, we can clip them by
 giving them giving it a maximum value. So that is also sometimes
 helpful to avoid exploiting gradient problems. And now the
 last set of techniques, I just put them also here, because
 those are the ones that we are going to cover in this particular
 lecture here, in addition to these topics. So we will talk
 about l two regularization and l one regularization for adding a
 penalty for large weights. So that helps with having smaller
 weights. And that also helps with making the network less
 sensitive to certain inputs. So that makes the network
 predictions a little bit less noisy. So reducing the
 variance. And we'll also talk about your early stopping by
 looking at validation set performances, and then drop out,
 like dropping random units in the network, which is kind of
 like a way of adding noise to the network. And then helps also
 with, yeah, overfitting. Alright, so this is just like the
 big picture overview of all the different techniques that
 spontaneously came to my mind, when it comes to improving
 generalization performance. Again, we will talk about many of
 these techniques, not all of them, but many. So just have to
 be patient, because we can only talk about one thing at a time,
 but think this is probably a useful overview for you. Alright,
 so in the next video, we will talk then about overfitting by
 considering making our data set larger, and by augmenting our
 existing data.