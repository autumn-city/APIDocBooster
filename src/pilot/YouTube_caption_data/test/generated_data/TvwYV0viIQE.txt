[Music]
[Music]
[Music]
[Music]
[Music]
[Music]
when we're gonna go live are we ready
all right hello world it's Suraj and
welcome to this live stream I'm so
excited to see everybody here what I'm
gonna be doing in this livestream is I'm
going to be solving this Kegel challenge
it's called the invasive species
monitoring challenge there's a lot of
plants that are possibly going to
destroy the ecology of this area and
what we can do is we can identify those
images with those invasive plants and
the way I'm going to do this is using PI
torch
okay so PI torch is shout out to all the
PI torch lovers in the house right now
PI torch is a neural network library and
it was invented by Facebook's AI
research lab and it's been gaining a lot
of traction especially in the research
community
now tensorflow is the leading you know
neural network deep learning library out
there and most people use tensorflow
however when it comes to research I'm
seeing a lot of pi torch happening and
in this video I'm gonna describe some of
the unique features of Pi torch we're
gonna code some PI torch we're gonna
solve this challenge we're gonna learn
about resin its res residual networks
which is a type of neural network that
has this really cool idea of skip
connections so I'm gonna talk about that
there's an identity mapping there's some
math there's a little bit of everything
you know you guys know you you guys know
what the deal is okay so let's get
started with this thank you everybody
for coming here today I've got a little
yeah yeah PI torch is amazing right
alright so here we go so pi torch versus
tensorflow right I've got this little
graph of like research use cases versus
production Google versus Facebook but
let's talk about some of the unique
features right so we're gonna just
compare it with tensor flow just because
tensor flow is the you know the most
used in terms of developer activity on
github library out there if we look at
the documentation I think tensorflow
clearly wins here I mean the tensor flow
documentation is massive because a it's
been around so long be they're really
putting an effort into it and when they
when I say documentation I'm not just
talking about the API Doc's on the
website I'm talking about github I'm
talking about number of examples I'm
talking about video content I'm talking
about a lot of different documentation
whether it's on YouTube you know
whatever tensor flow clearly wins
whereas if you go to the pipe
or CH documentation as I as I am right
now let's make me a little smaller on
the corner so we can see this
documentation if we look at the PI torch
documentation if you look at any of the
functions well these are these are like
guides but let's look at some of the
functions like Auto grad for example
they're usually just a single page for a
function whereas you know I feel like
there should be more here but yeah I
think such a flow wins in terms of
documentation but where PI torch wins is
when it comes to the graph itself the
computation graph so this is an image of
a neural network that was built in
tensor flow that we're visualizing in
tensor board which is the visualization
tool and if you think about it
neural networks are computation graphs
there are just chains of operations
applied to variables and it's just a
just a chain of that over and over and
over again right whether it's a dot
product operation whether it's an
identity mapping whatever it is you have
a variable apply an operation and just
keep doing that until you get an output
so there's a difference here right so
what PI torch does and this is what I
believe is their really the key feature
of pi torch what really sets it out from
the from the competitors here or the
other libraries is the fact that you can
create a dynamic graph so it's it a
graph is created at runtime dynamically
so just like this as you're saying in
this gif here at runtime the graph is
built whereas with tensor flow it's a
static computation graph okay so at
runtime the graph already was compiled
at compile time and now it's just static
right there so why why did tensor flow
do this well first of all for production
use cases it's very valuable to have a
static computation graph and there are
speed reasons for that there's
distributed training reasons for that
but a lot of the newer neural network
architectures in research especially any
of the recurrent models right where
there's this recursion happening it's
better to have that graph be
de-allocated dynamically at runtime so
the graph is built as it's being run and
the reason for this is because a lot of
times new operations are applied that
wouldn't be applied before so that's the
whole idea behind a dynamic graph right
you can make changes to it there's no
static
this status to City what's the word for
looks like the noun for static well
adjectives anyway it's more static and
so when it comes to these newer
recurrent architectures with attention
mechanisms with you know LS team
networks gru networks and now with the
the new one which I have an amazing
video coming out on by the way Nalu
which you know
Trask released a paper on I know you
guys are waiting for that I've been
waiting for that but don't worry it's
coming on Sunday it's a sick video I
have so many videos coming out this
weekend you have no idea
open AI 5 it's it's gonna be literally
social media will light up when these
videos come out by the way now back to
this the computation graph is different
you have a static versus dynamic graph
so PI torch wins in this case however I
want to note that that nalu exactly I
want to note that tensorflow did like
kind of tack this on later with the
imperative graph and that works too so
there is a way however it's not native
in the way that it was with PI torch for
visualizations tensor board clearly wins
pi torch has got nothing at least from
the PI torch team I don't know about any
like random rogue hacker on github who
created some this library but tensor
board is king here tensorflow wins for
debugging ok PI torch wins ok Pytor
twins for debugging because there's a TF
debugger called a TF dbg but a lot of
people there's a lot of github issues
about it not working about it not
properly documenting things that are
going wrong whereas people just tend to
generally love PI torches built-in
debugger so PI torch wins there right so
rather than like you know just saying
print print print print prints you can
actually use these you know set trace
functions that say PDB has which is
beautiful it gives you a very detailed
stack trace about what you need
and lastly interoperability right so
sometimes we can't just do everything
natively in those languages we have to
switch to C++ or CUDA and we need a
language that can talk to those in a
very easy-to-use way and in terms of
interoperability
I think pie-chart wins here because it's
all about writing into
face code that among the different
versions of CPUs and GPUs and as you can
see here it's very easy to have an
interoperable graph okay so that's those
are just some of the features that I
wanted to talk about and now before I
actually before I actually I so guys I
want to say I'm not the Messiah by the
way because I have been watching wild
wild country on Netflix and I don't want
to be that guy I am NOT a God I'm one of
you I'm a developer and don't worship me
as a god okay cuz that's not what we're
doing here I am just another developer
and we don't want that to be the case
okay so anyway let's code out some pie
charts if you want to call me a god
that's fine though no I'm just kidding
anyway so let's let's just code out some
simple pie torture so all we have to do
is say import torch and we have some PI
torch going on see I just compiled that
okay so now what I want to do is I want
to build a simple neural network so we
could just see like how this works okay
so I'm gonna do this really quickly I'm
gonna I'm gonna hack out a script here
using PI torch and once we do that then
we're going to have our neural net
okay so simple to layer Network okay so
so far the torch class has a module and
I'm gonna show you exactly what that
module is gonna let us do we can build a
linear layer very easily uh right
exactly yeah you guys are hilarious okay
so right we've got to have the super
function as in any like initialization
we've got to have our super function and
once we have our super function to
initialize our constructor we can say
now after I build this we're gonna start
looking at some we're gonna do some
exploratory data analysis but I first
want to just write out this script so we
have a we have something there for us
okay so in my initialization script I'm
going to be using the input I'm gonna
have it hidden state and I'm gonna have
an output right so D in H and D out
that's my input the hidden state and my
output and once I have that now I can
create this super function and so inside
of this
I'll have my first linear layer so
torches neural-net
module allows us to create linear layers
just like this now the inputs to the
linear layer is going to be the input
data and then its output is going to be
the hidden state okay and I see that
some people say that I have a battery
issue well that the battery issue is
gone all right so back to this so for
our linear layer and we want to make
sure the code is very readable so what
I'm gonna do is I'm gonna put it right
here and I hope it's very readable to
you okay
I hope let me see how much I can zoom in
I can't really please doom into the code
all right let's see you know what let's
just let's just strip this out and like
sublime or something so you guys can
really see what that's some naalu code
don't look at Nalu guys I was just
making this totally look at this sick
code by the way I was anyway where were
we right so let me make this bigger and
we make it in Python and then we're good
we're good to go
so that way because sometimes with you
know these like okay no I'll just call
it torch dot PI put around the desktop
boom
torched okay so torched by board yawn
laoco√∂n on twitter okay so now we have
the hidden state and now we're gonna
have the output right so the input to
the new hidden state is the the input to
the next layer is gonna be the output
from the previous layer we know that
okay so very simple stuff now what else
we need we need a forward method so for
some reason PI torch code usually names
their forward propagation method as
forward instead of just like build model
usually this function is called build
model but a lot of Pi torch code uses
forward and I think the reason for that
is because forward propagation is
dynamic is right it's happening at
runtime so people who are writing PI
torch code want to be very specific
about what they are doing and to be
specific and to be real it is better to
call you know whatever
it's better to call it forward because
that's what it is okay so I've got the
forward code now I need to have my what
I need a batch size so that's N and his
batch size I'm gonna have our input my
hidden States and my output and then
it's gonna be called 64 1110 so these
are gonna be my names for my number my
batch size so now I can create some
random tensors so the great thing about
torch is it's got it's kind of like
numpy is built into torch so we don't
actually have to use numpy numpy is that
basically built into torch so any kind
of like matrix multiplication that we
want to do
it's all happening inside of pi torch
okay and let me just see what else
people are going in here
keep it fun cool zooming yeah great much
love to everybody as well of course of
course
by the way Dean applications I'm where
I'm we're almost done reviewing
everything and that's coming out this
weekend
so there's a lot that's a lot that's
happening I also have my masterpiece
that's coming out but we can I can't
talk about that right now my
masterpieces is coming out soon don't
worry about it right so so we created
random tensors to hold inputs and output
so what are tensors tensors are
n-dimensional vectors right so you have
one dimension that's just a group of
numbers if two dimensions it's you know
two-dimensional it's a it's an Excel
spreadsheet three dimensions four
dimensions five you know etc so tensors
is a generalization of the vector format
for n dimensions which is what we need
because we don't know how many
dimensions are input data is going to be
and so now that we have our model we can
just build it just like this using our
input or hidden state in our output as
the values by the way if you're new here
hit the subscribe button to stay up to
date with my latest AI content okay
all right so and tell your friends
because I'm not going anywhere I'm gonna
be doing this until I will never stop so
get ready for this we have our loss
function our loss function we can choose
between multiple loss functions can
anybody guess what I'm going to use
right we have cross-entropy we have
what's what's the most
simple loss function that we could use
here you should know this if you've
watched at least three of my videos you
should know this
the answer is the mean squared error
loss why because it's the most simple of
them all for for demo purposes so that's
what right we're gonna find the mean
squared error we're gonna find a
difference between our output in our
what is pi torque useful for for
research for research not for
necessarily for production but for
resource for research now what's our
optimizer gonna be we all should know
this
every single person even if you've never
seen one of my videos to notice the
answer is stochastic gradient descent
the most important algorithm in machine
learning if you don't know how gradient
descent works my friend you need to know
because it is the most important machine
learning model that you need ever in in
machine learning okay or our algorithm
now okay so I've got that I think this
is supposed to be yeah right
no now lastly we're going to have our
training loop and then we're good so so
for 500 iterations we're gonna perform
our forward pass so why predict rmse
Adam I see we have some opinionated
wizards in here which I love to see I
love to see that so that's gonna be our
forward pass we're gonna compute the
predicted Y and once we have our
prediction what are we gonna do guys we
know how supervised learning works right
we've done this so many times if we
don't know how supervised learning works
this is the first step if you're new to
machine learning focus on supervised
learning which is what I'm doing right
now
once you finish supervised learning then
you move on to unsupervised learning and
what do you do when you finish what what
do you do when you finish unsupervised
and supervised learning who's who's got
the answer for me I'm gonna wait for in
the comments someone is gonna say the
answer to what you do after
you learn both supervised and
unsupervised learning because I know
somebody is gonna know exactly what it
is okay so now I'm going to and I okay
cool duh now what did I call this so in
my desktop I call this torch PI module
attribute has no attribute exactly
reinforcement learning yes good job
everybody reinforcement learning module
has no attribute nn yes ok ok ok ok let
me see what's up uh let's see here
Oh gotcha gotcha
torched uh NN module let's see here hmm
I wonder if I do this in collab what the
deal is
well actually I need to we just paste it
in here this usually goes for oh right
spaces and indentations and stuff right
right def forward oh my god
deep learning exactly let's try that out
oh my god see and now you're gonna watch
me debug in real-time guys because I've
got some cattle to do I've got some
cattle to do oh my god mm-hmm-hmm this
is not okay
guys I'm talking about pie torch pie
torch mm-hmm i torch examples not collab
see I was just searching that you can
see a bit of things that I was searching
duh d'duh I'm looking now for in the
meantime just ask some questions I'm
gonna answer while I well I get to sup
and running so just uh we've got some
great examples by the way I mean look at
the amount of examples pi torch has for
some of the most advanced models out
there I'm talking about variational
autoencoders my favorite supervised
model variational auto-encoders it's it
can actually be unsupervised as well
let's see what pi torch has here a
60-minute blitz what is pi torch damn
look at that neural networks right
oh right duh that's what it was I just
needed that right no modulename nn-no
module import torch done in my god
exactly in Soviet Russia that's what I
like to hear we've got a lot to do guys
I wanted to give a very basic PI torch
example before stepping into the
exploratory data analysis we've got a
lot packed in for this session seriously
what am I missing here let's see oh my
god okay let's see let's see let's see
seriously see this is that demo thing
where it's like the code is working and
then right when you get to the demo
everything just kind of i'm gonna
reinstall pi torch i mean if that's what
it's got to be I'll reinstall pi torch
okay all right
check the chat oh is the torch for
object you created it struck with the
reference or a numpy array or does it
use its own vectors to create an array
it's using its using numpy under the
hood I'm pretty sure it is actually
actually that's what I assumed I assumed
it was using its own version of numpy
but no no it's it's own thing nd arrays
are similar but it's not numpy that's
very cool okay what I want to do you
know what is check out collab and inside
of collab really oh right that could
help write fun dot pi yeah
nope alright let's see alright so we're
gonna use a new collab so Google collab
check it out if you haven't by the way
we can install pie torch like this into
collab now anybody can do this so we
just installed PI torch and once we
install pi towards we're gonna install
we're gonna see if we can you are seeing
some real time debugging by the way
let's see on a GPU this is definitely
going to work there we go we are in
samba let me answer some questions while
we're waiting here renamed torch PI
collab is the best isn't it you have
questions while we're while we're
waiting for this to load so what is
one-shot learning so one-shot learning
is the dream it is the dream of all
machine learning to be able to learn
from little from from small datasets and
what has been done in that on that front
memory augmented neural networks MA and
ends I think Facebook was probably not
the first but the most successful in
marketing as a as a research lab a
one-shot learning algorithm it's called
the memory augmented neural network it
came out about two years ago and they
did it with what I think not the MN is T
data set they did it with the Omniglot
data set which they proved that they
could classify images with only like I
think under 50 samples so if you want to
learn more about one-shot learning check
that out what is this tool of AI next
week so that's what I have to say about
that
seriously check this out see guys
sometimes sometimes you know in real
time you got some issues and you got to
just like exactly I think that worked
did not work
maybe that worked let's see now
so now we're gonna have some PI torch
happening yes good good oh my god yes
all right so let's see anyway okay we
got we got to keep going guys we have a
lot more things to code which are not
going to fail by the way so I wanted to
give you that this is going to work
check that out
no GPU seriously no GPU is available oh
we've got to activate the GPU torch
double to CPU Wow there's also this
thing called cago kernels which I'm
going to talk about as well okay let me
just place this in here let's see now
okay oh right right right notebook
setting where's that oh cool there we go
thank you okay so what I'm gonna do is
I'm like determined to make this work
now like I don't even care I'm like I
actually have a vendetta against this
because like it was totally working and
now it's not okay so where were we
I literally where did I get this from
where did I get this from there we go
this is the deal now
no modulename a torch install torch yes
we installed torch we did that and now
once we've installed torch in a new code
snippet I'm gonna have our code run
while this is installing answering some
questions here change the Python for
perfect perfect advice
there's no Python for okay well I did
that I did that that was literally like
while this is compiling we got to keep
going here let us do some exploratory
data analysis guys we have our data set
let's look at our problem and we're
gonna get back to torch we're gonna get
back to torch while this is running so
what do we have here let's look at our
data and then we're gonna look at
kernels as well we have our data which
is going to be a set of images of
different plans we have labels for each
and I went ahead and downloaded these
and so what we can do is look at our
data so let's see if we can do this
let's start off doing this locally and
then we're gonna we're going to go
further so let's write out a read image
function okay so we want to read images
and show them right here so that's the
first thing I'm going to do so I'm going
to say using OpenCV and I got to have
two OpenCV I have to import OpenCV which
is called cb2 OpenCV can be kind of a
pain to install so if you don't have
this installed locally I would recommend
using either collab or Kaggle kernels to
do this in the browser
but if you do have it locally then
things can be easy or for this okay so
that's a read image file and I'm gonna
have a show image I mean function now I
have a show image function and once I do
that
then I can show the image and I'm gonna
have to import map map plot live for
that import mat plot live pipe lot as
blt and I have both of those and this is
going to be a little percentage marker
that makes sure that it's in line okay
so now let's show some images so let me
see if that works
invalid syntax CVT color okay good
now let's see sample pic equals let's
take one of our samples from our local
directory which I have and call let's
just pick one let's say maybe like
number one the first jpg let's see if we
can show this in the browser image is
not defined right so the image is going
to be no not that sample pick show image
sample pick okay and now we can see it
yes okay so here's our first sample and
now we can we can kind of browse and see
what we have in our data set right
there's a bunch of images here okay let
me let me look at some more images right
and these images have an Associated
label that we're gonna now we're gonna
now look at okay so let's see how many
invasive versus non invasive images do
we have right so let's let's let's draw
a graph of that so we're our label so
our labels using a panda's data frame
let me import pandas as well import
pandas as PD okay so our data frame also
let me answer some questions what do we
have here
document clustering LD a latent darish
Flay allocation perfect method for that
it's a part of Andrew hangs machine one
in course what is Auto ml it is Google's
map aramet ur tuning framework in the
browser on google cloud pillow is old
pill is better you're right do you think
nowadays python is sufficient to use ml
for robotics or do we also need c++ and
c that's a great question i mean when it
comes to robotics you're dealing with a
lot of hardware and unfortunately at the
lowest level when it comes to reading
like these these in these drivers and
these like hardware inputs manually a
lot of that a lot of those libraries are
still in c so or c++ so if you're doing
a robotics specifically like physical
robots not even in simulation then i
think you need some c++ knowledge to be
real just don't think you'd get through
the whole thing with python that would
be a dream it's happening we're not
there yet okay so we have our training
labels and now we want to see how much
of each do we have right so we need to
import Seabourn Seabourn's gonna help us
visualize this
it's another data visualization library
okay
so what am i gonna have this style be
let's have it be a dart grid and we're
going to say the x-axis is gonna be the
invasive plants that we have and the
data is going to be our labels that we
just imported into a panda's data frame
and now using map plot live we can give
a title to this graph just like if we
had to like do a report or something on
this will say invasive versus non
invasive images yeah then we have our X
label be our invasive I guess we can
specify a font size let's say like 20
and then our Y label is going to be of
course the number right so the images
versus the number and we'll make it the
same font size I know I misspelled font
size there
and okay so let's see what that looks
like
of course there's a okay okay okay okay
right there we go yes good okay good so
what do we have here
count invasive versus non invasive
mmm-hmm good
so clearly clearly we have much more
invasive images where one is invasive so
that's that's a good thing to note okay
good that this is what we want you know
ideally it's like half and half so we
can see like what what has an ideal
ideally ideally we have a lot of data
but you know whatever I think this isn't
no this is enough data because this is
an older competition and if we look at
some of the kernels we can see that on
cattle we can see that it's been it's
been done uh comma right right so what
kind of model should we build here right
so using PI torch what kind of learning
model should we build here to build a
classifier now there's a lot of
different convolutional nets out there
there's you know alex net inception
ResNet vgg sixteen vgg nineteen there's
like a million of them and there's not a
million there's like ten ish like but
dense net what we can do is we can try
them all out but one that I have not
talked about before and I'm gonna take
this opportunity to talk about it is res
net so residual networks okay so let me
talk about that really quickly and then
we'll build that
okay we'll build a resident Orca texture
so um if we if when we're using
recurrent networks there's this there's
a problem that occurs called a vanishing
gradient problem now if you want to
learn more about that just Google
vanishing gradient Siraj at least
through videos will show up on YouTube
but TLDR when we are back propagating a
neural network using some kind of
a gradient descent based optimization
strategy like Adam you know stochastic
gradient sent all sorts of great descent
variations the gradient gets smaller and
smaller and smaller as that value is
back propagated right so back propagated
means you know this is the value that's
going to tell our weights how to update
to be better at predicting the output
the the the real output and so we are
using calculus to optimize remember
neural networks are models created with
linear algebra and they are optimized
with calculus so that's where you need
calculus you need calculus to optimize
and you need linear algebra to build the
models and to perform those operations
on them so during so the reason we use
calculus is because where you were
computing the partial derivative of the
output with respect to the weights the
partial derivative of the error with
respect to the weights and we keep doing
that for every layer so the problem is
that this gradient vanishes and this
image right here shows how you notice
how it's getting more and more
translucent over time that gradient gets
smaller and smaller smaller so these so
to be real that so the gradient is
actually a vector of the partial
derivatives that are computed right
that's the gradient and those values get
smaller and smaller and smaller so they
diminish over time so this is a problem
because the network is not it needs that
gradient value you can even think of the
gradient as a kind of memory right so
lsdm network solved this for a recurrent
networks but how do we do this for
convolutional networks for image based
networks so some researchers had this I
think it was a genius idea to literally
just skip over some of the intermediary
layers and have the by skip over I'm
talking about have the gradient skip
over intermediary layers and by doing
that it's not going to diminish you
literally just skip them over what do I
mean let's talk about this
mathematically so in the ResNet
architecture we have a bunch of what are
called convolutional blocks these are
standard convolutional blocks right so
convolution pooling activation that's
inside of a block and you repeat that
convolution pooling an operation and
an activation function and repeat repeat
repeat repeat but what if we can add
what's called an identity mapping so
what you're saying on the left here is
the the novelty of ResNet where we took
f of X and we modified it by adding an
extra dot product operation to it and
this is called the identity mapping so
what better way to explain the identity
mapping and then saying well first of
all we have our function f of X and we
are changing it to f of X plus X where X
is the identity mapping and this
modifies the original operation such
that the gradient will not diminish over
time so in chaos here's a very simple
example programmatically of what I mean
by a residual block so in the residual
network we are renaming a convolutional
block to a by the way don't leave I'm
gonna wrap at the end by the way we are
renaming the convolutional block to a
residual block by adding in this
identity mapping so normally we would
just say this right here right so in our
block we'd say convolution you know
normalization activation function which
is really Lu you know in this case which
helps with the vanishing gradient
problem for other reasons there's so
many like little explanation tangents I
could be going on here by the way
there's so much here but we are what
we're doing here is we're adding this
part right here so what I have
highlighted is the identity mapping it's
what is added to the initial set of
operations so notice here that the
shortcut is actually if first of all we
have to specify that we want this to be
a shortcut right so we're going to say
true if we want this to be a true
residual block and if we do this this
this adds this shortcut operation to it
right here and then we just and then
once we've computed that then we add it
to the what was initially the the chain
of operations and then we return that
block as a whole so notice that it's not
necessarily that something is being
replaced here it's an extra operation
that's added to the existing chain of
operations but what this does is we can
think about it as skipping over because
it's it because it's not a zero mapping
it's actually adding value to it
because it's the reason it's like
skipping over is because whereas the
value would diminish over time because
we're adding value to that existing
chain of operations it's a non zero
value which is what we want we don't
want a zero value for our gradient so
that's the idea behind a residual
Network the original mapping f of X is
recast into f of X plus X and it's the
theory was that it's easier to optimize
a residual mapping than to optimize the
original unreferenced knapping and that
here's the the paraphrasing of what I've
just said okay so it is a good place to
start learning AI yes now in terms of
hints about what's up to what's coming
up I would say a reinforcement learning
but that's all I can say right now so
EDA we did that ResNet okay so let's
let's do this now right so but first of
all let's see what taggle has for us so
cago recently released well they kind of
renamed what already existed but they
recently released this idea of a kernel
okay so a kernel is basically like
google collab but it's built into cago
by the way Googlebot Tagle so it makes
sense that they're using the same
infrastructure so if we look at a kernel
here let's let's let's see what they
have here for kernels this is the most
voted kernel for this for this data set
is saying use Kara's pre-trained vgg 16
so if we open up this kernel I'll go
ahead and force the notebook it's got
literally everything I need to run this
in the browser okay so what this is
doing is it's in the browser it's got a
notebook it's got a Python compile it's
got a Python interpreter in the browser
it's got all the dependencies I need for
the specific data sets it's got the data
set itself is imported right here that
you can see as input right here with all
those which is super handy right we
don't have to download test dot 7z we
don't have to unzips test out 7z then we
don't have to combine it with the labels
it's all already there which is super
useful now this is very cool but we are
not doing chaos right we're doing PI
tort so let's go back and
let's look for a pie chart one if we can
find one can anybody see a pie chart
here we go pi torch starter 0.98 that
looks pretty good so let's click on that
one and see what we got here
fork notebook this guy went ahead and
did this in PI torch which is super
awesome and how are we doing on time
cool Wow
time flies when you're when you're
debugging all right so 48 minutes okay
we got to end this before it's too late
all right so now what we can do is we
can just run it in the browser notice
this just compiled in the browser numpy
pand as we employ that okay what about
torch notice how I was having a problem
before
ain't no problem anymore it is all there
I'm compiling this code compiling
compiling now what is he using here by
the way okay so he's transforming the
data great he's loading up the data cool
l our scheduler that means a learning
rate scheduler he's deciding what type
of learning rate to use here based on
the number of looks like a predefined
threshold of number of epochs he's got a
pre trained wait file that he is so he's
doing transfer learning cool and he's
using dense net okay dense net that's
cool and he's our sample submissions
submit and train the network okay cool
well this is going to take a while
because training neural networks is not
something that just happens right so
anyway check that out so that's PI torch
in the browser everything you need
preloaded with the data set super useful
now what are we gonna do here well we
well we need to replace dense net with
res net but and also explain what dense
net is and how that works but it looks
like we've run out of time go ahead and
answer ask some questions I'm gonna end
this with a wrap but ask some questions
the GG is yes but we but we need to
understand how res networks ok Lua is
awesome love you too what is you net you
net great idea jo you net is in
because it's it's kind of a reflection
of itself in the later layers we'll talk
about that pooling is a great example
pooling is not all the data and the
input is relevant to the output so what
pooling does is it chooses what's the
most relevant so what is most mean while
most in the case of max pooling can be
what has the most what's just the
biggest number in some region of the
input and then we do that so that's max
pooling and that's generally the most
used pooling operation in neural
networks there are other kinds as well
can you talk about learning rate
schedule or API for PI torch I'll talk
more about PI torch where can we learn
more about PI torch guys so what I do
when I'm learning about any kind of code
is I don't look at anything but github
right I just go straight to github and I
look at some example code and hopefully
the example code is well documented this
is the repository to look at
reinforcement learning is probably what
I look at because that's what I'm most
interested in right now this is cool
okay I mean it could have better
documentation but that works now I'm
going to end this with a wrap and I'm
gonna just pick some random word from
the from the comments let me see here
for okay
helpful bit acrobatics okay alphabet
aerobics okay cool I found it I never
heard this before
are you serious guys you guys are
trolling me aren't you all right let's
do this I'm waiting for the beat
seriously is this even a rap like
okay fine all right wrap on Elon Musk or
going I liked golang I do it no no
restart I like go Lane no no see hold on
this this is just the most wack beat
I've ever heard I how how is anybody
supposed to rap over this it's getting
faster as well there's not even like a
set tempo here it's just I'm gonna just
do it anyway here we go
I like golang I'm slow made I like do it
like I'm a long must main it's going
faster and faster I like concurrent nisi
I keep my go routines running you see
yeah go routines this is so wack dude
this is seriously like I'm not even
gonna I'm just gonna occupy I'm just
gonna acapella this
I like golang it's got concurrent model
it's got concurrent subroutines that's
how it goes you see what I mean I do 1 2
3 it's distributed can't you see I do it
across different TP use and GPUs I do it
on my CPU I do it on my alright that's
it for this for this livestream guys
that is how we do it we don't we
literally don't care if we succeed if we
fail if we do both because we just keep
on going no matter what
nothing will stop us we are going to do
so much good for the world and thank you
for tuning in for now I've got to make
some videos so thanks for watching
[Music]
[Music]
[Music]