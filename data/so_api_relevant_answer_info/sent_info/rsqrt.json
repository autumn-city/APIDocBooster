["(Maybe this applies to only rsqrt here but it is an expensive calculation (it also includes multiple multiplications) so it probably applies to other calculations too)\n", "Also sqrt(x) is faster than x*rsqrt(x) with two iterations, and x*rsqrt(x) with one iteration is too inaccurate for distance calculation.\n", "So the statements that I have seen on some boards that x*rsqrt(x) is faster than sqrt(x) is wrong.\n", "So it is not logical and does not worth the precision loss to use rsqrt instead of sqrt unless you directly need 1/x^(1/2).\n", "rsqrtss gives an approximation to the reciprocal, accurate to about 11 bits.\n", "rsqrtss exists for the cases when an approximation suffices, but speed is required.\n", "edit: If speed is critical, and you're really calling this in a loop for many values, you should be using the vectorized versions of these instructions, rsqrtps or sqrtps, both of which process four floats per instruction.\n", "The difference might lie in how sqrt and rsqrt are computed.\n", "I'd suggest to start from reading about processor functions you are using, there are some info, especially about rsqrt (cpu is using internal lookup table with huge approximation, which makes it much simpler to get the result).\n", "It may seem, that rsqrt is so much faster than sqrt, that 1 additional mul operation (which isn't to costly) might not change the situation here.\n", "Once I was doing some micro optimalizations for my graphics library and I've used rsqrt for computing length of vectors.\n", "(instead of sqrt, I've multiplied my sum of squared by rsqrt of it, which is exactly what you've done in your tests), and it performed better.\n", "Computing rsqrt using simple lookup table might be easier, as for rsqrt, when x goes to infinity, 1/sqrt(x) goes to 0, so for small x's the function values doesn't change (a lot), whereas for sqrt - it goes to infinity, so it's that simple case ;).\n", "Also, clarification: I'm not sure where I've found it in books I've linked, but I'm pretty sure I've read that rsqrt is using some lookup table, and it should be used only, when the result doesn't need to be exact, although - I might be wrong as well, as it was some time ago :).\n", "The rsqrt* instructions compute an approximation to the reciprocal square root, good to about 11-12 bits.\n", "If 11 bits of precision are enough (no Newton iteration), [Intel's optimization manual (section  11.12.3) (hyper-link)] suggests using rcpps(rsqrt(x)), which is worse than multiplying by the original x, at least with AVX.\n", "This means it should have a throughput on Haswell of one per 3 cycles (given that two execution ports can handle FP mul, and rsqrt is on the opposite port from FP add/sub).\n", "On SnB/IvB (and probably Nehalem), it should have a throughput of one per 5 cycles, since mulps and rsqrtps compete for port 0.  subps is on port1, which isn't the bottleneck.\n", "However, the dividers / sqrt unit isn't 256b wide, so unlike everything else, divps / sqrtps / rsqrtps / rcpps on ymm regs takes extra uops and extra cycles.\n", "This is offset by using 2-cyle-slower 256b rsqrt, for a net gain of 1 cycle less latency (pretty good for twice as wide).\n", "(VRSQRTPS is 3 uops, 2 for p0, and one for p1/5.)\n", "the shuffle uop from rsqrt always goes to p5, never to p1 where it would take up mul/fma bandwidth.)\n", "sqrtps can be a throughput win vs. rsqrt + Newton iteration, if it's part of a loop body with enough other non-divide and non-sqrt work going on that the divide unit isn't saturated.\n", "on Haswell with AVX2, a copy+arcsinh loop over an array of floats that fits in L3 cache, implemented as fastlog(v + sqrt(v*v + 1)), runs at about the same throughput with real VSQRTPS or with VRSQRTPS + a Newton-Raphson iteration.\n", "There is a speedup from using just fastlog(v2_plus_1 * rsqrt(v2_plus_1) + v2_plus_1), so it's not bottlenecked on memory bandwidth, but it might be bottlenecking on latency (so out-of-order execution can't exploit all the parallelism of independent iterations).\n", "For double-precision, there is no rsqrtpd.\n", "If you are going to convert back to double right after, it may make sense to do the rsqrt + Newton-Raphson iteration on the two 128b registers of singles  separately.\n", "The extra 2 uops to insert / extract, and the extra 2 uops for 256b rsqrt, start to add up, not to mention the 3-cycle latency of vinsertf128 / vextractf128.\n", "With AVX512F, there's _mm512_rsqrt14_pd( __m512d a).\n", "With [AVX512ER (KNL but not SKX or Cannonlake) (hyper-link)], there's _mm512_rsqrt28_pd.\n", "rsqrtss is safer and, as a result, less accurate and slower.\n", "Why is rsqrtss safer?\n", "Why is rsqrtss slower?\n", "Why does rsqrtss use a reciprocal?\n", "What is rsqrtss?\n", "You could easily compute rsqrt on the fly here if the source data is contiguous.\n", "Otherwise yeah, copy just the diagonal into an array (and compute rsqrt while doing that copy, rather than [with another pass over that array like your previous question (hyper-link)].\n", "Either with scalar rsqrtss and no NR step while copying from the diagonal of a matrix into an array, or manually gather elements into a SIMD vector (with _mm_set_ps(a[i][i], a[i+1][i+1], a[i+2][i+2], a[i+3][i+3]) to let the compiler pick the shuffles) and do rsqrtps + a NR step, then store the vector of 4 results to the array.\n", "There is no function in the standard library that does this, but your compiler might optimize the expression 1 / sqrt(value) such that it does emit the RSQRTSS instruction.\n", "The RSQRTSS instruction is readily accessible via the _mm_rsqrt_ss() intrinsic declared in immintrin.h.\n", "But we can also emulate the instruction in software, as is done in the my_rsqrtf() function below.\n", "By simply observing the output of RSQRTSS one easily finds that its function values are based on a (virtual) table of 211 entries, each 12 bit in size.\n", "There may be some functional differences in the implementation of RSQRTSS between various Intel architectures, and such differences are likely between architectures from different x86-84 vendors.\n", "The framework below checks that the emulation by my_rsqrtf() delivers bit-wise identical results to RSQRTSS for all four rounding modes, two DAZ (denormals are zero) modes, and two FTZ (flush to zero) modes.\n", "We find that the function results are not affected by any modes, which matches with how Intel specified RSQRTSS in the \nIntel\u00ae 64 and IA-32 Architectures Software Developer\u2019s Manual:\n", "The RSQRTSS instruction is not affected by the rounding control bits in the MXCSR register.\n", "Or maybe just use rsqrt on the fly when loading this data later.\n", "Hopefully gcc will do something like this while unrolling, otherwise the rsqrtps + movaps will be 4 fused-domain uops on their own if they still use indexed addressing mode, and no amount of unrolling will make your loop run at one vector per clock.\n"]