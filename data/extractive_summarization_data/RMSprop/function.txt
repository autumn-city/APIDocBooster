 That means, it tries to make the learning rate inversely proportional to a sane cumulated history. 
RMsprop keeps the exponentialy decaying average of squared gradients. 
RMSprop uses a momentum-like exponential decay to the gradient history.  Gradients in extreme past have less influence. It modiﬁes AdaGrad optimizer to perform better in the non-convex setting by changing the gradient accumulation into an exponentially weighted moving average.
 So, that is the problem with the Adagrad  algorithm and in case of RMSProp instead   of using this cumulative or acumulative sum of  squared gradients the RMSProp takes exponentially   decaying average of the squared gradient.  And, it  does not consider the extreme past histories while   accumulating the sum of square gradient.  And, as  a result of this the algorithm converges rapidly,   once it reaches once your vector reaches  locally convex ball short of surface that .  And,   what you can do is we can assume this point once  it reaches that locally convex error surface,   that as if your Adagrad algorithm is initialized  at that point within that locally convex ball. 
However, RMSProp does not keep a moving average of the gradient.  But it can maintain a momentum, like MomentumOptimizer.
