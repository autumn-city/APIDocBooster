hi, this is Abhilash Majumder a research scientist from Morgan Stanley previously.
I was working for HSBC as a deep learning and NLP engineer.
I have also collaborated with Google research in matters
of Bert and Albert models and other language models.
I have mentored students and professionals for organizations
like Udacity and UpGrad in terms of NLP, AI, deep learning.
So I have quite some exposure in mentoring and, um today
we are going to talk about and Encoder Decoder models.
So if you, and also welcome to Co-Learning Lounge YouTube channel.
So in this channel, we'll be posting regular updates on different complex
topics of NLP, like attention, BERT, and state of the art language models.
along with this, if you want to connect with me, you can
find the details in the description, in the link given below.
Also feel free to comment and also subscribe to this channel
and let's get started with the encoder decoder architecture.
So we had covered LSTM GRUs in the last video.
So building on top of it, we'll be moving towards the encoder decoder model,
which is nothing but a sophisticated neural architecture, comprising of
different stacks of LSTMs systems, different variants of LSTMs, Jameson GRUs.
So in this case, We'll be seeing what are the use cases of these encoder
decoder models since the invention of sequence to sequence model by Illia?
So from OpenAI the the the details of neural machine translation and also the important
features of text classification, question answering semantic inference, all these kinds
of NLP tasks have again, massive popularity because of this sequence to sequence model.
It gradually in the context of neural machine translation, this has achieved a great performance
where we want to map certain words from a different language to another different language.
In the example, given over here how are you is an English and knew how ma is in Chinese.
So this is achieved with the help of sequence to sequence model.
And we are going to study a variant of the sequence to sequence
model that is known as the encoder decoder architecture.
In the 2016 paper from Google, which was named as the drank, aligning to translate.
I knew it was one of the most important papers which gave birth to this idea of encoder decoder.
And concept called us attention, which we'll cover later in our video tutorials.
Now coming ourselves to the basic architecture of encoder decoders, these are nothing but a
stacks of certain recurrent neural networks, which are couple on back of each other, like a stack.
So when we move forward to a general sequence to sequence model, it is generally
refers to mapping certain parts of vocabulary to certain parts of vocabulary.
So this model aims to map a fixed lens input with a fixed length
output where the length of the input and the output may differ.
So basically in the case of language models, where machine translation
is very much required, different languages in different may have
different side lengths of the sentences when they're translated.
Such as what are you doing today in English?
the Chinese version of this particular sentence of this particular
question has only five words and an output of seven symbols.
So clearly this general LSTM, GRU architectures do not you know, sufficiently, fail.
they actually fail to actually scale or try to reproduce these kinds
of results because LSTM networks tries to map each word to each word.
So they will tend to have an equal sized input and an equal size
output for mapping each of the input words to the output words.
But what will happen if we want to.
If we want to classify different sizes of inputs and different
sizes of outputs, which is the case of neural machine translation.
So in this case, English to Chinese translation is not achievable with
just a plain or a vanilla and LSTMs network or a vanilla GRU network.
Or any versions of RNN.
So we consider ourselves with the included decoder model, and this is the
diagram of standard encoder decoder model, where we have stacks of RNN cells.
These RNL cells can be LSTM cells.
This can be GRU cells as well.
So in basic sense, these are generally comprised of bi-directional LSTM cells or bi-directional
So generally we have the X as the input features and we have the
hidden state from the previous timestamp of the previous cell.
So these can be any variants of far, just like I mentioned, either LSTM
or GRUs, if we're considering LSTM, then each of the intermediate hidden
layers, each of the intermediate hidden cells will have two outputs.
That is the H and the C that is the hidden cell state and the current cell state.
H & C, but if we were using  then as we know, In GRUs,
the two hidden States that is H & C are merged together.
So we will have a single, hidden state that is known as H.
So depending upon the architecture, so the internal neural network structures that we use
LSTMs will have intermediate to outputs, whereas the geodes will have intermediate on outputs.
So in this, in this stack of LSTM cells are.
GRU cells.
The input gets passed one after the other, for each particular LSTM cell.
This is denoted by X one X two and X three.
Whereas output of the hidden cell is also transmitted one
after the other one cell after the other, for each timestamp.
This is denoted by H one H two and H three.
And in the intermediate part of this stack of cells, these
are returned as sequences rather than distinct outputs.
This part comprises the encoder model.
And this is particularly adhering to one language, particularly one language,
let's say English, what happens after this is the last LSTM cell or the last
RNN sell it outputs certain values traditionally and LSTM outputs, three values.
So we have the hidden cell state.
We have the cell state that is C and we have the output value.
And if we were to comprise ourselves of this particular part, then one thing
we can do is that The most important thing that we can do is that whenever we
consider ourselves the output of the last LSTM cell, we can try to earn it.
That means we will not, we will not consider the output that
is coming off from the last cell STEM cell from the encoder.
We will just consider ourselves with the hidden hidden outputs.
That is the H and the C.
If we're considering LSTM.
Now, similarly, if we're considering GRU use for our use case, then there will be a single
there will be a single combined, hidden state that is H and, and corresponding output.
So for the GRU use, there are only two outputs output
state owed at a hidden and a combined hidden state.
That is H so similarly the output of the Encoder GRU will be.
will not be taken into consideration and only the hidden state that
is the H state will be taken into consideration for the decoder input.
Now, this particular thing is known as the encoder factor.
That is the hidden cell output of the stack of LSTM cells.
And encoder cells is known as the input vector.
This is passed on to the coded as the input.
So the Decoder is a similar architecture.
If this, it comprises of the same version of RNN, right.
Or it can be variance of LSTM or it can be variance of GRUs as well.
So similarly, we pass the encoder vector, which is the output of the LSTM encoders.
Right.
And also pass, there will be another input which is corresponding
to the inputs of the different language that is Chinese in our case.
So the inputs of the Chinese language and the input vector, a hidden cell together that
is output of the important cell together forms the input of the decoders and in decoders.
We have a similar architecture where we pass the hidden States for each of the
individual LSTM or the GRU units or any RNN variant units of the decoder model.
So if we were to move in a descriptive part, so what will happen?
So the model consists of three parts that is encoder, the intermediate input effector.
That is the output of the inquiry and the input to the decoder cell and the decoder.
So the encoder can consist of a stack of recurrent units that is
LSTM, G R U or anytime, distributed recurrent neural networks.
So when these types, except a single element of input, collects
information for the particular element and propagates to that network.
So there's a stack of cells.
And then if we have a question on sitting problem, the input sequence
is a collection of all the words from that particular question.
Now, generally, if you want to do translation, or if you want to do question answering
or any kind of NLP task, there are tokens which are, which are getting upended.
That, that the stock  token, the start to consider that the start is this,
this, this max is the starting of the sentence in a particular language.
The stop talking signifies that this is a stop symbol,
or this is the terminal part of that particular language.
So, this is very important because when we're considering ourselves with neural machine
translation, we know that different size inputs will have different sized outputs.
So it is very important to have a start symbol and a start stop symbol or tokens who,
to which specifies the LSTM cell, which displays the encoder as well as the decoder,
where to stop our, you know, training purposes, where to stop where the input stops.
So this is very, very important.
And when we consider our source with the internal hidden layers in a particular stack of LS
teams for the encoder or the decoder, we know that inside and particular LSTM, we have the Tanh
activation and we have a similar kind of function that previously that we have WHD minus one, plus w
Input.
This is the corresponding.
weight vector for the input.
HD minus one is the previous timestamp hidden cell and WHS says corresponding
wait, and F is generally 10 H if we have seen the previous videos LSTM,
we generally use the teenage version and we get the hidden output.
Now, this is all good.
When it discussed, when we were considering ourselves with the intermediate parts, that
means inside these intermediate cells, which are returns sequence rather than output.
Now, if you want to generate the output, we generally want to do a
soft max of this HD with corresponding to its corresponding weight.
Now in encoder vector, this is the final hidden state that is produced from the end of the model.
It has calculators in that formula above where we only pass the hidden state.
We do not consider ourselves the output of the LSTM cells.
Now, this also forms as the input, the decoder part.
Now what is the decoder is a stack of several different units, similar to LSTM or similar to a GRU.
Now, if you're using the same version that if you're using the LSTM version with the
LSTM version, that is in a steam and the steam and Encoder decoder, it is known as a
homologous and encoder to encoder model because the architecture almost remains the same.
But if you were to use a variant, let's say a bi-directional LSTM
antibiotic, and a single LSTM version stacks in the decoder part.
Then it will be a hybrid version because in the, in part we are using by directionality.
Well, as in the recorded part, we are just using normal stack of fed LSTMs, sellers, a
stack of several recurrent units where each predicts an output YT at a time step team.
So this can be for the different languages, right?
When we're converting from English to a particular word,
word, Chinese is word or a different language word.
So each of the current unit accepts a hidden state.
That is from the previous unit, as we saw before.
Because there are stacks of recurrent units.
So these accepts the hidden States over here, and they also accept
the in encoder vector from the output of the encoder itself.
Right.
And this is computed as this particular equation that is
given overhead, which is, I think w H two HD minus one.
And because in general decoders, we generally do not have the XD part.
We generally have only the HD minus one that is only the input, hidden layers.
We didn't really do not have the extreme.
But there are other variants of this particular aspect as well,
because if we are using for other tasks, let's say you know, text
classification or sentiment classification, we want to use embedding.
So we want to use embedding with the input and we also want to use some other attributes as well.
So if we just consider ourselves with the embedding layer, if we want to use
embeddings for our text classification through and encoder to encoder model, then this
equation becomes a bit more elongated to also include the embedding vector as well.
So here, plus we will add, and we will also add the weights of the embedding vectors as well.
You what hidden States for our evaluation?
So that is the only difference that is required to be done because we
are particularly seeing for the case of neural machine translation.
When we, when we, when we are mapping a particular phrases of words or phrases to
another phrases in a different language, but in the case of text classification, we
have to apply some kind of embedding metrics as well, or embedding layer as well.
So, Moving forward the output, since the recording, also, we need the output.
We also get the output by using the softmax activation
of the final, hidden state of the decoder model.
So in the case, what is the difference between the encoder and the decoder?
Is that in the input or the output?
We do not consider.
We only considered the output hidden state.
That is the H of the encoder step . But in the decoder, we do not
consider ourselves with the output with the final output, hidden cells.
We consider ourselves with the only output that is the, Oh, we do not consider ourselves
with the H of the decoded part because we want the output output probabilities.
That is a soft max output probabilities.
So another analogy can be that we consider all of these.
That is the encoder and the decoder to be stacks of different LSTM or GRU units.
Least one after the other, where only the hidden cell
or only the hidden signals get past one after the other.
And the outputs does not come into the picture.
The output only comes into the picture at the last or the decoder last LSTM or GRU unit
before that no output comes into the picture here only the hidden cells or only the hidden
calculations comes into the picture through the throat, the encoder and the decoder.
So this is what the entire architecture of an article it looks like.
So generally in a decoder in neural machine translation.
In the case of a decoder, the equation is a bit reduced as compared to the
in encoder, because we only consider our source with WHH into HD minus one.
But in the case of text classification, we also have to add some embedding vector as well.
We also have to add some input embedding mattresses or input embedding
vectors for our language, for our extra plus for our moderator to classify it.
And needless to say, this included decoder architecture
from sir from some block of neural machine architecture.
Now, if we want to build on top of it, so let's say we want to add a feed forward
neural network, but a simple dense network on top of it, then that can also be done.
So whatever is the output of the decoder part, we can also
pass it through a dense network and we can have any kind of.
activation functions like Sigma soft max to generate our
corresponding probabilities for our text classification tasks.
So the encoder decoder architecture can be considered as a very sophisticated
neural network architecture based on sequence to sequence learning, which
tries to capture more and more information which standard LSTM cell will
fail to do, or a standard vanilla LSTM architecture will fail to do so.
It may be very important for neural machine translation, where
we have different lens of inputs and different kinds of outputs.
As well as classification question also generated generation
by the, by employing the use of certain start and stop tokens.
And this is very important because an improvement from the standard in LSTMs
of the vanilla architecture, because we are considering ourselves to the hidden
parts or the hidden computations of each of the encoded and the decoder models.
So this leads to more that memory retention as well as more performance.
So this was all that, that had already gotten encoders and decoders.
definitely in the next topic, we are going to cover
about attentions and, and sophisticated details about it.
So these included decoders have another model, which is
mathematical competition tool, which is known as attention.
There are different variants of it, and these attention mechanisms also
help to boost the performance of translation and any other language modeling
tasks, let's say classification or question also generation, anything.
So this was in general, all that I had to cover.
And I will see you guys in the next video tutorial.
Thank you.