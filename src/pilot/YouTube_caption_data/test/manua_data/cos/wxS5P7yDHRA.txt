hey in the last video we learned the theoretical 
details of decision trees so how they learned  
how they are trained and in this video i will 
show you how to implement them on scikit-learn  
using python we will look into some of 
the parameters that you can use we will  
look into some very helpful functions 
that it has built in and generally we  
will try to get a better understanding 
of how you can start using them today  
on a python on a jupiter notebook so 
without further ado let's get started  
we have a bunch of things to look into of course 
but the first thing that we need to do is import  
a data set for this one i'm using a built-in data 
set from scikit-learn you can import it like this  
basically just if you want other data sets of 
course go to scikit-learn data sets just google  
it and then you'll find a list of data sets that 
are built built-in in scikit-learn that you can  
easily import this one is called the breast cancer 
data set it's a classification data set let me  
show you what it looks like so it's basically 
these are all the calculations done on a or  
measurements done on a tumor that was collected 
or a mass that is collected from a breast uh of  
course we are not doctors we don't really know 
what these things mean uh but we don't really  
have to also you know we're just using this data 
set to understand how decision trees uh trees  
can be implemented with scikit-learn so this is 
just a data set looks like we have 30 columns  
each of them is a measurement for a tumor and 
then as a target variable we are learning if a  
one line just so one data point or one tumor is 
benign or malignant so that means benign means  
it's not cancerous or i don't i don't really 
know the medical term there but i think it's  
like just not bad for you everything's fine 
if it's malignant it means that it has cancer  
and you need to be treated basically 
uh on a very simple level let's say  
okay uh before we feed this data set to our um 
let me make it a little bit closer yeah before  
we trade give it to our decision tree of course 
we need to divide it to training and testing data  
so just to kind of give you a structure 
at first i'm going to show you how you can  
train a simple default decision tree and then 
we are going to go into the details of how you  
can change it to how you need um okay com coming 
back to training and testing separation inside  
scikit-learn there is a default built-in function 
to separate training and testing data sets  
so you might remember this from your other 
training of machine for machine learning  
x means just all the features that you want 
to give to determine uh if this data point is  
benign or malignant basically and this is all 
the columns that i have in this data set and y  
is a target value basically and that's going to 
be either 0 or 1 depending on if it's like good  
or bad um this built-in data or this built-in 
function only needs the x and the y basically as  
i showed here and the test size means how much of 
it would you like to set aside for testing so for  
training the training data set means we are giving 
it all the examples but we are also giving it the  
answer so the model can train and learn uh whereas 
for the test one we're only going to be giving the  
features for the or the columns and then it's 
going to create the predictions itself so that's  
how we're going to test to see if our model is 
performing well or not uh how to do predictions
how to train the decision tree is basically 
very simple you have to import the decision  
tree classifier or basically the um the 
model itself from the scikit-learn library  
and you have to create it once we start putting in 
parameters this is where we're going to put it in  
and then all you have to say is classifier fit and 
you give the training values for the x values and  
the y values and then it creates you a nice little 
decision tree model that's all um of course right  
now we're using the default parameter so if 
you run this one which is which gives you all  
the parameters that are being used in this model 
you're going to see that it's the default values  
um yeah so these are all the default values max 
depth is none max features is none we're going to  
learn more about what these are in a second uh 
but yeah if you want to use instead the regressor  
it's very simple then you only need to import 
regressor and how if you're like oh but i don't  
know how to import these things when you find the 
documentation from google so basically by just  
writing a scikit learn decision tree regression or 
second learn decision tree classification you're  
going this is going to be either the first or the 
second page to pop up so this is the scikit-learn  
documentation you just need to scroll down and 
then see these examples and it's going to tell  
you how to import it here basically so you just 
need to copy and paste this code to your notebook  
okay now we trained our data set but of 
course you want to do some predictions with it  
there are two ways how you can get predictions 
the first one is by giving it the x test data  
set so it's basically my data points where i want 
to where i haven't told the model the answers to  
i just want to get the answers 
that it can it comes up with  
let's look at what it looks like so it's something 
like this uh my training my or my whole data set  
had 569 rows and for the testing we set 
aside nearly 200. um if i do a prediction  
it's going to give me the class number that it's 
predicting so it's going to say for the first  
one i am predicting that it's malignant for the 
second one i'm predicting that it's benign second  
one i'm pretty good at sb9 so on and so forth 
so this is just a way to see generally how your  
model is predicting on the new information that 
it's giving you another way to see predictions  
is using predict proba then it will give 
you the probability it has for each of the  
um classes so let's see so it says for the first 
instance so the first data point so this one  
i am predicting that it's going to be um class 
0 with 1 out of 1 chance and class 1 with 0 out  
of 1 chance so there is a reason that our 
all our probabilities are 0 or 1 because we  
did not have any uh early stopping criteria so 
our tree just like grew and grew and grew and grew  
all the way until there was no other way to 
split but if we put a very simple stopping  
criteria like let's say max that can be four so 
the maximum depth that the tree can have three  
can have is four so you can see here and let me 
remove this one and then we do predictions again  
the predictions also are still going to 
look the same because it's going to give us  
uh the prediction so it's going to tell us either 
zero or one based on which one has the higher prop  
uh probability but if you look at the 
probabilities now we're going to see that  
they're a little bit different so now it's kind 
of less sure uh which one it should be because  
we stopped the tree before it was able to grow 
all the way where the leaves are going to be pure  
of of one class so if doesn't if this doesn't 
make sense to you you should go back and watch  
the first video i think then it's going 
to make more sense to you what i mean here  
all right um so we have the probabilities 
we have the predictions for each of the test  
instances but how are we going to see if this 
is good or not we have to compare it to the  
actual information of course how are we going 
to do that is by using a performance metrics  
most of the performance metrics 
that you're going to need or use  
are already going to be built-in and 
scikit-learn so first one is accuracy for  
example it says this model has an accuracy 
of 0.92 or 93. if you want to see you can  
find the confusion matrix uh so this is you 
know if the class is zero and when the class  
is zero and it's predicted as zero that happens 69 
times when the class is one and predicted as one  
this happened 105 times and these are the wrongly 
classified instances uh another one i can see is  
precision score for example so this is basically 
precision if you want to see recall so let's see  
okay um you know i'll just show you how i 
find these things so i could learn recall
and yeah this is the first thing 
that pops up and then i can  
go look at the examples it says i 
need this one to calculate recall
and uh my true values are called y test and 
predictions are called predictions yeah and  
then i get my recall score too so it's that simple 
there is also a nice function that they have here  
it's called classification report and i think then 
you can basically see precision and recall and f1  
score and everything together there also might be 
a regression report similarly that you can find um  
of course you see here the mac maker macro average 
weighted average etc etc so there are some or like  
precision and recall separately for malignant and 
benign uh i will not go into details of what these  
things are in this video because you know this 
is about decision trees but yeah i can make a  
separate video about that later but let me know if 
you would like to learn about it um all right so  
before we go into future importance and 
other things i want to show you some of the  
parameters that decision trees have so let's go 
back to our model so it was we're only using max  
steps 4 right now all right uh so let me pull up a 
list of the stopping criteria so these are all the  
settings let's say that you can change to stop 
the tree from growing all the way to its maximum  
passable length max depth tells me how deep can 
the tree be so when there is one node when there  
is one decision that's depth of ones so when 
you make the decision or when there's only one  
node then that's a depth of one when you have a 
decision node there and then you make a split then  
you tree your tree has a depth of two and then 
those nodes split and then you get a decision  
tree of that depth of three so that goes further 
and further sometimes decision trees can grow to  
be very long so if you if you wanted something 
these things are not really things that you can  
know beforehand you cannot really say oh yeah 
i want my max step to be three because i know  
that's going to give me the best results 
no most of the time what you do is you try  
uh you try different values for this 
and then you see which one works best  
so yeah max step basically gives you the depth of 
the tree um there are some other ones here as you  
can see these are all the stopping criteria if you 
want to learn more about what they are how they  
change when you stop the tree you can you can 
find them all here these are all the parameters  
that the tree has you can go and read about them 
here they have different ways of stopping the  
tree tree's growth let's say the 
training process i think if you have  
more than one stopping criteria set up it's going 
to just stop with the first one that it reaches um  
but yeah so just you know go ahead and learn more 
about them and then try it and then you'll see the  
difference that it creates in your performance 
or the creating the performance of the tree  
another thing that's important for 
us other than the stopping criteria  
are the approach of the decision tree so here 
are some of the approaches or the some of the  
settings that you can change for the approach 
the first one is criterion what is criterion  
so in the previous video i talked about if 
you remember that there are two different  
algorithms that you can use cart algorithm 
and id3 algorithm and they are using different  
metrics of which feature they should use 
to split the data set right so criterion  
basically depend or determines that 
one the default default one is ginny  
but you can also use entropy again if you don't 
know what these things are and if you're curious  
go back to the first video and then watch it the 
video about decision tree theory and you will  
know what i'm talking about here so this is the 
parameter that will help you determine how to uh  
grow the tree or what to use to make the 
splits the second thing that we can use to  
determine the approach to growing the decision 
tree or training the decision tree is splitter  
um basically you have two options you either 
choose the best one based on these two criterion  
or you can just choose it randomly that's also 
an option you can also say you know what i want  
to go crazy i want to just choose it in a random 
way which feature to split on and yeah you can  
do that that's also a possibility another one 
that's important to know is max features so  
in the decision tree we have or let me 
show you here we have 30 columns right  
you can say every time you want to make a decision 
every time you want to make a split only use 20 of  
them and then the decision tree will decide which 
20 randomly and then it will compare their entropy  
to each other or their information gained to each 
other and select the best one so maybe the actual  
best one is outside of that group of 20 but still 
it will choose only the one the best one in that  
group of 20. so that's an option that's available 
to you if you want you can choose the not the max  
depth but the max number of features to be less 
than the amount of columns that you have the  
total amount of columns that you have and then 
there will be some more randomization involved  
a random state is basically helping this 
randomization of um the when you choose  
less than the amount of columns that you have 
then there needs to be some randomization of  
course um if you give an integer for the random 
state then it's going to be creating the same  
same results from this randomization every single 
time but if you don't give anything to the random  
state it's going to be super random every time 
but if i add a parameter to be like random state  
five then i'm going to get the same tree over and 
over again even when i have some randomization  
involved um okay i think this was clear uh these 
are all the things that you can do to change the  
approach of the tree and there is one other thing 
that's important to know and that is this one  
class weight so why did i run this it's not like 
it's gonna run class weight is this is specific to  
classification this doesn't exist in regression 
you know so when you're doing classification  
you're going to have one two or three or maybe 
more classes that your model is going to try to  
predict and sometimes one of those classes might 
be a little bit more important than the other ones  
and with class weight you can determine this in 
the model and you're basically going to say hey  
it's more important or it's worse when you make 
an error predicting class one then when you do  
a mistake trying to predict class 
two so then it's going to take it  
into consideration into how it grows 
or into its genie index or entropy um  
all right so these are all the parameters that are 
relevant uh the next thing that we can look at is  
the future importance so with decision trees as i 
said in the previous video i think i mentioned it  
one of the best things is that 
it's interpret interpretable  
very hard word to say so you can actually 
understand how the decision tree is deciding  
and one of the things that comes with this 
is you can actually learn or you can see  
which features are more important than the 
others so let's go here i'm getting all my  
features these are all the features that i 
have and it's very simple you can just say  
classification future importances well let me show 
you what this creates anyways in the first place  
and then it will give me a 
list corresponding to this list  
of how important that corresponding feature 
to determine the result of your prediction so  
and when i put it into a data frame and everything 
i can see future importance like this and if you  
want this is very common to do you can make 
it into a plot and let's see yeah and then  
you can basically see it in a plot and then it's 
easier to kind of understand okay verse perimeter  
is the one of the best things that uh determines 
if a tumor is benign or malignant so this is  
very important to really understand your um 
model this this is not really possible with  
most other machine learning algorithms so this 
is like a really big plus for decision trees  
another way to see how your data set is or 
how your model is working is to create a  
plot of the tree and this is very simple you 
are basically creating this tree of like an  
actual tree of how it decides so for example you 
know when when you get one line one data point  
uh it says worst perimeter is it bigger than 
110.25 or greater than or lower than that one  
if it's lower than that one go here if it's 
greater than that one go here and then another  
decision point and another decision point uh so 
as far as the depth goes as you can see this is  
the first step second this is the third level 
fourth level this whole thing in the fifth level  
um or now that actually now that i counted it i 
said next step to be four so probably this starts  
from zero and then one two three and four uh we 
can actually see how it changes when we change the  
parameters so we said max steps should be 4 right 
so let's change that to like not before let's just  
have the default tree let's have it grow as 
much as possible and then let's look at the  
how the tree looks okay so this is a much busier 
tree maybe the depth doesn't even go further  
but probably this is then enough you know for the 
tree to uh grow but yeah then it's it's kind of  
like a more dense tree and one other important 
thing that i nearly forgot to mention is pruning  
for decision trees right we talked about it in 
the previous video how it's an important way or  
how it's a very used way of making sure that the 
tree is not overfitting to your data set how you  
do that with scikit-learn is with a parameter 
actually so this parameter is called ccp alpha  
um we can go and see the definition here 
it's basically that there is a algorithm  
on top of the decision tree algorithm to prune it 
and it's called minimal cost complexity pruning  
i'm not going to go into details but if you want 
to read more about it there here's a link for it  
uh to describing the details and the general 
math that is behind it but when you give it  
a value that's bigger than 
zero the default value is zero  
when you give it a value bigger than zero 
then it's going to prune your tree so let's  
try so we saw that our tree was a little 
bit big so if i give it this value  
i expect for the tree to be a little bit 
pruned so this is what we have right now  
and when the tree is a bit pruned then as you 
can see that value is apparently even already  
too big so then i have a very nice and simple tree 
that was pruned um but let's see how the accuracy  
changed i'm curious i didn't even no i have to do 
the prediction again um yeah let's see now okay  
it looks like the accuracy increased so you know 
that's perfect i guess our tree was uh overfitting  
maybe to our decision um to our data set i 
think that's all we have at decision trees so  
just you know it's very simple as i said just 
import them import one of the sets that they  
have built in in their system and yeah just 
play around with it change some of the settings  
and then see how it changes your accuracy but 
yeah i hope this was helpful i hope you learned  
something at least and yeah thanks 
for watching i'll see you around