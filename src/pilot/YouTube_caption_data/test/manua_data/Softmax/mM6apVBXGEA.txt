 as the last video for this lecture, let me now show you a
 code example, implementing the softmax regression model from
 scratch, using this vectorized form that I showed you to show
 you that it indeed works. Actually, I have two code
 notebooks here. One is a from scratch implementation using a
 simpler iris data set. And then I will also because I mentioned
 that before in the lecture, I will also show you the MNIST
 example, because I'm sure you were all waiting for it. Alright,
 so let's execute the boilerplate here first, just get it getting
 it out of the way. And here, so the data loading, there's
 nothing really new going on. It's the same code I used for
 these toy data sets and the simpler iris data set I used
 before. So I don't want to really spend too much time on
 discussing this again, it's essentially the same, only that
 this time we are using three classes instead of two. So we
 have now these circles, upside down triangles and squares. So
 left hand side is the training set. Yeah, and the right hand
 side is the test set. In the other notebook, I will show you
 also the pytorch data loader. So that is then usually how we
 load data, because in deep learning, we usually don't use
 CSV data sets. So we usually don't do this procedure, we
 usually use a data loader, I will show you in the next code
 notebook how that looks like. Yeah, so starting with our low
 level from scratch implementation, computing the
 gradients manually. So here, I just defined some helper
 functions, one that converts the class label to a one hot
 format, a softmax function and the cross entropy as the loss
 function. Note that you don't have to implement any of that
 when you use pytorch regularly. I mean, the regular version of
 how people implement code in pytorch, because pytorch will
 do the one hot encoding. Yeah, internally, kind of implicitly
 when we compute the loss, I will show you how that would look
 like later. And also, it has a softmax and cross entropy
 function already implemented. So you don't have to worry about
 this yourself. But yeah, I'm just doing it here to really
 show you that the mathematical concepts from this lecture
 really translate one to one into code. And it actually all works
 works. So similar to a logistic regression, we start with
 defining a class. So in the init constructor method, we construct
 the weights and the bias, so we initialize them here to all
 zeros. And notice that weights in contrast to logistic
 regression, and this softmax regression context, it's now
 matrix, right. So we have the matrix num classes, num
 features. So this is what we use to age, I think, for the number
 of classes, h times m matrix. And because we have also two net
 inputs for the two outputs, we also have now the bias as a
 vector, right. So this is a h dimensional vector now. So h is
 the number of classes. And here, here's the forward method. So
 the forward method is computing the net inputs. So I call them
 logits. But yeah, these are you can think of them as the net
 inputs. It's just the other multiplication between the
 inputs and the weights plus the bias vector. So this would be an
 h dimensional net input. So these are our z values here.
 Then I'm applying the softmax to get the probabilities. So the
 probabilities here with that, I mean, these units a, a one and a
 two are activations. So we can also call them the activations.
 And these are net inputs. And I'm returning both here. And
 yeah, here, this is the interesting part, the backward
 method, that is where we compute our gradients. And instead of
 doing everything with a for loop for each weight individually,
 we can actually use the vectorized implementation I
 showed you. And let me move this over. I'm implementing it really
 one two by one here, or I mean, like literally, it's not making
 any change, we use our x transpose times. So with a
 matrix multiplication, times y minus probability. So progress
 is really the activations here. So this part is essentially this
 party in the parentheses, and then also the transpose again.
 And this is exactly the same as I'm showing you on the slide.
 For the bias unit, it's a little bit simpler, because the
 derivative doesn't include the x term. So we can just so this is
 a vector, right? So we can just compute the sum. All right. Um,
 yeah, this is it. So here in the predict labeled thing, I'm just
 applying the arc max to get the index of the highest probability
 in the activations. So that's also what I explained to you
 earlier. I can actually move that back here. All right. So
 this is all all that's new here, really, the rest here is the
 same. So the evaluate function, that's the same as the one for
 logistic regression. And the training is also the same.
 Notice the only difference here for the train function, what I
 did is train method, what I did is I added this one hot
 encoding here to get the one hot encoded class labels. But again,
 here, we call forward, and we call backwards to get the
 gradients. And then we update the weights by applying the
 negative gradient. So minus, so minus the gradients times the
 learning rate. I'm also normalizing by the size of the
 mini batches, because I told you that is easier to find a good
 learning rate this way. And here, this is just for logging,
 again, to show our accuracy and loss during training. But yeah,
 again, this is actually not too complicated, right? I mean, if
 we look at the backward method is really just two lines of code
 here, if we use the linear algebra compact vectorized form,
 I should actually mention this is not so trivial, of course, to
 think about what should go in here. It looks very simple, but
 it might take you hours and hours to think about it. So
 personally, in the first pass, if I would ever implement
 something from scratch, I would implement it in Python for loops
 and then go to the linear algebra. Yeah, implementation. I
 think that's maybe easier for me. Other people may find it
 easier to think about in these matrices, matrix and matrix
 multiplication terms. I personally, it's not my my
 credit, my thing, I like things simpler, even they can be more
 verbose. But I'm using this, of course, because it's more
 compact, it's very information dense, lots of things going on
 here. It's more compact, though, and computationally more
 efficient. But again, also, I should say in practice, no one
 really implements this by themselves anymore, because we
 can use pytorch autograd, which has some computational
 facilities to do this automatically. Alright, so let's
 train this then.
 Alright, so we can see after the first epoch, we already got a
 pretty good training accuracy, it actually goes down, which is
 interesting. But yeah, more interestingly, the cost or the
 loss here goes also down. So I think this is really due to
 noise that the training accuracy goes down, it's probably
 overfitting here. But yeah, we see the loss goes down. That's
 our result. And let's take a look at
 what it looks like.
 So yeah, we can see the loss goes down, maybe training it
 more would be even better. And then let's compute the test
 accuracy, it's 80%. Let's take a look at how it looks like. So
 yeah, I'm using function, plot decision regions, we have seen
 that briefly in statistics for 51, if you recall that, but you
 don't have to know this or remember this, because in
 practice, we will actually never really work with two dimensional
 data sets. So we never really have the luxury of plotting it,
 I'm just using a very simple data set, which is why it's
 possible here. So here's how the decision regions would look like.
 So you can see, yeah, the softmax regression classifier
 is able to deal with the three classes by classifying them
 relatively well. It's not great in this region here, could have
 maybe been improved if the line would be more like this. But
 yeah, it's hard to tell. It's actually not a trivial problem
 for a linear classifier. A nonlinear classifier may have a
 easier time here with in this region. Now, but this is also
 not the point. Now, let's take a look at how we would implement
 it using pytorch module API. So the module API makes many things
 easier, like I said before, so we can use this linear layer,
 which is our net input computation here. So actually, we
 don't need all of this part in a real application. I'm just
 doing this part so that we can compare this implementation to
 our from scratch implementation, because above, I also used
 zeros. So if I go up, I initialize the weights to zeros.
 So otherwise, if I don't do that, here at the bottom, it
 will have small random numbers, the smaller random numbers are
 good for multi layer networks. But yeah, it would make it hard
 to of course, compare than this implementation with our above if
 we have different starting weights. So yeah, we just add
 this, but in a real case, you would just run it like this.
 Alright, but we will also talk more about weight initialization
 and later lecture, I think, like, I would say maybe in two
 weeks, I will have a lecture on how we can initialize weights
 and what these random weights are. And there's also, there are
 some computational tricks to choosing good weights. Alright.
 Um, yeah, the forward method is now very simple, we can use the
 oops, the net input computation here via the linear layer, which
 we defined here. And then here, this softmax is our softmax
 activation function, we use that from pytorch now. So F is from
 pytorch, I think I imported it at the very top. Let me see.
 Yeah, so here, this is the torch functional API. So and n stands
 for neural network, and since the functional API, instead of
 typing it all the time, instead of writing, let's say this dot
 softmax, and do the same for other functions, people usually
 implement the function API SF, just to make it shorter. So you
 only have to type F softmax, basically. So that's just a
 convention when you work with pytorch code. Okay. So yeah, and
 that is basically it, we are using the stochastic gradient
 descent optimizer here, optimizing the parameters of
 that model. And yeah, this is all all there is to it really.
 When we use pytorch and training it now, you can see test accuracy
 is also 80%. And actually, if you would scroll up, you look at
 this decision region plot. And this one, it looks almost
 identical, there's some slight numerical difference, if I think
 if you look at these values, your point 558, and minus point
 12. Let me just copy them. Maybe
 there's some slight, maybe not really even. Yeah, this is one
 is a little bit different, you can see, but it's only like three
 digits after the decimal point, this is just an numerical
 rounding error, I would say, because we implemented lots of
 things from scratch, like softmax, and the cross entropy
 function, and things like that. And we have the vanilla
 implementations, there are also some numerically more stable
 implementations of softmax by adding or multiplying the
 nominator denominator and numerator by C. So adding lock,
 C lock constant to the exponent that would make it more stable,
 but then it would change the numbers slightly think these
 differences are because of that. Yeah, of course, in practice,
 you don't want to implement softmax and cross entropy
 yourself, because it's numerically not as stable as not
 as fast as the implementations that are already provided. But
 yeah, overall, we get exactly the same results. I mean, the
 small rounding differences. Now, this is how we do softmax
 regression in pytorch. Let's now take a look at the same thing
 for the MNIST data set. Because I already showed you the MNIST
 data set in the lecture. And so just for completeness, we will
 actually be working with this more when we talk about
 multilayer perceptrons in the next lecture. So what's new now
 is I'm using a data loader. And this is usually how we load
 images in pytorch on other types of data. So here are just
 some settings for my model. And here, the MNIST data set is
 already provided in a library, let me scroll up again, the
 library called torch vision, it comes with a number of data
 sets. So if you install pytorch, it will also if you use the
 installer via the website, it will automatically install torch
 vision. So you don't have to install it separately, it
 usually is included in the command that you executed when
 you install pytorch. So we use this MNIST data set from torch
 vision from the data sets sub module. This is just specifying
 where to store the data. So it will create, if I go here, it
 will create a data folder in next to my notebook here,
 because I called it data, but you can actually change this to
 whatever you can change it to desktop data or something like
 that really doesn't matter, just where it keeps the data so
 temporarily. So if you go here inside, you will see it will
 it will keep a copy of that data. All right. Then it has a
 trust test and a training set. So train true test. This is for
 testing. If you say train faults, I will actually show you
 it's a little bit unfortunate that MNIST doesn't have a
 validation split. But I will show you in the multi layer
 perceptron lectures how we also can create a validation split
 for that one. This one is kind of required. It's because MNIST
 is images and they come in, I think it's a PIL Python imaging
 library format. And you want to convert them to pytorch tensors.
 So this one is converting the image representation in Python
 to a pytorch tensor representation. But we will also
 see later, we can use other data augmentation here like
 on randomly flipping or cropping images. So we will talk about
 this also more when we talk about convolutional networks.
 Also, I should say that to tensor normalizes, it normalizes
 the pixels to zero one range. So it's dividing them by 255. So I
 should mention that so we don't have to do our data normalization
 ourselves. Of course, we could actually do something better. We
 could standardize them for color images. Sometimes people
 standardized using on the mean, and the standard deviations from
 image net. We could technically also do that for color images
 for here, something like MNIST, it doesn't matter at all. To be
 honest, there's probably no difference in the performance of
 the model, because it's a very simple optimization task here
 with stochastic gradient descent. Alright, so this is
 defining the training data set. This is defining the test data
 set, the differences really that we set train false and here to
 true, we only need to download it once, right? So we don't have
 a download here. Then we using the data set, we define data
 loaders. So here, I'm defining a data loader for the training
 set, I can set my batch size, I set it here above, that's my
 mini batch size, I set it to 256. And then whether we want to
 shuffle or not. So this is shuffling before each epoch,
 shuffling the data set. Notice that here, I'm not shuffling the
 test set, because it doesn't really matter. I'm loading the
 test set in batches, it's also not really necessary, because we
 don't do any training and stochastic gradient descent on
 the test set. But in let's say, if we work with real image data
 sets, like image net, the test set can be quite large, and it
 might not fit into memory. So we can also load it one by one,
 basically, that's what I'm doing here. Here, just for completeness,
 I'm this is more like for verifying that everything looks
 good. So here, I'm just having a for loop over the images and
 labels in the training order, just to make sure everything
 looks okay. And you can see it's loading the images in this n h
 or n c h w format, the batch size, the number of color
 channels, we only have one color channel, because m is this black
 and white. And then we have the height and the width here. And
 the labels, of course, are a vector, because that's just the
 numbers. And I should also tell you, okay, um, these labels are
 not one hot encoded. So you can see that they're just the
 numbers of the class labels, the 10 class labels. It's not
 necessary to one hot encode them, because pytorch does it
 automatically, when we call the cross entropy loss. Alright, so
 here is now my implementation. This is the same as in this from
 scratch here. So there's nothing really different. Same thing.
 Um, yeah, so also notice, so yeah, the accuracy, I'm
 computing it using a for loop over the data loader. I don't
 have to go through this. I think this is something you could
 copy and paste if you need it. Um, yeah, so the interesting
 part, what I wanted to show you is, because I don't want to
 make this video too long. I mean, I could discuss everything.
 But if you have questions, maybe ask on Piazza, we can discuss
 this more. I don't want to spend too much time on things that are
 not that important right now. Um, yeah, so the important parts
 are here that we are concatenating. So that goes back
 to what I've showed you here. So this would be how our image looks
 like it's 3d tensor, one image is a 3d tensor 28 by 28. And
 then the color dimension. So if you don't think of the color
 dimension, it would be a matrix, a 28 by 28 matrix, but we need a
 feature vector as input. So we would concatenate it to a very
 long vector, 28 times 28. So 784 dimensional vector. So that's
 what I'm doing here. So I'm concatenating it to a vector, I
 can maybe show you how that would look like for one given.
 Let's say we take one image. So this would be the one color
 channel and 28 by 28. Actually, you can also I mean, okay, it's
 fine, I think. So you minus one and 28. So you can see now it's
 a one long vector, basically. So in this case, I think it's a
 matrix because we have the first dimension, but yeah, so but the
 same thing applies. So here. Yeah, we do that for the. So
 actually, this would be for the batch dimension one more time,
 sorry, on images. So this should be a 25, a 1250. So this should
 be five at 256 times 784 dimensional matrix. So this is
 for the is obviously this is like how our design matrix looks
 like it's then an n times m dimensional matrix now where m
 is the features and is the batches. It got rid of notice
 that got rid of the color channel here. There's no color
 channel because we only define two axes. If we do it like this,
 it would have a color channel. But yeah, we don't want to color
 channel here. All right. Um, yeah, and then we call our model
 notice when we use pytorch, we don't use dot forward, because
 that it's actually better to just call it like, like this, it
 will internally call forward, but it will do some extra checks
 before it calls forward, as I explained in an earlier lecture.
 And now the interesting part is, yeah, we compute the cross
 entropy cost or lost here. And notice that is important that it
 uses the logits as input. So it's not using the probabilities,
 it's using the logits. This was what I explained in the previous
 video. This is like the slightly confusing part is explained in
 this other previous video. So we have to pay attention that we
 provide the logits and not the probabilities here. Yeah, and
 then we zero the gradients to the backward. Or we are just
 averaging the costs here. Don't know why I did that. Don't need
 that. Oh, I Okay, I see. I have I compute the average cost over
 the epoch. I see. Yeah, so then I'm plotting it. Okay, need to
 execute this part.
 So this cost is the average cost per epoch. I could technically
 compute the cost by calling forward on doing this one for
 the whole data set. But it would be more work, right. So you
 would have to load the whole data set. So I'm just computing
 the average. Okay, might take a while. It's not too bad, though.
 It's like, um, point, point, four minute point, oh, four
 minutes per epoch. So even I don't know if these numbers make
 sense. One minute makes maybe sense. Alright, so now, yeah,
 this is how the loss looks like you can see it goes down as
 expected. Let's take a quick look at test accuracy. 92.16 is
 actually pretty good, right? I mean, given that this is an
 image data set, and we only have a linear classifier, we don't
 even use a neural network, I mean, a multi layer network, we
 already get a pretty good performance. And personally, I
 also recommend you when you work on classification tasks, always
 to run also logistic regression classifier, or softmax
 regression classifier as a baseline, just to get a feeling
 of how difficult the classification problem is,
 right. So if you train, let's say convolutional network, and
 you've got only 94% with a convolutional network, you would
 say, Okay, this is such a complicated convolutional
 network model, why does it only get 94% when I get already 92%
 with logistic regression, maybe my model with 94% isn't that
 good after all. So it's always a good thing to run also logistic
 regression as a baseline. Here's just an example, learning three
 random images from the data set, seven to one and zero. And then
 here, I'm doing my predictions. So calling this one on the
 feature vectors, and then the arcmax max to get the class
 labels, right, because this one would give us just the
 probabilities, you can see, so we are taking the arcmax. So for
 the first one, let's take a look. Should be this one should
 be the highest 01234567. This is the highest is actually should
 be the highest one. So in this way, we compute the class
 labels here just to show that these are indeed correct.
 Alright, so with that, um, yeah, this is all I think I have for
 this from scratch implementation. And then in the
 next lecture, I will talk about multilayer perceptrons.