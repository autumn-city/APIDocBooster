Tensorflows documentation ([https://www.tensorflow.org/api_docs/python/tf/nn/relu6 (hyper-link)]) points to the following paper:
You need to use a CustomObjectScope to import relu6.
You can create your relu6 like this:
One of the files in your checkpoint is a serialized tensorflow::GraphDef protocol buffer; searching for Relu6 in it and replacing those occurrences with Relu will fix the graph.
If you're in this situation you're better off changing your python training code to emit tf.relu instead of Relu6.
You need to find a_layers2.py in coremltools files and comment out from keras_application.mobilenet import relu6:
So you get Relu/Relu6/None by default without having to add an extra layer.
Reading your error it seems that the array WeightSharedConvolutionalBoxPredictor_2/BoxPredictionTower/ conv2d_0/BatchNorm /feature_2/FusedBatchNorm_mul_0 from WeightSharedConvolutionalBoxPredictor_2/ Relu6 does not have min/max information which is needed to do post-training quantization.
Try installing an older version of keras_applications, one that still has the relu6 function.
You can pass activation=tf.nn.relu6 to use ReLU6 activation.
The operator of relu6 has been just added 1 week ago.
I solved the problem by using proper version of the Keras which includes both Mobilenet (feature extractor) and at the same time "relu6".
coremltools does not supporting some layers for the moment (including relu6).
So, to fix your problem, you should feed a value for the placeholder called 'input' in your call to sess.run('MobilenetV1/MobilenetV1/Conv2d_1_pointwise/Relu6:0').
