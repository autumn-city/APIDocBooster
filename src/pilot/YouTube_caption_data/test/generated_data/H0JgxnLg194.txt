this video I want to briefly show you
how to kind of make the connection
between the hummus example and the
bernoulli example earlier because you
kind of need a little bit of that so
first of all the gut test and get train
files function sorry are not really
super low relevant because they're all
about reading in the data from this
amnesty ol file which is not the same
format as what you'd be using that said
there might be a little bit useful in a
sense that you see that here for example
you are defining batch access initially
as a 0 by 28 times 120 times 28 vector
and then what you do is you add more
examples right so here you're saying
take the current batch access and add
this stuff to it okay
it also might be useful to look at how
I'm defining the Y's so here I'm saying
one hot initially is gonna be a vector
of 10 zeros because I have 10 possible
outputs and then I'm saying oh but I'm
talking about digit number K so I'm
setting one heart at K to be equal to 1
so here I'm first getting all the XS for
0 and then all the XS for 1 and the old
V axis for 2 and so on and I'm also
building up that's wise in the same way
that I'm building up batch access here
so here also unlike in the assignment
the split into training and validation
and test is already done for you right
because you can you have the test and a
train over here those refer to names
inside this and list all structure which
you don't necessarily need to completely
know about so ok so what you end up with
is your train X which is like that so
you have 60,000 examples each one
example is 700 884 dimensional and you
have your train Y again 60 thousand and
ten out of each right so 60,000 by 10 so
you should get that with your data as
well whether it's language data or the
image data
so now let's see what we were doing so
in the maximum likelihood example what
we did is we built up the cost this way
so we built up the NLL and then we built
up the cost so here we also added some
regularization at the end didn't have
that at the beginning right and then we
ran the optimization so we actually
explicitly constructed this function in
here we're not quite doing that so what
we're doing instead is we're saying okay
so first kind of randomized stuff
whatever and now here what you're doing
is you're saying I'll define this model
and this model already contains both the
parameters so inside here are the
analogues for P and it contains the
functions so this entire thing is going
to be used as the function so as the
prediction function so here we don't
have a prediction function yet in here
what you need you need a prediction
function you also have the correct
outputs and then you'll do the
prediction you have the correct outputs
and the cost is going to be basically
the discrepancy between the two okay so
here you are defining the loss function
the loss function will take the output
of the model and the correct output and
here you're defining the optimizer so
instead of doing something like this
here where you actually said okay so I'm
finding the NLL and now I'm doing the
gradient descent step manually here what
you're saying is I'll define the
optimizer and instead of doing Grady
nice and manually I'll say optimizer dot
step and that'll perform the step okay
so what does this part do so here you're
computing the prediction so the outputs
of the model by saying model of X so
you're taking X which you've defined
previously as
as trained as trained acts at train ID
acts right so here you split into
validation and training this by the way
is something that you can use so this
would be the training set and now what
you're doing is you're saying okay so
compute the predictions for my training
set and now compute the difference
between my classes and my predictions
essentially compute this thing so the
cross entropy function loss which lost
function which I haven't actually
defined in this code it's just defined
for you by saying this and then you can
call loss function on white bread which
is model of x and y classes so you
should keep track of if you're using
this loss function of how to encode both
Y classes and how white bread is
organized right so white bread we can
look at what it is it's gonna have have
a shape of 10,000 Big Ten's so the size
of the training set times how many how
many predictions there are per case
there are ten predictions ten
probabilities right so the softmax
outputs as many outputs as you have
possible classes and here you have a
self max output which is implicitly
defined so you don't even say self max
because you're saying cross-entropy loss
and that takes care of computing this
off max okay so now let's look more
carefully at what the optimizer does so
what the optimizer does is it says well
I want to optimize whatever function you
want right so the loss for example with
respect to model dot parameters so model
dot parameters is all the weights that
are implicitly defined here so unlike
here where we said kind of P is equal to
this here the model defines both the
function that computes the model and it
also initialized all the parameters
initializes all the parameters here so
all the weights and the biases that are
built in here okay so now you still need
to say model don t regret so that's
analogous to what
dead over here and you do optimize a
step and you do it lots of times so here
again I did it for just ten thousand
iterations kind of blindly you could do
something smarter and certainly if you
can and shoot blogged about the training
curves
alright so okay once I'm done with that
so d so each time I do optimizer dots
tap the weights inside this modular
changing
once we've converged so our loss became
small right we can say okay I want to
compute the predictions so here's how I
can do it I can say okay so put test X
so test X looks like this so 10,000 by
784 so 10,000 samples in your test set
I'll put it into a variable once it's in
a variable I can say model of X then I
go back to a tensor and go to non fine
so Y prediction is gonna look like this
so the shape is gonna be 10,000 by 10
because 10,000 cases 10 outputs each so
I can look at the first case for example
0 everything that's gonna be like this
so the largest number here is this the
largest number corresponds to the output
that the model predicts right if you
want to just get the output so how do I
get that so I can say at and p-card max
of this and I'll get 0 because the 0
number is the largest that's what Arg
max does I can also do empty arc max of
test y 0
like so and that's also zero and if
those are equal that means that the
model guessed correctly so here I'm not
having the zero everything because I
want the art max for every row in white
bread and that's gonna be like lots of
zeros and you see the last thing is a
four whereas actually it should be a
nine because the digits are in order
so here the last so this one got it got
correctly this one it's got correctly
everything that correctly that we see or
is like there is lots of stuff here that
we don't see and then 4 is not equal to
nine so that's incorrect