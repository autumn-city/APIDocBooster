Artificial intelligence has become very popular
in the last 10 years.
With the development of ai, problems such
as image and voice recognition could be solved.
Hello everyone from Tirendaz Academy.
In this video, we'll introduce deep learning
with TensorFlow.
Before moving on to the lesson, we produce
content related to data science, artificial
intelligence, machine learning, deep learning,
and programming.
Don't forget to subscribe to our channel and
turn on notifications.
Although the idea of ai back to the 1950s,
have you ever thought about 
why this has improvedin recent years?
First of all, the amount of data produced
has increased with the development of the
internet and technology in recent years.
With the increase in data, ai models could
be trained.
Another reason is the growth of the gaming
industry and the development of graphics cards.
A lot of calculations are made while training
the ai model.
With the development of GPUs, calculations
that could take years could be made within days.
In recent years,
another reason for the development of ai is the development of 
state-of-art algorithms.
We will take a closer look at these algorithms
throughout our lessons.
AI is a broad field.
Machine learning is a subfield of ai.
With machine learning techniques, patterns
in the data are found, in other words, information
is extracted from the data.
Machine learning algorithms were insufficient
to find patterns in big data.
Deep learning techniques were used to analyze
big data.
Deep learning is a subfield of machine learning.
Deep learning techniques are based on artificial
neural networks.
Artificial neural networks consist of an input
layer, a hidden layer, and an output layer
while deep neural networks consist of multiple
hidden layers.
In this video, I am going to talk about
What are deep neural networks and how do they
work?
How to set up TensorFlow? and
How to do deep learning analysis with TensorFlow.
Let's get started.
Deep neural networks have been developed by
imitating the human brain.
The first layer is the input layer, as I just
mentioned.
Entries pass through this layer.
The data is then weighted and a bias is added.
Initially, the weights are randomly chosen
and the bias is taken as zero.
There are neurons in the hidden layer.
You can determine the number of these neurons
according to your analysis.
Incoming data by weighting pass through the
activation function in these neurons.
It has various activation functions such as
ReLU.
For example, the ReLU function takes values
​​less than zero to 0 and values ​​greater
than zero to itself.
Nonlinear patterns in the data are learned
with the activation functions.
Data goes through hidden layers and the last
output comes to the layer.
If your problem is a classification problem,
there are as many neurons in the output layer
as the number of categories in the result
variable.
As you know, a numerical value is estimated
in the regression problem.
If your problem is regression then there is
a neuron in the last layer.
The last neural network predicts a result
with its output layer.
But how accurate is this result?
The loss function is used to evaluate the
accuracy of the model.
The loss function compares the actual value
with the predicted value.
And it calculates an error.
We want this error to below.
Since the weights are chosen randomly, the
first estimate of the model is bad, so the
error will be large.
So far, the data has passed through the neural
network forward and a result has been created.
The next step is to update the weights to
reduce the error.
Using the gradient descent technique, with
optimizers such as Adam, the errors are distributed
backward and the weights are updated.
With the updated weights, the data passes
through the neural network again and the model
produces a new result.
Again, the loss function calculates the error.
The calculated errors are distributed backward
and the weights are updated again.
Thus, this process continues until the error
of the model is minimum
TensorFlow is an open-source machine learning
platform developed by Google.
With TensorFlow, you can easily make your
deep learning analysis.
It can be used in PyTorch developed by Facebook
for deep learning.
But TensorFlow is more preferred for end-to-end
projects.
To install TensorFlow, you first need to install
Python.
You can download Python from its official
website and install it on your computer.
I recommend using a virtual environment to
work with TensorFlow.
The virtual environment is used to prevent
the libraries in different projects from colliding.
You can set up a virtual environment with
the pip used to manage packages in Python.
You can find out how to set up a virtual environment
with pip in python's official documentation.
But my advice to you is to use the Anaconda
platform for a virtual environment.
Anaconda is a platform that includes the necessary
libraries and tools for data science projects.
Many important libraries come loaded with
Anaconda.
If you want, you can set up a virtual environment
in Anaconda.
Let's open the Anaconda prompt.
The basic environment comes across at the
opening.
Do not forget that the libraries that are
loaded when you install the anaconda are in
the base environment.
We will create a different environment in
Anaconda by setting up a virtual environment.
Conda is used to manage packages and libraries
in Anaconda.
Let's create a virtual environment called
TensorFlow with conda.
I specified the python version to be installed
in the virtual environment with the python
= 3.8.
When you go to TensorFlow's own website, it
says that TensorFlow has been tested and supported
with Python 3.6-3.8 versions.
If I had not specified the Python version,
the final version of Python would have been
installed.
The latest version of Python can also be used,
but you may encounter some bugs as TensorFlow
has not been tested with the final version.
When you press the enter key, the virtual
environment is set up and I will not install
it again because I installed it before.
Okay, we set up a virtual environment in Anaconda.
Now it's time to install TensorFlow in this
environment.
Make sure that the virtual environment is
active while installing TensorFlow.
Let's activate the virtual environment.
You can understand that the virtual environment
is active from the TensorFlow text at the
beginning of the line.
Setting up TensorFlow is easy.
Let's set up TensorFlow
TensorFlow is loaded with this command.
Normally the conda command is used to load
packages into Anaconda.
TensorFlow's official website suggests using
pip to install TensorFlow.
With conda, the final version of TensorFlow
may not be installed, but with pip the latest
version of TensorFlow will be installed.
When we press enter, TensorFlow is installed,
I will not install it again because I installed
it before. 
Later, friends who will watch these TensorFlow
lessons can use the latest version of TensorFlow.
Since I will use the basic commands in these
lessons, 
there will not be much change between versions.
Jupyter Notebook is often used in data science
projects because it visualizes data well.
Let's install this notebook in our virtual
environment.
Let's write install jupyter notebook in anaconda
to Google.
Let's go to the Jupyter notebook site.
I copy the command here and paste it into
the command line.
When we run this line, the jupyter notebook
will be loaded.
Let's open this notebook now.
After Jupyter Notebook is opened, let's go
desktop and create a new Python file.
Let's import TensorFlow with tf.
If you do not get any errors when you run
this command, it means TensorFlow has been
installed without any errors.
You can also use Colab to work with TensorFlow.
Colab is a cloud service similar to Jupyter
Notebook offered by google.
Libraries such as TensorFlow, Pandas, NumPy
are installed in Colab.
You can use these libraries by importing them
directly.
To use Colab, you must have a Gmail account.
Let's go to https://colab.research.google.com/.
and I am going to press the new notebook button
to open a new notebook.
First of all, let's name this notebook.
Do not forget to click on CONNECT in the upper
right corner while working in Colab.
Let's import TensorFlow with the tf.
Let's see the installed version of TensorFlow.
And you can see the version of TensorFlow.
We can now build a deep neural network with
TensorFlow.
Non-linear problems could not be solved with
classical neural networks.
Hidden layers have been added to classical
artificial neural networks 
to solve non-linear problems.
Artificial neural networks with more than
one hidden layer are called deep neural networks.
Extracting information from data using the
deep neural networks technique 
is called deep learning.
The following problems could be solved with
deep learning.
Image classification
Voice recognition
Machine translation
Driverless cars
Let's use the mnist dataset to show deep neural
networks.
This dataset contains images of the numbers
0 to 9 in grayscale in 28 * 28 pixels.
This dataset includes 60,000 training and
10,000 test images.
I will use more complex data in the future.
The purpose of using the MNIST dataset in
this course is to show how to build a neural
network with a clean dataset.
I am gonna use TensorFlow and Keras to install
this dataset.
Keras is a high-level API and Keras joined
TensorFlow in 2019.
Keras installs automatically when you install
TensorFlow.
With Keras, you can easily do your deep learning
analysis.
Let's first import this data set.
Let's split this dataset into training and
testing.
The model is established with the
training data, and the model is evaluated
with the test data.
Let's write
and run this command.
The images in the dataset are encoded as a
NumPy array.
Labels are numbers from 0 to 9.
Let's look at the shape of the data sets.
I am gonna set
Let's copy this command and edit it for test
data.
I am ganna set test.
Let's run this.
Scaling data in deep learning analyzes both
speeds up the training of the network and
increases the accuracy of the network.
Let's scale the data between 0 and 1.
Now we can start building the neural network.
You can create the neural network as a sequence
of layers.
You can think of Layer as a filter that extracts
information from data.
I will use the Sequential technique for this
analysis.
The first layer is the input layer.
Flatten is used to smooth the entries in the
input layer.
Let's enter size of input in the input_shape argument .
The second layer is the Dense layer.
The dense layer is called the full link.
All neurons in this layer are connected to
all previous neurons.
Let's set the number of neurons in this layer
to be 128.
Layers can learn only linear transformations
of inputs.
Activation functions are used to learn more
complex patterns in the data.
It has activation functions such as elu, tanh,
but mostly relu activation function is used
in deep learning analysis.
The relu function takes negative values ​​to
0.
Let's write ReLU to the activation function
in this layer.
If the model shows good performance in training
data but poor performance in test data, an
overfitting problem arises.
The regularization technique is used to overcome
the overfitting problem.
There are regularization techniques such as
L1 and L2.
Let's use the Dropout technique here.
Dropout is a regularization technique used
for neural networks.
When dropout is applied to a layer, a random
part of the output is dropped during training.
Thus, excessive memorization of the model
is prevented.
Let's write 0.2 in the Dropout method to leave
20 percent of the output.
The last layer is the output layer.
I am gonna set
Let's write 10 to 
this layer since there are 10 classes in the
target variable.
Notice that I did not write an activation
function in the last layer.
So I built a neural network architecture.
It is necessary to compile the model to prepare
the neural network for training.
To compile the model, you need to specify
the optimizer and the loss function.
Optimizer updates the weights used in the neural network according to the value of the loss function.
There are various optimizer functions such as RMSProp, stochastic gradient descent.
The Adam optimizer has become particularly popular in recent years.
This algorithm is an extension of stochastic gradient descent.
Let's set adam optimizer.
The loss function measures the performance of
the neural network over training data.
It does this by comparing the estimate of
the neural network with the actual value.
You can use SparseCategoricalCrossentropy
as a loss function
when you have two or more classes.
In this analyze, I have 10 classes.
I am gonna set SparseCategoricalCrossentropy
If you have coded labels with one-hot, you
can use the CategoricalCrossentropy loss.
Since I do not use the activation function
in the output layer, I am gonna set the option
from_logits = True to the loss argument.
The metrics argument is used to see the performance
of the neural network.
Since the analysis is a classification problem,
let's use the accuracy metric.
So far I have built the architecture of the
model 
and compiled the model using the compile method.
Now the model is ready for training.
Now let's train the neural network by calling
the fit method.
I am gonna set
Each iteration on training data is called
an epoch.
So it takes 1 epoch for all data to pass through
the neural network once.
At each iteration, the neural network updates
the weights according to the loss function.
During the training, the loss and accuracy
of the network were written on the screen.
After the 5th epoch, the accuracy of the model
was about 0.98.
After training the model, evaluate () method
is used to evaluate the performance of the
model on the test set.
I am gonna set
Thus, the accuracy of the model on the training
data was written on the screen.
The accuracy of the model on the training
data also turned out to be about 98 percent.
The accuracy value of the model
on the training data is desired to be close
to the accuracy value on the test data.
So we built our first neural network.
We will mention deep neural networks more
closely in future lessons.
Thank you for watching the video.
To support us, don't forget to hit the like
button and share our video with your friends.
Also, do not forget to subscribe to our channel
and turn on notifications.
Bye, for now, to see you in the next video.