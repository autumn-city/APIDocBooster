Let me just stress that the LSTM cell has learnable parameters which are shared among all the features. Anyway, the K matrix is nothing else than the output hidden state of our LSTM cell.
So it only receives one character as the input, then for the one layer of the hidden, the initial hidden and cell state, and it only outputs one output here, and then the hidden and cell set for the next state. So why that is more useful is in a way for computing the loss  essentially that you can get one thing at a time, essentially  instead of running the whole thing.
The first argument to super() should be class itself, not a different class.
The input gets passed one after the other, for each particular LSTM cell.
So these can be any variants of far, just like I mentioned, either LSTM or GRUs, if we're considering LSTM, then each of the intermediate hidden layers, each of the intermediate hidden cells will have two outputs.
