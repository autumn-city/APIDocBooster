So yes, lr is just starting learning rate. 
However, note that, by default, decay parameter for Adadelta is zero and is not part of the “standard” arguments, so your learning rate would not be changing its value when using default arguments. 
learning_rate: A Tensor or a floating point value. 
If you really want to use Adadelta, use the parameters from the paper: learning_rate=1., rho=0.95, epsilon=1e-6. 
Although as you can see in tensorflow [source code (hyper-link)] to achieve the exact results of Adadelta paper you should set it to 1.0: 
