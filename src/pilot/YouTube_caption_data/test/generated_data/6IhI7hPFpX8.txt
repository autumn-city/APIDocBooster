hello everybody welcome my name is Chris
screen ACK I am a partner Solutions
Architect and the global machine
learning segment lead for the Amazon
partner network I'm really glad you can
make it to this session on PI torch it
is a topic I am passionate about and
where it sits in the machine learning
stack is sort of right at that
fundamental grassroots layer starting
from the bottom up as you know we have
our infrastructure our compute structure
you know one level that's kind of
beneath this is the storage structure
right in machine learning I frequently
say data writes code that is the
fundamental Software Engineering change
that we're experiencing right now in
this revolution unlike in the sort of
the old days which were three years ago
you're really looking at data now to
write highly nuanced functions that were
either difficult or impossible to do and
that's all thanks to the advent of some
changes in the way that we do back
propagation for example in deep learning
models but also the hardware right but
you don't have anything unless you have
data this is a situation where the data
writes the code you need a
infrastructure like the kind that's
provided by s3 and the variety of
database systems purpose-driven database
systems that we have including
relational graph in-memory databases
even blockchain so that's that
fundamental layer and here we take a
very non prescriptive approach to what's
frameworks you should use pi torch
recently really just in the past year
has risen to at least parity and perhaps
even surpassed some of the other
frameworks mostly do in my view I hope
you guys agree that it's just so
readable it has a couple of technical
features that are really nice but when I
started with PI torch in the beginning I
just thought I can read this I can share
it I can work with my colleagues and do
things that I was not able to do with
other frameworks now the middle layer of
course is sage maker and now with the
advent of sage maker studio we have a
truly comprehensive approach to doing
machine learning
from the labeling that's required in the
beginning to deploying in the cloud and
also on mobile n IOT and then of course
the top layer of this cake are the
managed services where we've taken the
data we've taken the algorithms and
we've put it together where all you
really need to do is begin using the
endpoints in the way that you find best
for your purpose so I want to start with
a couple of basics first of all when
you're using sage maker you're also in
part using what we call the deep
learning ami ami stands for machine
learning image I'm going to do a very
brief demo on how to bring that up and
use it but the reason I pointed out is
it is a full set of it's a full set of
products for deep learning for machine
learning that is managed as a product
what I mean by that is every two weeks I
believe is the cadence a new version of
this ami comes out you can track it in
the machine learning blog please do and
when you start up a sage maker instance
it's inheriting a lot of that good work
then I'm going to take a look at what
are the fundamental tools that we use in
sage maker to deploy our training in the
cloud and our inference for hosting and
that is really our estimators now when
you look at this code right here what
you'll see is you know the word
estimator you've got a roll in there an
instance type what's actually happening
behind the scenes is the sage maker SDK
is helping you write a docker container
now when you go to fit that data then
you're actually going to build the
docker container and then submit it to
another machine in the cloud that's
going to go train that data or then host
that system I'm going to demonstrate
that as well but the fundamental
takeaway here is to know that docker and
really s3 because we usually your data
is in s3 that you're using to train are
these two fundamental foundational
components that help enable Sage Maker
to be the leading IDE last but not least
I'm going to demonstrate how you can
actually use your own laptop using a
feature called sage maker local so that
you can use an IDE like Visual Studio
code to develop code on your laptop but
then train in the cloud and deploy at
endpoints in the cloud as well last but
not least I want to mention briefly PI
torch lightning which is another tool
that's being used to help solidify the
code that you don't need to change when
you're doing nuance changes to building
deep learning models and really focus on
the stuff that is going to make your
model differentiated so I'm going to
switch over now to my demo machine here
hopefully I hit the right button there
we go and I have to start from scratch
here so forgive me while I login all
right cool so when you bring up sage
maker of course you're instantly
prompted or more likely to create a
notebook instance Jupiter notebooks are
the principal tool that we use to create
our models I want to show you that
within five minutes you can be running
and generating models using PI torch
I'll just you know it's December so I'll
name this notebook December and I'm
gonna come down here to the git
repository part of notebook creation and
say I'm gonna clone a oops that's not it
I'm going to clone the sage maker a nut
sage maker the PI torch examples repo
I've got that in my code right here to
just cut and paste and when I hit create
notebook instance here I'm instantly
going to create a notebook that has PI
torch code ready to go on the cloud now
I have a pre-baked a pre-baked notebook
instance here and here's the sage maker
browser I created something in the EM
mist data set where I just took the the
main PI file that's in there and created
a little notebook out of it and all I
have to do here is run all the cells and
within what was that although if you if
you take out the part that I pre baked
the two minutes to start up the instance
I'm already running genuine PI torch
code and training it in the notebook now
the question is how
do I get that into the cloud well before
I just go straight to the cloud what I'd
like to do is actually show you how to
bring up an ec2 instance with the deep
learning AMI now sage maker is the best
placed and the safest place to create
machine learning models without question
but sometimes it makes a lot of sense to
to create an ec2 instance from scratch
if you click on launch instance here
I'll push this over to the side and over
here on the Left you'll see
AWS marketplace you have a choice of
Amazon machine images now these are all
the things that you need to create the
operating system all the drivers that
connect with the GPUs etc and also if
you type here the deep just deep
learning whoops
you have just about correctly you'll see
that we have some Ami's that are
specially made that have a number of
components built into them let me just
select this here now as my machine
learning image and you can read this in
some detail later but here are all the
tools that are built in now if you've
ever tried to build this on your own in
other words take your own machine
whether it's Linux or Windows put in the
drivers for Nvidia for CUDA for your
your framework of choice whatever it's
maddening here it's all done for you
this is a managed product at AWS like I
said it's updated frequently my machine
I have a laptop that has an nvidia card
in it and i spent 3 grand on it just so
I would have that worked perfectly in
August but today in December it doesn't
work anymore why because these
dependencies keep changing I mean now
we've got PI torch 3/8 that's just one
thing the Nvidia drivers change here all
you have to do is select that ami press
Continue launch this this instance and
I'll come back here just a bit and show
you that I've got one running here right
now now the way I tend to access my ec2
instance is is I just use this little
helper code that's you can get when you
just right-click on this but just as
things along here today I have some
terminal windows here that are well it
looks like this one's already running
and what I've done is I just went into
that system and I brought up a Jupiter
browser there whenever you do that I'll
just make this really big so everyone
can see it gives you a URL that you can
use on your own machine on my other
window here from my local machine i
tunneled in through ssh to that machine
now I can just go back to my browser and
connect through that tunneled connection
and Here I am now I'm in a you see a the
standard the default Jupiter notebook I
can tell that because I don't have sage
maker examples up here and I'm looking
at code that is on that machine now one
of the things that I have up here is pi
torch lightning one of the reasons I
like to point out pi torch lightning is
it is a standardized way of delivering
code for nerps it also has those
features that I was saying that make it
really easy to focus on the code that
you're changing the model that you're
actually changing to make your model
unique one more thing that it does
that's really useful and in helpful is
to take the size of your nodes in your
deep learning model and actually use
something other than 32-bit it's called
partial precision so by default when you
create a deep learning network you're
going to have 32-bit sized nodes you can
actually do sixteen eight and even four
bit precision now that opens up a lot
more nodes in a smaller memory space but
it also has this effect that dropout
kind of has where it can actually make
your model a lot more efficient okay now
one more thing before I hand it over to
my colleagues here to talk about is sage
maker local now I mentioned that there's
a feature in Sage Maker called sage
maker local where you can specify when
you are deploying your system
either your instance type let me make
that a little bit bigger so that you can
see is either one of our ec2 instances
in the cloud in this case it's an M for
extra-large but where I to change this
in the estimator to local like that it
would train in my notebook or if you do
a pip install sage maker on your laptop
and you download those containers you
can also have the same functionality on
the on your machine now that's really
helpful for sort of the the busy work
that you have once some code is in
production you have a small sample set
of data on your local machine you want
to make sure that all the bugs are out
this also works for notebooks that
you've launched in the cloud but here's
the virtue of this particular feature of
sage maker each of the components in
Sage Maker whether that's ground truth
the notebooks the training or hosting
systems are designed to work independent
of each other and when you take
advantage of these features for your
workflow you can do something like this
when I run these these cells you know
let me just see if I can run the whole
notebook at once run current IO whoops I
didn't does it pick the wrong one there
just give me a second here there it is
that should run the entire file in my in
my Visual Studio code here what's going
to happen oops it looks like I might
have typed a little error in there just
as I was running this that's the beauty
of live demos I'm just gonna run these
all one by one so I absolutely ensure
that I fit this and here we go so fit is
like I said it's that commitment I don't
know why I'm getting errors here I must
have a typo so the short story here I'm
not going to debug live is that once
that gets to the fit command I can come
over here to search me sage maker go to
training jobs we're literally like an
hour ago I did this and it worked and
you'll see that the job is actually
training in the cloud so you're on your
laptop you're using visuals
not even a jupiter notebook and your
training in the cloud so those are the
features that I wanted to highlight from
sage maker and some of the things that
you can do with pi torch and now I'd
like to introduce Michael from Facebook
to tell us what's new in pi torch all
right thanks Chris
so hi everyone I'm Michael I work at
Facebook on PI torch and I'm here to
give you a little bit of a whirlwind
tour on what we've been working on
lately one of the newest features in pi
torch both in terms of the core library
but also the surrounding ecosystem
around pi torch that enable you to do
domain specific model optimization
domain-specific learning and all other
sorts of things with your models so what
is pi torch I assume because you came to
this that you kind of already know a PI
torches but I can I can talk a little
bit about what it started as it started
as basically a GPU accelerated tensor
library with an API relatively similar
to numpy one thing that we observed with
the old torch that was written in Lua
was that people really liked the
imperative style of neural network
construction that torch gave you and we
wanted to bring that to Python because
python is sort of the the lingua franca
of numerical computing and scientific
computing so you can see here the way
that a graph is defined in pi torch is
very similar to regular Python execution
right you know you define your operators
you can use overloads operator overloads
like this plus operator and a graph is
defined based on how you ran it so we
also added a number of services on top
of the base tensor library in particular
the thing that made it very useful for
deep learning is that we added a
tape-based auto grant system it composes
well with the imperative style of Pi
torch and it essentially allows you to
take gradients and compute law it can
compute gradients with respect to your
loss very easily with the simple call to
backwards instead of doing so
a symbolic differentiation step this
also makes it a lot easier to use things
naturally like control flow if
statements loops so your code really
looks more like Python code what Chris
alluded to when he said PI torch tends
to be very very readable it's because it
just looks like numerical Python code so
since then PI torch has evolved quite a
bit from the original sort of structure
of it we started with a dynamic neural
network library that supported tensors
and auto grad but since then we've added
you know both eager and graph based
executions so for when you need access
to the full structure of the graph in
order to properly optimize your model or
deploy it we've added distributed
training we've added other hardware
backends besides just GPU and
maintaining the idea that we ought to
prize simplicity imperative necessitous
over you know sort of doing things
magically for you or making the overall
library very complex our goal is for
model code to look very simple and our
goal is to always be sort of
understandable and we evaluate every new
feature on that basis so I'm going to
talk about a few of the new picha
features we're working on and have
released in pi torch 1.3 so you know if
you download the deep learning ami from
Sage maker you get all of this because
you know they've always provided an
up-to-date version of Pi torch for your
usage so the first thing we've added is
support for mobile so this is one of the
most commonly requested features for pi
torch it allows you to do an end-to-end
workflow from eager mode where you just
define your neural network in python to
deployment on ios and android without a
need to convert to another runtime with
different semantics or like a different
format it's the same all the way through
if your model as executed from python
will be precisely the same as your model
executed on device so we wanted
deploying to mobile devices to be as
natural as deploying to any other device
you know server or otherwise and we
support any model that is torch
scriptable and I'll talk a little bit
about torch script as an enabling
technology later and out of the box
we've provided pre-built binaries for
iOS
and Android was support for 8-bit
quantized libraries qnn pack and FB gem
had release this allows you to sort of
shrink your model so that it runs more
efficiently on device so another thing
we've released in 1.3 is support for
quantization so this goes very well with
mobile because it's a very component
important component of running on mobile
but it's also useful for server-side
applications so quantization refers to a
technique where we take the
floating-point weights or activations
and we essentially make them into and
eighth most commonly or even in for
people have done all sorts of different
things and essentially it leads to both
more compactness and form of memory
usage but also speed ups in the total
inference time and these are quite
substantial often there you know integer
factoring you can see we support out of
the box sort of a fairly explicit way to
do quantization you just write you know
you write quantization prepare and you
pass a model as well as a quantization
configuration if you want to customize
you know which modules are getting
quantized and which aren't but by
default we sort of quantize everything
that matters and then you know you just
write you write an eval and you actually
run the model a few times to calibrate
the quantization statistics you know
your scale and zero point and then you
just convert your model and this
essentially quantizes all the weights
and it's a it's a key component of
making pi torch work well at production
scale because this is a fairly common
optimization technique another thing
that we've been working on is named
tensors so this is a great example of
what we can do when we prioritize the
sort of core usability of pi torch as a
library and collaborate with the
community and other researchers to work
on new features so this was initially
proposed by Sascha rush at Harvard who
is a core user of Pi torch from you know
the very early days and he proposed name
tensors as a way to get information that
you typically like encode in comments or
don't encode at all and get it in the
code directly get it checked at runtime
so you can specify you know these are
the channels this is the width
and this will be you know any of your
transformations and operators will check
it run time that these remain consistent
so we're launching name tensors with 1.3
it'll be out of experimental in coming
release 1.4 and enables cleaner better
code that more expresses more directly
expresses your intent so we should also
take a look at torch grips so I
mentioned it before as one of the
foundational technologies that were
investing in in order to support pi
torch at production scale this is
essentially a way of getting a
computational graph out of a PI torch
model you know you get Python out of the
process you have access to the full
program so you can use optimization
techniques like quantization or
you know simply algebraic
simplifications like common
sub-expression elimination or dead code
elimination so as you can see the
workflow is pretty similar as simple you
define a torch and end module so if
you've ever worked with PI torch this
should be very familiar here we you know
we specify a single weight and we have
some state and we define a forward
function you instantiate said module and
you just pass it to torch it's script
and what that function does is
essentially runs a compiler that parses
your Python code and turns it into torch
script which is a format that we can
understand and an intermediate
representation that is more tractable to
optimization so there are a couple
things to note here we try really hard
to support pythonic language constructs
you know typically when we talk about
graph based machine learning frameworks
you think that okay now I have to sort
of fit me my ideas around the specific
kind of domain languages that are
provided to me but here an if statement
is just a Python if statement a list can
be represented as a list you can append
to the list and all side-effects are
sort of preserved loops also work and
you can just call dot save and you get a
serialized representation of your model
you can then pass that to mobile for
deployment you can then run quantization
on it etc and the counterpart to torch
script is the PI torch JIT which is an
optimizing just-in-time compiler for pi
torch programs it consumes these PI to
us
toward script programs and they're sort
of represented internally as you can see
here it's the structure is a lot more
regular everything is statically typed
the control flow is structured control
flow and this allows us to do full
program optimization it's also not just
for inference this seamlessly supports
auto differentiation so we can define
sort of symbolically differentiable sub
graphs from this graph and use and use
that for training as well backwards will
always give you the correct result and
it's sort of you know seamlessly
interoperates with control flow and the
default auto grad engine for pi torch so
those are the new features that we
released recently in 1.3 and are going
to be improving in 1.4 and now I'm going
to talk about some new libraries and
what highlights we have from both the
community and also released by Facebook
as well so the first is Krypton at
Facebook there's been a huge amount of
interest in pushing forward
privacy-preserving machine learning
techniques and the challenge is to
provide you know the same value that PI
torch gives you while protecting the
data used to train models so with 1.3 we
released 10 which is a platform in
PI torch for research and machine
learning using secure computation
techniques we aim to enable machine
learning researchers who are not
cryptography experts to experiment with
models using because secure computing
techniques and to get a realistic view
of what's possible you know what's
difficult how efficient these techniques
and as you can see we deeply integrate
and leverage PI torch in doing so we
closely followed PI torch design
principles so a crypt 10 tensor crypt
tensor is basically looks and acts
exactly the same as a regular PI torch
tensor similarly you can just add two of
them if you call common torch functions
it'll it'll handle all the sort of
encryption under the hood and so you
have sort of seamless interoperability
with the sort of base PI torch library
and we're hoping that this ease of use
lowers the barrier of entry for machine
learning researchers and developers who
are already familiar with PI torch and
the abstractions that it provides
similarly with the increase in model
complexity model interpretability
methods have be
very important and is a substantial area
of interest both in the community at
large and at Facebook so model
understanding is a really active area of
research and we also have a number of
practical applications across industries
using machine learnings so we're
releasing a new library designed to make
interpretability algorithms accessible
to all pi torch model developers it's
called captain and it allows you to
improve interpret outputs with respect
to inputs and layers and neurons with
respect to inputs it provides an
interactive visualization tool that
helps you understand and debug the model
predictions that you're getting and
finally beyond new libraries that add
sort of core functionalities to PI torch
we are also you know supporting a number
of frameworks that allow you to use PI
torch in a specific domain to solve you
know your specific business problem so
the first of these is detector on 2 we
open sourced this with 1.3 as well it's
the second generation of Facebook AIS
system that implements state of the art
object detection algorithms so as I
mentioned earlier it's used - it's used
in the portal and it's also shipping in
several other Facebook projects that
need object detection and segmentation
techniques so you know improvements over
detector on one include support for the
latest models and new tasks we've
increased the flexibility to aid
computer vision researchers as well as
people interested in deploying for
production and we have sort of you know
baseline improvements and
maintainability and scalability to
support our production users as well the
next framework is fair seek which we've
extended to support sequence to sequence
applications for speech and audio
recognition as well as what we you know
typically did which was language
translation so these extensions allow
fair seek to enable faster exploration
and prototyping of new speech research
ideas and are also used in production
today so these are some resources like
our social media stuff and our website a
lot of these features we you know are
still works in progress and we'd love
your feedback and using them if you file
an issue on github for any of these you
might see me you know like commenting on
it and so we'd really appreciate any
feedback on new features as you try them
out and now I'm gonna hand it over sorry
oh that's unfortunate well I assume you
can find us on Facebook or if you go to
like github comm /hi torch that's mostly
where we hang out in terms of PI torch
development so thank you and I'm gonna
hand it over to Alex to talk about how
they use PI torch at Autodesk hi there
so my name is Alex O'Connor I work for
Autodesk and I start I think I start by
talking a little bit about who we are so
our products range from construction
manufacturing and media and
entertainment anytime you see a major
construction project a large building a
high performance vehicle or some sort of
3d animation in games in movies there's
a good chance that one of our products
has been involved in it we have 30 years
nearly 40 years of experience with a
variety of these tools some of our most
commonly known products are things like
AutoCAD and Revit and newer products
such as fusion 360 in the 3d printing
domain so we have a millions of users
they are all around the world and
they're an interesting group because
they are highly professional highly
expert people who work in very complex
domains so that means that we get
thousands of queries and thousands of
customer questions every month and those
questions range in enormous enormous T
and the types of questions that they are
in the complexities of the solutions
that are required and even in the level
at which we can address the problem for
them so this really does range from
relatively straightforward tasks such as
how do i download a particular version
of a software how do I get an extension
how do I change a language pack all the
way up to one project which one of my
colleagues discussed with me which is
where they were doing a flow analysis of
a structure and it took two days for the
HPC environment to just resolve the file
before they could even begin to solve
the problem so we had real problems in
trying to understand how we would
support those people and that creates a
challenge because if you try even as a
human to understand the full range of
complexity of the terminology the
support domain the knowledge domain and
the professional practice that's
involved it's simply not possible and
you know very often K these technical
cases come in and I could spend a week
on Wikipedia reading the articles and
probably still not come to an answer as
to what they were actually talking about
so that creates an extremely complex
problem for our support teams and it
creates an extremely complex problem for
us to resolve and thankfully that means
they needed data scientists so they
hired us and what's interesting about
this is that we fundamentally have to
understand that we have a language
problem here we have a pattern matching
difficulty and you know this quote is
about how people use memes on the
Internet
but it's true about how people use
technical language it's how people use
keywords it's how people use
professional practice and language in a
way that is important to them so even
when we see it appears completely
incoherent to us even when it's
something that is totally opaque to the
to the layperson it is nonetheless
something that we can learn patterns
from and that we can make it take
advantage of and that we can scale and
automate and operate on so really when
you think about this domain we like to
think of it in the context of natural
language processing really what you're
asking yourself is how do we take this
set of words this set of repeated tokens
and represent the concepts the context
the meaning and all of those were in all
of those words to a computer in a way
that we can act on it and that's a
problem that's been with us for 70 plus
years and there's a problem that will be
with us for another I think as long as
we are all speaking to each other but
we've made enormous gains in the last
decade from the fact that we've been
able to leverage the sort of
technologies that Michaels teams are
developing and the sort of
infrastructure that Chris's team is
deploying and so we have this essential
challenge with language data the words
don't arrange themselves in neat ways
people get and play the language games
so they use metaphor they use simulator
they use contractions they indicate
typos and so we need to be meticulous in
how we prepare data to train the models
and how we use that to advance things so
just taking a very simple example we can
take this sort of
use complexity version of what you might
think of as being the sort of typical
data that most people will be involved
in seeing if they are in this support
natural language environment so we have
a customer they submit a problem they
say something like they're using this
particular Revit product which is in the
construction domain they have a serial
number and they also are not terribly
good typists and this is the sort of
typical data that we get this in fact is
what you might call a laboratory or a
spherical kayo version of these things
because we rarely get users that are
this explicit or this organized in the
way that they they state these things
because frankly their time is valuable
and they don't need to they don't
necessarily lay these things out but we
can imagine that this is a pretty
straightforward way of doing things and
part of the frameworks that you might
want to use when you're addressing this
is that there is in fact a PI torch text
framework or we use the Spacey framework
to extract these key phrases and extract
this boilerplate text at the top these
pieces of the form from the body of the
text that we're most interested in so
fundamentally our task is to take this
body of the text and ask ourselves which
team of agents should fix this problem
and in doing so we can find an enormous
saving in time an enormous saving in
resolution time for the customer and
enormous saving in time for our teams
and an enormous cost saving in getting
up to the right place at the right time
plus we don't have to hope to keep
people on hold we don't have to keep
people waiting for an answer and in a
professional context time is money
waiting to resolve a problem that is a
big one for an architecture firm who
have a project on the line means that
there's potentially millions of dollars
in getting this correctly done so that
leads to our team and our mission and
our come a fundamental question which is
you know how can we help our customers
ask questions they need answered in
their own words so we want to get back
to this situation where to the minimum
extent possible our customers should not
have to learn our terminology shouldn't
have to learn how we think about our
products and ensure instead we should be
able to meet them more than half way
along the way and that means that we
need access to the state-of-the-art in
machine learning we need access to rapid
deployment so that we can put those
models out there and when they break fix
them and replace them with ones that
work better and we need to be able to
share things amongst our team and among
other people in our company because
we're all working on these complex
terminology terminological collections
we're all working on large volumes of
data and we don't want to be in a
situation where everyone is downloading
the data multiple times processing using
in different ways and wondering why they
get different answers so the technology
that I'd like to talk about today is the
transformer the reason I'm choosing this
is that it is buzzword compliant in the
most extreme way it's the most kind of
interesting thing that's happened in
natural language processing in all of
two years and what it does is that
transformers represent a step change in
the performance of natural language
processing because they allow us to do
more with less in a way that is
revolutionary and the three main kind of
technologies I draw your attention to
are hugging face who are a start-up that
are in the natural language
understanding and virtual virtual agent
environment but also our amazing
contributors to the open source
community fast AI who have a another set
of transformers know the methodologies
for doing NLP and pi torch which for us
is the domain of the framework for
choice for a lot of this work so why do
we choose frame excuse me why did we
choose PI torch well there were really
three things that we found to be the
reasons for this and I sat down with
some of the members of my team to really
think about this because we wanted to be
sure of what we thought were the things
that would make it compelling for you to
pick up a torch in what is a very
competitive and very noisy marketplace
right now for your attention I think
number one was the interest in the fact
that it gives us access to research code
so there have been multiple papers over
the last couple of months which have
shown that PI torch gets the most
mentions in the major venues especially
in natural language processing and that
it provides act direct access to these
sorts of insights that the research
community has given us both from the
commercial research and from academic
research and the ability to get from a
paper to a deployed end point is an
extraordinary thing that even five years
ago I think would have been
unrecognizable the second feature is the
one that I think Michael talked about a
lot which is the intuitiveness of the
interfaces there's really one way to do
most things in PI torch which
a great advantage because if there's 12
ways to do something in a framework you
can imagine that most developers will
come up with a 13th and get it wrong as
well so for us it's very useful to be
able to very natively approach the
problem and be pretty clear on why you
have gone a particular way down down
that route and finally new features the
integrations as they come along your
execution and all those sorts of things
make it very attractive for us to be
able to take advantage of those things
as pi torch develops so what is a
transformer the key understanding about
a transformer is that when you think
about language there are a couple of
different tasks you need to be able to
do the first thing is that you need to
be able to understand and organize
yourself in the actual language itself
so there's a whole higgledy-piggledy
mess of words and tokens that are kind
of flying around and the the model needs
to make sense of that before it can
address your question which is which
support team should I refer this support
case to and so for us that meant that
when you were training these models
these more naive or more shadow models
in the past you had the problem that
trying to learn both of those things at
the same time was essentially impossible
or led to a great deal of confusion and
overfitting and a great deal of missing
of an important nuance in the models so
what we did was or what not what we did
excuse me what the research community
did was realize that what you probably
need to do is learn English or learn the
language that you're trying to learn
before you go at the specific task and
what in that case what we need to do is
we need to have a multi objective
approach we take a large large amount of
data hundreds of gigabytes of text and
we just read it and what we do is we
learn multiple simultaneous objectives
at once so we learn in in the case of
Bert you learn a master objective where
you try and predict the word as it is
you mask out a random number of words in
the sentence and you try and guess which
word should be in there with the
objective of overtime learning these
multiple objectives gives you a higher
performance on everything so the system
has learned a sort of a general model of
the of the language before it even gets
down to this task of your specific
system and then we use the concept of
transfer learning to
that's large general model and put it to
our specific use case and that means
that you can use it for text
classification you can use it for
language generation so the GPT -
transformer has got a lot of attention
because of people using it to create all
sorts of things like poetry and movie
scripts and all sorts of other things
and in machine translation as well and
in all of those things we have that
situation where it is if not
state-of-the-art then very near suitable
for a wide var of tasks and what's nice
now is that we know that it works pretty
well in Chinese and the exquisitely
named Cameron Burt demonstrates that it
works very well in French as well which
is I think further colleagues of yours
very briefly the intuition behind this a
Bert model that I'm going to talk about
is the idea of multi-headed self
attention that means that I'm going to
look at the whole sentence at once I
mean I'm going to try and infer the
importance of each token relative to the
others from multiple different
perspectives what that means is that I
don't just take the single context of a
token one at a time or just look
backwards I look across the whole
sentence all the time in multiple ways
and that gives me my subtlety my new
also my ability to to manage it and if
you want to train these from scratch you
can go at it with the torch and n
modulus transformer which we're very
grateful for and one thing I would say
about this is that this is not at all
limited to language transformers are an
excellent way of doing cogeneration
they're an excellent way of doing a wide
variety of other tasks anything where
you have about a hundred billion
instances I would recommend you have a
shot with a transformer but also check
your credit card limit before you go at
it so how do you use a transformer that
this slide is painfully out of date
because I think I wrote it six weeks ago
and so 1.6 billion parameters is now
that's amateur hour for transformers but
it's still a relatively large model
Trainer multiple objectives self
supervise the important thing about this
is someone else trained it so Google
trained it or Facebook trained it or
Salesforce trained it or open a I
trained it you can immediately take
advantage of their millions of dollars
of investment and straight up start
building your own model so you've
trained that pre trained model from
someone else
you optionally decide that you want to
tailor it so for our situation we have a
situation where we have a lot of
language that is not used in the
conventional way the meaning of words is
based on the fact that we have
architects or we have engineers talking
about things so we might want to
slightly teach not just English but
ordered as English to the general model
once we've done that once it's
repeatedly available we don't need to do
that again
and then you fine-tune the model
yourself so then what you do is you say
okay we have this general language model
it does all these multiple objectives it
knows English it knows grammar it knows
Chinese it knows all these sorts of
things so what we do is we attach a new
head to it and we say okay now instead
of doing all those old objectives that
we were interested in we want you to
predict a support label tell us which
support team is interested in this or we
want you to generate the next sentence
that comes after this or any number of
other objectives that might be
interesting along the way so how does
this look and one of the nice things
about the Bert model is that it's really
simple to produce the data all you need
to do is reformat it into the format
where you have the words separated by
two tokens and in fact this is how it
looks when Bert itself tokenize --is
that and you'll notice that there are
these m hash mark symbols and what those
represent is sub word components so the
Bert has learned not just whole words
but parts of words in a somewhat random
way but nonetheless you can see here
that we have our our sentence I can't
install Revit 2019 and it's split Revit
into two tokens because it doesn't know
the product name on itself so Bert can
take advantage of that it can take
advantage of these sub words it means it
doesn't isn't dependent on the
conventional approach so how do we
attack this problem from a practical
perspective step one is to decide if you
want to have a base or a custom model if
you've done this fine chaining or not I
recommend just starting with the base
model if you've nothing else it's free
and relatively easy to get and I'm
taking the adaptational the code here
from the hugging face transformer
library and this example run glue model
so glue is a large evaluation data set
which itself is now out of date because
as I said six weeks is a long time in
machine learning and we preload the
files from s3 and adapt the data loader
those are the only two tasks we need to
do to make this available on pike on em
sage maker so for this may be familiar
this is almost exactly the same code as
Chris had by by happy coincidence and
what we do here is that we have a
slightly different set of arguments but
otherwise is very much the same in this
case I'm using a p38 X but it apparently
today the AWS team managed to train the
base bird model in 69 minutes on PI
torch using a 192 DN instances so if
you're really in a rush you can do this
much faster but we use this sort of a
model we use this approach we use
Framework 1.0 apologies but that's
that's a dependency from the the the
transformer or that excuse me yes the
Transformers model and then you can see
as the same thing again we just have an
estimator and a fit and we just train
the model as per normal so almost
identical here except for how we we do
these things one of the interesting
things and one of the best features from
our perspective is you can just include
a requirements txt file with your Python
file for your estimator and it will pre
install all of this data for you so
it'll it'll pull down all of your
libraries for you it'll pull down the
transformer library so you get all of
that stuff without any sort of
management of the dependencies in a
complex way all the magic happens in the
in the hyper parameters is normal you
have to specify your labels so these are
seven different support labels that we
can use you can see the one on the right
is technical support and I won't get
into what the other ones are and then
the only other things you need to know
about are this is your base model in
this case we're using the burt base on
cased and we just have to make sure that
we lowercase everything because of that
thing so make sure those things agree
with each other Bert looks at a fixed
sequence length so you can set that
sequence length its trained at 512 so
you can use anything less than that in
whatever way you like and there are
different masking schemes for different
transformers there but that allows you
to speed up a little bit if you know
your sequences are shorter sequences
count sub words though so be careful
with that because
those M double hash I tease and things
like that will pop up and can sometimes
be slightly longer than you expect and
obviously then the normal things that
you know batch sizes learning rates and
number of epochs and typical typical
values here are as you would expect
between one and eight epochs and
something along the line of the start at
a lower learning rate and go upwards
sorry
yes that's what that sequence excuse me
that's what the max sequence length does
that is the maximum width of the tokens
and what it does is apply a mask to that
to pad out automatically if it's not not
see it not in that space so then we just
load the model load from pre-trained
pull it from s3 you need to learn load
you're tokenizer as well because you
might have a specific vocabulary there
one nice tip here is that you get some
inverts you have some unused tokens up
to a thousand so if you have custom
tokens that you would like to use you
can replace those in there and you're
fine tuning you should get access to
that which is a neat way of taking
advantage of the way Bert has been been
structured and then you in this case we
attach the head in the second line there
Bert first sequence classification is
the tool for creating text
classification models and it's a very
straightforward apart from that and in
this case that the the two the arguments
that you see there are fairly
straightforward one is a model path
because this is stage maker that can be
anything that sage maker can access the
number of labels just comes from your
training data and there are from your
hyper parameters and then from TF is
sometimes you might want to use a
tensorflow compiled model and you can do
that in this context if you want to use
the the truly based models or some other
things when we run the training and
typically the way these models work is
you warm them up so you give them a
higher learning rate for a certain
amount of time then you optimize after
that so we found that a hundred warm up
steps works don't ask me why maybe it's
cuz it's a nice random number you can
also use proportional warm-up if you
prefer you can change the number of
steps you can do all that sort of work
that's all that stuff is a good way of
testing your model but
would do that after you've done other
things with epochs and things like
learning rates I would also say that one
one interesting thing here is that it's
been observed that sometimes
transformers just fail so sometimes they
just don't initialize well and you
sometimes need to just restart the
training and it'll work better the next
time and maybe change the seed so there
are there are it's all that lottery
ticket stuff about whether or not you've
picked the right initial parameters and
so sometimes they just don't work and
that's not just an excuse apparently
it's true as well
and then we use the Adam W optimizer
that's all been very neatly reorganized
and in this case we're reproducing the
paper for Bert's so we have correct bias
off but you can change all of those
things as you see fit so that's
basically how you would use Bert go and
do it you can have state of the art or
near state-of-the-art text
classification probably in a couple days
and certainly leveraging an amazing
amount of research and work from
everyone else well I should say also
that's the other key thing about this
your label data set for classification
doesn't need to be remotely as large as
it as the full data set there you can
work on far fewer examples because you
have this ability to learn from the
baseline model so just a few other
things about how we use this one thing
that we do is we use everything we're
very sage maker focused in our team we
have a shared space on EFS that we put
data we put our analysis we put our
notebooks we do an awful lot of code
sharing visualization through it we put
our pre process no nope data sets
because of the large amount of work that
can often be done to have to pre-process
a dataset for language we use these
custom profiles for our sage maker
instances and we frequently backup
because there's not a lot of access
control on a share DFS and we often
worry about someone hitting the delete
button by mistake but for us it works
very well it gives us this global
scratch pad that we trust our and we
trust our colleagues with and we all
work in the same office so it's easy for
us to shout at each other if we get it
wrong you know what do we share we share
all the data that we've cleaned all the
pre process information all the enriched
data and then a wide variety of the
utility models all of those things
helped us a lot because they allow us to
get much faster at iterating through
models much faster at handing off code
and much faster repurposing specific
models for things and where you have
things
like these fine-tuned models they're
extremely useful to be able to share
those in a in a rapid way we're also
experimenting with the use of model
services we attach an endpoint or we use
a batch transform or we just put up an
ec2 and we have a you LM fit model in
this case behind it that allows us to
have an embedding service so people can
send text get back a fine-tuned
embedding model that allows them to
consistently and transparently make use
of that data so that allows us to offer
this service to people in a way that is
very much a transparent to them and very
much invisible as to the older
complexity and crying and debugging that
was happening in the background along
the way so I would recommend very much
thinking about these kinds of ways of
how you deploy the intermediate models
as well as how you deploy the final
products as you go along with this
because you can share and gain a lot of
infrastructure value especially natural
language processing from doing this sort
of work in a way that is open and
shareable and well documented so just to
conclude I mean I think the key thing
for us is that we find notebooks to be a
very natural way of doing data science
we're very encouraged to see how things
like the sage maker studio will develop
this further with the ability to model a
multiplicity of different concurrent
experiments in a way that we can
actually track it the that option of
magically changing one thing at a time
which requires a novel of an awful lot
of discipline but is a wise thing to do
is something that we think being able to
do that in this consistent way is
helpful we get a lot of value from the
fact that we can and have jobs running
in the background jobs running overnight
jobs running for days on these ec2 or
sage maker or all the other instances
that we have in a way that makes it very
cost-effective for us because we can
just shut them down when they're done
and because we don't have to give
everyone a three thousand dollar laptop
if we don't want to or well yeah that's
probably not not the only saving but it
is particularly one that helps us and
then finally I think the ecosystem is
immensely powerful we're very grateful
to our friends and colleagues across the
community at all levels of abstraction
here everyone who's sharing basic medium
articles on how to do this stuff all the
way up to the people thinking really
hard about
named tensors or continuous training
models or distillation models are all
that other stuff everything that that
the full spectrum of that is something
that we find immensely powerful and
immensely helpful in in our work as well
thank you very much
so I just have one or two closing
comments first of all I want to thank
Michael sOooo and Alex very much for
being here today and sharing that the
partners at AWS in the partner network
are very important to us it is a high
priority for AWS consistently the last
thing I want to leave you with here is
just a set of resources let me push the
button that will help you with see if I
can get this right here that will help
you find some of these PI torch
resources that are a little bit hard to
find once in a while there we go so
that's it so once again thank you very
much Michael and Alex and we'll be
around for questions afterwards thank
you very much for being here
[Applause]