As per the documentation we need to specify num_features parameter which is the input size of tensor. 
"Batch normalization works when batch size is greater than 1, so an input of shape (1, 32) won't work. "
In most cases you should be safe with the default setting. 
"If you pass torch.Tensor(2,50,70) into nn.Linear(70,20), you get output of shape (2, 50, 20) and when you use BatchNorm1d it calculates running mean for first non-batch dimension, so it would be 50. "
" So yeah, I here insert batch norm after the  linear layer, notice that there's a one D, it may be  confusing.  Why is there a one D? "
 But on by default bias is true if  you don't set anything and I found it was the same  performance. 
"For both functions, the d1 parameter is the number of features, and equals dim C of the input tensor. "
"Try a larger batch size, like 2. "
"It takes input of shape (N, *, I) and returns (N, *, O), where I stands for input dimension and O for output dim and * are any dimensions between. "
