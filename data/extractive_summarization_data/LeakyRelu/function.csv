"As for your second question on learnable parameters, ReLU and Leaky ReLU are simply activation functions that perform a predefined operation. "
"According to the docs [https://github.com/microsoft/onnxruntime/blob/master/docs/OperatorKernels.md (hyper-link)], LeakyRelu is only implemented for type float (32-bit), while you have double (64 bit). "
" Whereas if you have  a activation function that can only produce positive numbers,  you're a little bit more limited.  So if you use the chain rule, and your  derivative is one, if the inputs are positive, then yeah, you  don't diminish the product in the on general, okay, but it can  also be zero, which can be a problem.  So if you have negative  inputs to this activation function, you are your output  would be zero, which will then basically cancel the weight  update for that corresponding weight corresponding to this  activation, or connected to this activation.  So that can be a  problem if you always have very negative input.  So there is a  problem called dying neurons or debt. "
" A version of Relu  that some people find to perform sometimes a little bit better is  the leaky relu, which doesn't have the problem of these dying  neurons.  So here, the difference is that we have, so if we look  at the simplified notation, the the piecewise linear function  here, um, what you can see here, or the piecewise function,  sorry, what you can see here is that the only difference is that  we have now this alpha here, which is a slope, if the input  is smaller than zero, so for the negative region here, we have  now a slope, what value we can choose for the slope, it's a  hyper parameter, right? "
 So hyper parameters is something that you  as the practitioner have has to choose. 
Yes MMdnn support supports LeakyRelu.  Check the link below for pytorch_emitter.py implementation from MMdnn.
