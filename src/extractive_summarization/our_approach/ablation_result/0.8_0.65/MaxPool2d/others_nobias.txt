Pooling is of MUCH MORE IMPORTANCE in convnets. 
Pooling is not exactly "down-sampling", or "losing spatial information".  Consider first that kernel calculations have been made previous to pooling, with full spatial information. Pooling reduces dimension but keeps -hopefully- the information learnt by the kernels previously.
But if you look carefully at what's going on you may notice that the after first convolutional layer the dimension of your data might severely increase if you don't do the tricks like pooling. 
And, by doing so, achieves one of the most interesting things about convnets; robustness to displacement, rotation or distortion of the input.  Invariance, if learnt, is located even if it appears in another location or with distortions. It also implies learning through increasing scale, discovering -again, hopefully- hierarchical patterns on different scales. And of course, and also necessary in convnets, pooling makes computation possible as number of layers grows.
