hi everyone and welcome to the first
installment
um i've i've i've provided you guys
notes of all the stuff i'm going to be
talking about for
for this segment of of the class
and on the very first page the very top
of the page i have this statement
what is the question and that was a
phrase that was actually drilled into my
head when i was in graduate school
i was in a lab group that whenever we'd
be tossing around ideas about
experiments that we were thinking about
doing
or you know things that we were
interested in there was always this
barrage from everyone in there saying
what's the question
jim what is the question and
what i uh what i learned from that
experience
is that if you think really carefully
about what your question is
life is a lot easier for you
scientifically at least
and what i do and i still do this to
this day and i encourage all of you to
do this
and i encourage everybody else that i
interact with to do this
is to as you as you start to
develop ideas and you want to do you
know you want to do some science on it
is to actually literally physically
write down on a piece of paper what the
question is
you learn a lot about your question by
actually writing it down and sitting
back and looking at it you might realize
that
that question is way too big to answer
in one lifetime maybe i should refine my
question to make it something
manageable but the other thing that is
really handy when you start writing down
the question is then you can design an
experiment that is explicitly aimed at
answering that question and i know that
sounds obvious but you would be
surprised
how often people collect a bunch of data
they do an experiment
and then they haven't really thought
through
how they're going to analyze those data
and they realize that ah
that experiment actually isn't
addressing the question like i thought
it was
and so if you actually write it down and
you plan it out
you're much less likely to get into that
horrible horrible situation where you've
done
tons and tons of manual labor and spent
hours upon hours in the field or in the
lab
and then you're like how am i going to
analyze this data
and as somebody who who who does help
out a lot of people
you know with their statistical analyses
you know
oftentimes i i'll talk to them if
they're going to redo the experiment
it's like if you had just done this one
thing slightly differently
this whole problem that we're having
would go away
by doing all of that work beforehand
hopefully you'll never find yourself in
in that situation
and so what we really want to do is we
want to think
carefully about what our question is and
then we can design an experiment around
that question
and then hopefully what we've also done
is we've thought really clearly about
how we're going to analyze those data
the uh the best situation to possibly be
in which i encourage
all of you to try to be in at all times
is that when you collect that last datum
it should not take you long at all to
analyze those data
in principle it should take you maybe i
don't know
the minute it takes to boot up r you've
probably already written the code
because you've thought about it
beforehand
and you're done in a couple of minutes
and then you can just go ahead and you
can
interpret the results of those analyses
and see what you learned
and you know maybe write up the the
study or go on and do
an additional experiment to confirm or
or whatever
based upon the results of that first
experiment experiment
but again the important thing here is
that that is the
designing an experiment and thinking
about how the analyses are going to work
is really the challenging um
creative part in my opinion it's the fun
part of of science
well one of the fun parts the other fun
part is when you actually go out and
collect the data like
i get to hike around in the sierra
nevada um
swinging a butterfly net and sleeping in
a tent for a month and a half a year and
that's great but it's it
you enjoy it that much more when you
know exactly what you're gonna do with
the data that you collect as opposed to
always having this like haunting thought
in the back of your head of
how in the heck am i gonna analyze this
we never want to be there
and so so we're thinking about
that now we are going to be collecting
data i've been throwing around data
all data is is it's just a collection of
things that we can
we can um kind of assume
our facts that they are something that
we've observed or measured
that reflects a fact if i weigh
something and it weighs 10 grams
that is a fact that it is 10 grams
that's how i'm gonna
that's how i'm going to approach that
and when we have a collection of all of
that stuff
that constitutes data and then once we
have those data
now we can start thinking about how
we're going to model them
and the first step with thinking about
how we're going to model our data is to
understand what type of data do we
actually have and there's a few
different types of data that we want to
um consider so one type of data are data
like categorical data and categorical
data as the name implies
are things where things can be dropped
into various categories and there's
actually two different types of
categorical data
there's what's called nominal
categorical data and this could be yes
or no questions this could be
it was living or dead what bin does it
go in in the live bin
or in the dead bin it could be flower
colors you know purple
pink and orange those are bins that
we're putting things into they are
categories
and we are choosing to put the data into
their their appropriate category the
other type
of categorical data is what's called
ordinal data
and these types of data are data that
are inherently ranked so it could be
small medium large
it could be you know egg juvenile adult
i think the example i i used in in the
notes
was different insect stages so it could
be a first in star caterpillar it could
be a second in star
third fourth fifth so these are ordered
just like with small medium large we
know that small is smaller than medium
and medium is smaller than large which
means that small must be smaller than
large
right whereas with just nominal
categorical data
if we were thinking about flower colors
sure everyone has their favorite color
but there's no inherent order in those
data and so if you're dealing with
categorical data
you also want to think about well is it
going to be nominal or is it going to be
ordinal
because there's going to be different
statistical tools that you're going to
want to use
depending on what types of data those
those are
the second general type of data and the
type of data that we're going to
primarily be dealing with in this course
is numeric data and numeric data as the
name implies
is numeric it it's it's numbers and
there's two different
important um types of numeric data
that we're going to want to consider the
first is continuous data so continuous
data
are those data as the name implies that
occur
on a continuous scale and so
there they can take on in principle any
value between maybe some range
or any value between negative infinity
and positive infinity but the important
thing is they can take on
any possible value the
interesting thing about continuous data
that oftentimes people don't think about
is that we can actually never know the
true value so even though we're going to
think about
data as being a collection of facts all
we can do the best we can do
with continuous data is to come up with
an approximate
value for it and i know that might seem
counterintuitive
but think about if you were weighing
something let's say that we are going to
weigh
acorns and i don't know why i'm picking
acorns but
i'm assuming nobody has a problem with
acorns so let's say i was interested in
knowing how much an acorn weighed and i
could stick it on a scale let's say i
stick it on
one of those cheaper scales that you can
buy from fisher scientific for
you know 40 bucks and i could get there
and maybe i can get how many milligrams
that acorn is but that's all i can get
and then i could spend a little bit more
money on a more expensive one
and maybe i can get down to the
hundredth of a milligram
or i could spend even more money on a
more expensive scale and i can get to
ten thousandth of a milligram well in
principle
you could just keep adding significant
figures to that value
so you're going to get a more and more
precise estimate
of what that value is but you can always
add an extra significant figure to it
and clearly there are no scales out
there that have an
infinite number of significant figures
but in principle
why not just because it's the limits of
our technology
and so we can think about the
the accuracy at which we're measuring a
continuous variable is going to be
determined by the precision of the
instrument that we choose to measure it
with
and i often refer to this as grain so
you can have a coarse grained estimate
or maybe you're just using a a kitchen
scale
to measure the weight of of an acorn
or you could go into somebody's lab here
on campus
and you could weigh that acorn on a ten
thousand dollar scale that'll give you
you know ten significant figures as to
how much that acorn weighs
um and so that's just something to keep
in mind
that that the the precision at which
you're measuring something
is going to really determine how close
you are to the true value
the true value but that you can actually
really never know the true
true value the second type of numeric
data
are what are called discrete data and
discrete data
as the name implies are discrete they
are integers
generally we can think about things as
like things that we can count
um and so this could be like the number
of offspring
that that a bird has or maybe it's the
number of lesions
on a leaf if it's been infected by a
pathogen
or the number of leaves on a plant if
you're using that as some
response variable that's telling you
something about the performance
of of that plant the point is is that
they're integers
and as i said in this class we're going
to focus primarily
on numeric data and
when you're collecting numeric data it's
really really important to know right
off the bat okay am i dealing with
continuous or am i dealing with discrete
because we're going to use different
distributions to model discrete
versus continuous data
okay more on that on that later as we
start to
develop and play around with models one
last thing that i wanted to mention
because it's a it's a term i'll be
throwing around a lot over the course of
the semester is this concept
of a random variable and so in a
statistical mathematical
framework a random number
or a random variable means maybe
something a little different
than it means in the common vernacular
and so when in statistics when we talk
about random variables we're explicitly
talking about a situation
where we do not know the value of
something
until we measure it and i know that
sounds pedantic
but as long as we can kind of get our
heads around
okay that's what the definition of a
random variable is
i don't know the value of it until i see
it
concepts such as degrees of freedom will
will become much more intuitive as to
why we're keeping track of degrees of
freedom if we keep reminding ourselves
that we are going to model these data
that we collect
as random variables the other important
thing when we're thinking about random
variables is
generally when we're going to describe a
collection of random variables
is we're going to make some kind of
assumptions about what the underlying
distribution is that we're sampling
those from
and you know you can think of it as is
as
as the shape of the population that
we're taking
random samples from so for example when
we calculate the arithmetic mean of
something
um and we talk about like oh what's the
average height of people
or you know whatever it might be when we
are
when we are deciding to take the mean of
something
we are actually being very we are
implicitly being very explicit about
what we're doing what we're actually
doing is saying
we are going to calculate the mean
assuming that all of our data are random
variables drawn from a normal
distribution
and there's lots of other continuous
distributions out there that
might be more appropriate but if we
decide that we're going to take the
arithmetic mean
we're probably assuming that the
underlying sampling distribution
is a normal distribution which could be
right or could be wrong
that's why it's really important to
think about how you're choosing to model
your data
another thing that i'll say a bunch
probably over the course of the semester
is is the old adage that all models are
wrong
some models are useful and it's
important to always remember that that
we're going to have to kind of idealize
what we think
our where our data are coming from and
then we're going to model them based
upon those
those assumptions and actually with my
example with
with the calculating the arithmetic mean
of a sample saying that you know we're
being explicit that
you know that they're random variables
drawn from a normal distribution
actually more explicitly when we're
calculating the mean what we're doing is
saying i am going to calculate the
maximum likelihood
estimate for the parameter mu assuming
that my data our random variables drawn
from a normal distribution
and i know that sounds like a mouthful
and it sounds incredibly pedantic
and it is incredibly pedantic you
probably won't win a lot of friends
um by like pointing out to them like ah
it's not really the average
you know what it is is the maximum
likelihood estimate of your data
assuming that the random variable's
drawn from a normal distribution
so i'm not encouraging you to correct
people um but i am encouraging you to
always know that in the back of your
mind
that's what we're doing when we are
modeling data is that we are going to
make
certain assumptions about the underlying
sampling distribution and once we have
that underlying sampling distribution
then we can do statistics once we have
the statistics we can do statistical
hypothesis testing
if we want and you know ask various
questions
of of our data and so i wanted to point
out what random variables were because
as i said
i will i will use that term quite a bit
over the course of of this course
and so that's kind of our general
introduction and
what i want to do now is um we can go on
and and we can talk a little bit
about some discrete random variables
okay let's think about the simplest
discrete distribution that we can and
it's a distribution that's called the
bernoulli distribution and a bernoulli
distribution
is again it's going to be a discrete
and it is going to be a number
and in this case the number is going to
either be 0 or 1.
okay let's think about the simplest
discrete distribution that we can and
it's a distribution that's called the
bernoulli distribution and a bernoulli
distribution
is again it's going to be a discrete
and it is going to be a number
and in this case the number is going to
either be zero or one
okay and so the
the bernoulli distribution it's the
simplest one is that there's
only two possible outcomes and so
if you were thinking about flipping a
coin for example there's two possible
outcomes you're either gonna get heads
or you're gonna get tails
and then there's gonna be a probability
that you're either gonna get heads
or that you're going to get tails okay
and so generally that's written as
say p equals 0.5
and so that let's say is the probability
of getting heads
so every time i flip a coin
every time i flip a coin there's a 50
chance that i'm gonna get heads if i
flip a coin just once
that is a bernoulli random variable
i have made i i don't know the answer to
whether it's heads or tails until after
i have flipped the coin
and presumably i have some estimate as
to the probability of getting heads or
getting tails so in this case we have a
lot of prior knowledge about how coin
flipping works
and we feel pretty good that the
probability of getting heads
is is 0.5 and so that means that
the probability of getting
the the other outcome is going to be
one minus p which is going to equal
0.5 okay
a lot of times in books what you'll see
is that this one minus p
will be written as q
okay so again we can think about this as
being
i don't remember what i said the
probability of getting tails
and that means that q is the probability
of getting heads
p plus q will always equal
one the um probabilities
whenever we're dealing with with
anything with any kind of a distribution
the probability will always sum up to
one meaning that we are
com we are accounting for all possible
outcomes
there's a hundred percent chance that
you're either gonna get heads or tails
that's another way that you can
that you can think about it
and so let's let's play around a little
bit and think about how to deal with
these probabilities by by taking a
kind of a more interesting i would argue
a more interesting
example um than than a coin toss
and we're gonna we're gonna think about
this butterfly that
is really close to my heart um it's it's
one of my study systems it's a butterfly
called lysites melissa
it is or the melissa blue and i even i
have a picture of one just because
they're so
darn handsome everybody wants to see
something handsome
look at that guy that's a handsome bug
right there isn't it
that's my that's my bug so this is the
melissa blue
top side of the wings are this beautiful
blue color
and they uh
they have colonized patches of
alfalfa throughout the the great basin
out west and based upon
lots and lots of of work we know that
the probability that if you go to any
feral
alfalfa patch in the middle of the
stinking desert
that there's about a 32 percent chance
let's say that there is a 32 percent
chance
that the melissa blue butterfly will be
present
in any random patch and so going back to
this
we can go ahead and we can start with p
is 0.32 and so
oh that felt good so if if we know that
p
is 0.32 the probability that it's
present
that means that the probability that
it's absent has to be
one minus p
which again some people refer to as q
okay and that's going to be 0.68
and again 0.68 plus 0.32
equals 1. all possible outcomes are now
have been accounted for and so we can
start
with kind of playing around with with
how this
might behave by saying
let's let's assign p
to be 0.32 okay so now we have this
object called p
and it's worth 0.32 which means that we
know what q
is q is going to be 1 minus p
and so now let's take a look at q 0.68
turns out i did my arithmetic it worked
okay so let's think a little bit about
about how we play with probability so
if i wanted to know the probability that
that butterfly is present in any two
random patches
that that i would go to in the field so
i randomly go to two different alfalfa
patches and i want to know what is the
probability
that it's present in both of those
fields
assuming that the probability that it's
in any given field
is 0.32 and so to do that we always
multiply probabilities together
so it would be the probability of it
being in the first and then the
probability that it's in the second
okay so i would expect roughly 10
of the time if i went out to two random
fields that it would be present
in both of those i could also ask what's
the probability that i'd go out to two
fields and it would be
totally absent well that's going to be q
times q and so roughly 46
of the time i would expect it to be
absent
and then we can also think about what's
the probability if i went out to two
fields
and it was present in the first one and
absent in the second one
well if it's present in the first one
that's p
absent in the second one that's q
i expect that to happen roughly 22
percent of the time
and then likewise i can say well what's
the probability if
i went out there and it was absent in
the first one
and then it was present in the second
one
okay that makes sense again 21 22
22 chance and so what we have now is we
have a model
that is describing the probabilities for
any two fields
that we would go to any two patches of
alfalfa that we would go to
the probability here that it's present
in both of them
it's absent in both of them it's present
in the first one
absent in the second absent in the first
present
in in the um in the second yeah
absent the first present and the second
and if we were to add all these numbers
together
that plus
that plus
that plus that
it's going to equal 1. probabilities
will always sum
to 1. so what's kind of interesting
and and maybe some of you have kind of
caught this
is that what what have we really
what have we done here when we've done
this little arithmetic here we've done
the present the probability that it's
present in both of them
is p times p well that sounds to me like
that's p squared right
the uh
the probability that it's absent in both
well that's q squared
and then i have the probability that
it's present and then the prob times the
probability that it's absent
and then the probability that it's
absent
times the probability that it's present
and so i don't know
how familiar this looks to you yet but
and we also know the sum of all those is
going to equal 1.
let me make myself bigger because i'm
i'm rambling now
okay well we could rewrite this
as that and that should look familiar to
a lot of you because in general genetics
class
that's hardy-weinberg equilibrium and a
lot of times people get all bent about
hardy-weinberg because
books always are like here are the
assumptions of hardy-weinberg and
they're all totally unrealistic
assumptions
total random mating infinite population
size well that's impossible that's
ridiculous
no mutation there's all these different
assumptions that people kind of get
caught up in when they're dealing with
hardy weinberg but hardy-weinberg is
just simply
a model and it's a model based upon
these simple
probabilities so you know in hardy
weinberg we're thinking about you know
big a versus little a or being a
heterozygote
that's the same thing as us asking
what's the probability that the
butterfly is present in both patches
versus absent in both patches
present the first absent the second
absent in the first present and the
second
that's all hardy-weinberg is is it's
just a simple statistical
statistical model and it tells us what
our expectation is
and all those assumptions are in there
simply because
it's a statistical model and we're
assuming that they are random variables
drawn from
some distribution but maybe we digress a
little bit i don't know
let's go back and let's think about what
else we can do here so
these are pretty straightforward right
on how we calculated that
if i wanted to know what is the
probability that if i were to go out
to 70 patches let's say and i want to
know what is the probability that it's
going to be present
in 70 patches that that i would go out
in the field to
well it's going to be the probability
that it's present times the probability
that it's present
times the probability that it's present
and we're going to do that
70 times the easier way to do that is to
simply say p
raised to the power of 70.
okay that is the probability that it's
going to be
present in 70 selected
patches of alfalfa that we might find in
the great basin
very very low probability but it has a
probability
okay likewise we could say well what's
the probability that it's absent in
all 70 of those patches and sometimes i
have field seasons that feel that way
but let's do that okay
well just eyeballing that we can see
both of them are
pretty improbable but actually
it's way more probable that they're
going to be absent in all 70 patches
than if they're going to be present in
70 patches okay and so even though the
probabilities are small
there's still a really big difference it
is way more probable
that you're going to have no butterflies
present
than butterflies present everywhere okay
and so let's make this a little bit more
complicated if you will and let's think
about
what if we knew that there were 172
patches
of alfalfa out west in the desert
and of those 172
patches we want to know the probability
that our butterfly or our our handsome
little melissa blue butterfly
we want to know the probability that
it's in 70 out of those 172 patches
we don't care which 70 they are we just
want to know
what is the probability that they're in
70 out of
172 patches
and so the first thing that we need to
think about
is how many different ways can we get
70 patches out of 172
out of 172 patches
and to do that we take the product
of the total number of patches
and so what we would do is we would
let me go up here so we would say i want
172 times 171
times 170 and so on and so forth
all the way down to 103
all the way down to 103 patches
and so what we can do is we can use that
little shortcut
if i were to just type 172 colon 103
it's going to count as integers from 172
down to 103.
okay
and so what we want to think about
is what are the number of unique ways
that we can get
70 patches and what i mean by unique
ways
is we don't really care if we first
observe it in that patch and then we
observe it in that patch
you know you let's say you have two
patches patch a and patch b
when we simply just take the product
of all of those right that's a huge
number 2.2 times 10 to the 149th
that's all the different ways that we
can get 70 out of 772 patches but what
it's not
considering is that there are multiple
different ways that we could
get those patches so again if we think
about patch a versus patch b
we could first observe it in patch a and
then observe it in patch b but we could
also just because of
how we're driving around the desert we
could first observe it in patch b
and then observe it in patch a
and so what we really want to know is
how many unique ways
can we get 70 patches occupied
and what we'll do for that is we will
divide that
by the factorial
of 70. so 70 times 69 times 68 times 67
66 65 so on and so forth all the way
down
two to one and so
when we do that now we have
still a really really big number there's
a lot of ways
70 out of 172 patches that can be
occupied
but it's not nearly as many ways as
when we didn't account for the fact that
there's a lot of different ways
that we could get that
basically the same value i mean if we go
back to thinking about our coin tosses
here
there's two different ways to get heads
or tails right first you can flip ahead
then you can flip a tail
or first you can flip a tail then you
can flip ahead
but at the end of the day it's still one
head and one tail we don't care about
the order
and that's what we're doing here is
we're removing
not the problem but the fact that just
taking the product
the product here that's only going to
give us all that's going to give us
all the different ways we could get 70
out of
out of 172 whereas here
we're only interested in the unique ways
that we can get 70 out of 172.
so this
this is what is referred to as
the binomial coefficient
and it would be often written as
a big parenthesis out of 172
and then we would say 70 here
and we would read that as 172
choose and this is again what is known
as the binomial
the binomial coefficient so
r is a statistical programming language
and it shouldn't be entirely surprising
to anyone that there might
actually be a built-in function
that'll do the binomial coefficient for
us
and that function is called choose
and so if i were to just type in choose
172 70.
those are the two arguments that are
going to go into choose again we can
always hit question mark
choose and it'll tell us what our
functions are doing
and somewhere in there is choose choose
n k
[Music]
n is the total number of things that
we're
there that we're potentially sampling
from and then k
is the number of samples from that so we
have 172 fields
out out in out in the great basin
and we want to just think about how many
different ways can i
get sample 70 of those
172 patches and so if we go ahead and we
run that
we get the exact same answer that we got
up here okay
and so that's great so what we have done
now is we
figured out all the different ways that
we can have
that we can find 70 from 172
but we haven't really figured out the
probability
that melissa is in exactly
70 of those patches
and so to do that we're going to want to
think
rather carefully about
how we're going to deal with the
probabilities so first what we need to
do is we need to calculate
what is the probability that it is found
in
exactly 70 patches and we've already
done that up here
right and so that is going to be the
probability that it's in 70 patches
now we're going to have to multiply that
by the probability
that it is absent in the remaining
102 patches
and so we will go
multiply that by q
to the 102
all right and so this is the probability
that it's absent
in 102 patches this is the probability
that it's present
in 70 patches and now
we're going to multiply that by our
binomial coefficient
and we're going to multiply it by that
because all we've calculated here
is the probability that it's in exactly
these 70 patches and absent in exactly
these 102 patches
but what we what we have is we have 172
patches
total and we don't care which 70 of them
are occupied
we just want to know the probability
that any 70 of them are occupied
and any 102 of them are are unoccupied
so we're going to multiply that number
by all the different
unique ways that we could we could we
could get that
and that's where we're going to use this
binomial
coefficient and now here is
the probability and so interestingly
when we just ran this that would have
been for
very specific patches maybe you know
your 70 favorite patches are occupied
and the 102 remaining patches are
unoccupied
but since there's a bunch of different
ways that we could get 72 occupied and
102
um unoccupied we multiply that
by the binomial coefficient so any 70
specific patches
incredibly low probability that you know
if i had a list of the 172 patches and i
arbitrarily put a check mark
next to 70 of them
that's the probability that i would be
right really really low that that jim
would be right on that one
however when we multiply it by all the
different ways that we could actually
get 70 patches
okay well now we have
maybe uh you know a probability that
clearly is a lot bigger
it's like what here so it's a 0.35
chance that exactly 70 patches will be
occupied okay
um hopefully that's
kind of clear as to how we're dealing
with probabilities
and um how we're actually able to
to to to calculate the probability of
any kind of
series of events from happening and so
likewise we could say well what's the
probability that you know
maybe it's 60 patches instead
right and then we could just say well if
it's present in 60 that means that it
must be absent
and 112 and our new binomial coefficient
will be that
and so oh now we have a 4.6 chance that
60 patches will be occupied and so on
and so forth and so what we've been
doing here is that we've been ran
we've been modeling each patch
it's either occupied or not occupied
so our data if it's occupied or not
occupied we would write down
that why our data are distributed
as a bernoulli and in our case here
it would be zero point was it three two
this is the formal way that we would
describe what we've just been doing now
y is a random variable drawn from a
bernoulli distribution
with a probability of 0.32
and so this whole idea of of thinking
about
explicitly that we're measuring the
probability of things that's going to be
the cornerstone of
really most of the stuff that we do
that's what likelihood is likelihood is
really the probability that we're
observing something and
we're asking you know what is the
probability that 70 patches out of 172
would be occupied or what is the
probability that 60
versus you know versus something else
is is going to be occupied when you have
a series of
bernoulli draws when you have a
collection of
bernoullis that is what is referred to
as
the binomial distribution
okay so you can think about the
bernoulli as being
a special case of the binomial
and the binomial is simply a series or a
collection of
bernoulli draws with a set number of
trials it's often referred to as a trial
so you could think about every time we
would go out
and we would visit one of those patches
of alfalfa
we are doing a bernoulli draw if we go
to
all 172 of them well that's 172 trials
and what we could call the presence of
the butterfly as being a success
we found our bug that we were looking
for and
if the bug isn't there we could think
about that as
a failure and that's what the binomial
distribution
is is doing and so let's go ahead and
just take kind of take a break and
digest this and we'll come back
and we will think about the the the
binomial distribution
hopefully clearly
okay well let's let's think more now
that we've we've learned that
a bunch of bernoullis is as a binomial
and we
that means that we can really calculate
um
thinking about again sticking with the
butterfly example
we can calculate the probability that
it's in zero patches out of 172
probability that it's in one patch out
of 172
so on and so forth and since we got a
computer
why don't we have the computer do the
work for us so we already have p but i'm
going to go ahead and i'm going to
retype in p
okay so that's the probability that it's
present i said that there were
172 patches
and then let's think about
using i'll use the letter k to be the
number of successes
or the number of patches that were
occupied and i'm going to let that go
all the way from 0
to 172. okay so again that little colon
there
that's just creating a vector of numbers
starting at 0
ending at 172.
just as an aside one thing that you will
always want to kind of keep track of
when you're dealing with
discrete values is that notice
if the there's 172 patches
that means that there's actually 173
different possibilities because it being
absent from
everything is one of the possibilities
right so
the first value is zero the second value
is one
so on and so forth which means that the
173rd value
is 172.
and this sometimes can kind of trip
people up at first
because you know you're not you you
don't always think about the fact that
you need to have this kind of
this space being held
for the possibility of there being zero
so out of 172
patches there's actually 173 different
things that we might observe
it could be absent in all patches
present in only one patch
present and only two patch so on and
and so forth and so we could calculate
them the probabilities
i'm going to create an object called the
probs
and i'm going to go ahead and i'm going
to just plug it into this formula that
we have been playing with
before so if we're trying to calculate
the probability that the butterflies are
present
it is going to be p to the k
right so the probability we're gonna be
able to calculate the probability that
it's in zero patches the probability
that's in one patch so on and so forth
and we're going to multiply that by it
being absent
then in the remaining patches so
we're going to start with one minus p
right which again is sometimes referred
to as q but let's just keep it simple
and keep it at one minus p here i'll
even put some spaces in here so you can
follow along
one minus b this is the probability that
it's present
in any randomly selected patch which
means that this
is the probability that it's absent in
any randomly chosen patch again
0.68 plus 0.32 is gonna sum to 1.
probabilities are always going to sum to
1. and then i will raise that to the
power
of what it'll be the the power of the
total number of patches
minus the number of patches
that are unoccupied
and then i need to multiply it by that
binomial coefficient
so that i can think about all the
different ways that i might observe
something and so i will say
choose choose what
i will choose i will choose to move this
over i will choose
n k
okay so again there's 172 patches
and then k is this vector so if i were
to just run that
we can go ahead and we can look at some
of these earliest values up here
well if there's zero patches that are
occupied how many different ways can i
get
zero occupied patches out of 172
well there's only one way i can do that
because there's only one way that
everything can be
um butterfly less if you will
and so that's why you know that's what
that binomial coefficient is
if there's only going to be one patch
out of 172 occupied
well there's only 172 different ways
that that could possibly happen
but then as you start to increase well
how many different ways could i have
two out of 172 patches occupied
now we start getting into a much larger
larger number and so on and so forth so
by running this line of code because as
we mentioned in the r tutorial r
is conveniently vectorized
which means that we can stick a vector
into a formula like this and it'll do
the mathematical computations
across all of that so i'm going to go
ahead and i'm going to
do that so now i have a vector of
probabilities
all right and again the first value
that's the probability that
all the patches are unoccupied that's
the probability that
one patch is occupied this is the
probability that
any given two patches are occupied so on
and and so forth
if we were to take the sum
of the problems
it's going to sum to 1. probabilities
always have to sum to 1.
okay and so with a binomial draw
we have a set number of trials 172
and we can think about all the different
ways that we could see butterflies out
of those 172 patches
the probability of all of those
different kinds of ways
is always going to sum to 1.
so i'm going to go ahead and i'm going
to plot this so we can take a look at
this
and for our purposes here and i would do
the same thing if we were in class
i'm not going to plot it down here which
is the default because it always looks
crummy and the
margins are way too big so i'm going to
call up
a new plotting device for myself on the
mac
it's quartz
i believe on a pc it's just called
window and if you do the parentheses
what's going to happen
is i now i'm i have this floating around
window that's no longer
in our studio and it's not really that
big of a deal
when you're playing at home but over the
course of the semester i'll always be
calling up new quartz windows and that's
just because the windows are bigger
and it's easier for um for lecturing
purposes
so i'm going to move that out of the way
you'll notice that i often look up here
that's because i have a separate
computer screen so i'm going to move
this out of the way
so i can actually see what our code is
and i'm going to type
a function called plot and
plot's going to want the x-axis and it's
going to want the y-axis
so the x-axis is going to be 0 to 172
because that's what we were interested
in either zero patches being occupied
one patch being occupied two patches so
on and so forth all the way up to
all 172 patches being occupied
so i'm going to do that i'm going to say
plot that and the y-axis
i want it to be the props
so i go ahead and i do that
and this is what it looks like okay so
you can see that
and these are patches occupied down here
and here are the probabilities here we
can see that it's
really not very probable that you're
going to have only a small handful of
patches occupied
and it's really not going to be all that
probable that you have tons and tons
upon tons
of of patches occupied
so i'm going to play around a little bit
more with this plot
i'm going to zoom in to just this region
and here again is where there's just a
learning curve
as you're playing with r and so on and
all this code is available to you so you
can kind of start to
explore and play around but i'm going to
make this exact same plot but
i'm going to say that the x limits which
is an argument called x lim
and i'm going to give it the lower value
and the upper value so again i got to
tell r that i'm giving it
a vector because it's going to be a
vector of 2 the lower value and the
upper value
and i'm going to have it only plot out
the region between
35 occupied patches and 75
occupy patches
so now i'm zooming in a little bit more
right and you could look at this and say
oh well that looks kind of normal and
it's like yeah but it's not
that is describing a a binomial
distribution
and here are the probabilities and we
can look at this and eyeball it
and see like oh right around here that
is the most probable number of occupied
patches
which appears to be somewhere i don't
know 56 57 ish we'll get to in a minute
how to figure out exactly what that
value is
but we can see that there there is a
range of most
probable number of occupied patches
and once we get out to these extremes
either way
it becomes very very improbable that we
would observe that but they all have
a probability there is a probability
that we would have no patches occupied
and there is a probability that we would
have 172 patches occupied
but the most likely number of patches
occupied
are clearly somewhere here between the
values of
say 35 and i don't know 35 and 65
ish 45 and 65 somewhere around there
those are just the more more likely ones
and then what i've also included on
on the code that i've given you is just
a few other
arguments that we can put into that plot
function to make it look
prettier and so
something like this and so again what am
i doing
i'm plotting 0 to 172 on the x-axis the
probabilities are on the y-axis
i'm giving it its limits um i'm putting
in this extra argument here called type
and i'm saying type h which
means you want it to look histogram-ish
it's not truly a histogram but
it's histogram-ish it'll look like a
histogram
i'm going to use this argument lwd which
is line
width which is going to make the lines
fatter so that we can see them more
pretty when we're looking at
when we're looking at our plot and then
i'm going to give it a title and the
title i'm going to give it
is the probability mass function because
that's what we're describing right now
and then this final argument lend is for
line
endings and it's going to tell it that i
want it to have square line endings
and so if i went ahead and i ran that
code
now i got to find my quartz window here
it is
it'll look something like like this so
again
it's histogram me because it's bars
these are nice fat lines because i said
i wanted the line width to be 10
and they have square ends because of
that lend argument
at no point in this course will you ever
be graded on how pretty your graphs are
but to make them somewhat aesthetically
pleasing
i decided to put those in there so i
thought i'd explain
what those are and so
this right here
is what the the math the arithmetic that
we did when we did this
is what's known as the probability mass
function
for a binomial distribution and if we
wanted to see
formally how it would be described
mathematically
it would look something like this
in fact if you were to do a google
search
for binomial distribution and look at
the wikipedia site
you would see that on the right hand
side
of where it talks about distributions
pmf probability mass function there's
our
binomial coefficient which figures out
all the different ways that we could
observe
um k number of successes out of n number
of trials
here p to the k that's the probability
of of success and then here is the
probability of failure
okay and so again you know sometimes
people use
q sometimes people use one minus p it's
all the same thing it's the probability
mass function
for a binomial distribution
and so that would mean that we for for
this example here
if if we were to go out and look at
those
different fields and or patches and ask
if if they were occupied or not what we
would do
is we would we would formally
write out how we are modeling
the data um that
that we are collecting and so if i can
just make sure that i'm on the right
page
okay um what we would say
is that we would say that our data
are random variables drawn from a
binomial distribution
with a set number of trials um
and we're interested in and the
probability of
success actually occurring
and so we could write this in a formal
kind of framework
we could say that our data
are random variables drawn from a
binomial
and then in parentheses there's going to
be two
important parameters in this model that
are going to describe the shape that
model
it's going to be n the the number of
trials
and so in this case we have 172
trials and our p parameter
is 0.3 and so that would be the formal
way that we would write out
our model and and what we would use to
describe the probability of any
particular events occurring
and so let's go back and look at that
object that we created called
the probs
and let's say that we were interested in
knowing
what is the probability that we would
find a butterfly
in 60 or fewer patches okay and so we
have this long vector called the probs
the dot probes okay here is our long
vector over here and again
this is the probability that it's in
zero patches this is the probability
that it's in
one patch so on and so forth so if we
wanted to know
what is the probability that it's in 60
or fewer patches
we would want to first pull out just
those probabilities
and so it's going to be the very first
value
all the way up to the 61st value because
again
the first value is 0 which means that
the
second value is 1 which means ultimately
the 61st value is the probability that
it's in
60 patches and so now we have
go ahead and we can run that and here
are all these probabilities
right the probability that it's in 0
probability that it's in 1
probability that it's in 2 so on and so
forth
and so if we wanted to know what's the
probability that it's in 60 or fewer of
those patches
we would just simply take the sum of
that
and it's 0.81 so
that is our probability that it's going
to be in 60 or fewer
patches okay
and so again r is a statistical
programming language
and so it shouldn't come as a great
shock to anyone that there might be a
built-in function
that'll actually do this for us we don't
have to type
out this probability math function now
that we understand what it was
and from the ground up we basically
invented or reinvented this distribution
right this this this binomial
distribution
we don't need to type in all of this
each time we can use a built-in function
and that built-in function
in r is what's called d binom
and d binom
has a couple of arguments that we need
to consider x
size and probability those are the
important ones for right now
so x we can think about as the number of
successes
so maybe the number of
um patches that are occupied by this
by this butterfly and so we could say x
equals let's say zero just for starters
size is what size is the
total number of trials in this case the
total number of patches
172
and then we have our probability
probability of success
is 0.32
and so that's the probability that it's
going to be in zero patches
that is the same answer that we got when
we did it longhand
okay so the built-in function is going
to do our work
for us a little bit of technical
difficulties i'm back
um so we can use this built in function
d binom
right and again this is giving us the
same answer that we got
when we did it by hand and so again this
is the probability that it's in zero
patches
which is the exact same answer that we
got
when we when we did it by hand if you
will
and so again going back to the question
of what is the probability that it's
going to be in 60 or fewer patches
well we could take the sum of those
first 61 values because the first value
is 0 which means the 61th value
will be will be
60 occupied patches
we could do the same thing using the d
binom
but here we're saying that the number of
successes is
from 0 to 60 and again that that colon
there will simply just count
from 0 to 60 as integers
and so if we were to take the sum of all
those probabilities
we get 0.814
which is the same thing we got when we
did it when we actually programmed in
our own probability mass function
so another thing that we might be
interested in knowing is
what the um what the most likely
number of patches to occupy is so
remember when we looked at our plot
gotta find my plot now right we can
eyeball it here well
somewhere over here is the most likely
number of patches occupied
and so we could kind of squint and look
at that
that can be kind of challenging if it's
a very complicated graph if there's a
lot of things going on and
what we can do is we can use just a
little bit of
r coding
we have this object called the props
let's just go back to the props and if i
wanted to know what is the most
likely so it would be the one which
value
there is the most probable i could
take that vector for the probs
and i could ask what is the maximum
value
so one of those the most likely outcome
has a probability of 0.065
if i wanted to know exactly what value
that was
this is where we can use a little bit of
some basic r functions i could say which
which what which the probs
is super duper equal to the maximum
of the probs and recall that the
function which
will tell you what what is the value for
the index
to to to find a certain condition so
here it's going to tell us
what value of those 173 values
is the maximum value and if i do that
i get the 56th value the 56th value
will correspond to 55 patches
because again the first value is zero
patches the second value is one patch
so on and so forth which means the 56th
value
will be 55 patches that are that are
occupied
so if we were to look back here this
highest peak right there
that is 55 okay so that's a way that we
can
figure out the most likely number of
patches to
that that are that are occupied okay
good so let's go back to that question
that we that i posed before
which was what's the probability that
it's in 60 or fewer patches
so what we've done here is we've taken
the sum
obviously of it being in zero patches of
being in one patch being in two patch
so on and and so forth and so what we
can do
next um or or another way that we can
think about that is
all we're doing is we're summing up 0
through 60 which means that if we wanted
to know 55 patches it would be summing
up 0
through 5. and so we can do this in a
brute force way
i'm going to create an object that i'm
going to call cumulative
dot probs
and i'm going to use built-in function
in r
that calculates the cumulative sum
and so in other words it'll start with
the first value and then it will add the
first
and the second value and then it will
add the first second the third value
so on and so forth so i'm going to go
ahead and i'm going to do that
and so now i have
cumulative probabilities so this is the
probability that it's in zero
this then would be the probability that
it's in either zero or one
the next value would be the probability
that it's in zero one
or two and so we can look at what that
looks like
here i'm just gonna cut and paste from
the code that i've given you
we're just gonna c bind them together so
we can look at them like they're a table
and so here's the problem here here are
zero patches occupied
this is the probability that you would
have zero patches occupied and here's
the cumulative probability which is
going to be the same because right now
we only have
zero patches occupied but now when we
get up to one patch occupied
here is the probability that exactly one
patch will be occupied
the cumulative probability is the
probability that either
zero or one patch will be occupied
and so on and and and so forth so you
know for example if we go
to where we talked about our 60. this is
the same value that we got
when we took the sum of the first 61
values from the probabilities
the probability that it's in exactly 60
patches
is that but this is the probability
that it's in 60 or fewer
patches okay and so that's what's known
as
the cumulative probability distribution
or the pdf
and on the notes that i've given you
there's actual we could we could
formally
kind of show what that looks like in the
maths
here you can see this is just simply the
probability mass function the only
difference is
is that we're taking the sum from zero
up to a particular number of successes
and so this is the cumulative
distribution function for
a binomial and there
is a built-in function that will do that
for us as well so that way we don't have
to deal with
saying sum for example
and that function is called
p-binom
and what p
binom wants let's go ahead and just say
question mark p
question mark p binom so we can think
carefully about what we're doing here
okay p binom is going to want what it's
going to want some kind of a vector of
quantiles
and then of course it's going to want
the size how many trials were there
and then the probability and so
in in this case when we're interested in
the probability of there being 60 or
fewer
we could use p binom we could say 60
which is going to be the quantile of
interest the size
is 172 and our probability is 0.32
again i don't have to type out all the
arguments
because i'm putting them in there the
exact same order
that the function is expecting them to
be
but maybe just to uh for one last time
to just be careful let's go ahead and
put those arguments in there so we can
follow along
with no problem oops
oh cue pay attention jim
okay and so here is the cumulative
probability of there being 60 or fewer
patches occupied which is the exact same
answer that we got
when we used the probability mass
function
and took the sum when asked questions
like what is the probability of it being
in 60 or fewer patches or more patches
or whatever it
it might be it's always a good habit to
get into to just start using that
cumulative distribution function
because we're not going to be able to
play this little game where we take the
sum
of all the different probabilities when
we start dealing with continuous
distributions
because any given value is
infinitesimally small
so let's take a quick look at what the
cumulative distribution looks like i'm
going to just go ahead
and plot it and i'm going to look for my
window
here okay so here's what the cumulative
distribution looks
like and here you can see that you're
adding together more and more and more
probabilities
and i've put this this red little dotted
line here
just arbitrarily i put it there and
that's where you're 95
confident in something so where
something is
95 probable and so it's really
quite close to what our
our 60 was right and so it's probably
somewhere
if i were to go to 64 65 patches there
i'm 95 confident that there's going to
be 65 or fewer patches
occupied based upon the fact that we
have 172 patches
and the probability that any given patch
is occupied
is 0.32 and that's how we can use that
cumulative
probability distribution okay so we can
do that with any value what's the
probability that you know
20 or fewer patches is occupied
that's the probability that 20 or fewer
patches is occupied
not very probable which isn't terribly
surprising
because if we look at we don't even have
that
we know that when we get to a very few
number of patches occupied
we really start to have an incredible
low probability so there's a real low
probability that we're going to have 20
or fewer patches occupied
the cumulative distribution and the
probability mass function
are essentially telling us the same
information it's just done in a slightly
different way because the cumulative is
kind of
it's doing all that addition for us from
left to right
so one thing that can be tricky and
and it's important to keep this in mind
is i've been very careful or at least
i've tried to be very careful
when i've asked that question what is
the probability that it's in 60
or fewer okay which means that if this
is the probability
that it's in 60 or fewer
that means that the probability that
it's in more than 60
is gonna be one minus that value right
so
[Music]
the probability that it's in more than
60 is 0.18 the probability that it's in
60 or fewer is 0.81
with discrete distributions you have to
be very aware
and cautious about that whole thing of
being equal to or less than or greater
than
or greater than or equal to because it's
going to determine
exactly where the cutoff is going to
going to be so for example if i wanted
to know what is the probability that
it's in
60 or more patches
what i would need to do is i would need
to take all that probability mass that
it's in 59 or fewer
and subtract that from from
one so this right here
that is the probability that it's in
more than 60 patches
if i wanted to know what is the
probability that it's in 60
or more patches
i would have to change this number here
to 59
because i basically want to exclude all
the probability mass
that has anything to do with a number
that's less than 60.
with these functions in r in all
computer languages that i know
it's always going to read from the from
the left to the right so you have to be
aware of or at least cautious when when
looking at a question or trying to solve
a problem you have to think really
carefully about whether it's equal to or
greater than
um or just simply greater than
okay an alternative way that we could do
something very similar to this is there
is an additional argument here
you'll notice which is lower tail so by
default
lower tail equals true means that it's
going to look
at all of that mass on the left side of
the distribution working from left to
right
if i said lower tail equals false
dot tail equals false
it's going to do that arithmetic for me
okay so just be aware of that you can
say lower tail equals false
but again you have to be really really
careful
with with how you're um
defining where the cutoff is so
this code right here
that's telling me the probability that
it's in 60 or more
whereas this is the probability
that it's in more than 60.
okay again with that lower tail equals
false
if i didn't have that argument in there
and the default is lower tail equals
true
that is the probability that it is
in that it's 60 or fewer
okay whereas this is the probability
that it's in more than 60.
and so just be aware of that it it can
throw people off and it takes maybe just
a little bit of practice to kind of
to kind of think about how to deal with
that
okay well good done where are we
let's talk about stuff
yeah and so let's just really quickly
just take a look at the binomial we've
we've looked at
one binomial distribution based upon our
parameter setting of 0.32
and i'm going to just create a another
plot here so we can just look at
how the shape of the binomial
distribution
changes as a function
of the p parameter or the that
that uh probability parameter and so
here's the probability mass function
where p
is equal to 0.32 and this is the
distribution that we have been
playing with okay
as we decrease that p parameter things
are becoming
less and less probable and so it's going
to shift the mass
of of that distribution closer and
closer to zero
here i'm making it at 0.05
and then here finally i'm making it at
0.005 okay so
much much lower actually there's a typo
there's a typo way to go jim let's do
that again
okay so here is that 0.1
decreasing the probability now 0.05 so
now it's becoming really unlikely that
that you're going to see anything and
most of that mass then
is is over here near zero and now zero
is actually a pretty probable
outcome and then when we get it really
really
small probability now most of our
probability mass
is is that either zero or one patches
occupied and it's incredibly unlikely
to to have any of these other patches
occupied
so the the shape of the binomial
distribution
is going to depend on what those
parameter values are and eventually
when that when that p parameter gets
really really low
it's going to get slammed up next to
next to zero
and so based upon that if we have an
estimate
for our p parameter we
can model these types of data
as random variables drawn from a
binomial distribution
and we can come up with some expectation
as to
what we expect the the certain number of
or however many number of patches to
to be occupied so let's
play with that and let's think about
that and then um
we'll we'll revisit a few other things
so we can think clearly about how to
deal with the um
the binomial distribution
hopefully you've played around a little
bit now with the d-binom function and
the p-binum function
and understand how to use them
to come up with some kind of a
prediction or a probability that we're
going to observe
something happening up to this point
the parameter the p parameter or pi
parameter
but the probability parameter
i've given to you right so based upon
many previous years of work i have
this estimate for that p parameter which
is
that the probability of a patch being
occupied
is 0.32 and that's what we've been
that's what we've been using to think
about the probability of 60 patches
being occupied or
50 or more or whatever whatever we might
be interested
in learning but what we haven't talked
about is well how do you estimate
that parameter and
how we estimate the the parameter
for the binomial distribution is we take
the total number once
we're collecting data and we would look
at the number of successes
so the number of occupied patches in in
our butterfly example
divided by the total number of trials or
the total number of patches in our case
and so if we continue to think about the
binomial
as being the number of successes given a
certain number of trials
the the estimate the maximum
likelihood estimate for that p parameter
for a binomial
is simply the number of successes
divided by
the total number of trials okay
and so it conveniently it's it's
essentially it's the arithmetic mean
right it's how many yeses versus how
many total
how many total attempts
right and so let's think about that so
first of all
this is me telling you that that that is
the maximum likelihood
estimate but i hope that you're not
satisfied with
the fact that it is the maximum
likelihood estimate simply because
i said it was the maximum likelihood
estimate let's actually understand what
we mean
and how we can come up with these
maximum likelihood estimates and why
certain things are maximum likelihood
estimates
and to do that we have to understand
likelihood
and so what i'm going to do is i'm going
to set up i'm going to
stop talking about butterflies for a few
minutes
and i want to go back to just thinking
about coin tosses because it's really
easy to think about a coin toss because
everybody at some point in their life
has probably flipped
a coin and let's say
for example you were to flip a coin a
hundred times
and you observe 100 or you observe
60 heads and
40 tails okay
and so we're we're gonna let's model
this and assume that
heads are a success and tails are a
failure but if you flipped a coin
100 times and you observed 60 heads
your maximum likelihood estimate for
that coin
is going to be 60 divided by 100 which
is 0.6
so that is what your that would be the
maximum likelihood
estimate but there are
likelihoods or probabilities that
you could you know you could ask well
what's the likelihood that i would get
60 out of 100 coin tosses if the
probability was 0.5
or if it was 0.4 or if it was 0.3
and so what i want to do is i want to
reconstruct
what is the likelihood of our data
um given a particular parameter value
across a number of different parameter
values
okay and so what we can ask then is
what is the probability that i would get
60 heads
out of 100 trials or we could think
about it as
60 heads and 40 tails given
a particular value of that p
parameter and so i've written a little
bit of r code to kind of explore this
and this is an example of where you know
you don't obviously have to fully
follow all the code but i'm gonna make
things bigger
there we go okay
and so what am i gonna do i'm gonna
think about possible values
for um the p parameter
and so again with our example with the
60 out of 100
heads the the we know that the maximum
likelihood estimate
is 0.6 but let's explore how probable it
is that we would get
60 out of 100 across a whole range of
different
p parameter values so i'm going to
create an object called ps
and i'm going to use built-in function
called sequence
seq and i'm going to ask r to provide
for me
a sequence from equals 0.0001
so a really really small value something
that's incredibly improbable
i'm going to go all the way up to
a really big value for that p parameter
right that p parameter has to be
constrained between 0 and 1.
so these are really close to 0 and 1.
and then i'm going to ask for a thousand
values
and so all r is going to do is it's
going to count from
this number to this number
at equal increments and it's going to
give us a thousand values
from that minimum to that to that
maximum
okay and so here's all those values
there's a thousand of them
right and so now what we can do is
given the scenario that we flip the coin
60 times
or i'm sorry we flipped the coin 100
times and we got 60 heads
we can ask well what is the probability
that i would get 60 out of 100
if the p parameters value
was 0.001
and then what's the probability i would
get that if the p parameter was 0.001
and then 0 0 2 and so on and so forth
and so what we're doing here is we're
holding our data constant
and we're going to explore what the
probability is of our data
given a particular parameter value
and so i'm going to go ahead and i'm
going to plot that i've created that pz
thing um
and i am going to say plot the x-axis is
going to be the values the possible
values of p
and then the y-axis is going to be the
probability density
of getting 60 out of a hundred given
that particular
value and so generally when when we're
thinking about
likelihood
it's it's often written explicitly
you know again with with our example we
had 60 heads uh
and um in 40 tails we could ask what is
the likelihood
that we would get 60 heads
what did i say in that okay and 40 tails
those are our data
given given what given some
value for that p parameter
and so when we think about likelihoods
what we're going to want to think about
is
what is the probability of our data
given our parameters
so this is kind of a nice generic way
probability of our data
given the parameters
okay it's a nice kind of generic way to
always be thinking about
your data when you're modeling it and
you're estimating parameters
and so what i've done here whoa
is made everything disappear what i've
done here
is i'm going to explore across all 1000
possible values
of the p parameter i'm going to ask
what's the probability that i would get
60 heads
out of 100 coin tosses and so i'm going
to go ahead and i'm going to plot that
that's not what i want where is it
there okay and so here
is the likelihood of
of our data given the parameters
okay again the likelihood of the data
given the parameters our data
is 60 heads 40 tails
and so here you can say well what's the
probability that i would get 60 heads
and
zero tails if the probability of getting
heads was
0.2 let's say well that probability is
really really close to zero that's
really really improbable
what if the parameter value is 0.4 still
really really
improbable but you'll notice when you
start getting here around what our
maximum likelihood estimate is
those are the most likely values for our
parameters
and what we're interested in when we're
calculating
the um the the likelihood is we're
usually interested
in calculating the maximum likelihood so
what we'd like to know is
what p parameter here what value here
is the most likely value for that
parameter
given our data
and we can basically
look at what we're trying to do that is
the most likely
that is the mle that is the maximum
likelihood estimate
of our data okay and so what we want to
know
is well jim told us that we could
calculate this
as a 0.6 because he told us that if we
got six heads
or 60 heads out of 100 coin tosses
the maximum likelihood estimate is
is 0.6 and that's great
but let's demonstrate to ourselves that
it actually makes sense why that is the
maximum
likelihood estimate because whenever
we're dealing with
whenever we're dealing with
distributions we're always going to be
dealing
with trying to estimate parameters and
generally we're interested in
the most likely value for those
parameters
so the first step is let's revisit and
think about well how did we draw this
right we drew this using the d binom
function
we know that the d binomial function is
the probability mass function
for the binomial distribution
so this curve right here based upon our
data
this curve is being described by
that probability mass function and since
we have the underlying math
of what that probability mass function
is
we should be able to determine how do we
figure out
what value of p is
at this very very top here
and this is we're going to do a little
bit of math and this is really the only
time this entire semester where we're
actually going to do
real math but since we have a function
that
describes that curve
we could imagine that if we want to
figure out
what this value is we could take that
function that's describing this curve
we could take the derivative of that
function
right that's why calculus was invented
we can take the derivative
and then we can solve for where the
slope
is equal to zero okay so recall that the
derivative is telling us what the slope
is of any line that's tangent
anywhere along that whole curve and what
we'd like to do is we'd like to solve
for
a point where the slope of that line
tangent to that curve
is equal to zero and the only place that
that's going to happen
is right at the very top it's going to
be the maximum
likelihood estimate and so let's just
for fun because such things are fun just
to prove to ourselves
that we feel comfortable where these
maximum likelihood estimates are coming
from
i'm going to just walk through really
quickly the calculus
and all of this is in the the notes as
well
but i always find it sometimes or at
least not always but oftentimes i find
it easier
to follow along with written math if
somebody is kind of
walking walking me through it so let's
start with
what was responsible for drawing that
curve
again what is responsible for drawing
that curve
and it's the probability mass function
for for the binomial
and so what it is is what it's going to
be the probability
of a success
right times
1 minus p to the
n minus x
right so that's the probability mass
function
for a binomial sure we could multiply it
by the binomial coefficient to think
about all the different ways that we
might do it
but we flipped that coin 100 times we
know
we're only interested in our sample so
we don't really care about all the
different ways that we could have gotten
60 and also that that binomial
coefficient
is a constant right it's a number so
it's going to get dropped out anyways
when we start
doing um the little bit of calculus on
it
and so this right here is the likelihood
of our data
given given nice
real nice given our parameters
is that so this is going to be this is
our our starting point right here
so the first thing we'll do is here
we're multiplying and stuff like that
first thing that we would like to do is
to make that actually easier for
ourselves so
instead of thinking about the likelihood
we're going to think about the log
likelihood
people like to think about log
likelihoods because all likelihoods are
is a product of a bunch of probabilities
and since probabilities can be small and
if you start multiplying a bunch of
small numbers by a bunch of small
numbers
it isn't terribly convenient
um and the you know computationally your
computer will end up rounding it to zero
and you'll go nuts just trying to do all
that multiplication by hand
and so generally we like to put this all
into units of log because
multiplication in the log universe is
simply addition
and so what we can do is instead
of thinking about the likelihood we can
think about the log likelihood
and so let's go ahead and if that's just
the likelihood is that
let's say that the log likelihood
is going to be the log
of p to the x plus
the log
of 1 minus p
to the n minus x okay so all we've done
here
is
and this is why i rewatch all of these
so
although they are long imagine how i
feel i have to not only say it
but i have to watch it multiple times
that's a mistake
i have to redo this little part and it's
just how i've written the math
actually none of the other stuff i did
over the whole course of
the the now deleted um
the the now deleted part of of the
lecture was wrong
except for this one little this one
little spot
right here and maybe some of you
you saw it
this is wrong this little parenthesis
should be there
right that's where that's where that
parentheses
actually belongs um
and that means that we also need a
parenthesis that goes like this
right because the whole thing is being
logged
so 1 minus p to the n minus x that's all
being
taken to a log scale and so i just
miswrote it
so we're going to do it all again
okay so oh this is actually an
opportunity to
notice something else that i was kind of
i've been kind of throwing around kind
of loosely
likelihood and up to this point we've
been playing with d
binom and c binom we've been i've been
trying to be very explicit that it's the
probability of an event
and so what i've ended up doing in the
last five minutes
just i think because i'm just used to
kind of talking in a more casual
way amongst kind of statsy nerdsy people
is i'm using likelihood and probability
interchangeably
and actually that's fine for a discrete
distribution
likelihood is often or is also used
when dealing with continuous
distributions and when we deal with
continuous distributions as we see
as we'll as we will see we're not going
to really be talking that much about
the probability of any given observation
we're going to talk about it as the
likelihood because it's going to be
kind of set up in a slightly different
way
for our purposes today we can
interchangeably use
likelihood but so we can think about
this as the log likelihood or the log of
the probability
it's it's going to serve the exact same
purpose for for our purposes
so now we have everything in a log scale
and what we want to do now is we want to
take the derivative of this so that we
can solve for where the slope
is equal to zero and so
we just dig back deep into our memories
of of the joys of calculus and we
remember things such as the chain rule
where you do everything on the inside
inside first and then you do the outside
stuff
right so the first step that we can take
once we get to here is
we can start dropping dropping stuff
down so i'm going to say
x times the log of p
plus and minus
x times the log
of one minus p that looks like the
letter c
let's make that look like a parenthesis
okay so that'll be
oh well that's a problem
so so that that that's where we are at
this step
and now again remembering the chain rule
now we do the stuff that's on the inside
so it's going to be x if you remember
the
the take the derivative of log of
something
once you go into derivative land it just
simply becomes
the reciprocal of that value
plus n minus x
and i'll go ahead and i'll keep that in
parentheses
and now over here we have log one minus
p so we have a number of things
first thing we do is on the inside we're
going to take that negative p
and that's going to become a negative
one and then we're going to do the
remainder of the stuff that's in there
the log one minus p
which is going to be one over
one minus p okay so
there's our next step and then we just
do
a little bit of math and what we should
end up with
is the derivative so x times 1 over
p is going to be x over p
okay and then we can take this negative
one and just take care of all that stuff
here okay so it's going to be minus
minus what
minus n minus x
over 1 minus p
and that right there tells us the change
of the log likelihood
as we change our parameter
so that right there is the derivative
once we have the derivative now we can
simply solve for where at what value
of of at what value
is the slope gonna be equal to zero
and so i'll just go ahead and replace a
zero here
actually to save room
that's working real well
so i'll go ahead and i'm going to solve
for
for zero and now it's just a little bit
of
math playing around that we'll do so
let's say that it's going to be
n minus x over
n minus x over
1 minus p is going to equal x
over p
and then we can do our cross
multiplication
so we end up with p n
minus p x equals
x minus x p
right and go ahead and we can do
you know the the px's and the xp it's
the same thing so they're going to cross
they're going to cross themselves out
they're going to cancel each other out
so x equals pn we want to figure out
what the maximum likelihood value is for
p
when the slope is equal to zero so we
just simply say p
equals x over n
it's the number of successes divided by
the the total number of attempts
and that is the value for p and so
i know it's kind of convoluted and as i
promised it's the only time
to do math but the reason why i did this
is that it's this is pretty basic
calculus this is
you know first year of calculus type
stuff
and i do this just to take away some of
the
the kind of the black box the mystery of
you know
why is the number of successes divided
by the number of trials
why is that the maximum likelihood
estimate for that parameter p
well the reason why it is is because we
simply took the derivative of it
solve for zero and it turns out if you
want to find out
where that where that slope is equal to
zero the peak
of that that surface all you need to do
is take the number of successes divided
by the number of trials
and so that's calculus that's the extent
of our math for the whole year
there's nothing really magical
about maximum likelihood estimates
um it's just simply if we understand
what is the underlying function that's
describing our distribution so in this
case we have the probability
mass function all you have to do is um
is just do the calculus on it and solve
for zero
now this is the only time this semester
that we're ever going to do that
and this is a nice easy piece of
calculus to do
but when we start dealing with more
complex distributions like the normal
distribution
you can do the exact same thing there
where you could take the probability
density function of a normal
take the derivative solve for zero and
lo and behold the arithmetic mean
will be the maximum likelihood estimate
for for that particular
parameter so okay
that brings us to one last thing to kind
of think about
[Music]
which is just because it's the maximum
likelihood estimate
doesn't necessarily mean that it's right
it just means
it's the probability of our data given
some parameter so
the the accuracy or the precision to
which we're
we're able to um measure
or estimate a parameter is going to
very much depend on what
our data actually are which brings us to
last thing to to think about
which is called the law of large numbers
and so the law of large numbers is
simply
it's a theorem that describes
how precise you will be at coming up
with the true value
of a parameter um based upon
what your sample size is and so
there are certain situation if we if we
believe if we truly believe that the
probability of getting heads
is 0.5 for example
um if you were to flip a coin
you know three times you would never
end up at that 0.5 you couldn't because
the number of trials
would be three and so if you were to
only flip a coin three times
you will end up with a maximum
likelihood estimate
for the probability of getting heads but
we know that that maximum likelihood
estimate
is probably going to be wrong because
you're dividing by three there's no way
that you could get to 0.5
if you only flipped a coin three times
and so this is what the law of of large
numbers is going
after basically what the law of large
numbers is telling us is that
if we want to get a better estimate for
the underlying for the parameters that
are describing the population that we're
sampling data from
more information is always better
and so i just to kind of show as a proof
of concept
with with this i set up a quick little
simulation
where our response variable is going to
be y and what am i going to do
i'm going to take i'm going to use a
function now called
r binom which will give me a random
number
drawn from a a binomial distribution
and i'm going to have it work really
hard and i'm going to take
what is that is that a million yeah it's
a million
that's a lot so i'm going to take 1
million times
i'm going to take i'm going to do one
draw
from a binomial with parameter of 0.5 so
basically what i'm doing here
is i am simulating 1
coin tosses okay
and so if i were to do that just run
that code
here's all of my coin tosses ones are
heads or successes
zeros our failures and so on and so
forth i have successfully simulated a
coin toss
one other thing that i'm going to do
is i'm going to do what's called setting
the seed setting the random number seed
a lot of people that unless you play a
lot with computers you might not realize
that when computers are generating
random numbers they're
it's actually pseudo random numbers
they're not truly random numbers
the algorithm that the computer uses to
generate random numbers
um usually sets the seed
by whatever your clock says whenever you
turn on the computer
and then it ends up it ends up doing it
the the reason why we're going to set
seed is that whenever we're simulating
something
if we set the seed at two everybody will
get the exact same
answer so for example right now if i
were to just say
our binom i'm going to take 10 draws
0.5 so this is just doing
10 10 coin flips you know every time i
do it
it's different and every time you were
to do it at home
it would be different if i were to set
the seed
and then i do it and then if i were to
set the seed again
and do it i will always get the same
answer which means that on your computer
if you set the seed at 2
and you took 10 and you ran this code
right here
you would get the exact same answer as
me
if we didn't set that we would start
getting divergent
answers okay so a little coating caveat
there
so i'm going to take i'm going to set
the seed at 2 and then i'm going to do
these 1 million draws and then i'm going
to go ahead and i'm going to calculate
the maximum likelihood estimate for that
p parameter
okay so there's my million coin tosses
and i'm going to pretend
let's say that i only flipped the coin
10 times
okay so again the arithmetic mean also
turns out to be the maximum likelihood
estimate
for the p parameter of a binomial so if
i were to do that
that is that is my maximum likelihood
estimate for that coin toss okay
now we only flip the coin 10 times
but based upon our data that is the most
likely value for the p
parameter would be the probability of
getting heads
say being 0.6
so i can go and i can say well what
happens if i had actually paid attention
to the first 100 coin tosses
okay it's still wrong because again when
we simulate data one of the cool things
about simulating data
is that we are the omnipotent overlord
of this this computer universe that
we're creating
we know that this is the right answer
0.5
is the right answer because we simulated
the data
using 0.5 if we had only flipped the
coin 100 times our maximum likelihood
estimate for the probability of getting
heads is 0.45 you can say okay well
what if i what if i did it a thousand
times
getting closer to what we know the true
answer is
but still it's not exactly there and say
well okay well what about 10 000
coin tosses you got me flipping a lot of
coins jim
now we're getting pretty darn close
right but it's still not exactly 0.5
and if we were to just take all 1
million of those coin tosses
we're even closer again a million coin
tosses
we know the right answer is 0.5 that is
the correct
parameter for that simulated data
or those simulated data but even with a
million
tosses we're never really quite getting
there
we're just coming up with a more and
more precise estimate that's closer to
the true population value but we're
never
quite getting there and that's what the
law of large numbers
is all about is is that kind of stuff
so okay that was a lot of stuff but
hopefully
we have a pretty good idea of how
simple bernoulli draws when put together
creates something that's called the
binomial distribution
we've derived what the probability mass
function is
for the binomial distribution so
hopefully we have some appreciation as
to where those numbers are coming from
and then we've also learned how to use
those distributions given a particular
estimate for the parameter
to ask certain questions about what is
the probability of observing this event
versus what is the probability
of observing some other event so the
binomial
is appropriate when there's a fixed
number of trials
you know in our example there was a
fixed number of alfalfa patches that we
could have potentially gone to
or in the coin toss example there was
there was a set number of times that we
flipped that coin
and it it's appropriate unless the
probability of the event is extremely
extremely extremely low
um there are other distributions that
that you can deal with for extremely
rare events
i'm guessing that it probably won't um
be necessarily
that important for for the kind of data
that you will be
playing with over the course of your
career so
okay um and i'll say this a lot the way
to kind of get this stuff is to
you really just got to play with it you
got to play with our
um convince yourself that you understand
how to answer you know
different types of questions and that
you understand how these parameters are
being
estimated so so there we have it
um until next time see you later bye