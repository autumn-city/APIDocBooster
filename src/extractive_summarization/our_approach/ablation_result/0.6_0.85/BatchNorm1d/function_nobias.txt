It is simple: BatchNorm has two "modes of operation": one is for training where it estimates the current batch's mean and variance (this is why you must have batch_size>1 for training).  The other "mode" is for evaluation: it uses accumulated mean and variance to normalize new inputs without re-estimating the mean and variance. In this mode there is no problem processing samples one by one.
 Alright, so batch normalization, or in short, batch norm goes  back to a paper published in 2015, called batch  normalization, accelerating deep network training by reducing  internal covariate shift. 
Pytorch does its batchnorms over axis=1.  But it also has tensors with axis=1 as channels for convolutions.
BatchNorm1d can also handle Rank-2 tensors, thus it is possible to use BatchNorm1d for the normal fully-connected case. 
 In practice,  people nowadays, it's more common to actually recommend if  you use dropout to recommend having batch norm after the  activation. 
