 Yeah, as promised, let me now show you a brief code example
 illustrating the concept of the cross entropy in code using
 pytorch. So I have prepared a notebook I will share it with
 you, you can find the link as usual on canvas, or here just
 on GitHub. So we execute this just regular boilerplate. So
 here I have a function for implementing the one hot
 encoding in pytorch. It looks a little bit arbitrary with this
 scatter function. I don't really need to get go over this because
 I think there's no other context where I think this scatter
 function is really that useful. So you don't have to really
 memorize how this works. If you ever need a one hot encoding
 yourself, you can just copy and paste this function, it will do
 it for you. And also, um, when we do train a softmax
 regression model later, I will show you also code example for
 that. And the same for multi layer perceptrons, and so
 forth. On the loss functions in pytorch, they already do the one
 hot encoding for you. So you actually never have to worry
 about it yourself. But if you ever have to do it yourself,
 you can just copy and paste this function. So this is just how it
 works. This is exactly the same example that I showed you on the
 slides where we have the class labels 0122. And I have a total
 number of class labels here, this is the highest class label,
 or the number of classes, the same thing. So here we have
 three possible class labels, right? 01 and two. Also note
 that your class labels should always start with zero. So in
 this case, yeah, what we get back is for the four training
 examples here, four rows, and each has this indicator,
 whether it's this class, the one is denoting the class, and zero
 is, yeah, it's just a placeholder. So for the first
 training example, this represents class zero, class
 one, class two, and class two. Then here's this softmax
 activation. Sorry, this is actually the net inputs. So this
 is a matrix of net inputs, I created them just arbitrarily. So
 each row is again, for each training example, we have four
 training examples. So we have four rows, and this is how our
 net inputs might look like. So these are if I go maybe to the
 slides, these are the net inputs here, these. And then now we have
 to apply this softmax here, and then we get these activations.
 So yes, just for reference, how the softmax function looks like.
 And he has a code implementation of the softmax function. So
 again, there is actually a softmax in pytorch. So you
 actually shouldn't use your own implementation, because in the
 implementation pytorch is more efficient, faster, it's more
 optimized, but it's here again, to just illustrate how it works.
 So what we have is in the numerator, this exponential term,
 so when I go here, e to the power of z, and then in the
 numerator, we sum over these exponential terms for all the
 activations, if we have three classes, it will be a sum over
 three. So that's what we have here. Now let's do that. So we
 have our Z here, we have three classes 123. And for training
 examples 1234. So these are then the softmax activations that
 yeah, that we get back, notice that the columns should sum to
 one, can just double check that. So you can see they all sum to
 one, no columns. Looking good. All right, now, just to
 illustrate how we get the class table. So if we look at this one
 again, I mentioned we have an arc max here, arc max is, you
 can think of it, maybe I should demonstrate this. It's giving
 you the position of the highest value. So if I have an example,
 say torch 10, so 1234, the arc max would be the highest value.
 So in this way, the arc max should give me 0123, the value
 three. Um, that's weird here. Okay, what's going on? Oh, okay.
 Sorry, I think I should for here. So yeah, I was just not
 paying attention. All right. So this gives me the value three as
 expected. So if I put a five here, this would return zero
 now, because now the zero position has the highest value.
 If you have a tie, I honestly don't know what happens when we
 have a tie, but I suspect is it will pick the first value, the
 lower value. Yep, that's exactly the case. So if we have a tie,
 tie, it will always pick the lowest value. Okay. So, but in
 practice, it will be very unlikely that you have a tie
 here. I mean, might happen. But I mean, that could be something
 like this for the other classes where we have a tie. But
 usually, it's it's rare. All right. And this is also only a
 tie, because I put this as a tie. Usually, it's very rare
 that you have exactly the same values, because that means that
 the feature vectors look exactly the same. And the weight vectors
 are exactly sorry that the weight vectors are exactly the
 same. All right. Yeah, let's convert this to class labels.
 This is just to double check. So this these were all true
 labels. From the one that encoding is just to double
 check that we that this one indeed converts back to this
 one. And here, these are our predictions. So we use the arc
 max to get the column with the highest probability. So we get
 this one, this one, this one, and this one. And you can see,
 there's one wrong prediction, if we would compare those right,
 zero and zero, this is correct. One and one is correct.
 predicted class able zero, the true labels to this is
 incorrect. This one is correct. So we make one mistake,
 actually. So it's also, I mean, I did this on purpose to show
 you how this whole thing works. Because if we wouldn't make a
 mistake, we would get a loss of zero, which would be kind of
 boring, I think. Alright, so yes, again, the cross entropy,
 again, recall that there are two sums. So if I go back here, so
 we have these two sums here, I kind of entangled them a little
 bit. So we have, this is a sum over the training examples. And
 this one is the cross entropy for the 100 encoding. So this
 one is the inner, the inner one here. So let's compute first the
 cross entropy for each training example. So what I'm doing here
 is I'm computing these terms. I actually said that this loss
 will be zero. But that, I don't know why I said that the loss,
 even though you match the right class labels shouldn't be
 necessarily be zero, except if your activations match exactly
 here, one hot encoding matrix. Because remember, from the
 logistic regression lecture, we want to maximize the probability
 for the correct class. But it's only maximized if the
 probability is 100%. Right? So here, even this is correct, even
 if zero is the right class, this is not 100% probability. So
 here, we still have a loss, right? Because it's minus lock
 lock point 37. Just want to clarify, sorry. Where were we?
 Yeah, so here for the training examples, the losses we get is
 point 96.88 1.05 and point eight three. So this is computing
 this one, really the inner loop. So these correspond to these
 computations here. Alright, so that is how we would implement
 this. Now, maybe I can just briefly show you also how that
 looks like we are just taking the lock of the softmax
 activations, and then multiply it by the target. And remember,
 the targets are just zeros and ones, right?
 Define it, that's interesting. Oh, it's in, sorry, it's should
 be, I'm calling it as y ang, which gets passed to y targets.
 So let me print like this. So it says zeros and ones. So I can
 maybe also print it if you're curious. So this would be one
 value and the other ones should be zeros. So one value, and the
 other ones zeros in each row, there's only one value that is
 not zero in each row. And then we are summing, we're just
 summing over these, this is the auto sum here. So the first row
 would be on point 68. So and so second row would be point eight,
 eight, then 1.05. This is actually what we see here,
 right? So the summing is actually just selecting, it's
 just selecting the value here. Okay. Yeah, I'm cleaning this up
 here. Because when I upload this, so you have the clean
 version without my weird interjections here. Alright, so
 in pytorch, there is a function called NLL loss, which takes the
 lock softmax as input. So this function, the negative lock
 likelihood loss expects the lock of the softmax values. I'm just
 saying that we will use them in practice, we will actually use
 the cross entropy in practice. But I'm trying to explain how
 these functions and pytorch work. So you can see this
 negative lock likelihood loss is the same as our cross entropy
 here. Like I explained in when we go up, I think I had a video
 on that in logits and cross entropy. So where I mentioned
 that the negative lock likelihood and binary cross
 entropy equivalent in pytorch, it's actually the negative lock
 likelihood and the multi category cross entropy
 equivalent. I mean, in a way, you can also think of it as a
 multi category one, the multinomial logistic regression,
 then this would be still true. So they are equivalent. But
 notice, it's really important to pay attention to this is that it
 takes the lock softmax as input. So this might be counter
 intuitive, because there's already a lock inside, right. But
 the pytorch version does not perform this lock here, it
 expects that you perform it for pytorch. So pytorch really
 thinks you provide the serious input instead of the
 activations. Just want to clarify this, it's important to
 note because here we give the softmax as input. Here, we give
 the lock softmax as input. Right? Because when I just give
 the softmax input, I will get wrong results, I will get some
 just luckily, my glass was empty. Alright, sorry. Um, so
 you notice that now we have negative values here, right? So
 this is not what we want. So you have to pay attention that this
 is actually the lock. It's I'm just emphasizing this because
 it's a common mistake in practice when people implement
 deep neural networks, they accidentally sometimes provide
 the wrong inputs and then get weird results. And then the
 networks are not training. In practice, actually, I recommend
 using this cross entropy function over the negative log
 likelihood function. This is numerically more stable. So if
 you train a deep neural networks, the gradients and
 everything will be more stable. If you use that one,
 mathematically, everything would be equivalent. If we use negative
 log likelihood loss or cross entropy in pytorch. But
 numerically, like stability wise on the computer, the cross
 entropy one is more stable. So and also for this one, really pay
 attention to this one, it's taking the logits as input. So
 it's taking our net inputs as input. So here, again, this is
 our net input matrix. When we compute the cross entropy,
 because we use the mathematical formulas, we compute first the
 softmax. And then from the softmax, we compute the cross
 entropy in pytorch, they do all that work for us inside this
 function, they do it for us. So here, we give it the net inputs.
 And you can see, our function takes the softmax, it gets the
 same results as these other functions, right? So they should
 be all identical. But in practice, this is more stable, I
 recommend using this one. And notice that I said, redact
 reduction to none, which means it does not apply the sum or the
 average, which is this outer one here. So by default, when you
 use this cross entropy, it will perform the average, you can
 test it like this, see, it's the same same value. If you wanted
 to, you can also say reduction to consider reduction to some.
 Sorry, should be like this. So you can see, you can also do the
 can also do the sum, it's equivalent. But in practice, I
 recommend using the mean because I like I said, it's more stable
 in terms of choosing a good learning rate, it's easier to
 find a good learning rate if you use the mean compared to the
 sum. Right. So this is like a code example of how all these
 types of loss losses in pytorch work. I recommend you just to
 play around with that. When we implement the softmax regression
 in a later video, I will use this function. And also when we
 use later on, multi layer perceptrons and convolutional
 networks, we will be using this function. Alright, so and then
 in the next video, I will go over how we compute the
 gradients for softmax regression.