Let's now see a different version of collaborative filtering called item item.
The user user collaborative filtering algorithm we saw, recommended items that other users who are like you also liked.
An alternative view that often works better is called item item collaborative filtering.
Here we recommend items that are like items that you like. For a given item i we find items that are similar.
And we estimate your rating for i based on ratings for those similar items.
All of the similarity metrics and prediction functions can just be as they were in the user user model.
So the algorithm is rate an unseen item.
I as the mean of my ratings for other items weighted by their similarity to I.
So more formally, let's suppose we have a set N of items rated by me and similar to this unseen item i.
let's call it 2, let's say we have 2 similar items. For each of those similar items,
we have my rating (I'm x) my rating on that on that item.
(That should be a J). We have the similarity of that rated item to my unrated item I. So my rating
(I'm X again) my rating on i is the sum over all the items that are like i of my rating weighted by their similarity
to i.  the weighted mean of my ratings for other items that are similar to high weighted by their similarity to i.
Let's walk through an example again. Here's a utility matrix for users.
Rating movies.
And we'll set N = 2, meaning we'll be rating an unknown movie by averaging the 2 most similar movies that are rated by that user.
Let's say we want to know what would user five think about
movie one? So we try to fill in that cell. First, we have to do neighbor selection.
We want to find the two movies we picked and equals two that are most similar to movie one and are rated by user five.
And we're gonna use means centered, item overlap cosine as similarity. So we're gonna compute the mean rating for each movie.
So for each row and subtract it from the whole row.
And then we'll compute item-overlap cosine similarities between rows to find which movies are most like movie one.
I'm gonna give you a hint. It's gonna be movies three and movie six. So I'll just show the computation for those two.
First, we need to do mean centering for each movie. So we're gonna take the mean of row one.
What's the mean of row one. It's one plus three plus five plus five.
Plus four over five. It's gonna be 18. It's.
There it is written out neatly. And if we do that for the three movies we're talking about,
the unseen unrated movie one and the two that are going to be the nearest neighbors, movie three and movie six.
We computed separate means for movie three and the separate mean for movie six and subtracted out those means.
So now the low, low ratings like 1 have become negative.
Now, we just need to compute that cosine to see how similar these movies are to movie one.
Again, I'm just computing it for movie three and movie six so this video doesn't become endless.
So for movie one and movie three, we do the item overlap cosine.
So we're going to be computing the dot product. And that means we're gonna be multiplying elements for user one, user nine and user eleven.
None of the rest have elements in both vectors. So we'll be treating these as if they're vectors with three elements.
So here are the cosines for rows one and three. They both have values for those three users, one, nine and eleven.
And here's our cosine with our three values and our three values. And for rows one and six,
they have values for users one, three, andeleven. So we get our two cosines.
So our similarities of movie 3 to movie one 1 movie 6 to movie 1: .658, .768
We can put that .658  and that .768
In this column. In real life, of course, we'd have to compute the similarity of all the movies to movie one in order to find the two most similar.
But I'm cheating by telling you that all the other estimates are smaller. So we just need the two most similar movies because we're doing N equals 2.
So now we just take the weighted average of this 2 and this 3 to fill in the blank for movie one.
If we took the non weighted average, the non weighted average of 2 and 3 is 2 and a half.
It turns out the weighted average (here's my computation down here) is slightly different.
And it turns out weighted averages work slightly better.
Basically, it's saying movie six is slightly more similar to movie one than movie three is to be one.
So we're gonna weight movie six a little more. In practice, item item works better than user user.
Why is that? Items tend to be classifiable in simple terms.
For example, music tends to belong to a single genre. It's hard to imagine how a single piece of music could be both a Beyonc√© song and Bach Baroque.
On the other hand, there are individuals who like both Beyonce and Bach and listen to both.
Furthermore, items are relatively constant. A Bach fugue is always a Bach fugue, but people are dynamic.
Their tastes change. The consequence is that it's easier to discover items that are similar than users that are similar.
In summary, collaborative filtering works for any kind of item.
We don't need to define a set of features that define the item. On the other hand, collaborative filtering has some minuses.
It has a cold start problem. Whether we do item, item or user user similarity, we need enough users in the system to find matches.
We have a sparsity problem. The user ratings matrix is sparse. It can be hard to find users that have rated the same items.
It's hard to recommend an item that hasn't already been rated. It's called the first rater problem.
There's a popularity bias, we can't recommend items to someone with unique taste.
We tend to recommend popular items and it can lead to all sorts of problems
we'll have to deal with,  like filter bubbles and radicalization. We've now seen both the user user, and item item versions of collaborative filtering.