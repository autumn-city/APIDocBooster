Hi explorers and welcome to another
video today we are going to talk about
Tacotron first off before I talk start
to talk about Tacotron I really need to
thank Linda Johnson which you heard
before in the intro here she actually
spent more than 24 hours in a recording
booth recording different audio snippets
in order to create a dataset that I
could train with and create this
sound so huge thanks to her open source
contribution with her voice reading a
lot of material so if we are going to
talk about Tacotron and Tacotron is a
tool where you can run a neural network
with a lot of texts and trying to find
how to convert this text into speech
Linda is one of the contributors but
there is also some M labs that have a
lot more I think they have 700 hours of
speech on 10 languages so there is a lot
of speech out there you can use to train
your model so what I try to train here
is this little phrase here hello Hi
explorers and welcome to another video
you see here that I added next to D in
order to make that sound a little bit
more not natural or either she would
said video and I thought that sounded a
little bit off today we're gonna talk
about Tacotron I'm not sure if you
thought that it was good or if you
thought it was really bad but I think
that as the voice synthesis it's pretty
decent and I have trained the Tacotron
part of this network for about let's say
three weeks or something and there here
is actually a
wavenet that you can use on the Tacotron
result in order to improve this
even more and get a more natural speech
and that took me a month or a little bit
more than a month to actually train and
I thought a result was not as good so I
have scrapped that and I'm trying to
work around and get it to be even better
so how do you use Tacotron then this is
Tacotron 2 so it's with wavenet and
it's a tensorflow implementation of
deepmind Tacotron 2 through a deep deep
neural network architecture described in
this paper so if you want to read about
it you can read about it here and then
they described some structure here first
off you have some data set structure for
the M labs and then you have this L J
speech which is Linda's Johnson speech
and you will get an logs output Tacotron
this is the log the output from the
training of the Tacotron model and then
you have an output of the wavenet
training you will have some models here
so and output from the actual synthesis
and evaluations of the modules and then
we have the training data and this
training data will become quite huge
because you take some speech and then
you will create some numpy data
that we'll use in your training and
these are large I think that on my hard
drive at the moment they are about 30
gigabytes and all in all I think
everything here is about 110 gigabytes
or something like that and that's from a
sound sampling of one gigabyte becomes
30 gigabytes so if you download the
I think it's 30 or 50 gigabytes no 80
gigabytes of the training data from M
labs here M-AILabs then you will have
hard of data that you need to sift
through and it also requires a lot of
GPU to actually train your model so how
do we do this we get a data set either
Linda Johnson speech or the M labs we
pre-process the data so we get the
training data folder which is large I
actually ran into having two little hard
disk space I had to move the whole
project and then you train your model
you synthesize and evaluate that you
train your wavenet model and you
synthesize the outer and try and test
that out as well and pre-trained sample
a model can be found here I didn't look
at that at all I wanted to try it out to
train it myself this is the architecture
we you can't look at this if you like
you can see here that I have some
character embedding which goes into
three layers convolutional layer network
with a bi-directional LSTM so that's
long short long short term memory yeah
long short term memory and then you have
this little thing over here and then it
goes into the wavenet thing over here so
it's quite an involved network and
we start by running this requirement so
it will download a few Python
dependencies not that many it won't take
that much time and you need to setup
your H param settings I'm gonna go
through that file there is a lot of
settings and you don't need to know all
of them but some of them are will
highlights which are good to know
pre-processing is done like just running
the pre-processing step if you are using
the Linda Johnson dataset or you
will need to add a lot of parameters if
you're using the other because you need
to say that you want one of the voices
you need to tell it what language and
which reader and which book or if you
want to merge all the books and read
everything and they should take no
longer than a few minutes there Linda
Johnson took about a half an hour so a
few minutes is a little bit misleading I
would say but I guess it depends on what
kind of computer you have and then you
can train either both of the models or
you can train one model it's may the
wavenet to track a Tron it will save
checkpoints every 5000 steps and then do
an evaluation of the 10,000 steps but
you can add a variable to actually say
that it should save every thousand steps
and I did that because training and if
you need to break the training somewhere
you don't want to lose 4900 steps
that would be quite sad so adding a
variable to actually tell it to save a
little bit more often I think the
training data is about 300 Mb a time
so those are not super huge so I think
saving a little bit more often will save
you time in the end and then we have the
synthesis and this is just running the
module and getting some result out you
can listen and see what you think about
your training so it's pretty
straightforward
I download the master
branch somewhere in the middle of the
works I needed to go into some of the
scripts and remove some variables that
was not required and was added faulty
so it didn't run at all but I think if
you are downloading a specific version
or a branch that is more of a release
branch will not encounter those problems
but I wanted to have the bleeding edge
and then sometimes you need to actually
do some tweaking to get things running
so if we look at the H params you
first need to specify which language or
which cleaner you want and this is
something that goes through and removes
some things from the dataset to
ensure that it's clean so it's let's say
if you have a lot of special characters
in your training set or you have
characters that don't actually sound you
just write them because they have a
meaning in text but they don't have a
meaning in sound then you will clean
those out so this is to prepare the
training set and remove such characters
so the quotation signs for instance
those are there to bring meaning and
perhaps you will read that section a bit
different but when it you create sound
they don't actually make a sound and
then you have some audio settings here I
don't really know what all of these
actually mean now we have this very
strange LVS and it was not recommended
to run this if you did wavenet so I just
turned this off then we have some mel
spectrogram things here the important
thing to look at
here is the sample rate because when I
create my sound later on I want to bunch
a bunch of sounds together so if I
encode each sentence then I want to set
put them together with a little bit of
silence in between and to do that you
need to have the same sample rate of
your silence as you have on your actual
training set and then we have some
trims don't know we have mel
spectrogram normalization scaling and
clipping don't know so there's a lot of
parameter lot of things you can tweak on
here
everything here is something that
haven't touched it's just as it is yeah
a lot of things as you see then we come
down here to the wavenet and in order to
run wavenet and wavenet
training is the hardest part the
Tacotron training went just fine
and also synthesizing that was just fine
and if you using the linear sound
files it sounds quite good as you heard
in the beginning here but running the
wavenet that was where I encountered a
lot of problems and I got it to run if I
ran it in input raw I tried mulaw and
mulaw quantize as well but when I
did that I actually got the weird result
of having a loss rate of a minus value
so if you're starting from a big value
and then trying to make it smaller and
closer to zero I don't really know how
you should evaluate a value that starts
on minus 50
it's just don't make sense so I didn't
get those working at all and quantize
channels can try to run right either 16
bits or 8 bits
I've tried both 16 is
preferred I would say out channels here
they actually set that to 2 but as you
see here to Gaussian distribution as
outputs distribution instead of a
mixture of logistics set output channel to 2 instead of our channel 10 * 3 under
test so they're actually running some
tests in this repository and I set it
back to the old value and that worked
better but it still says that you should
have a number of distribution times 3
and I didn't really know what number of
distribution was I didn't find that
value but as they used 10 and earlier so
I reset it to that one thing here I
found today was upscale up sample
activation LeakyReLu never heard of
LeakyReLu before but you can actually
run with the standard ReLu if you
like so I'm gonna try that in my
training later and see if I get better
result but because I thought think that
my wavenet when I trained it it took a
lot of time and I didn't get a good
result so maybe this is to blame maybe I
should try a little bit less on the
LeakyReLu and then we have max steps
here this I changed a little bit as you
see here they recommend 8,000 to a
modest GPU and 13000 to a stronger
one I think I have a pretty good GPU but
I still had to move it down a bit in
order to actually get the model to fit
inside of the GPU memory I'm not sure
but I think this script or this training
set should require something like 16
gigabytes of memory on
GPU or 32 I only have eight so I'm a
little bit hampered there another thing
I had to exchange was the batch size I
think it was 32 or 64 before but I had
to tweak it down to 16 in order to get
it to run one thing you need to look
into then if you change that you need to
also change down here so these are
connected so 16 times the batch size
and the batch size you have up here you
need to change both of these values
important and then we have some start
decay and learning rate and so on I
haven't changed these these seems very
reasonable these beta 1 & 2 values and
epsilon is actually standard values for
the adam optimizer so it didn't see any
reason to change those and same goes for
the dropout zones natural eval sounded a
really fancy when I looked into this but
I don't really know what the teacher
forcing ratio is so I'm not gonna use
that one and then you have a lot of
setting for these teacher forcing mode
but there is a paper so you can read
about that if you want to try that out
wavenet training is very similar I try
this what with CPU because I thought my
GPU was too weak so I tried to run with
a little bit more of CPU it took forever
so get a better GPU if you want
to run this
don't even try this it's not worth it I
had to change this batch size as well I
think that was 32 before now it's 8 and
then we have the same as with the Tacotron
 setup up there we had some adam
we have some learning rates and clip
gradients I turn that off and train with
GTA and this is something that creates
GTA Mel's from the one of the
so ground truth activations from
one of the modules so the Tacotron
model so you can turn them over to your
wavenet model so if you turn this on you
have a connection between those but as I
said before I didn't really get my
wavenet to train correctly and actually
get a better sounding result afterwards
I need to still tweak these variables a
bit and try even more things but I think
I've been tweaking and trying things out
for four months now so I'm pretty happy
with the result I've gotten so far but I
have more work to do and then I created
this little fun script here just to take
text and create an output I first have
remove all the old evaluation waves and
then I will run the synthesize with the
model of Tacotron on a text file of
choosing this text file looks like this
you have one row for each sentence and
you can have as many sentences as you
like and then I create this little
silence so a piece of silence for one
second and then I will run sox and
loop over all the waves with a
linear so you have Mel's and linear
waves the Mel's are used for the
training of the wavenet but a linear
are the result that you that sounds
better
so you if you want to use them a right of
the bat that's what you want and I want to
sort these in a natural order so ls -v
will give me a natural order or else I
will get 10 before 1 and because because
10 is also 1 but 0 is larger than a dash
or whatever comes after ohh
dash comes off there before the linear here
and so I'm needed to sort them in the
natural order a loop over these and then
I output the file plus the silent wav
so this will give me a long list of
files to concatenate together with the
silence in between and then I end up
with an output which is the other file
name here so that will create a file
with continuous sound so you can hear
what actually is recorded can listen to
a little bit of a long search feature
action by katy reily long before it
opened in US District Court in Boston
unarmed 15 the trial over the use of
race in Harvard University Admissions so
here that it sounds very natural in this
speech and I really like this this
little project it's easy to run a lot of
things to tweak if you want to play
around with it it got me really geared
up and I can really see things that you
can do with this training takes a lot a
lot of time but actually synthesizing a
sound like the one I used in the intro
of this took about let's say a minute
and that's with the start up time and
concatenation and everything if you are
doing a long
thing so let's say that you're doing a
full article from a blog or something
that might take you 15 minutes or
something and then you have speech that
you can listen to so this is what I
wanted to talk to you about today I hope
that you found this interesting I hope
that you want to try this out I will
leave a link to the actual project down
in the description if you have any
questions about this or want to discuss
different things that we can try out to
get the wavenet to work or get even
better results and leave them down in
the comment section if you have any
questions leave those down
there as well if you like this video
give it a like share it with your
friends and colleagues if you haven't
subscribed yet please do that and I
really hope to see you in the next video