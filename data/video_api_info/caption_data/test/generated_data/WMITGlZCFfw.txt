all right hi everyone thanks a lot for
coming I'm very happy to be here and as
you probably guessed I'm going to be
talking about Pike perch so yeah I'm one
of the people who basically started it
although like a lot of people have
joined over the year it's a project
that's already like two years old so
we've gathered a pretty significant
amount of contributors which we're
really thankful for but yeah basically
we've met here I want to announce like
we've announced it before in fact but
like I want to talk about the new things
that are coming in pie chart 1.0 so just
a kind of sample at the audience how
many of you actually do know what pie
Taj does like what can you do you don't
need to know the exact API if you could
raise your hand just to kind of give me
an idea all right so we have a fair
amount of people so just like for those
of you who don't I'll just quickly
really quickly go over the stuff that
you can do with the library I don't
really have time because we have only
half an hour so I'll try to make it
quick whoops oh no those are my other
slides we didn't change them can we swap
them right now or yeah sorry for the
interruption there have been some
changes but
yeah exactly
no there wasn't there was an email
all right great we see the old logo so
we're back on track
yeah so basically those of you who like
nope I turds probably know it as a deep
learning framework I don't really have
like a lot of time to get into this
basically it lets you implement a lot of
like recent deep learning methods but
also more general machine learning
methods if like I'm going to give you a
like three minute kind of overview of
what it is it's really much simpler to
think about it if something is numpy
which probably more of you know
so basically numpy is like this library
that lets you have array objects in
python so like it kind of generalizes
lists and python to like higher
dimensional objects you have whole you
can have whole matrices of objects in
particular numbers or you can have
higher dimensional arrays and do some
manipulations on them and particularly
if you have areas of numbers it will
store them in a very efficient like
native data format for the computers so
like you can actually do some data
crunching really fast so basically in
here we allocate like 1 array that's
like 2 by 3 then just like create
another like wrap a scalar you can also
wrap scalar so you can add them it will
automatically like expand the scalar to
match the sizes you can like take the
two columns out of the array you can do
some you know linear algebra functions
on this like you can do matrix multiply
that's what the ad does you can sample
random matrices so there's a lot of
functions basically we can do and
ultimately at the lowest level pi George
is exactly this so you can also see that
like there's a lot of parallel and the
API is because like that's ultimately
like how people are used to dealing with
data in Python so but ultimately of
course just redoing numpy would be kind
of silly so obviously there are more
features that are there so in particular
it does integrate with numpy very well
because that's kind of the language that
every single package speaks and speaks
in python so basically are functions
that let you kind of get a numpy array
from a PI torch tensor and vice versa I
converted back so you can kind of use PI
dirge-like as part of your program well
everything else kind of uses any of your
favorite Python libraries like for
communication for plotting whatever like
it just works and it integrates nicely
and what's important about this is that
it's actually extremely
cheap so the cost is completely constant
it doesn't depend on the array size and
that's because the data in fact is
shared so we're only kind of
reallocating some objects but the data
that like you can read from the array
and that you can manipulate via the
array is exactly the same it's in
exactly the same place in memory all
right whoops we've got some freezes in
here anyway so yeah so one of the
features that actually differentiate
piperj from numpy is for example
accelerator support and accelerators are
really important especially for a
machine learning but also for data
science nowadays so basically what I'm
talking about accelerators I'm really
thinking mostly about GPUs which a lot
of you already have even consumer-level
GPUs can do a really fair amount of
compute those days like in particular
those array operations from have two
GPUs very well so because like GPUs are
extremely parallel machines and then and
so it's really and so like it's really
easy to transfer those programs and
actually execute them efficiently on
them and just to kind of give you an
idea again I don't have a lot of time to
go over the API but like that's
everything this runs in CPU this runs in
GPU so you can use the to function to
move between devices you can specify
what device some kind of you know
everybody should live on but basically
all of the operations all of the api's
are the same so it's really easy to have
like functions that are comfort
completely agnostic with respect to what
device they run on it only matters like
what kind of inputs do you supply to the
function and that's where all the
computer will be happening so you can
even use multiple GPUs if you have
alright and another really important
feature I think is really high
performance an efficient automatic
differentiation and so that's basically
something that lets you implement an
arbitrary function and you don't really
need to bother like writing derivatives
by hand you just kind of ask the system
like oh I like had those inputs I
computed this output now tell me what is
the exact derivative with respect to
those inputs right and so this is
especially important in machine learning
because it actually lets you like
gradient based optimization is something
that basically powers most of the
planning right now and and also like
widely appears in other domains
but also ideas like used in finance or
in like some engineering problems like
some physics simulations for example so
pi church also erase basically arrays
are kind of the first class object in
here
all of them support automatic
differentiation you kind of need to give
us some hints about what you will want
to differentiate and whatnot because it
unfortunately increases some overheads
but in general like in here you see that
there's like a regular polynomial
function that you could write in Python
there is nothing special like no
particular annotations in the code and
then you just use this torch autograph
grad thing and you give it the output of
the function you give it the input and
you basically get something that like
exactly matches the simple formula you
could get by differentiating this
function by hand laptop is really
surprised when I'm switching the slides
anyway yeah so that's kind of all the
time I have unfortunately to give you
the overview of the library so hopefully
you're kind of get an idea now I wanted
to move on because there's really a lot
of stuff that we've put into 1.0 that I
want to cover and yeah hopefully we'll
make it on time so yeah so basically 1.0
something to note is that it's a release
candidate which means the release is not
fully out yet you can actually download
it you can actually download it and you
can play with it we don't call it stable
not because the release candidate is
unstable but because like we're still
kind of making changes to it and like
you might still find some minor errors
that were kind of tweaking you know
before we kind of really like reach the
final point at which we'll freeze the
binary and like the actual code that you
will be running and that will happen in
a few weeks we're targeting basically
the beginning of December hopefully some
sometime around nips so yes you will be
you you you will be able to download the
full version like on all platforms
relatively soon now we'll wait for the
next slide all right
yeah so something that's been really big
about this about this release was
basically that piperj as you saw like
looks pretty much like numpy and this is
something that basically made it really
appealing for a lot of researchers and
fact that like nine driving force behind
pirate by tortoise popularity was like a
crowd of researchers started using it
and we really liked it and basically
that's because like that's exactly for
the reasons why Python became kind of
dominant in data science right like it's
really easy to iterate on various
pipelines that are you're building it's
really easy to kind of swap out some
blocks in the middle change free
processing it's like just a matter of a
few lines of code and like you have a
lot of higher-level helpers from
different libraries that you can kind of
compose very easily so this is something
really emphasized and like again we
really wanted to integrate with
everything very well so the research
side we kind of had covered something we
didn't had covered was something you can
call deployment and at this point it's
really important to kind of clarify what
deployment do I really mean because like
it's not like pipe I thought was
completely unsuitable for production in
particular there are people who are
running it in production right now and
we're running it if even a few months
ago before we even announced 1.0 like
it's completely fine to run Python in
production in many cases right like
Dropbox or Instagram basically are still
using Python for like certain parts of
our architecture however there are still
cases where like this is definitely not
something that you would like to do
right like if you're packaging a mobile
app like Python interpreter kind of ways
you know more than ten megabytes
probably that like adds to the your
binary size this adds overheads because
like it's not a particularly fast
language especially if you have a weak
CPU so you really would like to run this
without the overhead of the full you
know Python programming language and
there and so so so to kind of address
those problems something that the like
mode that runs in Python I'll be
referring to as eager mode and that's
the thing that you know it's really
simple to write and debug that that
basically is really good for research
you can iterate quickly and in fact we
believe that like most of the time that
people do will be spent in this part
like you will ultimately spend at least
a few days
weeks kind of trying to figure out the
solution and once you reach the final
solution you'll kind of want to package
it so then you need some kind of tool
that lets you take the final product of
your research and just distribute it
somehow in the sense of deployment that
I meant like if you can run Python
that's great you don't need to do
anything really and so something that
we've added is basically called the
script node and script node like kind of
underneath at the lowest level is really
a new programming language although
although that shouldn't really scare you
because that's really a programming
language that uses exactly the syntax of
Python and uses a subset of Python
semantics so like if you're right if
you're right if it writes a Python
script code basically you could run this
as Python code and it would run exactly
the same so like they're they're very
little boundaries between them so you're
kind of still programming Python no
matter what you do you're limited to
this subset but the benefits you get is
that we can actually extract like it's a
much simpler language so we can extract
a very concise representation that we
can serialize and later sent a mobile
device for example and we actually like
have more complete information about
like what kind of manipulations you're
doing inserter program so we can also
apply some optimizations that will be
really annoying to apply by hand
especially like do some optimizations on
the derivative code so just to give you
an idea of like what can you do in this
subset so types that are supported just
to that's probably like most informative
you can you can obviously work with
tensors integral and floating-point
scalars are fine basic control flow if
if conditionals while loops for loops
work print statements work strings work
tuples of anything lists also nested
basically polymorphic lists are there
and function calls so it's a relatively
limited language compared to like
whatever Python can support but if you
think about it like really this is
definitely enough to like express most
of machine learning programs and in fact
we've been using it with like relatively
with like some successes which is really
good
[Music]
all right so now the question is you
kind of iterate it on your program you
have you have this eager program that
you use the trainer model now the
question is how do you take the like
result of your work and actually package
it and there are two ways basically that
we provide you to do that so the first
one is toward to trace and basically
something that's really good about this
function is that it actually doesn't
require any changes to your code
whatsoever so you can still use the full
power of Python the downside of this is
that it actually like you have to give
it a function that you want to kind of
recover and you have to give it an
example and it will run this function
once and it will kind of record every
single tight wrote operation you do
along the way right so the downside is
that it actually doesn't record control
flow at all so this is like a single
list of instructions that happens so
like if you have a and if conditional
only the like branch that was taken in
there in the example will be present
will never really ate the condition
again if you have it if you had a loop
that executed four times it will always
execute four times will never check the
condition again and we also like don't
check for this in your code so like if
you accidentally produce something
that's not like whatever you meant
that's unfortunately on you and you
might think that actually sounds
dangerous but this is actually very
useful in a lot of use cases like if you
take convolutional neural networks
basically which will appear in a moment
in here so this is a picture from their
residual learning paper so basically all
most convolutional neural networks kind
of have this fixed structure right so
there's this like kind of mathematical
object that like always computes a
function that that's like specified kind
of like this so it doesn't really matter
how do you like what code do you use to
describe this structure what's important
is that you actually implemented every
single every single block in here is
basically an operation that's done
inside the network and so what's
important is that if you if you like to
use PI tert functions to implement those
we will recover exactly the same
sequence as it appears in here right so
and as an example like you can have for
loops and as part of your program and
and like have it be
completely fine so this is a dumbed down
version of the network I've been talking
about so if I were to implement a
network like this I would probably just
have a list of those blocks I don't want
to get into the details of like what's
on the on the top of the slide but every
single element of this list is kind of a
single block that you saw in the picture
a moment ago and basically then the
whole model would be like just iterate
over those blocks and kind of apply them
in a sequence right and so if you were
to trace it it would actually evaluate
to something like this like this and
yeah so it will kind of fix the length
of the list but ultimately the structure
if your model stays the same over the
whole training right so like it might
depend on some command line parameters
who might be dynamically specifying the
number of layers but while you're
training the model you probably won't be
adding more of them and if you add more
of them you can actually retrace it and
like keep training again but like as
long as it stays constant it's fine and
you can actually export this model just
fine in particular if you go to the
third fission package it's a package
that we have with like a lot of
utilities for training models for
computer vision problems basically it
provides some common data sets and data
loaders and pre trained models so if you
take any many any model pre trained or
not from this package you can safely
trace it because they're all basically
based on this static graph approach so
it doesn't really matter like whatever
code we use to implement them and the
other one is torch it script which is
exactly something that lets your writes
basically in the script notes so you're
so writing Python we do static and now
it's a function decorator so you can
kind of mark the functions you you
assert to be in the subset of Python
that we care about and we do static
analysis on those functions so we can
correctly recover control flow that's
happening we will be actually like
running loops as many times as they
should in like calculating how many
times we should run them but the
downside obviously is that you're so
restricted to the subset so like you can
do arbitrary things inside and the
example that
that's not my iPhone but the example is
the example that that like fits the
script really well
our recurrent neural networks so for in
in particular recurrent neural networks
you can treat them as sequence to
sequence models so you can have a in put
sentence in one language and output
another sentence in a different language
right because you have a translation
problem and sentence is obviously vary
in length right so you will want to run
a different number of steps depending on
what kind of input you got so and this
in this case you couldn't really use
trace because you would fix this is
their sentence length but you yeah you
couldn't use trace but you can use
script which will correctly recover this
and in fact in here like provided you
have this lsdm cell defined you can see
everything that you need to row to write
out an RNN in part and even in Python
script which is completely exportable
and the nice thing about those two
functions is that they actually make
seamlessly so if you have kind of a
large complicated model that's
ultimately almost fixed and that like
just looks kind of like a CNN and except
like in some part in the middle it kind
of has some you know component that
requires some dynamic control flow you
can just script this component and in
fact in this case script when it detects
trace when it detects that you know it
hit it hit a scripted component it will
not trace it it will just use the
there's this scripted kind of
description and it will actually have
control flow in the final thing and also
script supports function calls so if you
call something that's been traced before
you know from the surrounding scope that
will also be registered as a proper call
and what I mean a proper call it's also
important to say that actually script
can also call back to Python which is
very convenient because like we don't
really want you to kind of go on a you
know long adventure of taking all of the
model code you have and transferring it
to this subset and hopefully getting
it's kind of to work and you know after
two days of work you finally test it and
then you made a mistake somewhere along
the way and that will be basically a
user
nightmare so instead what you can do is
you can actually change your your
implementation function by function
checking every step along the way that
everything is so fine that everything
still works and also if you kind of have
a model that you export it but then you
kind of want to go back the research
cycle it's still fine to like remove a
few of those annotations and just I
don't know surd using numpy or any other
Python library inside so that we won't
complain that you're using something you
know outside of the subset but it like
you can still run your Python program in
particular and yeah and the descriptions
we get kind of looked like this from the
more compiler or inside of you if you're
interested basically you can also
inspect this like for tool for
visualization and off like whatever
whatever your models do but like you
don't really need to completely do that
like it's it's kind of form I say more
advanced usage and the important part is
that like I talked about export so
there's basically a single method which
is like you can call on the on the you
know a scripted function or on a traced
function you can call dot save and it
basically will dump a file on your disk
which you can later load like using the
single top line in here you can
basically load it back in C++ and we
also have a C++ interface that pretty
much matches our Python interface that
I'll be I'll be talking about it in a
moment as well and so and so in
particular we have a pure C++ binary
that we kind of allow you to use that
you can like integrate into your mobile
apps it's much more lightweight it
doesn't really require the Python
interpreter it has its own interpreter
for those programs that we that we like
support in this in this subset which is
much faster and much more efficient than
this and also so that was for export
mostly but like I also talked about
optimizations and so something that
we're kind of trying to help you with is
so high torque and of comes with a lot
of operations built in and if you like
if is something you want to do exactly
exactly matches the operations you have
basically like they're in there
two mice kernels written for them so
we'll use those and that will be really
fast but if you write through yourself
and Python it will like be five times
slower so JIT is kind of aiming to help
that what to help with that it will like
attempt to redo the optimizations
automatically for you
they're like roadblock sign in here is
because that's not really finished like
we're not promising magical games at
this point like it's 1.0 this is a very
new feature and we've been mostly
focusing on getting kind of the
usability right and not actually the
optimization but like on some LSD and
variant we've been so it's a recurrent
network that we've been benchmarking
it's actually like a 3.2 x feet up which
is pretty good anyway so another feature
that's really important I think for if
you if you want to use this in resource
constrained environments are extensions
in interface so actually spider CH lets
you take both this is for research those
are extensions so PI touch basically
lets you kind of bind CUDA kernels and
custom C++ code into Python without you
know having to go through a lot of pain
of creating a completely new package and
like using Python C API for extensions
and basically so in here there's like a
function that this patch is some CUDA
kernel and like multiplies one of the
input arrays by two and everything you
need to actually bind this to Python
from C++ or those three lines so we use
PI bind 11 which is a really convenient
library so yeah that's everything you
need to do in C++ the problem with C++
obviously is that at some point you
actually need to compile it and so we do
provide some hoppers for this as well so
there is a set of tools playing base
method which is for if you want to
actually distribute this but if you just
want to kind of quickly hack on a file
there is basically a one-liner where you
kind of provide a list of files you want
to compile and that will like set all
the appropriate flags and call into your
compiler and load the module into Python
so then you just like call that compute
which is exactly the name we use to
expose the function on the previous
slide and like and you can just provide
a race or like in fact it works with
like vectors of integers or tensors or
nested or whatever you really want so
it's really convenient and the
face the interfaces in beta but you can
try it out like even in 1.0 this will be
in beta so there might be some slight
changes that we'll be doing to it but
basically the idea is that like this is
an example of how would you write a
neural network in Python using pi troj
and this is how it looks in c++ right
now so it's like relatively close you
can see that it's mostly syntactic
differences in fact if you were to look
at a training loop that's how it looks
in Python and that's how it looks in C++
so those are like exactly you just
change like dots to columns and like
change the syntax of the for loop and
it's pretty much that so like if you're
doing some RL research if you have like
doing some robotics you just have a C++
application you don't want to figure out
how to integrate with Python now you'll
be able to just like have a pure C++
code base to do all of their kind of
training and development and there are a
lot of a lot of packages that we've
already ported so there are helpers for
knurled for building neural networks and
machine learning models there are
gradient based optimizers obviously the
ad so automatic differentiation and
accelerator support work in C++ as well
there are data loaders the stud like
efficiently paralyze data loading so you
can like always have something to train
on there are serialization utilities
utilities for integration with Python
and there is the JIT compiler that we
developed which is exactly this like
Python subset this script mode and the
last feature that I want to talk about I
don't have a lot of time left
unfortunately basically we did a kind of
distributed back-end overhaul so it's
like a completely new code that
interfaces with different back-end that
like reinforces new abstractions in
particular it emphasizes asynchronous
operation so we do keep api
compatibility with like all their
versions of pi toge but you you can
specify some flags that tell us to kind
of run some jobs in the background and
you get kind of future objects which we
can which you can wait on right so you
can kind of interleave multiple
transfers you can have multiple groups
that work using different backends so
you can use one library for exchanging
information between GPUs you can use
another library to exchange in
in between you know CPUs and different
nodes and and also there are some
performance improvements and in
particular the new API that like lets
you use multiple backends it also lets
you implement fault tolerance which is
something we didn't support previously
and also instead of fault tolerance you
can actually like leverage the same API
to also implement elastic sizing and
like you know use the on spot instances
when they're cheaper to kind of scale up
your working group and and and like make
your training progress faster and kind
of the last bit of news is that you
probably like those of you who kind of
follow pied rich news cafe to is merging
with pie chart which is really great
because cafe 2 has a lot of great
kernels especially for mobile and like
the cafe 2 team has like a lot of
experience and running in those resource
constrained environments that we're
working with them to kind of bring them
together and like use the research side
so the API will like remain whatever I
showed you but like a lot of the back
end bits especially will come from cafe
2 where they have been heavily optimized
over the years so yeah that's everything
I have pie chart is really like really a
community project like everything's
happening open-source all the
development is happening on github
basically through a pull request it's
supported by multiple entities both
academic institutions both academic and
and and corporate and yeah we have
really active community as I mentioned
we have quite a bit of contributors and
very active user forums so if you have
any trouble you can just go there and
someone will surely help you and yeah so
I hope to see you somewhere in the
forums or and github and thanks a lot
for coming
you