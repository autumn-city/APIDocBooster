 In practice, actually, I recommend  using this cross entropy function over the negative log  likelihood function.  This is numerically more stable. 
 Yeah, conceptually this is a tensor of integers, they can only be 0 or 1, but we, we�re going to be using a cross entropy style loss function, so we're going to actually need to do floating-point calculations on them.  That's going to be faster to just store them as float in the first place rather than converting backwards and forwards, even though they're conceptually an �int� we're not going to be doing kind of �int style calculations� with them. 
 What happens   if we're not predicting which of five things  it is but we're just predicting “is it a cat?”   So in that case if you look at this approach  you end up with this formula, which it's   exactly… this is identical to this formula but  in for just two cases, which is you've either:   you either are a cat; or you're not a cat,  right, and so if you're not-a-cat, it's one minus   you-are-a-cat, and same with the probability  you've got the probability you-are-a-cat,   and then not-a-cat is one minus that. 
 And so then these are the predictions  that came out of the model,   again we can use soft max or it's  binary equivalent, and so that will   give you a prediction that you're-a-cat, and the  prediction that it's not-a-cat is one minus that. 
 If we use negative  log likelihood loss or cross entropy in pytorch.  But  numerically, like stability wise on the computer, the cross  entropy one is more stable.  So and also for this one, really pay  attention to this one, it's taking the logits as input. 
