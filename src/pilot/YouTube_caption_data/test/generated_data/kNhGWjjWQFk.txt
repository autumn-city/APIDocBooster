hi this is abhinash majumdah a research
scientist for morgan stanley
previously i was working for hsbc as a
deep learning nlp engineer
i have also collaborated with google
research in matters
of bird and albert models and other
language models
i have mentored students and
professionals for organizations like
udemy udacity and upgrad
uh in terms of nlp ai deep learning so i
have quite some exposure
in mentoring and uh today we are going
to talk about
encoder decoder models so if you and
also welcome to co-learning launch
youtube channel so uh in this channel
we will be posting regular updates on uh
different
uh complex topics of nlp like uh
attention
bird and state of the art language
models
uh along with this if you want to
connect with me you can find the details
in the description
uh in the link given below also feel
free to comment
and also subscribe to this channel and
uh
let's get started with the encoder
decoder architecture so
we had covered till uh lstms gru's
in the last video so building on top of
it we will be moving towards the encoder
decoder model which is nothing but
a sophisticated neural architecture
comprising of different
stacks of lstms different variants of
lstms and gres
so in this case we will be seeing what
are the use cases of
these encoder decoded models since the
invention of sequence to sequence model
by ilia
so from open ai the
the the uh the details of neural machine
translation and also the important
features of text classification question
answering
semantic inference all these kinds of
nlp tasks have
gained massive popularity because of
this sequence to sequence model it
gradually
in the context of neural machine
translation this has achieved a great
performance
where we want to map certain words from
a different language
to another different language in the
example given over here
how are you is in english and how ma is
in chinese
so this is achieved with the help of
sequence to sequence model
and we are going to study a variant of
the sequence to sequence model that is
known as the encoder decoder
architecture
in the 2016 paper from google which was
named as the jointly aligning to
translate
by bharathanu it was one of the most
important papers which
gave birth to this idea of encoder
decoders along with the concept called
as attention
which we will cover later in our video
tutorials
now coming ourselves to the basic uh
architecture of encoder decoders
these are nothing but stacks of certain
uh recurrent neural networks
which are coupled on back of each other
like a stack
so when we move forward to a general
sequence to sequence model
it is generally refers to mapping
certain parts of vocabulary with certain
parts of vocabulary
so this model aims to map a fixed length
input
with a fixed length output where the
length of the input
and the output may differ so
basically in the case of language models
where machine translation is very much
required
different languages in different may
have different uh
side lengths of the sentences when they
are translated
such as what are you doing today in
english
the chinese version of this particular
sentence or this particular question has
only five words
and an output of seven symbols so
clearly
this general lstm or you know gru
architectures
do not um you know sufficiently
fail uh they actually fail to actually
scale or
try to reproduce these kinds of results
because lstm networks tries to map each
word to each word
so they will tend to have um an equal
sized input and an equal size output for
mapping each of the
input words to the output words but what
will happen if we want to
if we want to you know classify
different sizes of inputs and different
sizes of outputs
which is the case of neural machine
translation so
in this case english to chinese
translation is not achievable with
just a plane or a vanilla an stm network
or a vanilla
uh gru network or any versions of rnns
so we consider ourselves with the
encoded decoder model
and this is the diagram of standard
encoder decoder model where we have
stacks of
rnn cells these rna cells can be lstm
cells this can be gre cells as well
so in basic sense these are generally
comprised of bi-directional lstm cells
or biodirectional
systems so generally we have the x as
the input features
and we have the hidden state from the
previous time step of the previous cell
so these can be any variants of rna just
like i mentioned either lstm or grdus
if we are considering lstm then each of
the intermediate hidden layers
each of the intermediate hidden cells
will have two outputs that is the h
and the c that is the hidden cell state
and the current still state
h and c but if we were using g r use
then
as we know in gre's the two hidden
states that is h and c
are merged together so we will have a
single hidden
state that is known as h so depending
upon the architectures or the internal
neural network structures
that we use uh lstms will have
intermediate two
outputs whereas the gda does will have
intermediate on
outputs so in this in this
stack of uh lstm cells or grdu cells
the input gets passed one after the
other
for each particular lstm cell this is
get denoted by x1 x2 and x3
whereas the output of the hidden cell is
also transmitted one after the other one
cell after the other for each time stamp
this is denoted by h1 h2 and h3
and in the intermediate uh part of this
lstm stack of cells these are returned
as sequences rather than distinct
outputs
this part comprises the encoder model
and this is particularly adhering to one
language
particularly one language let's say
english what happens after this
is the last lstm cell or the last rnn
cell it outputs certain
values traditionally and lstm outputs
three values
so we have the hidden cell state we have
the cell state that is c
and we have the output value and if we
were to comprise ourselves of this
particular part
then uh one thing we can do is that uh
the most important thing that we can do
is that whenever we consider ourselves
the output of the last lstm cell
we can try to earth it that means we
will not
we will not consider the output that is
coming out from the last cell stem cell
from the encoder
we will just consider ourselves with the
hidden uh hidden outputs that is the h
and the c
if we are considering lstm now similarly
if we are considering gre use for our
use case
then there will be a single uh there
will be a single combined hidden state
that is h
and an corresponding output so for a gre
there are only two outputs
output state o and a hidden and a
combined hidden state that is h
so similarly the output of the encoder
gre
will be uh will not be taken into
consideration and only the hidden state
that is the h state will be taken into
consideration
for the decoder input now this
particular thing is known as the encoder
vector
that is the hidden cell output of the
stack of lstm cells
encoded cells is known as the encoder
vector this is passed on to
the decoder as the input so the decoder
is a similar architecture
it is it comprises of the same version
of rnns right or it can be variants of
lstm or it can be variants of gres as
well
so similarly we pass the encoder vector
which is the output of the lstm encoders
right and also pass
there will be another inputs which is
corresponding to the inputs of the
different language that is chinese in
our case so the inputs of the chinese
language and the encoded vector
hidden cell together that is output of
the encoded cell together forms the
input of the decoders
and in decoders we have a similar
architecture where we pass the hidden
states
for each of the individual lstm or the
gre units or any rnn variant units of
the decoder model
so if we were to move in a descriptive
part so what we will happen
so the model consists of three parts
that is the encoder
the intermediate encoder vector that is
the output of the encoded cell and the
input to the decoded cell and the
decoder
so the encoder can consist of a stack of
recurrent units that is lstm gre or
any time distributed recurrent neural
networks so where these types accept a
single element of input
collects information for the particular
element and propagates through that
network so the stack of cells
and then if we have a question answering
problem the input sequence is a
collection of
all the words from that particular
question now generally if you want to do
translation or if you want to do
question answering or any kind of an lp
task there are tokens which are which
are getting apprehended so this means
that the start token and there is a stop
token the start token signifies that
the start is this this this marks as a
starting of the sentence in a particular
language
the stop token signifies that this is a
stop symbol
or this is the terminal part of that
particular language
so this is very important because when
we are considering ourselves with neural
machine translation
we know that different size inputs will
have different sized outputs
so it is very important to have a start
symbol and a start stop symbol
or tokens to which specifies the lstm
cell which specifies the encoder as well
as the decoder where to stop
our you know training purposes where to
stop where the
input stops so this is very very
important
and when we consider ourselves with the
internal hidden layers
in a particular stack of lstms for the
encoder or the decoder
we know that inside and a particular
lstm we have the 10h activation
and we have a similar kind of function
that saw previously
that we have h e to hd minus 1 plus wxh
into x t
input this is the corresponding weight
vector for the input h t minus 1 is the
previous time stamp hidden cell
and w h is its corresponding weight and
f is generally 10 h if we have seen the
previous videos lstm
we generally use the 10h version and we
get the hidden output
now this is all good when it is when we
are considering ourselves with the
intermediate parts that means inside
these intermediate cells which returns a
sequence rather than an output
now if you want to generate the output
we generally want to do a soft max of
this hd
with corresponding to its corresponding
weight now encoded vector
this is the final hidden state that is
produced from the encoder
part of the model and
it is calculated using that formula
above where we only pass the hidden
state
we do not consider ourselves with the
output of the lstm cells
now this also forms us the input to the
decoder part now what is the recorder
the recorder is a stack of several
recurrent units similar to lstm or
similar to a gre
now if you are using the same version
that we if you are using the lstm
version with the lstm version that is
lstm lstm encoder decoder
it is known as a homologous encoder
decoder model because the architecture
almost remains the same
but if you were to use a variant let's
say a bi-directional lstm and a pi
direct and a single lstm version stacks
in the decoder part then it will be a
hybrid version
because in the encoder part we are using
bi-directionality well as
in the decoder part we are just using
normal stack of lstm servers
a stack of several recurrent units where
each predicts an output yt at a time
step t
so this can be for the different
languages right when we are converting
from english to
a particular word to a chinese word or a
different language word
so each recurrent unit accepts a hidden
state that is from the previous unit as
we saw before
because there are stacks of you know
recurrent units so these accepts the
hidden states over here
and they also accept the encoder vector
from the output of the encoded cell
right and this is computed as this
particular equation that is given over
here which is nothing but w h h to h t
minus one
and because in general decoders we
generally do not have
the x t part we generally have only the
hd minus one that is only the
input hidden layers we generally do not
have the xt
but there are other variants of this
particular aspect as well
because if we are using uh for other
tasks
let's say try you know text
classification or sentiment
classification
we want to use embeddings we want to use
embedding
with the input and we also want to use
some other attributes as well
so if we just consider ourselves with
the embedding layer if we want to use
embeddings for our text classification
through an encoder decoder model
then this equation becomes a bit more
elongated to also include the embedding
vector as well
so here plus we will add and we will
also add the weights of the embedding
vectors as well
in our hidden states for our evaluation
so that is the only difference that is
required to be done because we are
particularly seeing for the case of
neural machine translation
where we when we where we are mapping a
particular phrases of words
or phrases to another phrases in a
different language but in the case of
text classification we have to apply
some kind of embedding matrix as well or
embedding layer as well
so moving forward the output
since the decoder also we need the
output we also get the output by using
the softmax activation of the final
hidden state of the decoder model
so in the case what is the difference
between the encoder and the decoder is
that
in the encoder the output we do not
consider we only consider the output
hidden state that is the h of the
encoded stack of lstm cells or gru cells
but in the decoder we do not consider
ourselves with the output with the final
output hidden cell we we consider
ourselves with the only output that is
the o
we do not consider ourselves with the h
of the decoder part
because we want the output output
probabilities that is the softmax output
probabilities
so another analogy can be that we
consider all of this that is the encoder
and the decoder to be stacks of
different lstm or rgru units
placed one after the other where only
the hidden cell or only the hidden
signals get passed one after the other
and the outputs does not come into the
picture the output only comes into the
picture
at the last or the decoder last lstm or
gre unit
before that no output comes into the
picture here only the hidden cells or
only the hidden calculations comes into
the picture through the throughout the
encoder and the decoder
so this is what the entire architecture
of an encoder and a decoder looks like
so generally in a decoder in neutral
machine translation in the case of a
decoder
the equation is a bit reduced as
compared to the encoder because we only
consider ourselves with wh into html
but in the case of text classification
we also have to add
some embedding vector as well we also
have to add some input embedding
matrices or input embedding vectors for
our language for our text to classes for
our model to classify
and needless to say this encoder decoder
architecture forms a uh from some block
of neural machine architecture
now if we want to build on top of it so
let's say we want to add a feed forward
neural network or a simple dense network
on top of it
then that can also be done so whatever
is the output of the decoder part
we can also pass it through a dense
network and
we can have any kind of activation
functions like sigmoids
softmax to generate our corresponding
probabilities for our text
classification task
so the encoder decoder architecture can
be considered as a very sophisticated
neural network architecture
based on sequence to sequence learning
which tries to capture
more and more information uh which a
standard lstm cell will fail to do
or a standard vanilla lstm architecture
will fail to do so it may be very
important for
neural machine translation where we have
different lengths of
inputs and different kinds of outputs as
well as text classification question
also generate generation
by the by employing the use of certain
start and stop tokens
and this is very important because an
improvement from the
standard lstm or the vanilla
architecture because we are considering
ourselves to the hidden parts or the
hidden
uh computations of each of the encoded
and the decoder models
so this leads to more red memory
retention as well as
more performance so this was all
that that i had covered regarding
encoders and decoders uh definitely
in the next topic we are going to cover
about attentions and and sophisticated
details about it
so these encoder decoders have another
model which is mathematical computation
tool which is known as attention
there are different variants of it and
these attention mechanisms also help to
boost the performance
of translation and any other language
modeling task let's say classification
or question on some generation
anything so this was in general all that
i had to cover
and i will see you guys in the next
video tutorial thank you