hi everyone this is our research jam number three 
can't believe we're doing it the third time around  
and this time we we were soliciting submissions 
really late but still within a week we have  
eight nine submissions so really happy to bring 
these lovely eight folks on to have them talk  
about their projects we can get started but 
before that we want to announce that our next one  
uh usually we do it every eight or nine weeks but 
if you count eight weeks i think it's like almost  
exactly iclr deadline um so we want to move it 
one week forward so people can have like one or  
two more weeks to write about iclr if you want to 
talk about your iclr papers then write about them  
yeah so it's it's good timing so you have so seven 
weeks from today we'll have our next research jam  
as rosanne mentioned and then after that day 
you have two weeks to like write something up  
so if you're working on a project and you want 
a deadline to push you toward iclr seven weeks  
from now that is that deadline and then you would 
have two weeks to get the actual draft together
so should we start with um j k are 
you are you on the call yeah justin um
yeah so i am the maintainer of gym petting zoo 
now and as i believe we'll be discussing the  
other call with other products here i kind 
of need help um so for brief history lesson  
so for a history of so the history of gyms as 
follows um reinforcement used to be fairly hard it  
used to be fairly inaccessible um and it required 
a lot of engineering part uh work on this part of  
this was that they're a bunch of unmaintained 
libraries for a bunch of scattered projects  
they were inconsistent apis in many different 
languages for different uh things they lacked
uh and so all these things 
like these environments like  
interoperability because you know the 
environments and the learning code  
they all the different apis you can you know 
just try something out and see what happened  
uh you couldn't have any standard reinforcement 
learning libraries for learning methods  
um this of course code reproducibility 
and all sorts of productivity issues  
so um so about this time when when rl was starting 
to become increasingly big uh elon musk's side  
of the robot apocalypse was imminent and it 
opening up so jim was created for this reason  
um in an attempt to sort of democratize 
reinforcement learning i was created by  
largely by greg brockman who's now the cto of open 
ai and it's universal api between environments  
learning code and a and instead of uh something 
like 60-some reference environments which is um
uh and it's become just absolute field standard 
um and so to talk a little bit and this was done  
in the 2015. so talk a little bit about petting 
zoo then so there was the same problem multi-agent  
rl is hard because there are all these major 
code bases which are maintained with all these  
heterogeneous apis and i had this idea but 
someone should make gym for multi-agent rl  
it'll only take a few months uh it took about 
a year and a little under a year and about 7  
000 man hours in total we had to estimate it um 
it's like 13 go author paper but a year after  
release uh petting zoo has become the third most 
installed library and pi pi after gym which is  
the first most in sub library on pi pi um there's 
some like 20 total third-party environments with  
more in the works for with petting zoo every 
major mara learning library uh but one um  
uses uh it supports the pending zoo api the one 
that doesn't is currently working on it um and  
so it's the closest to jim from all 
the agents are all that we have now  
and so this was about a year ago but then moving 
back to gym so what happened to jim is that  
um like i said gem is most used rl library in the 
world there's about 30 million installs i'm sure  
everyone here has heard of or uses it but jim 
is starting over around two years ago a variety  
of reasons opening up sort of to progressively 
not maintain gem anymore like the pr to ad sport  
platform 3.9 took 10 months to merge which is a 
problem for those maintain our library on earth  
and i need a gym to keep working for petting 
zoo and so for a long time in this period  
um most maintenance of jim was actually you know 
opening up merging prs in my specific request  
and eventually just they just kind of gave in 
and made me the maintainer um albeit i'm not  
paid employed by or affiliated with opening 
at this time with this in mind um jim is  
the uh most um it has 30 ml installs it has 
two-year pr backlog it has it had 50 65 overpr  
so i got it down to about 15 including some 
new ones that i brought that we processed  
there are 250 issues now down to only 150 because 
openai stopped dealing with them uh there was no  
work in ci but that should be fixed now there's 
no documentation about cheaply working on that  
there's no code style checks or anything like 
that that's all fixed now it's like a complaint  
so they're all more of a story in terms of jim 
and petting zoo because i'm maintaining as one  
person the first and third most used rl libraries 
in the world by pipeline styles at least is help
and so there's some specific 
things we could use help with um  
so the first thing is that people can 
do any level of experience is that for  
the website that we're doing we're uh oh shoot 
someone already took this i forgot to uh sorry
people have been visiting me a lot of emails  
um and so the first thing that we could use uh for 
any level of expertise is that gym doll rappers  
the the wrappers for pre-crossing environments is 
a mess and we need to do a bunch of things uh if  
you'd like to you know write code or view code or 
do all sorts of things like that um then please  
coordinate that on the github issue there's a 
lot of things to do we need to add a host of  
new rappers um we need to do mains the biggest two 
ones there's a list you guys can take a look at um
what is sorry um for more experienced 
contributors on the opening igm github  
there's a bunch of issues with the help needed 
tag what the help need attack means is that  
somebody needs to figure out what to do with 
this you know is this just a question you can  
answer and i can close you ping me does this like 
bug exist if so if the bug exists we need to like  
do something with it and so on um there's also um  
issues tagged with a pr needed request you can 
of course create prs for the issues with the pr  
meta tag but the biggest thing if people just 
want to take a giant check of things is to go  
and process that it's about 75 issues with a 
needs help tag um then um then everyone asks  
gym documentation evacuation okay well a bunch 
of people are working on gym documentation um  
if you guys want to contribute to gym 
documentation um then one thing that you guys um  
then it's currently fully staffed if people want 
to volunteer to replace someone if anyone working  
on it you know flakes or doesn't work out or 
something um that's an option i please reach  
out to me another thing is that the initial 
pr isn't going to have amazing mobile support  
probably so if you want to do prs to improve the 
mobile website support once it's released that'd  
be great the other thing is once it's released 
uh after at least we want to do stuff where we  
automatically build parts of documentation from 
code uh and that won't be on release and so if  
people want to contribute to that again for all 
these things if you want to work with these email  
me to coordinate um and the other thing that if 
anyone has this background for this would be super  
super helpful to jim is the pie box 2d situation 
so to explain the pi box 2d situation um let me um
let me show you something so so the 
problem is this all right um in pi box 2d  
is uh is how all the box 2d environments work so 
box 2d is essentially uh it is the c library 2d  
physics engine so just to briefly explain what 
you're getting into for the pi box 3d problem  
box 2d is the c library that is how uh that's 
what angry birds use and all these things  
um the main python um libra bindings for 
the box 2d library called pi box 3d box  
pybox 2d is heavily used on gym for all the 
box duty environments like the lunar lander  
and all the things that are very have leaves by 
everyone problem is that well there's two problems  
the first problem is that gym depends on a fork of 
a fork both of which are unmaintained at pi box 2d  
um and the second problem is that pi box d 
the upstream thing that jim should be depend  
should depend on is completely unmaintained and 
having a library 30 ml and downloads depending  
on that is a problem so if anyone here knows c 
like same libraries and python and it's any as  
any interstall i mean hitting pi box duty please 
email me i've been in contact with the maintainer  
you can take over it it'd instantly be a very 
very large and heavily used uh installed thing  
and so then the final and so these are all things 
that we're looking forward jim uh miscellaneous  
work documentation adding to gym about rappers 
people dealing with all sorts of issues and the  
maintenance of high box duty like 12 different 
people are working on stuff so it's fairly well  
staffed um then on the petting zoo end if people 
want to do multi-agent rl there's a couple things  
that people can contribute to one thing is that 
rls major are a library for those of you who  
don't know um i have a half a two-thirds complete 
petting zoo tutorial for rlip that i don't have  
the capacity to finish but that would be fairly 
useful to a lot of people if it was finished um if  
you guys if anyone wants to take that please email 
me you need about um it'd be probably like 40  
hours of work you need immediate knowledge of rl 
you need to be able to speak english for reference  
um the last article i wrote like this was a the 
top article on data on towards data science it was  
that of your choice and it was viewed like tens 
of thousands of times so if having an article  
like you're being co-authored on article like 
that is professionally useful to you and you  
want to take over the petting zoo article that i 
don't have the bandwidth for but they can be very  
useful please email me um another thing people 
want to contribute on the pending zoom is that  
there's two notable um third-party environments 
that that still don't have the petting zoo  
api support there's a social sequential dilma 
games and there's a sumo rl environment um  
uh they would take a weekend or two both uh to 
add support for um and you can just if you want  
to email me you can we can also just create pr's 
for those these are the only two meaningful uh rl  
environments still don't have pending api support 
and the last thing that we're looking for is that  
we've been building a sort of cable rl online 
service um uh based around peninsula coliseum  
um and it um and we're looking for all sorts 
of backend uh help in addition to the various  
frontend developers starting in maybe a month 
um it a variety reason i don't want to discuss  
what this is involved publicly um but this would 
be a somewhat long-term commitment for maybe at  
least 10 hours a week if anyone is interested in 
working on a large project that the maintainer  
of gym and petting zoo thinks could be uh it is 
sort of an open source version of kaggle for rl  
please email me um and so this is my talk these 
are all the different um things that we're  
trying to help for right now uh roseanne 
told me that i'll be taking questions
after my talk
that's super cool justin i i didn't uh i wasn't 
aware of that history and the fact that you  
are now single-handedly maintaining this thing 
that's pretty awesome yeah um one good question  
you showed two emails they're the same right one 
with cs dot umd one without just just sure which  
one to send you the without is the technically 
correct one but the cs1 forwards to the umd one  
okay you probably want to correct yourself 
yeah i just fixed that yeah um second question  
so if people start contributing to say gym and 
doing like a lot of pr and you merge a lot all  
right yeah do you have the right to make them a 
contributor or is that very still organizing only
yeah uh one question is there anything 
that um besides individuals volunteering  
is there anything that mlc can do as 
an organization to kind of help you um
all this work is unfunded and unpaid so i don't 
know if so that's one end uh what uh what are  
you thinking of uh what are you doing what i was 
sadly not thinking of the funding angle because  
we don't have funding yet um we're still waiting 
for our 501c3 to come through before we can take  
um a nice volume of donations i was thinking 
like some some communication channels maybe  
um something on discord if you'd like on the 
open cloud discord um uh i mean there's uh  
media that you used to coordinate well 
so i've been using a coordinate has  
aside from github has been the uh there's 
a very very large rl discord that most of  
people who've worked in rl here i assume 
have are familiar with we just go out of  
that there's a private gym maintainers 
channel okay nice sounds like that works
yeah well so the it's a private channel for 
people who like are actively involved in  
the maintenance already okay so people 
should start by emailing you then yeah  
yeah i mean people can on most people a lot of 
people do my screen discord but email's easier  
uh any other questions or anything from
anyone okay well thank you guys so much
awesome um yeah thanks for presenting those that 
was awesome um next up is charles charles martin  
with an update on weight watchers which we've 
heard from we've heard about two and one  
first one has come three times great thank you 
let me i should share my screen yep go for it  
okay so we've uh we can see this okay 
so let me give you an update on um  
on where we are we've actually made quite a lot 
of progress uh so far uh weight watcher is a a  
diagnostic tool it's an open source diagnostic 
tool for analyzing deep neural networks and this  
is just something i've been working on in my spare 
time in the past probably the past five years or  
so in collaboration with michael mahoney who is 
uh that use was at stanford now at uc berkeley  
uh it's an open source tool probably has the 
least amount of downloads compared to the last  
tool we've seen but you know we have a few uh the 
goal here is to try to get people to use it it's  
meant to be a practical tool to help you diagnose 
problems and to help you fine-tune your neural  
network so you just say pip install weight watch 
it's very early release i think it's not even a  
trying to work on a 0.5 release of the tool um so 
i'll give you an update of sort of what this is um  
so weight watcher is an open source tool and again 
it's this it's designed to let you analyze deep  
neural networks and you can do this even if you 
don't have access to training or test data so for  
example if you are working with some network and 
you're trying to fine-tune it or you're working in  
a company and somebody hands off a neural network 
to you and you don't know how it was trained  
weight watcher acts as a diagnostic tool for 
you but you can if you have access to training  
data we can do more things but it allows you to 
do things even if you don't have access to data  
so it allows you to analyze pre-trained pi torch 
keras and now o in and x models we've added some  
basic o and x support i don't have that 
here it allows you to inspect models  
that are difficult to train and look at them 
sort of layer by layer to figure out which  
layers are not being trained properly and which 
layers are over trained you can see this layer  
by layer i don't believe there's any other way 
to see that kind of behavior it allows you to  
predict improvements in model performance and 
predict test accuracies across different models  
and to detect problems and it's based on the 
theoretical research i've been doing so from  
my own and joined with uc berkeley into why deep 
learning works and it uses ideas which are really  
from the theoretical physics and quantitative 
finance community ideas from random matrix theory  
from statistical mechanics and from the theory 
of strongly correlated systems where i did my phd  
you might see people talking about random 
matrix theory at icml or other conferences but  
what we're doing here is is much more subtle 
probably not well known outside of the theoretical  
physics community so i'm trying to introduce these 
techniques in a way that we can build a tool that  
people can use so the tool is meant uh the idea is 
that we analyze this the eigenvalue of the layers  
so we go layer by layer and we imagine you were 
to compute the singular values or equivalently the  
the eigenvalues of the correlation matrix of each 
layer and you were to put them on a loglog plot  
you would have sort of a plot that looks like 
this and weight watcher will generate these  
plots for you and the idea is we want to analyze 
both the shape and the scale of the correlations  
in the weight matrices because i mean the whole 
point of a neural network is you're trying to  
learn the correlations in the data so if you can 
understand what's happening in the correlation  
structure of your model you can make all sorts of 
predictions and you can diagnose problems so what  
the weight watcher does is a very simple tool it 
simply extracts out plots and fits what's called  
the empirical spectral density which is just a 
histogram of the eigenvalues that's all it is  
but you know the technical term is the esd um and 
it plots it for each layer or various slices and  
it tries to analyze the tail of the esd because 
this is where the most informative components are  
and we're going to show you that the tail contains 
tremendously useful information now typically in  
if you study like machine learning theory and 
we'll show you this that mostly what you look  
at machine learning theory is the scale meaning 
the sort of orange line how large of the weight  
matrix how much should you regularize things 
like this and we're going to show you that  
that just not only is that not enough but if you 
look at the scale in any it actually gives you  
information which is counter-intuitive so let 
me give you an example so how the tool works  
it's very simple you just import the tool import 
weight watchers ww you build a you construct a  
washer giving your your pre-trained model you 
say analyze and the analyzes very options such  
as plot equals true or you can get a summary the 
analyze method will give you a bunch of plots  
and it also gives you a details data frame and 
the data frame will contain various things and  
for each layer will contain this alpha metric 
which is our power law metric that we've invented  
we've the alpha weighted metric which we're going 
to discuss it will also give you the spectral norm  
or the maximum eigenvalue and a number of other 
metrics which uh i could sort of describe it  
in the documentation or you can join our slack 
channel and it will also give you summary  
statistics which are basically layer averages 
so the idea is you can use the tool to do  
layer by layer analysis of a neural network 
or to get summary statistics on your models  
so you know if you're training a 
model and you find that they're  
certain and you're concerned about you 
know you're not getting good performance  
you can go in and look layer by layer 
and see which layers are over trained  
which layers are over parameterized and which ones 
are working correctly and you can tweak parameters  
and monitor it this is basically the idea so let 
me give you an example of sort of how this works  
this is um this alpha metric the idea is we take 
the esd and we fit it to a truncated power law  
which means you take the eigenvalues and you fit 
them basically to lay up into the minus alpha so  
if you were to plot that at a log log scale the 
alpha is simply the slope of this red line that's  
all it is the slope of this line but you have to 
use a good statistical estimator to get it and the  
interesting thing is the better your models are 
the better trained they are the better this works  
and so here's an example of where you where we 
vary we've looked at this contest there was a  
contest that nips nurips uh last year 
and so during the pandemic i had some  
time to look at some of their data they 
released a bunch of pre-trained models  
in this case these are a bunch of models 
which are which vary by the number of layers  
so um the the label doesn't necessarily mean 
the number of layers the the models at the  
top this water has more layers and this model 
at the bottom of six has less layers but this  
is sort of the notation they gave us and what you 
find is that they varied you know batch size and  
the weight decay and the momentum and they just 
give us these pre-trained models and i just run  
the weight watcher through it and what you find 
is that the weight watcher parallel metric is  
very well correlated with the with the predicted 
test accuracy and we see this depe as long as you  
fix the depth if you fix the depth of the model 
and you vary your regularization parameters  
alpha actually is quite predictive and in fact 
as you see you see that the the blue line has a  
much smaller variance than the green line well 
the green model 6x actually has much lower test  
accuracy and the blue line has higher and so not 
only is alpha more predictive but the better of  
a power wall fit you get the better your you 
better you know your model will do and we and  
one of mike's postdocs has done an extensive study 
of this has really shown this really slush this  
out and so what's interesting though if you look 
across the depth you find well alpha predicts the  
hyper parameter changes quite well and remember 
we don't have access to any of the data we're  
just looking at the model itself but you know if 
you go but if you have to do it for a single depth  
now typically in machine learning theory what 
you look at is the scale or the spectral norm  
so if you look at like various people they 
do big pac-bounds theory or things like this  
they're looking at the spectral norm and one of 
the things we've shown recently is that if you  
try to look at the spectral norm well the spectral 
norm is also kind of correlated it's obviously not  
as good like you look at this two you know alpha 
works really really well and the spectral norm  
it's kind of all over the place but more 
importantly the spectral norm has the opposite  
behavior that you would expect it turns out if you 
vary the hyper parameters the spectral norm gets  
as the spectronorm gets larger the test accuracy 
goes up but if you vary the depth as the depth  
gets smaller the you know the test the spectrum 
norm goes up this is a classic simpsons paradox  
so if you try to make a single metric to 
predict test accuracy and there's a bunch  
of papers now and contests where they're trying 
to say let's let's have a single metric that can  
produce tech tax predict predict test accuracy 
you got to be careful because if you just use  
what's used in standard machine ordering theory 
you see a simpsons paradox so the spectral norm  
work will actually and you see the opposite 
results in fact if you think about spectral  
norm regularization when you do spectral norm 
regularization things are supposed to get better  
but if you look empirically that's not 
what's going on so so it turns out that  
the spectral norm and what's typically done in 
pac-bounds learning theory doesn't seem to work  
correctly and we're going to explain why this 
happens but in particular i want to point out  
we have in the weight watcher something called a 
multi-purpose metric and so this metric alpha hat  
allows you to predict the test accuracy for models 
with the same architecture series if you vary both  
the depth and the regularization hyper parameters 
and again without access to tester training data  
so here's the same data from that contest and 
you see if we lose our alpha hat metric you get  
very good results you know you get you you see 
that the alpha hat the smaller alpha hat is  
the better the test accuracy and you see that 
you get the same behavior the same direction  
even if you change the depth you don't have this 
weird behavior where you know the spec you have  
this weird maybe you see the sort of simpson's 
paradox everything's moving in the right direction  
so the alpha hat is simply a weighted average of 
alpha or you can think of it as a weighted average  
of the spectral norm so it combines shape and 
scale metrics so alpha hat can actually be derived  
from statistical mechanics i i did my postdoc in 
the statistical mechanics of neural networks back  
in the 90s it was really you know theoretical 
physics was greatly interested in this stuff  
and i have sort of a blog post that describes this 
theory and we're working on getting this written  
up and it's very very technical and involves 
techniques from random matrix theory and quantum  
field theory which um are maybe unfamiliar to 
people in the machine learning community so we're  
slowly writing this up but you can go to my blog 
i lay out most of the significant details here  
hey charles is this uh is this alpha framing uh 
different than the last time you presented this  
the same alpha but the result okay okay so 
there's alpha which is just the power law exponent  
and that works for hyper parameters and there's 
alpha hat which is the the the weighted average  
and and what's new is this this representing 
of the simpsons plots these this idea that  
why the spectral nerve doesn't work so that's 
something new that we've shown the spectral norm  
we should try to wrap up if you if we can in maybe 
maybe 30 seconds or so that's fine so this has  
been again if you want to use the tool you can use 
the tool to do this we've recently published in  
nature communications so we've studied hundreds of 
these models we have a paper right now in nature  
communications which just came out uh which 
describes how to use the tool when a variety  
of of um you know you can dig into the paper it 
shows various kinds of things you can do with  
the tool but basically the idea is by using this 
weighted alpha you you can get predictions of both  
the test accuracy when varying both depth and 
regularization i have other stuff to talk about  
but you know we're going to run a tight schedule 
so i'll leave it there and it just invites you  
to come download the tool pip install weight 
watcher um join our slack channel it can describe  
all sorts of other things we've been doing in 
terms of understanding the difference between  
pac-bounce theory and statistical mechanics which 
we can go into in great more depth if you'd like  
perfect amazing um seems like a pretty powerful 
tool um for time we should uh defer questions to  
um discord after so after this meeting we will all 
head over to discord and we'll have separate voice  
channels for each each presenter and you're 
welcome to ask any uh more in-depth questions  
there um next up we have mishka sorry if you're 
saying that correctly yeah that's fine i'm trying  
i'll try to share a screen one second perfect 
and i've done this before but i guess it kind  
of works well um so if we're getting close to the 
time i'll draw like a little uh 30 seconds here  
like that on the screen okay and then you people 
will know to kind of like try to start to wrap up  
okay i'm timing five minutes for everyone 
except for a couple that um pre-arranged to  
have more time so we fit them in the schedule 
right all right sorry take it away though  
okay so this is a request for plot i would like 
to see if uh if if this change if this change  
in translate in transformer attention would be 
fruitful just with your favorite learning curve  
and proof if you add this cross normalization 
let me first explain why i think it is good  
it is light is a good shot because and then 
i'll talk about why more about what it is  
i did an experiment i was interpreting monochrome 
images as matrices and multiplying them via matrix  
product just to trying to understand matrix 
product better and there's this poster with  
materials and yeah there are plenty of information 
you can see but if you soft max rows of the left  
matrix and columns of the right matrix then the 
result is much more informative and interesting  
but you need to do both not just on the left 
but also on the right and here is another pair  
and we did more experiments and in general my 
feeling is that if you do this soft max on the  
left and cross soft marks on the right interesting 
information goes through better let me show you  
more in more resolution like this like 
this see it's even has a 3d illusion  
just on its own okay so what do i want what is 
this soft max cross normalization of values this  
is the values matrix the values themselves are 
horizontal stripes the rows and you want to take  
vertical vectors vertical vector which consists 
of for every given e it consists of e coordinate  
and you want to soft max that now one more thing 
i want to add is that one can remix it in various  
ways with classical transformers for example 
one can consider this as a inside the formula  
and the original i was just thinking 
about training transformers from scratch  
but if you do it this way then you can 
start with maybe a pre-trained transformer  
and start with alpha zero and gradually increase 
alpha while fight tuning and this way you might  
be able to do this experiment with less cost 
and to contact me you can open an issue in this  
repository or you can send me an email to my 
university address which university kindly  
lets me use even though i graduated 
20 years ago thanks for your attention
interesting idea since we have a 
large audience here has anyone tried  
tried this or thought of trying 
this or try something similar  
i suppose people have mocked with the 
internals of transformers in many ways so
yeah good question also does 
anyone have an intuition  
for uh how much extra computation this would take
i know some soft max operations internally 
can be expensive if the dimension is large  
all right right yeah and maybe there are some 
ways to optimize that yeah that's okay yeah sure
any other questions
if not um feel free to talk 
to mishka in the discord after  
and next up we'll have arijit das i'll 
share my screen in a second um sure
you'll be able to see the screen can you 
see the screen right now yep awesome so
alrighty
so um hello everyone and uh thanks to 
m collective group for giving me the  
opportunity of sharing my project on how i build 
a pneumonia detector in less than hundred dollars  
so um first of basics about uh why pneumonia 
is a troublesome so pneumonia is a super common  
disease in the world today like uh the number of 
cases in india itself is to be about 10 million  
per year uh who sees that every year about 450 
million people get affected by pneumonia which  
results in about 4 million deaths in an average 
global the people who are highly affected by this  
are mostly the children less than 5 years old and 
the senior citizens who are above 60 years of age
so we can say that pneumonia detection can be 
out of reach for many how so far the detection  
of pneumonia is achieved using x-rays uh just 
scans or extracting blood serum in some of the  
severe cases but this requires time resources 
skilled personnel and it is quite expensive
so
i've built a pneumonia detector which can 
harness low cost hardware and software and then  
it can deliver effective detection for people like 
anywhere and everywhere regardless of the location  
skill or even affordability with the help of azure 
first base softwares and their hardware partners  
i can even show you how to effectively detect an 
alert of potential pneumonia the entire hardwood  
kit would cost you around 100 us dollars and 
can be used absolutely anywhere and everywhere  
our project actually enables a person to take 
and chest x-ray of the pc and use the hardware  
which lays out results your computer or even a 
mobile but no technical expert as required at all
so let me just quickly share 
my screen for a quick demo  
because as per the as for the picture 
series manual testing is important as well
so can you see my screen yeah yeah yeah yes yes  
so this is the camera that i'm having 
uh this small little hi-max v1 depth  
and you can see that the camera has been connected 
to the examples studio dashboard using web usb  
which is which then connects on to using 
the javascript based applications of the  
browser so it has connected and now what it is 
doing is that it is taking up the live camera  
feed the live imagery feed which is coming from 
the uh camera and now if i click on start sampling  
it will just then take up the picture and it 
will then wait for a couple of uh seconds for it  
to take your picture extract it and then want to 
running the ml model so for an instance over here  
i have already taken up the picture beforehand and 
this was the picture that we were having of the  
plasticity and in from them we can get to see that 
it sees it is to be 96 percent bacterial pneumonia  
and around 0.4 percent of viral pneumonia so 
this will actually help the doctors to first get  
to understand that which pneumonia is 
the patient being affected with and it  
can also help like for a faster amount 
of treatment in poorer countries so um  
you can even head on to the model testing page 
wherein you will be able to get to see the  
feature explorer graph which is actually a graph 
like this and the green ones that you see are  
the ones which are a normal case 
of the chest x-ray which means that  
the person is not affected with pneumonia and 
gradually as the color even increases uh the  
red one says for bacterial pneumonia and in 
the middle you can even get to see some yellow  
images and some orange images which are mostly 
for uh various different types of viral pneumonia
again um
all right so uh here you can uh get to 
use the resources that i've used in which  
there is a first blog in which i have mentioned 
everything about the on how i built the project  
plus uh the github repository and 
the slide deck even questions anyone
it's a really cool demo um i have some in-depth 
questions i could ask but i think we're at five  
minutes so i will try to find you on discord after 
and ask you there but awesome awesome uh awesome  
work and presentation so that was it uh thank 
you so much for watching our presentation  
well thanks uh next up we have joaquin  
cabezas um yes the the one the one who's 
gonna be presenting it's not myself it's  
going to be i'm going to try to pronounce 
it properly it's going to be piotr pierre
my colleague okay okay perfect believe  
you guys have 10 10 minutes right i think 
15 but we'll make it very fast lightning
all right so can you see my screen 
in the presentation yep all right  
so hello everyone my name is piotr and 
i would like to tell you something today  
about our journey through the kdd cup the open 
graph benchmark large scale challenge which we  
participated in and i will start with the team so 
everything started just at the beginning of april  
where a guy named edward just posted 
a message in the mlc discord server  
asking if anyone wants to collaborate on this 
challenge and where we are after a few months  
we've got already a team it seems like a group 
of seemingly random people but what connects us  
is that we all work in graph machine learning and 
we all met for the ml collective research jams or  
on the discord server and we are edward joaquin 
javier myself camille calvin benedict and aryan  
where are we from yeah we are quite 
distributed i would say over the world  
we've got people from canada from spain from 
the uk from poland and also from nigeria
and let's talk about now a bit about the challenge 
so it was hosted the kdd this year it was the  
large scale challenge prepared by the open graph 
benchmark force and there are three tasks so  
basically processing large amounts of graphs on 
the node level and the link level on the graph  
level and we chose the last option uh why do we 
even care about that problem first of all all  
molecules can be represented as graphs so you have 
a molecule you decompose it to atoms and atomic  
bounds the atoms are nodes and the bonds are just 
edges in the graph next you can use some pretty  
well known graph embedding or graph representation 
learning techniques to obtain a whole negative
um i just needed them okay um and 
having those embedding vectors  
you can then predict some molecular property so 
this is why this challenge has a chance to work  
and this challenge was about predicting the 
so-called homolumo gap of particular molecules  
we didn't actually know what that is but 
thankfully kelvin has some background in chemistry  
and helped us out there he prepared a presentation 
and explained what the summer lumu gap is  
to put it simply if you have a molecule or 
just even an atom you have different orbitals  
so energy levels where electrons can be and you 
care about the highest occupied one so the homo  
and also the lowest unoccupied orbital which is 
the lumo so that's the difference between those  
two levels some aluminum gap and traditionally 
what you do is you compute this value by some  
really expensive quantum calculations which 
can take up the hours for a single molecule  
but the claim here is that you can also use gnns 
or other machine learning models to predict it  
in an orders of magnitude faster like in this 
challenge for a single molecule less than a second  
and in total we had 4 million graphs and about 
55 million edges so it's quite a large scale  
data set and what did we try out first 
of all we contacted emma collective and  
wrote the one pager describing what we want 
to do and asking for some compute resources  
we also explained who will be 
participating and what we want to achieve  
and then after receiving some computational 
resources we started with examining the  
baselines that were provided by the authors 
of the challenge and as we can see they tried  
simple models that ignore the fact that we are 
dealing with graphs so just mlps and the basic uh  
and most popular models in graph representation 
learning so gcn's graph conversion on networks  
and graph isomorphism networks which turned 
out to be the best one in the baseline sense  
uh so we thought maybe let's reproduce these 
experiments with these graphics amorphous networks  
but modified as the authors did with a 
so-called virtual node which is just connected  
to all other nodes helping with the message 
passing stuff that happens in this network  
and then we wanted to build up on this model so 
thinking about the success of pooling operators  
and cnns we wanted to apply some cars in 
the graph and we used difficult for that
next we also thought maybe we could somehow 
model the uncertainty in the network  
so we wanted to check out bayesian neural 
networks especially biased by backdrop where  
each weight in the graph neural network is not a 
single value but the whole distribution of values  
completely oversimplifying you just 
add a kl divergence loss to the  
overall loss function and everything 
works but that's oversimplified  
in terms of other models we also tried graph 
attention in the form of 85 fp which the authors  
claim that there are some interpretable embeddings 
afterwards uh and also this model was used for  
molecular representations it didn't really turn 
out to be that groundbreaking model we also  
investigated principal neighborhood aggregation 
we used different aggregation operators to  
aggregate embeddings also there wasn't really a 
success there then we looked into unsupervised  
representation learning with graph auto encoders 
and putting the embeddings into downstream tasks  
also not a great success there graph units 
also has a different kind of encoder type  
um the problem was here that all implementation 
just ignore the edge features so this supervised  
setting was quite close to the baselines but 
still worse and nothing really helped there  
and then we found a quite interesting paper 
which tries to build motifs and devices and some  
graph sub structures tries to encode 
them and put it in the final embedding  
it was purely unsupervised so it 
was really perfect in our case  
but there was no implementation provided just 
in the paper some by torchish code snippets and  
after trying to implement it from these snippets 
it turned out that it is impossible there are  
major things missing in the paper so that didn't 
work out for us but the idea is still quite nice  
we also wanted to explore graph transformers they 
are like beating all the saltas right now but
we didn't really have that 
much of computational resources  
and you require really large graph transformers 
to make it really work the basic implementations  
of the smallest models didn't work at all 
so maybe in future we will also explore that
then after trying different models we decided to 
enhance the feature sets because you could also  
generate additional features using the so-called 
rdk library and we did it at volvo graph level  
uh passing it for an mlp but also trying to 
get additional attend features uh yet there  
are no there was no significant improvement 
on the validation data set that we've got  
then we thought maybe an assembling would help so  
we used both pre-trained and trained from scratch 
models like in the picture you see right now so  
this gin approach this difficult approach and 
this mlp on the scrap level features extracted  
the embeddings combined them and then built 
a common model that predicted the hormone gap  
but whatever we tried it just hugely 
overfitted so that was a light end  
we also thought maybe non non-graph neural 
network models would help like for example  
gradient boosting trees exegg boost but this 
resulted just for generalization we also wanted  
to compare somehow using our expert knowledge that 
we've got in form of kerwin which is a chemistry  
background to compare the embedding so they encode 
some substructures like functional groups in  
molecules but there was no really a clear result 
from this exercise but we keep trying with that  
so wrapping up to the outcomes what do we got our 
final solution is an ensemble but an ensemble on  
the predicted scores so we used the baseline 
gene virtual in free retrainings then this  
gin with an additional div tool layer also free 
models and one with the bayesian neural network  
all were previously pre-trained on the downstream 
tasks and then we just averaged the predictions  
and how it turned out it actually turns out that 
from above around 0.14 we can move down to 0.13  
using all assembling also we observe 
that there is some correlation between  
the uncertainty of the ensemble and the 
final error that we have that we get
finally in the whole competition we are 21st 
out of 49 with a score of 13.5 compared to 0.12  
and that's somehow okayish but regarding that 
we didn't knew each other beforehand and we  
just achieved it in two months it's quite well 
we also write wrote a paper on our solution and  
submitted it to the game workshop at dcml which 
reviews should be out in a few days i think  
and our preprint is also available in 
archive if you want to check it out  
so the key takeaways you should definitely use 
ml collective to get some collaboration expanding  
your network is always nice and trying to 
collaborate with different people that you didn't  
even don't even know it's also fun to do and we 
should always strive for this open collaboration
with our future plans what we want to do next 
first of all we want to further collaborate  
tackle other challenges we also write about to 
start a reading group from graceman and we also  
propose to create a common github report for 
the emo collective stuff that is happening  
and that will be everything so 
thank you for your attention
amazing amazing talk that's 
uh some pretty cool work  
i enjoyed seeing that project evolve the 
last last period of time it was pretty cool  
to see you guys kind of come together 
and accomplish something pretty big  
um for time we should probably defer questions 
to discord as as before but thanks for presenting  
uh pyotr um next up we have anayan uh nyan 
also has uh 10 minutes by prior arrangement  
um 15 but yeah we'll try to manage in uh 10. 
okay okay sorry the 10 versus 15 i would say  
yeah okay not involved in that part of the process 
all right um yeah just give me one second um
all right so hi everyone um i'm diane um and 
today we'll be presenting our work titled uh  
poisoning the search space in neural architecture 
search um this project is by myself and two other  
undergraduate students uh rohan jain and robert wu 
who will also be presenting with me today we also  
recently last week presented uh the same project 
at icml at the adversarial learning workshop um  
so the presentation is roughly organized as 
follows i'll provide the necessary background  
rohan will be presenting the methods and 
experiments and robert will talk about some future  
directions that we are currently pursuing so let's 
uh take a look at some preliminaries i guess um  
so uh starting off with neural architecture search 
so deep learning in general is uh something which  
is very highly effective and at the heart of this 
performance lies a neural architecture design  
which relies heavily on domain knowledge and 
prior experience on the researchers behalf more  
recently this process of finding the most optimal 
architecture was automated by neural architecture  
search algorithms so the the way these algorithms 
work is basically they continually sample  
operations from predefined search space uh to 
construct architectures that best optimize a  
performance metric over time you can also see this 
in figure one uh happening so this is reducing  
human intervention prevention to a great level 
because you just need to decide the predefined  
search space and the algorithm does all of the 
work for you networks derived from this procedure  
have shown uh state of the art performance and 
on a lot of tasks like semantic segmentation  
classification and detection and are also being 
used in real world settings like healthcare  
so for our project we focused on a popular nas 
algorithm known as efficient neural architecture  
search um it was recently developed by researchers 
at google brain um in 2018 i think so not so  
recent but uh basically the way this algorithm 
works is uh it does it has a search space which  
is represented as a directed acyclic graph which 
you can also see on the screen and every node in  
this graph represents one operation uh in the in 
the neural architecture design now the original  
search space denoted by s hat which you can also 
see um on the screen was very carefully chosen by  
by the original creators of enas which will later 
on play a very crucial role in our project um  
also the search strategy is basically a predefined 
lstm that samples operations from the search space  
through softmac classifiers and creates child 
networks finally these networks are then trained  
over the shared parameters with the goal of 
maximizing expected reward which is simply uh  
validation accuracy now one key thing to note 
here is that since this algorithm depends  
so much on the search space poor search based 
selection due to human error or by an adversary  
has the potential to severely impact the training 
dynamics of any algorithm nas algorithm in general  
next we look at training data poisoning 
which has traditionally been a very  
popular adversarial attack against 
machine learning algorithms  
so the idea is very simple basically you have 
a training set t and you add an extraneous  
data point to it which will lead to high 
prediction errors across training and validation  
sets while also impacting the loss minimization 
now while this is very effective and it has been  
shown to work against a lot of traditional machine 
learning techniques this is also a data dependent  
attack and a more relaxed assumption would 
be to make such an attack data agnostic  
uh which is very well possible in the case of 
enas and nas algorithms in general we explored  
this idea through a new poisoning technique that 
will now be explained um in the next section hi  
so uh i will begin by introducing our primary 
technique known as search space poisoning  
in short we'll just denote it as ssp so the main 
idea behind search space poisoning is to inject  
precisely divine uh defined uh highly ineffective 
operations into the original enas search space  
in effort to maximize the frequency of poor 
architectures being generated during the training  
procedure so then from the attacker's behalf this 
requires no prior knowledge of the problem domain  
or the training data set being used and making 
this attack much more favorable than the  
traditional poisoning attacks nyan mentioned and 
formally as you can see on the screen we define  
a poison search space as s where s hat denotes 
the original enas search space operation and p  
denotes a non-empty set of poisonings where each 
element from that set is an ineffective operation
so we also introduce an add-on known as multiple 
instance poisoning attacks alongside ssp so  
basically a single instance poisoning attack might 
not be as effective due to the tendency of the  
enas controller to draw fewer child networks with 
the same single ineffective operation over time  
so this behavior of enas results in the algorithm 
almost entirely discarding those networks with the  
single poisoned operations so basically it will 
get used to that and it'll afterwards they'll  
prevent it'll try to avoid that so to prevent this 
issue um we introduce multiple instance poisoning  
which basically increases the likelihood of the 
controller sampling an ineffective operation  
which we call o sub p from the poison uh 
poison search space and this is achieved  
by increasing the number of times an ineffective 
operation is repeated in p so under this framework  
we derive the result as shown on the screen 
basically showing that through this procedure we  
increase the probability of bad operations being 
sampled over the which is basically greater than  
the probability of sampling an effective 
operation from the original search space
next we move on to crafting these poisoning sets 
so we'll look at forming these poisoning sets with  
the operation so the simplest way to attack the 
functionality of enas is to inject non-operations  
within the original search space and we fulfilled 
this goal by using the identity operation  
now a more involved technique is this our 
second poisoning set where we use transposed  
convolutions to see if they can approximately 
counter the effect of the various convolutions  
from the original enastered space furthermore in 
our third poisoning set we studied the effect of  
including layers with high dropout probability 
such as p equals 0.9 and this is because dropout  
randomly zeros out some values from the input to 
decorate neurons during the training resulting  
in severe contamination of the search space 
and lastly our fourth poison search space we  
decided to combine the previous sets and see 
the effect of including multiple instances  
on the generation of these child models next 
we went to our experiments so here is our  
experimental setup it's a table summarizing 
the experimental search spaces so in the  
first row uh this is the baseline original e-nas 
search space with no extra poisoning operations  
so each poisoning set as you see in the first 
column has four sub experiments with it four sub  
experiments within it shown in the third column 
so if you consider the second row which is the  
identity our first poison search space it has four 
sub experiments titled one a to one d each with a  
different instance factor so if you consider six 
p one that basically means there's a total of six  
identities within that poisoning set so as you 
go down that list we increase the number for each  
sub experiment and the same idea works for each 
of the other poison search spaces like p2 p3 p4
um now these uh for each of these poison search 
spaces as shown from the previous uh table uh the  
following graphs represent the validation error 
as training progresses so as seen in graph a  
multiple instance operations increased the error 
considerably and the most effective results were  
seen when 120 and 300 identity operations were 
included as shown in space 1c and 1b and this  
resulted in pretty high errors approximately 
around 70 in the space 1d and overall the most  
effective poisonings from our entire study was 
the dropout in the grouped operations so in graph  
c uh we saw that adding multiple dropout layers 
like in space 3d for example resulted in pretty  
high error around 80 percent and same with grouped 
it was pretty consistent with our other findings  
um this is overall performance so 
these graphs demonstrate the final  
validation and test error as a function of 
multiple instance operations so in graph c  
the experiments are getting worse in air with 
experiment 3d hitting around the low to mid 80  
validation and test error in graph d 
spaces 4c and 4d were very close in  
val and test errors so around the q equals 20 
point as you can see we have hit this poisoning  
saturation point it's denoted by those flat 
lines so the conclusion we drew is that grouping  
a variety of poisonings is more efficient in 
attacking enas than duplicates of an operation  
now um overall just to summarize like our results 
show how the controller's dependence on parameter  
sharing and weight sharing lead to inaccurate 
predictions um even though it is efficient  
enough it compromised the functionality 
when we had injected these poor operations  
and lastly to summarize we proposed and developed 
theory behind a novel diagnostic poisoning  
technique alongside multiple instance attacks 
experimented on the sci-fi 10 data set which  
demonstrates how ssp results in child networks 
with inflated error rates up to approximately  
80 percent now rupert will take up for future 
directions all right just uh checking in jason you  
wrote 30 seconds on the screen uh what does that 
mean uh if you guys could try to wrap up within 30  
seconds that'd be great just because we're we're 
running pretty over time all right all right  
one minute is fine because i know you probably 
have a whole section all right cool okay so um  
yeah i i guess on wrapping up um we can uh talk 
about like what we're working on the future so  
of course we have like uh we're exploring like new 
operations um we're trying to run things multiple  
times and we're coming up with a more solid theory 
uh behind like the stuff we're doing um and also  
we're just taking a closer look at um how the 
controller behaves uh under the how control  
actually behaves um under our poisoning attacks 
so um one of uh the main idea that uh we're gonna  
be working on in the future is essentially like a 
type system for uh neural uh network operations so  
essentially when you construct a network how do 
you um like how do you choose operations how do  
you connect them how you how do you guarantee that 
they're going to be compatible the network will be  
valid and most importantly how can you generate 
a new search space that you can optimally sample  
from with some algorithms like enas so basically 
how this works is you have input and output  
dimensions for each sort of operation right and 
they kind of define how networks are compatible  
are you allowed to make a skip connection from 
one layer to another um of course like these  
operations have other properties themselves 
but you can curry them into a function called  
a dimension function which essentially just maps 
input sizes to output size um and so we basically  
use this to talk about compatibility where uh 
you know we have we're working on this entire uh  
type system on our own but the most important 
takeaways are you have instant equivalent and type  
equivalent operations so instant equivalent just 
means that for this particular input size you can  
interchange these two operations um hopefully 
to get better performance from one or the other  
and the type equivalent means regardless of input 
size you will get the same function therefore you  
can interchange them in any uh situation um so a 
practical application that came up during our work  
was uh dilated convolutions um so convolutions uh 
have uh these very accessible properties that form  
a linear function uh in the dimension function 
um and so we essentially can use this to adjust  
the different values and generate um 
essentially different convolutions  
uh to add to our search space um and so the main 
question is how do you optimally and efficiently  
generate a new search space um so i guess like uh 
in in summary some of the roadblocks we faced were  
like like obviously this algorithm was uh very 
heavy uh and computationally and uh we are some  
we are undergraduate students so like we didn't 
get that much support in the past or resources so  
that that would really help us out um in closing 
like uh we'll we'll basically direct you to our  
github uh repository where we put the 
actual code base and the experiments  
and if you're interested you can also read 
the preprint from our icml paper thank you
well thanks for presenting guys um 
we should defer questions to discord  
uh we have two more talks um and then we'll head 
over to this course so next is uh steven stephen
you're pretty quiet um just so you know oh okay
you can see my screen right yep beautiful
from nigeria and i'm currently my friend 
idea and my skill set is somewhere in the  
section of social engineering and machine 
learning for these days i'm delving it by into  
the energy side of things yeah so this is 
what i did and it's a sign language detector  
and uh uh it's it's it's it's able to detect you 
know um signs in image files or live videos and uh  
generates uh the equivalent meanings you 
know in tests and also you know the realistic  
voice of what you know accompanying the the 
sign language minute and it works in real time  
um for the inspiration i would love to say 
that maybe i i grew up you know being deaf  
or a family member what was there but 
steve you know but uh unfortunately  
i didn't have such an exciting inspiration so i 
was taking this course uh uh this specialization  
of course uh you know i'm trying to learn more 
about the building blocks of of tensorflow yeah so  
i think i was going some advanced computer vision 
uh i think it was part of the course instead so  
that was when i stumbled upon these tweets on on 
twitter you know and of course everybody loves a  
good um a good advent online rush yeah so yeah 
so i joined the competition yeah so this is my  
architecture the initial data set was created 
by my webcam and i wrote the python script to  
to uh open up my webcam and you know uh save those 
uh data sets beyond the college why i you know  
i perform those signs in front of my webcam uh i 
have this time instructor and it was quite helpful  
um trying to learn some of those sites so i 
tried out using tensorflow object detection api  
and the digital api gives that hdi is the api 
being used by uh it's the platform built by the  
deep quest exec so uh so uh for the tensorflow 
or object detection api i needed to annotate  
the images the signs uh in past hour formats and 
after that i needed to convert them to cf records  
uh and yeah i used a pretend model for start for 
starters and that was mobile knights b1 and i was  
able to achieve if you want to set up to ac so 
i did the same uh with the deep star api and i  
annotated the data using your loop formats and 
there was no pretend model you know that can do  
this type of object detection so i had to build a 
custom model with the deep star api the customer  
wasn't that the custom api wasn't really that 
complex you know it's because deep stack is used  
for developers so that they can build models very 
fast yeah so um so i didn't really go too much  
of storage building that and my first accuracy 
was 1897. so i went over creating a new set of  
data sets right because before i was particular 
about the process but now that i could achieve  
my results i was not particular about i was not 
particular about about the quality of my model  
yeah so i uh i created the new data set and i 
was able to achieve a 90 policy and i i deployed  
the model from the big stack server and right 
now you know you can use it in real time i mean  
if you go to this github link and install it 
then you can uh run it live i mean you can  
you can hear the voice of words and you can see 
the main name which will bring it on to you uh  
uh as tests yeah so uh one of the challenges i 
faced is that it doesn't work well for quizzes  
like a series of science for example 
nice to meet you is a combination of  
serious signs and uh my i annotated my data you 
know as images so it doesn't really work well for  
um for scientists that are series of of 
images uh that are series of science yeah  
so uh this is by the way i i i actually will want 
the sign language to speak challenge and i'll be  
joining a request at the end of august and i'm i'm 
very excited about the fact that i get to you know  
see what is underneath the building blocks of of 
our request of the deep stack of api yeah so the  
next step is i i plan to retrain uh the data on 
the on the software detection api you know using a  
data architecture and i'll probably explore trying 
to build it up from scratch as a custom model  
and uh secondly i'm thinking of increasing 
the data set range to support more than 50  
cyclists right now it's only supporting 10. yeah 
created the data set of first festival this time  
i plan to enlist the help of a of a of a undercut 
for school you know school for undercut children  
um owned by the government in my location yeah so 
that will make the data situation quite faster and  
i'm also thinking of you know creating stuff for 
article languages so that you know can we confess  
um voice of speech of the signs you know 
in afghan languages there's no real uh  
oh sorry okay the motivation behind that is just 
me trying to you know learn how to use a machine  
learning translation for uh for something like 
this and also i'm considering writing a paper  
but i'm i'm i'm i'm thinking that it's not 
it's not that it's such it's not that it's  
sacharian i think the project is more 
codex oriented that research area so  
i don't really know about that yet but 
of course i'll be open to suggestions so  
you can always reach me by mail and of course the 
discord uh server we could they could have a chat  
yeah so that's all i'll be taking any other 
question uh in the discord channel so thank you  
amazing it's uh that's so cool steven 
thanks for presenting that it's really an  
inspiring story and uh congrats for for 
uh winning that's amazing when did you  
start the intern internship uh by the 
end of august okay wow cool congrats  
thank you yes sir um see you over in discord 
um last up we have last presentation um
yeah yeah it's great thank you so much for 
having me i'll try to please presentation as  
quickly as possible since we don't have a lot of 
time left so i'll just start sharing my screen  
perfect uh is my screen visible yep okay so 
right so what i'm trying to do today is i  
have a proposal for uh replicating human 
handwriting with deep learning methods  
so let's uh so first of all a bit of a background 
on the problem that at hand but the problem of  
handwriting recognition has been a hot topic of 
researching the dominant document analysis and  
information retrieval and computer vision and 
it has become fairly advanced these days with  
different types of architectures and very strong 
and powerful ocr engines such as google lens which  
is more of a software module than an actual you 
would say a framework so for a framework we have  
quite a bite as that proprietorship ocr techniques 
and such but the problem but the more recent task  
of synthesizing human like handwriting isn't as 
widely developed so like we can recognize human  
handwriting but we can't yet synthesize them to an 
accuracy which you might say is convincing enough  
for a human leader that they would not be able to 
distinguish between a human within handwriting and  
one that is entered by computer so and that is 
primarily because uh an individual's handwriting  
is something unique to them which inevitably has 
a lot of variability i mean even my handwriting  
isn't as consistent uh in my first two first two 
pages of my examination paper and the last two  
so definitely modeling handwriting is an extremely 
challenging task given the specific nuances and  
the invariability of a person's handwriting 
so there do exist some approaches to generate  
handwriting at this point and they are not and 
they are not quite decent i mean most of them have  
been developed in the uh maybe in the last five or 
six years so uh i'll just go over some motivation  
for my presentation so i i basically got assigned 
a homework of 28 pages i was supposed to write a  
lab report for my project at school and it was 
supposed to be submitted in three days but the  
problem was i had a lot of work already i had 
my research to do and stuff i had my stuff at  
school homework and stuff so the thing is that i 
wasn't able to actually complete it on time and so  
i i started looking for alternative approaches if 
there was an a technique or a product that could  
actually mimic my handwriting to produce results 
that the teacher would not be able to distinguish  
if i had done them actually myself or or it was 
basically a computer-generated output so when i  
started researching i came across a software 
and it was and it is called my text in your  
handwriting and it's actually one of the closest 
things to what we are trying to do uh it it can  
basically replicate any humans given handwriting 
i mean given a sufficient number of samples  
but i realized that the problem with the approach 
was that it wasn't based on deep learning and that  
it was computationally very expensive so i thought 
that maybe you try to incorporate the more recent  
deep learning methods that have been able to 
perform even better than state of the art state of  
the art architectures on many previous problems so 
what we're trying to do over here is given a text  
input and the samples of a user's handwriting in 
an image form our model tries to learn and apply  
the handwriting style to a given text input to 
the given text input and generate quote-unquote  
handwritten text in image output the objective is 
to figure out the sum neural network as you can  
see indicator in my diagram we have to figure out 
the some neural network that can accomplish the  
set task so basically we have a sample of users 
handwriting it may be a page of handwriting two  
pages three pages or even maybe a few lines if 
you can make the model so efficient and we input  
some text we basically transcribed and we have 
a you know we have a neural network that will do  
the task of converting the the computer inputted 
text into a handwritten text in an image format  
so i'd like to discuss what we already have 
in this domain or the existing resource so we  
have deep learning based handwritten handwriting 
recognition systems and they are actually pretty  
fairly advanced so you can even detect uh the most 
creepiest of handwritings i would say the most  
uncommon types the most dirtiest of handwritings 
the next we have deep learning based hana  
consensus techniques such as deep writing and 
scrabble gap now i would like to elaborate on  
what these things do and what their drawbacks 
are a little bit since we are discussing  
kinetic generation and they do exactly that 
so the drawback with these present systems  
such as deep writing and scrabble gun is 
that the limitations are based on their  
output approaches so basically deep writing what 
deep writing does is deep writing only works on  
a digital pen and tablet which most people might 
not have an access to and scrabbling and does is  
what scrabble can does is pretty similar to what 
i'm trying to do over here but the limitation of  
travel game is it does not replicate a particular 
use of handwriting but it has some pre-fed styles  
and maybe you can just input some text and it will 
transcribe that text into those i mean you the  
user can choose from one of those prefet styles 
to be for their text to translate into so what it  
is doing it is basically not uh producing you know 
so it is not basically replicating any particular  
writing but it's just converting a given text 
into some preferred handwriting styles which  
is actually pretty efficient but it's not uh that 
we're trying to achieve over here we are trying to  
take it a step further so as you can see as i 
already mentioned a piece of excess technology  
that can do exactly this what we propose is 
my text in my handwriting it was actually  
introduced in 2016 which is not very recent but 
the drawback of this model is as i said it's very  
mathematically complex and computationally 
expensive uh and also it requires very high  
resolution images say about 700 dpi images which 
might not i mean which everybody or every camera  
might not have access to and thirdly it requires a 
lot of manual or human assistance for the model to  
perform over us over a certain threshold otherwise 
the results are pretty rubbish uh if you'll notice  
if you read the actual paper uh in terms of data 
sets we i mean there is an availability to address  
this and i don't think data will be a problem so 
there is an uh the most popular of them is the iem  
data set there is an offline online version 
so the option version basically consists of  
uh scanned images and the online data set consists 
of the images as well as a temporal component  
which gives us an advantage in using rnns and 
lstm based models the sequence based models so the  
model performance can be benchmarked on the above 
mentioned data such as cvl heinz and rhymes which  
is also in french so what our next steps could 
be as mentioned so the next is basically so our  
task in hand has precisely three components so the 
first is learning the input data i mean basically  
recognizing or the handwriting style and learning 
that particular handwriting style the second  
is recognizing the text in the image so as to 
basically create a correlation matters to this  
letter corresponds to this particular style of 
handwriting i mean computer doesn't know what hand  
it looks like so we have to tell it that yeah this 
letter e means this particular part of the image  
the third is combining the learned handwriting 
style and then applying it to the input text and  
generating quote-unquote handwritten text so what 
we could do is to improve upon the present output  
of thought uh that i've thought of a feedback 
network kind of thing that whenever uh so we can  
basically pass our model through again and when it 
produces images at first we can we can instead of  
using a discriminator we just classified it as 
an image created by a computer or a user we can  
we can provide individual feedback to the model so 
the model realizes where exactly it is going wrong  
in the process of generating the handwriting and 
that and that is somehow and that is how the model  
will actually have to improve on those particular 
parts of the handwriting which might be actually  
quite difficult to let video uh so yeah i'm open 
to questions and discussions if anybody would  
like to direct on discord and yeah see you 
there thank you so much for having me again
great thanks for presenting um it's a pretty 
cool pretty cool project i hope it works  
also it's uh one of just one of the best 
motivations ever um not wanting to have  
to do something thank you so much most of my 
projects work this favor life but i'm too lazy  
to do something and i find out methods so like 
find out ai methods to do them for me exactly i'm  
pretty sure human laziness led to the invention of 
the wheel and and every everything else since then  
um great so that was the last presentation um 
we're gonna head over to discord now and people  
can chat with um any of the authors or any 
other presenters sorry if you're a presenter  
please go to discord and try to be in your own 
channel at least for the first 15 or 20 minutes  
in case people want to stop by uh 
roseanne anything else to add at the end
um know how paste discord link  
to the chat if people don't have 
it but um i'll see you guys there
um yeah perfect if you paste it now um as soon 
as we close the zoom the chat will disappear  
so um if you don't have the discord link 
by some chance it's on our website um  
right yeah it's on the community page perfect okay  
see you guys over there thanks for 
joining and thanks all the presenters