 Yes, so in the previous videos, we talked about the VGG 16
 architecture and the residual network. Now I want to talk
 about a topic that is somewhat related to convolutional
 neural network architectures. It's not super important in
 terms of, let's say implementing better performing convolution
 networks, but I think that really helps like solidifying the
 understanding of how convolutions work. So the first
 topic I want to talk about is replacing max pooling with
 convolution layers. And then and then with video after that, I
 want to talk about replacing fully connected layers by
 convolution layers. So both videos will be essentially
 about simplifying a neural network architecture by
 essentially getting rid of max pooling and getting rid of fully
 connected layers. It won't result necessarily in a better
 performance. But it is I think, just an interesting thought
 experiment. So the first video here, or the two video series
 will be focused on replacing max pooling. So there's action
 architecture that is called the all convolutional network. It
 comes from a paper that is already seven years old. It's
 entitled striving for simplicity, they're all
 convolutional network. So here, the authors propose replacing
 max pooling with a convolution layer that has a stride of two.
 And this is sometimes also called strided convolution. So
 now on traditional neural network or convolution network,
 we have usually a convolution layer with stride equals one.
 And then we have max pooling, usually two by two max pooling,
 also with a stride of two. And then we have a convolutional
 layer again, with stride of one, and we continue like that. And
 usually, the convolution layers, they preserve the size, because
 we have a stride of one. And the max pooling will reduce the size
 to fold, it will have the size. So the size that comes out of it
 is one half. If we have, let's say a two by two max pooling
 with a stride of two, and can maybe also write the stone. So
 the kernel size is usually two by two, and this is stride by
 two. And this helps us also, you're achieving a little bit of
 this location invariance. However, it's not essential to
 have that. So you can technically get rid of this, and
 then just increase the stride here by two, or here, and you
 will have the same effect that you reduce the size by one half.
 So I don't want to go into too much detail here. But if you
 look at the experimental results, when I recall correctly,
 they found that the performance slightly decreases when you get
 rid of this max pooling. However, how you can get an even
 better performance than before is by replacing this max
 pooling by convolutional layer. Let me get rid of this two here
 first. So we have a one here, and then you can have a
 convolutional layer here with stride equals two. And this will
 also have this one over half effect that it will decrease the
 size by half. But instead of using the max pooling, you have
 the convolution, which has weight parameters, and you can
 think of it as something like a learnable pooling, at least
 that's what they argue here in that paper. So it will be of
 course, a little bit more expensive, because you have no
 more parameters. But in that way, you can also simplify the
 network in terms of having it look simpler way, just saying,
 okay, we only use convolutions, we don't use anything else, if
 that's desirable. Of course, you still need the activation
 function, but you don't need pooling layers, for example. I
 recall there was a talk by Geoff Hinton, I don't recall the
 exact words. But I remember, Geoff Hinton said something along
 the lines of that max pooling was one of the biggest mistakes
 in computer vision, in terms of convolutional networks.
 Personally, I think it's actually not that bad. Max
 pooling works quite well. There's not necessarily urge to
 get rid of this. So it's here more like a thought experiment,
 how you can potentially simplify a neural network architecture.
 And then you're talking about simplifying. So you can also get
 rid of the fully connected layer using convolutions, I will talk
 more about that in the next video. And another way to get
 rid of fully connected layers is by using your global average
 pooling. So I'm not sure if this was the paper that kind of
 proposed that in the first place. Um, I guess it was not,
 but this was nonetheless a nice figure. So usually, we use these
 fully connected layers, like shown here on the left hand
 side, we use these fully connected layers to map from the
 convolutional layer onto the number of desired class label.
 So usually, we have a fixed number of class labels. And in
 order to get from the feature map size to the number of class
 labels, we use these fully connected layers. So on the
 first fully connected layer here would have would have as the
 input size, the reshaped feature map size. So things will become
 more clear when I show you the code example. This is something
 you have seen before, it's not new, it may look a little bit
 different, because I haven't shown that in a figure here. But
 this is essentially when we reshape so that it matches the
 size here and the fully connected layer. And what comes
 out of here, let's call that H, like some hidden dimension, it
 goes into the next fully connected layer. And here, the
 output is then the number of classes. Of course, yeah, this
 fully connected part is usually very intensive in terms of lots
 of parameters. You can simplify this by using global average
 pooling. So here, what you do is essentially, first of all, you
 assume that the number of channels in the last convolution
 layer is equal to the number of number of classes. And global
 average pooling works essentially like that. You have
 heard about average pooling, which is just taking the average
 value. And global here means of the whole feature map. So you
 would technically just use the whole feature map here. And then
 average over all these values. And this will be then the single
 value here. And then you do the same thing for the next one. So
 you just average over the whole feature maps. And that is also a
 way to get rid of fully connected layers. And by the way,
 here on the left hand side, this is actually not necessary to
 have two fully connected layers, you could technically also
 reshape into this one. And this one takes out the number of class
 labels. It can be good to have a fully connected layer, it's like
 additional parameters to learn, but it's also not essential, you
 can just have all the parameters learned in this convolutional
 network, and or convolutional part and then have the global
 average pooling. And I will show you next code example, how that
 works. And in the next video, I will also show you how we can
 replace that by an equivalent convolutional operation.
 Actually, there are two ways to do that. So personally, again, I
 would say there's no reason to get rid of the fully connected
 layers, they are just fine. And many or even most architectures
 use fully connected layers. But if you wanted to, you could
 technically also get rid of it. All right, let me show you in
 the next video, a code example of an implementation of this all
 convolutional network. And I think this will probably make
 certain things a little bit more clear that I just was drawing
 here. All right.