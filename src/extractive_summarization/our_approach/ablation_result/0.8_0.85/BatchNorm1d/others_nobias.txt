 And  having batch norm before the activation, that's usually how  yeah, that was originally how it was proposed in the paper. 
 That  means it may be that we need fewer epochs to get the same  loss that we would achieve if we don't use batch norm.  So you  usually with batch norm, the networks train faster. 
 And this is  essentially it's in that way, step one of batch norm is  similar to the input standardization. 
For instance in image processing, feature maps ususally have 2 spatial dimensions (N, C, H, W), so [BatchNorm2d (hyper-link)] is useful here.  However for some NLP tasks, if there is only the length dimension to consider, one would use [BatchNorm1d (hyper-link)].
BatchNorm1d normalises data to 0 mean and unit variance for 2/3-dimensional data (N, C) or (N, C, L), computed over the channel dimension at each (N, L) or (N,) slice; while BatchNorm2d does the same thing for 4 dimensions (N, C, H, W), computed over the channel dimension at each (N, H, W) slice. 
