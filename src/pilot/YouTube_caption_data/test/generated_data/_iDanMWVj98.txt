hello everyone so today i want to look
at this module which is touch dot n n
dots
uh
multi
head
attention
and it takes some argument for example
the embedding
dimension
number of heads
and also we are going to see a bias
is true
so
for this example we are going to have
just one batch and also the number of
head is one just to make it easy to
understand
and the attention formula is something
like this
the soft marks
of
query multiplied by the transpose of the
key divided by the square root of e
embedding dimension this one
are multiplied by v
okay so let's take an example say i am
good
and also the tensor representation of it
the attention representation of i for
example say it is 0.2 0.1
for m is 0.9 0.4 for good it is 0.7 0.8
so the
dimension of it the input this is the
input
it is three by two
so
the e dimension
of this say e
is true
so each word has a dimension of two
and the sequence length number of watts
is three
so this is like the same as l by e
okay
so at the beginning
these are like your
this is like your key query on value so
you need to compute the new key
key query and values
all right
so to do that
we need to compute like the new key
query
and the values
so pydot what it does it's like creates
random weights
and also random biases and the dimension
of this random weight is
the random which is three times e by e
which is the same as like a six by two
similarly for this it's squeezed like a
random bias of dimension 3 times e by 1
which is 6 by 1
so let's try to visualize this
the top blue
is
something like this
so the first part of the w is to wait to
compute the new key the second part of
the w is the way to compute the new
query
the third part of the w is to wait
to compute the new v
and each of them has a dimension of
e by e
or in other words two by two
this is two by two
two by two
all right so
the bias similarly
say it is uh something like this
the
the first part of the bias is actually
the bias to compute the new key
the second part is the bias to compute
the new query this third part is the
bias to compute the new value
and each of them has a dimension of e by
one
in other words two by one
two by one so
the new key
is something like this you take your
input this your input
uh you multiply it by
the
weight of the
key
you you add it with the
bias of the key
so this is the way to compute the new
key
dimension of this input
is this and the dimension of
the weight of the key is 2 by 2
e by e right but we are going to take
the transpose just to make dimensions so
dimensions too much you add it with
two by one bias of the key is two by one
we're also going to take the transpose
of this
this is
three by two
plus
one by two
if you add them they are the result is
three by two
similarly
for the
query
we are going to take this input this
input you multiplied by the query weight
this two by two one
you add it with the bias of the query
and
the
dimension of the input is two by two the
same input like this one that we use we
used here
multiplied by the two by two
this one is two by two plus
two by one we are going to take the
transpose
this is three by two
plus one by two after taking the
transpose and the result is
three by two
all right finally the
uh the value you take the same input
this one
and multiply it by the weight of the
value
plus the bias of the value this two by
one
and the dimension of the input is three
by two plus the uh the weight of the
value is two by two we are going to take
the transpose plus
two by one
transpose and this is three by two
plus by 2 after taking the transpose and
the result is 2 3 by 2
all right so we need to divide by the
square root of e
so pythagoras actually instead of
multiplying this by this
and dividing by square root of e it
divides q
by square root of e
square root of two
and the dimension
after doing this operation it's not
going to change
it's still three
by two
three by two
all right so you need a new paper here
so
you can see right
okay
okay so
i think
we should write like the
formula again
it is a soft max of
q
square root of e
multiplied by v so we have already done
this part
this part
no no we have not done we have to
multiply q times k transpose
so
q
times
k transpose is
we have just actually just computed q k
and v
okay
so we multiply
q
by
k transpose and dimension of
dimension of q
three by two
multiplied by
uh
two by three
after taking the transpose of the key
and
it is three by three
next
so we have kind of computed this we need
to push this into a soft max
after pushing the soft into your soft
max the dimension doesn't change
okay maybe
like this dimension is not going to
change
right
it's like three by three
and the soft marks maybe i can just
write soft max is like e to the power z
i
divided by the summation
j
to k
e to d
z
t
uh okay let's actually write the soft
marks after pushing it into the soft max
you might have
something like this
the result of that is soft max
0.1 0.3
0.6
0.7
0.1 0.2
0.4 0.1
and the dimension as i said doesn't
change is still 3 by 3
in other words
l by l
so
like this is
like this actually
i am good if you can remember the input
sentence so this 0.1
0.1 is like the amount of attention i
pays to itself 0.3 is the amount of
attention and pays to i
0.4 this 0.4 is the amount of attention
i'm paying too good
all right so we need to compute the
final output
final output
is
so
you know we have computed this and let's
call this attention att
so it is att multiplied by v this att
is
okay uh looks like this one
multiplied by
v
this is
3 by 3
multiplied by
3 by 2. and this is three by two
output
but like this is not the
final output of this layer
uh pi touch actually create some random
weight that it's going to multiply this
by to get the final final output so i
said fine about this one to actually
define a
final output so python which creates
this
output
final output weight of dimension
uh
e by e
in other words
two by two
and also the final output
bias of dimension
e by 1
which is
2 by 1
all right so the final output i mean the
real final output
is
something like this what you do is
you take this
this one
you multiply it by the w out
the blue out okay so plus the bias of
this this bias
dimension of this if this is
three by two obviously plus
this weight is two by two we are going
to take the transpose plus two by one
dot transpose
this is three by two
plus
one by two
and the final
is three by two
it's your final output of this layer
which you can push into further layers
for further processing thank you