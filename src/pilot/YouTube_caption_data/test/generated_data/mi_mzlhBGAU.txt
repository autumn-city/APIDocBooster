[Music]
hey there and welcome to this video
today we are going to implement a pretty
cool reinforcement learning paper called
the sensory neuron as a transformer
permutation in variant neural networks
for reinforcement learning this is the
official github repo the author is open
sourced and it contains both training
and evaluation scripts together with
pre-trained models the implementation i
prepared for this video is heavily
inspired by this repo however i decided
to only focus on a single task the card
poll swing up the official code is way
more general since it can handle all the
tasks described in the paper anyway in
my opinion implementing the attention
neuron for the cardboard task should be
enough to understand the most important
concept as always all the credit goes to
the authors and i just want to stress
that i'm not affiliated with any of them
finally i apologize in advance for
potential misinterpretations of their
work and bugs in my code one thing i
would like to highlight is this amazing
and interactive website that the authors
created
one can experiment and play around with
the pre-trained models and i will use it
later to explain some important concepts
let me try to explain in my own words
what the paper is about i would
definitely recommend you to stop the
video here and go read the paper to have
more context first and foremost the
authors want to create a permutation
invariant policy network what does it
mean well it simply means that no matter
how we shuffle the input features the
output action is going to stay identical
the second goal is related to the first
one and it is to have a single function
that processes each feature
independently here i name this function
f local and it is only after this
independent processing that we try to
combine the local information into a
global picture using the f global
function in the next couple of slides
i'm going to discuss the exact
architecture of the policy network
however let me just point out that the
paper actually demonstrates two very
interesting things first of all one can
actually successfully train agents like
this and they work pretty well which is
in my opinion not obvious and second of
all agents trained like this seem to be
immune to noise which i find pretty
surprising too anyway let us now focus
on more specific things as mentioned
before to make things simple both these
slides and the code are going to assume
that we are dealing with the card pole
swing up environment the policy network
has two inputs the first one is the
observation vector or i will sometimes
refer to it as the feature vector is
composed of five real numbers the
position of the card the velocity of the
card the cosine and the sign of the pole
angle and finally the angular velocity
at each time step we will get a new
observation vector from the environment
the second input is the previous action
and it represents the force we applied
to the cart our objective is to take
these two inputs and define a function
or more specifically a neural network
that is going to turn them into a new
action here you can see the entire
pipeline of the most important steps to
get from the inputs to the output first
of all we apply element-wise functions
to the inputs this is in line with the
locality requirement that i talked about
and then we generate three new matrices
q kv which stands for query key and
value and they are going to be used in
an attention module and what comes out
of this attention module is latent code
which is going to store all relevant
information about the observations and
the previous action and finally we will
take this latent code and just run it
through a linear module to get the final
prediction which is the new action
regarding the terminology this entire
pipeline describes the forward pass of
our policy network and everything before
the linear module is called the
attention neuron layer so how do we
create the attention inputs before i
start explaining them let me just point
out that in the top right corner you can
see the variable names that we are going
to be using in the code plus their
actual values that were taken from the
paper anyway let's start with the
easiest one which is the value matrix it
is going to be equal to the observation
vector also i added an extra dimension
to it so that we can use it in matrix
multiplications then we will have the
query matrix which is actually not going
to contain any learnable parameters and
it will just be the so-called positional
encoding if you are not familiar with it
it is a way how one can encode the order
information and it is used heavily in
natural language processing the even
columns are just sine functions with
decreasing frequency whereas the odd
columns are cosine functions with
decreasing frequency lastly we want to
create the key matrix it is basically
motivated by two ideas first of all we
want it to depend both on the
observations but also on the previous
action that is why we concatenate these
two second of all at a fixed time point
we would like to not only have access to
the most recent observations and the
previous action but ideally also all the
previous observations and actions that
we gathered during our current episode
and in a way this can be achieved by
keeping a hidden state for each of our
features then we have an lstm cell that
takes in the previous hidden state and
the new observation and action and spits
out a new hidden state and that is
exactly what you see on the screen let
me just stress that the lstm cell has
learnable parameters which are shared
among all the features anyway the k
matrix is nothing else than the output
hidden state of our lstm cell here you
can see the attention computation
together with the shapes of the matrices
we take the query matrix and the key
matrix and project them in a linear
fashion and then we just multiply them
the way you can interpret this is that
for each row of the query matrix we want
to know what the most similar rows in
the k matrix are and after we scale
these similarities and run them through
the hyperbolic tangent activation we get
attention weights and finally we
multiply the attention weight and the
value matrix to get our latent code let
me just stress two things this attention
computation will work out for any number
of features and the final latent code
size is dictated by the number of
embeddings this means that our model
will be able to accept feature vectors
of arbitrary size second of all if you
disregard the fact that each feature
could have a different hidden state this
entire pipeline of going from the
observation to the latent code is
permutation and variant and finally we
take this latent code and we apply a
linear module to it and we get our final
action and since this latent code was
permutation and variant with respect to
the observation this new action is also
going to be permutation invariant also
let me point out that in theory we could
replace this linear module by something
more complicated like the multi-layer
perceptron however a linear module
should be enough to make a good model
and yeah that is basically all you need
to know about the policy network right
so let's start coding so first of all we
want to implement our new networks in
pytorch and write some utilities
all right so before we start
implementing the fancy permutation
invariant network let us just create a
benchmark network that we are going to
use for comparison more specifically
let's just create a multi-layer
perceptron
all right so we specify the number of
features and it is something we need to
know in advance in layer size this is
just a tuple of integers representing
how big the different hidden layers are
and internally everything will be
actually stored in the sequential module
so here we define a new tuple containing
all the layer sizes so we start with the
zeroth layer which is let's say just the
feature vector and finally we also
append this output layer which has just
one element and that is because our
action is just a real number
and here we iteratively define a bunch
of linear modules followed by the
hyperbolic tangent activations
we unpack all the layers into the
sequential module
this part is definitely not standard
because what we are doing is taking all
the parameters that our network has and
we are telling towards that we don't
care about the gradients and this is
actually something proposed in the paper
because the author suggested that for
the cardboard task the best approach is
to do a so-called direct policy search
and use an evolutionary algorithm that
doesn't need gradients so in other words
we only use torch to define conveniently
the forward pass however we don't care
about its autograd functionality at all
because we won't use gradients for the
optimization
the input to the forward pass is just
the observation tensor let me just point
out that one difference to i guess
standard deep learning code is that we
don't necessarily work with tensors that
have the batch dimension and that is
because when we roll out the task we
will only have access to one single step
at a time the output is nothing else
than the action
and the actual implementation is just a
one-liner what we do is that we
temporarily prepend the observation
tensor with the patch dimension so that
it's compatible with torch we run the
forward pass of our network and finally
we only extract the first sample in a
batch because that was just a single
sample two small commands in theory we
could have also included the previous
action as one of the inputs but whatever
and the second thing is that we are
guaranteed that this return value is
going to be in the interval minus 1 1
because we apply the hyperbolic tangent
activation and now we want to start
building all the different components of
the attention neuron let's start with
the query tensor and as discussed before
it is going to be the positional
encoding
all right so it depends on two
parameters the first one is the number
of embeddings which is just the number
of rows of our table then the hidden
size which is just the number of columns
what we return is a numpy array that
represents the positional encodings for
your convenience this is the formula
this is just the helper function that is
going to compute the angle
and this helper function is basically
just constructing a row in our table
we define the entire table of angles row
by row
we apply the sine function to all the
even columns and the cosine function to
all of the odd columns let me just
stress there is nothing learnable about
this table and it only consists of
constants anyway this is exactly how
it's going to look like we have 16 rows
and eight columns
now we would like to implement a module
that is going to compute the attention
matrix using the key and the query
tensors
so first of all we need to provide the
projection dimension we have to provide
the hidden size which is equal to the
number of columns of both the q and k
tensors and finally we can choose
whether to enable or disable scaling
internally the projections will be done
using the torch linear module and we'll
have a separate one for both the key and
the query tensor
so we instantiate two linear modules and
we set the bias equal to false
and we also prepared the scalar
all right so the forward pass will
accept two inputs the first one is going
to be the query tensor that has the
shape of number of embeddings times
hidden size and the second input is
going to be the key tensor of shape
number of features times the hidden size
we then return the attention weights
that are of shape number of embeddings
times number of features and what's a
little unusual is that they won't sum up
to one in general because we are going
to use the hyperbolic tangent activation
rather than the softmax by the way this
is the exact formula that we're trying
to implement in the forward pass
here we project the query tensor
here we project the key tensor
here we matrix multiply both of the
projections
we divide by the scalar and this
division is element-wise so it's not
going to change the shape
finally we apply the hyperbolic tangent
activation and again it's element wise
and it doesn't change the shape
and that's it and now we can finally
start putting things together let's
implement the entire attention neuron
module
we provide the number of embeddings
which is a hyperparameter and this will
actually end up being the length of our
latent code then we provide more
hyperparameters namely the projection
dimension and the hidden size internally
we will have this hidden state and know
that it is going to be specific to lstm
which means that it's actually two
tensors and then we will have this lstm
cell module and as discussed it's going
to be used to create the key tensor then
we will have an instance of the
attention matrix module that we just
implemented and finally we will have
this buffer that is going to hold the
query tensor
here we store the parameters as
attributes
we initialize the hidden state with none
but as you will see in the forward pass
this will basically mean that we will
populate it with zeros
here we instantiate the lstm cell so the
reason why the input size is two is
because we will concatenate one element
of our observation vector for example
the position of the card with the
previous action
we instantiate the attention matrix if
you're wondering why we set scale equal
to false which we know means that we
will just divide by one if i'm not
mistaken this is what the authors did in
their code so i just used the same logic
and here we use the utility function
that we rolled and we generate the
position encodings we convert them to a
torch dancer and finally we register it
as a buffer
so similar to the multi-layer perceptron
forward pass we see that we provide an
observation and again we are not working
with tensors that have the batch
dimension additionally we also provide a
number representing the previous action
well the forward pass returns is latent
code which has a shape of number of
embeddings and uh it also outputs the
attention weights in case we want to
inspect what happened inside of the
attention here you can again see the
formula for computing the key and it
will be nothing else than a new hidden
state anyway just to avoid confusion the
h and c hidden states that you see in
this formula will be grouped together in
the code into a single tuple called hx
here we just concatenate our observation
tensor with the previous action
in case the hidden state was none or in
other words if this is the first time we
run the forward pass we populate both of
the hidden states by zeros
and here we call our lstm cell
here we create a variable for our query
which is nothing else than the internal
buffer
the k tensor is nothing else than our
output hidden state
the value tensor is nothing else than
the input observations or we just add a
new dimension to it so that
it's easy to do matrix multiplication
and here you can see the exact formula
to generate the latent code
here we call our attention matrix module
and we get the attention weights
finally we matrix multiply the attention
weights and the value to enter and by
the way we again apply the hyperbolic
tangent activation it's not something i
showed in the formulas but again i think
i saw it in the official implementation
so i just apply the same activation
we squeeze out the dummy dimension and
we end up with a one dimensional tensor
which is our latent code and we return
it together with the attention weight
and yeah that's it let's just play
around with the attention neuron module
we defined all the important hyper
parameters and we want to instantiate
our module
first of all let me demonstrate that our
module doesn't care about the number of
input features the observation vector
has
we iterate through different number of
features and we always make sure that we
set the hidden state equal to none and
as you saw during the first forward pass
it will be actually populated with uh
dancers of zeros so we actually want to
reset it
here we set 0.5 as the previous action
and we generate a random observation
vector with the corresponding number of
features
the shape of our latent code is always
16 which is the number of embeddings and
it is independent of the number of input
features this is definitely a very cool
property of the attention neuron second
of all let us look into the permutation
and variance
okay so i create an observation tensor
with five features which is in line with
the cardboard task
again i make sure that we always reset
the hidden state which is equivalent to
just starting a new rollout or episode
here i generate a random permutation
and then we actually permute the
observation tensor and send it through
the attention neuron
in all three of the cases the latent
code did not change so in other words if
the hidden state is identical between
different calls then yes the permutation
invariants will hold however let us try
to remove the resetting
and here you can see that if the hidden
state is not normalized you will get
different latent codes and just to kind
of illustrate the point i'm going to use
the interactive website the author's
created so let us see what happens when
we click on the shuffle observations
button
as you can see it clearly disrupted the
performance of the model however after a
couple of steps the model is back to
like working perfectly shuffling the
observations actually does have a
temporary effect on the performance of
the model because unfortunately the
hidden states cannot be shuffled or
permuted accordingly because we don't
know what the ground truth permutation
was however it seems like after a couple
of steps the hidden states are correctly
updated too
all right and there is one more thing to
do and that is to create the entire
policy network
here again we have the three hyper
parameters exactly the same ones as for
the attention neuron module and
internally we will have an instance of
our attention neuron module and also an
instance of a linear module
we instantiate the attention neuron
and we instantiate our linear layer that
is going to input the latent code and
output the new action
here we do exactly the same thing as we
did with the multi-layer perceptron we
disable gradient computations for all
our parameters
so the inputs to the forward pass are
exactly the same as for the attention
neuron it's the observation vector and
it is also the previous action and the
output is the next action
we run the forward pass over the
attention neuron
we take the one-dimensional latent code
we temporarily add a new dimension to it
so that it's a batch of a single sample
we apply the linear layer and finally we
apply the hyperbolic tangent activation
and here we just get rid of the batch
dimension
so we have written our deep learning
modules and our goal is to turn them
into reinforcement learning agents and
ultimately train them however this will
require some extra code that some of you
might find boring so i definitely
encourage you to check out the chapters
that i provided for this video and skip
all the way to the end where i discuss
the results anyway if you're staying
let's go back to coding to make things
more readable and extendable we are
going to introduce a concept of a
solution which is just a unified high
level interface and its goal is to hide
all the lower level details related to
torch modules
we are going to create an abstract class
called solution and the idea is that we
are going to subclass it twice once for
the permutation and variant network and
the second time for the multi-layer
perceptron
all right so first of all we make an
assumption that the solution has an
attribute called policy that will
actually store the torch module
here we define an abstract method what
it means is that the child will actually
have to implement this this specific
method is called clone and its goal is
to copy the current solution
the goal of this abstract method is to
get the next action given the current
observation
all right so this method is supposed to
return the number of features the model
expects and we will have a convention
that if this method returns a none then
it means that it can take in an input of
any size as we saw it doesn't matter how
big the feature vector is whereas for
the multi-layer perceptron it will be a
fixed number
so this method is going to reset the
solution and the specific use case is
going to be resetting the hidden state
of the permutation environ network and
later on we will make sure that this
method is called at the beginning of
each rollout
first of all note that this is not an
abstract method anymore and the reason
for this is that we will provide an
implementation in this parent class its
goal is to get all the learnable
parameters our neural network has and
flatten them into a one-dimensional
array
here we trade parameter by parameter we
cast the torch sensor to a numpy array
and finally we flatten this array and as
you can see i'm not necessarily sending
this sensor to a cpu because in this
video and in this code we're going to
assume everything takes place on a cpu
which by the way is a simplification and
i think the original code also kind of
handles a possibility of running the
forward pass with gpus
and finally we concatenate all the 1d
arrays into a single huge 1d array and
the reason why this flattening needs to
take place is because we are using let's
say a blackbox evolutionary strategy
that i will discuss later and
it doesn't really care about the fact
that some parameters come from a linear
module or some other come from the
attention module there is no structure
all right so we implemented the getter
but we also want to implement the setter
this method is going to take in a 1d
numpy array of all learnable parameters
and it's actually going to go inside of
the neural network and just assign all
these parameters to the corresponding
weights
we're going to keep track of what the
starting index is and what the end index
is
and then we're going to take the torch
tensor and we're going to redefine it in
place using the user provided parameters
finally we just update the start index
and again this method is going to be
using the optimization because we will
have this black box optimizer that is
at each iteration going to suggest
multiple different parameter sets and we
will just want to evaluate how good our
network is given these parameters
and finally just a small method that
counts up the number of parameters and
yeah that's it for our abstract class
let us now subclass the solution class
and create a multi-layer perceptron
solution
so we provide the same hyper parameters
as we saw before and internally we will
store all the hyper parameters and also
the actual policy network
here we store the hyperparameters and we
also define the d-type to be flow 32.
we instantiate our multi-layer
perceptron module we cast all the
parameters to the float32d type and
finally we set it to evaluation mode
here we define what cloning means
here we take the observation which is a
numpy array we cast it to torch tensor
we make sure it has the right d type and
then we run the forward pass
the number of features is going to be
fixed
and finally the multi-layer perceptron
solution will not have any resetting
logic since there are no hidden states
now we want to create a second child
class and it's going to be the
permutation and variant solution
we've seen these hyper parameters before
again we will store all the hyper
parameters in this dictionary we will
have our policy network which is the
permutation and variant network however
what is different to the mlp solution is
that we will also store the previous
action internally that means that
whenever we call the get action the
previous action will get updated
we instantiate our policy network
finally we initialize the previous
action to zero
school method is actually identical to
what we did with the mlp solution i
guess we could have put it in the
abstract class
so to get the action first of all we
convert our numpy observation array to
torch tensor then we also provide the
previous section which we are storing
internally and then we just run the
forward pass of the policy network
here we make sure that we update the
previous action with what we just
predicted
and we also returned this action
so unlike the mlp solution here we
actually implement the reset method
there are two things that are happening
first of all we make sure that the
hidden state of our policy network or
more specifically of the attention
neuron submodule of the policy network
is set to none that means that we are
kind of deleting all the memories and
second of all we set again the previous
action to zero
finally the number of features is going
to be none this will mean that the
number of features can be anything we
want anyway now we would like to
actually implement the rollout logic
using the open ai gym and the cardboard
swing up environment the authors
actually use their own implementation of
this task which is supposed to be harder
since the initial state can have more
extreme values
however i did not really want to rewrite
it from scratch or clone their repo and
instead i just tried to look through
github for similar implementations
i found this repo which seems to
implement more or less the same thing
however it seems to be an easier version
of the cardboard swing up environment
but it is not that big of a deal
the author put it on pipi which is great
and we're going to use it so let us
quickly implement the rollout logic
here we just define the number of
original features constant which is five
it's mostly for convenience
so here we define a custom exception and
we're going to raise it in case the
model number of features is not equal to
the actual number of features inside of
the observation
first of all we can choose whether we
want to render the environment or not
this shuffle and reset flag will give us
an option to randomly shuffle the
features at the beginning of each
episode and this is actually the way how
we're going to test whether our agent is
able to deal with shuffled features let
me also point out that the authors
investigated the possibility of
shuffling during the rollout rather than
just before we actually run the rollout
and yeah our code is not going to
contain this possibility number of noise
features will control how many noise
features we will inject into the
observation vector and the main use case
for this is to really test out whether
our permutation and variant agent is
immune to noise then we have two
different random states finally we have
the maximum episode steps by the way in
this video i use the words episode and
roll out interchangeably internally we
will store the number of overall
features which is the original ones plus
the noise ones we will have this number
array that is going to hold the
permutation indices also we will have
the actual gym environment
here we make our environment
here we redefine the maximum episode
steps for the rest of this video the
number will be always 1000 because
that's what the authors used in the
paper
the number of overall features is the
original features plus the noise
features
here we just create a range of integers
from zero to number of features and this
array will be actually used for the
shuffling
here we set the noise standard deviation
to 0.1 i just took it from the paper and
when we are actually injecting noisy
features inside of the observation it
will always come from a gaussian
distribution with mean 0 and this
standard deviation
now we want to define a method that will
dictate what happens just before the
roll up
the only thing we actually do before the
rollout is to make sure we permute the
features
here we regenerate the permutation
indices to be again a range of numbers
and then if the user wanted to do the
shuffling before each rollout we just
shuffle it and know that this shuffle
method actually works in place
first of all we actually want to shuffle
the features and second of all if the
user wanted to have noise features we
will insert them into this array the
input is the raw observation feature
array that only has five elements this
is actually what the gym environment is
going to give us at each step what is
returned is a modified version of the
input it will be shuffled and it will
contain noisy features and this modified
observation is exactly the one that goes
on to our deep learning model
here we use the normal distribution to
generate our noise vector
here we concatenate the original
observation vector together with the
noise
here we use our permutation indices to
do the shuffling
all right and now we have everything to
actually implement the rollout
the input is going to be the solution so
it's either the mlp solution or
the permutation invariant solution and
we return a reward representing how good
of a job our solution did during this
given rollout
here we just wrote a check making sure
that the number of features is
compatible
first of all we run the reset for
rollout that we just implemented and as
you saw this will do the shuffling of
the permutation indices assuming the
user wanted to shuffle
now we will reset the solution and if
you don't remember it was a no for the
mlp solution however for the permutation
and variant solution this would set the
hidden state to none which means that we
basically forget everything that
happened
and lastly we also reset the gym
environment and this will give us the
very first observation
optionally we also render
so this episode reward will be the
return value and we were actually
iteratively add to it all the first step
rewards done is a boolean that will
signal whether our rollout has finished
or not and there are two ways we can
finish the first one is reaching the
maximum episode steps that we set to
1000 or the second option is that the
cart goes off the screen
we modified the raw observation which
means that we will add the noise
features to it and we will shuffle it
we send this modified observation
through to our solution to get the next
action
we call the step method of the gym
environment which accepts the new action
and it gives us the new observation a
reward and it updates the done variable
and that's it for our rollout but again
as i mentioned we are not able to
shuffle during the episode but only
before the episode before we start
writing the training script let me
quickly talk about the actual
optimization algorithm we are going to
use it is called the covariance matrix
adaptation evolution strategy and what
you see on the screen is a python
package that implements it and i'm not
sure why the authors decided to go for
this specific optimizer however we're
going to use the same thing
more specifically we will be using the
ask and tell interface the idea is that
at each iteration we will ask the
optimizer to give us a set of candidate
parameter vectors
and we are then responsible for actually
assigning a loss to each of them and
once we have it we will just tell the
optimizer what the losses are right so
let's implement the training script
so first of all let us write a safe
function that we will call during our
training
we simply create a tuple with two
elements first of them is going to be
the solver instance and second of them
is going to be our solution instance
then we just pickle dump it on the disk
now the goal is to write a get fitness
function that is going to evaluate how
good or bad a given solution is
so this function is going to input a
solution and also a couple of other task
related parameters and it is going to
return a list of all episode rewards and
we will need to run this function for
each of the proposed parameter vectors
from the optimizer
and the actual implementation is pretty
straightforward because all the logic is
actually implemented already inside of
the task class now i'll quickly write a
cli
all right so let me discuss a couple of
important arguments and options so the
first positional argument is name
solution and there are three options
linear mlp invariant in the background
what we want to do we want to
instantiate the corresponding solution
class and note that linear is nothing
else than multi-layer perceptron with no
hidden layers and this logging folder is
where we're going to put all the
tensorboard related records and also the
checkpoints this parameter is very
important because it determines how many
roll outs we will do when evaluating the
fitness the higher this number the more
precise estimate we will have however
the slower the training is going to be i
took 16 because i believe that's what
the authors did also we will implement a
multi-processing logic and here one can
specify the number of jobs here another
optimization related parameter which is
the population size and by default it is
set to 256 and basically dictates how
big a possible solution set is going to
be at each iteration i think the reason
why the authors put it to 256 is because
they had a cluster that had 256 cpus
again we'll just set it to the same
number and finally we can shuffle on
reset during the training and we'll
actually use it in some experiments to
kind of see whether one can improve the
multi-layer perceptron if uh at training
time we do this shuffling so in a way it
will be like augmentation
we instantiate the tensorboard
summarywriter and we dump all the
parameters
and now we just want to map the string
solution to an actual solution instance
first of all we define the linear
solution which is nothing else than a
multi-layer perceptron with no hidden
layers i know that here we need to
specifically mention the number of
features at training time
here we create a multi-layer perceptron
with a single hidden layer size with 16
nodes
finally the star of the show the
permutation and variance solution with
these hard-coded hyper parameters but as
mentioned these are the hyperparameters
they used in the paper i believe
now we would like to initialize our
solver
there is an option of providing a
checkpoint that is useful for continuing
training that was started before
if there is no checkpoint our initial
guess is going to be just zeros then we
just simply instantiate the solver
providing all the relevant solver
parameters
however if the user does provide a
checkpoint path we unpickle it all right
so now we just take our get fitness
function and we set all the keyword only
parameters
all right and here if the number of jobs
is minus one we actually want to use all
the cpus and if not uh it's just
whatever the user provided finally after
all this work we can write the training
or let's say evolution logic
first of all we use this
multi-processing pool context manager to
be able to do multi-processing in a very
simple way
then we will iterate all the way up to
the max number of iterations
and here we ask the solver to give us a
bunch of different proposals when it
comes to parameter vectors and actually
the size of this param set is going to
be equal to the population size
what we do in this iterable is that we
go one by one through the parameter
vectors and we just use our setparams
method of the solution class to actually
register those parameters
so we basically go
through each solution in our iterable
and we just compute what the fitness of
that given solution is
and remember our get fitness function
was actually returning a list of per
episode rewards and here we just
actually consider the mean of these as
the final fitness of that given solution
and since the cma solver actually looks
for minimums we need to flip the sign
and here we'll just concatenate all the
parameters together for evaluation
purposes
and yeah here we have a bunch of useful
metrics
and here we'll basically checkpoint our
model and the solver every couple of
iterations or if it's the very last
iteration
finally we actually tell the solver how
good or bad the proposed parameter
vectors were
we send the keyboard interrupt signal
the checkpointing will take place
and again that is our training script
all right i wrote a script that launches
multiple experiments
first of all we train a simple in your
model and there is no shuffling taking
place during the training then we train
a linear model again however this time
we're going to randomly shuffle the
features before the rollout and that
means that the fitness function should
basically encode how good the model is
when we give it randomly shuffled
observations this approach is very
reminiscent of augmentations and instead
of encoding a certain prior into the
architecture we want the model to learn
permutation and variance from data or at
least partially the third and the fourth
experiment are the multi-layer
perceptrons with and without shuffling
and finally we have our permutation and
variance solution and let me just stress
that at training time we are not going
to do any shuffling whatsoever also you
probably noticed that the number of
iterations for the permutation in
variant model is very high and let me
just warn you that unless you have
dozens of cpus you will not be able to
drain it it is totally possible that my
modifications to their code introduce
major slowdowns anyway it is what it is
i actually ran this entire script
already and let me just show you the
results
okay so here we have the tensorboard and
let me show you some plots
let's look at the max generation metric
so each iteration or let's say
generation the solver proposed 256
different solutions and this graph
basically shows the best solution per
generation let me just stress that the
upper bound of this metric is 1000 which
would mean that we held the pole
completely straight for 1000 steps
however since the environment starts
with the pole being down it is virtually
impossible to actually reach this upper
bound here you can see the five
different experiments and clearly the
multi-layer perceptron model
corresponding to the red plot without
augmentations was the model that was
able to solve this task and it didn't
really take that many iterations the
linear model in dark blue did not seem
to have enough capacity to learn the
tasks perfectly however it almost hit
the 800 mark the orange plot is the
permutation and variant agent and as you
can see it keeps on going up however at
a very slow rate i'm sure if i continued
the training it would get to higher
scores however running these 9000
iterations already to more than a week
on my machine finally if we look at the
ping and the bright blue experiments
they were not really able to hit very
high scores so that these experiments
correspond to the augmentation setup
where we were shuffling before draw out
during the training anyway the metrics
we were tracking with tensorboard are
interesting however the main question we
want to answer is the following how good
are these models when we shuffle the
features before each rollout and what
happens if we inject noise features into
the observation vector let me just
stress that the models train without
augmentation never came across shuffled
observations at training time and we
want to know what would happen to their
performance if we do shuffle the
features and for this i had to write
some extra code that i'm not gonna
present in this video however you can
find it on github here are the results
alright so this bloat investigates how
good or bad different models are at
dealing with feature shuffling on the
x-axis we have six different models a
small node here invariant hours
corresponds to the model i try to train
from scratch whereas the invariant
official is the permutation very model
with the weights the authors open source
on github the y-axis represents the
overall reward finally the two colors
represent whether shuffling was taking
place or not and the actual
distributions were obtained by running
200 rollouts so if we start from the
left we see the mlp model clearly it is
very good when there is no shuffling
however as soon as we introduce
shuffling it basically stops working
then we have the mlp model with
shuffling augmentations and here we see
that without shuffling the performance
decreases compared to the no
augmentation model however if we start
shuffling the features the model gets
worse but not by much this which
suggests that the augmentations did make
the model immune to shuffling to a
certain extent however not completely
then we have the permutation and variant
model and as you can see especially the
model with the official waves is
performing really well and its
performance is not influenced by
shuffling the features at all by the way
let me stress that these results are not
comparable to those presented in the
paper because we have a different
environment that is supposed to be way
easier finally we have the linear models
and here we see the same pattern as with
the mlp model without augmentations and
no shuffling the model is pretty good
however as soon as we shuffle it is
basically useless however the
augmentations do make the model immune
to shuffling to a certain extent but not
completely here we have a closer look at
the permutation and variant model using
the official weights on the x-axis we
have the number of noise features we
injected into the observation vector and
the rest of the figure stays the same as
the previous one and quite amazingly you
can see that the model is basically able
to ignore the noise features when there
are five of them
and when we increase the number of noise
features there seem to be rollouts where
the model struggles but still overall it
seems to be extremely robust to noise
and i believe this is one of the main
properties that the authors try to
highlight in the paper uh it is pretty
cool i guess one could question whether
their generated noise was really that
hard to spot since it was sampled
independently at each step with a fixed
mean and standard deviation however i
did not really have time to do more
experiments anyway this property is
definitely impressive and in my opinion
not obvious
anyway that's it for today i just
generated a bunch of videos of the
actual rollout so feel free to inspect
them let me stress that one thing i did
not really cover in this video was
shuffling the features during rollout
which is a more challenging task than
only doing it once before the rollout
anyway thank you very much for watching
this video uh and again i would really
like to thank the authors and give them
all the credit i hope i managed to
simplify their multi-purpose code
however
maybe i didn't who knows i would
definitely encourage you to check out
their code too it is more than likely
that my quote contains mistakes and
misses important details additionally i
would like to point out that the authors
were really responsive and supportive
when i asked questions on github so i
definitely appreciate that if you
enjoyed this video don't hesitate to
like it leave a comment and subscribe
this is probably the best way to let me
know that you enjoy my content also i'm
very open to suggestions and making
these videos better and therefore
constructive criticism is more than
welcome anyway have a nice rest of the
day and see you next time