Hello Everyone, my name is Aarohi and welcome to 
my channel. So guys in this video i'll show you  
how to build your own video classifier so video 
classification is the task of predicting a label  
which is relevant to that video and for example 
after watching this video your algorithm should  
be able to predict that this uh dancing uh dancing 
is a label okay and then watching after this video  
your algorithm should be able to predict it as 
a exercise and after watching this video your  
algorithm should be able to predict it as a yoga 
right so this kind of classifier video classifier  
we are going to build in our today's class so the 
topic which we are going to cover in a today's  
classes first of all uh i'll give you overview of 
video classification after that we will build our  
own video classification model and for that we 
need a data set so i'll show you the data set  
first i'll show you how to prepare your own 
data set if you want to build your own custom  
classifier video classifier then how to prepare 
your own data set for this video okay and after  
that i'll show you how to train your algorithm and 
finally i'll show you how to test your algorithm  
so let's begin so what is video so video is a 
sequence of frames arranged in a specific order  
right frames or images right videos are collection 
of images or frames arranged in a specific order  
and guys when we perform image classification 
how we perform image classification we first of  
all what we do we take images right and after that 
we extract features and we extract features using  
a convolutional neural network once we extract 
the features then we classify the image based  
on those extracted features this is how image 
classification uh problem works right so in video  
classification what are the steps to perform video 
classification so very first step is we extract  
from frames from the video right so we extract 
frames from the video after that we use feature  
extractors like convolutional neural networks to 
extract the features once you extract the feature  
then we classify every frame of that video on the 
basis of the extracted feature so this is the main  
idea this is how we are going to perform this 
video classification okay so now before further  
discussion on this video classification i want you 
to understand what is human activity recognization  
so human activity recognization is a task of 
classifying or predicting the activity or the  
action performed by someone okay so this is called 
activity recognization so now how this human  
activity recognization is different from normal 
image classification problem so in normal uh image  
classification problem as i just told you we take 
image we extract feature and on the basis of those  
extracted features we classify right but in human 
activity recognization we need series of frames to  
predict the activity performed by a human 
okay so see this example so in this video  
flip action done by this person okay we can only 
tell that it is a flip if you'll watch the entire  
video okay so for example we are not uh providing 
the entire video to our algorithm and we are only  
providing this particular um you know frame one 
frame the video of from that video of a flip we  
are just providing one frame to our algorithm see 
this frame from this frame our algorithm you know  
will give us a wrong result it can say that um 
activity performed over here is running or it  
can say activity performed over here is walking 
right so but it will not be able to predict as a  
predict flip over here but right so what i'm 
trying to say here is if you want to uh you know  
know the human activity performed therefore 
that you need a entire video you need a series  
of frames on the basis of those series of 
frame your algorithm will be able to tell you  
what uh human activity is being performed 
okay so now why we need video classification  
with the help of video classification model we can 
solve the problem of human activity recognization  
all right so that means whenever you want to 
perform any task which is related to human  
activity recognization then you can use 
video classification model for that okay  
now just see these uh two pictures over here in 
first image girl is dancing and in second image  
a girl is doing yoga okay now in both if will 
you know for example if you're giving these  
two images to your algorithm on the basis of the 
you know position of the hand the pose of a hand  
the arms are spread in both the pictures 
and um there is some pose with the legs also  
so on the basis of that our image classification 
model can give us a wrong result it can  
predict both images as a dancing or it can 
predict both images as a doing yoga right  
but for example now see these pictures 
if you give series of frames like this  
right for dancing also we are giving a series 
of frame a video video consists of frames right  
and for yoga activity also we are providing 
a series of frames now on the basis of these  
series of frames uh our algorithm will be 
able to understand that it will be able to  
distinguish between the similar actions by using 
environmental context right so what i'm saying is  
the girl who is doing yoga the environmental 
context over there is different and the girl which  
is dancing the environmental context is different 
over there so with series of images our algorithm  
will understand okay the girl which is uh doing 
some pose on the mat that is yoga and the girl  
which is you know performing some action without 
the man that is dancing okay so this is how our  
algorithm learn by the environmental context and 
it will be able to distinguish the actions okay  
now the problem here is when we provide this 
series of frames right with this technique  
there is one drawback of this technique what 
happen is our algorithm will you know classify  
on the basis of every single frame means for one 
frame it will classify the result for second frame  
it will classify whether it is dancing or yoga for 
third frame it will classify it as dancing or yoga  
for all the frames it will give you a separate 
separate output for all the frames now what  
happen is for example for some of the frames 
our algorithm is confident okay that this is a  
dancing but for some of the frame our algorithm 
you know uh might be uh confused sometime with  
some other activity or some other action right 
so what happen is sometime your algorithm will  
give you correct answers but sometime your 
algorithm will not predict the correct  
correct label for that particular frame so now the 
solution to this problem is how we can solve this  
problem so basically what we can do is we will 
classify the video not on the basis of single  
frame instead we will average over some frames 
and then display the results so you can see uh  
in this image here you can see these are the four 
different frames of one video and this p stands  
for the current uh current frame okay so uh this 
is the p means current frame predicted probability  
and p minus one means last frame predicted 
probability and p minus two means second  
last frames probability predicted probability 
and this in the same way p minus three means  
um third last frame okay so what we are doing over 
here is we will choose the uh value of n n means  
for um you want to average over how many uh 
frames so for example if you chose the value  
of n is four that means we want to average over 
the results over the four frames okay so these are  
those four frames and every frame every frame is 
going to a model and model is predicting that this  
frame in this frame dancing is 99.99 probability 
and yoga is one percent probability second frame  
for the second frame our algorithm is saying 
again it is giving the separate result third  
frame is this and fourth frame is this for every 
frame our algorithm is giving us separate results  
now we will average over the results and we'll 
finally uh you know classify the video right  
so this is what we do now let me show you the 
practical implementation of video classifier right  
so how we are going to build a video classifier 
we will use cnn convolutional neural network and  
we will also use lstm okay so lstm is uh comes 
under recurrent neural networks so we are using  
cnn to extract the features of all the frames 
of each frame right and then those the output of  
those independent convolutional neural networks 
so we have independent con you can see in this  
image we have independent convolutional neural 
network which is working for each frame right so  
this output this the output of these independent 
convolutional neural networks will feed this to  
our recurrent neural network and that network will 
fuse this extracted information temporarily okay  
so let me show you the practical implementation 
now so let's go to our practical implementation so  
this is my code okay let me show you the data set 
first so the data set is okay so you can see here
data set of my code is this i have a dataset 
folder here you can see i have two separate  
folders so you also create two separate folders 
i'm showing you how you can uh you know prepare  
your own data set so first of all create data 
set folder under that data set folder create  
two folders with the name of train and test 
now open the strain folder so these are the  
three different classes on which i want to work 
so basically i want to build a video classifier  
which can classify if the video belongs to 
dancing class exercise or yoga class okay  
inside this dancing folder i have few videos 
which are related to dancing so you can open  
some video and i'm showing you so these kind of 
videos are there right so you can open few more  
videos and you can see so these kind of small 
short videos are there in this folder right  
so all the videos are related to dancing so guys 
this data set is very small again if you want to  
um you know improve the results or if you want 
your algorithm should perform very well then you  
can increase the number of examples so in the 
same way if you open the exercise folder i have  
images uh 47 images related to exercise and under 
yoga also i have 48 images related to yoga okay  
so this is about my train folder now under test 
folder again we have these three folders dancing  
exercise and yoga if you'll open dancing folder 
you have five videos under it and the same way you  
have videos for exercise and yoga also okay so now 
on this data set i'm going to show you the code  
i'm reading the data set first i'm reading 
the train file and you can see these are the  
three folders i have now what i want is see i 
have written this code and the main intent of  
writing these few lines is i want to store results 
like this i basically wants to create a data frame  
and i have created one column with the 
name of tag and under that i'm writing the  
labels and over here i'm writing the 
video name which is the video this video  
belongs to this class this video belongs to this 
class okay so what i have done is i am using a for  
loop and then i'm reading the whole data one by 
one and then i have created a data frame over here  
created two columns under tag folder i'm storing 
the class label and under video name i'm storing  
the name of the video so this i have done for 
train folder okay and then i have created one  
the this data i'm storing this data 
into a train dot csv file okay so now  
i have one train dot csv file which have detail 
of the videos which i have in train folder in the  
same way this is the code which i have written 
for test videos so i'm reading the test data  
and then i have a loop which will read all the 
videos from all the folders one by one then i'm  
creating a data frame over here and then finally 
i am creating one test.csv file and i'm storing  
all their data frame over there now let me show 
you my train dot csv and test.csv file to you  
so here this is my test.csv file and this is 
my train.csv file and this is the code jupiter  
notebook which i'm showing you okay so over here 
in the same folder you will have your train dot  
uh csv and test.csv file and over here we have our 
data set folder okay so once you have your data  
set then you need to install this tensorflow docs 
i have commented it out because i have already  
installed okay if you are uh running this code for 
first time then you have to install it okay and  
these are the required modules uh once you import 
all the modules now we are reading our train dot  
csv and test.csp file which we have just created 
right and now we are seeing the total number of  
training examples and test examples we have 
so we have 145 videos for training okay  
so these 145 videos means it includes the video 
of dancing uh exercise and yoga in total we have  
145 videos for training and in total uh from 
all the folders of testing we have 22 videos  
for testing so this is our data set looks like 
this okay now we want to feed see now what we want  
we want to build a video classifier that means you 
need to feed the video to your network as input  
so how we are going to feed the video to our 
network so for that we are using opencv video  
capture right so video capture method will read uh 
the frames from the videos right remember i have  
told you in the beginning of our today's class 
uh what is video video as a collection of frames  
arranged in a sequence okay so when we we 
want to give this video to our algorithm  
so that means we need to fetch the frames those 
frames will go to our algorithm so for doing  
this functionality we are using opencvs video 
capture which will read the frames from videos  
and then we will extract frames from the videos 
until a maximum frame count is reached now until  
a maximum frame count is reached what i mean by 
this is see guys some of the videos are very short  
and some of the videos are might be a little bit 
longer than the other videos right so the number  
of frames are going to be different in each video 
right maybe one frame is more or 2 or 20 or 30  
frames will will most of the time frames 
will be different in um in every video  
number of frames will be different okay so now 
when the frames are different in every video  
that means it will be a problem for us if you want 
to put the data those frames in a batches and when  
uh the number of frames are not equal in all the 
videos then we will face a problem you know when  
you want to stack them into batches then there 
will be a problem so to solve that problem we  
will use uh padding okay so padding simply means 
let's suppose one video have 10 frames but the  
other video have 12 frames so we will put zeros i 
the video which have 10 frames we'll put two zeros  
in that video we are doing some padding now the 
number of frames are equal in both the videos okay  
so this is what padding is and what what we are 
doing is we are doing one thing we are fixing the  
uh maximum frame count okay so means for example 
we are saying that number of maximum frame should  
be 20 so we are going to pick 20 from frames from 
all the videos so this is the maximum frame okay  
so of okay so this this is clear now over here 
you can see the image size is 224 i have fixed  
and now i have two functions one function is this 
and second function is this first function is just  
a simple one it will crop the center square and 
this function is important load video function  
this function using this function will load the 
video to our algorithm and you can see that we  
are using the video capture over here right it 
will read the frames it will read the video and  
it will you know read the frames of the videos 
okay and after that we are using this crop center  
square function over here and we are resizing 
the videos and then we are storing the frames  
and this frames this frames is a blank list 
over here we are storing all the frames okay  
so this is how we are reading the video using this 
function we will feed the video to our algorithm  
now feature extraction now we want to use 
feature extraction right why we want to  
use feature extraction because remember i've 
told you how we perform image classification  
we read the input image we extract the feature 
and then we classify the image on the basis of  
the extracted feature in the same way for 
video we read the video after reading the  
video we extract the frames and for every frame we 
extract features right so now this is that feature  
extraction function i have created one function 
and under this function we are using inception  
v3 now this inception v3 it is a pre-trained model 
we are using this pretrained model we are using  
transfer learning over here using this pre-trained 
model we are extracting the features okay so this  
is a simple we are using this inception v3 model 
is trained on um imagenet so we are using that  
and in this feature extractor variable we are 
calling this function okay so this is the cnn part  
now guys remember that we are labels whenever 
we work on machine learning or deep learning  
problem we know that our algorithm understands 
the data in the form of numbers right but if you  
remember dancing exercise and yoga these are class 
names labels these are uh words these are strings  
right so we have to convert these strings into a 
numbers now so for that we are using this string  
lookup layer from keras and we are converting 
our data into numbers you can see earlier uh  
before encoding our data looks like this and 
after using this string lookup layer now our  
data is converted into zero for dancing and then 
we have one for the second class and then we do  
for the third class now our data is converted into 
numbers okay now after that so these are the basic  
hyper parameters like uh image sizes this bad 
size i'm using is this number of epochs are this  
max sequence length is this and number of features 
are this these two things are important because  
now we are going to use a rnn network right so for 
that uh guys if you remember if you have started  
recurrent neural network in that also we you 
know pad the text right we pad the sequences  
we pad the text to make the equal same length of 
sentences so in the same way we are using the max  
sequence length 20 if the number of frames are 
less than 20 then we will fill the zeros we'll  
use the padding and the number of 
features we are using is 2048 okay so  
guys the functions this label encoding and 
everything this um converting into a frame  
all those things are i have prepared one 
function in this function we are you know  
putting all that data together so this function 
is that only which i have explained you just now  
okay so over here we are printing the results 
the results means how much data what kind of data  
we have in train set and what kind 
of data we haven't tested you can see  
frame features in train tests uh train set 
is we have 145 videos in training remember  
in the beginning i've shown you that in total 
from all the three folders we have 145 videos  
this 20 means remember we have uh use this max 
sequence length so this is 20 that 2048 at the  
number of uh features okay so now these are the 
frame masks and then we have train labels we have  
145 and um 140 we have 145 videos and we have 
one column corresponding to that which stores the  
labels to that and this is we have 22 videos 
and these are the this shows the labels okay  
now we are using the sequence label the rnn part 
so this is the function which is doing that work  
so what we are doing is so frame feature input 
and mask input these two variables are important  
these two variables are the input toward this rnn 
network okay so what we are doing is over here  
max sequence length is 20 number of features 
are two zero for it this we have explained  
above right we have mentioned these things above 
okay and mask input is this max sequence length  
now these values over here you can see 
our model so frame feature input and mask  
frame feature input and mask input we are 
providing it as a to the first layer of our  
algorithm and then we have further more layers 
and the final layer is activation layer softnext  
activation function we are using okay and 
then over here in this rnn model variable  
we are calling the model we are compiling the 
model and then it will return so this function  
is our model code rnn code now we want to run 
our algorithm we want to perform the training  
so for that we have created another function with 
the name of run experiment and under temp folder  
you will get a video classifier folder where the 
model will be stored this is the check points and  
we are calling get sequence model remember this 
is get sequence model so we are calling the model  
we are fitting the data in it we are providing 
the training data to it validation split is  
this and number of epochs we have i have told 
to uh i have decided to run it for 30 epochs  
and the checkpoints it is taking from here right 
so now our model will run now you can see after  
30 epochs this is the accuracy we are getting 
i know this is not a good accuracy but remember  
we are working on very less videos okay so now how 
to test this so for that we have two functions one  
function is this and second function is this this 
first function is responsible to prepare our video  
in the format which our algorithm will accept that 
means to if the if the number of frames are less  
so for that we are using this max sequence 
length okay and we will put the number of zeros  
to pad the frames and these kinds of steps are 
here and after that in the sequence prediction  
we are calling the model and we are making 
the predictions okay so you can see we are  
loading the video after that this frame 
feature and frame mask is important because  
these two are the input to our algorithm 
okay and sequence model dot predict right  
so over here we will we are calling our model 
sequence model and we are predicting and finally  
you can see here i'm showing you the output 
test video we are randomly picking the video  
and here we are showing the path of the video this 
is the path of the video on which our algorithm is  
predicting okay and then sequence prediction this 
function we are calling this function which have  
the functionality to perform the prediction and 
on which video i want to form the performance  
the test video this and you can see the 
result over here right now let me show  
you the video it is uh saying dancing thirty 
three point four two percent exercise is this  
much percent and you guys this much percent the 
this video what this video is this is the video  
so for this video our algorithm is saying 
it's 33.42 as dancing video and these are the  
other two results so guys this is what this video 
classifier is and i'll share the github link right  
in my description section you can get the code 
from there and you can prepare your own data set  
just download some random videos from the internet 
and you can make the folders and put those videos  
in those folders and then you can try this so i 
hope this video is helpful thank you for watching