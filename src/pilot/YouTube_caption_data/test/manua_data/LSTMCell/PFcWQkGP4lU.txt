 Alright, so we are now slowly getting to our code
 implementation of the character RNN in pytorch. And like I
 mentioned at the end of the previous video, we will split
 this into two parts. So this video will be on some
 conceptual aspects about the implementation, some, I would
 say things that help us understanding the implementation.
 And then the next video will be the actual code example. So
 there's one thing I wanted to talk about, that's the LSTM
 class. We have used that before when we implemented the RNN for
 classification. But yeah, there was a question also on Piazza,
 and I made this ugly drawing, trying to better explain the
 inputs and outputs. And I realized this is like one of the
 really obscure things in pytorch. It's not so obvious
 what's going on there. Because yeah, there are lots of inputs
 and outputs to the LSTM. And I wanted to explain this again for
 everyone. And spoiler, there's also an LSTM cell class that I
 also wanted to explain, especially the difference
 between the LSTM cell and the LSTM class in pytorch. But yeah,
 one thing at a time. So we used this LSTM class before when we
 implemented the RNN for classification. And yeah, there
 are lots of things going on with that one. So here, just looking
 at the example, they initialize the LSTM. So there are three
 values. If we look up the values here in the usage, it's the
 input size, the number of expected features, essentially
 the hidden size, and the number of layers. So usually, well,
 not usually, but previously, we used only one hidden layer. But
 we can also, of course, use multiple hidden layers, I will
 have a visualization showing you how that looks like in the next
 slide. But you're focusing on that. So we have, let's say,
 initialized our LSTM. And then here, it's getting used and it
 receives two inputs. So the input, and then the second input
 is this tuple here consisting of h zero and c zero. So what are h
 zero and c zero, these are our initial hidden state and cell
 state here. So here, they use random numbers for everything.
 But usually, we initialize the initial states with zeros. But
 here, it doesn't really matter here, they just want to try or
 just try illustrating how, how inputs might look like from the
 dimension perspective. So what, what they essentially look like,
 what dimensions we need, in any case, so there are these inputs.
 And also as output, there's an output here, and also a tuple
 with or called HN and CN here, where this is the hidden state
 of the last time step, and the cell state after the last time
 step. So I had this relatively ugly drawing and Piazza, I
 actually searched on the internet a little bit to find a
 better visualization. And I found the following here on stack
 overflow, which illustrates this nicely what everything here
 is. So the input here, which goes into the RNN can really
 think of it as your data input. Whereas this whole, so maybe use
 a different color, this whole thing should represent your LSTM
 where here, this is the depth, the number of layers. So this
 would be layer one, layer two, and layer three. So previously,
 in the code examples, we only used one layer. But of course,
 yeah, we can have multiple layers, it's just, you can see
 that it's just one layer on top of each other. Right. So
 previously, we only had in the actual code example, in the
 lecture, we only had this, this one hidden layer. But we also
 talked about that we can have an arbitrary number of hidden
 layers, similar to how we can have an arbitrary number of
 layers in a convolutional network or in multi layer
 perceptron. Okay. So this is our input here at the bottom.
 Right. And now h zero and C zero, we also use a different
 color, maybe yellow. So these are here, our initial states
 that go into the LSTM. So here, the two is for the number of
 layers. Okay, and then the output from the RNN, this is
 also for each time step. So there's one output for each
 time step, similar to how there's one input for each
 time step. In addition to that, it also returns here, the
 hidden state and cell state for the last time step. So
 technically, what we could do is we could have, let's say
 another RNN call, say, some input sequence to together, so
 like we had it here, together with now, get these H n and C
 n, if we wanted to, for example, if we want to reuse that part,
 for example. Alright, so, but this, this is conceptually what
 we get out. So this, and this, these are our outputs. And this
 is our input here. Now, the LSTM cell class is, it's kind of like
 part of the LSTM, it's a smaller unit, like LSTM cell is, is
 only a small unit. And we can actually use both on either the
 LSTM or the LSTM cell for implementing the character RNN.
 And I've used both. And I will also provide you with code
 examples for both. But to be honest, I think using the LSTM
 cell class is a little bit more natural. Because the way we
 compute the labels, it's a little bit more easy to do that.
 And also for generating data. So, the LSTM cell, how it works
 is that it is just representing one small unit of what you have
 seen before. So the LSTM cell is naturally only one layer, instead
 of, of setting the number of layers, we would have to stack
 them on top of each other. And using the figure that I showed
 you before here in the red box, this is essentially what the
 essentially what represents the LSTM cell. So it only receives
 one character as the input, then for the one layer of the hidden,
 the initial hidden and cell state, and it only outputs one
 output here, and then the hidden and cell set for the next state.
 So why that is more useful is in a way for computing the loss
 essentially that you can get one thing at a time, essentially
 instead of running the whole thing. And if we would use the
 LSTM class, we would have to provide one character at a time
 as input. And it's kind of wasteful doing that. So handling
 wise, I find the LSTM cell a little bit more easy to use for
 this purpose. But of course, you can use both you can. You can
 also for the character on and for example, run this character
 on in here with only one, one input here, multiple layers, you
 can also run it like that by only having an input of one
 input. But if you if we want to do this in the first place, we
 can also just use the LSTM cell and use use that one. It's this
 smaller unit here, essentially, if we was would only use one
 layer, we would then use this output here, put it to a fully
 connected layer to get the class label predictions, and then
 compute the loss and then go on to the next character and so
 forth. And here we are going to do that in the code example in
 the next video. So one more thing, we are talking here
 about the many to many where we have all we want to create text,
 so predicting the next word. If you are interested in another
 type of many to many architecture, for instance,
 language translation, I found they have actually a great
 tutorial on the pytorch website. So we are not going to talk
 about language to language translation. Because yeah, there
 is already a great tutorial. And it would be kind of boring for
 me just to reproduce this tutorial. So maybe if you're
 interested, you can just check it out and work through it step
 by step. So the next video we will oops, in the next video,
 we will focus on the character level RNN generating new text,
 and we will be using this LSTM cell class.