hi everyone thanks for joining
let's code in lstm on some yelp reviews
this video is going to go over
basic data processing and the theory
behind these models particularly into
word embeddings
and how they can be actually used to
identify similarities between different
words or even different documents
we're also going to go into a little bit
of the math behind lstms
part 2 is going to actually go over
coding these lstms in pytorch but right
now we're going to focus on
preparing the data and get everything
set up for that experiment so let's go
ahead and get started
so this is actually from a yelp research
data set which you can download in the
link in description below
uh but this is ideas as they they
release this really large data set of
different reviews and we're going to go
ahead and just grab these reviews
um and so one of the files that we're
going to look at is this review.json so
we'll go ahead and take a look at it now
so once you download the data the first
thing i usually like to do is using a
editor like ubuntu
in windows or powershell in windows or
you can just use a terminal on mac it's
actually navigating to the folder
and then actually just using the
directory tools to check it out
the reason i like doing this in sort of
uh this terminal and not
on in python terminal is just because um
if i do this in in on the ubuntu or on a
powershell in terminal then it's a lot
faster
and that even if it i have a really
massive data set of 100 million files
i can still end up um
i can still end up loading this and
checking out what this looks like pretty
quickly so just an example this is a
we'll just say more academic data set
here
um and these commands should work with
really any of these so
so we see that we have um
each line will indicate a different json
file we have like a review
a user id a business id the number of
stars
um if they're useful cool text so maybe
i can try to zoom in
a little bit here but essentially it
looks like yeah we're kind of most
interested
in let's see i think we want business id
because so one thing i didn't notice
about this data set is that a lot of
that there are uh you know only so many
i
there's like 200 000 businesses and 8
million reviews so when we do our
training testing splits we need to make
sure that our training data is actually
independent from our testing data so we
can display on the basis of business id
not the individual review i think this
is pretty important because you'll get
really biased error results if i just
naively split on the
review instead of on the actual
businesses so keep that in mind for next
time
we also have like stars so we're gonna
need the stars
i think the business id and then looks
like uh this
text column is it just called text
um yeah it's just called text so go
ahead and look at how to do this in
python now
so let's actually take a look at our
data in python so we can just start by
saying import json
and because we you didn't need to
necessarily look at it using the
powershell or the ubuntu or terminal
like i had done
before just now um but it did give us
kind of a better idea of how to really
interpret it because i can i'm just
going to treat it like each
each line is a is a json object
essentially
exactly what it is so we can just uh
define some empty list as data here
we can say using data we can say with
open the name of the file yelp
academic review as
file and then for for every line
in that file
for every line in that file we can data
append we can append to our list
the json low so we want to load this as
a json
file
this should be line
so yeah so we i should have said jason
loads the
the line and not the file uh so yeah
essentially we just we open have the
json object where we have an empty
list and then we're going to open this
file
as as file for every line in that file
we'll append json
this is actually how you can do a lot of
these sort of review data sets if they
come
like json files so it's going to take a
little bit to load we'll come back when
this is done
so now that we've looked at the data
let's look at what we actually can do
with these reviews
so these reviews range from one to five
stars and we're just kind of kind of do
the most simple tasks we can think of
we're just going to predict the star
rating from each individual review
and so we actually do have a little bit
of a problem here because
if we're trying to make this like a
sentiment analysis classified
essentially predict
positive or negative uh what what we
should do with sort of the neutral
reviews kind of becomes
an issue because you don't really want
to train anything
on nothing that you don't want to like
train any neural network or any gorilla
model on something that you're not very
confident
in necessarily the result and this idea
of these neutral kind of reviews aren't
really indicative of anything they're
kind of just
you know not really that informative and
so when if we're really trying to make a
review that's you know in the real world
which you would think about as you would
make this sort of algorithm in order to
make a single binary decision should we
do it or not do it or something like
that so in that case it usually makes
more sense to just kind of
uh train on only the things you're
you're certain of so that's about what
we're going to go ahead and do here
we're just gonna pull out everything
that's uh at or below two stars and call
that a negative review
or everything that is up at or above
four stars call that a positive view
then we're just gonna throw away
everything that's not
really in those uh in between those two
so let's go ahead and do that now
so now it's loaded we can just go ahead
and take a look
so if we just return you know one of
these objects
uh we have a review a user id everything
that we saw in the previous
example um so
to in order to you know this is each one
of these is a json object so we just
would say you know to get the text you
just say
text that's the text then we would just
say
what are the stars and then what's the
last one we need
business id so let's go ahead and get
each of these so we'll just say
bid and this is just kind of more or
less basic python
but we're just gonna just find some
business id
list we're gonna define some stars
and i bet there's probably a much better
a more efficient way to do this
um but i i
i'm really i'm kind of lazy i i would
rather just
have something run and maybe take a
while than actually
um
spend like a lot of time optimizing it
for it to run super quick i'd rather
just you know
leave it running and go cook some dinner
or something but i'm sure if you're a
computer scientist you can you can make
this a lot easier maybe probably
even if you aren't but uh this will
hopefully just be very transparent and
we'll see so essentially what we're
doing is we're just gonna append
to each of these lists but we also want
to check the condition remember
um so we really don't we will not only
pull the
positive or the negative reviews
out so we don't want all the reviews
it's not really
informative so we'll say something like
um
we'll say star equals data
should just be i um
stars is the stars or
i think it is stars so then
if if star is
greater than or equal to 4.0
then sure then i'll bid appends
and i'll just copy and paste this here
so so
seriously i'm asking the question i
shouldn't done this i shouldn't have uh
run subject i can just rerun that but
essentially i'm going to ask the
question
of if it's if it's above
if it's above four stars so if it's four
stars four and a half stars or five
stars
i'm going to pin the business id i'm
just going to append a 1 to be a
positive label
and then i'll pin the text as well
so this will just be i text
and i'll just do the same thing if it is
lower than two
so this way we're only pulling out the
positive ones and
of course append a zero so now if you
look at this
it should work um
i i'm checking the stars if the number
of stars
is above or equal to four or if it's
less than or equal to two i'll go ahead
and keep them
so we'll let this run for a second
so just as a double check we're gonna
let's see how much uh
how long each of these are these all
should be the same length so business id
it's like 7 million so it's pretty good
we still got quite a bit
even after removing some of the uh
more neutral stars
cool so all these are the same length uh
so everything looks good here
so if we just want to let's go ahead and
just take a look at some of these um
dismal lukewarm system they they didn't
really enjoy this i suppose
know so this should be what do we have
star should be like zero
yeah so it's it's pretty nice i guess it
worked out we were able to successfully
extract these
so let's go look a little bit more in
depth into how these are our really uh
structured using word vectors
so now that we've actually cleaned all
of our data let's we can actually start
thinking about what we're going to do
with this lstm and actually making this
classifier so we need to do right now in
the rest of this video
is we're going to prepare this data for
the lstm that's going to involve
tokenizing the text
we're also going to do some
visualization of the word embedding and
take a look at those
and we'll go through a little bit of the
math behind lstm
one of the first things we need to
prepare our data for our neural network
is we actually need to tokenize and
remove the stop words from this text
so we'll go ahead and take an example of
this kind of string i am sam i like ham
if you just think about what a
tokenization method would be essentially
just splitting these up into different
uh characters into subsequent tokens
um so now this is just six different
tokens instead of a sentence
uh we understand that there is some kind
of you know sequential structure to that
we'll talk a little bit more about how
lsdn will actually
incorporate the fact that you know i
comes at 4am and that comes before sam
and all that and the other thing we're
going to do is actually remove these
stop words
and so these stop words are just an uh
word that they kind of use for
sort of common english words and they
kind of just remove these uh they tend
to not be very informative for a lot of
tasks
and so sometimes the model actually
takes is improved once we
remove these stop words so we'll go
ahead and do that as well
so what we need to do is feed a raw text
into these neural networks and somehow
represent this numerically
and this is what actually word
embeddings do and so these word
embeddings are just
essentially learned representations of
words
that are trained based on other models
that which are trained
to predict the context of a given word
so they're trained on a large amount of
data
usually you can have something from
wikipedia or twitter or pubmed or some
kind of financial statement
so there's a lot of special uh
dictionaries so i
definitely recommend if you're doing
something in a given field and you're
trying to do some kind of classification
sentiment analysis
you really should look into using word
embeddings that are trained on
the same sort of data that you're
looking to apply it to but this idea is
that they essentially learn for each
word a vector representation
that is some numerical representation
that represents the context of all of
each given word
and these in the fact that these kind of
end up being vectors give them a lot of
very interesting properties
and one of the properties that vectors
have is you can just add vectors
together
and you can also compare vectors uh to
find you know their distance from each
other
and that's what's using coastline
similarity and so
not only if you recall not only can we
represent words as given
vectors but we can also represent entire
documents as a given vector
and this is just the average this kind
of goes into the again the additive
nature of these vectors we can just add
them together and find the average
vector
that represents this document and so
this can actually be used a lot for
recommender systems or kind of
recommendation systems or finding
similar
documents to each other if you're doing
some kind of tasks like that
you could really just use something as
simple as just using the cosine
similarity of the average word vectors
uh for a good good word embedding
dictionary that you found
and using this will actually help you
find the distance so if you recall just
some basic sort of math here
if two vectors are lying on top of each
other the cosine
of that angle is going to be one
essentially so that's the maximum so
this this sort of cosine of theta is
maximized at one
uh when they're right on top of each
other and all the cosine of this angle
the angle between those two vectors
represents sort of this similarity
measure
and so vectors that are closer together
have a higher similarity than vectors
that are further together
and so you we see things like clustering
by genre for example for given documents
or clustering by similar words and we're
going to go into our reviews right now
and actually see how some of these
embeddings look and check out some of
the relationship that these word and
bangs are able to give us
so now we're going to actually look at
using some pre-processing taking a look
at the word vectors and some similarity
measures
uh with our data so as we recall we had
our three different lists that contained
the text of the reviews the number of
stars are essentially
positive zero or one positive or
negative and the business id
and so now we're going to import these
two packages i have some installation
instructions in the code
uh below but this is just spacey and ltk
but these are pretty
good packages for nlp work in general
they have a lot of really powerful tools
and so what we do is
is we're going to use the nlp model
we're going to say our nlp is equal to
spacey
uh load and then we'll go ahead and
choose whichever one we downloaded
because we want the word vectors we're
going to use the larger one if you don't
want to use all the room you could just
use the smaller one either one is fun
um and so we have it here
so we're loading that as our nlp model
um so it might take a second
while that's running we can also so so
the way that this works is essentially
we'll be able to call
this function nlp and we'll be able to
say you know on any given review let's
say we have the 33rd review
and we just want to look at the text
it'll it'll actually um so now
it'll just return this text of the sort
of nlp model
and this is actually really kind of nice
and convenient
and we'll show a little bit more so the
first thing we actually want to do now
though is actually remove these stop
words
um and so before we
we want to for each of these nlp models
we want to remove the stop word so we're
going to say
um
so if we want to take a look at the stop
words i think it's the command of
something like this
uh so this so the stop words are you see
the different one that's like i
me myself and ours and so you can use
these software and so we'll take a look
at how to actually
remove these from the text right now so
what we're going to do is we're just
going to
use that same list of stop words is
we're going to just call that into a
single variable called stopwords
and then what we can do is we're just
going to essentially loop over
every single thing uh well we actually
need to call this
a document first so we're going to use
this nlp object we're going to call that
some document here and so now if we
recall we just have this nlp object
uh document and we have our stop words
list
and so it's in order to just remove
these this is more just list pretty
basic python so it's like for
text in document
um
[Music]
you have to do really this or something
like you have to do print text text
because the that the text that that's
actually to get it to be the token
so we can say if text
so we'll do it like this so we'll
essentially just say if the token text
is not in the stop words list
um then we'll we'll append that to to
another list
and you can see how we'll just do these
for each of the um
i think we really need to stop this so
we can rerun it i think it's being a
little efficient so no stop
so essentially yeah we're just gonna
define an empty list
uh for every token in this document uh
we're gonna
we're gonna see if that text is in stop
words and if it is
uh not in there then we're going to
append it so that's our way of removing
stop word so we'll run that
and we'll come back when this is done
and so now if you just see so
after that ran we just see we run no
stop uh essentially we we get the same
list except without any of the stops
without any of stop words so we can use
you know with any other
another review and we get a different
one without uh
stop so what i want to talk about next
is actually using is using the
nlp tool uh and spacey to actually
uh compute similarity between two
different documents so say we had you
know this document we'll have document
one
and then document two uh so these will
just be
you know different documents we wanna
figure out the similarity
of these documents so let's go ahead and
take a look so document one
it's a wonderful nail shop eric is the
owner so hospitable and i take his time
your effort
on your nails to perfect them i love my
nails they're
so cute and symmetrical well so so then
next uh
we have another one who knew i did i
remember that meaning eating times in
the early childhood days back in
chipotle and and denver all right so
these are kind of different but we want
to figure out really how different they
are
um so actually using the whole document
we can use this this just a function
and to to find the document from
document two
what's the similarity to document one
it's as easy as that and we get some
number here so we could do something
like uh
actually computing similarities again
this is really helpful for
determining uh you know different having
a document what are similar documents to
that
so you don't even necessarily need to do
an lstm uh for some task and something
like this might be actually
a really useful and really powerful way
to take advantage of a large amount of
data
and finally i just really want to
demonstrate really uh so that you know
so
you really each if you recall each of
these are our document each of these
documents are now represented as a
vector
um you know the length of this of this
document vector is going to be
300 or they have different ones that are
200 or 100 or whatever
but it's the same as each of the
individual words so for me if i just
return these vectors
the documents we get you know 300
different numbers the same thing happens
if i if i take an individual word so if
i say
uh doc one for token
and doc one i want to print i want to
see
you know print token i'll say the length
of token
uh vector and so each of this is now the
word vectors for each of these
individual words
um and so you know uh each of these
words
as you can see here uh have have a given
uh word vector representation this is a
wonderful
nail salon so you know this is a and
these are each 300
units long so i think it's just really
important to understand that these are
you know vectors
so this is actually how we're going to
feed this into our lstm next time
okay so now that we finally have our
vectors we need to think about kind of
an intelligent way to sort of combine
them
and for certain sort of tasks like
similarity measures uh
it does actually end up being very a
very useful and very kind of
a good way to go is just by averaging
the individual word vectors and to get
some document vector
and then comparing those but when you
think about doing you know something
like classification
you actually want to really think about
the fact that you're adding that you
have a bunch of word vectors and they
have some sort of time structure to them
we understand kind of language
and sentences you read them right from
left and or in other languages
left to right in other languages you
read them right for left but you have
some sort of uh structure
that you kind of read and so we want to
really understand these word vectors as
a time series so how do we actually
analyze that
and that's through a method called lstm
and so lcm's really sort of notoriously
difficult topic to teach
because it tends to be very complicated
if you've been looking up
at this topic you've probably seen a lot
of people use this sort of depiction in
these pictures
and i actually don't find the pictures
very intuitive so i think the
for me the the easiest way i actually
understood this was by just by looking
at the math
and so if you look at the math here
essentially you just have to think about
what are the parameters that go into an
lstm cell
and so this idea is that there's some
function that represents some forget
which represents how much to forget from
the previous cell state
there's a function that represents uh
how much of the previous the current
input
is actually related to so so we have
again forgetting from the previous state
a function that represents the current
input a function that represents the
current output of the cell and how the
output
actually operates on the current cell
and the input and then how the actual
cell itself will change
and i think it's really intuitive if you
just look at these last two months like
this bridge is pretty complicated math
but the idea is essentially that we have
this idea we have a cell state
and the cell stays dependent on some the
forget function
this element stands for some
element-wise multiplication
and so we have some forget function
which forgets which multiplies by the
previous cell state
um t minus t minus one and we add it to
that to the uh input times the current
cell state which is the c
of this volatility thing um and and so
we actually
uh get the current cell state as a
function of the forget function times
the
uh the the previous cell state and the
input uh
times the current cell state which kind
of makes sense right we we
forget some of what we learned last time
but we also want to retain some
and we also want to incorporate our next
decision based on the current input
and then what we do is we have another
function for our output what what how do
we actually determine what the output is
times this is a sigmoid activated cell
stage so essentially just at the
activation function and this is
essentially
the way that i was kind of negated at
the output level and so
i'm not going to go really too much more
into lstms and i highly recommend
there's two really good papers that i'll
leave in the description below the
original paper and there's a 2017 sort
of review
which i think are great uh resources if
really interesting in the lstm
but essentially you just really want to
understand the idea is it is this sort
of sequential time step
and there are functions that represent
you know for getting the previous time
step as well as
uh incorporating the previous input and
how to output that to the next sort of
time steps
um but the other thing is you know all
these all these sort of operations
are differentiable and they can be back
propagating that's you know why we're
able to actually use them in the neural
network so these are all very nice kind
of functions
and using this sort of way we can kind
of make this way that that these very
smart
uh functions that are able to kind of
learn and essentially will feed the
words one at a time
and and at the end that once we we
finally have the final cell state
to actually generate the prediction so
we're going to go ahead and look at
applying that
in the next video
thank you all so much for watching as
always i really appreciate it
definitely help when you help that long
short-term memory of youtube
be able to look at a title text actually
be able to understand
how to actually go through that as word
vectors and see hey this guy likes this
video and subscribes his video he must
want to pay this person's rent because
that would really help a lot and
obviously the algorithm sees it i would
really appreciate it so
thank you guys again for watching see
you next time bye