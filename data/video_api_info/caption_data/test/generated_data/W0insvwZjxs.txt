uh hello everyone
um basically yeah i'll be talking today
about
uh as as mentioned on one of the
original authors
but you know the other list at this
point is very long and there's a lot of
people both from facebook
and from uh outside from various
academic institutions who are just
and random kind of open source
contributors uh who are working on the
library right now so
uh you know at this point there's a lot
of stuff there
um so i think there are a few ways we
can
do this talk because a lot of like a lot
of it depends on
kind of what how much do you know about
the library if you don't know very much
i can just do a kind of introductory
talk if you know a bit
i can kind of give you without like a
lot of prerequisites i can give you an
overview of like what are the
current directions that we're taking and
uh what are kind of the few
the future features that we're planning
for the library that we're developing
right now so
how many of you maybe have used pyth
perhaps with these ones who have wrote a
script in piper
okay so there's quite a bit of view and
how many of you have heard of pipers
like you've probably
use tensorflow or something else so you
generally know what hydrogen is
but okay okay yeah
um all right so um
probably since like half of the ovens
perhaps
uh has did has done something with the
with
hybrids for similar libraries um we'll
probably do
um the slightly more advanced thing but
don't worry i will also like explain all
of the basics so that all of
all the rest of you do not get lost so
basically if you google
online on what pyth is you will probably
see that it's a deep learning framework
but for though and this is kind of
where we started from this is kind of
the most important use case we're
thinking about but this isn't really not
everything it turns out
that like the needs of deep learning are
actually kind of aligned with a lot of
needs from scientific computing so um
for the sake of kind of introduction
uh you can really think about piper as
if it was numpy and really probably all
of you
if you've done any kind of data
processing in python
you will know numpy so at the lowest
level in case you're just going to recap
numpy is pretty much a library that
gives you
a way to express multi-dimensional
arrays right so
you have pretty much just those nested
lists where kind of the lens so the
lists have to be regular in each
at each nested level um and then you
have some kind of type of an object that
you're
that you're holding and numpy lets you
hold arbitrary python objects
towards the sum of a restrictive we only
allow you to hold numerical objects so
either
integral or floating point values plus
some
special special cases like complex
numbers and um yeah and
and basically what you can do is uh a
lot of
there are a lot of operations on those
arrays that you can perform right
like you can do for example addition of
all of the numbers in those arrays it's
just a single line and and the reason
why numpy is great compared to just
iterating over regular python lists is
python basically uses a highly
inefficient representation from
for data that he would call like just
any overall list numpy
and python will use kind of the native
data format of your computer
to pack the data into memory efficiently
and also kind of
utilize pretty much you know all of the
resources you have available to kind of
get out the peak performance if you have
a lot of data to process and of course
you know later you can
you can slice those arrays like in this
line that assigns to d
so this for example takes the first and
the third column node of the matrix
and assigns it to d you can you know
check the shapes you can do matrix
multiplies with like random matrices
as just at the end um
and you know at the lowest level again
high torch is exactly like this so
python's at the lowest level
it's pretty much like numpy but it has a
lot of
nicer features so the syntax might be
slightly different because of legacy
reasons
it's kind of a matchup of numpy and
matlab at this point
for weird reasons but anyway um i think
it
is it shouldn't be too hard to learn if
you just look up
you know functions in the documentation
you should be able to find things easily
but yeah
of course you know there is more just
redoing numpy wouldn't be very smart so
uh there is more so first and foremost
it's still tightly integrated with numpy
because this is pretty much the kind of
primary language that every single data
science
package in n-pipe in python speaks right
so
you have one liner so that you convert
between python to rays and numpy arrays
and the important part about this is
that uh
the cost of this is pretty much constant
no matter how many elements you have and
this is because
the data is entirely shared like if you
modify the python array the numpy array
will be modified as well and vice versa
so
this basically means that like this is
nice because
if you want to use some of the python
features that are missing in
numpy or like in another project which
kind of can also interface with numpy
you can just import like torch in a
single place you can kind of add a
single function boundary you can just
kind of convert your inputs which are
numpy arrays to buy their
tensors you can do some compute on this
and then you can again
convert them back to numpy arrays and
those conversions are pretty much free
so this way you can kind of start
progressively porting if you want to use
some of those features
you can start progressively porting
existing code bases which like depend on
numpy or some other
or some other packages that use it to
to you know utilize hybridge at this
point
and the primary big point which is
really missing from
uh numpy is accelerator support uh so
you know we've heard a lot of
good things about nvidia gpus and this
is really kind of also the primary uh
the primary accelerator i have in mind
when i'm talking about this
there is a bunch of work done on amd
gpus and also custom accelerators like
tpus and so on
so those things are generally kind of
still developing but they are there and
you cannot and people are actually using
them at this point
and python makes it really easy to use
those things right because
this is a this is pretty much a script
that just samples like
uh two two by two arrays and then just
adds them and prints them right if you
want to
do the same thing but you want to
perform the addition on the gpu for
example
uh you can do something like this so if
you
the random constructor it pretty much
will sample
an array where like every you give it a
shape and every element will be like
independently sampled from the normal
distribution
um so you can say that well okay but i
want i want the data to live on a
particular device
and this is what you specify with the
device argument and similarly if you
already have an
array which is kind of you know the way
you allocate why
uh this this is an array you didn't
specify the device so by default it will
be put on the cpu
you can also use the two method to for
example move it to a different device
and another nice thing is that um if
since you know generally for a lot of
those use cases you're dealing with
large data gpus are kind of
perfect for this kind of array
processing gpus are basically like
heavily parallel machines they have a
lot of cars but they're much simpler
than cpu cores
uh but they're kind of geared towards
numerical
computation instead of like heavy
branching and you know traversing data
structures
so this maps perfectly pretty much to
everything that we need to do
in libraries like piper and the line
basically at the at the top of the
of the slide it will tell you okay if i
have
a a cuda gpu available on my system
let's use this for my compute
otherwise let's just fall back to the
cpu and this is nice because
this single line pretty much will enable
you to like experiment on your laptop
which might not have a good enabled gpu
and then just send the script to like
more powerful server and then it will
just automatically trigger and start
using the gpu
so and and you know the important part
is that all of the operations are the
same like the
all of the operations are kind of
overloaded on the on the device that the
eraser on so all of the inputs to a
single operation have to be on the
single device
but it really doesn't care too much on
which device they are
it will kind of automatically adapt if
you give if if you give to the plus
operator if you give two arrays which
are on the gpu
this will run on the gpu as well so it's
really easy to kind of
write code which is completely agnostic
over basically pretty much any pyth code
will adapt no matter what kind of device
as long as you kind of set up and put
correct arrays on the like if you have
parameters you have you have to move
them to a correct device
but once you do this the code is exactly
the same no matter if you execute
on on the cpu or on the gpu
all right and the next feature which is
also kind of
missing from numpy which is very crucial
for
machine learning is really high
performance automatic differentiation so
all of machine learning is pretty much
gradient-based optimization
and and writing on gradients by hand is
not a funny thing really to do
especially if you're dealing with
complicated models right
so there is this technique called
automatic differentiation which
basically allows you to
only write the function of your model
and you know since
the differentiation is pretty much you
know there's just a bunch of rules which
you have to apply but it's kind of a
mechanical
uh a mechanical process this can be
entirely automated and this is something
that patrick does for you
as well so for example you know as an
example here you have this polynomial
function which is really easy to
differentiate in your cut if you want to
and basically what python kind of tell
us which
arrays you will you want to
differentiate with respect to and this
is this required strat annotation
id unfortunately is not free it
especially increases the memory pressure
uh greatly so you should not enable this
always but you know if you if you do
also do not worry it's not all that
expensive um and basically what you do
is you evaluate your function so you
evaluate for example your neural network
or the or
like a polynomial function or like a
simulation or whatever kind of numerical
code you have
and then you take the output so this is
the poly of x uh
in the in the like grad x line um
and you give the output of the function
to
to the charge autographed grad function
and you give the inputs to this function
and what this will do is it will
differentiate the output
with respect to all the inputs and it
will give you kind of the derivatives
as as the result so like if you print x
and if you print whatever you get
from the result of this differentiation
you will see that it matches and the
important part about this
is that this can differentiate arbitrary
python code so you can use anything you
really want
to implement this you can use car
routines you can use async and weight
like no matter
what kind of crazy python features which
would which like
few people use for implementing models
really this system will adapt to it this
is completely
of the source code you use you can use
control flow anything kind of goes by
so this is like the basic this is the
basic kind of layer of titration those
are really kind of the crucial features
and uh yeah apart from this like the
most
used the most heavily used things are a
bunch of kind of
helpers to structure machine learning
models especially neural networks
which i really don't have slides for and
here um i can briefly cover this
uh during our later tutorial but
just for now um you know we've hit 1.1
release lately
and um yeah and i just want to kind of
briefly touch
uh the base on what we what we're kind
of working on right now
and how we're trying to improve the
experience of machine learning research
and deployment so pretty much the
problem we've been trying to address
with one clientele
is that um you know previously piperage
is kind of
traditionally more used in the in the
research cycles and not so much in the
development cycles
and we were kind of unhappy about this
especially that it was used at um
at facebook a research and you know they
developed a lot of models and they
eventually
want to want to start actually
evaluating them online data
on the site right and previously this
meant that they pretty much have to take
any single model that the researchers
wrote
and they have to rewrite this for
example in cafe 2 which is like an
optimized
kind of framework for for for inference
right and so we wanted to kind of bridge
this gap and to make it much easier to
like
reuse the single code base you used to
do research and to later take this
and put this into into production and so
also
an important kind of mark here it's also
fine to like not use those features if
you're kind of running
on unload traffic it's completely fine
to like
um just expose a like flask api
use the regular python stuff it's people
are using python this way and
you know python is also used for writing
web servers right
and so this this works and so this is
really not strictly necessary
unless you know unless you're pretty
much a facebook scale
or unless you actually want to bundle
this with a mobile app for example
because you don't really want to bundle
the whole python interpreter like in
pyth
mod the definition of the model is
contained entirely is encoded entirely
in the python code and so you cannot
really separate them
very easily so if you were to deploy
this in a mobile app
you would have to kind of bundle the
whole python interpreter with you
and that's expensive and also like
mobile cpus are very slow so they would
this would be
like python is not very fast on the on
the kind of desktop class cpu it would
be terribly slow
on a mobile class cpu so you know the
the stuff that i showed you
before this is the kind of numpy like
stuff we're calling this eager move now
uh and you know it has a few benefits
it's really good for research and
experimentation because that's just
regular python
right you can use any kind of random
debugger and you can use print
statements whatever this just works
right
so it's really simple to write and debug
and iterate on this and it's still
pretty fast the problem is that it's
hard to deploy and also it's kind of
hard to optimize because there are some
optimizations
that can be done automatically but are
still kind of
painful if you want to do them if you
want to do them
manually right so basically our answer
to this is we've introduced kind of a
new programming language
but a programming language like you
don't really need to be afraid of it
because it uses exactly the same syntax
as python
and it has exactly the same semantics as
python the only difference is that it
does not support 100
of python because python is extremely
complicated and
like a lot of the language features make
it pretty much impossible to like reason
about
what your code actually does before you
actually run the code and this is
something which is crucial for the
optimizations
which we want to perform so like this is
pretty much just a very reasonable
subset of python so like you cannot
monkey patch
methods of classes for example at
runtime because this would make our life
so much harder but like this is not
something that people usually do
in their machine learning code so it
actually ends up
working out just fine for most of the
use cases
that our users are interested in and you
know generally you know
the as i said the most important feature
is going to work you have control flow
you have tensors you have
scalars you have print statements
strings function calls all of
the basic collections you can nest you
can define your own classes and we're
still kind of
ever expanding the the scope of the
language as
long as we cannot make the language kind
of inconsistent and impossible to reason
about
which is something that i will show you
later and i will be talking about also
later
so generally the question is how do you
take your eager program
which you probably use to like implement
your machine learning model
and how you change it to a script
program right and there are two ways to
do this which is
tracing and scripting um if you like
try it very hard it's not like you put
those you know functions in a separate
file
or anything like that they live really
side by side your regular python code
and if you just remove those kind of
annotations or those tracing lines
you will pretty much just fall back to
the python implementation and you will
be able to again use the regular
debugging tools
and everything so tracing is kind of um
conceptually in some way conceptually
simple but it also has a lot of pitfalls
which is why
i wouldn't recommend this for like a lot
of more complex models i think scripting
is like especially that the subset now
is somewhat well defined uh scripting is
actually pretty pleasant and we will not
cover tracing
during the hands-on tutorial we will
only be covering scripting but tracing
basically what it does
is your code actually does not even have
to conform to this website that i've
mentioned
the problem is that you will give us a
function and an example input and we
will run this function on this example
input
and we will kind of see what kind of
array operations you did inside the
function but the caveat
is that we only see the array operation
so if you have like something like okay
now read the first element of the array
and do this many
kind of iterations of the for loop right
we don't see the for loop
we will only like kind of record how
many times the loop executed during this
single
uh execution and we will never like even
check
if it should execute a different number
of times later so this sounds
kind of um dangerous but at the same
time it also makes some things
convenient so if you take convolutional
neural networks
they're pretty much just a sequence of
array operations right so it doesn't
really matter
what kind of python code you will use to
implement this
this kind of model what matters is this
kind of sequence
of of of different like convolutions and
non-linearities and kind of data flow
pattern
but this is just fine recovered by
by tracing so you know if i were to
implement
a convolutional neural network i would
probably do something like this so there
is
at the top it doesn't matter what dnn
thing is you don't need to know the api
exactly the point is
those things at the top they're kind of
the boxes that you saw in this diagram
right
and so you know since this is kind of
mostly sequential i'll you know skip the
skip connections for now if you just
have a sequential network i would
probably implement this like that so i
would like
depending on some command line arguments
i would kind of change the length of the
list add more
you know add more of those layers or or
kind of trim the list somehow
or change their parameters but then the
model is like as simple as just iterate
over this list and like just apply
every single box that you have right so
it might seem that you have a loop in
here so tracing is not really the good
choice
but this is really not the case because
if you just inline this like once you
fix the list which is only at the
beginning of the program
this builds up your model and it's
completely fine to like rewrite the code
as this
you're just kind of too lazy to write it
like that no same person would do this
you would just do a four loop right but
if tracing and rolls is this to
something like that if you're not so
if you're not going to change the
structure of the model
this code will do the same thing every
time so if you just record it once
what it did this will also be like a
valid transformation so tracing is
sometimes useful
if you take any model from we have a
torch vision package which is a lot of
kind of
pre-packaged data sets which you can
kind of load with a one-liner it can
even download the data set for you if
it's open
and we have a lot of pre-trained models
there so you can pretty much take any of
the
since the computer vision models are
generally kind of
simple data flow graphs with those boxes
that i've showed you
they're all generally kind of fine to be
traced so you can take any torch vision
models
you can trace it on an example input and
this should give you pretty much a good
representation
of your model but now running inside
towards script
um and another way is scripting so you
again
this time you are confined to the subset
of python but we do kind of recover
control flow and do some and like we
will check that you will actually you
are actually in the subset
like it either you know you don't get
any error and it works
or you get an error and it doesn't work
um so
you know if you're doing a recurring
neural network for example this is a
case
where you want you want to use script
like if you're doing machine translation
you will be
applying your recurrent neural network
over kind of different time steps right
because you have a
sentence of a certain length and
sentences will have different lengths
right so if you want to have a for loop
over the length of the sentence and so
in that case you really want this for
loop to kind of be
part of your model and this is this is
something that is captured by script
but you know of course kind of the
standard python features also work
you can do basic stuff like this uh you
can you know
use collections you you have all the
control flow basic stuff
uh collections can be arbitrarily nested
and
uh and yeah all of this works also
another note is that this actually
requires
type annotations uh ford script is like
statically typed you cannot really
change the type of a variable
so you have to annotate the types of
inputs
output annotations are actually optional
but then the rest from
this kind of annotation we're able to
check if the
rest of the program is valid and we're
able to enter all of the
types that are kind of occur in the
program later
and the important part really is that
they both mix seamlessly so like if you
put those script decorators and you like
call something that has been traced
before this will register as a
first class kind of call and also if
you're tracing something and you have a
scripted block inside this will actually
put the control flow
from that block into the trace thing but
really
the more important part is that this
still allows you to call back to python
so if you have a function like this
you of course you can call that function
from python it and it will behave
exactly as a python function
with that code would but but another
thing is that
from this function you can also call
other python functions so if for some
reason there is something which is not
supported
you can kind of momentarily escape to
python you know
run something like i don't know issue
http requests
and then kind of come back to a torch
script to like an optimized runtime
um uh yeah so you know you have
you have you have an example in here
like if you don't trust your random
number generator you can like get drop
out probabilities from a
say random source at every iteration and
that's like a valid for script
torso program apart from that you have
classes um
which which uh yeah apart from that we
have classes this is kind of taken from
it's a bit small but this is taken from
a real kind of research
project uh from facebook and really the
most important goals
um are yeah the most important goals are
twofold so one
you can take if you add those
annotations
uh to your programs uh first and
foremost you can take them you can
export them
on disk and later there is like a pure c
plus plus library which you can kind of
link to your
model apps or something and using this
you can kind of bypass the python
interpreter entirely
you can run those models in those model
apps in a resource constrained
environment and you know the checkpoints
are also very easy to kind of read
they're actually zip files
so you can even edit them but really um
you know since a lot of people in here
will probably be doing research
the more important part which is also
kind of underway so do not
expect especially the things that i will
show you later
do not expect you know kind of magic uh
to happen later for every single model
that you're implementing
but we're also working on improving
performance of the array based code
in python so one one case is that well
from first
we're kind of bypassing the python
interpreter entirely so if you script a
function
and it's kind of slowed down by the
python interpreter not by the
array operations it might get faster
anyway but this is not always the case
for a lot of so this is not always a win
i mean it's not it never makes it worse
but it's not always a win
um but the more important part is that
we have a lot of built-in center
operations and some of them for example
dispatch the optimized kudn library
so if you have a recurring neural
network for example
the korean library implements like six
variants of recurring rule networks
right
which is great because those are kind of
hand tuned kernels there's a lot of men
hours kind of that went into writing
optimized pretty much assembly kernels
for different gp architectures and all
that
and it's kind of hard to reproduce this
on your own right but the problem is
once you
once you want to implement a slightly
custom variation
of an rnn if you want to do research on
new kinds of
like recurrent sales the problem is that
you're pretty much faced with a
5x performance drop so i'm actually
tracking some
um benchmarks benchmark numbers from
high court
so you don't have to kind of the x axis
is time and it's get comments so even if
you could read this this wouldn't be
very helpful
uh the important part is that this is
time uh
this is like real time i said we're
developing the projects over this
timeline
and this is the time as in how long does
it take to execute the given benchmark
that we're looking at
and basically the purple line you can
see at the top over there
um this is like what would happen if you
re-implemented the coding implementation
using the regular python code regular
python code
the blue line here at the bottom like
later it's kind of obscured but in here
you can see that it started kind of
like as distinctly fast this is the how
much time it takes
to compute the purple function but using
coordinates so you can see that this is
kind of heavily
optimized uh compared to that and all of
the middle lines
what they are is that you know we you
can see that we've been kind of
developing this
uh this project as it went and what you
can see is that if you just add the
annotations for
scripting we will be able to rewrite
your rnn code to suddenly become
pretty much at 300 speed and the
important part in here is that
well okay this is for an lsdn network so
it's somewhat standard
but even if you change the equation
slightly this will still pretty much run
at korean and speed even though kurianan
doesn't really support this network it
does not have handwritten kernels for
this
so we're able to kind of generate code
dynamically for your network
provided that you will give us this kind
of ahead of time knowledge
like we cannot do this if we're just
operating as a library because we never
know
what kind of python code you have and
what kind of operation will execute next
but if you script your function we kind
of have this insight into the future we
have this insight into what exactly
what is the exact sequence of the
operations that our code will perform
and hence we can actually apply um some
optimizations which would otherwise be
impossible
and this is something you can see on
this slide so in other cases you know
maybe you came up with something like
alpha dropout and usually
you know it has a bunch of uh math
inside and usually this would be like
three or four times slower than the
usual optimized drop-off that exists
today
but if you actually just write out
something like this today and hyper to
just script this
this kind of straightforwardly maps to
the script subset
this will generate like a single uh
single operation on the gpu that will
kind of
contain all of the code that you can
that you can see in here
instead of executing every single of
those operations that you see
separately which like saves us a lot of
memory writes and it's generally a lot
faster
and all of this is really happening
without any user annotations like we
we do not really want you to kind of
tell us too much about your code
a lot of the a lot of the magic that
you're seeing in here is kind of based
on one time profiling so
also another important thing is that
your models might not
you know run at full speed the first
time you execute them
if you're benchmarking something always
give it a few warm-up iterations
to kind of uh because for example we're
doing this profiling so we're gathering
some information
as you run your models and after a few
iterations we when we
you know discover that well okay now we
have enough information about your model
we will start aggressively applying
optimizations based on
based on what we've seen it's kind of
similar to like writing javascript
it's also a complicated uh language that
would be
like otherwise very slow but a lot of
runtimes
implement just in time compiler so they
will like see what what your code
actually does
and then for example in the machine code
and this is somewhat similar
to what we're doing here all right
yeah apart from that we're also not kind
of doing this in the void there are
other projects that are aiming to do
something similar
there's xla from uh from google there's
tvm that's kind of supported by
amazon i think even though it's kind of
coming from open source
we're also integrating uh with with
those
so uh yeah i have a bunch of other stuff
which i'm not sure might be interesting
for you since we have
uh quite quite a few people who are new
so i'll just kind of skip over this
uh pyth also makes it kind of easy to
write extensions to it in c
plus plus so you if you have some code
which is slow in python
you can just kind of those you have some
function that like works on tensors in c
plus plus it doesn't matter what's
inside those three lines at the bottom
expose it to python from the c plus plus
side
to load it to python there is a more
structured way if you want to like make
it into a package
but there is also a kind of hacky way
you can use for your research which is
again in one liner
that will compile the c plus plus code
and expose it and later you can just
kind of call it as if you were calling
in python function
um yeah all right let's skip this
there's an interface so this is there's
you can also if you really like c plus
plus for some reason you can also do
research in c plus plus now
this is how you for example do a simple
network in python
this is how it looks in c plus plus this
is how a training loop
looks in python this is how it looks in
c plus plus so you can see that it's
pretty much the same
code you just kind of have to you know
put up with
some annoyances in the syntax of c plus
plus
but this works even though it's kind of
in beta right now
yeah and also 1.1 came with the
tensorboard release so if you want to do
monitoring of your
of your training if you want to see like
the distribution of your weights or the
loss function and plot this
we've been working hard to kind of make
this make this available
to you um finally i think something
that's very helpful
for like a lot of kind of tensor code is
is
is sometimes hard to read because you're
saying okay well let's sum all the
values along the third dimension but
like
if someone comes across this code
they're like okay but what what is the
third dimension really
so we're working on something called
name tensors uh which has been proposed
somewhere outside the community
basically what you can do is now you can
have instead of just saying well
like having some kind of implicit
conventions in your code which say that
oh well okay
if i'm representing an image then the
first you know first dimension
is how many channels like how many
colors for example does the image have
or how many features did we never
produce over this image
and then you have some order on height
and width which you generally have to
fix in different software packages will
choose different
orders now you will be able to basically
give them names so you will be able to
say okay give me a sum of this image
overall channels so you'll kind of
collapse it to a single child or you
mean
so you'll kind of collapse it to an
image of the same size but with a single
channel which is like a mean
for every pixel with with this and so
this is much easier to read
we're also working on that yeah apart
from that
there is a bunch of distributed stuff so
it's
it also kind of went under a complete
overhaul
in the in the in the new in the new api
so uh also check this out if you're but
this is like a somewhat more advanced
so again i will not be getting into this
the important part is that it really
kind of gives you very good scaling um
so if you're training like vision models
if you have let's say eight computers
it runs pretty much seven times as fast
as if you were running
on a single computer if you're training
like um
you know this is i think the second
language translation this is like some
kind of a language model
it kind of scales worse because those
models are more complex
but it still like gives you significant
speed ups like if you have good
connections
on eight machines it still like executes
six times faster
so like if you look at uh some inventory
to publish to facebook
they're pretty much now training models
uh you know they cut down for the time
from six hours to half an hour
using 16 machines but of course well
you need a lot of machines to do that
with very good connections
and yeah finally i mentioned cafe too so
a lot of this effort is also coming from
the fact that we are
merging those two libraries so
catholictube has been kind of more on
the infant side
fight with us more on the on the on the
research side
now we're kind of bringing them together
into a single library
and yeah and a lot of the stuff that
i've been talking about
uh it's really kind of supported by a
lot of institutions both
uh kind of academic and from the
industry facebook's kind of the biggest
supporter of hyder's their funding a lot
of the development
but there is also a lot of other
organizations nvidia is kind of
contributing some
developers who are helping us make it
run fast on a video gpu synchronously
uber is developing a probabilistic
programming language on top of this if
you're if you're
doing if you're interested in doing some
something like that like bayesian models
you you can check this out and there is
also a bunch of
other people from either universities or
smaller companies or like
you know contractors and and uh
and consultants who are just kind of
helping us out so if you're also
interested in helping us
it's very much an open source project
it's kind of developed entirely in the
open
uh everything is happening through
issues and pull requests on github
so if you're interested in contributing
even simple things like improving
documentation they're like immensely
valuable for us because this means that
you have more time
to develop the core library so even if
you want to do uh
some you know everyone has to start
somewhere so
uh you know we're very much welcoming
all kinds of contributors so
yeah so that's kind of the chord
presentation it might have been
a bit dense because well we have kind of
varying people with varying levels of
skills
uh in here so now we'll kind of go into
the
go into the notebook and session um
but yeah if you have any questions to
the presentation i don't know
maybe we should do them now or should we
do okay if you have any questions the
presentation
um maybe we can we can like take two or
three now and then we'll sit down
to the notebook
[Music]
said that google recently introduced
tensorflow tfx rights infrastructure
right
there are plans to do something like
that around the pythorg
i mean we're not really we weren't
planning on building any kind of
support for building out like you know
systems that will automatically take
your model and scale because
i think it's a really hard problem and
like a lot of models will you know take
different kinds of inputs and they will
be dynamic in various other ways
so we're usually just focusing providing
the path that you can well take your
python code export it and run it without
python for example
or just run your python code but the way
you write your servers you know there's
a thousand packages that will let you
kind of deploy a server and maybe you
prefer some to the others
so why not integrate this into whatever
you're actually using and it shouldn't
be really too hard like it's a one-liner
to load this code it's another line to
kind of run this code and get the
predictions right
so in general it shouldn't be too hard
but the distribution really depends on
like your
your kind of data center architecture
and like on your network and how you
want to if you have like load balancers
there is a lot of complexity there so i
honestly don't
like in fighters we really want to kind
of build the core set of features
but not in a very opinionated way it's a
like it's more of a library than a
framework so you should be the person
who's like using those features
and we do not really want to dictate in
like what ways you will be using them
so we're kind of depending on the
community to build on the opinionated
um front ends like you know dgx would be
somewhat opinionated
but otherwise you can really kind of fit
it to whatever
whatever use case you're doing and
that's kind of the card part of the
of the project
yes that was a very nice presentation
thank you and uh regarding the script
mode um
i'm more like a researcher and i don't
do any deployment usually
but people ask me often and um the
script mode i'm
based on what i've understood it's more
like for me researchers
for a researcher to just support running
my code faster
but i also saw that you recently merged
the code base of caffeine 2
into the github repository is this a
parallel effort or are you planning for
example
so right now when i understand correctly
you also export to um cafe 2
or nnx if i pronounce that correct yeah
yeah so is this um
becoming more kind integrated into
python or is it still like a separate um
thing if i want to deploy my model on a
use support in mobile device yeah it's
becoming more integrated
basically we're like integrated how we
run kernels we kind of want to take cafe
2 has a lot of those like model specific
implementations and also some optimized
implementations for the operations that
we have so we're kind of merging them
together
and torstrip really is the future
interface to whatever cafeteria does
today
so um yeah it's we're merging them
they're already using like the same kind
of tensor data structures in c plus plus
and reversing kernels and all this
it just takes time because they're two
like big and distinct projects
but this is moving along and this is
really kind of the future interface to
all of this functionality
so for me as a researcher the future
would be i just use my torch the script
to make my code faster but then also
some
industry person could just take my model
and deploy it yeah exactly you can there
is a one liner that takes any scripted
code and you can dump it on disk and you
can load it anywhere you want without it
sounds awesome thank you again
one question yeah there was someone at
the back yes
hi so i really like what
fighters does i think it's really
influential and
i wanted to ask about that influence it
has now on
tensorflow 2.0 with their autograph
and tf function functionality do you
have an opinion on that how is it
the same how is it different from what
package does i mean there's a lot of you
know
kind of learning going both ways right
we kind of started from different
it's like everyone kind of started from
different points and now they're
converging for something somewhat
similar
like we started more on the eager mode
side and then started converting towards
the script
um tensorflow started on the like well
this used python is a meta programming
language to like describe some
computation
computational graph like data flow graph
symbolically and then just have a
completely
kind of different evaluator that you
call into from icon which is opaque
and first strip is similar in some ways
but the interface is completely
different right because you no longer
build up this using
python we just analyze your python
sources to kind of get this
representation that like
the one and so tensorflow is now kind of
following a similar path we just kind of
started from different points
and now it's kind of converging as you
know as we're discovering like what are
the
good usability patterns what what things
give us good performance
uh a lot of those libraries are
converted into a very
similar interface but of course we're
doing a lot of uh
a lot of like innovation there's a lot
of innovations that are kind of
happening
in the in the background that we
generally don't talk about so much
unless it's like specialized venues um
because they're kind of implementation
details so like from the front-end side
it might seem that they're pretty much
the same
but in the background like a lot of
things that are happening are actually
somewhat
different but this is kind of technical
so i'll probably if you're interested we
can chat about this
but i'm not sure if now is the best time
to answer you know a very
specific question maybe last question
there was
somewhere here there was a question
before yes
yes hello i'm sorry you use the
gradient descent for uh optimization
purpose right
or optimization of the network yeah so
as i said be careful about this because
this is work in progress so a lot of
things might not get very
like it should never make anything
slower for you if you see something
becoming slower just report a bug report
because it should never be the case
but this is very much work in progress
so while certain models like the lstm
will be faster
not every single model will get faster
right magically so
actually i'm trying to uh apply a meta
helistic algorithm unsupervised learning
method for
optimization so have you tried
uh unsupervised like genetic algorithm
or any other
unsupervised algorithm for
optimization purpose optimization
of uh the the weights and the inputs
and the weights optimization as in you
know alternatives to safety or
yeah svd well i mean it's kind of
orthogonal to whatever we're doing right
it's like
what you're talking about are concrete
kind of algorithmic improvements
in now what we're doing is we're really
building systems that let you express
the let's say alternative optimization
algorithms right
and so like you're kind of asking about
the research that our users is doing
are doing not about the kind of core
library that we're building which is
just a tool to kind of empower people to
do the research
like you've mentioned so me personally i
just spend most of my time
kind of thinking about the you know
engineering side of it and there's a lot
of research in there but it's like more
like
systems research and programming
language research and and and stuff like
that so i personally haven't done
that i mean we have i think like half an
hour 40 minutes so we should probably
move on to
um to the to the notebooks
all right so i've put uh i've put the
wait let me
yeah you can see the address in here i
guess yeah
but it might be slightly smaller
yeah i cannot make the address larger
anyway
so if you go to i will wait let me
i will just write it so
the address is
so this is the address you have to go to
if you want to run this
um basically something like a lot of the
things that i told you we've been kind
of focused on gpu optimizations and this
platform does not have gpus
so i will actually not be running this
on this platform
i will be running this on a google
collaboratory
so you can if you want to kind of follow
me this this script will work as i
mentioned i have some conditionals in
here that make it kind of
device agnostic so it will work on both
platforms
but if you want you can just go to file
download as notebook and you can if you
have a google account
you can go to collaboratory and you can
you can upload it there
and there if you choose runtime
change runtime type you can just have
like a gpu or tpu
we'll be using the gpu now you can just
enable this
and connect to it and this will give you
pretty much a free instance
with a g with a k80 gpu item so
this is what i'll be using but it's
completely fine if you just follow in
here
it will work exactly the same way but if
one will be running some kind of
benchmarks
you will be getting obviously different
numbers because the the kind of
computation we're dealing with in here
will be faster on the gpu
than on the cpu and also our
optimization since we're kind of
more gpu focused will be more visible on
the gpu as well
all right i'll give you a moment to get
started with the
in any case this is the this is the
address
all right is there anyone who wants more
time to start this notebook are there
people who haven't hasn't haven't
started if i want to start it
if you raise your hand i can wait a
little more if not i'll just go ahead
okay
[Applause]
[Applause]
yeah so i think the the stuff that we'll
be covering again it will be mostly
focused on port script
but really you should i will be
explaining kind of the bits of the api
as we go you should not worry very much
if you do not know the exact interface
that we're using
in python you can read up a lot a lot of
it is like easily available online and
you can catch up
with this stuff from like other
tutorials but like we do
we have very few like ver very good
torso tutorials so i think it's better
to kind of give you some exposure to
this right now
you will kind of catch up on the library
if you want to use it later at home
and but like you will generally have a
you know you will have a general
understanding of
how can you start using tart strip as
part of your code if you just follow
this notebook so
um i will be doing everything on screen
so don't worry
you can just try to replicate my steps
at the end there are some exercises
where i'm pretty sure we will not have
time to do all of those
or maybe any of those uh so you can just
try and play around with this the
exercises are mostly just
to like you know make the
the exercises are mostly just repeating
what we will be doing now together
just so that you can kind of you know
better remember uh whatever we did at
home and you can also then do some you
know
offline googling and find documentation
and
anything that you making all right so uh
is everyone done did you
figure it out okay
i guess no one protests so uh yeah
sorry all right yeah so we will be
mostly as i've mentioned
you know we have done some optimizations
not a lot of them but some of the models
that we have looked at are
nuns and again we do not promise that
this will be fast we have been mostly
focusing on making the user experience
somewhat good and the language somewhat
expressive
so this is kind of one of the few cases
where you will actually see optimization
benefits
but there will be more coming you know
along the way and another
you know benefit as sebastian mentioned
is that you know even if you're a
researcher if you publish
your code people will be if you use
script people will be able
to like use use your models kind of in
in production if they want
all right so let's get started so uh i
think both
this collab and the the environment from
the
organizers they have uh torch uh
torch 1.1 installed you can verify this
if you type something like this
so yeah i have 1.1 you should also see a
1.1 string
so some things have also changed in the
latest version and like the 1.2 release
which will be
happening soon but yeah for now we're
just we're just using this
and now um this this line you've pretty
much you have pretty much seen it in my
presentation so if you run this on the
on the environment of the organizers you
will see that this would probably say
device type equals cpu so this kind of
conditionally
will select a gpu if there's one
available and
otherwise it will just give you the cpu
but this is fine
don't worry about this um yeah so now
so we will be
okay okay i mean yeah so if if there are
people who are not
uh accustomed to notebooks uh
the way notebooks work are you generally
have some cells you generally execute
them in the order in which
the images another book tells you but
you can also do this out of order
and so if you like import something in
here this will be like
kind of forever visible for all the
cells so if i define fast device in here
if i run this cell
i could for example refer to fast device
in here and so if i ran them in order
that would not work but if i run this
cell first and then that cell
this fast device would have already been
defined and to run excel
to kind of progress to the next one you
just do shift enter
and that will that will execute whatever
code you have inside
yeah so now we will be again talking
about cells but this time not the
notebook cells we will be talking about
a retired neural network cells because
so the way uh i don't know how many of
you don't know what every car neural
network is if you can raise your hand
okay so there is a few people all right
so generally what do you have
if you have a recurrent neural network
i'm not in
i think it's going to erase i'm not a
you know
very much a machine learning person so
this will be a kind of
plain explanation of what's happening in
here but generally what you have is you
want to process some kind of
sequential data in this case right so in
general you would have
if you have a feed-forward network you
have like a single input
right and then you just apply like a
sequence
sequence of operations it might be
somewhat you know not non-linear
but in general you just have a sequence
of operations and you get the output
right
in a recurrent neural network what
happens is that you have a whole
sequence of inputs so you have some
input at time zero
you have input at time one you have
input at time
two and so on right so what you will do
is you will want to have some kind of
context
that will be preserved across those time
steps so
this for example so generally the the
way recurrent neural networks are
structured is that first
you define a cell and a cell is a
function so i will represent this
i'm not sure if you can see this in the
back but i would represent this with
like a big box
which basically has kind of one input
from the bottom and this is the input of
the current time step
but to kind of incorporate the
historical influence into this
actually maybe i'll just find a picture
online that might be simpler
i hope i'll not infringe similar
copyrights
but yeah generally well you have towards
data science.com that's the source in
any case
so generally you can you can think of it
this way what
okay thank you yeah so generally you can
think of it this way so
to define a recurrential network you
first define a cell and this is the blue
block
so orange blocks or yellow blocks or
whatever this color is
those are the inputs at every time step
the dark orange or
red whatever those are the outputs at
every time set but you kind of do not
want to do a
separate prediction because the past the
past of the sequence
might have some influence on what how
the future predictions should look like
right
so you will also have those kind of
intermediate states
that are produced by the by the function
in the middle
which will also kind of pass the context
between the time steps
so generally this blue function which is
called a cell
it will have two inputs one for the
input and one for the context
and it will produce two outputs one of
the outputs is the
is the output at the given time step and
the other output is the context for the
next
time step right so you kind of have this
updating content which also kind of
produces outputs so
you can think of the inputs as like a
sequence of words in a sentence
in one language and the outputs will be
a sequence of words
in another language but like the
predictions for each words are not
independent
as you're reading the sentence you want
to include the like knowledge
you gained by reading the previous words
which is something that such a model
for example will let you do of course
this is a very simple
recurrent neural networks generally you
will you know you will find more
complicated models in the wild
but this is the simplest case and this
is like a building block of many of them
so for example we will be looking at
something like this
all right so yeah so we will now
define the blue box which is which is a
cell
and so we will be using uh something
called an elastim cell which is
again the standard thing uh which is
also supported by kurinon so what it you
can see that
again it has some parameters because
it's a function that we want to learn
so there will be some parameters it has
the input and it has the hidden and the
hidden state
is basically this like kind of context
that we're passing
between between the time steps and so
the context does not have to be a single
value it does not have to be a single
array
in case of lstm it's actually two arrays
so we will unpack the hidden
into two values and the parameters we
have three parameters in this case so
we take the input we multiply it with
some set of weights some set of
parameters
for the inputs we add to this the
you know some kind of weighted weighted
function
of the context and we add some bias just
as in
the regular kind of linear layer then we
split this into four parts
we apply some non-linear functions to
this and then there are just some you
know computations so well okay
we want to mask out we want to let's say
forget some information right this is
some kind of an interpretation
which some people might believe in some
might not
and then you just have some kind of
gaining mechanism so i will not be
getting into the details of how this
works
this is one of the most commonly used
cells if you're doing
recurrent neural networks and then what
we return
so really if we want to have again a
separate kind of
output for the output state and for the
kind of state tuple we would have to do
something like this
so the output of the cell is hy and the
next state is h
y c y because again it contains two
values
uh but because we kind of don't want to
do this this way we will just return it
like this because h y would be
duplicated right
so yeah you can just run this and this
will define this function for the for
the next cells
so now the question is how do we make it
recurrent so there are
two helper functions in here that are
not very important
so the first one is how do you how do
you define the initial hidden state like
if you start processing a sequence
you do not really have the like context
to give to your model right
so what this code does is it kind of
from the input and from the parameters
it kind of magically somehow figures out
the sizes
of the of the of the context that are
appropriate for a function
and then just allocates arrays uh that
are
filled with zeros pretty much because we
have no useful context
we'll just tell our model that you know
we have we have all zeros
and you can notice that by using this
device it will put it
on the same device as as where where the
input lives
so this is for example a code that will
adapt automatically
to whatever device our computation is
running on
and apart from that again we have some
parameters about like the
dimensionality for input and the desired
dimensionality of our
of the context and uh using those
parameters
uh we can we can allocate we can
allocate pretty much the parameters
for our cell and once we have those
helpers which are just kind of
to let us start the process we implement
we implement the actual function which
you know
like we have the cell defined this was
the lstm cell and now the lstm function
will do exactly this so if you give me
an input which is a like array with an
extra dimension which is a time
dimension
i will just uh i will iterate online
will kind of
give me a list of slices along this
dimension so
well you can actually do something like
this um
yeah and so we will kind of slice this
array over the
over the time dimension and at every
step we get a step inputs
we will apply the cell to the step input
to the current hidden state which is the
context
with the same parameters and this gives
us the new hidden context
and so we will not unlike this picture
we will not be gathering the
inputs at every step uh we will only
care about the final step which is kind
of
well what the you know how how the what
what the how the network can summarize
the whole sequence
into a single state one of the exercises
is to modify this code
to actually start returning every like
the concatenation of those
similarly to how the input was processed
this is an exercise this is not very
complicated
you can try to do this later so yeah
this is
this pretty much takes it there's also
this part which you know if you don't
specify the hidden state
because you well usually the zero
thing is kind of natural to do you might
want to make it convenient for your code
to like not have to care about this too
much
this will just allocate it like all zero
states unless you provide something in
here
all right so we can run this cell we can
define this and now we can define
so we will now look at how quickly this
runs for example so we can take the
this is a benchmarking function this
line is actually
so there are a few things to cover here
because a lot of people are getting
benchmarking their models wrong if
you're trying to make your models faster
a few things
you have to remember if you want to if
you want to betray them so first and
foremost
um if you're running on the cpu there's
nothing you have to do really
but if you're running on the gpu the
operations actually execute
independently on the gpu and on the cpu
so cpu is really only when you
for example add to arrays that are on
the gpu the cpu is only telling the card
to like add those to a race but the
operation might not happen and like
we'll
then proceed to run the rest of your
python code while the actual addition on
the gpu might execute later
and also like there is a whole sequence
of those operations that might be kind
of
the backlog usually when you're running
gpu code you will want the cpu
to be ahead of the gpu so if you want to
accurately measure the time
your you know your compute actually took
on the on the gpu
you will have to use the synchronize
function which will block
the execution on the cpu until all of
the work finishes on the gpu
um yeah so you know another so this
synchronization is really crucial
if you will be executing on the gpu you
can also read this
up in some of our tutorials then again
as i mentioned it's also important
because we're doing a bunch of caching
and some kind of benchmarking behind the
scenes it's also important to include
some of the warm-up iterations in this
case by default we do five
uh just for the sake of like
initializing those caches
the real training performance will be
like we will get stable after some time
and this is pretty much the you know for
example this might be the right time for
this model
to like let it stay low so we'll execute
it a few times without actually
benchmarking anything
then we will synchronize to weight to
make sure that the gpu if we're using
the gpu has finished those warm-up
iterations
and it's not messing up with our with
our real timing
then we actually you know start
measuring the time
we run the function a few times in here
a 800 times
and we synchronize again because we want
to block and once we know that you know
the whole work has been completed
we measure the end time so this is kind
of a simple way
to benchmarks and then we just subtract
you know take difference in the time we
measured
and divided by the number of iterations
and that's
yeah go ahead so what's the scope of the
system
i mean it's like in here when you
execute this function f
which is an argument it will like add
some work on the gpu for example right
but so you will add this work a hundred
times but since the work takes more time
than actually telling the gpu to do the
work
this for loop will likely complete
before the gpu will be done
so this synchronized call will tell you
okay i don't want to execute anything
that's you know past that line of code
before the gpu actually finishes doing
the work
yeah go ahead is it possible to
synchronize just
some words some jobs that you said or
even if there is only this
synchronized function yes it is possible
but you require
in case of kuda you have to use streams
we do support that but it's somewhat
more advanced usage and we will not be
covering this right now but just put on
the benchmarking right so that otherwise
you wouldn't get the cpu
yeah yeah in general you never put
synchronizes in your code unless you're
actually trying to benchmark something
usually there is no reason
unless you're doing like a complex
asynchronous thing with a lot of streams
then you need to kind of make sure that
they don't kind of get ahead of each
other
and then you need to take care of manual
synchronization in
you really only need to synchronize you
will never need to synchronize in your
training code which is why a lot of
people kind of forget about this
when benchmarking but this is really
important for what if you want to do
benchmarking yeah
so if you want to access the option
yes there's an implicit synchronized
yeah which is so
generally everything runs asynchronously
if there's some need for the
actual let's say print to wait on the
result we will synchronize
for you unless again you use complicated
streams then you'll be a senior and
the advanced user you can take care of
yourself so but in general you never
need to really care about this
apart from the benchmarks where you just
want to accurately measure the time
yeah so we do this and then we can for
example run
the the so in here we just allocate
random inputs
we don't really want to learn anything
right now we just want to see how fast
it runs
there will be other tutorials which will
tell you how to run things how to learn
models
uh this is not the tutorial about this
then we initialize the parameters this
is one of our helper functions in here
then we initialize the hidden state
because we don't want to benchmark kind
of
the cost of initializing the hidden
state we just want to
benchmark the cost of actual running of
our network and then we
say okay benchmark the function which
just evaluates our model
on those initialized parameters right
and okay in here we actually say
20 times maybe let's make it 40 to make
it
somewhat more stable generally the more
times you run this the more stable
this will usually be so okay so we can
see
for example a single with those sizes if
you were to
like your input has 128 features your
context has 256
features uh you have a sequence of 164
and you have 32 examples that are
processing at the same time
then a single iteration a single
evaluation of
something like this will take uh we'll
take what 20 milliseconds
right all right so now we can go and
actually start
uh annotating this thing with script so
the basic thing is you just add an
annotation
with torjit script but this really is
not
sufficient because as i mentioned this
is this will tell you that a tensor
cannot be used as a tuple
and this is because um as i mentioned
this is a statically typed language and
if you do not provide us
with any type annotations we will assume
that all your inputs
are assumed to be tensors so in this
case hidden and firearms are not
tensors they're tuples and we want to
unpack them here but this sees a tensor
and it says well okay you're trying to
unpack a tensor that doesn't work this
way
so if you want to use this we will be
using python 3 syntax in here for type
annotations
it's also generally a good idea to add
them to your code we
we also need some extra imports which
are at the top
so now we will say that okay hidden is a
capital that contains two tensors
rms is a topical that contains three
tensors so this looks something like
this
for input we can say that this is a
tensor and that's probably a good idea
but this is optional since it will be
unfair to be a 10 survey default but
i'll just put it in
just for the sake of having it now if
you try to evaluate this you can see
that this now compiles fine
and you can see that i didn't put any
any any return type
annotations in here this was just
inferred automatically
now if you want to do the exercises
later there is an exercise that asks you
to implement the
mi lstm which is a variant which is for
example not supported by kudianon
and again see how how it runs
this actually uses the function phi this
mostly changes how you
kind of use those inputs and the context
but the actual transform which is
contained in here
is exactly the same so now let's just
isolate this
into into a separate function so we can
say
something like that and
what do we want to take we want to
do this and now we say
i think that should be
something sorry can you speak
yeah exactly so now we can run this and
here you can see that
this actually works out fine um
but this is a python function so it's
also a good idea to add type annotations
in here for python functions we will not
be able to infer the return
types so now you can see that okay we
have a scripted function which actually
calls into a regular python function
which is something i've mentioned
and this is really important because for
example now
even if we had a system later where like
we have everything scripted
um but well we have we think we have a
bug in this function we want to debug
this
we can actually just uh we can actually
just kind of comment out this
and this will all kind of keep working
right so you can revert to python
selectively on some functions
and debug them easily so this works but
if you actually try to investigate
what's happening
you can ask for the code that we have uh
that we have
uh produced for for this function you
can see that we
there is this kind of um i don't know
dash
and they call to lstm pointwise and this
dash indicates that it calls
outside of the tort script this is a
simple function so it actually should
just get put
into this code you can see that this is
some kind of a representation of your
you know the code that you wrote in here
if we just enable this script in here
and evaluate the what now
undefined value cx oh yeah we need to
take cx as an argument
as well
yeah so you can see that in python if
you don't define something you would
also get an
error at runtime in tar script if this
cx was missing and we refer to it now
first people complain that well you have
you know you have an undefined
identifier we don't know what this
is so this also gives you some kind of
ahead of time
you know guarantees about about your
code yeah so now if we print this
you can actually see that this call all
of those like sigmoid and 10h and all
those non-linearities
they kind of got inlined into this
function which means that this correctly
got recognized as a call to a regular
script function
so if if something is a dot code
attribute it means it has been scripted
and you can investigate like how you see
the
how how we represent the uh the function
that you scripted
in in internally right this is going to
be pretty printed
version there is also graph uh which
kind of produces a
not so nicely looking uh thing but if
you're
like working on compilers this is
interesting one note is that the code
will always show you the unoptimized
version we're doing a lot of
optimizations but there are other ways
to actually ask for this anyway yeah so
we have this
uh now we will also want to script this
because this
is also uh this is
this is also part of our model um okay
input is just a tensor
arms again is a tuple of three tensors
and hidden the syntax is somewhat
awkward
uh hidden is a tuple oh yeah
didn't say double up to tensors now if
you try to run this
it will tell us that it expected a
default value and that this value does
not really work
and this is because none is not really
tuple right so what we have to say
is that this is an optional couple
uh what now expected
value oh yeah and again if you if you
this is a python function that is
defined in here
if you do not add any annotations it
will again assume that all the
parameters are tensors and that the
return is a tensor
again in here okay this is a tensor that
is kind of unnecessary
but this is a tuple of three tensors
and this returns a pair of tensors
so we again need to add annotations even
though this is a regular
python function and now you can see that
this compiles just fine with without any
errors
and again if we print lstm.code
you can see that okay there is this for
loop there is this like call to
intellistem hidden
and also since the cell is somewhat
simple it also got inline
into into the into the for loop
immediately
so this call kind of got eliminated so
you can also structure your code without
kind of worrying about
if like the function call overhead will
be high or not okay so now we have a
scripted model
and the important part is that now if
this compiled this means that
this behaves exactly the same as the
unscripted model would have so if you
get something to compile
and you know you only have type
annotations this will guarantee you
that uh has been changed so we can just
try to re-benchmark this
and uh yeah we can see that okay we have
like a 3x
speed up pretty much just by adding this
annotation and so in here you can see
the unoptimized code
but if we actually investigate it's
probably not something you want to do
yeah so to ask for a specific kind of
code that is executed for your given
inputs
i will just like allocate those create
those
uh because we will need to run this you
you can ask for a graph for and this is
like a low level function i'm just
showing it to you
to kind of show that we are doing a lot
of
processing outside if we do this
yeah so you can see that the actual code
that we've run looks
very different if we print this like
ugly representation
instead of the pre-printed python code
so this is the graph
this is the same thing that you saw but
in this alternative representation it's
kind of short and it looks somewhat
similar
in here you can see that you know there
have been some crazy things going on
we have a lot of kind of extra functions
that have been somewhat lifted
we have some fusion groups which means
that you know we will generate code for
the gpu for this
and so this is how pretty much this code
can run
faster but all of this is happening
entirely in the background
and like you do not even need to be
aware of this so this is one benefit of
scripting but again i'm showing this to
you because this is kind of a case
that we've looked at this is very much
under construction so you can try to
experiment with this on your models
but i do not guarantee that like it will
make everything 100
you know 100 times faster generally be
kind of
uh if you see that you know some other
systems make things faster
then we'll probably miss something you
can open a bug request you can look into
this you can implement this that would
be even better
we can we can help you do that but in
general you can also try scripting
either because you want to deploy this
or because you want your models
uh to be like used in any in a wider
setting
all right there are some other things in
here um
but i'm not sure if we have time for
that
um i'm not sure what what time the next
talk was supposed to start
five minutes ago okay okay
yeah all right uh so we'll just quickly
go into so another feature in piper
which you will see a lot for those of
you who haven't used it
are n modules and those are kind of like
stateful function
stateful functions that encapsulate the
parameters right so generally they will
have an initializing method
uh where you will like initialize the
parameters here we'll just reuse the
helper that we had
and you can also have like a lot of
other state like you can save those
parameters and then you know investigate
them later
unlike with a simple function where you
have to pass this around all the time
so generally you will implement in it
and you will implement
forward and forward is the actual
computation that this will do so in this
case this we call an sdm module
and we have a already you know ready
implementation
given in the lsdm function um which you
know which we wrote
which we wrote in here
but generally would i would probably if
i were to do this model i would just
start writing this out
as an n module already so scripting path
is slightly different for now modules
um generally you i will not be covering
a lot of this
we have a tutorials for that so um it's
unfortunately it's somewhat awkward
we're kind of trying to make it nicer
for modules
but generally to script this you you
need to change the inheritance from an
end module
and then module is torch an end module
you change it towards jit script module
and every single method that you want to
get scripted similarly to function
you say script method instead of script
and
yeah you can try to do this one caveat
in here is that it will actually delay
the compilation so unlike
with script where you get an immediate
error if we try to benchmark this now
um this will only give us an error now
like a compilation error
and this is because this will first run
in it and only then try to compile all
the methods so like if you missed some
type annotations in here
this will you know complain complaining
that oh well
we have an operator which initiative
takes tuples of tensors we wanted an
optional tuple of tensors
but it gave us tensor right so you need
to add again
this is this is like that
and this is like that and yeah and here
you can see that we can automatically
since we know that those are parameters
we can automatically kind of figure this
out
hopefully that should work oh yeah
that's true
yeah okay so this works now for example
so you can see that the workflow is
slightly different
but it is also somewhat similar so you
know i haven't explained a lot
of you know how the library works you
just got a kind of deep dive for those
of you who haven't used it you kind of
got a deep dive into like
what are the what are the latest things
but you know hopefully if you're if
if this kind of got interested to you if
you if you know if you see some value in
the stuff i talked about
you you should be able to just you know
use any kind of regular tutorial i don't
need to come here to kind of
give you the regular tutorial you can
you can you can read online
hopefully this kind of is some kind of
an inspiration to actually
um start learning so i think we're
running out of time
um so if you have any questions you can
either ask them now if you have
questions that you don't want to ask now
you can catch me you know either after
the talk
um during a break or you can send me an
email you can easily find me on github
uh yeah that's
just my handle this is his first name of
my
uh yeah first letter of my name and then
my surname so you can
find my email in here uh if you want to
reach out i will unfortunately need to
leave today so if you want to catch me
at the school you will have to do this
like pretty much at the
next break um but yeah otherwise
i'm available at my email if you have
any questions right now i'm happy to
answer that as well
yeah so to in inside python it's just a
one line
you can uh you have torches save
right and there's all documentation
about this you just kind of tell it
uh you just give it like i can take
portrait save
um like you know lstm and
say that i want to make it lstm but i
don't know
some kind of random extension and this
will
uh all right anyway yeah that probably
works with
with uh
let's try this yeah
in any case uh it it has some compiled
hanger but anyway you can just look at
the
the documentation of this function this
will produce the
zip file and later
uh if i
you can just do
[Music]
you can just do this
jeez yes you can just do this so this
like cartoon loud and c-plus buses again
in one liner that will load this archive
as a legit model there's also a whole
tutorial i think on how to do that and
if there's not there should be