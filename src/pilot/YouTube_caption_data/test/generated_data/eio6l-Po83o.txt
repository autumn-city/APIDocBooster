welcome off today you will need pen and
paper this is going to be a bit of an
experimental lecture we will talk about
natural gradients what they are and why
they are useful in the context of your
network optimization so why bother I can
give two reasons one is one is that if
you plot a training error or test error
as a function of the number of training
a box then a typical pattern that is
observed is the following we will have
some kind of decay of standard grading
descent and we often see a much faster
convergence of natural gradient descent
now why doesn't everybody use natural
gradient if that is the case the reason
is that a single Epoque is much more
expensive than natural grain descent so
if you put here not a pox but wall clock
time on the x axis then things look less
favourable however it is still the open
question of what will work better in in
long run
and the second reason why it's
worthwhile looking at natural gradients
is that many methods like batch
normalization or specific activation
functions like the exponential in your
unit who are motivated either in advance
or once they have been found out to work
well post hoc as approximations of
natural gradient descent so many
optimization tricks turn out to
approximate natural gradient descent
this field was founded so it has been
understood by many it has been in
natural grain has been invented in
several fields but it is associated with
one name in particular namely with Amari
who has spent half his life popularizing
the concept and it is a concept from
so-called information geometry which
lies at the intersection of differential
geometry and statistics so field called
information geometry and this is
something popular with physics with
physicists because they understand it
yeah differential geometry places quite
role in relativity and so on so they
like to indulge in this ok
so what's wrong with standard gradient
descent I want to start by revisiting
standard game to send and then see that
it makes it has certain not so desirable
properties which are addressed or
improved upon by natural gradient
descent
so there are Center grading descent can
be justified as follows so we have a
parameter update
I start by repeating what it is yeah so
we find new parameters by starting at
the old parameters and then having some
magic step size times gradient of the
loss so the loss depends on the
parameters and on the training set and
I'm here using the convention that this
gradient is a row vector which then
works out better with the jacobians and
so on when we do back propagation and
this simple recipe can be justified by a
method called proximal gradient
in pr√≥xima gradient we start by looking
for a vector which on the one hand is
aligned with the greatest possible loss
reduction that we can achieve so we try
and find parameters theta star which on
the one hand try to follow the direction
which will reduce the loss most we have
just a vector product here but on the
other hand we also have a retaining
potential which says we don't want to
walk too far from where we started so we
have this is a harmonic force with the
stiffness here of 1 minus mu and then we
have a quadratic term that binds theta
star to our old or previous parameter
estimates theta T and if we solve this
quadratic problem for the optimum theta
star then we find exactly the formula
above so the second term here would be
the procs operator
so if we have as you see is just know a
harmonic potential in some parameter
space and where's the problem here the
problem is that there is some
arbitrariness in how we parameterize a
statistical model let me illustrate this
so let's look at the simplest model that
we can imagine just linear regression
okay
there are so many ways to parameterize
this I could say here that my output is
a slope a times some input X plus some
offset or I could have some Alerian form
I could say that I have some vector W
transpose times X and O minus some
number C should be 0
excuse me so this is 0 and this is an O
and there are so many other ways now if
we take and that's here constrain 1 to
have unit length now if we look at the
family of all possible models here so if
we take all possible values we can have
a and B on the real line and C is also
arbitrary the families are the same it's
just all conceivable linear regressions
that we can imagine same family of
models just a different parameter
ization and i now want to make an
argument not about linear regressions
but about an induced distribution over
outputs and how that depends
on these parameters so let's say we have
some input distribution P of X and
because we are discussing your networks
here I can easily so there's a cheap way
of turning a no network into a
probabilistic method namely by just
letting the output neurons be
probabilistic so we have a neural
network with a probabilistic output so
for a classification for example I could
say that we have a distribution of some
random variable Z conditioned on some
sin input X and that would be in a two
class classification problem a variable
which is Bernoulli distributed with the
chance of success given by always that
the deterministic output of the neural
network
and this o in turn depends of course on
the input and on some parameters which
we may have and this will give me some
probability mass function which I can
write as P of Z given X now for
regression which is a better match for
the linear regression problem I've shown
above our Z distributed X would be given
by a normal distribution with a mean
again given by the deterministic
prediction of the new network and some
variance that I can choose and this now
has a probability density function P of
Z given X so here for binary
classification Z it is in 0 1 and here Z
can be anything on the real line
okay so in this sense I can say that for
each input my network induces a
distribution over possible outputs and
how you know this choice of
distributions how does that relate to
what we normally do by minimizing loss
and so on it's like in classical
statistics you can either simply say I
want to minimize the sum of squares and
this will give me my best linear model
fit to my data or I can say I can make
this more probabilistic argument and
assume that my measurements have a
certain error distribution for example
the Gaussian one here in regression and
I want to find the maximum likelihood
solution so just to do this once I will
relate this to the loss that we usually
discuss in networks we have some
likelihood which for a given training
set would be a product over all the
samples that we have in the training set
and then I have some ground truth
targets
and I am looking at this conditional
probability of my ground truth targets
given my inputs and that depends on some
parameters I could have written the
parameters up here explicitly if I had
wanted instead of working with the
likelihood it's often more convenient to
work with the negative log likelihood
and then my product turns into a sum I
have some of a minus log of these
probabilities here
and
in this log of these output
probabilities this is what we usually
use as loss functions so that would be
the last that we usually have between
some ground truth TI and some output
that I obtain from a given input and a
given set of parameters and this is the
empirical risk that we usually minimize
when we train a neural networks and so
this is also the justification why when
we train let's say for brown background
classification problem why we're not
using the squared loss but why were you
using cross-entropy
because for foreground background the
Bernoulli is a better model than the
normal distribution in the opposite
argument goes for regression so here's
the connection with what we usually do
in your networks and we don't think
about you know probabilistic argument
but what we get overall here is a
distribution
of outputs or a drawing distribution of
outputs and inputs P of X and Z so comma
because I'm talking about the Joint
Distribution here and this is
probability of z given x times P of X
and P of X there's nothing we can do
about it yeah so this is just given by
the world yeah so these are the training
inputs and the only thing we can
influence a train time is this second
term here so our traditional
distribution over over outputs given
some input okay and let's look at this
thing now so P of Z given some input X
and that depends on some parameters
theta is a family of probability
distributions and when we want to
address a specific member of this family
we call it by its name the name here are
the settings of the parameter values so
going back to the linear regression
example I gave the path
the probabilistic model that we have a
linear regression is that we say that we
have a probability of outputs Z which is
given by a normal distribution and the
mean of this normal distribution is
given by so we have a normal
distribution so Z Z given X here is a
normal distribution and the mean of this
normal distribution is given by the
output of our network which depends on
the input and the parameters and then we
assume here some standard deviation for
these normal distributions okay so each
linear regression model gives me this
family or gives me this output
distribution and if you specify some
distribution over inputs P of X this
gives me an overall Joint Distribution
of inputs and outputs and the point now
is no matter how we parameterize this
linear regression model if we use slope
and offset or if we use you know there's
so many ways to parameterize I could use
here the angle and and offset or I could
use the normal vectors no matter how we
parameterize this linear regression
model I always obtain the same output
distributions however in this linear
regression model we can only have two
independent parameters so in other words
this family of probability distributions
lives on a two dimensional manifold and
the choice of parameter ization gives me
the coordinate system
on this manifold so what I've been
trying to sketch here is the Romanian
manifold of probability distributions
that can be realized by some model and
this Romanian manifold that's
fundamental to the model okay so for
example let me sketch a titanium network
let's say this is the network here I do
have freedom in how I want to
parameterize the perceptrons but this
model here corresponds to one specific
neural manifold so let's say I have a
total of nine parameters because each of
these perceptrons has two inputs and one
bias so this thing here spans a nine
dimensional manifold or euro manifold
if on the other hand if I now add a
perceptron
this thing now spends a twelfth
dimensional manifold it's not right
because this one has services them three
six nine and four this is a thirteen
dimensional manifold because my last
perceptron now has a total of four
inputs so by choosing in your network
and by saying you know which which
neurons we want to have and what
connections we want to have between the
neurons this is a fundamental decision
this will define the family of possible
outputs of the model but then how we
parameterize the individual units is
arbitrary so in other words I have some
coordinate system here let's say let's
say this is just for the sake of
argument now let's say this is
parameterization in terms of slope and
offset but I can have some different
parameter ization and sometimes these
can be very non linearly related to each
other this could be parameterization
in terms of let's say the oil a normal
form
so the manifold stays the same if I
choose a particular statistical model
and the parameterization that I choose
defines the local coordinate system that
I use in this manifold so the manifold
is fundamental but the coordinate system
is not and again why bother
because up here scrolling back up our
procs operator is given in a specific
and in fact an arbitrary coordinate
system
so let's I'm going to flatten the
manifold here just because it's easier
for me to draw so for example in this
blue parameterize ation here my procs
term let's say around this point here is
going to be a nice isotropic parabola
but if I'm using the identical so I'm
showing here the identical manifold but
a different parameter ization let's say
I'm just using an affine transformation
here and I should start at the same
point so the point in the middle let's
pretend it's this point here and now my
procs operator will look like this so in
other words my great inter cent will
give me a different direction so even
though the loss function you know this
is maybe surprising after all what did
we want to do we started from some
parameters and then pick new parameters
so as to follow the direction which
minimizes the loss and it so turns out
that the direction which we then truly
follow depends on the arbitrary
parameter ization that we use so in this
sense gradient descent is not innocent
or a particular direction that we choose
is tainted by this arbitrary choice of
paralyzation that we used and this is of
course a sorry state of affairs so we
would like to extract from this
arbitrary choice we want something that
is independent of whether I use slope
and offset or
normal vector to specify the way my
linear regression model looks and this
is exactly what the natural gradient
allows so this family of possible output
distributions this Romanian manifold is
also called euro manifold by an avi it's
a fancy name right next time you write a
novel let's cook the title the neural
manifold is a Romania manifold and to
those who know about differential
geometry what we want to follow here is
a covariate gradient so we need the
metric tensor of this Romanian manifold
and the metric tensor happens to be
given by the Fisher information matrix
that we will derive in a few minutes so
just out of curiosity who knows what a
metric tensor is all the physicists in
this is essentially question of who
studies physics ok so not so many people
ok good
[Music]
so there okay so let's say we have a
simple statistical model which gives us
a normal distribution and the only
parameters that we have are the location
and the spread of the normal
distribution and even for an innocent
normal distribution I have different
ways of permit rising it because so the
way we usually do it is to say we have
some X - you like this but I can have
other ways for example I can now
multiply out this square expression and
then have different coefficients
associated with the terms my polynomial
and so this family of normal
distributions in this again is a is a 2d
manifold because I have fundamentally
two parameters and now there are other
models which it would be or other
distributions which could be desirable
to fit so maybe I would really want
something let's say that is square or I
would like to have something which is a
bimodal distribution
and these live outside the manifold so
maybe the square lives here and maybe
the the bimodal distribution lives there
so if we have a small model this traces
out a very low dimensional manifold in
the space of all conceivable probability
distributions and you ask why is it not
a plane so why why is it not a surface
and you know but I so the same a small
standard deviation there are simply
peaks and they would be so distance
between just a single point someone very
close to each other
would your really small sorry the one
with it with the big send it will be
really small very small standard
deviation should always
[Music]
becomes a some hawker essentially and I
can send around example let me give
different answer okay I thought of one
so let's look at this point and at that
point which lie on the manifold no and
let's say this point corresponds to a
Gaussian sitting here and that point
corresponds to a Gaussian that's even
used to say the same standard deviation
the Gaussian sitting there and if I now
take the arithmetic mean between the two
so I connect them by a line and I picked
the intermediate point then if I just
add these distributions I've say I take
the first distribution plus the second
distribution so one-half times the first
one plus one half times the second one I
get a new distribution which now is
bimodal and the bimodal distribution I
cannot you know of course not represent
by any single Gaussian so it must be off
the manifold and this shows me that
and yeah ask us more questions please
yes this is like the first term is go
find a better place and the second term
is don't venture too far you see this is
a cautioning term because so you know
this eventually translates to the step
size because this says something about
how strong the curve do I expect my loss
surface to be so I know in which
direction it is better to go but I don't
know how far to go and if I go too far I
will overstep the minimum I will
overshoot the minimum this is why there
is the second term witches don't walk
too far from where I am right now more
questions why does the direction which
move change with the feminization
because in terms of differential
geometry
this the parameter itself is contra
variant so that mean I say know what it
means and the gradient here is covariant
and if I add up a contravariant and
covariant thing then looking for a
weaker word not Pettis Rafi well you
know then you know trouble ensues yeah
so what does contravariant and covariant
mean the simplest example is if you
simply have a scaling of one of your
axes so let's say we have some parameter
and the units of this parameter are a
kilogram and then we have a different
parameter ization the units of which are
gram so there is a factor of 1,000
between the two and if I now want to
change from one parameterization for the
other in grading descent
I always start at some point and then I
walk to the next point and walk to the
next point and so on now so let's say
this is theta 1 at I 1 this is theta at
time 2 this is theta at times 3 now when
I change the parameter ization in my
model and if I want to start at the same
effective model then for example if my
initial setting of the parameter was 1
so I started with my parameter being set
to 1 kilogram or 1 meter per square
second or whatever then I need to start
with theta tilde equals to 1,000
not to get the same defective model and
that means as I changed my as I changed
my parameter ization I also had to
change the value in the corresponding
fashion and position vectors location
vectors or parameter vectors are always
contravariant that's why you you correct
for the scaling or squeezing of a
particular axis the gradient on the
other hand behaves differently and
because the units are in the numerator
instead of the denominator and if you
look at the definition of gradient and
this is why it depends on the specific
parameter ization now this would be my
answer to your question starting from
this formula here my answer to the SEC
to your question starting from the
second formula would be the picture that
I've shown here on the side so I have
some restraining term but the shape of
the restraining term depends on the
parameterization which is arbitrary and
hence my overall descent direction will
be different between the two cases
more questions yeah okay
down here yes okay so it can have
singularities however if you make if you
take let's say playing a network with
Sigma riddle activation functions let's
say then as you change one of the
parameters infinitesimally the
distribution over outputs changes
infinities in value and this is so this
tells you why the manifold is connected
now what is the space in which it lives
[Music]
yeah that is correct but I know if that
satisfies him so yes so especially
because we can have continuous inputs
this already tells you it's you know
it's infinitely so you want to know what
is it a metric space or not it's the
space of probability distributions like
you said so non-negative functions
integrating to one
okay I cannot answer your question I
need to think about her tomorrow
yeah maybe questions that I can answer
no question that I can answer okay good
yes yes yes
I just have trouble drawing these thanks
for pointing out yeah hmm okay good so
let's have a break and then after the
break I I will argue that the Quebec lab
with divergence is a good I will give us
something that helps us compute inner
products between vectors and that can be
used to get a metric tensor on on these
manifolds