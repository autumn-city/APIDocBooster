And we can actually use both on either the  LSTM or the LSTM cell for implementing the character RNN.
"So I just have something to keep track of the  hidden size. This is our embedding layer that goes from  the integer, the character integer to the embedding vector,  a real value vector of size, let me see of size 100. And the  hidden dimension is 128. Okay, so we have the embedding size,  and then the LSDM cell takes vectors of size 128, and has a  hidden size of one, sorry, of 100, and has a hidden size of  128."
The vectorize sequence of notes will be input into the first LSTM cell.
In case the hidden state was None or in other words if this is the first time we run the forward pass we populate both of the hidden states by zeros.
"Usually, people use LSTM layers in their code. Or they use RNN layers containing LSTMCell."
"As it seems, the LSTMCell implementation is more hands on and basic in relation to how a LSTM actually works."
"As we want to feed our LSTM Layers with a sequence, we need to feed the cells each word after another. As the call of the Cell creates two outputs (cell output and cell state), we use a loop for all words in all sentences to feed the cell and reuse our cell states.
This way we create the output for our layers, which we can then use for further operations."
Custom LSTMCells don't support GPU acceleration capabilities - this statement probably means GPU acceleration capabilities become limited if you use LSTMCells.
"Obviously, It is easier to apply parallel computing in LSTM."