so
let's
get started
the first thing i mean
for the final project
the ta
pontia tried to arrange like a
group matching because
some group didn't you know find the
matching program but
there are some
we have only we have
number of groups
left
so
at the end
the tier assigned i think one of the
big group contains three small groups
okay just want you guys to know that
because that that was because like uh
um we have our number of small groups so
that's um
you know um
since two groups from one large group
but
um at the end one small group will be
left out so
um in order to avoid that so
um
the ta just assigned when you assign
like a three small group one of the uh
large group to contain three small
groups
okay but um if you are if you happen to
fall in that group i mean that large
group i mean don't be surprised on that
i mean the
[Music]
the grading everything will be uh
exactly the same anyway okay
so
okay
um
that's for the
final project and
today we are going to finish this
um
chapter
and hopefully
go on to talk about
auto encode i mean we we including
today's lecture okay we have three
lectures
left okay because like uh um
the two lectures in january
the first one will be used for the final
exam okay
i hope by now you guys know we have
final exam
okay the first week of the january okay
and the second week will be
uh um
second week in january it's actually the
final week uh
the lecture that week will be used for
the
group presentation for the final project
okay
and i will um just like um the
um
the previous uh um
semester uh what i'm going to do is i'm
going to order peter okay
because our class starts at one o'clock
right
so
i'm going to order pizza like around
like 12 o'clock
and by hopefully by 12 30
we should have pizza here
okay and the drink okay
so
you guys are more than welcome to
to to to arrive earlier
to enjoy the lunch before your final
presentation
okay
so
and and that the pizza is on me okay
just want you guys to know that okay
so
um back to the uh lecture
last time we talked about
the
layer normalization right
uh we said we talked about like uh the
regular
um
normalization fashion organization
does not work well for
the
island
okay
it's a little bit better than nothing
but
really doesn't help a lot
on the other hand layer normalization
is the idea that was proposed
by some other researchers and it turns
out to be more helpful
um to
um
to avoid iron
um you know um
make sure that girl
the the gradient like
doesn't does not go like a does not
explode or like vanish
during training okay so but how do we
implement the layer normalization okay
in keras okay
uh um
there's really no
um
um
sale okay
that the provider normalization by
default okay so what you can do what we
can do here is um
we
we could subclass
um
the iron cell okay
and then okay create
um
um the
uh
argument cell with the layer
normalization
functionality in there okay
so
so this is the thing okay thank you
okay
okay
so you can see
uh
this one we subclass
the layer
class okay to create the ln simple iron
sale
okay basically okay uh here i mean uh
we we have um uh state size output size
okay because this is basically uh
this one allows us to create a layer of
layer normalize
iron sale okay so
um
you cannot just create a cell with layer
normal normalization in it because the
layer normalization is done but in i
mean
you know by layer okay
so
so you have to create it uh you know as
a layer okay so in there okay so you can
see okay um
you could like create like a multiple
units
uh
micro iron
unit uh
cells in this layer and the activation
function uh by d4 is said to be
hypertension
okay
and uh i think
there's really nothing to
say too much about it here okay the only
thing
that's uh that we can mention is that
you can see
we call chaos.layers the layer
normalization function
prior
okay
prior to
the activation function
okay so that's normally how we do it
okay we we first like uh compute like um
the linear summation
for each cell
but before we
apply the activation function
okay we try to do the layer
normalization so that's why you see this
line is inserted between
this uh
simple ion cell
and the
activation function
okay
and this is like a uh this is like a
basic structure
okay and in in here is the uh the core
function is the the
kind of like a constructor
for these uh
um
um
um
for this uh uh this class okay so you
can see okay it takes two output
okay
when you
create this thing okay and these two
output okay the outputs new states
basically represent the same thing okay
because remember for simple r and okay
the
uh the the the sales state and its
output
understand remember
we talk about that right so so um if we
have a number of uh symbol ion sales
okay
their outputs
as well as their state
will be kept here so they will be uh
they will have two
different outputs but these two output
even though i say that i mean they
actually uh
are the same thing okay same thing
and we also have a
normalized output okay
by we use with here i mean be careful we
first
call the self dot layer no no okay to
quick to
to calculate a layer normalize a
normalized uh result
and then
apply the activation function
to get this known output
okay and then you can see okay this
known output okay it's written twice
okay why do we return twice okay it is
for the same reason okay because
well for basic iron sale
the state and output are the same right
so so we return two
of the same things
okay
so that's pretty much about it for this
uh
um
um
layer normalization version of the
simple iron cell
sorry
so
sorry okay so i think i already
explained like the code okay
here so i will skip this
this slide
and
so how do we use
this uh
uh class that we just created so it's
very simple okay um
just just
okay
instead of using
um you know
uh
basic r and
okay for kerastar layers that basic
island we use chaos the layers got iron
and this is like a generic
iron
layer okay and inside
here
okay we try to provide this
this uh we want to specify we want to
create
a
layer normalization uh version of the
simple island sale
we want 20 unit of that okay
and uh the other part remains the same i
think we already explained okay the
first two lines why do we need to have
return sequence to be said to be true
right because we want the output to
remain
um
to be sequence
sequential okay so
um
uh we set the return sequence to be true
okay so that it will you output
sequences like for each input okay it
will not just output the left
uh the last output only okay and input
shape i mean it's a noun because
the first line actually expect any
number of input
okay and that's about it okay
and the last line okay we use time
distributed
to to to to wrap the the dense layer
okay to
allow the dense layer to accept the
the sequence sequence input okay
um this part remains the same okay
so we could use this one okay to predict
the
um the the the sequences okay
um that we we discussed
earlier uh
here here
these sequences remember we talk about
this
so this is the code that we can i mean
can be used to do the same thing okay
and also okay you could okay for the
dropout okay we could also create a
custom sale to apply dropout okay but um
okay
for dropout
since
the the
the sale already
has this functionality building
so
we really don't need to write our
custom
sale for that okay you just need to
specify the hyperparameter a dropboard
hyperparameter
which is like the dropout rate okay
and but be careful uh that there are two
different dropouts
that we are talking about
for
the
iron sale okay the first one
is just a dropout the second one is what
we call recurrent dropout
uh you could i mean which can be set for
for for
basically uh for for basic iron sale or
like for
the future more complicated are in sales
such as lstm or gru i mean which we we
are going to talk about today okay
um
so
um the the first one okay the the
dropout okay
basically define the dropout rate to
apply to the input
okay so that i mean you know
uh uh you know once a while okay
uh when when if you throw a dice and you
know it turns out that
this no the shouldn't
uh participate in this the computation
is wrong okay the input will be disabled
okay
and the later which is recurrent dropout
okay it defined the dropout rate for the
hidden state
so it it is going to tell
uh the sale okay
not to use the hidden state for this
particular step
okay so
so
because remember the
uh ion cell has two different inputs the
first one is
just regular input the second one is the
the feedback
okay if you really want
um
the the the all the input to be turned
to to be turned off okay
uh from time to time of course i mean
then you need to set not only the
dropout but also the recurrent drawback
these two things are actually you know
handled separately
in the iron
um
you know uh recurring neural networks
okay
so so we already have such uh uh um you
know hyper parameter to
to to to deal um that you can set it up
so um so there's no need to create a
custom sale to apply dropout at
each time step in on okay
even though we could do that but
well
there's no need to do that okay so
so
um
so with that okay we talk about
dropout
and the recurrent dropout we talk about
layer normalization
all these techniques okay can be used to
alleviate unstable gradient problem
okay and which
allowed us to train the other more
efficiently okay
so
um
so we sort of
handle
uh
one of the two problems
no right i mean of course like uh uh
the
the unstable gradient problem okay uh uh
you you have more than these solutions i
mean uh last
last lecture we also talked about like
uh
uh gradient clipping
okay if you notice like a gradient
uh like uh become too large you can also
apply gradient clipping and
most of the techniques okay
that you have learned in
chapter 11 okay which uh that that is
used to learn to to train like a deep
neural nets
okay should help as well okay uh except
of course the
personalization
and uh uh um
uh we don't it doesn't the the the the
reload okay it doesn't help that much
okay but
but most of the other techniques okay
that you have learned in the previous
chapter should
also apply okay
so so we we we know how to deal with the
unstable gradient at least you have some
tools at hand
okay
for that problem so now okay we we can
focus on the second problem okay which
is a short shorter memory problem
okay
shorter memory problem is like uh you
know uh um
um when you use iron to train
uh
long sequence okay long sequence
um
uh the the
the earlier sequence the information in
the earlier sequence
tend to
fade away
gradually fade away
okay so here i mean
you know text would you use uh
dory dory the fish i mean you guys know
dory right
okay you have seen the movie uh finding
nemo right so the story in that movie is
uh is uh is the best example to talk
about this problem i mean
she
i should say she right i mean it's a she
i mean she
um
she really does not
couldn't memory memorize like a long
sentence she probably can only memorize
like show sentences
if you if you say something to dory for
two for i mean
talk too much too dory i mean she start
forgetting about what you uh said
earlier okay so this is like the issue
uh uh the shorter memory issue okay
so
to tackle this problem okay
there are
different types of cells
which can
you know can memorize okay can have a
long-term memory okay it's able to
memorize better okay
uh have been introduced okay
um
to be honest like uh uh they are so
successful okay
um
nowaday okay when we talk about
nobody use basic rn sale okay nobody the
reason we use basic rncl no okay i mean
at least i mean before
uh uh
this moment it's just because like it's
easier to
for you guys to understand
okay but uh from now on okay
uh i would suggest you okay uh you know
when you if you need to create a
uh iron
okay uh model okay don't use basic iron
sale okay you should use um you know one
of the two
uh
sales that we are going to talk about
okay the first one
is called lstm sale okay which is
perhaps the most popular one okay
what is lstm sale okay lstm of course
it's an abbreviation okay it stands for
long short-term memory
okay long short-term memory
why is it called long-shorted memory
it's um
i don't know it's really not a good name
okay it's uh it's um
but uh but this name has been
you know become like
as i said like this sale has become so
uh
popular okay
um
that everybody use it i mean for to
create our other network okay so this
was proposed in 1997 okay
and
uh actually uh um
there are a lot of
changes
a lot of updates
uh being made to this uh
lstm so
um
so the current iron sale structure is
really very different from
the one that was proposed in 1997 okay
but the idea
of course remains uh similar okay
so
if you consider the lcm sale as a black
box okay you can just use it just like a
basic uh basic iron sale
except it performs much better
trend training will converge faster it
will detect long-term dependencies in
the data so so basically it it gives you
all the desired property that you want
okay
with just a switch of the cell type okay
it's very very convenient okay
um
and in keras you can simply use the lstm
layer instead of simple iron layer
something like this okay so
keros.layers.lstm
you can see
okay
you can use this
command to directly create a layer of
lstm cell here we specify we want 20
lstm sale and the other part remains the
same okay so i will not go into detail
for the other part of this uh
but
you get an idea okay it's very
convenient to use
the kerastar layers that
to
very convenient to use the on last cell
okay
and
you could also use the genetic
general purpose
uh chaos layers dot
and layer and inside here okay you
specify you want to use lt lstm cell
so this approach will be similar to what
we did for the
layer
normalization
uh uh
iron sale
will be similar to what we
sorry here
you can see we use uh kerastar layers
and then here we use ln simple iron and
cell
right so so this the second uh the
second
way to use
uh lstm cell is just like uh
what we did for the
the layer normalized basic normalization
simple iron sale okay
um
the other part again remains the same
okay
so what's the difference between these
two
mostly they are mostly the same mostly
the same okay but i wouldn't say they
are exactly the same why because
okay if you use this approach
okay
when you
do not have gpu support
okay if your
machine does not have gpu support
then okay these two
uh way to to use austin cell will be
exactly the same
okay but if your machine
has gpu
okay
then
it is recommended that
you use this way to create a layer
uh of lstm sale
why because okay
uh this comment actually uh has some
optimization
okay
uh uh tour like a machine that has gpu
support
okay so
supposedly like uh if you
if your machine has a gpu
okay then
this method is preferred
okay okay just wrong you guys know okay
um so maybe maybe okay
it's better that you just remember this
way
okay
because well it doesn't when your
machine does not have gpu well it just
works the same way okay it's just
you just uh the the optimization part in
this uh
in this implementation will not function
that's it okay
so um
so how does the lstm sale
work how how can lsd and sale remember
memorize better
okay let's have a look
at the inside of the island sorry l stem
cell okay
let's open the black box
okay so this is
the
inside
the content of the black uh insider is
lstm cell
okay
so you have a c
you have an edge
okay
c
and the edge
up
you can say they are they represent the
states
of this lstm cell
okay they represent the states of this
error stem cell
okay so what's the difference
okay
c stands for the long-term state
h stands for the
you could say it's a shortened state
okay you could you could say that
okay
and
the
edge actually will be exactly the same
as the output you can see h t
is the same as the y t okay the same the
same flow okay is splitted into h t and
y yt
okay so so you could you could say the
shortened state okay is exactly the same
as the output
okay
and
um
here okay
uh when the the the the
the feedback comes in okay uh c
t minus one the h t minus one comes in
okay
uh you can see we have actually four
blocks here right
okay
this fc stands for
fully connected
network
okay
fully connected network so
in other words okay
lstm is a cell
that contains
multiple
neural networks inside it
okay
so
um
let me let me let me let me let's take a
closer look at this uh long-term memory
okay when c
comes in okay
first it goes through this uh forget
gate
okay this okay it's a multiple right
it's a multiplication right so it will
be multiplied by the output of this uh
fc we call this forget gate
okay forget forget gate okay and this
forget gate
is
um of course like a control okay just
like the other gates
it is controlled by both
the
the the shortened
state okay the previous the feedback of
the shortened state and the current
input
okay
so it basically uh come uh here okay it
control like
because
f2 okay uh we are talking about
even though okay
it's a long-term memory okay you cannot
because imagine that okay we have only
like a fixed amount of
storage right we have only a fixed
amount of like a space
to store information
right so you cannot expect okay we just
we just keep adding the information to
this uh c
okay and expect it to to to just just
remember more and more content
it's not possible it's not possible okay
if you want to see
this long-term state to remember some
new
important information for long run
you have to forget something first
you have to drop some information in
order to put in some more information
okay so that's the spirit okay
remember like uh
we this is a fixed amount of space
that
so you cannot put in like infinite
amount of information there i mean it's
just not possible okay so first you it
goes through this forget-gate to forget
some
information that is no longer such
important
no longer such important okay
and then
okay
when he comes here
okay first
we use this uh uh
we use this this this block okay this is
the only block
you can see
uh which is using is uh it's only three
connected uh network that use the hyper
tangent
as the output
okay so it will
try to extract
that from the the the previous edge
the shorter
state as well as the current input
you will try to extract
the important information from these two
sources
okay and uh
this is will be the uh
something that is important okay which
and then
when it goes here okay
we use this uh this is the input gate
again
here okay we try to you know
um
determine like uh
you know uh try to try to
uh
determine what information
should be
kept
long term okay
by multiplying this output with this
because this is sigmoid right remember
all these sigmoids will
output a value between zero and the one
okay so by out by multiplying this
output with a zero to one value okay
it's sort of like a select
certain information to be kept
long-term okay and
then you will have that that part of the
information
to be put here okay to
to be added to the
to the long-term
state okay
and uh well so this one will
pass
straight
okay out as the the the next
long-term information
okay
and then
okay
this
uh new
uh c t okay the same thing yo comes here
okay
comes here and uh uh go through a
hypertension
again
okay to extract some more some
information that is important
for as a shortened information and this
shorter information again it will be
you know it's going to be controlled by
this gate
okay
um so that like a
extract information from here to
to be the ht
as well as the
output okay
so keep in mind okay
all these uh
uh there will be a lot of like uh like a
weights
biases
in these fully connected layers fully
connected networks
obviously right
so those weights and the biases
will vary will be determined okay when
you train the network
okay
so when you create a lstm
island network
it actually it is a network of a network
okay because each iron
uh so each lst and cell contain
multiple networks
okay
so
uh by using this uh this type of uh
network okay
we are able to identify important
information to be kept long term
and we are able to identify like a like
a shorter information
as well okay
so so this is like you know uh the
why lstm perform better than the simple
rncl obviously you can see there are a
lot of like uh
mechanism
in building the rstnc or that can
perform they can
that allows the lstm sale uh
to perform much better than the simple
island sale okay
but you may wonder you may wonder
how come
you could you could understand okay you
can
it will perform better but you may
wonder how come it trains faster
it converge faster right
because it's it is actually complicated
okay to have all these um
things
uh including the the network inside
the election sales inside all the lcm
cells to stabilize okay to converge
okay but
well
uh
we have observed like uh some
something similar to that earlier right
remember like for example um
it's not necessary
uh
a very simple
activation function always converge
faster remember
we we talk about like
a very a very variation of uh lst uh of
the the
the riru
uh that's what's that what's the name
for that okay
it's like
a um
it's a linear function okay coupled with
exponential function
i think it's called elu right
remember
the eou function it's complicated
but uh
when you use that to train the network
okay it actually converges faster
okay so so
uh
the the the content in the cell to be
more complicated doesn't mean
uh if you use that sale to create
network it will converge slower not
necessarily okay so this is a very good
example of that but of course you could
i mean
um
you can still imagine okay even though
okay
uh the training may
be slightly improved but uh
you you probably could
could have done it much better if you
try to simplify the architecture of lstm
sale right
okay
so um
so i'll skip like the the the
discussion okay because i i already uh
tell you guys like how it works okay and
uh
um
um yeah
and uh so lcn sale okay can learn to
recognize the important input okay store
it as in in the long term state
uh learn to preserve it for as long as
you need is needed okay
and learn to extract uh the information
whenever it is needed okay
so this explains
why
the lstm based island has been amazingly
successful
okay it can capture long-term patterns
in time series long text and audio
recognition recording and so on okay so
um
and this this uh slide basically tell
you like you know the the the
mathematics formula okay behind that i
mean so each w basically represent like
uh
basically like uh
what is computed in in this fully
connected layer
okay in each of that so
so but really like uh it's it's it
should shouldn't be too difficult to
understand anyway so
uh
i'll skip these things or if you are
interested you can look uh take a closer
look but
basically like uh each of them just just
like a very basic uh um
um
perception okay multi-layer perception
calculations okay but uh
just like what i said okay uh um
you could we could
consider
improve
improving this llcm sale okay the first
some people at first some people think
okay
the gate controller okay
look at
only the input and the previous
shortened state right to determine you
know uh
uh you know such as like what to forget
how to keep okay things like that okay
but uh some people believe okay it'll be
a good idea to give them
also
the long-term
information
okay so in other words
okay
at least i mean this is the current
version of lstm sale okay you can see
this gate
the input include the current input
and the previous
short-term state right
but
some people believe okay
we should
also provide like a
this long-term
state okay
as the input for this gate
which may allow the lstm cell to perform
better okay so in other words okay um
we call such a thing as a
peephole okay because we are sort of
like looking
in the future okay so they call this
thing as a people but you you get ideas
so basically i mean uh in addition to
this okay now this ct plus one i see so
ct minus one
has also a link
to go to
this gate
to control like uh you know
uh you know how to forget what to
keep
things like that