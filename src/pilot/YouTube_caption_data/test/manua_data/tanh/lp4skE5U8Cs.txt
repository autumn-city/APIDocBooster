Hi , I'm Kobayashi from Sony.
In this video, I will explain the basics of
designing a Neural Network.
First, let's briefly review about Neural Network.
The Neural Network has been a technology to
simulate the learning functionalities of the
brain with a computer.
Artificial neurons are computer simulations
of the nerve cells in our brains.
The Neural Network is the result of combining
multiple artificial neurons.
For example, in the image classification literature,
if you input the image to this input layer
and enable Neural Network computation, you
will get the classification result on the
output layer as the calculation result of
the Neural Network.
For example, this is 0 in handwritten digit,
and you get a classification result of 0.
And that is Neural Network.
Any combination of these artificial neurons
is called a Neural Network.
Therefore, there really are various forms
of Neural Networks.
Typical applications for image classification
use this kind of Feed Forward, that is, Neural
Network, in which data flows from left to
right.
And this Deep Learning is a multi-layered
version of the Neural Network.
By using a Neural Network with a very large
number of neurons and a very large number
of layers, it is possible to achieve high
performance that exceeds any classification
performance.
You can learn more about Deep Learning in
the video titled "What is Deep Learning?".
If you haven't seen it yet, please refer to
the link in the description box.
Now I would like to explain the design of
the Neural Network.
Here is an example of Feed Forward Neural
Network, the simplest structure of Neural
Networks.
And this Neural Network can be expressed in
terms of a combination of functions.
The Neural Network shown here is a four-layer
Deep Neural Network that performs handwritten
digit classification.
The input is a monochrome 28x28 pixel handwritten
digit.
And the number of neurons is 1000 in the first
layer, 300 in the next layer, and then 100,
and 10 in the last layer.
The reason why the number of neurons in the
last layer is 10 is because we want to identify
the handwritten digit of this input image
from 0 to 9, so there are 10 output neurons
corresponding to the digits 0 to 9.
This 4-layer Neural Network can be represented
by a combination of 8 functions, as shown
below.
From the top are Affine, Tanh, Affine, Tanh,
Affine, Tanh, Affine, and Softmax. The Deep
Neural Network can be represented by a combination
of these eight functions.
Now, let's take a look at each of the functions
mentioned here.
The first one is Affine, which is a function
called fully-connected layer.
Since the input neuron is a monochrome image
of 28x28 pixels, there are 784 input neurons
of 28x28 in total, in this Affine function.
And the output neurons in the first layer
were 1000.
It is called a fully-connected layer because
the input and output neurons are all connected
in all combinations.
As explained in the previous video on artificial
neurons, the value of this output neuron is
determined by multiplying and adding the values
of all the input neurons by different weights
w, respectively.
Multiply this first neuron by a weight w and
add, then multiply it by another weight w
and add, then multiply it by another and add,
then multiply it by another and add, for all
784 of these neurons, and you have the value
of the output neuron here.
Then, to determine the value of the next output
neuron, we again multiply these 784 input
neurons by a different weight, w, than we
did earlier, and add.
This function called Affine does all these
processes for these 1000 output neurons.
The weight w exists for all combinations of
inputs and outputs. For example, if the number
of input neurons in our example is 784 and
the number of output neurons is 1000, that
means that there are 784,000 weights w here,
which is the result of 784x1000.
This function Affine may be called by a different
name depending on the arrival of the Neural
Network.
For example, it is called linear because it
is linearly connected .
Or, since all inputs and outputs are connected
in this way, it is sometimes called "Fully
Connected".
In addition, since all of these inputs and
outputs are densely connected, it is called
Dense, and many other variations exist.
And the next function that comes up is Tanh.
This Tanh is a hyperbolic tangent itself,
but in the world of Neural Network, Tanh converts
the input value into a non-linear one and
keeps it in the range of -1~1.
It is used to serve as an activation function.
Tanh looks like this when written in a graph.
The horizontal axis is the input value and
the vertical axis is the output value.
As you can see, when a large positive value
is input, the output value is stuck to 1,
and when a large negative value is input,
the output value is stuck to -1.
And when the value in this area is input,
the output changes almost linearly.
And I would like to remind you of the formula
for artificial neurons.
Look at the electrical signal that came in
for this one neuron.
In the artificial neuron, the first process
was to multiply these electrical signals (X1,
X2, and X3) by a weight W and add, and then
add the bias.
The parentheses in the formula for this artificial
neuron correspond to the Affine function.
Then there was an activation function that
performed nonlinear processing on the output
result . This time we use Tanh as the activation
function.
There are many other types of activation functions
besides Tanh.
Here I will explain using this traditional
activation function Tanh .
In the Deep Neural Network, which I mentioned
earlier, this affine and tanh process is repeated
three times.
This combination of Affine and Tanh can be
used once to function as a Neural Network
for one layer.
By repeating this three times, a three-layer
Neural Network is constructed.
Finally, we use Affine again to compute the
values of the 10 output neurons corresponding
to the digits 0 to 9.
Then, using a function called Softmax, these
10 values are converted into probabilities
that add up to 1.
For example, if you input a digit 9 and the
probability of 9 is 0.8, or 80%, then the
remaining probability of the digits 0 to 8
should be 20%, or 0.2.
Using the Softmax function, we can convert
them to probabilities that will sum up to
1.
Next, I will explain the structure of the
Neural Network called the Convolutional Neural
Network.
The Deep Neural Network, described earlier,
is rarely used in the image classification
literature these days.
Instead, it is known that this structure,
called the Convolutional Neural Network, can
be used to achieve very high performance.
Let's take a look at the functions used in
the Convolutional Neural Network.
At the bottom of this section, we have Convolution,
MaxPooling, and Tanh. Here we have new functions
called Convolution and MaxPooling.
After that, repeat this Convolution, MaxPooling,
and Tanh .
And after this, it's exactly the same as the
Deep Neural Network I mentioned earlier.
The combination of these functions, Afifne,
Tanh, Affine and Softmax, make up this Convolutional
Neural Network.
So, the difference from the previous one is
that the functions Convolution and MaxPooling
are used.
â€™This is the difference from the Deep Neural
Network I mentioned earlier.
Now, this is what this function called Convolution
does. It looks like this in the animation.
This process performs a filter convolution
operation on the image and outputs the processed
image.
If you're not familiar with this convolution
operation, think of an application that processes
images on a smartphone, for example.
There are a lot of applications that blur
or sharpen or change the color of an image.
For now, it suffices to know that Convolution
is to process images like this.
Normally, when processing an image, a single
image is output for each input image.
However, in this Neural Network world, Convolution
applies several different filters to the input
image, in this case six different filters,
and outputs six different images.
For example, if the first filter is a blur
filter, the first image will look like the
input image is blurred.
If the second filter is a sharpening filter,
the second image will appear as if the input
image has been sharpened.
That's how you would get six different images
in this example, each corresponding to a different
filter for a different image.
This process is Convolution.
This is followed by a process called MaxPooling.
The MaxPooling function is, in a nutshell,
the process of downsampling.
For example, halving the resolution.
In this example, the result of the convolution
process is six 24x24 pixel images, while the
input image is 28x28 pixels.
The MaxPooling function will down-sample the
image in half lengthwise and widthwise, and
at this point we will have six 12x12 pixel
images in half lengthwise and widthwise.
Tanh after this is exactly the same process
as the previous one.
It is a process to fit the result of the process
so far to a range between -1 and 1.
And repeat the same process.
Using Convolution, the filter is convolved
into six of these 12x12 images to produce
a single output image.
This filter also uses multiple filters in
the same way as earlier, in this case, 16
filters are used in total to output 16 8x8
pixel images.
Now, we'll run MaxPooling again to halve the
resolution, in other words, to 16 4x4 pixel
images with half the height and the width.
The Tanh function is then used to fit the
value between -1 and 1.
After this, we have the same structure as
Deep Neural Network with Affine, Tanh, Affine
and Softmax.
At this point we have 4x4x16 pixels, or in
other words, 256 neurons, so we take 256 neurons
as input and perform a full connection and
Tanh with 120 neurons as output.
And from there, we do Affine with 10 neurons
as output.
The Softmax function then converts the 10
output values to probabilities that add up
to 1.
You can configure the Convolution Neural Network
in this way.
Now, let's talk about the Convolution process,
comparing it to Affine.
In Affine, the input and output neurons are
fully connected, that is, each output neuron
receives the values of all the input neurons.
On the other hand, the difference in convolution
is that in a 5x5 Convolution, for example,
the top left pixel of this output image is
only connected to the neurons in the top left
neighboring area of the 5x5 pixel.
Specifically, this 5x5 kernel, which is equivalent
to the weight w, is multiplied by the values
of the upper-left 5x5 pixels of the input
image to the 25 values of this 5x5, and the
values of the neuron in the upper-left corner
of the output is determined.
If you want to calculate the value of a neuron
in one neighboring pixel, you can move this
5x5 area to the right by one pixel, and then
multiply and add the 5x5 image in the Input
image by this 5x5 kernel weight w. This is
how to determine the value in one neighboring
pixel.
This process is repeated from the top left
to the bottom right, and so on and so forth.
The value of the lower right neuron is similarly
determined by multiplying and adding a 5x5
pixel image near the lower right pixel by
this 5x5 kernel.
As you can see, there is a big difference
between Affine and Convolution in the fact
that it is only connected to the neurons near
the output pixel as opposed to full connection.
One more thing is the weight w. Earlier, in
the case of Affine, if one output neuron was
different, it was multiplied and added by
a completely different weight w.
In the case of Convolution, we use the common
weight w when calculating the top left pixel,
when calculating the one next to it, and when
calculating the bottom right pixel.
In the case of the fully-connected layer,
if there were 784 input neurons and 1,000
output neurons, there were 784,000 weights
w of 784x1,000.
In the case of Convolution, the 25 weights
w, which are 5x5 are commonly used throughout
the image, so there are only 25 weights w
per image processing session.
Another major difference between Affine and
Convolution is that the parameters are shared
by each output pixel in Convolution, and the
number of parameters is significantly less
than with Affine.
Now that we have introduced the Deep Neural
Network and Convolutional Neural Network structures,
we have seen some functions such as Affine,
Convolution, MaxPooling, the activation function,
and Softmax.
Basically, however, the Neural Network is
composed by arranging the structure that composes
this single layer many times over.
When using Affine, which is a fully-connected
layer, Affine and the activation function
are repeated.
When Convolution is used, the Convolution
and activation function are repeated.
However, in the case of convolution, this
downsampling process called MaxPooling can
also be added if necessary.
Basically, t he Neural Network is constructed
by alternating the functions that perform
multiplication and addition shown in the blue
area and the activation functions that perform
nonlinear transformation shown in the red
area.
Next, we will explain MaxPooling using the
figure.
MaxPooling is a function that takes the maximum
value of neighboring pixels and outputs it.
For example, if you want to do MaxPooling
of 2x2 pixels, take the maximum value of the
adjacent 2x2 pixels, that is . 4 pixels, and
output it like this.
This process halves the vertical and horizontal
resolution.
There are many other types of pooling.
For example, AveragePooling takes the average
value of these four pixels and outputs it.
In the case of MaxPooling, it takes this maximum
value, and that's why it's called MaxPooling.
And what is often asked is the total number
of Neural Networks, the number of neurons
in each layer, and in the case of the Convolutional
Neural Network, the number of images, and
the type of activation function.
We have introduced Tanh as an activation function
earlier, but there are many other types of
activation functions.
I'm often asked how to determine this.
There is no particular correct answer to this.
The optimal structure of the Neural Network
depends on the type of problem to be solved
and the amount of data.
Therefore, each time we build a new classifier,
we need to decide the optimal number of layers,
number of neurons, types of activation functions,
and other Neural Network structures by trial
and error.
Basically, a Neural Network that can achieve
higher performance in a problem to be solved,
or one with performance that requires less
computation, is better.
Therefore, in the process of developing a
classifier using the actual Neural Networks,
we try to change the total number of Neural
Networks, the number of neurons, and the activation
functions.
In this way, we train them and measure their
performance to find Neural Networks with higher
performance or less computational complexity,
through trial and error.
This is a very time-consuming process, so
in recent years there has been a lot of research
into automating this design process.
The Deep Neural Network and the Convolutional
Neural Network introduced so far can be used
for both vector-based and image-based discriminations
and domains.
However, only the end of the Neural Network
needs to be designed to fit the problem you
want to solve.
In the classification problem and category
classification introduced so far, the Softmax
function is used at the end to convert the
output of Neural Network into probabilities
that add up to 1.
Then, we add a loss function.
This loss function specifies a training indicator
of what the Neural Network should be trained
with.
In this classification problem, we use CategoricalCrossEntropy
to calculate how well the probability of each
category, which is the output of Softmax,
matches the correct answer given by the person
using cross entropy.
In order to give an indication that the Neural
Network should be trained so that the cross-entropy
is small, we specify a loss function called
CategoricalCrossEntropy afterwards.
In the case of binary classification problems,
we use a function called Sigmoid instead of
Softmax as the last activation function.
This function, Sigmoid, is similar to the
Tanh function we introduced earlier. While
Tanh fits the input value between -1 and 1,
this Sigmoid fits the input value between
0 and 1.
The value from 0 to 1 output by this Sigmoid
is treated as a 0% to 100% probability.
We then use BinaryCrossEntropy as a loss function
to calculate how well this probability of
Sigmoid's output matches the 0 and 1 given
in the training data.
This BinaryCrossEntropy is specified in the
sense that it minimizes the CrossEntropy between
the output of Sigmoid and the training data
given by the person.
And when we want to estimate a regression
problem, that is a continuous value, we often
don't use an activation function.
The output of Affine in the Neural Network
is used as the estimate.
Then we use this loss function called SquaredError
at the end to minimize the squared error between
the output value of the Neural Network and
the correct answer given by the person.
As we start to solve various problems in the
Neural Network, the loss functions, or optimization
metrics, are often designed by the developers
themselves to fit the problem.
For now, as typical loss functions, if you
remember the binary classification problem,
the classification problem, and the regression
problem, you can apply them to various tasks.
So, let's wrap up this video.
The number of input and output neurons in
the Neural Network is determined by the size
of the input data, depending on the problem
you want to solve.
The number of neurons in the output is then
determined according to the size of the answer
you want to get as the output of the Neural
Network.
In the example of handwritten digit classification,
the number of input neurons was 28x28 since
the image was 28x28 pixels, and the answer
we wanted to get was a digit from 0 to 9,
so the number of output neurons was 10 neurons
corresponding to each digit from 0 to 9.
We first determine the number of neurons in
the input and output.
Then select the last activation and loss functions
according to the problem you want to solve.
From the input to the last activation function,
the combination of Affine/Convolution for
weighted addition and the activation function
was used to form a single layer, which was
connected many times to form intermediate
layers.
And at this stage there is no particular correct
answer for the number of neurons in each layer,
the type of activation function, and the number
of layers.
When designing a Neural Network, we search
for the best structure by trial and error
in terms of the number of layers, number of
neurons, types of activation functions, and
so on. In this way, we can achieve higher
performance, require less memory, and require
less computation.