Have you tried adjusting the learning rate of RMsprop optimizer ?  try with a very small value first ( default is 0.001, in keras implementation) and try to increment it with factors of 10 or 100.
Adam is a recently proposed update that looks a bit like RMSProp with momentum.
Just a question for you, why aren't you using Adam Optimizer which seem to be the best optimizer in a lot of cases ?  (It is even partially inspired from RMSProp that you use)
Although the expression "Adam is RMSProp with momentum" is widely used indeed, it is just a very rough shorthand description, and it should not be taken at face value; already in the original [Adam paper (hyper-link)], it was explicitly clarified (p. 6): 
There are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running average of first and second moment of the gradient. 
