hello everyone so yeah as Roland said I
will give an introduction to Piper
today probably the thing you all want to
know is what is pi toys but let me ask
you a question first like whom is using
Python in this room justly and to use
numpy and who would like to numpy to run
30 times faster ok then pi touch is
something for you because pi torch is
basically an ndre library with GPU
support and if you run stuff on the GPU
it goes faster if you can paralyze it
which is most of the time the case with
arrays but instead of going through like
what pi/2 it can actually do and how
awesome it is let me just show you code
that's always better so similar to numpy
you always have to import torch check ok
that was boring nothing happened you can
build tensors with PI torch just by
saying I want to have a tensor it size 5
pi/3 can you read it in the back row
otherwise I will make okay awesome
the really cool thing about PI torch is
that you can actually print the tensor
and you get values on the console
because if you use tensorflow
or Karis it will tell you like oh yeah
this is a tensor somewhere in the graph
but you never really see the values
which are in this tensor which is super
annoying for debugging because that's
like the most awesome debugging tool to
print you can see there are like really
weird values in there everything from 10
to the power of minus 41 to 10 to the
power of 21 or even higher this is
simply because if you just say I want to
have a tensor it just tells you ok
here's your data and it's just whatever
was in the memory before so normally you
want to go with something like this you
for example want to randomly initialize
the array which gives you an error which
is basically containing values between 0
and 1 randomly you can as well always
ask pie-chart ok what is the size of the
tensor it will tell you 5 by 3
you can again print the values but of
course you normally especially in deep
learning you don't really want to see
the whole tensor because it's simply
huge the cool thing of Pi torch is you
can as well slice like you used to do in
numpy so you can just say okay I want to
have basically in this case the first
column and the second column and it
gives you the second column so since
it's so similar to numpy there's of
course a bridge to numpy because that's
what everyone uses in python nowadays
when they have areas like the first
thing they do is okay I read it into
numpy area and then I figure out what I
do the really cool thing is so we built
this tensor X and want to have the numpy
version we just say okay dot mam PI and
we print it and it prints an umpire E
but the really cool thing is it does
that without any overhead so none PI and
PI torch are pointing to the exact same
data so it's basically just telling like
okay one outputs you the PI torch
version of it the other than PI but if I
for example now at one to the PI touch
tensor it adds it as well to the
number-10 so simply we have because we
have the same reference and that simply
makes the conversion between an PI and
PI George super fast because basically
just tells you like okay now interpreted
as pipe towards data and don't copy it
at all the same thing holds for the
other way so if you have numpy data like
in this case an area of once you can
just call torture from numpy and it
makes a torch tensor out of it and again
if you add something to the numpy data
it will be added to the PI torch data so
that's really nice to replace numpy but
that's not why most of you are here on
the conference so numpy offers other PI
torch office other things as well it has
an automatic differentiation engine
meaning that it can every variable knows
how it was calculated and can calculate
the gradients with respect to every
variable in the system which simply
allows you to do
keep learning on this with this
framework or reinforcement learning on
top of that you have some gradient based
optimization packages like simply
gradient descent optimizer or atom
optimizer or you name it okay you not
name it because it's one year old so
they are still implementing some of them
but they already have sick or the 6m7
most common ones implemented and you
have utility so there are futilities to
load data they provide by default some
standard data sets with their API so of
course m-miss but as well see fartin and
some other ones so the way this is
working and now it's getting a bit
uglier is basically from the autograph
package from Nampa from torch you import
a variable the variable is wrapping the
tensor so it can remember how it was
calculated and how the gradients flow
through the system the functional you'll
see later is just a convenient library
where you can find function notes so
that you just have to import and then
for example let's build the four
different variables like in this case X
previous age two weights and then we
calculate I to H and H to H we sum them
up and we put an activation function on
it so basically building a really simple
neuron so in this case it calculates now
next H so you can actually already see
what is in next H if you have given an x
and y which is different competitor
tensorflow because there you build a
graph then you put data in and when you
run it you actually get the result with
patriots you get the result immediately
but now if you want to have the
gradients with respect to a variable you
just called dot backward with a given
variable so in this case I just put once
in there to show how it works and then
I'll just add a cell you can
just for every variable print the
gradients given this output for this
input so if you have as well as some
problem where you have an input and you
know the output and you want to
calculate the error gradients you can
just print them easily to the console
compared to tensor flow where you would
again need to build an output node to
then catch the data to then export them
to the CPU to them and print them which
is a bit cumbersome if you have to debug
it so let's see how this actually works
on your network so with torch you can
install torch vision which is simply
this utility package in this case we
will um use m-miss because everyone uses
em mist because it's good to show some
cases so I hope it's still downloaded so
what it does is it's downloading the
data in the background and just provides
you with the training the test set and
related loaders to actually train your
newer network so for example in this
case it just go and have a look at the
training data and at the training label
so you can see it's a 5 and it kind of
looks like a 5 as well sorry for the
heat plot I just do heat plots all the
time
they look fancier than greyscale so we
of course now want to build a neural
network which can actually figure out
how a mist works the way this is done in
pi torch is that every newer network
inherits the n + dot module class which
simply does most of the parameter
handling in the background and you have
it all and capsulated like you're used
to an object-oriented programming it's
like everything you need for the newer
network is part of this newer network
class which is quite handy
compared to tensorflow or Kerris where
you have to figure or you have to find
the parameters if you actually want to
change them you have to find the
variables in the GPU memory
until you just go through the parameters
of this object and you have basically
everything which is related to the
neural network in there so in this case
I just use a really simple model module
model neural network with two
convolutions and three linear layers and
a bit of dropout in between
I choose a criterion for optimization in
this case with classification simply
cross-entropy
and an optimizer so I just choose the
item optimizer because I like it the
most it's like it's a bit of dark magic
and everyone likes to take his own
optimizer you will get a preference
throughout you're working with deep
learning and I have to hurry you then do
a forward pass which is basically then
replacing calling the object so every
time you call net and put an input there
it will call the forward function and to
train the neural network you zero the
optimizer that you just have to do
because the buffers gradients you
calculate the output you calculate the
loss you do backward propagation with
the loss and then you step the optimizer
and that's how you call it it's
basically you build the model you built
an input variable in this case I just
start with random stuff and this is how
you calculate the output you just call
the object cool thing is if you want to
do it on your GPU you just call once dot
CUDA it puts all the data into the GPU
does the calculation and if you then
need to get the data back you call dot
CPU it puts it back into the CPU and you
can just print it to the console and
it's all really nicely embedded into
Python so I could like go way more into
this but I was told I have to stop
[Laughter]
strictly doing ten minutes with Python
but I hope you all took something from
it and you at least saw how it
integrates
[Applause]