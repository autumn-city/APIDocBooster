Short answer: There isn't a standard way to use AdamW in Gluon yet, but there is some existing work in that direction that would make that relatively easy to add.
Gluon-NLP has a working version of AdamW - possibly slightly different from the one in the original paper: [https://github.com/eric-haibin-lin/gluon-nlp/blob/df63e2c2a4d6b998289c25a38ffec8f4ff647ff4/src/gluonnlp/optimizer/bert_adam.py (hyper-link)]
The adamw_update() operator was added with this pull request: [https://github.com/apache/incubator-mxnet/pull/13728 (hyper-link)]  This is first released in MXNet 1.6.0.
Yes, Adam and AdamW weight decay are different.
Hutter pointed out in their paper ([Decoupled Weight Decay Regularization (hyper-link)]) that the way weight decay is implemented in Adam in every library seems to be wrong, and proposed a simple way (which they call AdamW) to fix it.
AdamW follows the second equation for weight decay.
In AdamW
Plugging the tensorflow optimizer AdamWOptimizer directly into the optimizer argument of fit runs perfectly.
The problem is that weight_decay is the first positional argument of tfa.optimizers.AdamW.
see [https://github.com/adamw/elasticmq/blob/master/rest/rest-sqs/src/main/scala/org/elasticmq/rest/sqs/SQSRestServerBuilder.scala (hyper-link)]
Also, there is a Keras implementation of AdamW, NadamW, and SGDW, by me - [Keras AdamW (hyper-link)].
I haven't yet run any tests of single-cycle schedule or AdamW for the learning rate, but I did a very basic test with a two stage learning rate adjustment for both Actor and Critic (starting with a high rate and dropping to a low rate) and the results were a clearly more precise solution that converged quickly during the high learning rate and then honed in better with the low-learning rate.
I imagine AdamW's better weight decay regularization may result in similarly better results for avoiding overfitting training batches contributing to missing the optimal solution.
Based on the improvement I saw, it's probably worth trying single-cycle methods and AdamW for the actor and critic networks for tuning the results.
