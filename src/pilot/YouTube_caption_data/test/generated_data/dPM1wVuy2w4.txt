um my name is venkat gurswami i'm a
senior scientist at the simons institute
and it gives me great pleasure to
welcome you all for the boot camp of the
data driven program just i'll keep it
pretty brief um brief history for
those may not know the simon's institute
was born in 2012 10 years ago thanks to
generous grant from the simon's
foundation
and it has ever since hosted
basically two programs
every semester um on topics related to
theory of computation you know theory of
computation but also its interface with
all areas in science engineering
humanities economics you name it so it's
really been setting the agenda for the
field and so many people have come
through here and recently it has also
expanded its activities to summer
programs hammer clusters and all sorts
of outreach activities as well
uh the institute just celebrated its
10th um birthday uh in may and there was
a big symposium celebrating that and i
guess in terms of programs uh the first
program was in fall 2013 so i guess this
marks the
first program in the 10th year of
scientific programs of the
uh
of the institute so no pressure so i'm
sure it's going to be great
program and there will be a parallel
program of course which will have its
boot camp i think next week
just very few
logistical things i think the schedule
is out there the coffee breaks are going
to be sometimes we have it outside but
now we're going to start out
in the
area where you were just there and one
um kind request please uh you know
finish your coffees and you know and
snacks outside you know i think we have
sort of a water in bottles is okay but
no drinks policy inside the auditorium
keeps it clean for everyone
other than that
i don't think there's anything specific
we have
staff and others always on crew if you
need something and i wish you all a very
successful boot camp and program
the first stock of the first workshop of
the first program or something something
no pressure
all right
so uh i'm shipra graval and i'm one of
the organizers of this program and uh
really delighted to see you all here and
thanks sid for organizing the boot camp
with his co-organizers
and
so i will be introducing stochastic
multi-arm band-aids it will be a
two-part introduction of one hour each
and
the general theme of today i believe is
based on bandits we'll see more
information about bandits going further
so let's get started
so uh what is this this topic about and
you will see more of this theme going
forward today later in in other talks
and tomorrow as well
overall it's about learning from
sequential interactions so how when we
make decisions that interact with people
and collect data and we make them
sequentially so that the data collected
from the past decisions and the response
is used in future to make better
decisions so for example uh think about
online retail where you are presenting
some options to customers that they can
purchase and you observe what they
purchase and that gives you or purchased
or interacted with and that gives you
some data that you can use in future in
going forward to make more decisions
that gives you more data and so on same
way you can think about ad placements
movie recommendations
and more classic applications like
clinical trials where you uh collect
data from the results of trials on
patients to improve the trials or infer
something about future trials
and same way in reinforcement learning
applications like robots robotic
manipulations and so on so what's common
among these is that decisions are
serving two purposes now so one is the
usual purpose which is you have some
kind of objective like immediate reward
clicks conversions
purchases that you want to maximize but
there's also another purpose which is to
collect data so you're also thinking
about what is the information that these
decisions are providing you beyond just
the immediate reward or the immediate
clicks and conversions and so on so the
information could be of the type uh like
what did the customer not like right so
even if it failed in terms of generating
a click it might have succeeded in
providing you some information about
their preferences
so there is a
as as in life there is always a
trade-off between what you want for now
or and what you want for future so there
is a usually a trade-off between
selecting decisions in for the
information they provide versus the
immediate reward they provide and this
trade-off is what you will see
repeatedly in many uh of the topics uh
related to this which is the trade-off
between learning and optimization
between exploration and exploitation
and to formalize this trade-off
we'll start today with the very classic
and
relatively simple formulation called
multi-arm bandits
so the name of this problem which is
formulated long back
comes from this idea of a casino and
just indulge me for a bit as i go
through this casino example you might
have seen it before
so
imagine you have a hundred dollars and
you walk into a casino which has 10
different machines and you want to
decide where to put your money all these
machines are rigged in different ways
but you don't know which machine is
better than the others
and the only way to find out about a
particular machine is to play it there
is no other information available to you
and now you every time you play you have
to put a dollar in it so now you have to
decide how to use your hundred dollars
in the most wise way obviously your goal
is to get out with as many as possible
in the end
so you could either use your hundred
dollars and just randomly play on all
the machines or you could first
randomly play for fifty dollars learn
about the machines and then
play the machine that you found to be
the best among the among the 10 or you
could do something more adaptive you
could put like two dollars in every
machine then
narrow down to a few machines put more
money in that then narrow down to a few
machines which look better and so on and
so forth
so you have to decide how to divide your
money between exploration exploitation
that's why you will have this trade-off
whether to learn more or to just use
your current information and exploit
that information all the time
so
more formally we think about stochastic
multi-um bandit and here in this talk my
talk i will focus on the stochastic
multi-arm bandit problem specifically
this version of the problem later on in
the day you will see other formulations
like adversarial combination of
stochastic and adversarial and so on
so in the stochastic multi-unbanded
problem uh we think about discrete
rounds uh discrete capital t rounds in
every rounds you have to pull one out of
some n discrete arms so this is the
basic version we will extend it later to
more general versions
one out of n discrete arms and every
time you pick an arm a reward is
generated so this is one of the key
assumptions reward is generated iid for
that arm independent of other arms and
independent of time
from a fixed but unknown distribution
and we assume bounded support again
there are generalization to unbounded
support depending on assumptions made
so one of the key things that we call
bandit is some kind of information
restriction to what you play right so
here in this version we will assume that
you only observe the reward of the arm
you pull and you don't learn anything
about the other arms which means like if
you if i put a dollar on one machine i
only get information about that machine
i don't learn anything about other
machines again we will generalize this
assumption to so that you can learn a
little bit about others and so on
okay
so in this basic version we are thinking
about regret
regret is how much i could have made
if i knew which is the best machine
right so if i knew what is the best arm
in terms of expected reward so
so every arm has a mean mu i expected
reward me mu i and the best
best thing you can do in terms of
expected reward maximization
is to pull the arm with the highest new
mu all the time
and if you pull something else regret is
the difference between what you got in
expectation versus what uh what the best
flavor so here the expectation
i have already taken new off the arm i
pulled i t is now my pull but the
expectation is over my choice of i t
which depends on my previous plays and
so on
so before we move on to uh kind of
diving deep into this formulation i just
want to mention uh another formulation
of stochastic banded problem uh which
was in fact the
which was what you would you would have
thought of as bandit before
machine learning people got into it
basically so
so the classic
meaning of bandit uh is is is a slightly
different formulation or slightly
different way of thinking about the
problem which is called the bayesian
bandit so the idea there is that you
have a
uh
you have a prior distribution on the
unknown parameter which is the mean
let's say or the distribution of every
arm of the reward distribution of every
arm
so you know this prior distribution and
every time you play the arm you the the
distribution your belief about this uh
this arms mean evolves for example you
may think that every arm has a bernoulli
distribution with mean mu i you don't
know mu i but you have a belief about mu
i and every time you observe something
from this arm your belief about nu i
changes according to bayes rule
okay so now
you can think about how should i
play the arms so that not only i get a
good reward in every step but also my
belief evolves fast enough so that i
know more and more about every arm
so you you can kind of lay out the whole
tree which is if i start from this prior
distribution if i play in this way this
will be my beliefs at these times and i
can lay out the expected total reward
over all t capital d time steps so this
is a stochastic process
for which you have complete knowledge of
how it evolves but of course it's a
difficult problem to find out what's the
optimal policy of this for this
stochastic decision process okay so the
this version of the problem considers
what is the optimal policy let's find
out what is the optimal policy for this
stochastic process so it's it's overall
a stochastic optimization problem in
this setting
and this is a very hard problem
it's not a data driven problem in the
way we define it right now but it is
still a very difficult stochastic
optimization problem and a celebrated
result by uh john c giddins in 1979
showed that for certain setting
discounted settings you can actually
have an optimal policy that you can
compute and you know what exactly what
to do in every time steps in the optimal
way and this is very important this kind
of solutions are very important
especially when you don't have big data
right if you have a clinical trial
setting with only 100 trials or very few
trials then you maybe want to do the
optimal thing and not want to minimize
regret and so on
so so
more recently there is also work in
regret kind of framework for for this
problem where you're not looking for
optimal policy but still trying to
minimize regret but you have a prior
distribution still so you can do better
potentially
uh and and i will not go through these
works today but this is something for
your reference if you're interested in
this setting
so
so this is just my tribute to this
important field but if you want to know
more yeah
can you
tell us a little bit more why is
this one yeah so it is the same it's
just that the focus is on finding the
optimal policy which is very difficult
in most cases so this only you can only
do it in certain specifications
but it is the same if you know the prior
yeah so these two are exactly the same
yes yes so the problem essentially is
the same
except that first we don't know the
prior second uh finding the optimal
policy is very hard in most cases except
for specific cases where there is
indexes
uh
[Music]
but not
so i will not go back to that again but
feel free to discuss more in the break
about it if you are interested
uh so what i will talk about is the
algorithmic techniques for the
the version i discussed the stochastic
multi-unbanded problem where we don't
know the prior and we want to minimize
regret so the way we treat it would be
basically to focusing on can we have a
regret that doesn't grow linearly with
time
compared to the optimal
action
and i'll discuss two techniques ucb and
thomson sampling and then
generalizations
of of the of the problem to
to remove some of the strict assumptions
we made
and later on uh tomorrow i believe you
will see some of these techniques being
used for markov decision process
reinforcement learning and so on
you just do just this is to refresh your
memory of what my problem definition was
i'm just flashing this slide once so
that you can recall the
notations so so what is the issue here
so i i refer to the issue a little bit
as well i want to balance exploration
exploitation i want to collect
information not just
use the information i have but let's
concretize this in terms of theory like
where does where does it break if i
don't explore why what breaks right so
here's a very simple toy example um you
have let's say you have the
multi-unbanded problem with two arms one
let's call a black arm and a red arm and
the black arm is better of course you
don't know it uh black black arm has a
mean 1.1 the red arm has mean one
and you don't know anything about this
this mean
the optimal if you knew what is better
you would play black arm all the time
and you will get 1.1 times t
now what would be the exploit only
strategy in the sense that if you didn't
care about learning or
collecting data in a better way
then the exploit only strategy the
typical machine learning
strategy would be let's look at all the
data we have
find out the best estimate the best
predictor of the mean of each arm
and let's play the arm with the better
mean the let's say maximum likelihood
estimate or empirical mean which is the
best estimate of the mean
and then let's play the arm with the
better mean because that will maximize
the immediate reward of the next play
okay
so if you do keep doing that what can go
wrong so here is an example of what can
go wrong with a very like constant
probability almost
so let's say you you did something
initially and you you are in this kind
of stage where you have played the black
arm for a few times the red arm for a
few times and these are the outcomes
let's assume for simplicity the red arm
is actually deterministic it always
gives you one so problem is easier you
don't have to learn anything about the
red arm okay so you are at this stage
three trials of each arm and they are
giving you this so
it can happen even though the black arm
has mean 1.1 it can happen with a
constant probability that right now with
three plays its means fall falls below
one okay
so right now the mean is below one for
the black arm the mean is one for the
radar so now what will happen according
to your uh best estimate strategy you
would play the red arm for now because
it seems to have a better mean
and that's fine in machine learning we
do make a mistake sometimes if you don't
have enough data but what goes wrong is
that because of that you play the red
arm and you get another sample of red
arm
which is useless you didn't learn
anything new you already knew radam has
mean one
then again you calculate the mean the
situation remains the same you will play
red arm again and you will keep doing
that endlessly
so the problem is not only you made a
mistake you have no way to correct your
mistake because you don't learn anything
from playing the this disarm
and your exploit only strategy keeps
asking you to play that same arm to
maximize the immediate reward
so to to get out of this situation you
have to kind of do some exploration so
you have to say that even though right
now this arm seems best but i don't have
enough place of black arm to trust it so
i need to collect some more data about
it and explore it
and this is what we call the exploration
exploitation trade-off
we have to choose between playing the
best arm according to the current data
which appears to be best for the next
step versus playing the less explored
arms or let's explore options to to make
sure that we have good empirical
estimates going forward
okay
and this has to be done carefully
because if you don't have two arms if
you have a lot of arms later on you'll
see infinity infinite number of arms
then you have to explore not too much
but not too less either
okay
so what is the best we can do at least
in this simple problem so do we know
uh is is there a way to completely avoid
mistakes in the sense of can we get away
from not playing the bad arm at all
or
is there some minimum that you have to
do so you can in imagine information
theoretically there has to be a minimum
number of mistakes if you don't know
have any prior information as we assumed
here
if you don't know anything any you don't
have any prior on the arms then you have
to and you can only learn by playing
them then information theoretically
there is a minimum number of times
you'll have to play all the arms in
order to figure out which is the best
times and in fact there is a very strong
lower bound given by lyon robbins in
their 1985 work where they show that for
any instance so this is not like there
exist some instance which is really bad
they show that for any given instance of
the problem and by instance i'm defining
the distributions of the arms that you
you fix the
the reward distribution for any instance
of the problem
any reasonable algorithm and what is
reasonable means is you cannot say that
just play arm 2 all the time because it
will be optimal for some instance you
know so so that is not any reasonable
meaning they have a particular
definition of reasonable it means that
it should work at least it should
converge for all the instances okay it
should be an algorithm not just a
uh
deterministic play or something
so for any such algorithm
uh you will play any sub optimal arm at
least log
log t times and in fact they give exact
constants which we'll discuss later okay
so they they give exact dependence on
the parameters of the problem
yeah assuming that the arms are
stochastic
yeah so so the the idea is that uh
the the constants here that i'm
referring to they are basically about
how much information there is in the
distribution
so
so there's also a worst case model so if
you if you want to say okay i don't want
to think about the particular
distribution or something you can also
show that there exists something there
exists a bad instance so that's more
like a traditional uh bound where you
say that for every algorithm there
exists some instance with square root
and d regress okay so here there is no
dependence on the parameters of the
distribution or anything
so what we will try to do is show some
algorithms now which can achieve this
lower bound both the instance specific
so the instant specific logarithmic
bound will have dependence on the
instance parameters whereas the instance
uh free square root nt bound will not
have any dependence on the
parameter of the instance
so
first algorithm is the ucb algorithm and
this is this might be one of the most
popular techniques right now in
multi-arm bandits at least in theory
and uh it's a it's a very nice simple
idea but uh it works beautifully and the
proof is quite simple
so the the idea is
think about exploration exploitation in
terms of these two terms when making
decisions so
firstly whenever we
we see a
whenever we pull around we get a reward
so based on that you can compute an
empirical mean for every arm so that's
what you would have been doing if you
were just caring about exploitation you
would just play according to the highest
empirical mean okay so this crazy
looking expression is nothing but saying
take the average of the rewards on the
time step where you played the arm okay
and
now when you make a decision don't make
it based on the empirical mean but boost
every empirical mean by the uncertainty
in that arm
so even if an empirical mean is small
but
if the arm has not been played enough so
nit is the number of times the arm has
been played so far
so if that is small you kind of give it
a boost so you're saying if i don't know
enough about the arm i'm going to give
it some benefit of doubt and i'm going
to boost its estimate
so so the the collective estimate
developed here is called the upper
confidence bound because
the way it's constructed is this is
exactly
the error bound that you would get from
applying a turn off bound or a zuma
hopped in bound on the empirical mean so
this is the
error of mu hat i t
versus nu i
and you can show this is smaller than
4 log t over n i t if nit is the number
of samples available with probability 1
over 1 minus t squared okay so this is a
high probability bound on on the error
and you have added that error to the
estimate which means that your your your
your ucb is always above the true mean
with high probability so it's an upper
confidence bound on the mean and this
algorithm simply says at every time step
just play the
play play according to the highest upper
confidence power
and why does this work so the overall
algorithm is simply this every time pull
the arm with the highest upper
confidence bound
and then
update the upper confidence bombs
according to the sample observe
initially you kind of do a little play
one play every arm once
this this is just to make sure that this
is well
your estimates are well defined and so
on
but let's not worry about that for now
so so let's see why does this work so
how do we show that it has a small
regret
so regret recall is simply the
difference so delta i t i'm defining the
gap between the best mu star versus my
mu mu i
so the regret is simply the delta the
regret of playing every arm at time t
okay this gap so you can also think
about this as saying that it's delta for
every arm i delta is the regret for
playing arm i then it's delta i times
expected number of times i played that
term by mistake okay so to bound the
regret by log t so my my goal here would
be to bound the regret by sum over i
not equal i star log t over delta i so
this is in fact
uh
very close to the lower bound given by
lyrobians so del instead of delta i they
have
kl divergence there
so they have instead of that there their
bound is close to
their bound is log delta i log t
over the kl divergence between between
delta uh between the
the optimal and the
between the optimal and the ith
distribution
but since kl divergence can depend on
the entire distribution and not just the
mean this is kind of harder to write if
you don't have specific form of
distribution so a good approximation to
delta i over kl divergence is 1 over
delta i
so this achieving this is basically
almost the best you can do in terms of
instance specific regret okay
so
so to to prove this kind of bound now
looking at this expression here prove
this kind of bound i essentially have to
prove that the number of times i play
an arm i which is not optimal can be
bounded by log t over delta i squared
okay
so i want to prove
this bound on the mistakes
but the number of times i play
and i'm i by mistake
so that will give me the entire regret
bound that i want
right
so now we let's just focus on proving
that you would not play an arm i more
than this many times
in expectation
so and this proof is quite simple so
let me
so think about uh what what is the
main thing that we observe we observe
that every mu i
is smaller than ucb i at time t so this
is how this was the basis of our whole
formulation right so we always use ucb
the upper confidence bound with which
high probability is more than the actual
actual mean
which also mean that mu star is smaller
than ucb of
the optimal arm okay
now for an arm to be played we want to
show that a sub optimal arm i will not
be played
after more than more than some constant
times delta i squared steps so let's
assume ni
at time t is more than this number of
times it has already been played is more
than this
okay so let's say we are given this
so now i want to say that okay i played
it already this many times now i will
not play it
so what will make me play an arm i at
this point so for an rmi to be played
its ucb must be greater than ucb of the
optimal arm
but
let's look at the ucb or um i
we know that it has to be
smaller than mu hat i
plus
this is how we developed the ucb right
so this was the formulation of ucb so in
fact it's equal to that
but i know that mu hat i
is smaller than
mu i plus that same thing
right because this is from any
concentration like
turn off of big bounds or something okay
so i know that now let's look at this
expression now think about this c that c
over there let's just put that as 16.
you can get stronger bounds but i don't
want to go into using more tighter
inequalities so if you put that there
this would become delta i over 2.
and this will become delta i over 2
and so this is equal to mu i plus delta
which is actually exactly equal to mu
star
so after these many plays basically your
estimates are tight enough so that your
error is not enough to ex make mu i
exceed mu star okay so it's it's smaller
than the gap between mu i and mu star so
your upper confidence bound for arm i is
below mu star
and for i star irrespective of how bad
your estimate for optimal arm is it's
always above its own mean
so this is always so so at this point
this is less than ucb of i star
so as a result you have that ucb of the
optimal arm will exceed arm i and it
will not be played anymore
okay so this shows that after these many
plays you cannot play arm i and this is
all with high probability because the
the concentration bounds are holding
with high probability
so so if you can bound the number of
plays by this much we get our regret
bound
of
of the desired formulation okay
any questions
oh the question was can you uh replace
the this t here oh sorry this is
the t little t here in the log by n i t
so
so you the the the probability that you
will get is is this yeah probably like
you can do something like that to get
tighter bound i don't know how much the
bound will improve
it might improve by uh yeah if you care
about the logarithmic terms yeah
so right now there is a
yeah i i'm not sure if that will give
you much better order
but if it might give you something
inconstant yeah
yeah
you need a
warm-up
right so you can't yeah so you'll have
to technically like you'll have to do a
few things like because nit is
can be very small in general so you'll
have to do a few initial trials to make
sure nit is not too small to begin with
or any arm
yes you have to be careful about it
yeah
also nit is like not a constant it's
it's uh
uh
so yeah
in the when you're applying the
concentration part you have to be
careful as to what high probability
bound you get
this ucb version will not get it to get
the knives is something called kl ucv
where you have to modify
uh this term
to incorporate a kl divergence term
so you have to you this is too much
rules physical so you have to make a
tighter
where they do that also then you have to
assume something about the form of the
distribution so you can do it for
bernoulli or something
scale divergence doesn't depend just on
the mean
no not yet yeah so this is instant
specific bound so from here i just get
yeah i had reserved it for if someone
will ask so
so let's use that
so
so if you get
this so you have this bound so far
so now you can kind of
it's kind of unintuitive because you're
saying that it's bad for an arm to have
a small delta i
right so you're saying that delta i if
it's too small then the secret goes up
whereas if you think about it small
delta is tends to be great it means your
arms are almost optimal so you should
not be suffering by making mistakes so
you should have smaller video so the
reason it's happening is because small
delta i makes it difficult to learn
because it's hard to distinguish between
optimal arm and the some optimal arm if
the guy is small but it's you have less
regret
so to get a worst case bound in this
case you have to kind of divide it into
two parts saying that okay there is that
border where it's
not enough
like not so close that you can ignore it
but also close enough that you have to
just make it hard to distinguish it
right so so that border line seems to be
okay let's call let's say delta i
let's look at the arms with delta i less
than square root n over t
okay
so and delta i greater than square root
n over t
so these ones we will ignore right so we
will say that all these are almost
optimal
so the total regret of these arms can be
bounded even if i don't even care how
many times they were played even if they
were played all the time the total
regret here is square root
and
yeah
so i have a question about the previous
proof
so so the total
here is square root nt from these arms
okay
but for these arms i will say that i i
will use this regret so i'll basically
count all these arms as optimal and then
substitute
delta i as at least this much here
and get this expression will become then
square root n t log t
so if i substitute this delta i here
i will get square root okay maybe i'm
missing a log t here yeah
log t
yeah
so i will get a
yeah
yeah
so there is some work uh
more recently by on removing this lock t
i don't know if someone will discuss it
but
yeah there has been some
um
some chasing of that lottie in the
literature yeah
and i believe they have removed it right
so bubeck has a paper
there's anyone knows yeah yeah
all right yeah your question so that
high probability was one minus one over
t squared right yeah yeah doesn't depend
on the number of arms if you have a lot
of arms isn't that an issue that like
you're going to get an error in
something
no because it's t squared yeah i'm
assuming n is much smaller than t yeah
if you if you want to consider it larger
then you might want to have log n t or
something
all right
so let's move on to the next one uh
thompson sampling which some of you
might know is my favorite uh but
so so this is a here is this is a
so it's kind of a bayesian heuristic so
it's more
generated from the setting that i
discussed in the beginning uh about the
bayesian way of learning
but uh we have kind of now adapted it to
our prior free setting and kind of as an
algorithm for the prior free setting so
the origin of the heuristic is beijing
which means that you you kind of do the
same thing so you start with some prior
about the arm distribution uh and then
every time you see the reward you may
you update the belief in the bayesian
way and
one of the main difference is instead of
trying to compute the optimal policy
like
like getting syntax does which is very
hard in most cases
you simply pull the arm with this
posterior probability of being the best
arm so so let's parse this sentence a
little bit because often
people read it wrongly so
so when you have bayesian posteriors the
what is the common like machine learning
way or the bayesian optimization way of
doing things is you can compute which is
the most likely to be the best arm
according to the bayesian posterior
right so maybe uh according to the
bayesian posterior uh the probability of
one first arm to be best arm is 90
percent the second arm to be the best
arm is 10 percent and so on
so
the maximum likelihood
wave would be to say that play the first
time because it has 90 percent
probability of being the best down here
we are saying play the first time with
90 probability and the second round with
10 percent probability so we are
exploring by giving every arm a chance
proportional to its chance of being the
best arm so even if an arm has one
percent probability of playing being the
best you still play it with one person
property okay so it's very different
from maximum likelihood uh choice okay
so
so what we will do is we will take this
algorithm but we will not assume that we
know the prior we will construct our own
prior so we will choose a prior which is
very less informative for example
uniform distribution
and then we will similarly evolve the
posterior and play the every arm with
its probability of being the best arm so
we for us this this will be the the
posterior will be a way to manage
information rather than actually knowing
that this is the
posterior distribution of the reward
right
i hope that that point is clear because
that point is sometimes confusing
so what we will do like we'll do we
consider two versions of this algorithm
so when the arm when we know that the
reward is either zero or one when we
have
when we have a binary reward in that
setting it's very convenient to have
beta distributions as as the
posteriors because they are conjugate to
bernoulli rewards okay so in that
setting this is a convenient setting
where you use
you start with a uniform distribution
which is same as beta11 so beta
distribution if you some of you haven't
seen them is this two parameter
distribution i plotted a few here uh
alpha and beta so alpha is the
so larger alpha means you have uh more
this to the this side the probability of
being one is higher and
smaller alpha means you are on to this
side so you can see that alpha
2 5 which is the orange one is smaller
alpha is on this side
and alpha
five the blue one is on this side right
like the the peak is on this side so
larger alpha is good in some sense we
get more rewards
so which means that when you write the
base rule
uh the posterior
given given a beta prior
and a bernoulli reward observation the
posterior is also b
so it's conjugated
so so in particular
the update is really easy if you start
with a beta distribution with parameters
alpha and beta
then the any observable newly reward of
one then posterior you can calculate is
simply
be the same beta distribution but with
alpha parameter increased by one
and if you observe a zero then the
posterior is simply the one with beta
parameter in stress by one so it's very
simple update in general a posterior
distribution may not belong to the same
family as the prior or it may not even
be analytically like a parametric
distribution that you can write easily
so you might have to do methods like
mcmc to simulate it and so on but here
you can very simply generate the samples
from the next posterior
so
given this setting uh the algorithm
becomes really simple basically you
start with uniform distribution for
every arm which is beta11
and then for every arm
any point you know what the posterior
will be so whatever number of successes
or once you have observed
that will form your first parameter
because every time you observe the one
you increased alpha by one so at any
given time your first parameter will be
the number of ones observed for that arm
and the number of zeros observed for
that arm
plus one because we start with beta one
month okay
so at every time this is the posterior
for a given arm i so you simply generate
a sample from that posterior for every
arm and play the arm with the maximum
sample now this process of sampling i
never mentioned it in terms of sampling
this is simply to simulate that sentence
that play every arm with its probability
of being the best term so instead of
computing the probability that beta
distribution of i is more is more than
beta distribution of every other arm
i simply simulate that line by saying
generate a sample from everyone and this
the max has the has the same probability
of being
i has the same probability of having the
highest max as the probability of i to
be the best okay so i'm just simulating
that that line okay
so this this is the thousand sampling
for beta distributions and bernoulli
rewards
and uh and and you can see as it's it's
as simple as ucb to simulate basically
now
we don't want to restrict just two zero
one rewards
and so there is a version that you can
use for
uh for general rewards which is with
gaussian so
so we start basically with standard
gaussian so mean zero
and uh
standard uh
and one standard deviation for every arm
now
this is something we assume for deriving
the posterior this assumption doesn't
have to be satisfied for the regret
bounds for regret bounds is enough to
have a bounded distribution of rewards
okay so if you assume that the gaussian
like it's the arm has a gaussian
likelihood uh with mean mu some mean mu
and standard deviation then you can see
that the posterior
after observing n samples is simply
going to be again writing the bayes rule
it's a gaussian is has a
conjugate prior for gaussian
distribution
so posterior will simply be
the empirical mean for those n samples
as the mean and the standard deviation
will be
sorry variance will be 1 over n plus 1
okay
so so this is something you can derive
using base rule and that that that
observation be used to write this
algorithm which is you start with a we
don't start with this normal prior we
start with some boost to the normal
price this is like a logarithmic boost
and the reason is technical for the uh
for the proofs
but we start with this
normal prior and then every time step
the on ice posterior will be the
empirical mean as the mean
and one over and standard division
proportional to one over the number of
trials that arm has seen
so now the algorithm is simply sample a
sample from every arm and play the arm
with the maximum sample
so you're simply replacing the upper
confidence bound by like a sample from
the distribution with the mean roughly
the same as what you had in ucb
but now you have a instead of instead of
finding like one point here as ucb
you are generating a sample from this
distribution
and then playing the arm with the
highest sample
so
so one reason that this algorithm
generated interest almost 10 years ago
was that
sometimes in some cases instead of using
this extreme point to make your decision
using something around the mean
can be practically uh resulting to less
exploration like wasting less time in
exploration and this was observed
empirically and therefore
we wanted to analyze the regret of this
algorithm the algorithm itself is very
old
and there's not much chance of improving
the regret beyond ucb's regret because
ucb's regret is almost optimal already
right so we we don't hope to improve the
regret but the hope was that you can
show that it also has a good theoretical
properties in addition to the practical
uh properties
so the regret bounds for uh for this
looks like this
so first of all for bernoulli rewards in
fact the basic algorithm so this was
like really nice observation that
even the basic algorithm without doing
any modification
gets you the right kl divergence turn
under it so the the intuition is that
because you are not making up these
confidence bounds like it's not an upper
bound it's really posterior captures all
the information you have about
the arm and nothing more and nothing
less right so so it gets you the right
term here in terms of kl divergence the
the bound so it's logarithmic bound like
ucb but the constants are slightly
better in terms of kl divergence so
instead of 1 over delta i you have delta
i over kl divergence
and
the the other case which is gaussian
prior so if you don't have bernoulli
rewards if you have general
just general bounded rewards you have to
use the gaussian version of the
algorithm then you can get
same as ucb basically log to your delta
i
and and you also get the same
worst case bound
so i'll discuss next the proofs of some
sampling a little bit unless you have
some questions before that yeah
yes so all these results are extendable
yeah um just make sure you understand if
you're saying that that the key
advantage of thompson sampling against
ucb is that you get slightly better
dependence and they're wet is it is that
is that that what i mean theoretically
yes although you can also modify ucb
like to have more complicated
restaurants in terms of care diversity
you know you're dealing with bernoulli
and then you can modify your
confidence bounds a little bit to get
the same bound so i won't say like
i wouldn't infer that's the main
advantage i mean it kind of happened the
reverse that it works well and then you
want to analyze it and show it also
has theoretically problems so
do you view the main advantages that
empirically it works yeah
yeah
so the epsilon term the one over epsilon
squared terms i guess
so usually people don't even write this
first of all like i i like i'm being
very very technical here to write this
so like for example if you just make
this epsilon one you can just remove
this part
uh because it just makes the constant so
we wrote this because it then it matches
exactly the lower bound
so so lower bound says like you can get
arbitrary close to one
if you allow this element like this
because this energy term doesn't
increase with time
so that's why it's significant right so
this all this time increases the time
yeah
so it depends on how much you yeah how
much you are comparing n versus t i
think yeah whether n is longer than time
or it's constant
about significance i wanted to mention
one more thing the reason uh i like tons
of sampling is because it's easier to
design
so
uh it's like you can you don't have to
design confidence intervals like if you
want to implement a thompson sampling
algorithm you just complete the
posterior example
for every every extension of the problem
you don't have to compute confidence
intervals and carefully
no prior no prior knowledge so we start
with the prior which is not informative
like uniform or standard answer because
which may have nothing to do with actual
distribution
okay so what is the idea behind uh
improving such a bound in this why does
this work so we kind of understand
intuitively why does upper confidence
bound work uh so why would this why why
do we even hope for this to work what is
it doing that will allow us to capture
exploration versus exploitation right so
the idea is that uh you are maintaining
these uh posteriors and these posteriors
are capturing exploitation in their mode
or their mean and exploration in their
spread okay because posterior
distribution is not a one point estimate
it's an entire distribution so it's
telling you not only what it believes to
be most likely to be the parameter but
it also telling you how uncertain it is
about the parameter so so the blue
distribution for example has a
is most like it says the most likely
estimate of the parameter is here
right which is uh slightly worse than
than the red one but it also also saying
i'm actually not as certain as the red
one it can be anywhere with a very good
probability it has a very wide range of
probably
a prob
of how much the parameter could be right
so it's telling you more information
than just what the estimate is telling
you both about the uncertainty and the
most likely estimate so the idea is if
we sample from it then
then if a distribution has a lower
uh mode lower likely to be the value of
the parameter it could still have a
higher sample because you are uncertain
about it and that will provide you a
benefit of doubt to a to a arm for which
you are uncertain about by sampling from
the more spread so the spread will
enable exploration whereas the mean or
the mode will enable exploitation
so let's see how that idea shows up in
regret calculation formally so let's
look at just two arm example to keep it
simple
so you have two arms and again i'll
assume arm one is the higher one of
course the algorithm doesn't know that
and the delta regret of playing arm 2
instead of the optimal arm is mu 1 minus
mu 2. so there's only one arm there's
only one delta so overall to bound
regret in this simple case uh you want
to show that the number of poles of arm2
is bounded by log t over delta square so
sorry i will not be able to show the kl
divergence version of the proof but the
idea is the same
okay so i want to prove log g over delta
regret by showing that the arm two the
bad arm will not be pulled by more than
constant times log t over delta squared
okay so how many pulls will will can
happen so i will kind of divide the
whole timeline into into three parts
and the easier
part of the timeline is is some point
which is a random point i don't know
when it will happen
that when both arm two and arm one have
number of pools more than some constant
times log t over delta square so this is
both n1 and n2
so after this happens and i don't know
when this when this this time will
happen but whenever this happens after
that i am golden why
if both arm one and arm two
have been pulled enough times which is
that much
then both of them will have a very
highly concentrated posteriors why
because the posteriors as we saw like in
beta distribution on both both in beta
and gaussian the mean of the posterior
for example in beta is this
and this is exactly mu hat basically
okay
and same way in gaussian so mean is the
empirical mean and you can see once you
have this much this much place like in
ucb we talked about the mean will be the
empirical mean will be very close to the
actual mean in fact the exact
calculation can show this will be less
than in this case like delta by four
okay
and then
[Music]
so this is this is that but
so empirical mean is close to the actual
mean so that's the mean of my posterior
but what about the spread the spread in
both beta and gaussian is actually 1
over alpha plus beta which is number of
trials of the arm okay
so now the number of trials once it is
that again you can show that the the
variance is 1 over n so that will also
give you that the spread the standard
deviation 1 over square root n is also
less than delta by 4. okay so this gap
here
is also less than delta by 4.
so the total gap
which is delta
right so you can show that because the
spread and the error in the mean both
are less than delta by 4 the the two
arms will be separated completely
like by separated i means the the
overlap between them will be very very
small
so overall that you want basically we
are saying once both the arms have had
enough number of samples both will have
some peaked posterior so they you cannot
confuse them when you take samples
because samples from here would be
higher than the samples from here so
this is a nice picture you'll get once
both of the arms have been played
now this is problematic because you
remember in ucb we said oh we are golden
after our bad arm has been played enough
and because if badam has not been played
enough anyway regret is small so we are
done right
now this is here i'm saying no not just
bad arm i'm saying both the arm have
been played enough but before that what
about the regret before that
i cannot say the regret is small here
because i have to wait for even the good
arm to be played and by that time badam
might have been played a lot
okay so i have to bound this regret now
this is a problematic area
because
exactly this situation maybe the badam
has been played enough already but the
good arm has not been so the way down we
are waiting for the good arm to be
played enough and by that time bada
might have been played a lot more times
and we accumulate a lot of regret
so how long will it take for the good
arm to be played so now let's look at
this middle point
so we are also golden before
n2 has been played played in enough okay
so let's consider this point
as the point where n2 exceeds it plays
okay so we can wait till this point
because until this point there is not
much regret anyway okay
until bad arm has been played so we can
wait till this point
but how long will it take to go from
here to here when we get good arm to be
pointing
that is our concern this middle portion
okay
so we are in this middle portion means
that bad arm has been played the red arm
has a good concentration nice
big posterior but the good arm is all
over the place how long does it take for
good arm to go and be here okay
so what is this situation here the
problem is there are two problems
because we are not doing ucb we are
sampling so it can happen because the
good arm has this huge spread first of
all its empirical mean could be bad
even if his empirical mean is good it
could have a huge spread and we could be
sampling from below the actual mean much
below the actual mean and therefore not
play the good arm
so what we'll show is that this middle
portion this bad portion we can't
quickly go through it exactly because of
the spread of this arm
so because of the spread of this arm the
spread of this arm is actually useful
why because the spread allows it to be
randomly played sometimes
not very often but at least constant
probability
because if spread is so huge
it will yeah with constant quality it
will fall below but with constant
probability also fall above
so every constant number of steps i will
play the good arms okay and constant
number of steps is not good for regret
but it is good enough
to get out of this situation and get
this many plays
because it means to get this many plays
i only need constant times that many
steps
if i'm being played every constant
number of steps okay so this is
basically the idea we show that because
of its spread until the the arm one
reaches its peak enough number of place
it has enough spread to be played
roughly every constant number of steps
and so to get log t over delta squared
place it only need constant times log t
over delta square steps
okay
so after that we will reach that golden
place where both arms have enough place
okay and of course this constant is all
roughly speaking you actually get better
and better as you play so you can get
tighter bounds you like you can have
better things here than constants
so so that's the idea
and you have to get tighter bound exact
constants and so on you have to do more
careful analysis but what i want you to
remember is that the variance of the
posterior the spread here exactly what
enabled exploration of the good arm
right because because of the spread the
good arm was played every constant
number of steps and get out of the
situation the bad situation
right so when you when you want to do
multiple arms
this doesn't work exactly i said because
now you don't have this nice three
partition of the timeline because you
have to wait for all the bad arms to be
played and you cannot say okay when will
it happen that all the bed are like you
you cannot say let's just wait till this
point when all the bad arms are played
because that might take a long time for
all the bad arms to complete their their
plays so you have to be
careful in that case and uh you have to
we prove this very nice uh
inequality which is the key to the whole
analysis should be i star sorry uh so
the key to the analysis to show not that
the the good arm will be played every
constant number of steps or it has a
constant probability of playing but
saying that good arm has a constant
probability of the probability of
playing any given bad arm
so it's not that it just has a constant
probability it's compared to every bad
arm it has constant probability which
means that every time you play a bad arm
like every time you get a get charged
for a bad arm you also get a point for a
good like a constant times that point
for a good arm play so you kind of can
do this charging argument where you can
get enough number e like every time you
suffer a regret for playing a badam you
also get like information for about the
good arm so you kind of can show that if
you accumulated enough regret you also
have gotten enough plays of the good arm
uh so i think i'm almost out i'm out of
time so uh let's we can take a few
questions and then take more questions
in the break
yeah
i have not done that kind of uh thinking
but i saw something related to this at
various points
um
[Music]
yeah i'm sure like you can you can make
a distribution
like that and try to randomize all the
bad distribution instead of
instead of the posterior that they
design and then come up with the
expression like this
just pick the uniform distribution
foreign
but yeah i guess
but for curiosity
so you talked about the sort of expected
improvement today yeah are things known
about the distribution of regret of the
two algorithms
yeah so for what any of these things you
can improve even like a
so there are two things but if you want
to prove high probability regret but you
still define regret as
but if i still define regret as in terms
of the expected values you see like my
points like i i if i still define regret
as as the mean
difference
right but i remove this expectation
then i believe you can get high
probability bounds
if you want to define the great in terms
of actual reward then you can start
depending on the distribution
of the reward not just the mean
because it will depend on the variance
of the actual
uh distribution
but i think
it should still be possible by using
high uh like if it's boundary
distribution
and is one of is it known if one of ucb
or thompson is like more variable than
the other
i don't think theoretically there is any
difference
but in the freedom
uh you mentioned as empirically thompson
sampling works better than ucb so does
this universal phenomena may be across
the different number of arms so when you
have small number of arms
so yeah again this is the physical
statements are always subject to you
know conditions under which it was
tested and i'm sure there are conditions
which recipe works
better but um i have seen it in uh for
example my own experiments on
the
problem i talked about in ml
which is assortment optimization it was
very very visible
um
i mean there are sometimes you can like
play around with the confidence in terms
of pcb to make it tight enough
to get it right but then you need to
know like it's usually in hindsight it's
like hard to tune it so i would say as i
was saying before even if you can tune
in to be really good i think the
advantage of from something i found is
requires less timing
which could be very
important sometimes right
and um
and there were some experiments in the
original paper that inspired us uh by by
lee hundley and
oliver chappelle they had experiments
showing in the case of contextual media
contextual bandits that they were
significantly different
again one could argue that the
confidence intervals they used are not
tight enough and
it's best support kind of
how sensitive is the proof to bayesian
updates like can you prove something
like as long as you do uncertainty
quantification in a reasonable pay yeah
so so you can try it up again because
the differences between these algorithms
is more conceptual rather than actual
because you get the same difference
bound and the kind of random lines
in the assortment optimization work we
did the posterior was very difficult to
calculate so we approximated it by
something like an inserting the
quantification
to to not to avoid doing the ncmc
sampling for example so you once you do
that it becomes closer and
as long as you do uncertainty
quantification in a somewhat reasonable
way and you're somewhat optimistic yeah
it should be true
okay um it's like me or enjoy that's a
huge amount uh the good news is we have
more of shipwreck after this so
we have one more
from uh so we're gonna we have a break
right now and we should come back before
11 just so that we can have as much more
of this tutorial as possible so feel
free to i think there should be some
reflections outside that
11b
you