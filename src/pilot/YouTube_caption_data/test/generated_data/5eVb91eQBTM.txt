in this video i will implement wgan or
wasserstein gan from scratch with
pytorch and as a prerequisite to this
video i did another one just yesterday
which was about understanding the
architecture of wgan so
if you are not too aware of the
architecture of wgn do watch that one
first otherwise let's start
another quick point that in this video i
have implemented a very simplified
version of wgan so to keep it simple i
have kept one slightly advanced topic
within wgn called gradient penalty out
of this but i'll do another video after
this which will include gradient penalty
implementation as well but that's in the
next one so stay tuned
hello my name is rohan paul and very
welcome to my computer vision and deep
learning youtube channel let's get
started
one more thing that over the next month
i will do many gan or generative
adversarial network implementation from
scratch with pytorch so do subscribe
because you hitting the subscribe button
is what keeps me motivated to do this
let's start
first a quick recap on wgan so let's see
what is the difference between a w gan
and a normal regular gan so here in this
slide we can see a regular gan on the
top left corner image and the wgan
architecture on the right bottom corner
so some of the differences with a
regular gan is that
in discriminator of a regular gan it
takes real samples and also images that
is samples generated by the generator
architecture and then the discriminator
discriminates between them that is it
produces a binary output of zero one
that is real or fake but in wgan the
discriminator is called critic and
that's because it really does not
produce a binary classification kind of
output rather it produces a score a
critic score so it takes both real
images and fake images and then it
produces just a single score which is
high for
real images and low for
low for the fake images
so
in wgan the critic is actually uh
actually a regression problem because it
produces a score
and then the next important point to
know about wgan is uh the concept of
ocerstein distance so basically uh in in
a w gan architecture scenario we have
two distribution one distribution is
coming from the real images this is
denoted by p data or p r and then we
have a second distribution denoted by pg
which is which is the distribution that
comes from the images generated by the
generator so we can see these two
probability distribution are far apart
and the whole
purpose of wgan is to bring these
distribution as close as possible so
it's just this to bring them closer and
to bring them closer that is to
distribution to be as close as possible
we have to calculate the distance
between these two distribution and that
distance calculation is based on
wasserstein distance in wgan and that's
why it's called the volce stein gan
and then overall these are some
important points to note comparing wgan
with a regular gan the first one is
after every gradient update on the
critic function we need to clamp the
weights to a small fixed range and the
range will be defined as a hyper
parameter and normally it is denoted by
between minus c and c so we have
discussed this point in detail in the
previous architecture understanding
video that uh the weight clamping or
weight clipping is required to maintain
the one lip sheets continuity and what
that means in the context of wgan
then the next point is use a new loss
function derived from the wasserstein
distance
no logarithms anymore the discriminator
model does not play as a direct critic
but a helper for estimating the
washington metric between real and
generated data distribution and
empirically the authors recommended in
wgan rms prop optimizer on the critique
rather than a momentum based optimizer
such as atom which which is normally
used in other regular game like dc gan
so the main three points in wgn is
critic weight clipping then update
critic more than generator and use rms
prop stochastic gradient descent
so putting it all together this is the
workflow of wgan first the critic
network is trained on a real batch of
data then trained on a batch of data
generated by the generator
uh then the critic's loss function is
arranged such that it estimates a
washington distance that is it maximizes
the distance between the two true two
distribution and the two distribution is
one coming from the real images and one
from the fake images then
the
clips the critics own weight to ensure
it is one lip sheets continuous
and then the generator generates a new
batch of images from the noise vector
passes those images through to the
critic and the critic informs the
generator of the washington one distance
between the true distribution and the
distribution of the images that the
generator just created
then the fourth step is
so it does these
via the loss function of the critic the
critics weights are frozen and the error
propagates all the way back through to
the generator who then updates its
parameters to minimize the washington
distance and these whole steps are
repeated until the loss converges to
near zero and the descript and the
distributions are approximately equal
and now i'm back in my vs code where the
full implementation code is there for
wgan and just to mention the overall
structure of this project i will have
three files one is wgan.py which will
have the
generator and the discriminator that is
a critic code
and overall i have used a very simple
linear layer i have not used conf 2d
layer here to build the w gan uh and
also to note again that in these
architecture that i'm building in this
video i will not include gradient
penalty because here the purpose is to
build absolutely simplify wgan and in
the next video i will have another
implementation where my structure
the architecture will have will include
conf 2d layers and also gradient penalty
so anyway that's my wgan dot py then i
will have utils dot py file that's a
very small file just to include couple
of utility methods and finally i have a
train dot py file
uh uh which will have the whole training
uh flow uh codes here and finally to run
this entire wgan i will
do i'll run this on mnist dataset and
for that i will have another
jupiter notebook where i will just run
this trend.py file that is a
running this entire architecture or the
entire project is as simple as just
running a single command of python and
then trend.py that is executing the
trend.py file all right now starts with
uh
wgan.py
and just like all gans the first class
that i have within wgan.py is my
generator class and here i have a very
simplified implementation of generator
class the first method within generator
is this block method and this just
implements one linear layer then a batch
norm 1d based on a condition
and finally just appends a leaky value
to my layers and finally returned that
layer so i have
and the application of this batch norm
2d is based on these normalized argument
that will come as
as an argument to the to the method
and this condition is because the first
layer when i will implement this block
the very first
layer will have the normalize set to
false because that will not have a batch
normal 1d and rest of the layers will
have the batch number on the applicable
so
and then in my next next uh next section
here i'm just applying or invoking this
block method four times so i have an n
dot sequential defined here the first
one is block in features will
just be as usual the latent dimension
which is my noise vector
that is that is the very start of my
generator network because the whole
generator network's purpose is to take
in this noise vector and finally output
image from its uh output layer so that's
why the in features here i'm passing the
latent dim out features i'm keeping at
128 and because this is the first
application of the block i'm setting
normalize equal to false in this case
this batch norm 2d will not be appended
to my layer
and then i'm uh
invoking three more blocks and each time
only difference is that the out features
from the previous block becomes my in
features from the next block
and then the out features of this block
is something that i am defining
separately
so
and obviously because these block will
also have these
apart from the linear layer it will all
these block individual blocks will have
batch norm 2d and leaky value appended
to it
and then the forward method in my
generator class just executes or invokes
these
sequential model that i have defined
here so the forward method takes as its
argument the image shape and the z noise
vector and passes that z through my
model which will produce an image and
then it just applies view to flatten it
and returns that image
so basically my whole generator model
takes as input a point in the latent
space
and outputs a single 28 by 28 grayscale
image remember in this case i'm applying
this whole uh w gan generator and the
critic network on mnist dataset so uh it
has to produce the generator has to
produce an image which which resembles a
single image of amnes data set and
that's a 28 by 28 grayscale image and
that's why the output from generator
here is also 28 by 28 and this whole
generation process is achieved by using
a fully connected layer in this case i
am using linear layer which is a fully
connected layer to interpret
each point in the latent space this is
this is then upsampled through these
through the sequential models uh
doubling the size each time
so that's my generator and the next one
is the discriminator or as it's called
in wgan the critic class
so here is my critic class a very simple
implementation of that
and
remember our
the last layer of generator was to
produce
this linear layer where the out features
were
these value integer of np dot prod image
shape so that should be my in features
at the very first layer in in my critic
class so that's why we see the exact
same thing in the in features of the
first linear layer
so here basically i am taking this
number and ultimately at the end of all
the layers in the discriminator i am
producing a score
because as we know in wgan there is no
sigmoid at the very last layer because
here we are not producing a zero one or
true fake kind of binary output rather
we are producing a score a scalar score
so that's what these layers are doing in
my critic class the first one is linear
then a leaky value uh with 0.2 and in
place true as well then again the next
linear layer and here the in features is
come this 512 number is coming from here
here that feature source 512 so that
will become my in features in my next
linear layer
and keeping the out features uh equal to
256 here
if you look at the structure compare the
structure between generator and
discriminator here i was up sampling
that is
uh out features 128 256 512 1024 and in
the discriminator or the critic i am
doing the opposite going in the opposite
direction so out features five one two
then uh next one out features two five
six and finally my out features is one
because that's the last layer of my
critic and that should produce a single
scalar score
from its output layer
uh all right and that's the forward
function
from in my critic
and basically what the forward function
doing is it's just implementing these
models so basically
here the forward method takes an image
and produce a single score the single
scalar score so it takes the image as an
input to my forward or argument
and then flatten the image and then the
final validity score
uh to produce a final validity score i
just have to pass this image to this
model and that's what i'm doing in this
line self.model and that takes the image
underscore flat as the input or the
argument
and this validity score is the final
output from my critic
so with that i'm done with this file now
i should be able to train but before
going to train dot py just quickly look
at the utils.pi file there i just have
two methods
and they are actually just uh utility
method so
uh no complexity so nothing too much to
learn here it's just the first method
weight in it normal so this method will
just create the initial initializer
weights to the network based on whether
it's a conv layer or batch norm2d etc
and uh so when i'm using torch.net.init
this takes the wet data as a tensor and
then it takes a standard deviation and
mean as uh two more arguments
[Music]
and
two image is just again a very simple
method just to clamp
my the input between 0 and 1 and this
method for visualizing it just takes an
image and
passes that image image tensor to the
cpu then use the make grid function to
create a grid from that from the images
and then finally just use plt.i am show
to actually plot or show the image in my
notebook
so finally here i am in my third file
which is trend dot ui uh and remember
this train dot p th these two files are
pretty much independent files standalone
files but train dot py has dependencies
on these two files other two files
because it has to import all these
classes to my
to my trend.py file and that's why i'm
doing this line from utils import star
and the same from wgan
and
yeah so in my train file first i'm
defining these hyper parameters class
and then actually initiating the values
of each of the hyper parameters like
number of epoch batch size learning rate
number of cpu latent dim is 100 this is
a noise vector to start my entire
generator network
image image i started to uh let's see
where i'm using 32 here we'll come to
that in a in a second
and also i'm defining two uh variables
here cuda and tensor uh four they're
pretty much obvious and standard codes
so this will be uh
this will be true if my uh if my gpu is
compatible with pi torch else it will be
false and if gpu is available then all
my tensors will be uh we'll need to use
the gpu and that's what the second line
is doing that is this tensor will be
using torch.q dot or float tensor if the
queued is true else dodge dot float
tensor only difference between this and
this is that a torch dot q dot or float
sensor will be computed with help of the
gpu power
else only torch dot float tensor will be
occupying the cpu and hence the
computation will be on the cpu cpu power
only
so
i mean most probably for most of my
torch computer vision projects you'll be
using
gpu uh else it will be super slow so
this line is kind of important
all right now coming back to the next
section okay so this method this is
again a standard uh
standard code that you will see that is
here i'm just data loading do
implementing the data loader method of
pytorch to load my data and also do some
basic transform uh on the data so this
uh here i'm using torch
visions.dataset.fashionmnist
this is uh this is a 28x28 grayscale
training images and download equal to
true means that it will download this
data set from torch visions
if that is not already downloaded
or if it is already downloaded then it
will not download so that's the
significance of this parameter download
equal to true train equal to true will
just
uh pull the training data and not the
test data set and also i'm applying some
basic transforms here that is resizing
my images to
image size
and here remember this is part of the
hyper parameter and i have defined it to
be 32 here
and then
standard code i'm converting them to pi
torches 2 tensor and also normalizing
using
mean and standard deviation of 0.5 and
0.5 respectively
and batch size is coming from my hyper
parameter and i have kept it at 64
shuffle equal to true all right
and then before starting my train method
i'm just creating the generator and the
critic object
from the generator and critic class
respectively and to the generator i'm
passing the image shape and also image
shape is actually a three-dimensional
tensor
because in by torch the
dimensions of image are set like
channels first then
height and then width and that's what
this image shape represent
and the second argument to the generator
clause is latent deem which is a hundred
dimensional 100 element noise vector
all right and the critic also takes the
image shape to
create the object and then my two
optimizers just following the advice
from the paper i am using rms prop
uh to create my two optimizers
and to that to that rms prop i'm passing
generator.parameters and
critic.parameters and learning rate is
coming from the hyperparameter
and
then generator dot apply weight in it
normal we defined this method in our
utils file
so that's what i'm applying here to
create my to initialize my weights
and then my trend method so here uh the
regular structure so i'm first looking
there there'll be two loops the first
one outer loop is for each epoch and the
number of epochs is is a hyper parameter
and then in the inner loop will
enumerate through my train data loader
we already have defined it above here
so that's my train data loader and
that's what i'm looking through inside
my trend method
and the very first step is to create two
vectors
which will one of these will represent
the valid targets which will be all
filled with once
and that's what i'm doing here uh using
variable module of of torch then
creating the tensor passing that
images.shape0
because my valid target number of
targets should match with the original
data sets number of samples
then filling them all up with 1.0 it's a
float value and that's what i'm using
this format 1.0
and then because these are just
target values i do not need this tensor
to have the gradients so that's what i'm
saying
uh i'm setting request grad to false and
doing the exact same thing for fake
variable as well but here only
difference is all the values will be
filled up with 0.0 because they all
represent my fake
images
and then these are my real images which
is coming directly from the images and
what is these imgs this is what i get
while enumerating over my trained data
loaders because
that will return a tuple of uh the index
number and also the images
and only thing that i'm doing here is
i'm converting them appropriately with
my tensor variable because this tensor
has been defined earlier to use the gpu
to make the calculation faster
and
the first uh and then my training actual
training starts from here and as a very
first step i'm obviously making use of
the zero grad
and that's because pytorch stores
gradients in a mutable data structure so
we need to set it to a clean state
before we use it otherwise it would have
old gradient information from the
previous iteration
and then in this line i'm just
creating or initiating my noise vector
and i'm making use of these numpy
random.normal
method which will draw a random samples
from a normal or gaussian distribution
and the first parameter is mean second
parameter standard deviation and a third
parameter is the shape of output that is
output shape
and here in this case the image dot
shape will will be the number of samples
and image dot latin deem it is the
noise vector number of elements and in
this case uh this is hundred
and
so that's my noise vector and the given
so my noise vector is ready so now that
means if i pass this noise vector as a
as an input to my generator that will
produce my fake images so that's what
this line is about that is to my
generator object i'm just passing my
image shape that we have defined earlier
and these noise vector z
and the output of this is my fake images
from the generator model
and now that i have the fake images
generated from the generator model the
all important
step now which is to pass my fake images
through the discriminator that is a
critic here and get my
loss score from the critic
now the first thing to understand here
is that in wgan the critic loss is the
difference the wasserstein distance
between two distributions and very much
it's something like this that is dx
minus d of gz
what that means is this dx will produce
the
this x here the actual real images so uh
dx
means that i'm passing the actual images
to the critic to produce a score
and then in this
the next part of these of these credit
clause this is
the
critic applied on the fake images so g
of z this part that is the inside of
this function
this will produce the fake images and
then those fake images will be passed as
an input to the critic or the
discriminator and that will be my
so the difference between these two
terms is my critic loss
and as for the original paper and the
very fundamental building block of wgan
is that
these expression need to be maximized by
the discriminator or the critic
and now here in the in my train method
i'm doing a gradient descent so my
optimizing i'm actually
running a minimization algorithm so how
do i maximize this mathematical
expression well you can maximize the
mathematical expression by minimizing
the negative of this expression and
that's why in this line here i am taking
the negative of this so this will become
positive and this will become negative
that that is very much what i'm doing is
minus
dx
minus
of
minus d of
g
z
and because these two are minus here
actually this will become the next this
this expression will become just a plus
plus take out this symbol yeah so this
is what i'm doing in in this line so
minus dodge dot mean critic
of real images plus torch dot mean
critic of fake images
and now my d loss is calculated so i am
ready to do the back propagation and
also updating the weight and that's what
these two lines are about
so this will back propagate this loss
and the next line step will actually
update the
weights next the all important step of
wet clipping
so
once my weights are updated
that is my critical weights are updated
now i have to i have to clamp them or
clip them and i do that with these lines
that is i i run a loop for each of the
critic.parameters values and then i do p
dot data.clamp
to uh
and then pass these clip value minus
clip value and plus give clip value and
what is my clip value that's coming from
the hyper parameters
and i kept the clip value at 0.005
so the way this clamp underscore method
will work is that it will take two
values minimum and maximum
and
if the values are less than the minimum
they will be replaced by the minimum
value and if the values are greater than
the maximum then they will be replaced
with the max values
so up to this point was my discriminator
or the critic training that is up to
here
it started from these using zero grad so
now i have to start the generator
training and as we discussed in the
initial architecture that we have to
make sure that the generator trains
after only so many times the
discriminator is trained
so basically we are
going to increase the training
iterations of the critic
so that it works to approximate the real
distribution sooner
and that's why i am making use of these
if condition and that is taking the
modulus of hp dot in underscore critic
uh to be zero and only then i will train
the
generator and what is my end critic
let's quickly see the hyper parameter
definition at the very top
my end critic is 25 so only after 25
iterations of my critics training
then i will train generator so generator
will train after each 25 iterations of
critic training is done
all right now let's see what i'm doing
in the generator training so first again
the same line i'm
making a clean set of the gradient by
making use of zero grad
and after zero grad as per the original
workflow of generator training i first
have to create some fake images with
generator model and then apply critic on
those fake images so this line is first
creating
the fake images by
bringing in generator object and to that
i'm passing image shape which has been
defined earlier and z
is coming from is that's a noise vector
that also we have defined earlier and
then the generator loss will be
just pass these fake images as input or
argument to my critic function and take
a negative of that
that's my
generator loss
and then after the loss is calculated
i'm just implementing or just just
executing backward and step to do the
back propagation and update the waves
that's it and then pretty much my critic
and generator training is done and the
next these few lines are just
pretty obvious i'm just calculating the
total number of batches down till now
which will be equal to epoch uh the
current epoch multiplied by
uh the length of the train data loaders
plus this iteration uh
so this i
this i is a current index number
so that's my batches done and why this
is recorded is just to print some
statement here in these lines here i'm
just
printing uh the current state of the
training
so that's this whole thing is my uh is
my training method
up to here and then as a final
uh line in my trend dot py file i'm just
executing this method and that's it now
everything is ready only thing is i have
to
run these or execute this train dot py
file so let's go back to my google
column
uh yeah so this is a simple notebook um
that i'm using over here uh it has got
just two pretty much just a three cells
the first cell so i'm connecting my
google drive
and because i definitely need to use gpu
so i am changing the runtime to use my
uh gpu change runtime type and you can
see i'm using the gpu here
and these nvidia smi command will give
me what kind of gpu i got so here i got
a got a tesla k80 with 12 gb of ram
that's good enough
and then the important thing is that
that to the these collapse so to the
collapsation storage you have to upload
these uh three files that
we have are all codes in that is
trend.py utils.py and wgan.py
so
that's what i did here i have uploaded
these files you we can just double click
it
to see the files yep that's my files
and the way to upload it is just simple
you just content is your default
uh default directory when you open up
your google collab just right click on
it upload and select your drive that's
all that's all you need
uh so that's how i uploaded these three
files so i have got all the three files
and now i only need and you can check
that by running this ls command
yep so i have uh
dot py utils and wgan and then the final
cell just python train dot py that is
you are executing the trend dot py file
and why i needed to connect my g drive
is because
in my trend dot py file my root path has
been defined to be my google drive
because that's where i have saved my
mnist data
uh where is the root path yeah here so i
have a root path defined here that's
where my uh fashion mnist data were
saved
so let's execute train dot py
and it started running yep the epoch
started actually uh here i am actually
printing the or showing the images but
when you run a trend.py file like this
in a collab notebook you will not be
able to see the actual
uh images rather this something like
this will come and this is
this has already been recognized by this
is an open open issue in their github
repository uh to us to visualize actual
images that this training is printed
that is when my images are gradually
getting trained and they're resembling
more and more like the original fashion
mnist
that will for to see that you have to
run the entire code in a notebook
and to the description section of this
video i will also give a link of this
entire code and you can just paste all
the codes in a single notebook and then
thereby you will see the uh see the
actual images but here just for showing
you i will let me run the actual
notebook so that we can see the images
so anyway we can see the epochs are
running it's uh it's actually running in
a pretty reasonable time each epoch just
getting over in one and a half minute uh
averagely
and here i am i just uh included all the
codes from the three files into a single
jupyter notebook and uploaded that
notebook in google drive
sorry in google collab and
now i will be running this uh this
notebook and see what kind of result i'm
getting
uh
yeah let's uh let's let's just rest i
have to restart this whole notebook uh
and then i will run it and to show you
the images that is being produced
through the training
and this is where i just started the
training in google collab the entire
notebook and
so after each epoch this kind of images
will be produced so i am still
at my epoch 2
uh and that's why i'm getting these
almost um
almost noise kind of output but you
after you
go past to like 50 or 60 epochs you will
see a very much realistic looking
fashion mnez dataset being generated
from the entire network
and that pretty much wraps up this video
and i will put the link of the github
repository in the description of this
video just search by the number and also
note that in this video my purpose was
to implement a very simplified version
of wgan and that's why i did not include
gradient penalty uh in this one so in
the next video i will do another wgan
implementation where i will include
gradient penalty and also instead of the
fully connected linear layers in
generator and discriminator i will use
conflair so stay tuned
and finally over the next couple of
months i will do many popular gan or
generative adversarial network
implementation from scratch with pytorch
so stay tuned and do subscribe thank you
very much for watching see you in the
next one