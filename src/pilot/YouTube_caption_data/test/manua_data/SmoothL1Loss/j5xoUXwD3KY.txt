Nice to meet you guys
The content we will study together this time
will be Object detection & Segmentation.
By using the basic structure of the
convolutional neural network we did so far
It will be such a research field
that has been applied the most
It is said to detect an
object, but an object is
It's different for every application
So, the representative ImageNet
classification that we have existed
Or something like Pascal
There are many different,
some 10, some 20, some 100,
Something is a thousand,
like this, the number of classes
What can be distinguished is decided
But when we are usually given an assignment
This is a customized
assignment, usually 5, 10, etc.
Then all of it
In simple terms, I train again
Those things will actually become new tasks
So how do we do those things
Can it be applied well to the
existing convolution network?
You may also be curious
about the story of this.
I will do those practical
problems later later
This time it will actually be the
last part of supervised learning
With RNN, Recurrent Neural
Network, which we will do next,
Because it’s the final
stage of supervised learning
It’s difficult to cut the time into one
The data will be created by
dividing it into two, so let’s look at it.
What we will do is Prof. Li,
The material at Stanford University
I made some materials that
I used and supplemented
The most familiar thing
was image classification.
All the examples we made before
Image classify, that
is, what kind of video is
When given, it finds out what the video is
Because the background video or
the like is not designated as what
You can also classify this report as Sky
Or it could be a tree or a mountain
But most people look
at this and say it’s a cat
It would be normal for
us to classify it normally.
So now those things
called classification
Representatively, what
kind of network is there
What is this like? You already know,
It's AlexNet
Using AlexNet, I use the
convolution network all the time
Then even full connection
I made 4096
Do you even remember using two GPUs?
This last
Depending on how many classes
are you going to use, will it be 1000?
Or something like 200
It’s divided into 100 or 10?
The full connection layer
determines the number of classes
Because it divides,
it is called fine-tuning
It will be the part that
needs to be corrected.
From there, each Softmax, namely, cross entropy
error is used to create a score like this.
This score is the largest value
Probabilistically, this has
the meaning of probability
Because I found the
probability by doing exponentially
Classify this as the largest of this one
This way we know
So now, if these are
the general explanations
What else do you do a lot
If I review it all the time
It says vision task
Among CV tasks
This is a story that can
be divided into 4 types
Mostly including classifier
It can be considered one of
the four. Dealing with object
Including tracking
now, tracking is tracking
Then it's 3D
Because it is included
in the temporal direction
It becomes an extension line
Now first, what is
semantic segmentation now?
The definition of semantic segmentation is
Is to divide the region of a given image
There are some things to share
This is now called instance segmentation,
which is connected to the object.
This is the attribute it has,
depending on each attribute
With such a concept divided like this
Semantic is meaningful because it means
Sharing things with the same meaning
It will be like that. So I
think with a pixel concept
Split it into this pixel concept
It's called selective search
I keep searching around
It will be a task that
distinguishes its area
Then what we already know
classification
Then where the class is located
Doing a bounding box
like this to localization
I use it a lot for exactly competition
Now, when this is a single
object, it is called classification.
Otherwise, when using multiple objects
Now not a classifier
It is said to detect
an object in the middle
There could be only one here. Of course
Then it is the same as classify
If there are multiple,
like this, what is this,
dog, right now there
are 2 dogs, cat, 3 objects
If you look around, here
are some other similar objects
If this is a sofa
Even these things, if we
There is a definition that can be found.
Finding where such a
defined object is located,
All things including localization
are defined as object detection.
Then take it as a bounding box
If we say detection
localization
Otherwise, like segmentation
rather than a bounding box
To mark similar things in it like this,
May overlap
If they overlap, according
to the bounding box
You don't know what will happen
So, because it is usually
processed within the bounding box
Dog, dog, cat divided into three
For instance segmentation
I went to the back and explained it again.
So if you look at semantic segmentation
In other words, (text)
Specifies the category label
After dividing like this
Its area, pixels, is a collection of pixels
This is sky, this one is tree,
Then cat, grass at the
bottom, so it will include label
If you look at the second video here
This is cow,
Looks like two are stuck together
It’s like this because it’s
the same, it may be different.
Then it will be divided like
this. If you look here, it's sky
It's hard for the tree
to work clearly like this
Anyway, it went well
To not differentiate instance here means
that cow 1, cow 2 will not be like this.
Without doing that, just by pixel
It’s just examining what
a pixel is in probability.
So the number of objects
here will be divided by how many
It is also important
semantic segmentation
gives some prior information
I will practice
So, according to the sliding window
It means doing each area like this
In 2013, labeling was done
using hierarchical features,
superincumbent
If you see through the
patch, n x n, 50 x 50
If it is like this, what
is this, probability
Is to share what is most meaningful.
So this is cow, this is unfortunately
I can see my nose
Because a large area is grass
It will be such a story that
we are classified with grass
So the way to do it like this
It's a very inefficient, inefficient story
Do not use shared features
for overlapping patches
Because we don’t use that information
Inefficient because it is not reused
I’m talking about that.
So this center pixel here
It's a concept we classify
By using all the patches around one pixel
Whether this pixel is small or not
It will take a lot of
time because it is grass
Depending on the patch size, this
means that the overlapping is severe.
These things are actually computers,
so they seem to end quickly.
Doing this as it is is a bit problematic
I’m pointing out such a story.
So I design the net again
(text)
Because I am not classify here
This is just simply
It's only about 1 pixel
So this only uses the convolution network
3xhxw, h and w refer to pixels, or RGB
With this RGB as 3 dimensional
and construct a convolution network
Each has its own depth
Concept map
Depth, depth, like this to get
the score of the pixel at the end
The meaning of what one pixel is here
coloring, that is, labeling is such a story
The problem with how to do this is
I keep taking this whole area
Original image resolution
takes these H and W
It means that everything
convolution is too expensive
The performance may be good, but
Downsampling like this
rather than doing this
Using upsampling, what
we want is this and this
Because they are input and
output, the size should be the same
This is just prediction
Score values for 1 pixel
If it's 3, it's 3, and if it's 4, you
want something labeled like this.
So taking this and simply downsampling it
In simple terms, what are you doing
pooling. To reduce.
2x that turns 2 into one
So, changing 4 pixels into one feature,
continuously reducing it by half like this
Another half, after reducing like this
Features have been reduced
Then there, upsampling again now
It makes this by increasing it like this
That's the story. So
upsampling is important
downsampling is that we just
use the convolutional data feature
If we do nxn, 2x2 or 3x3
One of them is either max
pooling or average pooling.
The way to do that
How do you do upsampling?
Opposite. Take one sample and take two
Or one sample to 4,
widening the pixel like this
How do you do that?
I need your understanding
Anyway, our purpose is
It consists of a full convolutional network
I reduced this to save
time, then widened it. So this
There will be no ground truth
The ground truth is
painted by a person like this
Then it and the pixel
value that this created
We train this net by comparing the scores.
So, when the training result
converges, classify using it,
You can do semantic segmentation.
Then pooling is now called unpooling
It was upsampling earlier
Like downsampling and upsampling, the
opposite of pooling is called unpooling.
In other words, here 1,2,3,4
If there is an input like this,
take it and double it, so per pixel
If it's 2x2, it's easy to put all four
This would be the simplest way
nearest neighbor, fills all adjacent
That's the story
Then now another
The way to do this is to put only
one and fill all the rest with zeros.
We do this
It’s called Bed of Nails,
I put a nail in the bed
The magicians are just
walking with nails in them.
Then it’s said that it won’t
hurt and you don’t feel pain
They actually said yes
Because there is a center of gravity
If a certain degree of
fineness of the nail is secured,
You can walk almost
like walking on the plain
Even general people, but
at first it was scary to see
It's like a can't do it
like that, one by one
It is likened to such a concept.
There is a way to do this
What else do you have?
When pooling, you do max pooling.
Take the maximum
Only the larger values of 2x2 are
subtracted one by one from 2x2
The problem is that the opposite
of maxpooling is max unpooling,
This pulls out max like this 5,6,7,8
If you take out 1,2,3,4
This is max, but you
don’t know the location
These values here are set to 0
If we say one, bed of nails earlier
1,2,3,4 How to do this
You need advance information.
(text)
You need to know the location
information of some max value in advance
If you save it in advance
and send or receive it,
I need to inform the receiver
Then, using the value you gave me,
You can unpool it.
So by matching these to each
other, down pooling from this,
When max pooling here
When you go over from here and upload
It tells you the position
This is what we call U-Net, it looks like U
Reduce and widen like this
It will also appear in the
auto encoder concept later.
At that time, we decided to look
at unsupervised learning again
Now to reduce the amount of calculation
take downsampling and
upsampling symmetrically
I'll look at it like this
Then learnable upsampling,
Let’s look at what is
called transpose convolution
Rethinking the general
convolution, the input is 4x4
3x3 convolution was
done with pad 1 of stride 1
because there is a pad
We attach pads here,
stride moves one by one
As you can see there is a pad on this side
If you put the pads down,
up, right, and down like this,
There is one here using 3x3 kernel
All here are padded with 0
Filter 3x3 on the value around it
It comes out when convolution
Move one pixel to make a second pixel
One third pixel, eventually 4x4 becomes 4x4
same. Just this is a normal convolution
If Stride is 1 and pad is 1
So what if the stride is 2
Of course the size will be reduced.
Since I pad, I did this first
One pixel here, two, so how
do I move it? Move two like this
If you move 2 pixels here,
you will get 6 pixels here
Combines with the pad
to produce the output here
That’s why the filter
here is shifted by 2 pixels.
So, as a result, the 4x4 original data
Some kind of convolution that
is converted into 2x2 features
It was done. This is a general convolution
So what we want to do now is
transpose convolution,
the opposite of convolution
transpose, or we deconvolution
You can also use it like this
Or DeConv. Is the same story
The term transpose convolution, more
It is also used as the word Deconvolution.
Then transpose convolution
Now I'm doing 3x3
Considering that stride is 2 and pad is 2,
This was 2x2, now the
opposite of convolution
From here, you make it this way
Then it will be 4 x 4
You have to decide how many x
Because it's the reciprocal
of what we just did
Of course 4x4 will come out
Here 3x3 transpose convolution
Here's one position value
What does the value here determine?
It makes 9 pixels including the pad here
Among them, only 4
pixels will remain as a result.
It becomes 1:2
Horizontal, vertical, 2.4 in
one. stride 2, what happens?
If this moves
The value of one pixel is again like this
6 here
I’m going to erase the pad
Now it affects six pixels
The output created using
this one we did earlier
In the middle of here
At these two pixels, it becomes an overlap.
So the output is merged
I did before
The result of the transpose convolution
is combined with the second operation,
Eventually becomes summation
It is such a story. So, if
stride is 2 and pad1, 2x2 is
It makes 4x4. If stride 1,
pad1, it will be different.
Just think of what we
did in the first place
I did this here in the first place
It was the same for stride 1 and pad 1.
If stride 1 and pad 1 of transpose
convolution, output 4x4 is
It changes to input 4x4 again
So it becomes the same size
So, having a stride enlarges the size
It will be such a concept to narrow down
The case of stride 2 is
generally applied in deconv
It can be seen
I just opened it now
(text)
input and output movement
One input is 4
Including nine. I think there are 9
9, just because the data of this is useless
I just don’t use it
I can think so
There's another name
for this one I just did
It says Deconvolution, bad,
which means don't use it.
Convolution, then
fractionally strided convolution
Then backward strided convolution
I gave stride 2 here
because of the overlap here
deconvolution, in the
transpose convolution right here
Because it overlaps like this
is to attach it fractionally stride
Now this
Let’s look at the filter as a 1D example.
This is now data 2 and filter 3
So, this one and three filters
3 will come out like this by coefficient
One to three
As if one made 9
Since it is 1D, three come out like this
When we use b as an input,
another 3 comes out, overlapped here
One. Overlapping is because stride is 2
As this moves
In the end, a and b shift like this twice
stride 1 would overlap like this
So how many two inputs eventually made?
I made a total of five, but actually
How many should we use?
One can pad
So, (text)
So this value, in the end, you only need 4
Then you can cut it like this here. You can
get rid of it. It doesn’t matter if I do this
Anyway, these two become overlap,
overlap and become summation
That's how it becomes
a one-dimensional filter
If you think of it in 2D, it
is 3x3 and 9 comes out.
Because it’s a bit difficult to express
1x3, it's like this
Take a look here
in the case of convolution
This is the filter
kernel size is 3, this is kernel
stride 1, shifted one
by one, right? 0, x, y, z
As it shifts like this, the
general data a, b, c, d
It’s 2x2 here
I changed 2x2 to 4x1
And I made a pad
with 6 pieces of data including pad
If you multiply the padding
with stride 1 like this
Here, meeting a is ay, bz
After that, it becomes
a, b, c. So like this
How many of these should be made?
Here are 4 data 2 x 2
How many of these are made? There are four
equally made. It’s the same because it’s Stride 1.
So, 4 pieces were made
4 inputs to 4 outputs
Here, when the pad becomes 2, it changes.
X, y, z, so much more
There will be six
The output is spread out like this
4 inputs, 8 outputs, doubling.
Then what happens to
the transpose convolution
You just need to know this
Actually, I think this should go this way
take x transpose
This made 4x6
This is because of the
stride, moving one by one
Change x, y, z between column and row
Then it becomes 6 on
this side and it becomes 4
Then, if data is a, b, c, d
Now you’re expanding it here
It's 2x2, but after replacing 2x2 with 4x1
If you multiply this, what comes out?
6x1 It comes out like this
Of these, if only 4 in the middle are taken
deconvolution, transpose convolution
If it becomes stride
2, it will increase now.
As I said earlier
4 will expand to 16.
When stride increases to 2
Anyway, overlap and add like this
It’s a story like you get drunk here
This is the case for stride 2
First in the case of convolution,
3 x 3 called x, y, z
The kernel size is 3, I put
one pad on this side, a, b, c, d
data is now 4x1
What happens when you convolution four?
It turns into two like this
Convolution turns into two
Because convolution
decreases, 4 is reduced to two.
If you multiply it, it comes out like this
Previously, if the stride
was 1, 4 were displayed.
When stride becomes 2
It means giving them in half
one by one. When convolution
This deconvolution,
When transpose convolution
Change this 6
Change it to 6x2,
If there are two inputs
If 2x1, 6
6 are output
How many should be made originally?
4 can be made
2 times 2
Since you have to make 4,
you only need to take 4 of them.
Only four like this in moderation.
You can also take it like this
In this way, we use the
concept of transpose convolution
Enlarging the data, you checked earlier
One becomes two, one becomes four,
These things are vague
Given this simple filter,
like the convolution concept
What is deconvolution?
Take this and in the transposed state,
transpose convolution
The kernel is turned over. transpose
Original data output, this is
now a feature, creating data
ax, ay, this is overlapping,
by, bz. You only have to
get twice as much here.
You can make it like that
In general
Concept diagram. You probably
If you search, you will
find something like this
2x2, 3x3 Enter 4x4 into the filter kernel.
How many x how many come
out when convolution on the input?
Assuming the Stride is 1, you will get 2x2
It is multiplied like this and the filter is
masked like this, so 1 pixel comes out from here
The rest comes out when one moves
like this, so a total of 4 comes out
4x4 is reduced to 2x2. 4 pixel is 2 pixel,
In 1D, we did before
if stride 1
In that case, if the matrix form
If you say you need to
convert it to something
Then we arrange these in one dimension.
After listing from x0 to x15, these are
stride 1,2,3,0, pad,
Since this is 16, you can jump one by one
Pad the rest
Then 4,4,4,12 and 16 come out
Then multiply it like this
The first row of the matrix and this data
When you multiply the matrix, the
value of the first pixel is obtained.
Then this
Make this by making 1 pixel stride
Then 4 are made
If you multiply the second row by this data
Second value, in one dimension
If you change this to 2D, y0, y1
You can change these two
into two rows each like this
This is the concept of
convolution, which makes it a matrix
It means that it is
generally composed like this.
The convolution concept of
multiplication creates two matrices
We can easily implement this
is the product of matrix
Then as we did a little while ago
It comes out on the back slide
We now transpose convolution
Let's transpose this matrix over here
This is 4x16
This is the 2D to 1D
So what this is, the output goes in here
not trans. So it changes to 4x1
This is coming here
So what should this matrix be?
Since it becomes 16x4,
you can transpose this
Then, as I just said, the
row turns into a column.
Y0, y1, these
If you multiply it by 4,
you get the result of 16 x 1.
From x0 to x15
I got y by multiplying it with
a general convolution matrix
In the opposite case, the number of 4
pieces of data increases to 16 pieces.
In this way, 1 pixel is
changed to 4 such transpose
What we can do with
the concept of convolution
What was the transpose convolution earlier?
Like this, the four dogs were
max unpooling in some position
It's simple to get 4 times bigger
We can easily find the price
It is such a story
This is max unpooling and
I can get it regardless
Let’s look at the picture we saw before
down sampling
Upsampling again after downsampling enough
Here the transpose convolution takes place
It is such a story
(text)
It goes up once like this
from here. Once, twice
There is no pooling here, no unpooling
As a result, the value here
is semantic segmentation,
The pixel here is
changed to the feature here
That's the concept
classification and localization
This is what I did, so let’s simplify it
As we know
With this net
because it has to be done with localization
Localization is location
information. Box score
x, y, w, h where the
center point (x, y) and w, h
Contained in this
Where the object is
located is a bounding box
It means making it. The same here
It's the same until vector 4096
When dividing this into two, class score
Then, when making a
box, it is made of box data,
This is what we target,
It's called the ground truth,
but you have to do it here
It’s not like this, it’s
going to be like this
If the center point of
x, y here and w, h are
I have learned this
We keep making sure
this is exactly what it is
becomes a regression
linear regression
If you look here, the
correct value is given
one box of correct is given
This value and the result value from
this full connection axis calculated here
Compare and make L2 Loss
Here's the class score
When it says cat
If it's not cat, its score
comes out as a Softmax loss
Summation these two into a multitask loss
It is a story of combining
classification loss and localization loss.
This is later called RCNN. Region CNN
Apply as it is when it
comes out. Those parts
It doesn’t change, so
you can understand it here.
The important part here is
In front of here
used as a pre-train
This means it won't touch
What is a pre-train?
I do not divide this task into 1000
If you only have three
dogs, cat, dog, and cow,
This score over here is equal to 3
It is the same until
4096, but as it becomes 3
Changing this full connection
to reduce the Softmax loss
Then instead of
Do not do back propagation with
stochastic gradient descent method
Fix it as it is
Likewise for the L2 loss.
This is for each detected object
L2 loss is determined in advance
Position the correct box
I can compare it and make it
Doing this
In other words, it is
called transfer learning.
In front of this pre-trained
The first few nets are almost all.
Because we are all, usually
image nets designed as classifiers
In addition to the network designed
to be divided into 1000 classes
There is this called pascal
voc. There is some competition,
It’s divided into 20 there
So that it can be divided into 20 classes
The score here,
By just changing this,
learning will be much faster.
You just do that
There aren't any great secrets
In this case, the transfer learning method may
always change the first stage slightly here.
In some cases, this first-tier
net is changed a little.
It's a unique Alex net, but in
fact, I use a lot of VGG networks.
The pre-train network
is a lot of VGG network.
So, not VGG network, V16
network, especially V19 network
Fine-tuning the first stage on
16 networks, transfer learning, or
Or you can fine-tuning the
back part or distinguish it like this
So I've done it so far
Now this is the important part
40 minutes have already passed now. anyway,
The time is 60 minutes,
I'll upload one for a 70 minute lecture
Please break in the middle
This is called instance segmentation
I said it's called object detection
Looking at the impact on deep learning
Until the deep convolution network came out
in 2012, so-called mean average precision
It was far below just 50%. useless
Actually, it couldn't be used
Pascal VOC came out at this time
Actually, I did it before Image net
came out to divide it into 20 classes.
But then, of course, deep learning
is not used and it is in a low state
Since finally using the deep net, if you
use the deep net, like the previous example
I mean, transfer learning is also possible.
Using what came out
So this is rapidly up to 80%
increasing accuracy,
That means you get the result
of increasing the precision.
Then this is a regression story
The important thing in object detection
Where is this located
There are three here
When there is one
I only need one object, but I
don’t know how many there are.
Its position for every object detected
So the localization center value,
Next, you have to mark
everything using w and h values.
If the convolution network says
that dog has become a class
How many nets do you need to get a
dog? You will need as much as this number
That's RCNN now
Then, if you look here,
the duck moves around.
Each one has to be bounding box
It's the same story
Even the duck, which is
the same type of object,
10 for 10, 20 for 20 nets
It should be operated
So each image has a different
number of output, of course
This is 4, this is 4x3, I
don't know how many of these
Because it is overlapped by the number of
May overlap
If you do it wrong, three can be one.
These things can be seen as difficulties
How to detect object
We are first
The easiest thing to
know where it is located is
This is a sliding window. To move
One bounding box is appropriately,
this is also called an anchor box,
I move it all the way
using the box I decided
So, it means that it judges whether
the cats or dogs are detected here.
If you see here, what is this
close to if you judge like this?
It's close to the background.
In other words, background
Nothing different
I have previously defined it as an object
Dog, cat, mouse, bed
Everything else is background
That’s why the background
is considered high
CNN classifies each crop
as an object or background
To crop like this means to crop
One image like this. It can be sized
It could also be different.
It is called region proposal
We take the size that comes from there
In the case of a second like this
There is a dog here
So what happens if you judge there?
I have to regression
You just need to regression 4 x, y, w, h
They overlapped while
moving. dog. This is cat
So what's the problem with explaining now?
(text)
Its region proposal
I just thought it was a bounding box
This value can be small or large
The theory of this is very easy
How would a computer do this
I don't know. So everything
from small to large
A huge amount
It means that you have
to do it all the same way
So it's computationally too expensive
Time consuming
There is such a downside
To improve those shortcomings
(text)
The region likely to
contain the object is the first
What this is called region proposal
(text)
Selective search means
Divide the whole area
into similar things like this
Dividing and merging like this
It would be helpful if I had some pictures
Take a look at the papers here
The 2014 paper called selective search,
If you look here, it is divided like this
Flowers, and a big dog, this is a cat,
It is analyzed in its own way
The problem is these things
It’s wrong in our view.
These are not objects, nevertheless
It comes out when you do a
selective search. So usually
Thousands of times, 1,000
region proposals are made.
We put these one by one into the net
There was a net before, this is it
What is R-CNN, Region CNN,
when input image is entered
By performing a selective
search, the ROI area
Region proposal, about
2k, that means up to 2,000
I'm trying to do all this
Whether this is really an object
I'm looking for, even here
Anyway
What I did by turning with
the selective search algorithm
Now, to simplify the operation here
It's called warp. What is it?
It reduces X and y equally
Crush it
By dividing all these long things by nxn
After warping, it passes
through the CNN network.
Pass this CNN network to see if
the class we are looking for is correct
If the class is correct,
regression the region
Proceed with that
Now, if you organize it, it looks
like this. Looking for the ROI of this,
Regions of Interest. Region proposal
Instead of a region
proposal, initially, in RCNN
Here's a paper written by Girshick.
This is a 2014 thesis.
Warping each of these
warping to keep the size
constant. 128 x 128, or
It depends on the size of the entire image.
Then half of this
Reduced to about a fifth, then each
Each of the Conv networks is
configured. This is the feature
At the same time. This is the problem
We're not going to have
a Conv network structure
Then, if you say VGG, if
there are 1,000 proposals,
I'm spinning all 1,000
Of course you can run it serially
But, in fact, it takes a lot of time.
So we learn this
Each is now as I said earlier
It means that if there is an SVM
class, if there are 20, then 20 is judged.
Likewise, the bounding box
If this is a class, the bounding box is
regressed and the position is ground truth.
Compare it. Anything that is not
an object will not run here anymore
Then this is thrown away
If I try SVM on ConvNet and it
doesn't match the class at all,
It's the background in the ground truth
There will be no ground
truth. Man made it in advance
The part where things other than
the bounding box do not overlap
Of course, if there are some
overlapping parts, it’s ambiguous.
This is between object and non-object
In that case, we define a
percentage of the ground truth.
If more than 10% of the ground
truth is reported as an object
50% of them are usually
done in experimental papers
If it is less than 30%, it becomes a background because
it is said to be non-object by the ground truth.
Disappearing like that
In this way, I take this and learn it
Learn the net of this
(text)
How far does the bounding box move,
You have to find it left and right
How to do it in RCNN training
First train the CNN model
with the image net data set
This is the Conv network of this over here
The important thing is this
Because imagenet is one thousand classes
You have to train the network
to make a thousand class scores.
This is actually a pretrained
I’m not saying you guys do this
This is already available to you
All you have to do is
choose. Will you do VGG
Will you be Alex,
You can take these
pretrained ones and use them
The values here have
their weights already set
Then, from here on, if you have
20 classes and 1 background
The background is also obvious
You can think of it
as a class. So its class
is to judge whether it
is the background or not,
So, with 21 classes in the background
At the end, we change the 1,000 class to 21
So, 21 class scores will be
given, and a loss will come out.
Using that loss function
Just these two, leave this as it is
You may want to fix it
You can modify some of the
front parts from the back like this
It’s when you want to touch the net
If you don’t have this alone, then you can.
But if this works very
well, you don’t need to
remove the last full connection layer
Lower this
As I said now, having 21 layers
Fine tuning with such FCC network
(text)
I trained this using pascal VOC data
Then extract feature,
this is region proposal
Number 1 and 2 here means that
we are going to re-edit the back here
The proposal I made from the third
Its location, object, crop
Well, even if you do this a little smaller, the
size will remain the same and it will stab like this
So this is an autobicycle now,
but it may not be an autobicycle
That’s why, it has to be cleaned up later
Take this and put it in
convolution and pooling to train
Then SVM for each class,
Train SVM of each class for SVM,
So the last man of this
Here it is. To compute these two networks
This is the net that makes the score
Because this is a network
connected to distinguish it
weight will be here
So now the cropped image
Now compare it with the ground truth
These two are motorbikes
Positive, right
But the motorcycle was
cut like this in the middle
The side face is ambiguous
In the case of bicycle, this is all
It's a non-sample. You
can see it in the background.
So when these things come in now
It’s not like this
This is what we have the
possibility of doing SVM
This one can be used as a
motor even when people see it
Maybe not. So negative
It's called positive. Harr
Cascade is the same way
What is usually an SVM?
It's splitting the two. Like and not
After dividing the
positive and the negative,
When tested later
Because it is to learn to be distinguished
That we are motorbikes
To find it properly later,
a positive sample and
Obtain each negative
sample and put it in the input
Divide it by 0 and 1 like this
Dividing like this, svm is dividing by two
But it’s judging like that
and classifying the classes
Now, then we’re going to bicycle,
It's a bicycle. These are only two
bikes. It's the same as the picture before
These two are positive
bicycles, this is negative bicycles
Even if these things are the same image, if
I want to judge the bicycle in the output,
Its weight by putting it in the input
That’s the story of learning coefficient
Each class we have, 20 for 20, 30 for 30
I said there are 21 pascals
Actually, there are not many classes
What you do
As mentioned earlier, it usually
depends on the application
Usually identify vehicles
on the road for example
Is there a bus now, what else?
Truck, car, then large express bus,
There are about 10 bicycles.
Then it also gets all the positive and
negative samples for each and trains them.
This is what machine learning does
Train it
Prepare this now
test is the same,
After making the region
proposal the same, the score here
So you just have to regression
what it is and where it is.
Problem, RCNN came out first
As a 2014 paper, RCNN is region CNN.
At first it wasn't written like that
Initially Rich feature
hierarchies for accurate object detection
and semantic segmentation
What's our title now, object
detection and segmentation
Hierarchy for it,
rich feature hierarchy
Later people called it R-cnn
Actually, it wasn't.
Now, 2014 is the year ResNet came out
Was VGG the year? Anyway,
it’s the year that came together
Then let’s look at the problem
(text)
This is the linear SVM. So at the
end, I’m saying to do SVM classify
(text)
The problem is with the adhoc
method that you have to do these things
If training uses one GPU, it is 84 hours
Because it was the GPU at that time
In fact, compared to now,
about 8 years have passed since
If you do the same now,
it will be much faster now.
It also takes up a lot of disk space.
Then inference,
test or detection
It takes another 47
seconds. image on one sheet
When using VGG 16
If Conv used VGG16
Then it came out in 2015.
Then it was fixed by the SPP net
SPP comes after
Modified by the SPP network
It's a story that was fixed. Anyway,
training also takes a lot of time
47 seconds is difficult to apply.
So this RCNN is just a reference
To fix this
fast R-CNN in 2015, a year later
The same person made an offer
Oh, not this one
So I made this
And I simply wrote the name Fast
R-CNN. That's why R-CNN is known
How to do R-cnn
What is fast R-CNN, input image
Here we use one Conv
network in the Conv network
Previously, I made a region proposal and wrote
a selective search or something like this
After making a distinction, do each ROI
You have a separate Conv network
But without doing that
With features in this Conv network
Take this and calculate the ROI from each feature network
domain and from the feature network output network.
Does this. There is no last FCN. Currently
And this is called ROI pooling
This is the concept of pooling warp
Warp is a reduction in
input region proposals.
The same ROI network in the future
Pooling itself in the feature domain
That will be explained later.
With this one coming out now
Now through FCs, fully connected layer
Now in case of classifier,
class, linear + Softmax
And bounding box
regression is linear regression
In this way, we teach two
at the same time like this
Including the weights here to the
Conv network. What are the features?
It removes proposal from one
image and shares a convolution layer.
If you do that, it's faster than RCNN.
After that, it is characterized
by using ROI pooling.
Now explaining this, for example
Video is 3x640x480
This is the original
video. I ordered CNN here
The size has changed. The number of filters
Then, assuming the size is feature 28x10,
You can change it to 512 x 18 x 8
Anyway, no matter what shape
Convert this to 7x7
20x15 to 7x7. then
To change the x-axis 20 to 7
It should be reduced by
one per 3 pixels like this.
In the vertical direction, all 2 pixels are
reduced by one, so a grid is created like this.
Features appearing in each
filter for 512 filters of 7 x 7 size
They are listed as 7x7. With these
full connection layer
To learn. For example, like this
There is a VGG network, 16 like this
From the very first stage,
it’s going to be FC 4096
I use 4, and then I use FC 1000 at the end.
The value shown here, the
last value came out like this
And the values will now change size
Well 40x20, 60x80
Even if it came out like this
How to make all this,
change it into 7x7 size
This is rectangular, but I drew
a picture in rectangular size
So, for example, the 21 x 14 video is wide
Then pool the horizontal to 3
The vertical is pooled with 2
Because I use 3x2 stride
As I said before, I change 3 to 1
As a result, it is a
story that makes it 7x7
Now pooling is max pooling. 6 pieces in 3x2
Pooling with max value among pixels
Now there are 7 coefficients
All 49 coefficients are made
For example, if 35x 42, this is long
Still the same size
Why should I do this,
You should ask the author
for the validity of this.
I must have experimented
7 x 7 or 9 x 9, there should be a lot
Anyway this is what we call pooling
So when we come back and put the
input image back to the training stage,
Here we have 7x7 data pooled on this side
You have to judge whether
this is a so-called object or not
If it is an object, you have
to determine what class it is.
In its soft max score, linear
Coming down from this side
Log loss value. As you know, log loss
So-called cross entropy, log entropy
Next, this is a smooth L1 loss. I'll
explain the word smooth a little later.
So, it means multitask
anyway. I said earlier
I do two each, but I put the two together
There will be a combined
amount of weight in the middle
We can mark it as lambda
Conversely, going down like this
These are all weights. All weights down
You have to regression like this
(text)
(text)
7 x 7 that makes the
feature vector constant
7 is the experimental number
7 is just a fixed number.
Output 2 pieces of information,
K+1 class label. 1 here is
because of the background
20 classes called K,
In the case of Pascal. Then 1
21 in the background
Each label has a loss
Here it is. log loss
What is P here is predicted class score
The next U is the ground truth class score.
Simply what 20
It could be 8, and with a log loss score
That we have to calculate
bound box size location, about location
V is predicted box coordinate,
There are 4 x, y, w, and h.
Because it's a vector value. Then the
true box coordinate. Because U here is true
2 box coordinates
like this, 4 for x, y, w, h
Take each value and put it in this equation
Each summation of the two
differences becomes squared
If X is small and the value is
relatively small, it is squared
This way
If not, just go straight like this
If you draw this, it will look like this
If you call this function to draw
If you say 1, -1 for x, this is
This is a quadratic function, and
then a straight line goes up like this
Eventually, it is a quadratic function,
but this part becomes a linear function.
In this way, we create a smooth
function and use its loss function
It’s called lambda
When this lambda is
greater than 1, U is now
In the case of classes excluding background
In fact, you shouldn't use
location for background.
Because there is no location
In that case, lambda is usually
According to the thesis, it says about 10
And its loss function
When total loss of the
location loss function
This can be done experimentally
by changing the lambda.
If we rearrange Fast
R-CNN, the Conv network
In this case, multiple ROI areas come out
through the Conv network. There is not one
Multiple ROI pooling layers for each,
The Conv network is
doing multiple proposals.
One by pooling is Softmax,
FC, bounding box FC
to learn this FC network
This is generally FC
after the ROI pooling layer
I'm behind the Conv net now
This is exactly this part
I used the pooling layer right now
Then go through these two
in a straight line, then in series
The last net is now K class
Dividing by, these two
are soft max and regression
Each of the two is
parallel, so the score of this
You can add the score
of this as I said before
So, if I talk again, what are you doing?
Each Conv network is regression
What kind of network is this
class? This net is a regression
So L2 loss or L1 loss
This loss is actually a smooth
loss in the paper, neither L2 nor L1
Total summation of each of x, y, w, and h
So if you look at the
result, RCNN was 84 hours
This is 9.5, about
eight to nine times faster
Test time from 47 seconds to 0.3 seconds
speedup is faster again
After that, the precision doesn't
drop much. precision should fall
Actually, it’s right because
the speed has fallen
As I said earlier, in Pascal through VGG16
It's a net that we
re-trained and fine-tuned
This mean average precision is
almost the same. So this is dead
Around that time, something called
SPP-Net comes out in the middle. In 2014
And Girshick is R-CNN
Another fast R-CNN is in the middle
In one year, it’s almost the same year.
So, the training timing was
reduced by a third and further reduced
The title of SPP is spatial pyramid pooling
If you are interested, just look for it.
Now, 1 hour and 10 minutes have
passed and you have to get faster
Go ahead to that
I'll finish the material,
so I hope you guys know
Looking at the test time, R-CNN doesn't make any
difference when including and excluding region proposals
2 seconds. Then SPP is also
the same, and the problem is this
In the case of fast,
2.3 to 0.3, which means
The runtime is much
longer for region proposals.
2 seconds seems short, but
actually it belongs to the long side
So, it is much faster to
do without region proposal.
Faster in case
I can see it like this
The fact that it is a region proposal is
In the case, it’s just not talking
There must be some
accuracy problem. Accuracy
Then finally, now about the same year
Now a guy named Ren is
Faster R-CNN, of course
When you see Faster, you see Fast R-CNN.
At about the same time
Region proposal what
can be done in real-time
brought back the network
If you put it up quickly, in the
case of faster, there is a CNN
I made a feature map here
But this is just ROI pooling
using the feature map
I went like this
Now I use this to create
another region proposal network.
So this is the classification loss
What is this? class or not
Determine whether the
object is correct or not
Is this an object in a
kind of region proposal?
One feature is that they divide like this.
Then make a regression box
After that, the final object
classes are 20 and 21.
Final regression and
Using two together to make
the exact position of the object
This means that it is fast
Even if I keep thinking
still, it's actually fast
Because this one was added between faster
Of course, isn't it natural to take long
This is it now. Compared
to this, 8.5 seconds
Qualitatively, this makes it faster
I don't understand a little common sense
Then I'll see
I don't know the accuracy
This is detailed when
you look at the thesis
I'll explain it simply because
I don't have much time
faster R-CNN creates k anchor boxes
This is now the feature map
The feature map will be large
This is where you are now
region proposal, this position,
you can think of looking at it here
This is it. this
Here, not here. I mean the
network that makes region proposals.
What's here
Now the region has been decided
Then take this feature map value
The location is good 10 x 10 is also good
Well, I'm assuming that
50 x 100 is good too
Then this is called an anchor box
Centered here, the size
One thing to take the size of this
At that size, the aspect ratio is 1:2
Then make the width 2:1
In this way, k is 9
So if you look here, 128, 256, 512,
I did three like this. Multiply this
Multiply 1:1, 1:2, 2:1, this is 3x3
There are nine of these. Have that size
Put this in a convolution
network with 256-depth
So, with anchors, each anchor
Put it in like that
I'm talking about regression
It would be helpful to explain here.
Now, I chose two of these here
3 x 3 convolution, this is now
128x 128 and 256, 512 plus 1:2
Then we make this 3x3 convolution
And now 256 filters and then
512 in case of using VGG network
ZF came after you guys Alex network, 2013
So, based on this, do
this one with 256 depth
Make this one by 1 convolution
So for the first pixel over here,
This is to determine
whether it is a class or not
Here is the score. So the score here,
Nine here right now
Each size, two. Is this an object or not
Comes out. score
18 values
Make it in the first layer
And after making the
same in the second layer
Do the same 1x1 convolution
It is x, y, w, h position, regression,
So nine proposals,
In the number of anchors, 4x9 36
outputs, creating bounding box values
So, making a proposal like that
This is the value. 4k, k is
anchor, 4x9, this is 2x9, 18
This is what makes this regression
This will be the region proposal network
These two, loss. You have
to change these two weights
There will be ground truth
Correct this with the ground truth, the net
The same goes for this.
Because this is the same as before
I use the same one
Its faster R-CNN's test time is
R-CNN, 49 sec
test time. Then fast R-CNN is 2.3 seconds,
Then faster R-CNN is 0.2 seconds
The region proposal
When I used it, it was
better than when I excluding it
By a tenth, it is improved
when doing faster R-CNN.
Training time doesn't appear here,
It will take a lot longer to learn this.
It will take more than this
It takes longer than 8.75,
which I haven't verified
I'll ask you to browse the paper.
Test time is good
It becomes faster like this
Actually, training is what we
do once we are an encoder
Because the rest is reused
Actually, the test time is
faster than the training time
Of course it would be nice
If compare, test time is
0.2 seconds, 2.3 seconds
average precision does not decrease
The average precision does not
decrease, so it will be a feature of this
Now so that’s it
We are now taking a
general look at that detection.
Here, now and after, YOLO
YOLO, SSD. I don't have time now
In fact, you can continue
Because I separated the data
and recreated it as a separate data
Let’s look at the next
As in the picture earlier,
instance segmentation is
How is the difference between
R-cnn or fast, faster R-cnn here?
While this needs a
bounding box, what is a kid
What kind of region do you need
Because we have to
separate the three values
The paper that did it
was called He in 2017,
I read huh because I’m Chinese.
What is this guy talking about? less net
You are the person who made it.
I did this guy called Mask R-CNN
To tell you the point, the detailed
algorithm is not included in the data.
This is almost the same as fast R-CNN
fast R-CNN
is to regression the location of the region
This is not regression, it aligns
Location, through the CNN here
Align region, align pixel unit
You also have to ask which
class this one belongs to
20, box, this is the same
By being aligned, the
prediction becomes like this
Each of the values here in pixel units
We make it
It will be a concept that makes a mask
This is actually a straight line
Actually, what kind of
location will come out like this
Location here
Values can change in such
a tight manner. It’s going well
Even if you make it like this, the
ones here divide the anchor box.
Whether made by pooling ROI according to the
size of an anchor box or whatever region proposal
After making them
Masked like this. It will take time
Because it is in pixel units. But in front
Because it is not a full search
like semantic segmentation
When the bounding box goes well like this
Because it proceeds only within
It’s a much improved method.
So with mask R-CNN
Typical R-CNN, faster R-CNN
That everything is
connected. The net is the same
So you can do it yourself
Now, for now
We have a basic overview of
object detection and segmentation,
I explained simple algorithms,
It’s best to do this by yourself
What you are interested in
Downloading directly
from github and running it,
Don't forget
The lecture will be over here.