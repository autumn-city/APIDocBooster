Now, let’s think about LSTM and GRU networks this time.
This is a special case network of RNN network
I will come in more detail later
long short term
memory-Gated recurrent unit
The two networks are similar
It’s worth comparing each other
We will continue to explain this in detail.
Now, as mentioned earlier in the Recurrent Neural Network
ht stores a1, a2 outward, weights
it again, and recurrent it again.
What we do repeatedly like this is to see the hidden layer as memory
Intermediate internal memory as a result
It was the weight summation of the tangent hyperbolic function.
It means typing and seeing together
How is the LSTM module called
long short term memory organized?
There are one two three gates. Originally, it has four inputs and one output
Originally there are 2 inputs
1 and h and out
In long short memory. First of all, this is input
Then the signal control input
gate that controls the input gate
After that, this is important
forget that it is a forget gate
This actually functions as a memory erased
I will not use the memory of the past anymore.
And memory writes and adds inputs
So in case of clearing the memory forget gate
And finally, the output gate is only exported when needed.
I keep writing what's here only in memory
Output when you output
These inputs are signal gates, input gates, and forget gates.
Next, input and output 4 inputs, rather than 4 inputs
One input and three gates.
It’s up to you to think
Now, the composition is like this
z is the input
Controlling the input gate is zi. This is sigmoid
What is a sigmoid?
Opens and closes, changing between 0 and 1
These functions are closed if 0 and open if 1
It's a sigmoid function.
And it's multiplied here
If the output from the sigmoid function is 0
There is no input from here
If the sigmoid function is 1, it passes
With the g(z) function this is a tangent hyperbolic
The input is tangent hyperbolic, the gate is sigmoid.
So the value here is a memory cell, the memory cell is called C
There was an h
Think of it as c instead of h
So, I’m going to go through the input gate
The past data that was originally stored in C
The Zf value that forgets this is composed like this
What is g(z) here now?
G(z) was added by doing the input gate. This is it
It's a plus concept
Two are plus
c is the past value,
f is the gate
It is a value between 0 and 1 by z
Because it usually falls down so quickly
Just a little plus or minus 1, one
becomes 0, and one becomes 1.
Now, add these two and print them out
This is a function h that is printed and memory, this uses a tangent hyperbolic
And then it’s sigmoid
Now, multiply the two. Input to input gate, output to output gate
If this is 0, there is no output.
If this is 1, the output goes out.
Not 0 and 1
Sigmoid function
As a result,
Some big enough value like this
If you put it in the input of the output gate
Binary values are one minus 1, one plus 1
In this way, the value passing through the output gate
0 or something like this
Now, this is how we organized the memory
I configured the memory
It will be this addition that adds an input to
the gate that forgets the value outside the past.
For example, there are X1, X2, X3.
y is now the output
What is X1 is input. X2 is important.
X3 is an output gate. Is to open the output
If X2 is 1, the number of X1 is put into memory.
X1 here is the input
Enter the input value
And, now, if X2 becomes minus 1,
Now forget, reset the memory
Most of the time, if X2 here is 0
No input
If X2 is 1 then X1 3, 4, 6
Put in C
X2 has a forget gate
X1 is the input gate
-1 is the forget gate
Now, looking at the picture here, it’s the first time I saw it
Take a good look. I'll explain, just listen to it.
X1, X2, X3 are all entered here
I can control it. Plus
Now this is the weight
0 means there is no connection
No matter how X1 goes in, this value is 0
What is this?
4. Multiply 4 in binary
This becomes minus 2
It is structured like this
The X3 doesn't react, only where does it react?
It only reacts here. Last
output gate. X3 is all zeros
I don't need the X3
The problem is X2
I added it by 2
This is the sigmoid gate
Because this is a tangent hyperbolic function
I put 1 for that
It’s a concept of biasing the value
The problem is X2. Where do you usually use the X2?
I wrote it on two
One input gate, one forget
gate, no need for output gate
I do not need it
Input is also not included here.
I can't go in here either. Only here. Not here either
Now it’s organized like this
So even if you put the same X1, X2, X3 values, you don't need anything
In fact, it’s not connected.
I just marked it like that
Actually, X3=1,
Some of them are composed of X=1, and some are composed of something like this.
Now then, put them in order like this
3, 1, 0 of x1
There is no need to put in 3. X1. I put 3, 1, 1 like this
Now then 3, 1 is the value
It doesn’t make sense for the connection line here to be 0.
Now, because the one here is 1
When I put in 3, it went through the input gate
This is 3, which is 3
What is this? I assumed that this is now a linear gate
Tangent hyperbolic now changes back, but
Once you think of this as a linear gate, it's 3. indeed
Just to give an example. Comes back later
In the case of linear gates, what is it?
Who controls the gate
As you can see here, it is already replaced by 1, 0
This value is X2. Because the value of X2 is 1
That's 4 This is -2. How much is the addition? Joe 2
sigmoid would be 1
3 goes through C
In C, the value of the past
First, the past is 0
So of course not
Here 3 goes up as it is
Since this value is 0, this value has no meaning.
Now, what comes in here? Let's get in
What is it? Because X2 is 1
This value is plus, and it adds to it, so what is it?
This is 4, plus 2 is 6.
It's 1. Only 1 interest is 3 because the original value is 0.
How much is the output gate?
The output gate is this value. Because it's 0
Even if you multiply it by 4, this value is 0, so it is 0
3 comes up to this point, but there is no output
Now, so if you put in the first 3, 1, 0, y becomes zero
Then the second 4, 1, 0
Now you can just look at this
4, 1. Because y is 0. This is important for this
4, 1. Enter X2=1. You just need to be interested here
Then look
Because this is 4. This is not applied here
This one works here
4 goes in and 4 comes out
How is the gate here. Same as before
Because it is 1, it passes because it is the same 1
Make 4 What was here originally
There were three. I remember. There were 3
There were 3
I am doing this
Now, 1 has entered the forget gate
100, so it's 4
Since this is 2, it becomes 6, so it becomes 1
Add 4 to get 7
Since the output gate is 0, 7 reaches this far, but
The output is still 0. Ok
Here I checked the value that the memory was updated from the previous value
Now, I put 2, 0, 0
2, 0, 0. There is a 7 in this right now. All you have to do is remember this
Now this is 2. The problem is this is 0
X3 here is 0. 2 go in and pass
Passed
Making the input gate zero here means that the input does not pass.
Since this is 0, 4 has disappeared.
It's 000 here, and it's minus so it becomes minus 2
Since this is a sigmoid function at minus, it will be zero
So we multiply so we get 0
So what's going on here
Here is the 7
What happened to the forget gate right now
Since X is 0, 10 is here to say that X is 0
1 becomes 0, 2
Since 2 came in, of course, the gate here is 1
As it is, 0 and 7 in the past will pass as it is.
It cannot be erased here.
1 here is important
Whether this is 0 or 1, it always passed even when it was 1
It passes even when it is 1 or 0. Because I have this
Even if it is 0, this value is 4, but since there is 2, it is not deleted.
I am doing this
2,0,0. Since this is 0, what happened to the output?
Minus 2 goes over here
Now minus. As before, this value will be 0
There is no output
Now, finally, there are 1, 0, 1 outputs. I'm looking forward to it
This is the case
1, 0 This is meaningless
1 0 1, here it is. 1 0 0 0 You only need
to look at X1 for input and X3 for output.
The rest are not connected
Now, 1 went in. Then you will get 1
What happened to the input gate here?
It's 0. It's -2. Then of course it's 0
Because this is 0. X equals 0
It means that the input cannot be delivered.
This and this two were delivered
These two are not delivered right now.
Entering this value is meaningless
So, only these two in the past, the 7 is still maintained.
Since X2 is then 0, what does 0 mean
The forget gate doesn't work.
Because this is 1
7 comes in and it becomes 7
Because it's 0 here, it's 0 plus 7. 7 goes out
The problem is because this is 1
X3 equals 1 turns into 4
This is minus 2. Then 4 minus 2 will be 2
Then sigmoid will be 1. Output is 7
All of the two things that have been in the past are not reflected
It receives input only when X2 that came in before that is only 1
It prints out what was saved here
That I bring only what I need in the past
Only the information I need through X2
It means that I will not stack up things I don't
need, and erase everything I don't want to memory.
Finally let’s look at this
You should have figured it all out
Minus one is important here. Here and here
The last zero is here
Next 3 goes in here. Just look at this
3 will go in
Now, the minus 1 is now changed What is the minus 1?
The minus 1 of the input gate here is
Because this is 4, minus 4, minus 6, so it’s 0
No input was passed
Whether it's 0 or minus 1 here
I knew the input was never passed no matter what, and the problem is this
If -1 goes in here, it's 4
This is 10, so it's 2. How much
is minus 4 plus 2? Will be minus 2
So this is 0
I said this is multiply. How much data was in front of here
It was a 7 on the slide a while ago.
They go out, but they have it.
When this comes in, the value of 7 comes in and
becomes 7 x 0, so it becomes zero, and here is also 0.
The memory becomes 0 as it is, and the output becomes 0.
This is not printed
Since the value here is 0, there is no output
Nothing goes out
It's over after only seven. In this way, this is also a vector
X1, X2, X3 are vectors with one scalar value.
I used this X2 to control here, here and here.
This is the input
In fact, only X1 is important
X2 is important here and here.
The X3 is important here. Is structured in this way
It's roughly like this
The important thing in the middle was erased through a minus in the forget gate in this cell
In addition to memory, sometimes you don’t.
Not addition means blocking the input gate here
It blocks anything that entered the input.
Keep things in the past, but don't care what's in now
Who has to decide this
Someone has to decide
When deciding the part of speech, the unnecessary order is erased
Now that’s it
I have to use all of those things to learn
That’s an important factor here.
So here a1, a2
You went separately for the X1 and X2 inputs.
You can spread this like this
Then we have two values here, and these two values are as follows
I used four vectors with the Xt value here
One of the four vectors is a forget gate, one
is an input gate, and one is an output gate.
And, one input
You can put 4 as vectors like this
You can put a value of 3 here
I put 1 in the forget gate, and also the minus 1, so they are vectors
Because they are scalar values, they are vectors.
Also stored here is a vector
A vector called vector Ct-1. In the past, the forget gate merges
This is Ct-1
All these are shared
This is all called a single overall LSTM
Then we shared the inputs
It's like sharing and controlling.
If you open it like this and draw it like this
Where Xt goes in
at the input gate
Multiply sigmoid function input
At the sigmoid entering the forget gate
The forget gate multiplies, inputs, and adds at input Ct-1.
Remember the previous one
And then you pay for it
Passing or subtracting from the output value to the output
This is tangent hyperbolic, this is sigmoid. There is a difference
It is organized like this so you can organize it like this
You can make a vector
Input and then the required input and forget gate value
Just put in the output gate value
You don't have to close the output gate.
You can just leave this as 1
Just keep the connection with the forget gate.
In this way, the configuration diagram of this is continuous
Ct-1 comes out to memory
What is ht-1?
Ht-1 and data from the past
This is what rules the forget gate
In this picture now, these two ht-1 and xt
are added together to determine these values.
Has two inputs
Here ct-1 is an internal value
The cell is an internal network
Because h is a net flowing from the front
You can draw like this
Control this by two values
In the flow t after this time, ht, xt+1 continue to pass like this
Y occurs and
You will have this general configuration. Let’s interpret it
For the Standard RNN module
The output now takes ht-1 and did a bit of making the output
not y, but h
I remember when I was standard
Put X and h. h is that these networks that are
transmitted all the time keep coming in recurrently
xt, xt+1 If you look at this one
Then we multiplied ht-1 by w
What was it? It's changing from ht-1 to ht, right?
This was Whh, and what is this? This will be Wxh
X through U
After adding these two
After adding this, add a bias
If we call this tangent hyperbolic function, it will be h.
I did it before
The important thing is that there is a gradient vanishing issue.
When you keep going from the back to the front like this
It doesn’t affect the front
The value at the final output is hardly affected by the first time step
It’s not space, it’s time.
The rate of change that occurred at X-1000 is up to X0 here.
I have to hand over it and update the weight
They don't give an impact
Since this is too small to affect it, it cannot
be corrected, so there will be some errors.
Earlier, there is e after h, and l
after e, and this is the first one
If an error occurs from this, then everything is wrong.
Now these things are eventually called gradient vanishing issues
Because I lived in France, I speak French fluently.
If you think that French is predicted from this word prediction
You have to modify this value by this value
It doesn’t affect the front
The structure of the standard RNN module is simple.
How did you do this
Is it that LSTM has solved those problems?
Now, so here is a simple structure in the middle
Sigma here is sigmoid
The output of x, h comes out here, and there is a C line in the middle
Inside this is C
Cell. What's coming in here?
As I said earlier
Zi, Zf, Z, Zo
This goes into the vector
The Zf value forgets this
Multiplying this is 0
If this value is -10, this is 0
The value in Ct-1 disappears.
Anyway, it’s passed so that Z comes into the input like this
tangent and hyperbolic
Zi and what is this by the Zi gate value
It multiplies. This cannot be delivered
Anyway, if this is not there, it is added like this.
Ct occurs at Ct-1
Ct goes over and ht occurs. What is ht?
In Ht-1, the weight values of xt are determined. Simply
The value is determined by its hx
So now ht is created and ct is in the middle
The only difference is that there is one more
This is simply again called h
W_hh and W_xh by these two
You said that this value is difficult to converge
However, C here is the multiplication and addition caused by a kind of forget gate
There is no convergence for W here.
In other words, when this effect goes all the way back
What is the C line when going from front to back?
It’s just like a conveyor belt.
So this rate of change in the value can be affected all at once
Even if W is not well modified because of such a concept
Through C line, through memory
We have such problems
It comes out here, long term dependency problem
If you solve the problem of converging with about 1,000 time series
Of course, I truncate a little to solve it
Anyway, theoretically, the long term dependency problem
It’s simple to add and multiply here through the C line.
This is adding and subtracting this value.
Absolutely no weight decay occurs for this value.
So, LSTM solved the vanishing problem
The problem comes to the conclusion later, but it is
actually a difficult problem. LSTM and GRU are coming out
If I say the conclusion first
LSTM's vanishing problem is ok
LSTM couldn't solve the Exploding problem
In other words, when the W value was 1.01 and multiplied by a thousand times, it became very large
So, after converging like this, the W value suddenly went to a different plane.
So, the vanishing here is
It is solved in this line
I can't solve the exploiting problem that occurred here.
Solve what's underneath, how can I do something smaller
I can’t solve the problem
The method called GRU
We solved exploding, we solved vanishing
Actually, this is better
It’s excellent
So LSTM is a basic structure, I simplified it
It improves performance.
I will explain later
Two keys of LSTM
In going from t-1 to t
There are two gates here
This is the forget gate. Cell state works like a conveyor belt
It works as a straight down on the entire chain.
So information flows without any change
You can go straight from the back to the front along this side
(text)
In this state, whether to make memory or not
In the past, the information that I have stored at most is lost here.
Then all the past things disappear anymore.
It's currently being reset again.
Reset Reset
The word forget gate is used in LSTM
The word "reset" is used in GRU
I'll do this again
Because Forget Gate was explained earlier
The forget gate is based on these two values
Put two values and use the sigmoid function
If you make this to 0
This disappears
(text)
This is the problem of h -> e
In this problem, cell state can express gender
Cell status can distinguish gender
(text)
For example, in the previous sentence, if a person named Jane appears
Jane is a woman. Jane would be the pronoun she
But from here, suddenly John appeared in the X input
Then he will be from this point on. indeed?
From here, the memory corresponding to the gender called she is erased
You can easily solve this problem of changing she to he
When that happens, when generating text
We have to learn
Even Jane, when a woman comes out, from then on she will be
Match it and the man John says he again
If this is wrong, the sentence is messed up.
Now, so this problem can be easily solved by using the forget gate
In general, it cannot be used on an RNN.
We can’t implement that way
So when we go to a new subject
I want to get rid of gender in these old subjects
By this, you can put it in the x input and remove it.
The input gate is this second one
The input gate it is determined by the combination of these two
Binarization this value to 1 and 0
We are in logistic regression
Wi and ht-1 is a sigmoid function with a bias to the weight of xt.
This is what we solve
It’s not like there’s no weight here, right?
Then there is a weight called Wc
Make a second gate, and whether you are going to pass it or not?
What is an example?
Tanh is a value that exists between -1 and 1.
Here we need to update the Wc
Now, so this is two weights between ht-1 and xt
You can say that there is only one RNN
But here are two, I modified them
and added a value to the input gate
Add it to the cell of the past
Output, this not only changes the cell state
It comes after like this, and this value is connected to the output gate
(text)
Are you going to put a new input in the cell state?
The first layer is a sigmoid layer
called the input gate layer and this
(text) city
Ct value is the value added to Ct-1
This value is the tangent hyperbolic function weighted by this.
In the next step, these two make an update
Now, the next cell state update
Now, these two gate values are multiplied.
This value is between 0 and 1. It doesn't have to be completely 0 and 1
What is this one? weighted summation
In fact, as it is updated, new ones enter
This concept is the concept of GRU's gated recurrent unit,
If this is completely 0,1, remove it
You put all this in
Now it can come out like 0.2, 0.5
So what percentage here is 20% for
Ct-1 and 50% for the new incoming input.
Update Ct through weighted summation
(text)
Delete and drop the previous information on
gender, She or He, and add new information.
Like we did before
So this is completely divided into two states with 0 and 1
Of course, there is a high probability of 0 and 1. It's not easy because it has to be centered to be 0.5
Most of them are 0 or 1
I can't delete both.
Both could be 0
If you think about it, the forget gate is also activated, no input is required
In the case of john, he is correct
If someone other than Jane comes in, you have to leave he alone.
You don’t need to input it
The output gate is as I said now
Ct made it here, right?
And output gate
The output gate takes the sigmoid function
and the input generated at ht-1 by weight.
Because Xt has zo here
is determined by the zo
So make it like this
What was in Ct here?
Ht component in Ct is reflected as much as tanh and comes out
ht reflects Ct
ht-1 has already been reflected here
This value is added back to the previous state and reflected as hyperbolic tangent
This is what keeps going like this
It's the same as RNN. The RNN can be thought of as updating h
Now, this is a summary. I don't know if I need summary
First of all, the gate is the forget gate, the input gate, and the output gate
Input and update, cell state change,
output gate ht, here yt is obtained
This way we call LSTM
Now it is called GRU called gated recurrent unit
This is a Korean method made by Dr. Kyung Hwan Cho
It is very similar to LSTM, but the gate is not a
three gate, but two gates, so there are few parameters.
Look at the parameters above. There are 1, 2, 3 4, 4 parameters
There is a sigmoid function
There are 3 biases. Wf, Wi, Wc, Wo
Earlier on the RNN, there were three types of Why, Wxh and Whh
It was divided into four. Let’s see it’s two gate
Now, here's the update gate and
The word reset gate exists
Now look here. The same goes in here
ht-1 and xt went in. If you see here at
the beginning, xt makes something called rt
rt is the reset gate
How to combine new input and previous memory
Now, rt is made like this through a parameter called Wr
Now that’s it
Then the second is how much zt is going to be the update gate
This zt is made like this using two values ht-1 and xt
After doing this, it becomes 1-zt
This becomes zt
Multiply (1-zt) by ht-1
There is no update called Ct here
Instead of the word cell state, it has been replaced by the usual ht.
Cells are included in xt and ht, right?
So how much memory are you going to erase here?
What if zt equals 1? The past becomes zero
But the two are complementary to each other. To be 1
I entered something earlier, but what is ht in the past
This is zt and what is the value in the input?
By multiplying rt ♪ ht-1 determined by this,
the value is lowered like this and added to xt
This value becomes tanh and becomes ht hat
This value is eventually contained in the rt component
reset gate. If this value is 0,
ht-1 is not reflected in ht hat
That concept goes in here. Do a reset
gate like this in a kind of recursive way
I updated these two later and modified them like this
It is complicated with this modification, but the gate is also reduced
Of course, how many parameters have you reduced to now? 1,2,3 decreased from four to three
Earlier in the four Wi, Wc, Wf, Wo
It was reduced to three like this
Wz, Wr How about this?
Because this is a sigmoid function
It's not that serious
The most important thing is W
What does this mean
Because of the value of W
The gradient is vanishing
Gradient also said that exploding, right?
But the probability here is greatly reduced
Even if the W value becomes 10,000, by this tanh function
There is a problem we have with RNN. Look ahead on the matter
So this one solved the gradient vanishing
problem and the gradient exploding problem
We solved it by simply applying a function like this to the update
Instead of printing x to the output of this function
By updating the gradient here and memory it with this value
Because it keeps moving
This updates rt while updating W.
This is the consequential story
This is difficult to explain theoretically
When this internally occurring W value goes
back and forth between plus 1 and minus 1
GRU has solved the convergence problem that occurs so that it is not so serious.
Please remember this term
What did the update gate use instead of the word update?
Input gate. There were three
output gates and forget gates
These two are new ones. I made it like this
If you compare, look at this, you will see
The weight is 1,2,3,4 like this
In this case, the W decreased by 1, 2, 3
Then there were 1,2,3, and three gates
This one had two gates
Two gate. r and z are two
Zt and rt are these two gates
This is supposed to be multiplied with a 1-zt solution
It's just supposed to multiply zt
In a way, the RNN has problems, so it can be quite popular.
When you program with LSTM, RNN coded like this
But if you go inside, LSTM is the placenta.
It means using LSTM without using RNN.
That's why I keep that in mind
It hasn't been a few years since GRU was released.
Because it came out around 2014. Not correct, look for it
It is used a lot now now
Because the comparison is done
LSTM can handle gradient vanishing problems
well. Except for gradient exploding problem
Memory and input were added
(text)
The effect does not disappear until the forget gate is closed.
No gradient vanishing if forget gate opened
Gate recurrent unit is simpler than LSTM
Simple but much better function
zt controller control input and forget gate.
Controls input and forget gates at once
Control ht-1 using reset gate instead of no output gate
The detailed reason for prevent gradient exploding
was briefly explained earlier, but it is difficult
I hope you guys browse the internet once
Structure W does not continue to propagate
Because this becomes the basic concept, I simply understand that it is.
This is the theoretical one, let’s look at an example.
But actually, it’s good to look at the RNN code
I leave it to you now
Now, first looking at text generation
Here is the code
I simply copied it from this code
Take the data tinyshakespeare,
batch size, seq length,
hidden size, learning
rate, number of epoch
And read this
Read data, TensorFlow and target data will come out
And Softmax w, b
So if you convert it to Softmax
There is more in the back, I didn’t bring it
Now, what would come out, I put Shakespeare text in the input
I print it out at the training stage
Very strange words just come out
Little by little, the correct words come out
It means that as you meet little by little, you get better and better.
Finally this sentence is obtained
It was the training stage earlier, and
this is a small clue in the test stage.
From then on, these things
As in the Shakespeare comedy, of course, if you read
it closely, there are still many wrong words.
However, you get a sentence that is pretty similar and feels like a typo
It’s a tremendous development.
RNN is writing a novel.
Can write a novel
But this is more surprising
Putting in C code or something to learn
Then you make something like this by yourself
Put an indent and even make a comment so that there is no mistake
It is because I have already seen it. This, then this, etc. Like I saw hello
If you put this code in the learning stage
It's a story of self-generation. Try it
After that, it also creates the Linux
source code. Linux has copyright
Linux says what to do if you violate certain constraints, etc.
Copy all of them the same
The include statement is also properly written like this. up to the back slash
Try it out so you wonder if these things are true
This can be a project assignment
Then what did you say is the representative of many-to-one?
Putting in a sentence and judging it
input is a vector sequence. output is one vector
I got felt too bad
Then, of course, it can be positive or negative.
Or you can write super, good,
normal, bad, worst as one-hot vector
You just have to tell them at the learning stage.
I have written all the sentences
This is called super so that you can learn
If you put a lot of those things, now when you test them later
When you put in a sentence, you will get a review.
Even if you don’t read how people felt
The computer does it for you.
After that, the many-to-many input is a sentence
Output refers to the form of translating and printing English into French.
This is Translation
For example, when you speak French in the input, you
analyze each one and translate it into English and align it.
You just need to be in the right order
English and French match, so the word order is the same
It’s different if you match Korean and English.
The order should be filtered again through translation
Because you can do that through another method
That means we do this through RNN
This is an image captioning done by Professor Fei
It is many-to-many, but this picture can also be
done one-to-many, but you can see it as many-to-many
When you see this, the input is going straight into this side
straw hat
The hat and the crowd do this, but the problem is I don’t just put the image
I put it through CNN
You make a feature through CNN.
Captioning is performed by combining
convolutional and recurrent networks.
Now, something like this
Now a man is sitting on the lake or in the sea, undressing and wearing a hat.
This is caption
I put an image. Through the convolution network
This is a VGG network, but I removed the classifier at the end
I don't need a classifier so I Softmax and remove the FC-1000
And, the vector came out through FC4096.
It creates a vector that represents a representative property.
And put the created vector here
There are many V vector values from here
Multiply the real value by Wih
Then Whh ♪ h, Wxh ♪ x
I can actually put people in the image at start
I would find and put information that people put in
I put in the value from the start
This is the original value, this is added
Information on CNN
The image I put in is combined and input
Is that the output value is much better learned
It comes down like this and the input is straw
Again, at the second time
sequence, input hat and learn to go up.
Simply put out two straws and hats
Since we learn the image one more time using CNN
Generally speaking, it is excellent in the test stage.
The result comes out
This is a simple example
You'll get a lot more
A cat sitting on a suitcase on the floor
Detailed. Beyond what people wrote
This is the test result
A tennis player in action
Almost right. indeed?
Ti giraffes standing in a grassy field
A man riding a dirt bike on a dirt track
Image captioning failure cases
A woman is holding a cat in her field
I can’t see it because I have a Cat in my hand.
Weird
A woman standing on a
beach holding a surfboard
It’s completely different
A man in a baseball uniform
throwing a ball. Not so
It’s correct so far, but I’m not trying to throw it, I’m trying to catch it.
The same goes for the rest. There are many fail cases
It's difficult to have a fail case
Now, so in summary
Now the RNN application we were talking about at first
The reason why we say that RNN is the flower of neural networks
CNN is limited
Because it is limited to such things as
Classification, Detection, and Localization
Now with unsupervised learning to learn later
New things like GAN or autoencoder are coming out
Among them, it’s the two Mountains.
Not all of them like they are
here in Supervised Learning
Looking at what's important, we used machine translation,
text-to-speech, and text generation. Generation of novels.
Time series prediction. We put the past stock price for a year
Predicting the stock price tomorrow or next weekday
You can learn anything like this
By the way, it is difficult because there is another environment variable
Speech recognition. All time series
data changes in time are similar
Then music composition this is new
When humans make music, they put music
data into the Midi board and learn it all.
Then we create some new music
Those things can be obtained from Github as music
composition, and you can build the environment and run them.
Then handwriting recognition handwritten
human action recognition,
Then Anomaly detection,
Task prediction in business management
Medical care is an X-ray photo
You can do these things while watching videos on RNN.
This is a little special because it's not just RNNs now
So I finished explaining RNN, LSTM and GRU for a long time.
Thank you for listening for a long time, and I hope it helps
In addition, I hope you will have time to run the code.
You worked hard today. I'll finish