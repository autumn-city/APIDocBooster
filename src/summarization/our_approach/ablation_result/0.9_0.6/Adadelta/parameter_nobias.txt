If you really want to use Adadelta, use the parameters from the paper: learning_rate=1., rho=0.95, epsilon=1e-6. 
To match the exact form in the original paper use 1.0. 
and then add its instance to the callbacks when calling model.fit() like: 
On the other hand, rho parameter, which is nonzero by default, doesnâ€™t describe the decay of the learning rate, but corresponds to the fraction of gradient to keep at each time step (according to the [Keras documentation (hyper-link)]). 
Although as you can see in tensorflow [source code (hyper-link)] to achieve the exact results of Adadelta paper you should set it to 1.0: 
