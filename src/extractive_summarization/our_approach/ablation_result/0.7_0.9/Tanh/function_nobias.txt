 This Tanh is a hyperbolic tangent itself, but in the world of Neural Network, Tanh converts the input value into a non-linear one and keeps it in the range of -1~1.  It is used to serve as an activation function. 
 As you can see, when a large positive value is input, the output value is stuck to 1, and when a large negative value is input, the output value is stuck to -1.  And when the value in this area is input, the output changes almost linearly. 
In this way, it can be shown that a combination of such functions can approximate any non-linear function. 
On the other hand, to overcome the vanishing gradient problem, we need a function whose second derivative can sustain for a long range before going to zero.  Tanh is a good function with the above property.
Advantages: 
