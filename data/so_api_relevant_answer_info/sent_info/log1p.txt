Using that log() implementation as a building block for the two log1p() variants, I found the maximum error of Goldberg's version to be < 2.5 ulps, while the maximum error in the GSL variant was < 1.5 ulps.
However, note that this is not the official source of log1p in MATLAB, but rather it is someone else's empirical solution intended to mimic the behavior of log1p.
since log1p(x) = Math.log(x+1), finding a natural log fast algorithm is sufficient for what you need.
Answer from user docendo discimus: "It's because log1p(6e5) = 13.30469 and you set the binwidth to 10000.
You may need to use the std prefix: [std::log1p() (hyper-link)] instead of log1p().
Y = np.log1p(X) just calculates Y = log(1+X)
You can use [numpy.expm1() (hyper-link)] which is the inverse of [numpy.log1p() (hyper-link)]
The function np.log1p1 contains a default where=True parameter that seems to function slightly differently now, compared to the 1.15 version.
@Raymond Chen is spot on: "Negation of floating point numbers is exact, so log1p(-x) is as accurate as log1p(x)."
Actually, the CUDA library defines the log1p function, and so was an obscure part of the code I'm trying to add CUDA to.
log1p(x) does not compute the logarithm of x and should not be used if you want a logarithmic scale.
when transforming data to log scale for charting purposes, is it more "correct" in some way to always transform using np.log1p than with np.log and does it break any common user expectations?
It is almost never correct to use np.log1p instead of np.log if your goal is to compute log().
Here's the same function with the y axis in log1p scale instead:
log1p could be what I want if the values are very close to 0, as it will have better numerical stability than log, right?
The functions log1p and log are just mathematical functions.
But log and log1p are simply mathematical functions, not algorithms for computing functions, and as such, forward and backward stability do not apply.
The importance of log1p is that the function log(1 + ) is well-conditioned near zero, and often turns up in numerical algorithms or algebraic rearrangements of other functions.
So if you find yourself in possession of a small real number , and you find yourself wanting to know what log(1 + ) is, then you should use np.log1p(x) to answer this question.
If you wrote np.log(1 + x) instead of np.log1p(x), then the subexpression 1 + x may commit a rounding error, giving 1 ⊕  = fl(1 + ) = (1 + )⋅(1 + ).
But if you already have a number , even if it is near zero, and find yourself wanting log(), then np.log(y) will give a good approximation to log(), and np.log1p(y) will give a terrible one (unless  is very large).
Could np.log1p ever be relevant in plotting data on a log scale?
samples$grv <- log1p(samples[,resvar]) or samples$grv <- log1p(samples[[resva]])
Change it to np.log1p(x).
Why did they apply the exp and log1p ?
The log1p is fit so that it can come close to approximating a gaussian (normal distribution) because most models make the normalcy assumption.
Firstly, you may want to look into the accuracy and speed of your log1pf function: these can vary a bit between libms (I've found the OS X math functions to be fast, the glibc ones to be slower but typically correctly rounded).
Since my question is about minimizing the error for the argument transformation which is incurred in addition to the error in log1pf() itself, the most straightforward approach to use for experimentation is to utilize a correctly rounded implementation of that logarithm function.
The issue seems to be that a float operand cannot convey enough information to log1pf() to produce more accurate results.
It's difficult to be certain because you haven't been sufficiently explicit about the model, but it looks like you need to simply drop the log1p on the left hand side of the formula; the Poisson glm already has a log link function by default (but you don't need to add 1, because it's the mean, not the data that is transformed to the scale of the linear predictor).
Only calculate log1p once
Verify boost::math::log1p(   ) prints out the expected result.
Unfortunately, mean(log1p(x)) == mean(log(1 + x)) is one way operation.
so having mean(log1p(...)) you can't get mean(...).
For a given mean(log1p()) there are infinitely many different corresponding means whenever the collection has more than one item.
But in Verilog, you would have to call a VPI wrapper function that called log1p from code that you would have to write in C.
You can always write a polynomial or rational or [table-driven (hyper-link)] approximation to log1p, or find a library where someone else already implemented it, of course.
expm1 is the MATLAB operation that performs the inverse of log1p.
The popular library Accord.Math indeed has a [log1p (hyper-link)] function which is [pretty simple (hyper-link)] and can be implemented through following one-liner:
log1p() is in fact included in the CRT library that the CLR uses but it is not exposed through the framework.
Edit: I had missed log1p.
Edit:
Per @tobiask's comment, you will need exp to be called like math.exp and log will need math.log unless log1p is being defined elsewhere
It should be 8*(exp(1-float))+7*(log1p(float)).
log1p gets you half way there, but what you need is a function that treats the error for both the log and the exp parts.
Read the manual page for log1p.
Your axis is on a log1p scale, so your xlim should be wrapped inside log1p to do a zoom.
If you transform the xlim using log1p, you can zoom on the corresponding values of the x-axis of plot g. You can do that as follows:
Your code works fine, once you indent it properly and add from math import log1p .
A workaround I've found is to manually implement a Log1PlusExp function with its backward counterpart.
One trivial approach is to use expm1 instead of log1p:
When you use the log1p transformation, you are also able to include a 0 in your breaks: scale_y_continuous(breaks=c(0,1,3,10,30,100,300,1000,3000), trans="log1p")
