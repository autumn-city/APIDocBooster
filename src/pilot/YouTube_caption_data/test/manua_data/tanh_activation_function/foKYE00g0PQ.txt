Hello World, it's Rahul. Welcome back guys
to Deep Learning tutorial. So guys today we
are going to discuss what is Activation function
& how can we use different activation functions
at different places. After completing this
tutorial, you will know that
first, Sigmoid and hyperbolic tangent activation
functions cannot be used in networks with
many layers due to the vanishing gradient
problem.
& second Leaky ReLU activation function overcomes
the vanishing gradient problem, allowing models
to learn faster and perform better.
So guys watch this video till end for understanding
this particular thing as I am going to explain
you about each & every aspects of Activation
function.
let's understand what is Activation Function?
So guys If you remember my last video on Neural
Network then we had a neuron as shown on the
screen & this particular neuron is getting
some of the input signals which is nothing
but the different features & these input signals
are multiplied by some weight matrix so that
each signal differs from one another. now
if i take summation of all incoming input
signal multiplied by weights & add a bias
term to this then we pass it to an activation
function & then move forward to next hidden
layer. so guys here we will not discuss about
bias because if i will go to bias then back-propagation
will come into picture, but you need not to
worry as i will come up with bias & weight
matrix in my upcoming videos. Okay, so once
we got this activation function activates
or deactivates neuron that's it. if neuron
is activated then output is provided to next
layer otherwise that particular neuron does
not participate for next layer. so When i
say Activation function activates the neuron
that means if weighted sum of incoming signal
is more than some specified threshold then
only it activates neuron.
So now we can say that Activation function
decides whether a neuron should be activated
or not. Whether the information that the neuron
is receiving is relevant for the given information
or should it be ignored.
So guys we have two types of Activation function
one is Linear & other is Non-Linear Activation
function. So we already know that Linear functions
are the equations which graph a straight line
in an XY plane so the output of the functions
will not be confined between any range. whereas
Non-Linear function means the graph is not
a straight line instead it is a curved line
or some range. So guys Non-Linear function
further divided into 6 parts i.e.
Threshold function
Sigmoid or logistic Function
hyperbolic tangent activation function also
known as Tanh then
Rectified Linear Unit known as ReLU
Leaky ReLU &
Softmax function
Let's discuss all of them one by one.
So let's start with Threshold activation function
so guys if value of y is greater than or equal
to 0 which is weighted sum then it will represent
it as 1 if y is lesser than 0 then it will
represent it as 0. so this type of activation
function is used where we have to just classify
element either as 1 or 0. so you can see graphical
representation of what i have just explained
you. here value above than 1 considered as
1 & value less than 0 as 0. Now why 0 & 1
is required or why is this activation function
is required? so let say accidentally i touch
a nail so neuron at my hand gets activated
due to that nail & that send a signal to brain
then only i will be able to understand that
i have touched nail which can hurt me & then
i can responds to that particular stimuli
& move my hand back. so for that you know
activation function & weights are important.
so if we get 1 then that particular neuron
will get activated otherwise it will deactivate.
Now let's move into Sigmoid Activation function.
so guys if you can see on the screen. graph
for sigmoid function looks like S-shaped.
so basically it reduces extreme values or
outliers in data without actually removing
them from data. so formula for Sigmoid is
shown on screen which is nothing but 1 divided
by 1 + e to the power minus y & here y is
summation of weights multiplied by the X +
bias. so here what sigmoid function does is
let this product be any value either negative
or positive, whenever we replace this in formula
it ranges the independent variables between
0 & 1. so here question is how can we assign
any value as either 0 or 1. so it's very simple
we decide some threshold over the graph. let
say i draw a threshold of 0.5 & whenever it
is greater than 0.5 output is considered as
1 & when it is less than 0.5 then output will
be considered as 0. so guys from this you
can understand that how it reduces the outliers
or extreme values. One more thing guys, we
call this function as Logistic function because
we are dealing with 0 & 1 value & we are finding
the output probabilities by the same mean
we were using during the time of logistic
regression. So if you don't have any intuition
about Logistic regression then please go & watch
my Logistic regression video, those who are
following my sessions definitely know that
I have already uploaded these videos into
my Machine learning tutorial playlist. You
can find out the link as well into description.
Alright, guys let's discuss hyperbolic tangent
activation function which is also known as
tanh activation function. Unlike the Sigmoid
function, the range for tanh function is -1
to 1. So it is similar to Sigmoid function,
where we were catering the output in a range
of 0 to 1 & setting up the threshold at 0.5.
so in tanh we extend the lower side of the
curve till negative 1 & our mid will be at
0. So guys by definition tanh function is
derived from the trigonometric function that's
why it's formula is written as tanh(x) = sinh(x)
divided by cosh(x).
A general problem with both the sigmoid and
tanh functions is that they saturate. This
means that large values snap to 1.0 and small
values snap to -1 or 0 for tanh and sigmoid
respectively.
So guys now coming to Rectified Linear Unit
or ReLU function as name suggest it acts like
a linear function, but is, in fact, a nonlinear
function allowing complex relationships in
the data to be learned. so just like we were
passing the weighted sum of input signals
+ bias in case of sigmoid function. if we
pass the same to ReLU function then according
to the formula used for ReLU & i.e. max(0,
y). so what does it means is if our y is weighted
sum of input + bias & this value is coming
as some positive value then the max of 0 & that
particular positive value would be that value
itself but what if our y is some negative
value then max of that negative value & zero
would be 0 only. so by same intuition we can
draw a graphical representation of ReLU function
as shown on the screen. so guys for y=0 or
any negative value our output will be transformed
to 0 but it increases with the same y value
if y is greater than 0. let say y is 2 then
output will be 2, if y is 4 then output will
be 4. So guys this is very powerful activation
function as this is used at most of the places
in deep learning. so guys we will understand
in upcoming sessions that where can we use
ReLU function or any other function. so just
to give you a heads, for most of the classification
problem we use sigmoid function at the last
layer because we want to classify the output
between 0 or 1 but if we are solving any regression
problem then mostly we use ReLU function let
it be in any hidden layer or output layer.
let's get back to graph again. so guys as
you can see for any negative value or 0 our
output is considering as 0 it means we are
not activating those neurons but for positive
value our neuron's strength is progressing
exactly in the same way what value of y we
are getting. let say if we are getting positive
infinite value for y then in that case our
output would be positive infinity only & for
negative value our output will be 0. so once
we have the output value with us we basically
back-propagate the derivative of output value
with respect to term x for adjusting the weight
matrix as I have told you guys in my last
video that we have to adjust weight matrix
for getting the correct output. I will not
go into deep of this back propagation because
this is really a great topic to explain. I
will upload a separate video on this topic.
here I am just giving you a basic intuition
so that you guys can understand the problem
we are facing when ReLU function is applying
to the weighted sum of input signals. let's
understand how can we back-propagate the output.
so if you can see on the screen this is how
our graph looks like for ReLU function. now
if i want to find the derivative of this line
the value will always be 1 because this line
is at an angle of 45 degree & we all know
that tan 45 degree is 1 so for this line I
can draw a straight line at 1. so for any
positive value derivative of our output would
always be 1 . now for negative side so you
can see that this is just a constant value
at 0th axis. so according to derivative rule,
derivative of constant value is 0. so for
any 0 or negative value derivative will be
marked at 0. so range of derivative of ReLU
function will be 0 for negative value & 1
for value greater than 0. guys now you must
have one doubt that how is this different
from Sigmoid function as Sigmoid was also
ranging between 0 & 1. so In Sigmoid the derivative
will always range between 0 to 0.5 & in tanh
it is always less than 1. now let's apply
derivative values in updation formula for
derivative. so we have this formula y old
= y new - learning rate into derivative of
loss with respect to derivative of w. let
say here we have 2 derivatives & it's a chain
rule. so we will consider both 1 & 0 scenario.
first let's put 1 for these derivative. so
learning rate will be multiplied by 1 & it
got subtracted from y new value. till now
everything is fine but what if we consider
derivative of output as 0, let's quickly see
that. so here put derivatives as 0 & when
we multiply this with learning rate that will
become 0 only. now if you will see y old is
equal to y new as remaining term becomes 0.
so it is acting like a dead neuron as we are
not updating anything & still we have activated
this neuron which is of no use. so this is
the main problem with ReLU activation function
which is known as Vanishing gradient. so guys
for fixing this problem we have one concept
& that is leaky ReLU function.
On the same note let's understand Leaky ReLU
function as this is an improvement to ReLU
function. now let see how can we fix our problem.
so here we will try to add some value to constant
side that means line will not be exactly 0
for negative number instead it will be some
addition to this value. so here if y is greater
than 0 then output would be y itself but if
y is less than 0 then instead of putting it
as 0 we will add some value to 0. let say
we take 0.01 & this will be multiplied by
x. now if we find the derivative of this term
i.e. 0.01 * y divided by derivative of w then
our output will be some 0.01 value but not
0 & now if we subtract this value from y new
then we will get some value which we can use
for updation. so by this way we have solved
dead neuron problem by just simply adding
some value to negative side.
so guys now we only have to discuss Softmax
activation function. let see what Softmax
activation function states so it is a mathematical
function that converts a vector of numbers
into a vector of probabilities, where the
probabilities of each value are proportional
to the relative scale of each value in the
vector. let say we have multi class classification
problem then we apply softmax function so
that it will give us relevance probabilities
for each class. let say we have 3 class A,
B & C now we have created neural network & we
have applied this activation function at last
layer for getting the output. so we will get
the output in terms of probability matrix
as shown on the screen. Class A with a probability
of 0.25, Class B with a probability of 0.12
& then Class C with a probability of 0.63.
so according to this result our prediction
would be Class C as it is showing the highest
relevance probability for this particular
Class. Now again range for this will be 0
to 1 only as we can see in the example what
i have just shared with you.
So guys this is all about Activation function
& its types. hope you guys understand each
segment of this video. Guys this is just basic
intuition of activation function. We will
see how can we use these function in upcoming
videos.
Thank u guys for your time. do like share
& subscribe. hit the bell icon to get the
latest updates.
Happy Learning & stay safe