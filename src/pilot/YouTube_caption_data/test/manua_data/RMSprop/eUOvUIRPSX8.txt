 Yes, in the previous video, we talked about adding this
 momentum term, which is essentially a velocity term that
 helps dampening the oscillations in stochastic gradient descent.
 But it also can help with overcoming like flat regions on
 the loss surface, for instance, on saddle points or local
 minima. So in addition to this momentum term, we learn now
 about a slightly related also, slightly different concept
 called adaptive learning rates. So adaptive learning rates are
 essentially about accelerating and accelerating the learning
 rate at the right moment. So speeding the learning up when we
 are going into the right direction and slowing it down
 when we change directions. And then we will see how we can
 combine this with momentum learning. And the combination of
 both there's an algorithm that does that it's called Adam. So
 Adam is essentially a combination of adaptive
 learning and momentum learning. Yeah, there are many different
 flavors of adaptive learning rates out there. And discussing
 them all would be a little bit of out of the scope of this
 course. However, um, yeah, just to go over the main concepts, I
 will show you a simple example. And yeah, the key takeaway, like
 I said in the previous slide is that we decrease the learning
 rate of the gradient changes the direction, and we increase the
 learning rate if the gradient stays consistent. So for
 instance, if we do these updates, and they are all going
 to the same, roughly same direction, then we accelerate
 because yeah, in this way, it's probably likely the correct
 direction. So in that way, we can just speed it up and
 converge faster. However, if we have something where we change
 the direction, for example, so let's say we have an update like
 that, and there's another change, then it will slow down
 the update, so that it is, if it's a wrong direction, there's
 maybe some noise, so that it is not heading too much into this
 wrong direction, for example. So if there every time there's a
 change in direction, it will essentially slow down.
 Yeah, so how does it work? How can we use an adaptive learning
 rate? There are essentially two main steps for doing that. So
 step one would be initializing a so called local gain for each
 weight. And if you look at this, here, when we compute the delta
 w ij term, and if you only if you remove this one, this would
 essentially look like the regular delta term that we
 compute. So this is essentially our gradient of the loss with
 respect to that weight that we want to update times the
 learning rate. So this is the learning rate. But the difference
 now is that we add this gain and this gain, as you can see is
 also index ij. So it's the gain associated with that particular
 weight. And in that way, you can think of this one as a learning
 rate just for this particular weight. So you can have unique
 learning rates for different weights or different directions
 in that way. So now after initializing this local gain, we
 have a second step. So this is what we then do during learning,
 we are modifying this gain. So similar to modifying the weights,
 we modify this gain. So we either increase it if it's
 consistent, or otherwise if it's inconsistent. So if we change
 the direction, then we decrease or Yeah, we Yeah, we change the
 dampen the gain essentially. So like I'm saying here, multiplying
 by a factor has a larger impact if the gains are large, compared
 to adding a term and addition to in contrast to this addition.
 And that means, um, so how this is set up, it means that if we
 are going into the right direction, we are slowly
 accelerating. So, so this is if we are consistent, and we are
 adding this beta term, and we make it bigger by a small value
 here by beta, let's say, beta is point nine, we increase it by
 point nine in each round, essentially. But if we change
 the direction, we slow down faster by multiplying by this
 term here, for instance. So here, for instance, we would
 multiply a beta is point nine, we will multiply this by point
 one. And this can have a stronger dampening effect when
 we change the directions. This is kind of intuitive, because
 yeah, if we let's say, drive a car, and we see the road is
 clear ahead, we start to accelerate, but we don't go
 crazy and stomp on the gas, we start accelerating in a
 reasonable manner. And let's say we want to make a turn or
 something like that, or we are in the turn. Usually, we
 accelerate a lot before a turn, we don't slowly accelerate in
 most cases, at least I mean, if it's a very sudden turn, if we
 have to avoid an obstacle or something like that, we have to
 push the brakes pretty hard to turn left or right to avoid the
 obstacle.
 Alright, so there is a particular implementation of an
 adaptive learning rate that was very popular for, yeah, for a
 time. It is called RMS prop. And I think it's from a course
 Geoff Hinton taught, you can find references to that on the
 internet. But there is no official paper of that or
 corresponding to that. It's something Geoff Hinton was one
 of the persons popularizing and working on your networks
 throughout the last 60 or so years, 50 years. Yeah, it has
 been mentioned in one of his talks, and people started using
 that and performed pretty well. It's essentially a modification
 of our prop, which was published in 2000 around 2000. And yeah,
 it's also relatively similar to another concept called delta.
 So the main idea behind this RMS prop, I mentioning it because
 it will be relevant when we talk about the Adam algorithm. So the
 main concept is we divide the learning rate by an
 exponentially decreasing moving average of the squared
 gradients. So it's essentially a modified version of this
 adaptive learning rate that I showed you on the previous
 slide. So the previous slide was a very simple type of adaptive
 learning rate, this is a little bit more sophisticated. Because
 it takes us into account that gradients can vary widely in
 the magnitude. So some weights or gradients are larger than
 others. And the RMS and RMS prop stands for root mean squared,
 because it's related to these squared gradients. And it also
 has an effect of this dampening effect. In addition to the
 adaptive learning, it also has this adept dampening effect of
 the momentum on the oscillations. But in practice,
 yeah, people thought it might work better than using
 momentum. So this RMS prop, people found works better than
 just momentum. But yeah, nowadays, people use a
 combination of Adam, or the people use Adam, which is a
 combination of RMS prop and momentum. Okay, but let's talk
 about this RMS prop first, before we go into the Adam, Adam
 algorithm. So there is now a mean square term here that is
 the moving average of the squared gradient of each weight.
 It's kind of, in a way similar to momentum, but we have now,
 instead of just considering the moving average, we have the
 moving average of these squared gradients. There's a beta term.
 So t is again, the time step. For instance, the iteration, and
 w ij is the weight we want to update. So we have the mean
 squared gradient of that weight at a given time step. And this
 is computed by multiplying a beta with the mean square value
 from the previous iteration. So and then, on top of that, we add
 these squared gradients for the current iteration. So this is
 the mean term here, right? So this is something we keep from
 from the previous rounds. And this is only for the current
 round for time step t. So it's essentially the gradient of the
 loss of partial derivative of the loss with respect to the
 weight should be a partial here. And we square those essentially,
 so they are always positive values. Yeah, and then this is
 how we compute the mean square value. And then we use that mean
 square value to scale the gradients at that update. So
 here, we are just computing the mean squared term, and here we
 are applying it. How do we apply this? So if you look at this,
 again, this is like the regular gradient descent update, right,
 we take a step into the direction, the negative
 direction of the gradient. So we subtract the gradient times the
 learning rate. But now, in addition to doing that, we scale
 by this mean squared term, and we take the square root, so it
 has the same unit as the weights. So and this is
 essentially it, it's very similar to what I showed you
 before, the adaptive learning rate where we have a gain that
 we modify, except that this is a certain type of gain that is
 working a little bit differently. And yeah, we have
 the small epsilon to avoid division by zero errors. In a
 way, it's also somewhat similar to the momentum term. So when we
 look at Adam, we will see there are two things the momentum term
 and this term, and they are actually also themselves very
 similar. Except this is like a scaling factor, in a way, and
 the momentum is something that we add on top. So let's then
 now talk about this, Adam, because I mentioned it so many
 times, it stands for adaptive moment estimation. Again, it's
 probably the most widely used optimization algorithm in deep
 learning as of today, I use it a lot, because I find it just
 works very well out of the box. And I always find I get almost
 always the same, or even better performance as with SGD, plus
 momentum and learning rate schedulers, but I need way less
 tuning. I mean, here, I mean, tuning of the learning rate and
 the strength of the scheduler. So like I said, it's a
 combination of momentum and RMS prop. And to get to make this a
 little bit easier to read. Um, I was also should say, if you
 want to read the original paper, which contains way more detail,
 this is the paper where it was proposed from 2014. So here, you
 can find also more information. So here, I'm rewriting this
 slightly to make this a little bit, I would say easier to see
 how it is related to momentum. So here, on top, this is the
 original definition of the momentum that we defined in the
 last video. And I'm just replacing this delta w by this
 MT. So I'm just using a different notation here, I'm
 just changing the notation, not changing any concept. I'm just
 saying, okay, let's call this one now, m at time step t. So
 the momentum term, m for momentum at time step t. And
 here, the, this is the current time step. And this is the time
 step for the next round, essentially. So here, this is
 the rewritten version of that. But you can see, it's slightly
 different now, because we have this modified version where we
 have alpha, but instead of using the learning rate here, we use
 one minus alpha. So it's not exactly like the same, like the
 original momentum term, it's slightly different. So we have
 this one minus alpha, instead of the data in the original
 momentum, but you can see how similar it is, right. So we have
 we have both on alpha times m, which we have here. And we have plus here, we have the gradient, the same thing, except again, the only
 differences here, we have the learning rate. And here we have
 one minus alpha. Okay, so here, I was just rewriting the momentum
 term from the previous slide, exactly the same thing, just
 carrying it over. And now we have this RMS prop term that I
 showed you a few slides ago. So here we had the better term for
 multiplying it with a mean square, I'm just abbreviating
 here as R. And then we have one minus beta times the squared
 gradients. So this is essentially the same I also
 showed you before, now I'm just using our so it's shorter. And
 then for Adam, we combine both, we have momentum. So we have
 the learning rate times the momentum term, it's essentially
 our velocity. And we scale that momentum term by this arm as
 proper term. So square root r plus this epsilon. And this is
 essentially how Adam works. So we have the momentum term and
 here, the arm is prop term. Okay, um, there's a slide, I
 would say modification of that in the paper. So here, that's
 the original paper, how it is defined in the paper. And what I
 haven't contained or included in the slides for simplicity was
 this bias corrected, first moment estimate, and the bias
 corrected second raw moment estimate. So here, the
 difference is essentially that I should also say, I'm calling it
 alpha and beta. So here, they have beta one and beta two. So
 if we go back, I call that alpha, just because I was, I
 don't want it simpler. But this is essentially beta one. And
 here, this is beta two. In the paper, I just found it somehow
 more intuitive to say alpha and beta, because we also used alpha
 before when we talked about momentum. So yeah, they just
 correct this term by having another scaling by so they scale
 the momentum term by dividing by one minus beta to the power of
 t, where t is the time step essentially, and they do the
 same thing for on this V term. Yeah, and this is essentially it.
 So in the next video, I will show you how we can use these
 optimization algorithms in pytorch.