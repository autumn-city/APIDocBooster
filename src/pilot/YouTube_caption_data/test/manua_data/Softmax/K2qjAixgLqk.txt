Welcome to an introduction to advanced actor
critic methods. I am your instructor, Dr.
Phil Taber. I'm a physicist and former semiconductor
engineer turn data scientist. In this course,
you're gonna learn the fundamentals of advanced
actor critic methods. Now, if you've seen
some of my prior work here on the Free Code
Camp, then you may have seen some work related
to deep learning as well as actor critic methods,
there will be a little bit of overlap between
the other actor critic courses and this material,
simply because I can't assume everyone has
seen my earlier content, no need to go back
and re watch that though. Although you are
free to do so if you wish, I will include
enough information in this particular course,
for the motivated beginner to get started
in the field of deep reinforcement learning
and actor critic methods in particular. Now,
why are actor critic methods important? That's
a great question I'm glad you asked. The basic
idea is that things like cue learning are
great for learning problems with a discrete
action sets like playing video games where
you can move left or right or shoot your phaser
blast or the invading aliens. But it falls
down when attempting to handle things like
continuous actions. So this is important in
fields like robotics, where you are applying
continuous voltages to motors enjoins to actuate
movement. And so we do technology to handle
robotic movement above and beyond deep learning.
Now, far from being a theoretical exercise
in March of 2021. This year, group at Berkeley
did, in fact, use deep reinforcement learning
to get to bipedal movement in a robot named
Cassie, and I'm going to detail that on my
channel. By the time you see this, it may
already be out. So go ahead, check me out
at machine learning with Phil, I'll leave
a link in the description, you can go subscribe
if you're interested in more deep reinforcement
learning content, but enough of the shameless
plugging. So actor critic methods are necessary
for dealing with continuous action spaces.
And they work by approximating something called
a policy. A policy is a mathematical function
that takes a state of the environment as input,
and outputs some action. Now, in the case
of a robot, that action could be a just an
actual voltage that we apply it to our motors,
or it could be a probability distribution
that we sample to generate some action. So
for example, Gaussian distribution, you know,
a normal bell curve that you sample to get
some value for your action. We're going to
cover both cases. In this course, we're going
to cover a whole host of algorithms, starting
with the vanilla actor, critic method, deep
deterministic policy gradients, twin delay
deep deterministic policy gradients, proximal
policy optimization, soft actor, critic, as
well as a synchronous advantage, actor critic,
try saying that five times fast, as far as
software requirements go, they are relatively
light. So we will need NumPy pie torch and
Matt plot line, I highly recommend using the
versions that I will leave for you in the
description, because NumPy in particular,
likes to deprecate stuff. And that tends to
break my code. So if you use the versions
that I've linked in the description, it's
almost guaranteed to work provided we didn't
make some mistakes along the way. As far as
hardware, you're going to need a GPU unfortunately,
and 2021 the GPU market is totally broken.
So hopefully you already have one on hand.
If not, don't despair. In particular, the
a three c algorithm, a synchronous advantage
actor critic is designed to run a multi core
CPUs. So at the very least, you'll be able
to run that and get really good results. Other
algorithms, you may be able to get something
that converges all of the timeline, the amount
of time you're gonna have to train is going
to be a little bit longer. So you may want
to leave things running overnight. With all
of that said, I will check in periodically
for questions. Obviously, I don't get notifications
from the Free Code Camp channel, but I'll
do my best to patrol the comments to see should
there be any confusions or need for clarification
I can swoop in to render assistance. Once
again, if you like this type of content, check
me out at my YouTube channel machine learning
with Phil where I go over all things, deep
reinforcement learning, and occasionally natural
language processing as well. Let's go ahead
and get started. And I look forward to seeing
you on the inside. Welcome back everybody.
In today's tutorial, you are going to get
a mini crash course in actor critic methods
in TensorFlow two, we're gonna have around
15 minutes of lecture followed by about 20
minutes of coding, and you're gonna learn
everything you need to know to go from start
to finish with actor critic methods. If you'd
like to know more, you can always pick up
one of my Udemy courses on sale right now
link in the description. Let's get started.
Welcome to the crash course in actor critic
methods, I'm going to give a relatively quick
overview of the fundamentals of reinforcement
learning in general. And then of actor critic
methods in particular, finally, work together
to code up our own actor, critic agent and
TensorFlow two. This is geared toward beginners,
so feel free to ask some questions in the
comment section. Reinforcement Learning deals
with agents acting on an environment, causing
some change in that environment and receiving
a reward in the process. That All of our agent
is to maximize his total reward over time,
even if it starts out knowing literally nothing
about its environment. Fortunately, we have
some really useful mathematics at our disposal,
which makes figuring out how to beat the environment
it difficult, yet solvable problem. The mathematics
we're going to use relies on a very simple
property of the system, the Markov property.
When a system depends only on its previous
state, and the last action of the agent, we
say it is markovian. As we said earlier, the
agent is given some reward for its action.
So the set of states the agencies, the actions
it takes, and the rewards it receives forms
our Markov decision process. Let's take a
look at each of these components. In turn,
the states are just some convenient representation
for the environment. So if we're talking about
an agent trying to navigate a maze, the state
is just the position of the agent within that
maze. The state can be more abstract, like
in the case of the lunar lander, where the
state has an array of continuous numbers that
describe the position of the lander, the velocity
of the lander, its angle and angular velocity,
as well as which legs are in contact with
the ground. The main idea is that the state
describes exactly what about the environment
is changing. And each time step. The rules
that govern how the states change are called
the dynamics of the environment, the actions
are a little simpler to understand. In the
case of a maze running robot, the actions
would be just move up, down, left and right.
Pretty straightforward. In the case of a lunar
lander, the action is consist of doing nothing,
firing the main engine, firing the left engine
and firing the right engine. In both these
cases, the actions are discrete, meaning they're
either one or the other. You can't simultaneously
not fire the engine and fire the right thruster.
For instance, this doesn't have to be the
case, though. actions can in fact, be continuous.
And there are numerous videos on this channel
dealing with continuous action spaces. Check
out my videos on soft actor critic deep deterministic
policy gradients, and twin delayed deep deterministic
policy gradients. From the agents perspective,
it seeing some set of states and trying to
decide what to do. How is our agent to decide?
The answer is something called the agents
policy, a policy of the mathematical function
that takes states as inputs and returns probabilities
as output. In particular, the policy assigns
some probability to each action in the action
space for each state. It can be deterministic,
meaning the probability of selecting one of
the actions is one and the others is zero.
But in general, the probabilities will be
somewhere between zero and one. The policy
is typically denoted by the Greek letter pi.
And learning to be the environment is then
a matter of finding the policy pi that maximizes
the total return over time by increasing the
chances of selecting the best actions and
reducing the chances of selecting the wrong
ones. The reward tells the agent exactly what
is expected of it. These rewards can be either
positive or negative. And the design of rewards
is actually a tricky issue. Let's take the
maze running robot. If we give it a positive
reward for exiting the maze, and no other
reward, what is the consequence of that? The
consequence is that the agent has no motivation
to solve the maze quickly, it gets the same
reward if it takes a minimum number of steps.
Or if it takes 100 times at number, we typically
want to solve the maze as quickly as possible.
So the simple reward scheme fails. In this
example, we have to give a penalty or a reward
of minus one for each step and a reward of
zero for exiting the maze, then the agent
has a strong motivation to solve the maze
in as few steps as possible. This is because
the agent be trying to maximize the negative
reward, meaning get it as close to zero as
possible. So now that we have our basic definitions
out of the way, we can start to think through
the mathematics of the reinforcement learning
problem. From the agents perspective, it has
no idea how its actions affect the environment.
So we have to use a probability distribution
to describe the dynamics is probability distribution
is denoted p of s prime and are given s and
a was just raised as the probability of ending
up in state S prime and receiving a reward
are given we're in state s and take action
a. In general, we won't know the value for
this until we interact with the environment.
And that's really part of solving reinforcement
learning. Since we're dealing with probabilities,
we have to start thinking in terms of expectation
values. In general, the expectation value
is calculated by taking into account all possible
outcomes and multiplying the probability of
that outcome by what you receive in that outcome.
So in our Markov framework, the expected reward
for a state and action pair is given by the
expectation value of that reward, which is
the sum over all possible rewards outcomes
also by by the sum over the probabilities
of ending up in all possible resulting states.
For a simple example, let's consider a simple
coin toss game. If we flip a coin and it comes
up heads, you get one point. If it comes up
tails, you get minus one point. If we flip
the coin two times, what is the expected number
of points? It's probably of getting heads
multiplied by the reward for getting heads,
plus a probability of getting tails multiplied
by the reward for getting tails. So 0.5 times
one plus 0.5 times negative one. This gives
an expected reward of zero points, which is
what you would intuitively expect. This is
a trivial example, but I hope it illustrates
the point, we have to consider all possible
outcomes, their probabilities and what we
would expect to receive in each of those outcomes.
When we go to put theory and practice, we're
going to be doing it in systems that are what
we call episodic. This means that the agent
has some starting state, and there is some
terminal state that causes the gameplay to
end. Of course, we can start over again with
a new episode. But the agent takes no actions
in this terminal step and thus no future rewards
follow. In this case, we're dealing with not
just individual rewards. But with the sequence
of rewards over the course of that episode.
We call the cumulative reward the return and
it's usually denoted by the letter G. Now
this discussion is for games that are broken
into episodes. But it would be nice if we
could use the same mathematics for tasks that
don't have a natural end, we have a game that
goes on and on, and the total sum of rewards
will approach infinity. It's absurd to talk
about maximizing total rewards an environment
where you can expect an infinite reward. So
we have to do a little modification to our
expression for the returns, we need to introduce
the concept of discounting, we're going to
reduce the contribution of each reward to
the sum based on how far away in time it is,
from our current time step, we're going to
use a power law to describe this reduction
so that the reward gets reduced by some additional
power of a new hyper parameter we'll denote
as gamma, gamma is between zero and one. So
each time we increase the power, the contribution
is reduced. If we introduce gamma into the
expression for the return at time step t,
we get to the return is just a sum over k
of gamma to the K and multiplied by the rewards
at time t plus one plus K. Besides being a
trick to make sure we can use the same mathematics
for episodic and continuing tasks is counting
as a reasonable basis and first principles.
Since our state transitions are defined in
terms of unknown probabilities, we can't really
say how certain each of those rewards were
states that we encounter further out in time
become less and less certain. And so the rewards
for reaching those states are also less and
less certain. And less, we shouldn't wait
them as much as the reward we just received.
If you've been following along carefully,
something may not quite add up here. All this
math is for systems with a Markov property,
which means that they depend only on the previous
state and action. So why do we want to keep
track of the entire history of rewards received?
Well, it turns out that we don't have to,
if you do some factoring in the expression
with a return a time step t, you find that
the return at time t is just the sum of the
reward at time t plus one and the discounted
return for the T plus one time step. This
is a recursive relationship between returns
at each time step. It's more consistent with
the principles of the Markov decision process,
where we're just concerned with successive
time steps. Now that we know exactly what
the agent wants to maximize the total returns,
and then function for how it's going to act,
the policy, we can actually start to make
useful mathematical predictions. One quantity
a particular interest is called the value
function depends on the agents policy pi and
the current state of the environment, and
gives us the expectation value of the agents
returns starting from time t and state s,
assuming it follows the policy pi as a comparable
function for the value of state and action
pairs, which tells us the value of taking
action a in state s and then following the
policy pi afterwards, it's called the action
value function and is represented by the letter
Q. So how are these values calculated in practice?
Well, in reality, we don't solve these equations,
we estimate them, we can use neural networks
to approximate the value or action value function.
Because neural networks are universal function.
approximator is the sample rewards from the
environment and use us to update the weights
of our network to improve our estimate for
the value or action value function. estimating
the value function is important because it
tells us the value of the current state and
the value of any other state the agent may
encounter. Solving the reinforcement learning
problem then becomes an issue of constructing
a policy that allows the agent to seek out
the most profitable states. The policy that
yields the best value function for all states
in the state space is called the optimal policy.
In reality, it can be a set of policies, and
they're all effectively equivalent. various
schemes to find these optimal policies exist,
and one such scheme is called the actor critic
method. In actor critic methods, we're using
two deep neural networks. One of them is used
to approximate the agents policy directly,
which we can do because it's just a mathematical
function. We call that the policy is just
a probability distribution over the set of
actions where we take a state as input and
output a probability of selecting each action.
The other network called the critic is used
to approximate the value function. The Critic
acts just like any other critic telling the
actor how good each action is based on whether
or not the resulting state is valuable. The
two networks work together to find out how
best to act in the environment. The actor
slugs actions, the critic evaluates the states
and then the result as compared to the rewards
from the environment. Over time, the critic
becomes more accurate in estimating the advisor
states, which allows the accurate slide the
actions that lead to those states, from a
practical perspective, are going to be updating
the weights of our deep neural network at
each time step. Because actor critic methods
belong to a class of algorithms called temporal
difference learning. This is just a fancy
way of saying that we're going to be estimating
the difference in values of successive states,
meaning states that are one time step apart,
hence temporal difference. Just like with
any deep learning problem, we're going to
be calculating cost functions. In particular,
we're going to have two cost functions, one
for updating our critic and the other for
updating our actor. To calculate our costs,
we want to generate a quantity we'll call
Delta. And it's just given by the sum of the
current reward and the discounted estimate
of the new state and then subtracting off
the value of the current state. Keep in mind
that the value of the terminal state is identically
zero. So we need a way to take this into account.
The cost for the critic is going to be delta
squared, just kind of like a typical linear
regression problem. The cost for our actor
is a little more complex, we're going to multiply
the delta by the log of the policy for the
current state and action the agent took. The
reason behind this is a little complex, and
it's something I go into more detail about
in my course, where you can look for it in
the chapter on policy gradient methods in
the free textbook by Sutton and Barto. So
let's talk implementation details, we're going
to implement the following algorithm. initialize
a deep neural network to model the actor and
critic. Repeat for a large number of episodes,
we set the score on terminal flagging environment,
all the state is not terminal, select an action
based on the current state of the environment.
Take the action and receive the new state
reward and terminal flag from the environment.
calculate delta and use it to update the actor
and critic networks. Set the current state
to the new state and increment the episode
reward by the score. After all, the episodes
are finished, plot the trend and scores to
look for evidence of learning, there should
be an overall increase in score over time,
you will see lots of oscillations because
actor critic methods aren't really stable,
but the overall trend should be upward. Another
thing you may see is that the score can go
upward for a while and then fall off a cliff.
This isn't uncommon, because actor critic
methods are quite brittle. And they're really
not the best solution for all cases, but they
are a stepping stone to more advanced algorithms.
Other important implementation details, you
can use a single network for both the actor
and critic. So you have common input layers
and two outputs, one for the actor and one
for the critic. This has the benefit that
we don't have to train two different networks
to understand the environment, you can definitely
use an independent actor and critic, it just
makes the learning more difficult for an algorithm
and is already pretty finicky. What to play
almost 2000 games with a relatively large
deep neural network, something like about
1000 units in the first hidden layer and 500
units in the second. The hard part is going
to be the actor. As I said earlier, the actor
models the policy, which is a probability
distribution, the actor layer will have as
many outputs as our actions. And we use a
softmax activation because we're modeling
probabilities, and they'd better sum to one.
When selecting actions, we're going to be
dealing with a discrete action spaces. So
this is what is called a categorical distribution.
We're going to want to use the TensorFlow
underscore probability package for the categorical
distribution, and then use the probabilities
generated by the actor layer to get this distribution,
which we can then sample and use the built
in log prop function for our cost function.
As far as the structure of our code, we're
going to have a class for our actor critic
network. And that will live in its own file.
We'll also have a class for our agent and
they'll have that functionality, choose actions
save models and learn from its experience,
Matt goes in a separate file. The main loop
is pretty straightforward, but it does go
in its own file as well. Okay, now that we
have all the details out of the way, let's
go ahead and get started coding this. So now
that we have all of our lectures out of the
way, we're going to go ahead and proceed with
the coding. We're going to start with the
network's begin as always with our imports.
So we will need OS to handle file joining
operations for model checkpointing. We will
need carols and we will need our layers which
for this example is just going to be a dense
layer. So we will have our actor critic network
and you See a case of converging engineering
here where TensorFlow and pytorch both have
you derive your model class from the base
model class. And we can go ahead and define
our constructor. That will take a number of
actions as input the number of dimensions
for the first fully connected layer, we will
default that to 1024. And for the second,
we will default it to 512. We will have a
name for model checkpointing purposes, and
a checkpoint directory. Very important, you
must remember to do a make directory on this
temp slash actor critic before you attempt
to save a model. Otherwise, you're going to
get an error. The first thing you want to
do is call your super constructor. And then
go ahead and start saving your parameters.
Now, also very important for our class that
we've derived from the base class. In this
case, the actor critic network class, we have
to use model name instead of name because
name is reserved by the base class. So just
be aware of that not a huge deal checkpoint
directory. And then we'll have our file and
that will be OS path join the directory name
plus underscore Pacey. I like to use underscore
algorithm, in this case, AC for actor critic,
in case you have one directory that use for
many different algorithms, if you're just
using like, say a working directory, you don't
want to confuse the model types. Otherwise,
if you have a good model saved, you don't
want to override it with something else. Now
we'll go ahead and define our layers. And
that will be fully connected dense layers.
The neat thing about Kairos is that the number
of input dimensions are inferred so we don't
have to specify it. That's what we don't have
an input dims for our constructor, and it
will output FC one dims with an activation
of rally, FC two will be similar. And then
we will have two separate outputs. So we have
two common layers and then two independent
outputs, one for the value function. And that
is single valued with no activation. And the
second is our policy pi. And that will output
an actions with a softmax activation. Recall
that the policy is just a probability distribution.
So it assigns a probability to each action.
And those probabilities have to add up to
one because that's kind of what probabilities
do, right? Next, we have to define our call
function. This is really the feed forward.
If you're familiar with that from pytorch.
So we'll just use some generic name like value
doesn't really matter, and pass through the
second fully connected layer. And then get
our value function and our policy pi and then
return both value function and the policy
pi. So that is really it. For the actor critic
network. All of the interesting functionality
happens in the agent class. So let's go ahead
and start writing the agent class. So we'll
begin as always with our imports. We will
need TensorFlow, we will need our optimizers.
In this case, we're going to use an atom optimizer.
Probability it t we will need TensorFlow probability
to handle our categorical distribution to
model our policy directly. You have to do
a pip install TensorFlow probability before
you can run this. This is a separate package
from TensorFlow. And we also need our actor
critic network. Let's go ahead and code up
our agent. So our initializer is pretty straightforward.
We will need some default learning rate news
0003 It doesn't really matter I'm going to
pass in a specific learning rate in the main
file. We will have a gamma of 0.99 and Default
in actions have some number like, say two.
So we're going to go ahead and save our parameters,
we'll call the gam is our discount factor,
we're gonna need a variable to keep track
of the last action we took. This will be a
little bit more clear when we get to the Learn
function, and has to do with the way we calculate
the loss because we have to use a gradient
tape for TensorFlow two, which is a bit of
a workaround for how TensorFlow two does things,
we need our action space for random action
selection. Let's use a list of actions from
zero to n actions minus one, we need our actor
critic 
want to make sure to specify the number of
actions and we want to compile that model.
So after critic compile with an atom optimizer,
and learning rate defined by alphab. Next,
we have the most basic functionality of our
agent, the functionality to choose an action.
And that takes the current state of the environment
as input, which we have to convert to a tensor.
And in particular, we have to add an extra
dimension batch dimension, the reason being
that the deep neural network expects a batch
of inputs. And so you have to have something
other than a 1d array SB two dimensional,
so we just add an extra dimension along the
zeroeth dimension. So then we will feed that
through our deep neural network, we don't
care about the value of the state for the
purpose of choosing that action, so we just
use a blank. And we will get the probabilities
by passing the state through the actor critic
network. And then we can use that output the
probabilities defined by our neural network
to feed into the actual TensorFlow probabilities
categorical distribution, and then use that
to select an action by sampling that distribution,
and getting a log probability of selecting
that sample. Sorry, that's TFP, categorical,
and probabilities given by prompts, and our
actual action will be a sample of that distribution.
And we don't actually need the log prompt
at this stage, we will need the log problem
we calculate the loss function for our deep
neural network. But we don't need it now.
And it doesn't make sense or rather, doesn't
actually work. To save it to a list, let's
say for use later here. Because this calculation
takes place outside of the gradient tape.
TensorFlow two has this construct of the gradient
tape, it's pretty cool. It allows you to calculate
gradients manually, which is really what we
want to do here. But anything outside of that
tape doesn't get added to the calculation
for backpropagation. So the log prob doesn't
matter at this point, so why bother calculating
it. One thing we do need, however, is the
action that we selected. So we will save that
in the action variable. And we will return
a NumPy version of our action because action
is TensorFlow tensor, which is incompatible
with the open engine. It does, however, take
NumPy arrays, and we want the zeroeth element
of that because we added in a batch dimension
for compatibility with our deep neural network
and a little bit confusing, but that is what
we have to deal with. Next, let's do a couple
of bookkeeping functions to save and load
models won't take any inputs. And so it will
save the weights of the network to the checkpoint
file. We do the inverse operation to load
models. And we will load weights from a checkpoint
file. So that is it for the basic bookkeeping
operations. Next we have the real heart of
the problem and the functionality to learn.
This will take a number of inputs, we'll take
the state reward received new state and terminal
flag as input The first thing we want to do
is convert each of those to TensorFlow tensors.
And make sure to add a batch dimension. And
I like to be really pedantic with my data
type. So I will cast it to tf float 32. And
we don't have to add a batch dimension to
the reward, because it is not fed to a deep
neural network. So now we get to calculate
our actual gradients using something called
the gradient tape. And we'll set persistent
to true. I'm not actually sure that's needed.
I'm going to go ahead and experiment with
that. One, we go ahead and run the code. But
I have it that way, I might have just copied
and pasted code from somewhere else. So let
me double check. But we want to feed our state
a new state through the actor critic network,
and get back our quantities of interest. So
we feed the current state and then the new
state. But for the new state, we don't care
about the probabilities, we just care about
the value. And that is for calculation of
our delta. But for the calculation of our
loss, we have to get rid of that batch dimension.
So we have to squeeze these two parameters.
And the reason you have to do that is because
the loss works best if it's on a one dimensional
quantity, or rather a scalar value rather
than a scalar value inside of brackets. So
it has to be a scalar instead of a vector
containing a single item. It's just something
we have to do, I encourage you to play around
with it to double check me on that I move
between I move between distributions, excuse
me a framework. So sometimes, stuff isn't
always 100% necessary, even if it doesn't
hurt anything. So we need our action probabilities
for the calculation of the log prob. TFP distributions
categorical. We define our props by the output
of our deep neural network. And then our log
prop is action probs dot log prop of the self
dot action. And this is the action that we
saved up at the top when we calculate the
action for the agent. So this is the most
recent action, then we calculate our delta.
That is reward plus gamma multiplied by the
value of the new state times one minus end
of done. And the reason for that is that the
value of the terminal state is identically
zero, because no returns no rewards follow
the terminal state, so it has no future value.
and subtract all the state value. So our actor
loss is minus log prob times that delta. And
the critic loss is delta squared, and the
total loss, he goes after loss plus critical
loss. And then we can go ahead and calculate
our gradients. So our gradient is taped out
gradient, total loss with respect to the trainable
variables. optimizer apply gradients, and
it's expects a zip as input. So we're going
to zip the gradient and the trainable variables.
Alright, and that is it for the actor critic
functionality. So I'm going to come back to
this in a few minutes to see if I need that
persistent I don't believe I do. I believe
I need this in the case when we have. If we
were to have, say, separate actor and critic
networks and had to calculate greatest perspective,
two separate sets of trainable variables,
I believe that's when the persistent equals
true would be necessary. And when when we
had coupling between the loss of one network
and the other, it's so that it keeps track
of the gradients, average does the back propagation,
kind of like in pytorch, where it throws it
away and you have to tell it to retain the
graph. I'll double check on that though. So
let's go ahead and write and quit. And then
we're ready to go ahead and code up our main
file. So with our imports, we will need, Jim,
we will need NumPy we will need our agent,
we all need our plot learning curve function,
I'm not going to go into any detail on this,
it's just a function to plot data using matplotlib.
With some labeled axes, it's nothing really
worth going into. first thing I'll do is make
our environment. And I'm using the card poll,
because it runs very quickly. And the actor
critic method is quite brittle, quite finicky,
you will observe in many cases where it will
achieve a pretty decent score then fall off
a cliff because the learning rate was just
a little bit too high. So there are a number
of problems with the algorithm. And it's easiest
to test in a very simple environment. In my
course, we use the lunar lander environment,
and I did more hyper parameter tuning to get
it to actually get pretty close to beating
the environment, I believe. In this case,
we won't quite beat it, we achieve a high
score like 140 points or so when beating it
is 200. But I leave the exercise of hyper
parameter tuning to you, the viewer, I gotta
leave something for you to do as well, right.
So we'll define our agent with a learning
rate of one by 10 to the minus five, and a
number of actions defined by our environment,
action space underscore, and then we'll have,
say, 1800 games, about 2000. With a file name
of card poll dot png, I would encourage you
if you do hyper parameter testing, to put
the string representations of those hyper
parameters here in the file name, so that
way, when you look at it later, you don't
get confused. And you know what hyper parameters
were used to generate which plot. So our figure
file is just plots plus the file name, I split
it up, you don't have to do it that way. We
want to keep track of the best score received.
And it'll default to the lowest range. So
that way, the first score you get is better
than the lowest. So the range is a you save
your models right away. And if you list to
keep track of the score history, I Boolean
for whether or not we want to load a checkpoint.
So if we're going to load a checkpoint, then
I'm going to load models. And then finally,
we want to go ahead and start playing our
games. We want to reset our environment, we
set our terminal flag, set our score to zero.
And while we're not done with the episode,
we can choose an action 
get the new state reward done and info from
the environment, increment our score, we're
not loading a checkpoint, then we want to
learn. Either way, we want to set the current
state to the new state. Otherwise, you will
be constantly choosing an action based on
the initial state of the environment, which
obviously will not work. You also want to
append the score to the score history for
plotting purposes, and calculate a score an
average score of the previous almost say 100
games. And if an average score is better than
your best score, then set the best score to
the average score. And if we're not loading
a checkpoint, then save your models. So this
inner conditional statement keeps you from
overriding your models that had your best
scores when you're actually testing. If you
just saved the model every time then you'd
be overriding your best model with whatever
which you know, may not be the best model.
So at the end, if we're not loading a checkpoint,
actually, we can just plot either way and
let's do that. It's got our x axis and plot
learning curve x score. Figure file. Okay.
Now I have to do a major On plots, temp that
I call it temp slash actor critic. Otherwise,
this stuff won't work. And you also want to
do a pip install. Flow TensorFlow probability,
because that is a separate package. Of course,
I already have it. So let's go ahead and try
to run this. And see if I made any typos.
I'm certain almost certain I did. So it says
something. Something is not callable. Oh,
that's because I have forgotten my multiplication
sign. So that is in line 49. Yeah, things
I'm trying to call something here when I really
want to multiply. Oh, you know what I did
forget. One thing I did forget, of course,
is down here, I forgot my debug statements.
So let's do this. That's pretty funny. episode,
I score. Set one F. 
Always have to forget something, of course.
Okay, there we go. So one other thing I want
to do is come back here to actor critic, and
get rid of this persistent equals true, I
don't think I actually need this. Sometimes
I just copy and paste code. And then as I'm
doing the video, I realize, Oh, hey, I don't
always need all of that stuff. Okay, so yeah,
it does run. All righty. So I'm going to go
ahead and switch over to another window where
I have let this finish up. Because there's
no point letting it run for another 1800 games.
So let's go ahead and check that out. So here
you can see the output of the other 1800 games
I ran. And it does achieve a score of around
160 768, about 170 points or so which is almost
beating the environment is pretty close, you
take a look at the learning plot here, you
can see that it has an overall overall upward
trend, and it's linear. And the reason I don't
let it continue is because as I alluded to
in the lecture, these models are very brittle.
And so sometimes you can get on a very narrow
slice of parameter space where your model
is doing well in any small step out of that
range blows up the model. One thing to fix
that is replay memory, as you get a broader
sampling of, of experiences and get a little
bit more stable learning. But that doesn't
work by bolting directly on to actor critic
methods, at least from my experience, I've
tried it I have a video on that. I wasn't
able to get it to work. Maybe some of you
can. That would be fantastic if you could,
but in my case, I didn't get it to work. I
thought it would work. It does not. And in
fact, there's a whole separate algorithm called
actor critic with experience replay that deals
with bolting experience replay on to vanilla
actor critic methods. So I hope this was helpful.
It's pretty hard to give a really solid overview
and like a 3040 minute YouTube video, but
it serves to illustrate some of the finer
points of Agile critic methods and some of
the foundational points of deep reinforcement
learning in general. In my courses, I go into
much more depth. And in particular, I show
you how to actually read papers, how to turn
papers into code, useful skill that's really
hard to find anywhere else on YouTube or Udemy.
So if you like the content, make sure to leave
a like, subscribe if you haven't already.
And leave a comment down below with any questions,
comments, criticisms, concerns, and I will
see you in the next video. Oh, really quick
before we do, let's check in on the other
terminal to make sure it's actually learning.
So here is the output. And you can see as
it's going along, it is saving some models
and the score is generally trending upward
over time. And that's why you get the saving
models because the score is best in the last
best score. So we didn't make any fundamental
errors in our code. If it doesn't achieve
the same result that isn't entirely surprising
because there is a significant amount of render
run variation. But the code is functional.
What's up on my GitHub. If you want to go
ahead check it out. I'll leave a link in the
description. See you in the next one. Welcome
to a crash course in deep deterministic policy
gradients, or ddpg for short. In this lecture,
we're going to briefly cover the fundamental
concepts and some notes on the implementation,
so that the stuff we do in the coding portion
isn't such a mystery. So why do we even need
ddpg? in the first place? Well, ddpg exists
to answer the question of how do we apply
reinforcement learning to continuous action
spaces. This is particularly important in
something like say robotics, where we are
applying continuous voltages to electronic
motors that cause the robot to move in three
dimensional space. Now, you may think, Well,
why can't we just use something awesome, like
deep q learning. And that's not such a terrible
idea. However, the problem is, it doesn't
work. In particular, q learning can't handle
continuous action spaces. If you've coded
up a Q Learning Network, a deep learning network
that is, you know, that it outputs discrete
numbers has actions. And of course, that doesn't
cut it when you're dealing with continuous
action spaces. Now, you might think, why can't
we just discretize our action space. And that's
not such a bad idea. But the problem is, it
doesn't work. And the basic idea here is you
have some finite interval for your action
space, and then you just divide it up into
a number of discrete chunks. And then every
time you want to output the action in that
chunk, you just use that discrete integer.
The problem with this is that these robots
tend to have many degrees of freedom, meaning
they can move in many different directions
in space, right, they can rotate around axes,
multiple axes, in general, they can move up,
down left and right, they can rotate. And
so your number of discrete actions approaches
the 1000s very, very quickly. And so q learning
while it does handle discrete action spaces,
doesn't handle large numbers of discrete actions
particularly well. However, that doesn't mean
that the innovations from cue learning can't
be applied to actor critic methods. And in
fact, they can, and that was the motivation
behind the work done in the ddpg paper. In
particular, we're going to make use of a replay
memory, where instead of just learning from
the most recent state transition that the
agent has experienced, is going to keep track
of the sum total of its experiences, and then
randomly sample that memory at each time step
to get some batch of memories to update the
weights of its deep neural networks. The other
innovation is the use of target networks.
So in cue learning, we have to do two different
things, we have to use a network to determine
the action to take. And then we have to use
a network to determine the value of that action.
And that value is used to update the weights
of the deep neural network. Now, if you're
using the same network to do both things,
the problem is you end up chasing your tail,
you end up chasing a rapidly moving target,
because each time step those weights are getting
updated. And so the evaluation of similar
actions, excuse me, the evaluation of similar
states, changes rapidly over the course of
the simulation causing the learning to be
unstable. The solution to this is to keep
two networks, one of which use the online
network is called to choose actions at each
time step. And then another network called
a target network, to evaluate the values of
those actions when performing the update for
your deep neural network. Now, in this case,
you will be doing a hard update of your target
network. So every, let's say 1000 steps, it's
a hyper parameter v region, but a typical
value would be 1000 steps, you would take
the values for the network parameters from
the online network, and directly copy those
over to the target network. It's what's called
a hard update. The authors of the ddpg paper
took inspiration from this, but instead of
doing a direct, hard update, they use something
called a soft copy of the target networks.
All this means is that we're going to be doing
some multiplicative constant for our update,
and we're going to be using a new hyper parameter
called tau. And it's going to be a very small
number of order point 001. It's also worth
noting that we're gonna have more than one
target number here. And the reason we need
more than one target network is because DDP
G is a type of actor critic method. And so
you have two distinct networks, one for the
actor and one for the critic. And in fact,
in this implementation, we're going to have
four different networks, one actor, one critic,
and then one target actor and one target critic.
Now, for simple problems with discrete action
spaces, you can get away with having a single
network, where the lower layers learn the
features of the environment and the upper
layer splits off into outputting, the Act,
the critic evaluation as well as the output
for the actor network. But in this case, we
do in fact want to totally distinct networks,
as well as two copies of those for the target
networks. The basic idea is that our critic
network is going to evaluate State in action
pairs. And so we're going to be passing in
states and actions. And it's going to say,
hey, given that state, the action we took
was pretty good. Or maybe that action was
pretty terrible, we could probably do better
next time. And similarly, the actor is going
to decide what to do based on the current
state or whatever state we pass into it. Something
worth noting is that this network is going
to output action values, in other words, a
continuous number that corresponds to the
direct input for opening a gym environment,
rather than outputting probabilities. So if
you've been doing this for a while, you may
know that the policy is actually a probability
distribution, it is a function that tells
us what is the probability of selecting any
action from the action space given an input
of a state or a set of states. The deterministic
part comes from the fact that ddpg outputs
the action values themselves. And it's deterministic
in the sense that if I pass in one state over
and over again, I'm going to get the same
action value out every single time. Now, this
does have a bit of a problem. So the problem
is that the agent has something called an
explore exploit dilemma. And this is present
in all reinforcement learning problems. It's
a fundamental concept in the field. The basic
idea is that our agent is attempting to build
out a model of the world, the agent wants
to know how to maximize his total score over
time. But it starts out knowing absolutely
nothing about its world, it has to figure
out how states transition from one end to
another and how its actions affect those states,
and in particular, how its actions give it
rewards. So it starts out knowing none of
this and has to build out that model over
time. The problem is, the agent can never
be quite sure that its model is accurate.
No matter how long it spends playing the game
interacting with the environment, isn't 100%
certain that the action it thinks is best
is actually the best. Perhaps there's some
other strategy, some other action out there
that is significantly better. And the degree
to which the agent takes off optimal actions,
is called the Explore exploit dilemma. So
of course, taking off optimal action is called
exploration. And taking the optimal action
is called exploitation, because you're just
exploiting the best known action. And this
is a dilemma that is present in all reinforcement
learning problems. And the solution here is
to take the output of our actor network and
apply some extra noise to it. Now in our implementation,
we're going to be using simple Gaussian noise,
because it's sufficient for the problem at
hand. However, in the original paper, the
authors use something called Orenstein lundbeck
noise. It's a model of Gaussian processes
in physical systems, it's overly complex,
and it's not needed. So we're not going to
implement it. Although in the course, I do
show you how to implement it exactly. But
for YouTube, it's not really necessary. And
in fact, when other authors implement ddpg,
they just throw that right out the window,
because it's pretty dumb. Next is the update
rule for our actor network. And it is somewhat
complex. So I'm going to show you the equation,
and then I'm going to walk you through it.
So this is the update for the actor network.
And don't panic, this is a little bit easier
than it looks at first glance. So from left
to right, we have the Nabla operator that
is the gradient and the subscript there theta
super mu means that we want to take the gradient
of the cost function J with respect to the
network parameters of our actor network, or
the actress denoted by mu, and its parameters
are denoted by theta super mu. So theta Super
Q means the parameters for the critic network
or the critic is denoted by Q. And it's just
given by an expectation value or an average
of the gradient of the critic network. Where
we're going to input some states and actions
with the actions are chosen according to the
current policy. Okay. So, in practice, what
this means is, we're going to do is randomly
sample states from the agent's memory. Now,
the memory keeps track of everything we want,
it keeps track of the states, the agent saw
the actions that took and the new states that
resulted from those actions, as well as the
reward the agent received at that time step
and the terminal flag to determine whether
or not the episode ended on the new time step.
So the scent the memory keeps track of all
of that. But here, we just want to sample
the states the agent saw, okay, and then we're
going to use the actor network to determine
what actions it thinks it should take based
on those states. Now, these actions will probably
probably be different from the actions we
have stored in memory and that's okay. This
is off policy, meaning we're using a separate
policy to gather data and use that data to
update a different policy, the current policy
so It's ok that these actions don't match.
Don't worry about that. It all works out in
the end, then you then the next thing you
want to do is plug those actions from the
actor into the critic network along with the
states, we sample from the memory and get
the value for the critic network what it thinks
that state action pair is worth. And then
we're going to use the gradient tape from
TensorFlow to to take the gradient of the
actor network, excuse me, the critic network
with respect to the parameters from the actor
network. And we can do that because they're
coupled through this selection of the actions
based on the actor and network. Okay, so it's
a little bit complex. It's much easier in
code just in code, you sample the the states,
then you get the actions based on the critic
network, and then plug those states and actions
into the critic network. And then you just
have the loss proportional to that it's actually
much simpler in code than it is on paper.
But that's the basic idea. So then the next
question is, how do we implement our critic
network? Well, fortunately, it's a little
bit more straightforward, and it's more reminiscent
of deep q learning. So we have this relatively
simple loss function, that is the mean squared
error between these, this target value y sub
i, and this Q for the current state and action.
So what we're going to do here is randomly
sample states new states actions and rewards.
And then we want to use the target actor network
to determine the actions for the new states.
Then plug those actions into the target critic
network, to get your why and multiply it by
the discount factor gamma, and add in the
reward from that time step, which you sampled
from the memory. And then that is a target
the via we want to shift the estimates for
our critic towards. And then we want to plug
the states and actions into the critic network.
In other words, the actions the agent actually
took that we sampled from our memory, This
is in contrast to the update for the actor
network, and take that difference with the
target. And then we're going to input it into
the mean squared error loss function for TensorFlow
two. So in code, this is going to be relatively
straightforward as well, we're going to have
a sample function to get the state's new states
actions and rewards, we're going to plug in
the new states into the target actor network,
get some output, we're going to plug that
output into the target critic to get our target
y, and note, it is S sub i plus one. So what
is the new states that we get from our memory
buffer. And then we're going to plug the current
states and actions from our memory into the
critic and take the difference with the target.
So it only looks kind of scary on paper, when
you see it written in code, it'll make much
more sense. So the other piece of the puzzle
is how we're going to handle the updates of
our target networks. So at the very beginning
of the program, we are going to initialize
our actor and critic networks with some parameters,
of course, are going to be random. And then
we're going to directly copy those parameters
over to our target actor and target critic
networks, that'll be the only time in the
simulation that we do an exact hard copy.
every other time step, we're going to use
the soft update rule. So here you have the
two parameters, theta Q, theta Super Q prime,
so the weights of the target critic network,
and theta super mu prime, which is the weights
of the target actor network, you're going
to update those with tau multiplied by the
respective value of the online network, the
critic or the actor, and add in one minus
tau times the current value of your target
actor or target critic network. And so this
will be a very slowly changing function, because
there's going to be some small number tau,
multiplied by some parameters plus one minus
tau is, which is approximately one multiplied
by the current value. So it's going to be
approximately equal to its last time step,
just plus minus a little bit. So it's relatively
straightforward. We'll see it in code. It's
not all that bad. So then the next question
is, what data structures are we going to need
for all of this? So we're going to use a class
to encapsulate our replay buffer, and that
will use NumPy arrays, there are a myriad
of different ways to handle the replay buffer.
I like the NumPy arrays because it's easy
to enumerate. It's easy to know, what is being
stored where, and it makes for an easier YouTube
video because it's easier for people to see
what's going on. If you have a different way
of doing it, by all means use that it's not
something for which there was only one right
answer. We will need one class each for the
actual network and critic network and those
will be handled using the TensorFlow two framework,
these will have the functionality to perform
a four pass as well as the usual initializer
function. We will also need an agent class
to tie everything together, the agent will
have a functionality for the memory right
it will have a memory buffer to store memories,
it will have the actor network critic network
target actor or target critic network, it
will also have a function for choosing actions
based on the current state of the environment.
And that will involve passing a state through
the actor network, getting the output and
adding in some Gaussian noise. It will also
have functionality to learn from the memories
it has sampled. And it will also have functionality
for checkpointing models because this can
take quite a while to train for complex environments,
we're going to use a simple environment. But
if you want to go ahead and try something
more complex, the checkpointing functionality
will come in handy. Finally, we will need
a main loop to train our network and to evaluate
its performance. Or that has been a very brief
introduction. Let's go ahead and get into
the coding portion of our video. Alright,
now that the lecture is out of the way, it
is time for a shameless plug. In my courses,
I show you how to go from paper to code rather
than relying on someone else to break down
the material and then paper for you. It's
the best way to gain independence. And to
level up your skill as a machine learning
engineer provided through Udemy, they're on
sale right now check the link in the description
below. Let's go ahead and start with our buffer.
So the point of our buffer is to store the
state's actions rewards new states and terminal
flags and the agent encounters in its adventures.
And we're going to use as I stated in the
lecture NumPy for this is going to be relatively
straightforward. The reason I use NumPy is
because it makes it much more clear what everything
is, it's just a little bit simpler and cleaner
from an implementation perspective, although
it is by no means the only way to do things.
So the first thing we'll need is a max size,
the memory is bounded, and input shape from
our environment, and a number of actions for
our action space. Now in this case, since
it is a continuous action space, number of
actions is a bit of a misnomer. What it really
means is number of components to the action.
The purpose of the meme size is that the memory
cannot be unbounded. And our memory will have
the property that as we exceed the memory
size, we will override our earliest memories
with new ones. So for that, we will need a
memory counter it starts at zero. And then
we can go ahead and start with our actual
memories. First we have a memory size, excuse
me a state memory. And that will be in shape
memory size by input shape. We have the new
state memory which is in the same dimensions
we need an action memory. And that will be
and shape mem size by n actions. And one thing
I want to make clear there was a question
on the discord server. By the way, also check
the link in the description for the discord
server, we have many really bright people
in there talking about a lot of complex stuff.
It's a great little community, I would encourage
you to join. But the question popped up whether
or not I'm writing this stuff on the fly.
And I am not I don't know if that was clear,
you can often see me looking off to the side
here. That's because it's not a second monitor,
which is a really large monitor where I have
a second window open with the already completed
code. And if you've seen my tutorials before,
you know that I make a lot of typos. And so
the probability of making a logical error
where I have state instead of new state, for
instance, is incredibly high. And that when
you are making YouTube videos is enormously
painful to try to do everything from memory
and then swap states with new states and then
not have it work and then have to go back
and re record. So it's just easier to have
the already written code and I'm kind of reading
off of it as I go along. And occasionally
I'll make modifications. So all the code is
mine, but it is not written on the fly. I
just want to make that clear. Next we'll need
a reward memory and then as your shape mem
size and that is just going to be an array
of floating point numbers. And we will need
a terminal memory and that will be and type
Boolean. I use Boolean because in in pytorch,
we can use masking of tensors I don't think
it works in TensorFlow. So we'll have to do
something slightly different in our learning
function, but I will note that when we get
there we will need a function to store a trend.
Whereas transition is the state action reward
new state and terminal flag, the first thing
we want to do is determine what is the position
of the first available memory. And that's
given by the modulus of the current memory
counter and the memory size. Once we have
the index, we can go ahead and start saving
our transitions. Yeah, see, I've already made
a mistake here. Then we have actual memory
and terminal memory. And we want to increment
our memory counter by one. Now, one thing
I don't think I stated in the lecture, the
purpose of this terminal memory is that the
value of the terminal state is zero. Because
no future rewards follow from that that terminal
state, we have to reset the episode back to
the initial state. So we have to have a way
for accounting for that in our learning function.
And we do that by using the terminal flags,
excuse me by using the terminal flags, as
a multiplicative constant in our learning
function as a function to sample our buffer,
and that will take a batch size as input,
we want to know how much of our memory we
filled up because we have a memory and it
starts out as entirely zeros. And until we
fill up that memory, some portion of it will
be nothing but zeros. It doesn't do us any
good to learn from a bunch of zeros. So we
have to know how much of the memory we've
actually filled up. And this is given by the
minimum of the memory counter or the memory
size, that we can take a batch of numbers,
random choice maximum batch size. And, you
know, I think I want to pass the Replace equals
false flag in there. I don't have that in
my cheat sheet. But the point of passing and
replace equals false is that once a memory
is sampled, from that range, it will not be
sampled. Again, that prevents you from double
sampling memories. If you're dealing with
a really large memory buffer, the probability
of sampling two memories, two identical memories
is astronomically small, but up until that
point, it's non negligible. So we should be
careful. Then we want to go ahead and dereference
our NumPy arrays. And we will return those
at the end 
is a typo. Alright, that wraps up our replay
buffer class. Now we're going to go ahead
and handle the network classes. Alright, so
our imports are relatively light, we will
need OS for file path joining operations for
model checkpointing. We will need the base
TensorFlow package we will need Kairos and
we will need layers. Now we're going to be
dealing with a very simple environment. So
we just need a dense layer. We don't need
any convolutional layers. So we will start
with a critic network. And that derives from
the Karol stop model class that will take
a number of actions fully connected dims.
a name and a checkpoint directory. Now very
important, you must do a make der temp and
make der temp slash ddpg. Before we run this,
so that you can actually save your models
otherwise we'll get an error. Let's go ahead
and call a super constructor 
and then start saving our variables. We have
to call our model model name due to the TensorFlow
package keeping name as a reserved variable.
You can't just say dot name, it'll give you
an error. Not a huge deal, just something
to be aware of, then we'll have our checkpoint
file. And chaos models get saved with a dot
h file extension. And I throw in the model
name, because we're going to want to distinguish
between the target and regular networks. And
I add an underscore ddpg. Because if you do
development in a single directory, then you
don't want to overwrite models from say, TT
three with ddpg models vice versa. It's just
a way of keeping all of your models distinct
and very secure. Next, we need to define our
network. So our first fully connected layer
have a lot of put self that FC one dims with
a rail you activation, then we'll have FC
two dims. Rail you and our final output layer
is single valued with no activation. A couple
of things to note here is that in the original
paper, the author's use batch normalization.
Turns out that isn't necessary. In the course
I show you how to implement that. But it's
use an overly complex implementation, it doesn't
actually add much to it. They also do a number
of kind of tweaks with the initialization
of the layers, we're not going to do that
here, I show you in the chorus, but it's not
really necessary. Next, we need our call function.
This is the forward propagation operation.
And this is the critic so it takes a state
and action as input. So we want to pass the
concatenated. State and action through our
first fully connected layer and we'll concatenate
it along the first axis. The zero axis is
the batch. And then we will pass the output
of that through the second fully connected
layer. And we will go ahead and get our Q
value out and return it. That is it for the
actor network. Excuse me, critic network.
Very, very straightforward. Next, we have
our extra network. And that is pretty similar.
equals 512. And actions are just defaulted
to name equals actor. And check pointer. And
you know what, now that I'm looking at this,
we don't need the number of actions here,
do we because we don't use it. Let's get rid
of that. Yeah, I'm looking at my cheat sheet
as well. These are some of the modifications
I make as I go along. Sometimes I'll have
I'll have stuff that doesn't always make 100%
sense. And then when I do the YouTube video,
I go ahead and rectify it. So we'll go ahead
and call our super constructor and save our
values. This time, we will need the number
of actions because the critic network does.
Excuse me, the actor network does need to
know how many actions it has. And we will
need our checkpoint actor. And then our model
is going to be pretty similar to the critic
network, there's going to be a number of fully
connected layers with a row activation. Now,
we don't want a neural activation here no
activation or linear activation, if you will,
we do want an actual function here. And what
we want is a function that is bounded between
plus and minus one. And the reason is that
most of our environments have an action boundary
of plus or minus one. However, if the action
boundary is larger than plus or minus one,
it's easy to get the The action within that
range by multiplying a function which is bounded
by plus or minus one by the upper bound of
the environment. So, if your environment has
bound to plus or minus two, then he would
just multiply the tan hyperbolic function
which is bound to plus or minus one by to.
Next, we have our call function, we'll go
ahead and call it prob, it's a bit of a misnomer.
These aren't really probabilities. And then
we will get our mu, which is from the paper,
it's our actual action. Now, if you had not
plus or minus, one can multiply here, you
can multiply it there, or you can multiply
it in the agent class, when we do the Choose
action function. Either way, is logically
the same, I would probably do it in the agent
class, just for my own personal preferences,
but either way, you want to multiply the output
of the deep neural network by the bounds if
those bounds are not plus or minus one. All
right, that is actually it for the network
classes. Let's go ahead and code the agent
class and tie all of this together. Okay,
our imports will be quite numerous, we will
need NumPy. TensorFlow, we will need Kairos.
We will need our optimizers. Which we use
Adam for this project, we will need our replay
buffer. And we will need our actor and critic
network. I misspelled it not belux. Correct.
So here's our agent class. And we have an
initializer. And that will take input dims.
A learning rate for the actor network alpha,
a learning rate for the critic network beta.
These are distinct, they're not the same.
And in fact that the critic networking can
get away with a slightly higher learning rate
than the actor network. And that is because
in general and policy gradient type methods,
the policy approximation is a little bit a
little bit more sensitive to the perturbation
and parameters. So he wiggle around the parameters
of your deep neural network, you can get big
changes in the output of the actor network.
And so it has to have a slightly smaller learning
rate with respect to say, like the critic
network, we will need our environment for
the max admin actions because as I said, on
lecture, we're going to be adding noise to
the output of our deep neural network for
for some exploration, and we have to clip
that into the maximum actions for environments
we don't trigger an error. When we try to
pass that action into the open AI gym. We
will need a gamma. And that is the discount
factor for update equation. number of actions.
A MAX SIZE for our replay buffer defaulted
to a million a default value for our soft
update of 0.005. That's from the paper. default
for FC one and FC two. In the paper, they
actually use 403 100. I'm going to go ahead
and do that now rather than in the main program
that makes life a little easier. A batch size
for a memory sampling and a noise for our
exploration. So let's go ahead and save our
parameters gammon towel, we can instantiate
our memory. We can save our batch size and
our noise 
and then we'll go ahead and get the maximum
and minimum actions for our environment. Then
we can instantiate our actual networks. So
our actor, our critic or target actor, And
our target critic. That is it for our networks,
now we have to actually compile those we'll
use our atom optimizer learning rate to find
by alpha. Similar we for our critic network,
we will give it a learning rate of beta. And
then we have our target networks. That is
a bit of a misnomer, because we aren't going
to be calling the we are going to be doing
gradient descent for these networks, we're
going to be doing the salt network update.
But we have to compile the network, so we
have to pass it a learning rate. It's just
a feature of TensorFlow. Don't get confused.
If down in the Learn function, we don't actually
call an update for the loss function for these
target actor and target critics, then we have
to call our update network parameters function.
And this is where we do the hard copy of the
initial weights of our actor and critic network
to the target actor and target critic network.
And the passing of tau equals one to facilitate
that hard copy. Let's go ahead and write that
function now. And so we have to deal with
the base case of the hard copy on the first
call the function and the soft copy on every
other time we call the function. So tau is
none. In other words, if we don't supply a
value for towel, and just use the default
defined in the constructor, and so you notice
here, the first time I call it towel is one,
so tau is not none. So we're going to be using
a value of one for tau instead of the 0.005.
So we'll say weights is an empty list. And
our targets will be target actor weights,
or eye weight. We're going to iterate over
the actor weights and append the weight for
the actor multiplied by towel, plus targets
sub i know, which is the weight from the target
actor times one minus towel. And then after
we go through every iteration of the loop,
we're going to set our weights for the target
actor to that list of weights. So in the first
iteration, towel starts out as one, and so
you have weight times tau, which is one, so
just the actor weight plus the target actor
weight, multiply by one minus tau, which is
one minus one or zero. So on the first iteration,
we get a hard copy. And then we do the same
thing for the critic network. And that is
all we need to do for our software update
rule. So now we'll go ahead and write an interface
function for our agents memory. They'll take
a state action reward new state, and terminal
flag as input. And then we'll just call the
store transition function. And this is just
good clean coding hygiene. It's an interface
function. When you have interdependent classes,
the in theory the we shouldn't be able to
call the agent dot memory dot store transition
from anywhere else in the program except within
its own member function. It's just basic,
object oriented programming stuff. Now let's
deal with model checkpointing. And we'll say
self dot actors save weights after a checkpoint
file. And likewise for our other networks
should be a dot underscore. I think I've done
that before. Specific mistake also have an
extra space here. And the load models function
is just the inverse operation. We say what
is it self dot, dot load weights? dot checkpoint
file. Critic check one fall Yep. Okay, that
is it for our basic bookkeeping type functions.
Now we can get into the functionality for
choosing an action. And that will take the
observation of the current state of our environment
as input, as well as a flag I call evaluate.
This has to do with training versus testing
our agent, remember that we use the addition
of noise for the exploration part of the Explore
exploit lm, if you're just testing the agent
to see how well it learned, you don't necessarily
want to add that noise, you can just do the
purely deterministic output of your actor
network. And I facilitate that with a Boolean
flag here. First thing you want to do is convert
our state to a tensor. And we have to add
an extra dimension to our observation to give
it a batch dimension. It's just what the deep
neural networks expect as input, they expect
the batch dimension. And I specify a float
32 data type to be pedantic. And then we pass
the state through the actor network to get
the actions out. If we are not evaluating,
in other words, if we are training, then we
want to get some random normal noise in the
shape of self dot n actions, with a mean of
0.0 and a standard deviation of whatever our
noise parameter is. Now, it's entirely possible
that the output of our deep neural network
was one or point 999. And then when you add
in the noise, you're adding in something like
let's say 0.1. And you end up with an action
that is outside the bounds of your environment
biggest perhaps is bounded by plus or minus
one. And so you want to go ahead and clip
that to make sure you don't pass any legal
action to your environment. We'll say actions
equals TF clip, by value as a function we
want actions as between self min action and
Max. Action. And then we want to return the
zeroeth element because it is a tensor and
the value is the zeroeth element, which is
a NumPy array. Then we want our learning function.
And this is where the bulk of the functionality
comes in. And right away, we are faced with
a dilemma. So what if it's the case that we
haven't filled up at least batch size of our
memories. So remember that the memory starts
out as all zeros. And if you've only filled
up, let's say 10 memories, you don't really
want to sample those 10 memories, you know,
batch number of times batch size number of
times. So you can just go ahead and say well,
I'm not going to learn I'm going to wait until
I fill up my memory into until at least batch
size number of memories. Alternatively, you
can play batch size number of steps with random
actions, and then call the Learn function.
That's another solution as well and they're
both valid. I just find this to be a little
Bit more straightforward. So then we have
to go ahead and sample our memory. And then
we can go ahead and convert these two tensors.
New states, singular, not plural. And we don't
have to convert the terminal flags to a tensor.
Because we're not going to be doing tensor
operations with it, we're going to be doing
regular NumPy array type operations. So we're
going to use a gradient tape for the calculation
of our gradients. If you're not familiar with
a gradient tape, the basic idea is we're going
to go ahead and load up operations on to our
computational graph for calculation of gradients.
So when we call the Choose action function
above, those operations aren't stored anywhere
that is used for calculation of gradients.
So that's effectively detached from the graph.
So only things within this context manager
are used for the calculation of our gradient.
And so this is where we're going to stick
the update rule from our lecture. So let's
go ahead and start with the critic network,
where recall that we have to take the new
states and pass it through a target actor
network, and then get the target critics evaluation
of the new states and those target actions.
And then we can calculate the target, which
is the reward plus gamma multiplied by the
critic value for the new states times one
minus a terminal flag, and then take the mean
squared error between the target value and
the critic values for the states and actions
the agent actually took. So let's go ahead
and write that out. So target actions is given
by a target actor. What are the things we
should do for the new states and the critic
value for those new states, that's going to
be given by the target critic evaluation of
those new states and target actions. And squeezed
along the first dimension, we have to put
in this squeeze because we have the batch
dimension and it doesn't actually learn if
you pass in the batch dimension, I feel like
I am I missing there we go parenthese. So
we'll have a squeeze everywhere that we had
to actually pass up through the network. So
then the critic value, which is the value
of the current states, the original states
and the actions the agent actually took during
the course of the episode is given by the
squeezed output of the critic, or the state's
actions along the first dimension. I guess
it is right. Yep. And then we have our targets,
and that's reward plus gamma, times this critic
value underscore times one minus done. So
this one minus done is one minus true or false.
So when the episode is over, done is true.
And so you have one minus one or zero. And
so the target for the terminal new state is
just the reward, whereas for every other state,
it is reward plus the discounted value of
the resulting state, according to the target
critic network. I'm sorry, you can hear the
landscapers outside is Tuesday they're doing
the neighbor's yard. Hopefully the noise suppression
filter takes care of that, but if not, I apologize.
Next we have a critic loss. And that's the
mean squared error between our target and
the critic value. outside of the context manager,
we want to go ahead and calculate the gradient
so we'll say credit network gradient was taped
out gradient critic loss self critic trainable
variable so it is the gradient of the critic
loss with respect to those critics trainable
variables, and then we want to apply our gradients.
So our optimizer dot apply gradients and that
expects a zip as input. We want to zip up
the critic network gradient and the critic
trainable variables and that is it for the
critic loss. Now we have the actor loss. So
with, we want to do essentially the same thing
we want our context manager gradient tape
as tape then we say new policy actions accurate
states. So, this is the these are the actions
according to the actor based on its current
set of weights, not based on the weights it
had at the time of whatever memory we stored
in the agent's memory. So, then we have our
actor loss. And that is the negative of the
critic output of the states and the new policy
actions, it's negative because we're doing
gradient ascent and policy gradient methods,
you typically want to take the, you don't
want to do a gradient descent because that
would minimize the total score over time you
want to maximize total score over time. So
you do gradient ascent, gradient descent is
just the negative of gradient descent. So
we stick a negative sign in here. And then
our loss of just the reduced mean of that
after loss. And then we can go ahead and calculate
our gradients and apply them. So the actor
network gradient is taped out gradient of
the actor loss with respect to the actor trainable
variables, and this is how we're going to
get that gradient of the critic loss with
respect to the mu parameters of theta super
mu, is by taking this actor loss, which is
proportional to the output of the critic network.
And that is coupled, the gradient is nonzero
because it has this dependency on the output
of our actor network. So the dependence that
gives you a nonzero gradient comes from the
fact that we are taking actions with respect
to the actor network, which is calculated
according to faders theta super mu. And that
gets fed forward through here through the
critic network. That's what allows you to
take the gradient of the output of the critic
network with respect to the variables of the
actual network, it's how we get that coupling.
And if you read the paper, they actually apply
the chain rule and you get the gradient of
the critic network and the grading of the
actual network. This form is actually easier
to implement in code. That's why I do it based
on the first equation, not the second equation
in the paper, it's just easier to do. So why
not do it the easy way. Then you want to apply
those gradients, which again, takes a zip
as inputs. We want to apply zip up our attic
radio network, and the actor dot trainable
variables. One other thing I want to point
out is that this accurate, trainable variables
we didn't define, it comes from the fact that
we derive our actor network class from the
Kairos dot model class. It's just comes from
the properties of object oriented programming.
Once we have updated the main networks, we
want to go ahead and perform the soft update
of our target networks. And since this is
not the first time we're calling the function
that gets no input, so we'll use the default
value for tau of 0.005. And that is it 113
lines for our agent class. Now we're ready
to write up the main loop and test it out
to see how well it does. Okay, I have an error,
it says keyword cannot be an expression. Let's
see rewards to convert to tensor. Where have
I gone? Wrong? Right here, it's a period instead
of a comma. All right. Now we're good. So
let's go ahead and start with our imports.
We have Jim we have NumPy. We have our agent
and are plot learning curves. So go to my
GitHub, do a git clone and get those utils.
It is just a matplotlib function to plot our
learning curve, which is the score versus
time the running average of the previous 100
games. Over time, it's nothing magical. I
don't go over it because it's relatively trivial
doesn't really contribute to your understanding.
You can just do a plot of the scores over
time to see if it's learning 
So we start with our making our environment.
And we use the pendulum and one stanchion,
our agent, getting the observation space,
from our environment for input dims, passing
on the environment, shape. And we will use
the default value for our noise and every
other parameter because those were good defaults.
let it play 250 games. Let's go ahead and
to find a figure file, pendulum dot png, we
need to keep track have the best score. That's
the lower bound of our reward range. And to
keep track of the history of scores, Agent
receives, and a load checkpoint, that'll be
false. That's if you want to set up our training
versus testing. If we're going to load that
checkpoint, then 
we want to set number of steps to zero while
n steps is less thing agent dot batch size.
What we're doing here, okay, so I should explain
this. So my understanding and this could be
wrong. If it's wrong, drop a comment down
below to correct me, I don't profess to know
everything about TensorFlow. But from what
I've read from Google, the model loading is
set up such that you have to call the learning
function before you can load your model. That's
because when you instantiate the agent, you
aren't actually loading any values onto the
graph. And so it's basically an empty network
with no values loaded into that network, it
doesn't load any violations, he tried to do
something with it. And so we're going to go
ahead and fill up the agents memory with dummy
variables, dummy values, they don't really
matter, we're just going to go and load it
up with dummy values, and then call the agents
learn function so we can load our models.
And so we can do a random action doesn't really
matter. Get our new state, reward dawn info
from our environment. And then remember that
very important one increment number of steps.
Once you've done that, call the Learn function
and load your models. And said, evaluate.
Value eight true. And if we're not going to
be loading our checkpoint, we'll just say
evaluate equals false, I guess we could just
use load checkpoint in place of evaluate our
call it evaluate, but whatever I've used to
separate variable assuming. So for i in range
and games, we want to go ahead and reset our
environment. At the top of every episode,
reset the terminal flag and the score to zero.
While the episode is not done, Beijing can
choose an action based on the observation
and the evaluate flag. Good the new state
reward Don and info from the environment.
Incorrect our scoring, and store that transition
for not loading a checkpoint that we want
to learn. And the reason I put in that conditional
statement is because if you're evaluating
the performance of the agent, you probably
don't want to disturb its parameters, you
want to just go ahead and see how it performs.
As of the last time you saved it, rather than
trying to get to learn a little bit more.
Feel free to change that. And very importantly,
we want to set the current state of the environment
to the new state after the agent took its
action. So then we want to at the end of every
episode, we want to append the score and calculate
the average to get an idea of whether or not
our agent is learning If the average score
is better than the best known score, then
set the best score to that average score.
And, again, if we aren't loading a checkpoint,
go ahead and save your models. And at the
end of every episode, we want to print some
basic debug information. So I score 
and at the end of all the episodes we want
to print our plot our learning curve. So our
x axis number of games. Okay, that is it for
our main loop. Let's go ahead and test it
out to make sure I made a sufficient number
of typos. But as I said, the first thing I
want to do is make dir temp. Okay, I already
have that to make dir temp slash ddpg. I didn't
have that and make der plots. I think that
already exists. Okay. Now we can go ahead
and run the main file and see what I messed
up. Okay, so Oh, yeah, of course our critic
network 
does not get a number of actions. That's something
I changed on the fly. So let's fix that here
as well. We go ahead and put this back up
here. I think that is right. Let's try it
again. agent has no it's because its target
actor dot weights not target actor underscore
weights. That is in line 39. So right here
that I do. Nope, the target critic was correct.
Okay. has no attribute men sighs Okay. Oh,
it's in action. That's in line. 73. Yeah,
it's mean action. And Max action. Of course.
All right, that's in main line. 41. See, this
is why I have a cheat sheet because even even
with the cheat sheet, I make a number of typos.
So you can imagine what it's like if I were
to try to do it on camera that is in line
41 if not load checkpoint. There we go. Good
grief call takes three positional arguments,
but four were given all that's because I have
my parentheses in a wrong place. Okay. That
is in line 92 so the 
state's target actions so the parenthese goes
here. Yeah, that's right. Oh my goodness concat
missing one required positional argument Oh,
because I have the because I have an extra
Brenda z okay. So, this is bad, even for me,
line 23. So, then we need a second parenthese
there. Critic trainable underscore variables
once again that is a trainable dot variables.
That is in line 90. I thought I looked for
that. Oh, no, I didn't critic dot trainable
variables. I did the same thing here. Alright,
what I just fixed that, did I not? line 100?
Did I just fix that? No, I did not just fix
that. Right? Okay. Actor dot critic dots.
Okay, this is what happens when you don't
do this for a month. making YouTube videos
is a perishable skill. Okay, perfect. Now
what is actually working, I've got gotten
through all of those typos, and it has started
to run. So I'm gonna let this go ahead and
finish up and we're going to see how it does.
Alright, so it has finished running. And you
can see that at the end, it kind of tapered
off a little bit in performance. If we scroll
up, you can see that about halfway through
it was achieving record performance with pretty
much every single simulation that isn't entirely
a typical with actor critic type methods.
So oftentimes, what will happen is the agent
will achieve some reasonable performance and
then kind of started to taper off. Because
the as I said, the actor network is relatively
sensitive to changes in its parameters. In
this case, it didn't fall off a cliff like
I've seen with things like actor critic or
policy gradient methods, but it is still nonetheless
sensitive to changes in its weights, and is
prone to deteriorations of performance late
in the number of simulations. If you take
a look at the learning curve, you can see
pretty clearly that it has an overall upward
trend over time. So it is in fact learning
our technique is working. It's doing its job,
but it's not, you know, it's nothing, at least
for this environment. It's not the greatest
thing since sliced bread, we could do a little
bit more tuning to get it even better. But
for now, I think this is sufficient to a demonstrate
that it works and to be have a solid tutorial
on how to implement deep deterministic policy
gradients. Once again, shameless plug if you
want to know how to go from paper to code,
I show you how in my two Udemy courses on
deep reinforcement learning, where we go through
several papers per course, one on deep learning
one on actor critic methods, and implement
all these algorithms from scratch, I show
you how to implement pretty much everything
from the papers minus some super superfluous
features. Either way, if you made it this
far, please leave a like, subscribe, drop
a comment down below, and I'll see you in
the next video. Welcome to a crash course
in 20, late deep deterministic policy gradients
or TD three for short. This is a very brief
lecture that's going to cover the fundamental
concepts and implementation notes for our
coding tutorial, which will follow this lecture.
If you want the full details of how this algorithm
works, I highly recommend you read the paper
titled addressing function approximation error
and actor critic methods. It's very well written
very technical, very detailed, and it covers
significantly more detail and depth than I
will do here. I'm just going to give you broad
strokes, and some idea of what we're going
to be doing in our coding tutorial. And we're
going to be using TensorFlow too, by the way
for our coding tutorial, so make sure you
have that installed. So TD three exists to
deal with a fairly straightforward issue.
How do we deal with overestimation, bias and
continuous action space actor critic methods,
if you're not familiar with over estimation
bias is a tendency of agents to incorrectly
estimate the value of a state on the high
end. So it says this state is worth more than
it actually is. And this is a problem because
the agent will attempt to access that state
in the future, which leads to a suboptimal
policy because there are other more profitable
states out there. So this is a pretty big
problem, particularly when you're trying to
approximate the agents policy as we are in
actor critic methods. But it's not clear where
this would even come from. in Q learning,
we get biased because we take a max over our
actions in our update rule for the either
the table or our deep neural network. So there
is some maximization or overestimation built
in right from the beginning. And that's pretty
easy to see. But there is no max in our update
rule for actor critic methods. So what gives?
Where could this overestimation bias come
from? Well, on a somewhat theoretical level,
I kind of argue they don't do this in the
paper, but the way I think of it is that we
have Stochastic gradient descent, or we're
attempting to maximize the product of our
probabilities or policy and the rewards or
returns the agent receives over time. So there
is some implicit drive to maximize score over
time. And so when you get natural variation
in your rewards, or in the trajectory through
the state space, you can end up with some
incorrect estimations. of the visor states
because of natural, you know high variance,
which is typical for deep reinforcement learning
problems. However, more fundamentally, overestimation
comes from approximation errors. Now, this
isn't something they prove in the paper, because
this is a result that dates all the way back
to the early 1990s, when people were just
starting to talk about q learning. And so
it's been known for a while. But the basic
idea is that when you attempt to use some
mathematical apparatus, to estimate a function
in high dimensional space, you get approximation
errors. And that results in over estimation.
Now, what is our source of overestimation
or approximation error, in this case, it's
going to be a deep neural network. Neural
Networks are, of course, universal function
approximator. Mostly, there are obviously
exceptions, you know, discontinuous functions,
they can choke on the function has to be differentiable
and all that good stuff. But for all the stuff
we deal with, neural networks are going to
be a universal function approximator. And
so that leads to a source of error. Now, this
is inherent to any approximation method, either
tiling, like, binning anything like that.
So it's not that neural networks are bad,
it's just whenever you have a function approximation,
you're going to have some error. Okay, because
you don't have an infinite amount of time
to collect an infinite number of samples to
get infinite precision, you're always going
to have to lop off your estimate, at some
point where there are more decimal points
waiting for you, should you be able to collect
more data. Worse yet, actor critic methods
are bootstrapped? Well, this means is that
we're going to be using our initial estimates
to perform updates to later estimates. And
so you have some errors in your early estimates
that propagate to your later estimates over
time, so you get an accumulation of error
over time. Naturally, there are other ways
of dealing with this. And in fact, double
q learning uses a rather ingenious solution
they use in that algorithm, two different
cue networks, and you alternate their use
in the update rule. And so you're never taking
a max over actions of the same network that
you use to choose an action when you're trying
to update the value of that action. And so
it seems like it's a reasonable thing to try.
So in particular, that's what they do. Now,
I want to point something out that in double
q learning, deep, double, deep, deep double
q learning, they use a not exactly analogous
solution to the tabular case of double q learning,
they use something slightly different, which
doesn't work an actor critic methods, as they
detail in the paper, they're going to use
a more exact replica of the tabular version
of double q learning. And they're going to
perform a slight modification where they're
going to clip the inputs of the actions to
the cue networks, the double cue networks,
which is going to tend to underestimate and
we're going to be taking a min. So later on,
you'll see in the algorithm that we're going
to feed some clipped actions to our cue networks,
and then take the minimum. So whatever the
minimum value is, we're going to take that
which tends to underestimate. Now you may
say, Dr. Phil, isn't that a problem? Not really.
And the reason it's not a problem is because
if we underestimate actions, those actions
become less attractive to the agent. And so
it's not likely to take those actions again.
And so that kind of dampens that out over
time. It's a natural feedback mechanism to
deal with that issue. So that is rather nice.
And it's baked right into the algorithm. The
other innovation is that they're going to
delay policy updates, to give the credit Network
Time to converge. So the Policy Network is
very slowly changing function, or as the queue
network can change much more quickly. So you
have two different timescales there. And that's
where the delayed part comes from. Another
innovation is we're going to use target networks
for the actor and both critics. Since we're
doing an analogue of double q learning, which
uses two cue networks, it stands to reason
we're gonna have two critics. And we're going
to target networks for all the things so we're
going to have an actor, a target actor, to
critics, and to target critics. So total of
six deep neural networks, we're gonna have
all the neural networks in the world. The
other thing we're going to need is a salt
update to the target networks. So you have
a couple of options for updating the weights
of retarget networks. One is you could do
Stochastic gradient descent on those directly.
Another option is to take the parameters from
your online networks that you're actively
performing gradient descent on and copy those
to the target networks directly. Or a third
option is to do some slowly varying interposed
between the two. So you're going to update
the online networks every time separately
n time steps in the case of the policy, and
then you're going to do some slowly Changing
update to those target networks. And that'll
introduce an additional hyper parameter called
tout, or agent, which you will see later.
So this is an actor critic method. And we're
going to have actually, six distinct, I have
a typo there. Sorry, I can't count despite
having a PhD in physics. So we're going to
have an actor to critics target actor. And
then to target critics. As I said, the purpose
of the critic is to be critical it is to evaluate
the values of states and action pairs. So
it says, Hey, in this particular state, we
took some action, I think this was valuable
or not. And that will help to update the agents
estimate of the VI's of those pairs, and to
choose better actions for given states over
time, the actor will decide what to do based
on the current state. And it's important to
note here that our deep neural network is
going to output action values, which are continuous
numbers, not discrete numbers, not probabilities.
Now, that makes us a deterministic algorithm.
That's where the name of the algorithm comes
in twin delay deep deterministic policy gradients
is an extension of deep deterministic policy
gradients. And so we have a bit of a problem
there. Because we're dealing with approximation,
right? We're approximating a cue function,
we're approximating a policy, we're approximating
all the things, and we never know exactly
how accurate our approximations are. And so
we never know if we are correctly evaluating
states and action pairs. So we never know
if given some state if this action is really
the most beneficial action we could possibly
take. Or if there's some other, more beneficial
action out there waiting to be discovered.
That is called the Explore exploit dilemma.
Do we explore sub optimal actions or exploit
what we think are the most beneficial actions.
And the extent to which you engage in those
two activities is the dilemma. And there are
a number of solutions to that deep learning
uses epsilon greedy action selection, where
you just take random actions, some fixed proportion
of the time, but in this case, we're going
to be adding noise to the output of our actor
network, when we decide what actions to take.
The update rule for actor looks a little bit
scary, but it's actually not. So this is the
update rule from the paper j is going to be
our loss function for our actor. It's a function
of phi, the parameters of the Policy Network.
And you want to take the gradient of the cost
function with respect to the parameters of
the Policy Network. And it's given by one
over m times the sun, the sum, or a mean an
average. So our loss function is going to
be a mean. And it's going to be the product
in this particular equation of the gradient
of the first critic network with respect to
the actions chosen by the Policy Network,
multiplied by the gradient of the policy network
with respect to its parameters. Now, this
looks intimidating, but it's actually not
this is the application of the chain rule
to the loss function. So they have taken the
gradient of the loss function with respect
to phi. But the loss function is proportional
to the output of the first critic network.
Of course, the first critic deep neural network
has its own set of of neural network parameters,
it doesn't have an explicit dependence on
the neural network parameters of the Policy
Network. So it's very difficult to take a
gradient write something that doesn't depend
on something else. So the dependence is implicit,
it comes from the fact that those actions
are chosen according to the output of our
policy network. And so you have to apply the
chain rule. In reality, all we're going to
do is the following. We're going to randomly
sample states from our memory, we're going
to use our active network to determine actions
for those states. So we're not going to be
using the actions from our memory, we're going
to figure out what actions the agent thinks
we should take. Now, we're going to plug those
actions into our critic and get some value,
specifically, the first critic never the second
only the first, that's just by design. And
then we're going to take the gradient with
respect to the accurate network parameters.
Now we don't have to calculate that gradient,
TensorFlow is going to do it for us, we just
have to do the first three things where we
have sample states. Use the actor determined
actions for those states and plug those into
our critic along with the states to get some
value, and then take the gradient with respect
to the accurate network parameters. Now, keep
in mind that this update isn't performed every
time step is performed every other time step.
How often you perform it is a hyper parameter
of the algorithm. But it is not every single
time step. Now, nominally the update rule
for the critic is a little more straightforward.
So again, you're going to sample a batch of
transitions from your memory, you're going
to put the new states that the agent received
observed after taking some action through
the target actor network, that's at a tilde
a parameter. And then you're going to add
in some clips noise, it's just going to be
a normally distributed noise with mean zero,
and some standard deviation, something like
0.2. And we're going to clip it in the range
of minus 0.5 to positive 0.5. So that's where
the clipping comes in for our double q learning,
and the actual double q part comes in. And
when we calculate our targets y, so we're
gonna take the reward that we sample from
our buffer, and add it to the product of the
gamma, which is the discount factor 0.99 or
so. And we're going to take the minimum of
the output of the two target critic networks.
So we'll say, we're going to feed the new
states through the new states and those clipped
actions through both target critic networks.
And we're going to see which one is the minimum
and take that for our target value. And then
we're going to input that target value into
our loss function. Again, you have a one over
n multiplied by some multiplied by something
squared, that has a mean squared error. And
it's the mean squared error between that target
y and the output of both of our critic networks.
So our loss is going to have two different
components, it's going to have a loss for
critical one, we have q sub theta sub one,
and then our have a loss for Q sub theta sub
two. So our two losses, and in TensorFlow
two, when we do our gradient tape, we're gonna
have to pass in that persistent equals true
flag to our function call, so that it keeps
track of network parameters between gradient
ascent steps. And so these, the rest of this
verbiage is just kind of the verbal description
of what we want to do, it's going to be much
easier once you see it written in code, I
assure you, it's not that difficult. Next,
we have to handle the question of target network
updates. So at the very beginning, in our
constructor for our agent class, we're going
to go ahead and initialize actor to critic
networks, and then to target then a target
actor and to target critic networks. And we
first start out, we want to initialize those
target networks with the exact values of the
online networks. And so we're gonna have a
special case in our target network update
function that handles the very beginning of
the program. every other time step, we're
gonna use the following expression to update
the weights. So on the left side, you have
theta and five prime where the AI on the theta
denotes either critic one or two, phi is the
parameter for our critic network, excuse me
Policy Network. And the thetas are the parameters
for our critic networks. And so you're going
to multiply town some small number of point
005, in this case, by the values of the current
online network, and add in one minus tau times
the old values of the on the critic network.
So it'll be a small number multiplied by the
current values of your online networks plus
something that's almost one, multiplied by
the old values of your target networks, it's
going to be a slowly changing update to our
target networks. Another thing to note is
that we're only going to be performing this
update when we update the actual network.
So it's not every time step. In this case,
it will be every other time step very, very
important. So for this program, we're going
to need a number of data structures, we're
going to need a class for our replay buffer
the agent's memory. Now, I like to use NumPy
arrays is not the best way or the only way
to do it, it's just my preferred way. So follow
along with me in the tutorial, do it that
way. And then when you play around with the
code later, to understand it better, go ahead
and rewrite the replay buffer to something
that makes more sense to you. That's a great
way to get started with modifying the program
is with the replay buffer. Next, we're gonna
have classes for our actor network and our
critic network. And those are of course written
in TensorFlow two, we have another class for
our agent, and that is really going to tie
everything together, it's going to have a
memory that keeps track of transitions, it's
going to have an actor to critics target networks
for each of those a function to choose an
action based on the current state, a function
to learn that performs the update rules, we
just went over an interface function with
his memory that I call remember, just to store
transitions in the agent's memory, as well
as functionality to save models and perform
target network updates, which I forgot to
write here. Finally, we're gonna need a main
loop to train and evaluate our algorithm.
So we're going to be using the open AI gym
and the bipedal Walker in particular, because
this is a kind of difficult environment for
other algorithms. Now, it's a continuous action
space with a pretty large state space. I think
it has 24 different components in the state
space, if I'm not mistaken, some relatively
large number. So it's a bit difficult for
agents to learn. And in fact, it's going to
take my computer around six or seven hours
to complete the evaluation. filming this after
I do the code, so it'll take a while to run.
So if it takes forever on your computer, don't
Don't panic. That's normal, quite normal.
So all that out of the way, let's go ahead
and get started in the coding portion of this
tutorial. Alright, so we begin as usual with
our imports, we will need NumPy the base TensorFlow
package, we will need TensorFlow dot Kairos.
We will need layers from important dense that
is for constructing our deep neural networks.
And we will need our atom optimizer for the
gradient descent. And we will need LS for
file joining operations for model checkpointing.
Let's start with our replay buffer class.
Now, this should be very familiar to you for
deep use to deep q learning will need a max
size and shape and number of actions as input
to our constructor. Now remember, we're dealing
with continuous action spaces. So there's
number of actions is really a number of components
to our continuous action. I just named it
that for consistency with my deep q learning
code. So we will save the appropriate number
of variables. We use a memory counter instantiated
at zero because our memory is finite. We'll
need our state memory, which will initialize
as zeros. The shape memory size and star input
shape the star idiom just unpacks a, an array
in this case, whatever the shape of our input,
dimensionality is from our environment, we
will of course need a new state memory. And
that's the same shape that keeps track of
the new states that we are going to see. And
actually memory. And remember, of course,
again, number of actions is number of components
to or action. Reward memory in the shape of
memory size, and a terminal memory. We're
going to use NumPy NumPy NumPy bool as our
data type, but don't keep track of our terminal
flags, the reason being that the value of
the terminal state is always zero. And so
we keep track of the done flags from our environment
to accommodate that. So let's store a transition
takes the state observed action taken reward
received new state observed and terminal flag
received as input. We want to know what the
first available memory position is. And that's
the memory counter modules mem size that has
a property that it will overwrite earlier
memories with newer memories as soon as the
memory fills up. And then go ahead and save
our variables. All state underscore trawl
memory. And that's all of them I believe.
And very important, we want to increment our
memory counter by one. Next we have to handle
the function to sample our buffer. And that'll
just take a batch size as input. We want to
know what the position of our maximum filled
memory is. And that's given by this minimum
of memory counter and men size. Because we
don't want to sample zeros, we initialize
our memory with zero so we just sample the
entire buffer, then we're probably going to
end up sampling zeros until we fill up that
buffer which is totally useless. So then the
batch is going to be a random choice zero
to maximum in shape batch size. Then go ahead
and do you reference our variables 
and DUNS All right. wraps up our replay memory
that's very, very simple, probably the most
straightforward class in the entire project.
Next, we're gonna move on to our critic network.
And that will derive from Kerris dot model.
So we get access to all of the properties
of that particular base class. That'll help
with using the gradient tape for learning
later on. We're going to take some inputs
for the number of dimensions for the first
and second, second fully connected layers,
number of actions again, number of components,
a name for the purpose of model checkpointing,
and a checkpoint directory, we will have to
do a make directory on that Before you begin,
otherwise, you will get an error and it will
not work. Call our super constructor and start
saving stuff. Do I need to do that? Now why
not. So the purpose of the name is the fact
that we're going to be saving target networks
as well as regular networks. And they're going
to be to critics. So we want to be able to
keep all of those straight when we handle
model checkpointing. So we'll save our model
name, checkpoint directory, and the checkpoint
file. And I like to append the algorithm name
to the checkpoint files so that if I do everything
in one working directory, when I'm experimenting,
all the names, tell me exactly which file
correspond to which algorithm, you don't have
to do that. It's just my own personal convention.
So for our deep neural network, we'll start
with a dense layer with a rail you activation.
Second dense layer with value activation and
an output that will be single valued with
no activation. Now keep in mind, one interesting
thing about TensorFlow two is that we don't
have to specify the number of input dimensions
it infers it from the inputs. That's a pretty
nice feature. So now we define our feed forward,
on this case, we call it call and allow us
to use the name of an object as a function
call and basically need a state and action
as input will have a q1 action value. And
we will want to concatenate our state and
action along the first axis. And we will feed
that through FC to q1 action value. pass it
through the final layer, action, value, and
return. Now keep in mind that the critic evaluates
the value of both the action and state. So
that's why we have to concatenate the two
values. That is it for our critic network.
Very, very straightforward. Next, we're going
to handle our actor network. And that, again,
derives from Kerris dot model just the same
as a critic network. Our initializer takes
dimensionality as input again, as well. number
of actions. And you know, what did I I'm sorry,
I'm checking something here. No, I did not.
For a second there, I thought I passed in
in good shape to the critic network that would
have been totally unnecessary. checkpoint
directory equals temp, TD three, we want all
of the models to live in the same directory
very, very helpful. And then we can go ahead
and start saving stuff. Looking at this and
as I look at the what I'm doing here, so I
like to modify stuff on the fly. I don't think
I actually need this actions here. So let's
go ahead and delete that. Just for the sake
of cleanliness. We will need it on the actor
of course. Yeah, let's keep it nice and clean
model name, checkpoint directory, plus TD
three. And just for clarity, that name will
have stuff like Target actor, target critic,
actor or critic, so we can keep all of those
particular networks straight. And of course,
the two critics will be critic one or critic
two, because we have some very interesting
naming conventions. So now we'll have our
deep neural network. Again, a simple dense
layer with raw you activations for the first
two layers, and then you for our output. And
that will take actions as our output dimensionality,
with a tan hyperbolic activation, the tan
hyperbolic is bound between minus one and
plus one, if you want to take into account
boundaries of actions that are beyond plus
or minus one, you can multiply this output
by the maximum balance for your environment.
So some environments have a max action of
plus or minus two, which of course, plus or
minus one is oftentimes less than, you know,
two. So you want to take that into account
depending on the environment. So again, we
did a call function to handle the feed forward.
So pass our state through the first fully
connected layer, second, fully connected layer,
and pass that through the final layer and
return it. So that is it for our actor network.
Next, we need an agent class to tie everything
together and to handle all the really interesting
functionality. So our agent doesn't derive
from anything. But our super constructor,
excuse me, our constructor is going to take
a whole slew of inputs. So we need a couple
different learning rates. Reason being you
want to accommodate the capacity for different
learning rates for your actor and critic network.
Sometimes they learn best with different learning
rates. Input demos, you'll need that for your
memory towel for your software update rule,
your environment for a number of important
variables from the environment. Default gamma
is 0.99. The update act date up date actor
interval will default it to every other iteration,
a warm up of 1000 steps. Just a default value
for an actions max size of a million transitions.
layer one size 400 layer two size that's RFC
one and two tins respectively. A batch size
default on 300 and a noise of 0.1. Let's go
ahead and start saving stuff. Since we will
be adding in noise, we're gonna have to perform
a clamping on our actions to make sure that
the actions the action plus the noise don't
fall outside of the allowable bounds of the
environment, or below our memory, that's a
replay buffer. Then we need batch size. We
need a learn step counter that will need that
because we're doing the delayed part of TD
three we're going to delay the updates of
the actor network. Every two every two different
iterations of the update of the critic network
to get the critic network time to converge.
Then we have you know, I'm looking at my cheat
sheet here are all a time step. Why do I have
a time step? Excuse me one moment now I believe
the time step is for the warm up procedure.
We shall double check that later. If not,
I'll come back and delete it in the GitHub.
And we don't want to forget number of actions.
Sorry, I write this code you know sometimes
well in advance of doing the video because
I get distracted by other stuff. And so when
I come back to it, I don't always know what
I was thinking that is a benefit of comments
which I don't really do for this stuff. Sue
me I probably should. We do need our update
after iteration. And that is update actor.
Interval excuse me, let me close my door.
My toddler is rampaging. And next we can go
ahead and start defining our actors And critics
and the name will just be actor. Because we
are quite creative. To maintain compliance
with the pepp eight style guide. Let's go
ahead and delete a couple spaces. Critic one
is a critic network or one size layer to size.
And we don't need number of actions there
because I deleted it. So its name will be
critic one. Likewise for critic two and again,
the purpose of this is to handle the double
q learning update rule. Next, we will need
a target actor layer to size. Let me go ahead
and delete spaces. What silly style guides
and finally and name of target actor then
we'll need target predict one 
with a very original name of target critic
one. Similarly, critic net target critic two.
And that is it for our network's Next we have
to compile them because this is a TensorFlow
two. And that is where our learning rates
come into play. So we will use our atom optimizer
with the learning rate defined by alpha for
our critic, our loss would just be a mean.
And our critic one learning rate of beta a
loss of mean squared error and critic to have
to do the same thing. Where equals beta do
I need? Yes, I do. I do need to parentheses
there. mean squared error. And then we will
handle our target networks ness. Target networks
nest. Next that is a tongue twister. So we
have to compile the target networks, just
by convention with TensorFlow two, we're not
going to be performing any Stochastic gradient
descent or Adam. In this case, on those particular
networks, we're gonna be doing the salt network
updates, but we still have to compile them.
Nevertheless. That is just by convention.
We read alpha loss equals mean target critic
one. Okay, so that is all of our networks.
So that noise will keep track of as well update
network parameters with a default value equals
one. I do that because on the first step of
the update, we have to set the values of our
target networks equal to the starting values
of the online networks. And so we pass in
a value tau equals one to perform an exact
update or a heart update instead of a soft
update. We'll handle that function toward
the end. For now I want to get to the Choose
action remember and learn functionality because
that's where all the really interesting stuff
is. So let's go ahead and choose an action
based upon an observation of the current state
of the environment as input. So far, a time
step less than our warmup period. I yeah,
that's why we need the time step to handle
the warm up. As I suspected, lad, I didn't
delete that, we're going to select an action
at random, with just a normal distribution
with a scale defined by our noise parameter
in the shape of number of actions, comma,
so we get a batch fare. And sorry, an array
of scalars. Otherwise, we want to go ahead
and convert our state to a tensor. And add
on a batch dimension, that's just all the
way that the inputs are expected to be fed
into the deep neural network, we have to add
that batch dimensionality. And I have to float
32 here must be due to some sort of precision
thing that makes TensorFlow happy. So then
when we want to pass our state through our
actor network, and receive our mu, and we're
doing that because it returns batch size of
one, one scalar. Then we'll say mu prime,
which is where we handle the noise equals
mu plus MP random, normal scale equals self
dot noise. And your prime is TF clip. Bye
bye, because again, that noise could take
us outside the bounds of our environment.
So we'll plant mu prime between min action
and Max action, increase our time step by
one very important. And we want to return
mu prime. Okay, that's it for our choose action.
Now let's handle the simple interface function
to remember a transition. So remember, state
action reward, new state done. And we'll say
memory dot store, transition, state action
reward new state done nice just because we
have to interface with the memory in some
way, we don't want to have the agent class
calling. You don't want to have the agent
class interacting with private variables from
your memory, that would be poor software design.
So next, we handle the most interesting function
in the whole program, which is the Learn function.
And the very first thing we want to do is
say, hey, if we haven't filled up at least
batch size of memory, we probably don't want
to be learning. So we'll say self dot memory,
that meme counter loss and batch size. Now,
if it's not, if it's not greater than the
batch size, or equal to go ahead and return
essence, we're doing a warm up, that won't
be the case, because the batch size is just
a few 100, the warm up is 1000. So by the
time we get through the warm up, then we're
already well into filling up batch size of
memories. But if you decided not to do a warm
up, then that would be important. So we'll
start by sampling our memory. So memory, sample
buffer, pass in our batch size. And then we
want to convert all of those to TensorFlow
tensors. And I have to be very pedantic with
data types here. I think. There could be issues
if you do not, as I recall, I think it barks
at you about data types, because it expects
certain types of floating point variables
in some places and other types elsewhere.
And we don't have to convert the Dunn's to
a tensor because rod sticking that in the
deep neural network, we're just using that
as a multiplicative factor. So we can leave
it as a NumPy array. Now we're going to handle
our update to the critic network, because
we do that every time step. And then we'll
handle the update to our actor network. So
we'll say with TF three and tape, and I'll
have persistent equals true Oh, because I
have two different networks. Yeah, so you
need two different if you're using two different
updates for one group, excuse me, we're using
two different apply gradients for a single
tape. And you need to pass any persistent
equals true variable, parameter excuse me,
or argument. Otherwise, you don't need that
versus n equals true. We just have say a single
network that you're performing an update on.
So we want the actions according to Our target
actor for the new states. And then we're going
to go ahead and add on a noise parameter to
that, that we're going to clip between the
range of minus point five and positive point
five. So clip by value MP random, normal 0.2
minus 0.5 4.5. And then we're going to go
ahead and clip that again, because again,
the addition of that noise could take the
action outside of the bounds of our environment.
And then we are free to go ahead and start
calculating our critic values. So q1 underscore
the critic value, according to the first target
critic is the feed forward of the new states
and target actions through the first target
critic, cue to underscore is very similar.
It's just the evaluation of the new states
and target actions according to the second
critic. Now, again, we're gonna have to go
ahead and squeeze that output. And the reason
is, is that our shape is batch size by one
want to collapse to batch size. And we have
to do that for q2 as well, excuse me, q2 underscore.
And then we're going to need the the value
of the states and actions the agent actually
took, according to the regular critical one,
excuse me, one and two networks. So we'll
call those q one. And we'll just go ahead
and squeeze those right away. Critic one state's
actions squeezed along the first dimension
critic to states actions, one and then we're
going to say that our critic value for the
new states is the minimum of q1 underscore,
q2, underscore and then we're going to need
our target value the Y from our paper rewards
plus gamma times critic value times one minus
dunnes that will set the value of the second
term here gamma times critic value there should
be an underscore there sorry to zero everywhere
the done flag is true. And then we have our
losses. So critic one loss karass losses got
mean squared error between the target and
Q one mean squared error target and que tu
so that is it for the calculation of the critic
losses. Now we have to handle the calculation
of the gradients. Now, we don't have to do
anything special for that. The TensorFlow
package handles that for us. So tape dot gradient,
the gradient critical one loss with respect
to the critical one trainable variables. So
I should be dot not an underscore I make that
mistake frequently. Critic one critic underscore
one dot trainable variables yeah that is right.
And then we need the critic to great critic
to loss critic to trainable variables Same
deal then we need to go ahead and apply those
gradients. So calling our optimizer dot apply
grip gradients function and that expects a
zip as input. We're going to zip The Critic
one gradient and the self critic one trainable
variables simply recruited to optimize or
to gradient Okay. And then we want to increment
our Learn step counter Because that gets incremented
every time we update our critic networks,
and then we have to address the question of,
is it time to update our actor network. So
we'll say if that learns step, counter modulus,
self update actor, interval, it or sorry,
is not equal to zero, then return so it's
not every n steps, then go ahead and return.
And if we haven't returned, then we're going
to go ahead and calculate the loss for our
actor network. So with TF gradient tape, as
tape, and here, since we're just dealing with
one loss, we don't have to call the processing
equals true. We don't have to pass in the
verses and equals true argument. So what's
our new actions are the actions chosen by
the current parameters or actor network for
the current set of states, the states the
agent saw along the way, the critic one value
is self critic, one of those states and new
actions. And then our actor loss. There's
negative TF math reduce mean, critical one
value. And this may look a little strange
to you. But this is how we're going to handle
the gradient of the output of one network
respect to the parameters of a network, it's
kind of like how you apply the chain rule
to the to the loss of the output of the critic
network with respect to the parameters of
your target actor, your actor network, sorry.
Oh, that makes sense. So then we do the same
thing where we calculate our gradient gradient,
tape dot gradient. Sorry, taped out gradient
that I call, let me make sure I didn't make
a mistake, I did make a mistake up here. Sorry.
So this should be taped out gradient. Not
TF. That is one less error to worry about
when we get to running the program. Sorry
about that. So take that gradient, factor
loss factor trainable variables, step our
optimizer by applying our gradients, again
a Texas zip as input actor, gradient cell
dot actor trainable variables. Okay, so then
finally, at the end of the learning function,
we want to update our network parameters.
Okay, so that really handles all of the learning
algorithm for our agent. All that's left now
is to update our network parameters, and then
handle the model saving so just a few functions
left and then we can go right our main loop
and see how it does. So network parameters.
So we're going to pass in a default value
of towel, Fernand, remember that at the top
of our rather the end of our initializer we
pass in tau equals one to handle heart update.
every other time we're going to pass in a
nun. So we'll say if tau is none, then tau
i go self dot pal. So every time other than
the first time we call this we're going to
use the stored value for towel. So weights
equals the list targets equal self dot target
actor dot waits, I wait in enumerate self
actor waits waits dot append, wait times tau
plus targets sub i times one minus tau. There
we go. And then sell dot target actor dot
set weights and I am going to yank this and
paste and paste again and then say target
critic. One self dot critic one weights. Set
wait Sorry, I forgot to set the actual via
the weights how sloppy of mean, and then say
CELTA Target critic, one set weights weights.
And then we have target critic to numerate
critic to weights, weight start append. And
then so got target critic to set weights.
And that handles our update rule for our two
networks. Sorry, my Num Lock key is off there.
Okay, so if it isn't clear what's going on
here we are iterating over the weights of
our actor and critic one critic two networks,
then we're doing the calculation for the soft
update rule saving that in a temporary list
and uploading that list to the target actor
or target critic one or critic two networks.
Now, we can handle the Save model functionality.
This is the easiest part of the whole project.
So print saving models just a little debug
statement to let us know something is going
on. Save weights self dot accurate a checkpoint
file that critic one checkpoint file, checkpoint
file, and then we have our target numbers
as well. Good grief. Then we do the inverse
operation of loading our models. So let actor
load weights from the accurate Check Point
file. We have critic one load weights. So
blog critic one dot checkpoint, firewall.
And then we have our target networks. I should
have chosen shorter variable names to save
my risks a little bit of work here. But hindsight
is always 2020 I guess. Target one, check
one file. Okay, so that is it for the main
code for our TD three algorithm. Now we get
to handle the main loop. So let's go ahead
and code that up. Of course, before we can
have an invalid syntax right here at the very
beginning. I'm TensorFlow dot Kairos dot layers.
Oh, sorry. That's because it's a from import
dense. I'll have to notate that in the video.
And I have another issue here. def store transition,
where is it unhappy? I am missing a comma.
Of course. And I have another issue Oh s path
joined. name equals my wrists are nonfunctional
today. Cell dot target actor dot set weights.
Oh, ah. Why did that happen? Interesting.
Did I type those at the end and have a stroke
or something to remember very strange Okay,
unexpected and a file, let's delete their
same. am I forgetting a parentheses somewhere?
I am. Because I have right there. All right,
finally, good green. That's a whole lot of
typos. Now some people suggest that I upgrade
my vim to actually catch that stuff on the
fly. And you're absolutely right, I'm gonna
do that. When I finally forced myself to do
it, let's go ahead in the meantime and write
our main loop. So we want to import Jim NumPy
we'll need our agent. And we'll need our utility
file plot learning curve, you can do a git
clone on my GitHub to get that it's just a
map plot live pie plot with some labeled axes,
where we're taking an average of the previous
100 games running. I don't include that in
all my videos, I just kind of reference my
Get up. You can just do a plot, if you wish,
name equals main gym dot make. We're going
to be doing the bipedal Walker, v2. And we're
gonna call our agent constructor, alpha 0.01,
a beta of 0.001. Our input dimensions will
be determined by our environment. So we don't
have to hard code anything. Tau of 0.005 pass
in our environment, batch size, I have 100
here equals 400 300. And n actions determined
again, by our environment, all this bad would
play 1000 games, and the call our file name
plots, plus locker underscore. Now keep in
mind, you have to do a make der plots and
make your temp slash gt three to correctly
execute the code. Because it'll expect that
those directories exist. Pass a number of
games as a variable for your file name. That
way, if you run it over and over again, with
different numbers of games, it won't overwrite
the same plot. You can also include things
like learning rates as part of your variable
name for your file names, I recommend doing
that. I'm just not doing it here. So we need
to keep track of our best score subminimum
the score of our environment. And the reason
is, we want to save our best models. Keep
track of your score history. If you want to
load models, now's the appropriate time to
do that. Actually, you know what we may need,
there may be an issue where we have to actually
instantiate our network with some variables
to load the models. Open up an issue on my
GitHub. If that's the case, and I will write
the correct code. I won't bother with the
video, I'll leave that as an exercise for
the viewer. But if it turns out to be a problem,
raise an issue and I can fix that. Not a huge
deal. Let's go ahead and play our games. Start
by resetting the environment at the top of
every episode, we set the done flag and zero.
And let's play our episode of Walmart done.
action equals agent dot choose action based
on the observation. Let's take that action,
get the new state reward Don and debuginfo
from our environment, call our Learn function.
Keep track of our score and set the current
state to the new state very important. If
you don't do that, nothing is going to go
well for you. As a punter a score at the bottom
of every episode and calculate our average
minus 100 onward. If our average score is
greater than our best score, then set the
best score to that average. And save our models
and then we want to print some debug information
episode on Score, one of average score 
average score. At the end of all the games,
let's go ahead and handle our plotting our
x axis is just the number of games and called
plot learning curve. Alright, so moment of
truth. Let's go ahead and see where I have
my invalid syntax. I forgot an addition sign
there. Very simple. Alright, let's try it.
Oh, you know what I'm running it over in this
other terminal. And okay, so it saves models
it is learning. That is good to know. Let's
do a make der temp slash TD. Three. I should
already have plots. Python main Td three.pi.
Moment of truth. Okay, so it says actor network
object has no attribute checkpoint file. I
didn't Oh, it's checkpoint directory. So that
is in TD three. Do you have to that is in
line 70. So that is here. Okay. Try it again.
Got an unexpected argument name. That is in.
Oh, that's because Okay, that is in line 48.
That is super trivial. Thought name equals
hits name. Plus that I do that down here as
well. No, I did not. Not looking forward to
editing this, this is going to be a lot of
work. Okay, so it saves models right off the
bat and starts running. Okay, so I'm gonna
let this run for a while. And then we're gonna
see how it does. Okay, so I did something
very stupid. And I let it run and noticed
it wasn't actually learning. And so that's
a problem. And the reason it's not learning
is because I forgot to store the transition.
So we have to say agent, remember, observation,
action, reward, observation, underscore and
done. Okay, and then I got to get rid of the
print statements I stuck in here, for debug
purposes, because I'm a noob. And use debug
statements. Okay. Now let's do Python, main
Td three.pi. And now it should work without
any funky print statements. Okay. Now I'm
going to take off for a little bit and see
what's going on. And the reason I noticed
this wasn't learning is because it was executing
much too quickly. I blasted through 350 games
in just about a minute, which tells me it's
not doing anything useful on the GPU. So let
this run out, or probably take an hour or
two, and then I'm gonna come back and see
how it did. And we'll take a look at his performance.
Now, here we are, it's the next morning. This
took around six or seven hours to run. So
I just waited until morning to film this.
But in typical Phil's style, the filename
for the function call for the plot learning
curve function has a typo in it. And so we
don't have an actual plot from the performance
of this particular run. However, I will show
you a plot of a similar run, where it achieved
an approximately similar score, you can see
that it does indeed learn it issues a high
score of around 285 to 88, about 290 or so
depending on the run, you get some run a run
variation. And 300 is the highest possible
score you can get for this environment. So
I would consider this pretty much strong evidence
of learning. It's not a world class results.
But that's not what we were aiming for any
way we just wanted to understand the gist
of the algorithm and implement it correctly,
and demonstrate that we do in fact understand
how it works. Mission accomplished. You can
pat yourself on the back for that. I'll also
show you some footage of the walker kind of
stumbling along so you can see how it looks
once it's fully trained. You can see it has
kind of a funny gait, but it does in fact
managed to learn to walk that is pretty impressive
starting from just totally random actions,
learning how to walk within just six or seven
hours only humans. Were so competent. I hope
that was helpful. Leave a comment down below
with any questions, suggestions, anything
you'd like to see next, give a thumbs up,
subscribe if you haven't already, and I'll
see you in the next video. Welcome to a crash
course in proximal policy optimization. Before
we begin a quick shameless plug my Udemy courses
on deep reinforcement learning specifically
actor critic methods and deep q learning are
on sale right now, learn how to turn papers
into code, link in the description below.
So proximal policy optimization or PPO for
short, was created for pretty simple reason.
And that is that an actor critic methods,
oftentimes we see that the performance can
fall off a cliff, the agent will be doing
really well for a little while. And suddenly,
an update to the neural network will cause
the agent to simply lose. Its its understanding
of how to play the game. And so performance
tanks and never really recovers. Now this
happens because actor critic methods are incredibly
sensitive to perturbations. The reason being
that small changes in the underlying parameters
to our deep neural network, the weights, for
instance, can cause large jumps in policy
space. And so you can go from a region of
policy space where performance is good to
a region of policy space where performance
is bad, just by a small tweak to the underlying
parameters of your deep neural network. PPO
addresses this by limiting the updates to
the Policy Network. It has a number of mechanisms
for doing this. But the basic idea is we're
going to base the update at each step on the
ratio of the new policy to the old. And we're
going to constrain that ratio to be within
a specific range to make sure we're not taking
really huge steps and parameter space for
our deep neural network. Of course, we also
have to account for the goodness of state.
In other words, the advantage how valuable
each state is, and the reason being naturally
that we want the agent to select states that
are highly profitable to it over time, so
wants to find the best possible states. Now
taking into account the advantage can cause
the the loss function to grow a little bit
too large. And so we're going to be introducing
a way of dealing with that by clipping the
loss function and taking the lower bound with
the minimum function. Something else we're
going to be doing that's different than what
you may be used to is that instead of keeping
track of something like say, a million transitions,
and then sampling a subset of those at random,
we're going to be keeping a very small fixed
length trajectory of memories. And we're going
to be doing multiple network updates per data
sample using mini batch Stochastic gradient
descent. It's worth noting that you can also
use multiple parallel actors on the CPU something
like what you would do in a three C, but we're
not going to deal with that, in this particular
tutorial, I'm just going to show you how to
do the GPU implementation. So let's talk about
the mini batch gradient sent for a second.
So we're going to keep track of a list of
memory indices from say zero to 19. And that's
for the case of taking a look at 20 transitions.
And let's say we want to take a batch of size
five. And so those batches could start at
position 05 10, or 15. Those are the only
possible positions were distorted such that
you get all the memories, you don't get any
overlap, and that it all works out evenly.
So what we're going to do is we're going to
shuffle our memories, and then take batch
size chunks, so we'll start at position zero,
from zero all the way up to four, that is
one batch. And then position four, five, up
to nine is the next batch, and so on and so
forth. It's relatively straightforward when
you see it in code, but it's kind of difficult
to explain as you're coding it. So just know
that we're taking batch size chunks of shuffled
memories for mini batch Stochastic gradient
descent. Other things we need to know is that
we're going to be using two distinct networks
for actor and our critic instead of having
a single network with shared inputs and multiple
outputs. Now, you certainly can use a shared
input with multiple outputs, but it complicates
the loss function a little bit. And I found
that performance is generally adequate with
two distinct networks for simple environments.
So the critic will evaluate the states that
the agent encounters and it gets the name
critic because it literally criticizes the
decisions that the actor makes, based on which
states it ends up in. So it says, Hey, this
particular state was valuable. We did good,
or this state is stupid. We did bad do better
next time. Now this is in contrast to state
and action pairs for something like say deep
q learning, but it's in line with what other
actor critic methods use And of course, the
actor decides what to do based on its current
state. So our network is going to output probabilities
using a softmax activation. And we'll use
that for a categorical distribution and pytorch.
So we'll have, in the case of the card poll,
we'll have a couple actions, and some probabilities
selecting each action. And then we will use
the probabilities determined by our deep neural
network to feed into a distribution that we
can sample and use for the calculation of
the log probabilities more on that momentarily.
It's also worth noting that exploration is
going to be taken care of for us, due to the
fact that we're using a distribution. So it's
probabilistic and is set up so that each element
has some finite probability. So even if the
probability of one action and goes arbitrarily
close to one, the probability selecting the
other action stays finite so that at least
some of the time it's going to get some exploration.
This is in contrast to something like say
epsilon greedy action selection and deep q
learning, where you select off off optimal
actions about 10% of the time. As I said earlier,
our memory is going to be fixed to a length
of capital T. In this case, we'll use 20 different
steps, we're going to keep track of the state
c agencies, the actions, it takes rewards,
it receives the terminal flags, the values
of those states, according to the critic network,
and the log of the probability of selecting
those actions that'll become important later
in our update rule. As I said, we're going
to shuffle those memories and sample a batch
size of five. And we're going to perform a
four epochs of updates on each batch. Now,
these parameters are chosen specifically for
this particular environment. And that's one
of my criticisms of PPO is that there are
a number of parameters to play with hyper
parameters. The memory length is one hyper
parameter, the batch size, and number of epochs,
as well as learning rate. And another parameter
we're going to see later, all play roles have
hyper parameters in our model. And so there
is a lot to tune here. But these parameters
work really well for the carpool environment,
so you won't have to do any tweaking for that.
Other thing to note is that this memory length,
capital T, should be much less than the length
of the episode. So in the case of the carpool,
the maximum episode length is 200 steps. And
so 20 steps is significantly less than that.
So I think it qualifies, you wouldn't want
to use something that encompass more than
one episode, for instance, that would probably
break the algorithm and result in poor performance
relative to using a capital T much less than
the episode length. So all this is relatively
simple. But what isn't so simple is the update
rule for our actor. So here's where all the
math comes in. So we have this quantity loss,
the CPI, the stands for conservative policy
iteration. And it's given by the expectation
value, which is just an average of the product
of the ratio of the policy under the current
parameters to the policy under the old parameters
multiplied by this a hat sub t, one that in
a second. When they do that, and they just
abbreviate that ratio is r sub t. Now, if
you're not familiar with deep reinforcement
learning or reinforcement learning, in general,
the policy is a probability distribution.
That is what our actor is attempting to model
is a probability distribution, the policy.
And this policy is a mapping between states
and actions and probabilities. So given your
in state SMT, and you took action a sub t,
what was the probability of selecting that
action, according to the distribution, and
so in the denominator, we have theta old,
that is the probability of selecting action
a sub t given state SMT, under the old parameters
of your deep neural network, so we're going
to play 20 steps, and then the agent is going
to perform a learning update. And it's going
to do mini batch Stochastic gradient descent.
And so after computing that first batch, the
parameters of the deep neural network change,
right, that's all the batches work, you compute
the loss with each batch and update your parameters.
And so right after you've calculated that
first batch of memories, the loss for that
and updated your deep neural network, the
theta changes, and so the policy pi is going
to change as well. So we have to keep track
of the parameters, excuse me of the the probabilities,
of selecting each action at each time step
in our memory. And then on a learning function,
we're going to pass those states through our
actor network, get the probabilities, the
probability distribution and find out what
the probability of selecting action a sub
T is sampled from our memory according to
the current values of the deep neural network.
It'll be a little bit more clear in code.
Just know that we have to keep track of log
prompts. As we go along and we're going to
be recalculating them in the learning loop.
One thing we also see is that it takes into
account the Vantage, which is at a hat sub
t. So the advantage is just a measure of the
goodness of each state, we'll get to the calculation
of that in a few minutes. But one thing to
note is that this ratio, pi sub theta, or
pi sub theta old, can have an arbitrary value,
right? Because you could have, let's say,
pi, theta being point nine, nine, pi theta
old point 01. And so that's a pretty large
number. And in particular, if you multiply
it by an advantage, that is like, say, 1020,
of whatever, then that can also still be a
large number. And so we have to deal with
that, right? Because the whole point of this
is that we want to constrain the updates to
our deep neural network to be some relatively
small amount. And so the way you deal with
that is by adding an additional hyper parameter
epsilon that you use to clip that ratio. So
what we're going to do is we're going to clip
that ratio within the range one minus epsilon
two plus one plus epsilon. So let's say from
0.8, to 1.2. So that ratio is going to be
constrained to be close to one. And you're
going to multiply that by the advantage. And
so that'll give you some number. And then
you want to take the minimum of that clips
number, the clipped ratio multiplied by the
advantage, and the unclipped ratio multiplied
by the advantage, take the minimum, and that
is what we will use for the loss for our actor
network. So this serves as a pessimistic lower
bound to the loss. And they don't go into
any real depth in the paper on the reasoning
for this. But to my mind, and this could be
wrong, you know, I am an idiot sometimes,
but my understanding is smaller loss, smaller
range and smaller update. That's the whole
point of it. So let's talk about this advantage
now. So this advantage has to be calculated
at each time step and is given by this equation,
don't freak out, this is relatively straightforward.
Once again, it tells us the benefit of the
new state over the old. Well, how do we know
that we know that because it's proportional
to or equal to the sum of the Delta sub t
with a Delta sub T is just the reward at a
time step, plus a difference in the estimated
value of the new state and the current state.
So it tells you, what is the difference in
the value between the next the next state
we encounter and the current state. And of
course, you have the gamma in front of the
V, which is the output of the critic network,
because we always discount the values of the
next states, because we don't know the full
dynamics of the environment. And so that that
reward is uncertain, there's always some uncertainty
around state transitions. And then in the
top equation, you just sum that, where you're
going to be summing over gamma multiplied
by lambda. So this quantity gamma is again,
the normal gamma 0.99 that we typically use.
But this parameter lambda is a type of smoothing
parameter, it helps to reduce variance, and
we're going to use a value of 0.95. And for
implementation, we're just going to use a
couple of nested for loops. So you're going
to start out at time t equals zero, and then
some from that step all the way up to capital
T minus one. So if we have 20 states, you're
going to go from zero to capital T minus one,
zero to 18. And you have to do that because
you have the V of S sub t plus one, you don't
want to try to evaluate something beyond the
number of actual states that you have, that
won't work out, right. And so it's going to
be relatively straightforward once you see
it in action. And we're going to be keeping
track of that gamma times lambda, which is
a multiplicative constant that increases its
power by one with each iteration of the inner
loop. All that'll be made clear in the code.
But fortunately, the critic loss is a little
bit more straightforward. So we need something
called the return. So the return is just equal
to the sum of the advantage and the critic
value based on the memory. So whatever the
agent estimated the value of a particular
state to be at the time that it took it is
what we're going to be using for the critic
value in our return. And then the loss of
the critic is just going to be the mean squared
error between the return and the critic value
based on the current values of the deep neural
network. So once again, we're going to be
passing the state to the critic network to
get its estimate of values. And we're going
to be also using the values from the memory
as well. So relatively straightforward, even
easier when you see it in code. So we have
two different losses, and we have to sum them,
and so that'll be the sum of the clipped actor
and critic. So a couple things to note here
is that one, we're actually doing gradient
descent. And so the coefficient of C one for
the loss of our critic is going to be positive
and the loss of our actor is going to be negative
because We are doing gradient ascent and non
gradient descent we have to multiply by negative
one other thing to note is that we have this
other parameter here, C to this coefficient
multiplied by S, S is an entropy term. And
that only comes into play when you have a
deep neural network with shared lower layers
and actor and critic outputs at the top. So
we don't have to worry about that, in our
particular implementation in this tutorial,
because we're doing two separate networks
for the actor and the critic. And I'm going
to use a coefficient of 0.5 for the loss for
the critic. As I said, we're not going to
be implementing the entropy term, because
we're doing two distinct networks. We can
also use this for continuous actions there,
you would use a different output for your
actor network. And indeed, that's what the
paper really is geared for, is for continuous
action spaces. But we're going to be doing
the very simple discrete case. Other thing
we don't implement is the multi core CPU implementation.
Because that introduces even more complexity,
we're just going to be using the GPU. So what
do we need for this project, we're going to
need a class for the replay buffer. And we're
just going to use lists for this. Normally,
I like to use NumPy arrays, but in this case,
lists turn out to be a simpler implementation.
So that's what we're going to go with. We're
also going to need a class for our actor network
and a class for the critic network. We'll
need a class for agent that's going to tie
everything together that'll have actor and
critics that invoke actor and critic constructors
as well as a memory for storing the appropriate
data. It also functions for choosing actions,
storing memories, saving models, and learning
from its experiences. And then a separate
file, we're gonna have a main loop to train
and evaluate the performance of our agent.
Before we get into the coding section, I want
to do a quick shout out to William Woodall,
he hangs out in our discord channel, which
is also linked in the description below if
you want to come hang out with some really,
really smart people who talk about artificial
intelligence ranging from all sorts of different
things every single day, check out link in
the description for the discord. So William
came to me and said, Hey, Phil, I found an
implementation of PPO that I find to be in
line with your general philosophy of software
minimalism. And he showed it to me, and I
looked at it, and it helped clarify quite
a few questions I had after reading the paper.
Now, the software you see here is pretty much
my own code. But it was inspired by Wayne
Woodhouse code. So shout out to him for helping
me out on this because the paper really isn't
all that clear to me, even after reading it
a few times. Other thing I want to say, and
I'll talk a little bit more about this in
the coding section is that when I normally
define deep neural networks, actors and critics,
in particular, I will use the convention of
saying self dot layer name equals n n dot
linear self dot layer name Next, you know,
equals and n dot linear. And then I'll write
the feed forward function where you use the
member variables that self dot layer one as
something you can call as an object to call,
and then calling activation functions within
that. Now, what I found is that doesn't really
work very well. In fact, I have to use an
n dot sequential to create the models for
this. And that's one of the biggest takeaways
I had from we would always code is that by
using the nn da sequence where you really
get this thing to work. And for whatever reason,
I cannot get as good a performance using my
conventional, typical way of writing these
networks. Now that I can't think of any reason
why that should be the case. But it is something
I've observed, I tested it, just altering
that one chunk of code, how I defined the
models, and running it several times to take
into account run to run variation. And it
seems to be repeatable for me. So maybe, I
don't know, maybe it's a configuration issue
on my system. Maybe it's something I'm doing
wrong elsewhere. I don't know I don't think
so. All of that out of the way. Let's go ahead
and get into the coding portion. All right,
so let's go ahead and jump right into it with
our imports. They're going to be pretty light
will need us to handle file joining operations
NumPy for NumPy type stuff, and all of the
torch packages will need an N for sequential
model. We will need up Tim and we will also
need our categorical distribution. So we'll
start with our PPO memory class. And this
will be pretty simple. For the most part.
The only input for our constructor is a batch
size. And we will just implement the memory
with lists. So we'll keep track of the states
encountered the log process. I'll just call
it prompts for brevity, the values that our
critic calculates the actions we actually
took the rewards received, and the terminal
flags. So next, we need our function to generate
our batches. So our strategy is going to be
the following, we're going to have a list
of integers a correspond to the indices of
our memories. And then we're going to have
batch size chunks of those memories. So indices
from zero to say, four, and then five to attend,
so on and so forth, or whatever our batch
size is, we're going to shuffle up those indices,
and take those batch size chunks of those
shuffled indices. So the first thing we need
to know are the number of states we are going
to want to get our batch start list or array,
I suppose that'll go from zero to n states
and batch size, steps, batch size. It would
help if I could type our indices. And that
is just the number of states in our trajectory.
We run on a shuffle that so that we handle
the stochastic part of the mini batch and
Stochastic gradient descent. And then we can
go ahead and take our batches using a list
comprehension. So it's going to be those indices,
from eye to eye plus self dot batch size for
i n, brain for AI, N, batch start. So it's
going to take all of the possible starting
points of the batches, either 05 10, etc.
and go for in the indices from that all the
way up to AI plus batch size. So we're going
to get the whole batch from our indices, then
we're going to want to return an array for
each of those. And this gets a little bit
messy. Let's be very careful not to mess up
the order. Because of course, the order in
which you returned the memories definitely
matters later on. We'll need rewards. And
then we're also going to want to return the
batches. And the reason why will become apparent
later. It's because we're returning the entire
array here. And we're going to want to iterate
over the batches. So now we need a function
to store a memory. And that'll take a state
action, probability, value, reward, and done
as input. And all we're going to do is append
each of those elements to the respective list.
Not as reward singular. And then finally,
we need a function to clear the memory at
the end of every trajectory. And I forgot
the self argument here. And mini rant here,
I really don't like some aspects of Python.
It took me much longer than I would care to
admit to get this to run, not because the
algorithm I implemented was incorrect. But
because I had a mismatch. So here I had, I
believe action. And up here it was actions
or promise vice versa, and my original implementation,
so it didn't flag as an error. Because it's
not really an error, particularly where Python
is concerned. And so it was quite a nuisance.
Pretty, pretty painful to track that down.
Of course, if we're more strongly typed language,
then that wouldn't be an issue. But I digress.
So now let's handle our actor network. And
that will derive from the base nn dot module
class. Our initializer is going to be pretty
straightforward. We're after we will need
the number of actions. The input dims a learning
rate alpha number of fully connected dims
for the first and second fully connected layers
and a checkpoint directory. And we're also
going to need to call our super constructor
And then create our checkpoint file, checkpoint
directory and actor, torch PPO. Now, I do
it this way, because I'll often do development
in a single root directory. And I don't want
to get models mixed up. If you have a different
way a more organized way of writing software,
then you could perhaps skip this path joint
operation and just use a file by itself. But
let's move on to the actual deep neural network.
We're going to want a linear layer that takes
starred but dim, so we're going to unpack
the input dim, so we have to pass in a list.
And it's going to output FC one dims. Array
you activation function, another linear layer
that takes FC one dims as input outputs FC
two dims. That gets a raw you activation as
well. Another linear layer that takes FC two
dims as inputs and outputs a number of actions.
And then we're gonna use a softmax activation
along the minus one dimension. So that's the
whole of our actor network. The softmax takes
care of the fact that we're dealing with probabilities,
and they have to sum to one. So our optimizer
is going to be an atom optimizer, what are
we going to optimize the parameters with learning
rate of alpha, of course, we need to handle
the device, which would be our GPU if possible,
then we want to send the entire network to
the device. Next, we have our feed forward
function. And that'll take a single state
or batch of states as input. So we want to
pass that state through our knee deep neural
network and get the distribution out. And
then use that to define a categorical distribution,
which we are going to return. So what this
is doing is it is calculating a series of
probabilities that we're going to use to draw
from a distribution to get our actual action.
And then we can use that to get the log probabilities
for the calculation of the ratio of the two
probabilities in our update for our learning
function. Then we have a couple of bookkeeping
functions save checkpoint. We're going to
want to say, torch dot save the state dictionary
for our network. And we're going to say that
into a checkpoint file, then we need to load
checkpoint. And that is self dot load state
dictionary. What are we going to load a checkpoint
file. And that's really it for the actor network,
it's pretty straightforward. The critic network
is also straightforward. And that also derives
from nn module. Here, we don't need the number
of actions because the output of the critic
is single valued, it just outputs a value
of a particular state. So it doesn't care
how many actions are in the actual space.
But it does need a learning rate alpha. It
does need some dimensions 256 and a checkpoint
directory. And then we need to call the super
constructor. And same deal the checkpoint
file the check point directory and critic
torch. So that way, we can differentiate between
the actor and critic model files. And we will
again use a sequential model. And so in the
shout out I was talking about William moodles
implementation, as well as something else
I observed. So what I meant by the alternate
method of doing a model was if you say self.fc,
one and in linear you know, if you do it that
way, you have FC one FC two the separate layers
defined without the sequential model. It actually
does significantly worse than if you do it
with the sequential model and I don't know
why I don't have a certainly there's no theoretical
reason they should do it. It must be something
under the hood with the way in which pi torches
and Limiting things. And it's no disrespect
to the creators of pytorch. But this is one
of my, you know, one of my biggest gripes
with using these third party libraries is
you never know how they're implemented. So
something doesn't operate the way you expect,
you can certainly go look it up, it's open
source. But that is much easier said than
done, right? You have to be familiar with
not the entire code base, but a really significant
portion of it to be able to make sense of
a single file or a single way of doing things.
So it really makes things opaque. It's an
abstraction on top of an abstraction. And
so I don't know, it's part of the good part
of the it's the bad that comes with a good
for having, you know, a robust library like
pytorch. But I do it this way, because it
seems to work the best. And as an aside, I
also can't get it to work very well in TensorFlow
two. And I suspect the The reasons are related
because the performance of the TensorFlow
two is on par with the type of performance
I get from doing it the other way, where you
just define individual layers instead of a
sequential model. So pretty interesting stuff,
maybe one day, I'll get super motivated, and
decide to go ahead and figure it out. But
I wouldn't hold my breath on that. So this
is going to be very similar model, linear
layers with relu activations in between, the
main difference is that our output layer is
going to be a linear layer with no activation,
and a single value output. Now, of course,
it handles the batch size automatically. So
if you pass in a batch, you're gonna get the
batch of outputs as well. Again, we need our
optimizer with learning rate of alpha. As
an aside about the optimizer, I'm going to
use the same learning rate for both the actor
and the critic. And it's entirely feasible
and possible, and perhaps even advisable to
use separate learning rates for both the actor
and the critic. At least in something like
deep deterministic policy gradients, you get
away with a much larger, you know, by a factor
of three or so learning rate for your critic
than you do the actor. Reason being. As we
outlined in the lecture, the actor is much
more sensitive to changes in the underlying
parameters of its deep neural network. Now,
ostensibly, or theoretically, the, the PPO
method should account for that and allow you
to use a similar learning rate, because the
actor should be less sensitive than in the
case of ddpg, but I haven't tested it. So
one thing you can do in your spare time is
play around with different learning rates
for both the actor and the critic. Our forward,
feed forward function is pretty straightforward.
You want to pass a state through your critic
network and return that value. And we're going
to need saving and loading checkpoints, I
am just going to yank and paste those because
the functions are otherwise identical. And
so that is it for our two networks. Now we
come to the heart of the problem, which is
the agent class. Do I have an extra? Yes,
I do. And this, of course, does not derive
from anything. This is our base agent class.
With a number of actions, a default value
for gamma, which is the discount factor in
the calculation of our advantages. Typically,
we use something like 0.99, a learning rate
of 0.003 minus four, I got this from the paper.
So if you read the paper, they do give you
the hyper parameters and a little bit of detail
on the networks they used. But it is not a
very well written paper, it's rather obtuse.
So I'm not a huge fan of it. But we do have
some good default values from it. So policy
clip. So in my cheat sheet here, I have a
value of 0.1 as a default, although in the
paper, they use 0.2, perhaps I was experimenting,
I will have to be careful with that a batch
size of 64, a default and a 2048. And so that
is the horizon, the number of steps before
we perform an update, and the default for
the number of epochs. Now these parameters
come from these parameters come from the values
for continuous environments. So the actual
numbers we're going to be using are going
to be significantly smaller, as I said, we
use an N of 23 ybox batch size of five instead
of 64. And I'm going to go ahead and set that
policy clip to 0.2. Now that I'm looking at
it, we need to ga lambda that is the lambda
parameter. But of course you can't use lambda
because that is a reserved word for Python.
What else do we need? Um, yeah, I think I'm
missing something in my other file here. That's
okay. I'll fix it on the fly. So then we go
ahead and save our parameters, number of epochs
and RGA, lambda. Meet our actor network dems
and learning rate 
takes input dims. And alpha, baby memory.
batch size input one second. All right, hopefully
that is not as loud now. The toddler is playing
with his grandparents always a hoot. So now
we need a function that handles the interface
between the age and its memory. And it's just
going to be very simple self memory store
memory. It's just an interface function, then
we need a function to save our models. Print
saving models 
is just going to be an interface function
between the agent and the Save checkpoint
functions for the underlying deep neural networks.
And very similar for the load models function.
We have a visitor. All right, that is it for
our bookkeeping functions. Next, we need something
to handle choosing an action. That'll take
an observation of the current state of the
environment as input. And we want to convert
that NumPy array to a torch tensor. And we're
going to add a batch dimension because the
deep neural network expects a batch dimension.
And we'll be sure to specify that it is float.
And then we're going to go ahead and pass
that through our neural networks. So dist
equals self dot actor state, that'll give
us our distribution for choosing an action,
we need the value of that particular state.
And then to get our action, we just sample
our distribution. And then what we want to
do is go ahead and squeeze to get rid of those
bash commands. And this might be something
I added. For TensorFlow two, I'm not I don't
remember if torch requires it, but it doesn't
hurt anything. So for the prompts, you want
to go ahead and return the log probability
of the action, we actually took that item,
so that item will give you an integer. And
likewise, for the action, we want to squeeze
it and get the action the item out. And similarly
for the value and then just return all three.
So this will make our main function look a
little bit different than we're used to, because
we're going to be accepting three values from
our transaction function instead of one. But
that's necessary for keeping track of the
probabilities and values as well. Next, we
come to the meat of the problem, so to speak,
our learning function, so we want to iterate
over the number of epochs. So we're going
to have, in this case three epochs. At the
top of every epoch, we want to get our arrays,
the old probabilities, the values, the reward,
the dance and the batches. Do that and then
I'm just going to use a different note. tation
here, and go ahead and start calculating our
advantages. So our advantage is just going
to be a NumPy array of zeros. Len reward,
type MP float 32. And we're going to say for
T and range, so for each time step, Len a
reward array minus one, because we don't want
to overwrite the, our go beyond the bounds
of our array, our discount factor is going
to be one, the advantage of these times have
starts out as zero. So we're okay in range.
So we're going to start out at T and go from
t to the same reward array minus one and say
a sub t plus equals discount so that ga times
lamda factor, which starts out as one times
we need parentheses, or array sub k, plus
saltdogg, gamma times values k plus one times
one minus and dot array, okay, minus values
sub k, then we say discount times equals self
dot gamma times G, or lambda, at the end of
every calculation, the end of every case steps
advantage sub t equals at a sub t. And at
the end, we're going to turn advantage to
a tensor. In particular, a CUDA tensor. And
this is just a strict implementation of the
equation from the paper. So this, right here,
in parentheses, is the Delta sub t. So it's
a reward plus gamma times v sub t plus one
minus V sub t, where, you know, we swapped
the K and T here, and you need the one minus
dunnes on the values as a multiplicative factor
the vize of a T sub t plus one, because the
value of the terminal state is identically
zero, that's just a convention in reinforcement
learning predates the deep neural network
stuff is just how we handle it, it's assumed
that's why they don't put it in the calculation,
it is assumed it's just a matter of convention.
And then that discount is the GA the lambda
multiplied by the gamma, that takes care of
the multiplicative factor. So it is the the
gamma lambda to the T minus one power, or
is it t minus k minus one, something like
that power, multiplied by the Delta, and then
you're summing it all up. So now we have our
advantage. I'm going to convert the values
to a tensor as well. And I fully admit here
that going from Val's array, to you know what,
in fact, let's do this. Now, let's see what
the weight is, may not be the most effective,
or excuse me, the most efficient way of doing
it, but sometimes, I just get stuff to work.
And then don't go back and clean it up. If
you want to clean it up, please do. So I always
invite that. And it looks like I'm missing
something here because it is not automatically
indenting. So I'm probably missing a parenthese
somewhere and it is right here, I believe,
if I'm not mistaken. Yeah, there it goes.
Alright, so then states, it's just going to
be a tensor state array, sub batch, the type
to float to salt actor dot device. And we're
kind of violating the pep eight style guides
their style guide by going beyond 80 characters.
But I think we'll be all right old probabilities
gets converted to a tensor. And I don't need
an explicit D type there. I don't think that
vice to salt that accurate advice that works
and then actions. Okay, and then. So we have
the states we encountered the old probabilities
according to our old Vector parameters, the
actions we actually took the next parameter
we need. So we have the bottom of that numerator
pi theta old, we need pi theta nu. So we have
to take the states that we encountered and
pass them through the actor network and get
a new distribution to calculate that new,
those new probabilities 
will also need the value of the the new values
of the states according to the updated values
of the critic network. So you may as well
get those now. And we can squeeze those. And
then we can calculate our new probabilities
and take the prob ratio. So here, I'm going
to exponentiate the log probs to get the probabilities
and take the ratio, you could also do this.
Those two are equivalent by the properties
of exponent, exponent exponentials, excuse
me. And then we're going to calculate our
weighted probabilities. And sorry, our probability
ratio. Now, yeah, the weighted probabilities,
think I have two lines that do the same thing
in there, that's fine. That's going to be
the advantage batch times a probability ratio,
and we need the weighted clipped probabilities.
And that is going to be the clamp of the proper
ratio between one minor self dot policy clip
and one plus self dot policy clip multiplied
by advantage sub batch. Now our actor loss
is going to be the negative minimum of the
weighted probs or the weighted clipped Prague's
that mean, and our returns for our critic
loss are going to be the advantage plus the
proviso for that particular batch. And so
our critic loss, then is going to be the returns
minus critic value squared. And the mean value,
our total loss after loss plus 0.5 Times critic
loss. Remember, we're doing gradient ascent
and there's a negative sign in front of the
actor. So we're not doing descent that's another
thing that's kind of suboptimal by the way
the paper is written, you can get kind of
confused about negative signs if you're not
paying very careful attention. Next, we have
to zero our gradients. I think you can probably
hear that my son is giving a concert downstairs,
he's playing the drums by whacking on his
toy box with some drumsticks. So we're going
to back propagate our total loss. And then
step our optimizers. And finally, at the end
of every epoch Yeah, I think that's the right
indentation, we want to clear our memory.
So at the end of all whoops, at the end of
all the epochs we want to clear our memory.
Let me just make sure I'm not doing that.
Eg POC No, I'm not okay. That is good. So
now let's do a write quit. And I have an indentation
error here. I see. Oh, that came in when I
did the yank and paste. Alright, so that is
it. For our agent file. Let's go ahead and
take a look at Main. So we start with our
imports. We'll need a gym will need NumPy
to keep track of the right
Average our scores from PPO torch will need
our agent. And if you're new here, I have
a utility file that I use a map plot live
pie plot function to plot the running average
of the previous 100 games. For the learning
curve, it's pretty trivial, you can just do
a plot of the running average, just do a git
clone. If you want to use my exact version,
I don't go over it in every video because
it's kind of redundant, but I'll leave a link
in the description to the GitHub. So go ahead
and do a clone of that. So you have that file
or just write your own. So we're going to
use the very basic cardpool v zero. Reason
being, we don't need to spend a whole lot
of time on a very computationally complex
environment to realize we made a mistake.
So it's very easy to see if something got
screwed up with the card pole environment.
This certainly will work on more advanced
environments. But it does require a little
bit of fine tuning. So we'll just start with
the card pool. And then you can play around
with other environments at your leisure. So
we'll use parameters I dictated in the lecture,
I think I change the number of epochs to four
to three, we get the number of actions directly
from our environment, very handy. Pass in
all the other relevant parameters got a number
of input dimensions from our environment.
And we're only going to play 300 games as
I'm looking at this, I do realize that the
parameters I did the last time I ran, I did
do a policy clip of 0.1. The 0.2 comes from
the paper at hand. I'm pretty sure it works
both ways. So we will find out if we need
to, we can go back and change a policy clip.
Not a big deal. So plot slash card poll dot
png need to keep track of our best score this
minimum score for the environment, empty list
for a score history. And number of learning
times we call the Learn function, you can
make this a member variable of your agent
if you want. And an average score starting
at a zero we don't actually need that, but
whatever. So we'll say at the top of every
episode to reset our environment. So the terminal
flag to false and a cumulative score to zero
or we're not done, we need to choose an action
based on the current state of the environment,
get the new state reward done and debuginfo
back from the environment, increment our score
by the reward and store that transition and
the agents memory worn and done. And if n
and do need an extra variable here say n steps
equals zero and that's the number of steps
to take. And we need that because we have
to know how often or when it's time to perform
the learning function. So every time we take
an action, the number of steps goes up by
one. So then steps modulus, n equals zero
then agent dot learn fitters plus equals one.
And then no matter what happens, we want to
set the current state to the new state of
the environment. At the end of every episode,
append our score and calculate our mean that's
the previous 100 games then average score
is better than the best known score then set
that The best score to the current average,
and save your models. And we also want some
debug information is so I score pore size
should be an average score. The I like to
pronounce this isn't necessary. But the number
of steps that have transpired in total, and
the number of times the agent has called the
learning function. This gives you an idea.
This is I did this because when I compare
with the results of the paper, it wasn't clear
to me if they were talking about the number
of times they called the learning function
or the actual absolute number of time steps
in the environment. So I print out both these
time steps and learning steps are totally
optional. You don't have to print it out.
It's not something that is required. So I
just do it for my own clarification. We need
an X axis for our plot. One score history
and plot learning. Oh, certainly. Let's do
this. We don't want to do it every single
game. We want to do it at the end of all games.
All right. Now moment of truth. Let's see
how many typos I made. So it's telling me
it got an unexpected argument, input dims.
That's interesting. What do I call it? I don't
have it there. And the reason is, my computer
had a hard lock up and I had to do a reboot.
And it mutilated my cheat sheet for this.
So there's bound to be some errors in here.
I didn't do my make directories. So temp,
PPO and plots. Let's try it again. Named Dunn's
array is not defined, it's probably done array
that is in line 161. So it is Dunn's array.
Yeah, just change it there. I guess. Old property
is not defined, that's probably the same thing.
Where am I? Yeah, old prop array. I'll do
the opposite here. I'll make it singular,
just for just for the sake of not being consistent.
Index eight is out of bounds. Okay. So then
something has gone extremely wonky with the
generation of the batches. Okay, let's take
a look at that. Oh, wait, let's read this
a little bit more carefully. It says index
eight is out of bounds for access zero with
size zero. So our action array Oh, you know
what? Let's take a look at our memory. So
it's action array. So here we have self dot
actions. We return the self actions. Self
thought, there we go. That's why. So action
and action. All right. Now let's try it. Name
advantage is not defined that as a typo, that
is in line 183. A advantage today, yes, try
that. try once more. has no memory underscore
clear memory, it's memory. Clear memory 197.
Alright, so now it is running. So I'll let
that go for a few minutes and we will see
how it does. Alright, so it has been running
for a little bit, maybe just a few minutes.
Now it runs relatively quickly. And what I'm
seeing is that we do get some oscillations
in performance, you see, it'll hit 200 for,
you know, several games in a row, and then
it'll drop down into the mid one hundreds,
even, you know, 66, something relatively low
like that. And there's a little chunk here
where it dips below 100 points. So it's not
a silver bullet, but it looks to be recovering.
So we'll give it another 80 rounds and see
how it does. Okay, so it has finished up.
And you can see that it finished strong with
a long run of about 50 games 45 games have
a score of 200. So I commented, when I was
writing the agent that I was looking at my
cheat sheet and had a policy clip value of
0.1, it could be that I'd settled on that
value based on some experimentation, and then
changed it back just to be more consistent
with the paper for this particular video.
So that's something I would play with. Other
thing to consider is that there is significant
run to run variation that is a facet of pretty
much every algorithm in deep reinforcement
learning, it just has to do with the way the
neural networks are initialized, as well as
how they number a random number generators
initialize the environment. So when you see
papers, you'll typically report average values
for say, five or 10, or whatever number of
games and then a band to show the range of
variants for run to run variation. But this
is clear evidence of learning in order to
achieve the score of 200 and under 300 games.
So I call this good to me this is fully functional.
Now there are a number of things you can do
to improve on this, you can get it to work
with continuous environments, you can bolt
on some stuff for doing Atari Games, where
you would need to add in convolutional neural
networks as your input layers, and then flatten
them out to pass them to a linear layer for
the actor and the critic. And you can see
my earlier video on an AI learns to be Pong
for Q learning. There, I go over all of the
a lot of the stuff you need to do to modify
the open AI, gym Atari environment to do a
frame repeating that something they do in
Q learning. That's an exercise to the reader.
Actually, I don't know, thinking back to the
paper, I don't recall if they actually do
any frame repeating or not in this particular
algorithm PbO. But it's just something to
look at anyway. So there's a number of things
you can do to improve upon it. I haven't added
this module to my course yet. I'm still working
on it, I really want to take some time to
give more thought to the paper because the
paper isn't very well written. And I'll probably
have to do a lecture like what I did for this
YouTube video in the course, because the paper
isn't very easy to implement just by reading
it. So I hope that was helpful. That is PPO
and just a few 100 lines full implementation
in pytorch. Solving the carpool environment,
then you can easily modify this to do other
environments as you wish. If you've made it
this far, please consider subscribing hit
the bell icon, leave a like a comment down
below. And I'll see you in the next video.
Welcome, do a crash course and soft actor
critic methods, you're going to learn the
least painful way to quickly implement the
salt actor critic algorithm using TensorFlow
two, we're going to implement this and tested
on the PI board environment, the inverted
pendulum, because it's relatively quick to
compute, it runs pretty fast. So we'll know
whether or not we got it right relatively
quickly. So what exactly is assault actor
critic algorithm? So this algorithm sets out
to address a pretty fundamental issue in deep
reinforcement learning, which is how do we
use maximum entropy framework in actor critic
methods? We're going to go a little more detail
into this in a moment, but that is the basic
idea behind what we want to do. So why would
this even be something worth considering?
Well, as you may be aware, actor critic methods
have a number of fundamental limitations,
not the least of which is the fact that they
have what is called brutal convergence, meaning
that they suffer from a high degree of hyper
parameter tuning. If you go monkeying around
with hyper parameters, the agent breaks and
doesn't know what to do in the environment.
And so once you find a set of hyper parameters,
you really have to stick with them. Worse
than that, you have a problem called high
sample complexity, which is just a fancy way
of saying you need to play a whole bunch of
games for the agent to figure out how it works.
It's not very efficient for environments in
which you have a large number of large state
spaces and high number of actions in a continuous
action space. Worst of all, these two fundamental
drawbacks really limit room replicability?
And is probably one of the big reasons behind
why we haven't seen any widespread adoption
of deep reinforcement learning something like
say robotics. So sometimes your critic, as
I said, is a maximum entropy framework. And
what this means specifically is that the agent
is going to maximize both long term rewards
and entropy. Well, what does entropy even
mean in this context? Well, if you're not
familiar with the concept of entropy, it's
strictly speaking, a measure of disorder in
your system. It stems from statistical physics
and thermodynamics. It's basically the log
of the multiplicity of your system, the number
of ways of arranging the components of your
system. What does that mean in the context
of deep reinforcement learning? It means the
randomness of the actions of your agent. And
you might wonder, why would you want to maximize
both long term rewards and randomness of your
agent, the region, the reason is that we need
to have some degree of random actions to test
our model of the world, the agent starts out
knowing absolutely nothing about the world,
and so should act as randomly as possible.
And then as it starts to figure out what actions
lead to rewards, it should eventually start
to converge on taking mostly those actions,
but still spend some time exploring to make
sure that there isn't some better action out
there. This isn't a totally alien concept
to you, if you're familiar to cue learning,
or familiar with cue learning, there, we use
what's called epsilon greedy action selection,
where some proportion of the time say 10%
of the time, we take a random action no matter
what even if we know what the best possible
action is, we may still take a totally random
action, simply because you can never be 100%
certain that you're right. In this algorithm,
we're gonna be modeling entropy by reward
scaling, so we're gonna have a multiplicative
factor for our reward. And there's going to
be an inverse relationship between our reward
scale and the degree of entropy in the system.
And you can kind of think of this intuitively,
because if you increase the scaling on the
reward, you're going to increase the contribution
of that reward to the cost function. And so
you're going to kind of tilt the neural network
parameters towards maximizing that reward.
Whereas if you lower that parameter, then
you're going to lessen the contribution of
the signal of the reward to the updates of
the neural network, and so decrease its overall
importance to the cost function. This algorithm
is also going to leverage all the neural networks
possible around networks for actors, value
network and critic networks. And in particular,
we're actually going to have two critic networks,
which is going to be an exact analogue of
the double q learning algorithm, as well as
twin delayed deep deterministic policy gradients,
another awesome algorithm for continuous action
space environments. Please see other videos
on this channel. If you don't know much about
those, I have a multitude of videos covering
both of those topics. They also make use of
another innovation from Q learning, which
is the use of a target value function. So
the idea here is that we're gonna be using
our value function to track the values of
states to tell us which states are viable.
So we can, you know, seek out those states
again, that's the idea behind reinforcement
learning. But the problem is that we're going
to be updating that value function with each
time step. And so if you're updating at each
time step and using it to evaluate the values
of the states, then you're really chasing
a moving target. And so the algorithm will
suffer from instability. And that happens
a lot in Q learning as well. The solution
is to have a target value function, which
is updated only periodically or very slowly.
So in queue learning, we do a periodic update,
where we just exactly copy the networks, the
network parameters from one network to another
from the online to the target network, here,
we're going to make use of a soft that date,
where it's going to be some kind of moving
average of the online and previous values
of this target value network. I'll show you
that equation later in this little lecture.
So our actor network is going to model both
the mean and sigma of a distribution. So this
is a probabilistic policy, it is not a deterministic
policy, like in ddpg. So we're our actor network
is going to output two parameters, a mean
and sigma. And then we're going to put that
mean and sigma into a normal distribution
and sample a to get an action. Now the original
paper use what they call a re parameterization
trick. And I will fully admit I'm a little
fuzzy on exactly what that means. It kind
of sounds like they are sampling some normal
distribution, then adding on some additional
noise to it. I implement this in what I do
a what I believe to be a faithful implementation
of this and pytorch but I'm not going to do
it in this tutorial. I think it's a little
superfluous and I'm actually getting, you
know, really good results without it so I'm
kind of throwing it away. We do have a special
function to enforce And bounds as well as
to calculate the log of the probabilities
of taking some actions because the log of
the probability will play a big role in our
update rules for our networks, which we'll
get to momentarily, and I'll show you that
function later on in lecture as well. Another
thing to note is that we can use multiple
steps of gradient descent like we do in proximal
policy optimization or PPO. But we're not
going to do that, we're going to just use
one step of Adam opposite Adam optimization
per time step of the environment. Some other
implementation notes, we're going to have
a replay buffer based on NumPy arrays, I prefer
NumPy. arrays, that is by no means the only
way to do it not even necessarily the best
way, it's just my preferred solution, change
that if you want. Now we're going to keep
track of the states the agent sees the actions
it took the rewards that received the new
states that resulted from those actions, as
well as the terminal flags from our environment.
And the terminal flags are important, because
we have to use that in the update for our
deep neural networks. Because when we're evaluating
device states, we have to take into account
whether or not that state was terminal. The
reason is that terminal states have no value.
Because the episode is over no feature rewards
follow the value is just the present value
of the discounted future reward. So the reward
is the episode is done, it's zero by default.
Now, this is the equation I alluded to earlier,
where we are calculating the log of the probability
of selecting some action given some state.
That's what that pie means. It's a probability
slotting an action which is good continuous
parameter, given some state, this mu is our
actual sample of a of a distribution with
mean and segment given by our deep neural
network. So what is not the output of our
deep neural network, it is the sampling of
a distribution where the mean and sigma are
given by our deep neural network. And another
thing to note here is that we are going to
have to multiply our action by the max action
from the environment. Reason being is that
the action is going to be proportional to
the tan hyperbolic function. And that is bounded
by plus or minus one, which not all environments
are bound by plus or minus one. So you want
to take into account that fact, we don't want
to cut off half of our action space arbitrarily.
So let's talk about updating our deep neural
networks, our actor network update is pretty
straightforward. What we're going to do is
sample states from our buffer, but compute
new actions. So this is an off policy learning
method, where we're going to be using samples
generated by one policy to update some newer
policy. We're going to stick those states
through our actor network and get new actions
out on the other end, compute that log prob
based on what we saw on the previous slide.
And then we're going to subtract off the minimum
value of the two critics. So we're going to
pass the state sampled from our buffer the
actions computed in our update, according
to the current visor actor network, and then
take the minimum of the critic evaluation
of those state and action pairs. This is kind
of the double q learning rule in action. That's
why as cute men, and the one over N some tells
you that we're taking a mean of that quantity,
that difference. Our via network update is
a little bit more convoluted. So we're going
to have one half the mean squared error, that's
what the one over N, the one half of the difference
squared, tells you of the difference between
the value function using the current parameters
or a value function for the state sample from
our buffer. Again, we're going to subtract
off the minimum of the Q values, where the
actions are chosen according to the new values
of our hacker network. And we're gonna again,
subtract off that log of pi or again, the
action that is computed according to the current
values of our actor network. And again, the
log is computed according to a couple slides
ago. The target value network, on the other
hand, doesn't get any gradient descent to
determine its new parameters, we're going
to be updating its parameters with the following
equation. So we're going to take this hyper
parameter tau and multiplied by the values
of our online via network given by Psy that's
just the symbol for the parameters for our
value network. And we're going to add on one
minus tau. So point 995, multiplied by the
current values of the target value network,
take the sum of those two and upload those
values to our target value network. And it's
a slowly moving average of online and target
networks. Our critic network gets rather interesting
update. So here both critics get updated So
there's no minimum opera operation here. So
there's two cost functions, one for each critic.
And it's just given by the mean squared error
of the difference between the output of the
critic for the states and actions sampled
from the buffer. And we're not calculating
new actions here, that is one difference between
the update of the critic and the actor advised
networks. And we're subtracting off this quantity
Q hat. And that is where the actual entropy
comes into play. So it is a scaled reward,
plus the values of the new states according
to our target value network. So this is where
all the magic of the algorithm happens. That's
where the maximum entropy framework comes
into play in this one little equation. kind
of neat, huh? So we're going to need a whole
bunch of data structures. class for our replay
buffer, again, that'll be NumPy. arrays, the
actor network gets a class a critic network
gets a class, so does the value network, our
agent gets its own class. And that's going
to tie everything together that'll have functionality
for choosing actions, saving models, learning
as well as saving memories. And we will have
a main loop to train and evaluate the performance
of our agent. Now we're going to need a number
of packages will need TensorFlow, I prefer
GPU obviously, this is a highly computationally
expensive algorithm. So if you have the GPU,
please use it, but need pi bullet for our
environment, Jim, of course, NumPy and TensorFlow
dash probability, that is, the only way we
can get access to this probability distributions
are not built into the base TensorFlow package
for some reason. Okay, that's it for mini
lecture, your Crash Course. Now let's go ahead
and throw you in the water and start coding.
Alright, so let's go ahead and get started
with our buffer class. And then when we want
to do our networks and our aging class, so
we're going to be using NumPy arrays for our
memories and so NumPy as our NumPy will be
our only package dependence for this particular
part of the agent will need a max size, in
good shape and number of actions as input,
we will need to save our MAX SIZE. And we
will also need a meme counter to keep track
of the first available memory position. And
our agents memory is represented by NumPy
arrays that we will initialize it as zeros
in the shape mem size by input shape or input
shape is some tuple or list. We need the new
state memory as well that are the new states
that the agent sees as a consequence of the
actions it takes in the environment, we do
the action memory. And that is in shape men
size by a number of actions. Keep in mind
number of actions means number of components
to the actions since these are continuous
actions, they will have components along some
dimension and action space. It's not a actual
number of discrete actions, it's a little
bit Miss aptly named reward memory. And that's
just a vector and an array to keep track of
the terminal flags we received from the environment.
And I'll go into that as num, type NumPy bool.
Next, we need a function to store Trent store
transition. And if it isn't clear, the reason
we need to store the terminal flags is because
the value of the terminal state is always
zero because no future rewards follow the
terminal state and so it has no value. So
we will need to save all those parameters
in our agents memory. And we want to know
what is the position of the first available
memory. And that's given by the modulus of
memory counter and mem size. This has the
property that when we overwrite the agent's
memory, we will go back to the very beginning.
So we don't have to worry about going beyond
the bounds of our array. So why don't we just
go ahead and save our variables 
and terminal memory. And most importantly,
we need to increment our memory counter by
one. Next we need to function to sample our
buffer. And I want to make a quick I want
to make a quick aside here. So in a question
in the comment section someone asked Is there
a way to sample memories according to how
viable they were reading our early memories
agents acting randomly doesn't know what it's
doing. So what You know, what good is it to
sample those memories, isn't it more important
to sample memories where we kind of knew what
we were doing and had some reasonable model
the environment. And you can do that in something
like prioritized experience replay where you
prioritize memories based on their utility.
But in this case, we're just doing uniform
sampling. So we just take the good with the
bad. So we want to know the first available
memory how far we have gone in our memory.
And that's given by the minimum of the meme
size and the meme counter, so that we're not,
you know, if we've filled up half of our memory,
we don't want to sample the latter half, which
is all zeros, we only want to sample the first
half, which is filled up with useful transitions.
So then we're going to go ahead and take a
batch size of memories from from our memory
bank, and go ahead and dereference. Sorry,
we need an extra space there. And then go
ahead and return those states actions towards
new states and terminal flags. And that is
it for our memory buffer class. Next up, we're
going to handle our networks for the agent.
So the first thing obviously, we want to do
is come up and fix our imports. Because we're
going to need more stuff than just NumPy.
We're going to need OS, the base TensorFlow
package, care OS and we will need that flow
probability has TFP now, you will have to
install this package separately, it is not
included in the base TensorFlow package. So
see the TensorFlow documentation if that is
something you don't know how to do. But you
do need to do that installation separately.
So our critic network will derive from Kerris
dot model, I guess has access to making use
of the gradients and the gradient tape and
all the other good stuff that we get from
the base class. We will need a number of actions
number of F's DIMMs FC one FC two dims a name
so the name is useful for model checkpointing.
So we can keep track of which file corresponds
to which network and a directory for saving
models you have to do make dir on that otherwise
you'll get an error. First thing you want
to do is call your super constructor. Save
the relevant parameters. One of these days
I'm going to shorten this checkpoint dur name,
but not today. So then we want to join the
checkpoint file name checkpoint directory,
excuse me, with the name and underscore assault
actor critic so that when we save a bunch
of stuff in a single directory, we know which
file corresponds to which algorithm. Next
we can define our model. Our first fully connected
layer is just a dense layer that has implied
input dimensionality and takes FC one dims
as output with a row activation. Likewise
for a second layer. And our final output is
single valued with excuse me a single node
with no activation. So, we have to input a
state and action to our call function, because
the critic network evaluates the values of
state action pairs. And so the first thing
we want to do is pass the concatenated state
and action pair through the first fully connected
layer. And we want to concatenate along the
first axis and pass out through the second
fully connected layer. Pass through The topic
layer and return. Pretty straightforward there.
Next, let's handle our value network. And
again, that arise from karass model. We don't
have to specify any dimensionality here other
than for our first and second fully connected
layers. And we want to call our super constructor
again. And if you want to get really fancy,
you could write a base model based network
class, and then have everything derived from
both the I guess the base model class would
derive from the karass model. And then everything
that derives from that would derive from the
base class base model, sorry, base network
class. But that can get kind of messy really
quickly, you have to deal with multiple inheritance.
Maybe it's not as hard as I initially think
it is. But it's an alternative paths, it's
an alternative way so that you don't have
to keep writing like the same stuff over and
over again. Because you notice there's a lot
of overlap between the code of the network
so you can fix that with inheritance in our
output is again going to be single valued.
No activation. Now, the call function here
only takes the current state of the environment
as input, the value function is only concerned
with the values of states not state action
pairs. Then we just go ahead and pass all
of those through and return them. Pretty straightforward.
The real difficulty doesn't occur until the
actor network and the difficulty will come
in the call function as well as the sample
normal. So we want to pass in a max action
all using that here. Sorry, I always kind
of make stuff on the fly kind of modify stuff
on the fly. Yeah, so we do need that FC one
dims. FC two dims. A name number of actions
are used to by default again, that's number
of components. And a checkpoint directory.
protocol are super constructor and save the
relevant parameters. Okay, then we have our
model name. And just as an aside, you can't
use name, it's a reserved variable, we can
say self dot name equals name, because name
is reserved by the base class karass model,
so you have to use modeling. So we're not
going to be using three parameterization.
But we will need some noise factor to make
sure we don't get something that blows up
when we take the log. You'll see what I mean
very shortly. Our neural network by now should
look pretty familiar. Row your activations.
Here's where things get interesting. We're
going to have a new for our distribution as
well as a sigma no activation So let's go
and handle our call function. So we're gonna
get our I've called it prop here, but I don't
know if I like that name now that I'm rewriting
it. But I'm gonna roll with it. So mu, really,
that problem is just the input of our first
two layers, you know, the pastor of the state
through the first couple layers. Why are sigma
which is the standard deviation of our distribution.
And we're going to want to clip this. It may
be the case that TensorFlow two doesn't need
it. But when I was coding this up and pie
torch, it, the distribution would blow up.
If we had a noise of sorry, a sigma of zero,
it didn't like zero standard deviation distribution,
which I can understand. I guess that's a direct
delta function, it's not exactly something
that you would encounter every day. And I
think the TensorFlow might actually fail a
little bit more gracefully than the PI torture
in that respect. But I go ahead and clip it.
So that goes between one by 10 to the minus
six and one, so we're going to constrain the
standard deviation here Do not be too large.
That is something you can optimize on your
own. I don't do extensive and detailed studies
on all this stuff, I just get it working.
Check that it conforms to the specifications
laid out the paper, and then I get something
reasonable on the output. But we're not done
for the actor network, we have the output
of our deep neural network, which is a mean
and standard deviation for our distribution.
Now we have to sample that distribution to
get the actual action for our agent. And I've
stuck it on a function called sample normal.
And as I stated, we're not going to do in
the re parameterization trick, show students
how to do that in the course. And in fact,
in my pie torch video, I show you how to do
it. Pie torch has a very simple function called
our sample that does basically the parameterization
for you. You can implement the code yourself,
if you really want to be completionist about
it. But I'm not bothering here because I do
get good results without it. It's kind of
like what the ddpg paper where they use the
Orenstein will lindbeck noise. And then people
that implemented a later kind of threw it
away, because they're like, Wait a second,
this is unnecessary complication. That kind
of seems to be the case here where it worked
without it. And it's an unnecessary complication.
But I mean, I could always be wrong. But I
do get solid results without it. So here,
we're going to go ahead and instantiate our
normal distribution defined by mu, and sigma.
And our action is going to be our actions
are going to be a sample problem bill it T's
dot sample, if you want to do the re parameterization.
You could re parameterize it there. And the
action is math, a tan hyperbolic times max
action, that max action takes care of the
fact that our environment may have max actions
outside of the bounds plus or minus one, which
are of course, the bounds of the tan hyperbolic
function. So you want to not arbitrarily cut
off half of your action space if you don't
have to. Log probs it's probabilities, bilities
dot log prob of actions, and then go ahead
and subtract off math. Log one minus Tf dot
map dot path, excuse me. Action, so we're
going to square the action plus self dot REAP
program. Call it noise did self dot noise.
And so then I had to change it here as well,
don't I? Yeah. self dot noise plus self dot
noise. Do I have the right number of parentheses
here? Yes, I do. And so since you have a logarithmic
function here, you don't want to take the
log of zero that is oftentimes not advisable.
So I just add in some really small number
to make sure we're taking the log of something
finite. So then we're going to go ahead and
do a reduce some on that. Log probs x equals
one to keep them true, in return the actual
and log props. So that wraps up the network's
portion of our code. Now we have to handle
the agent class, which will tie everything
all together. So of course, again, we have
to come back up to the top and add in some
imports. We're going to need our optimizer.
So we'll say from TensorFlow Kerris optimizers
import Adam, and I think that's the only other
import we will need. And then we come back
down here and start our agent class, which
derives from nothing. Our initializer is going
to take a number of learning rates. Now we're
going to use two separate learning rates,
you could in principle, use three one for
the critic one for the actor, one for the
value network. In this case, we're gonna use
one single learning rate for the actor and
then the same, the beta learning rate will
be for the value and critic networks. And
book dims something like eight, that's just
the default for the lunar lander, it's the
only default I know off the top of my head,
we're gonna want to pass in the environment
to get some useful information from it a gamma,
the discount factor for our update equation
backsies for a memory, a million transactions.
Default Value of towel was 0.005. layer one
size layer two size to fit the six bite size
default to 36. And a reward scale default
of two. Let's go ahead and start saving replay
buffer, Mac size and input dims interactions
pretty straightforward. Then we're going to
go ahead and define our networks. Sir Max,
Max action is going to be the high value of
the access space from our environment. We
need to our first critic, second critic. And
these are you know, identical in terms of
their definitions, except for the name. Because
again, when we save files, we want to save
separate files for both critics otherwise,
we're going to get the most value network
in our target value. Okay, so now we have
to compile our networks. We'll need our critical
one. Great a beta, or environment network
and our target. Now, of course, the target
via network doesn't perform any optimization,
we just copy of weights using our soft update
rule. But it is part of a which is part of
the framework that we have to compile the
model before we can use it our scale factor
and our initial update network parameters,
where we're going to do a hard copy of the
parameters from the online network to the
target value network. The first function I
want to handle is the Choose action function.
Then we'll get to the remember function, the
update network parameters. And our saved model
parameters. Finally, we'll get to the Learn
function will save the best for last. So we
want to convert our state our observation
tensor. And the neural network expects that
we want to have a batch dimension. So we have
to do that. We have to add an extra dimension
to get our batch. We want to get the actions.
We don't really care about the log prompts
at this stage. And then we just go ahead and
return actions zero, because the tensor I
believe is the output and we want to return
a NumPy array because the environment doesn't
accept the TensorFlow tensor as input to this
function. So this is a simple interstate interface
function between the agent and its memory.
And this is necessary because you don't want
to directly access the values of the memory
class from the aging class. It's just bad
software design, you don't want to go overriding
parameters of one class with another, you
want to have an interface function that handles
all that for you that where you use one function
to call another, it's just clean software
design. Always keep that in mind. I think
state action reward new state. And it looks
kind of silly, we could get away with it in
this context, because we're not going to be
building on this codebase later. But I always
find it important to use strong and consistent
software design principles wherever possible.
Our update network parameters function. Pretty
straightforward. So we're going to pass in
a towel, which is a default value of none.
So tau, is none, then we want to go ahead
and use our default value for towel. And this
has to do with the fact that appear on line
153. We're calling update now parameters and
the value of tau equals one to facilitate
the hard network of hard network weight copy.
So we're basically going to just go ahead
and iterate over our network weights, do the
calculation, append those to a list, and then
upload that list to or by you target by your
network. wait times to helpless targets, times
one minus tau. one too many parentheses there.
That looks right. weights. And that is it
for update network parameters. Pretty straightforward.
Next, we have two bookkeeping functions to
save our models. Those don't take any inputs.
Sylvain is not used word formation. So we
just want to save the weights to the checkpoint
final checkpoint, I'll take a point that sounds
pretty good. And then the load models function
is basically the inverse. So I'm just going
to go ahead and yank and paste then make sure
to change save to load. So that way, we don't
do anything wonky. After dot save weights,
we want to change to load weights. And same
deal we want to load from the check point
files. Okay, now we come to the hard part
of the problem, the Learn function, we've
got a kitty cat joining us, perhaps she will
hop up on the desk, the first consideration
we have. So the first consideration we have
is what we would do what we do in the event
that we haven't filled up enough of our memories
to actually load a batch size of those memories.
And you can do many different things. You
can play batch size of transitions randomly
and store the transitions and then call your
learning function. Or in this case, you can
just say, hey, if I haven't filled up at least
batch size of memories, just go ahead and
return. So that's what we're going to do.
And this is a this violates my principles
of good software design where I'm calling
the mem counter or the memory class directly
here instead of using a function to get it.
This is bad design. So don't do this. If you
have an option not to I'm just doing this,
because it's like I said, it's not gonna matter
if this isn't a huge code base that we're
going to be building upon later. But just
know that this isn't consistent with what
I said earlier. I'm kind of backpedaling.
A little bit. Now we have a sample our memory,
we're gonna pass in our batch size, then we
want to go ahead and convert those two tensors.
And I want to be really consistent with my
data types here. Reason being a lot of these
frameworks, TensorFlow and pytorch, specifically,
get a little bit finicky when you start mixing
up data types. They don't like to mix data
types and calculations, because that screws
up the precision of the calculation. They
want everything to be. They want everything
to be consistent and explicit, so that you
get exactly what you expect, because of course,
there is a rather a significant difference
in large calculations between floating point
32 floating point 64, even in floating point
16 calculations. So it's very beneficial to
go ahead and specify what data type you're
using. In this case, we use float 32. And
we even in theory could get away with sorry,
these are TF not NP, we could get away with
16 point because 16 point precision 16 bit
precision, excuse me, because we don't need,
you know, a whole bunch of decimal places
in our rewards or anything like that. But
we use 32. Just to be safe. So now we're going
to handle the update rule for our value network.
So we need our gradient tape. So we're going
to pass the state states through our network
and then squeeze to get rid of that batch
dimensionality, then we're going to do the
same thing with our new states. except we're
going to pass through the target by a network.
Squeeze again, what are the actions according
according to our current policy, and the log
probs. So we're going to sample normal states.
And we of course, don't do any real parameterization.
Within our log probs, we have to squeeze and
we get the values according to the new policy
critic one state's policy actions, and then
the value according to the second critic,
then our critic value you TF that squeeze.
Do you have math minimum between the q1 new
policy and q2 policy and I'm missing a privacy
there. There we go. Now we're to squeeze along
the first dimension. So then our value target
is going to be our critic value minus our
log prompts. That's from the paper value loss.
One and a half keros losses dot mean squared
error between the value and the target value
value targets sorry. Sorry, my toddler's rampaging
again. So now we need to calculate our gradient.
Good grief. I have the cat on my lap as well.
It's making it very difficult to type value
loss self dot value dot train abort variables,
and we can stick that on new line to be good
little programmers. Then we have to apply
our gradients optimizer dot apply gradients
zip by network gradient value trainable variables,
and this is just how we do gradient descent
using the gradient tape. And so that is it
for our value network loss. Now, we have our
Actor, network and critic networks to worry
about. So we need more gradient tapes. So
we need our new policy actions and their log
probs on a sample are states Oh, that's interesting.
I have I see. That's interesting in my cheat
sheet here I have the NumPy arrays, and it
works. Interesting. I'll go ahead and fix
that. So let's see if it breaks when I pass
in the TensorFlow tensors it shouldn't I just
have a typo in my cheat sheet. So let's go
ahead and squeeze our log probs and get some
new policy values one policy actions here
two new policy 
we have our critic value squeeze math minimum
then I'll actor loss just the log probs minus
i critic by Michael loss goals. Math reduce
me Petra loss. Then we have our actor network
gradient. Tape gradients after loss. The gradient
of our loss with respect to our accurate trainable
variables that we want to apply our gradients,
actor optimizer. And of course that expects
zip as input. And that handles our actor loss.
And finally we come to the critic loss. Now
we have to pass the persistent evil true parameter
to our function call, because the loss is
going to have two components. So we'll have
a critic one and critic two loss. If you don't
pass versus an equals true flag, it only keeps
track of stuff for the application of a single
set of gradients. So you can only do the update
to one of your critic networks instead of
both. So you tell it to just keep track of
the gradients even after it Go ahead, even
after it applies gradients one time. So you
can apply gradients twice. Or q hat is our
scale factor and multiplied by reward. That's
where the entropy comes in. gamma times value
underscore one minus done. That is interesting.
So this gives me a little bit of pause, because
first of all, I know the code works, I've
tested this, and I haven't accidentally deleted
a line. But what gives me pause here is that
I have this value underscore, which is defined
up in this other scope here. I'm actually
missing a equal sign there. That would have
triggered an error. So it does work. That's
interesting. I didn't know that the context
manager shared the scoping of variables. I
didn't know that that is new information to
me. So I want to go ahead and roll with it
for now. If this is suddenly broken and doesn't
work, then I will come back and re edit this
and put in the new code that actually works,
of course. But that is new information to
me. Say I learned stuff even while making
content. So we have to get the values according
to the old policy. Why do I call it old policy?
Perhaps go ahead and check the GitHub for
this when I upload it. I will probably do
some variable name swapping to make things
a little bit more logical. If I have to scratch
my head while I'm typing out the code here.
You're probably scratching your head watching
it Of course, the losses are pretty straightforward,
just mean squared error between the q1 and
q2 old policies and this Q hat value, then
we can go ahead and calculate our gradients.
Critical network gradients. Want to go ahead
and apply our bruneians. Sorry, so blood critic
to network. Brilliant, predict two trainable
variables. And then we want to call our update
number of parameters function after we've
done all of our updates, okay, so that wraps
up all of the heavy lifting for our code.
Now we can move on to coding up the main loop,
which is going to be a cakewalk by comparison.
And of course, I have an invalid syntax error.
Oh, that's easy. There is no self there actually
is blank equals. And I've done it yet again.
And oh, because it's a sorry, my cat wants
to steal the limelight. There we go. I did
the same thing here. Of course, why wouldn't
I do the same thing there, I like to be consistent.
Okay, so now we're going to handle the main
function. So of course, we will need our pie
bullets. And these could be because we're
going to be dealing with the inverted pendulum
and bullet environment, we need our gym we
need NumPy we need our agent. We need our
plot learning curve. And that is it. Want
to make our environment inverted? pendulum
board envy me zero. Need to make far agent
passing all of our roles and parameters to
play 250 games. Figure file 
is just the directory plus the file name,
you can condense that into one line if you
want, it's not a problem. We save best score
in the reward range zero score history want
to keep track of the scores the agent receives
over time so we can see if it's learning as
well as to plot them later. And a load checkpoint
variable, which you can set to false if you
want to load a checkpoint. So we're going
to load the checkpoint and load your models.
And one thing I like to do is set the render
mode to human so that we can see the agent
and play the game because if you're loading
a checkpoint, you probably want to evaluate
the performance. If not just comment out this
line. In other words, if you're doing checkpointing
so that you can do more training later than
just get rid of that line. No big deal. Let's
go ahead and play our game. To reset your
environment, at the top of every episode,
you set your done flag and set your score
to zero. Play your episode by choosing an
action. Take your action, get the new state
reward done and debuginfo back from the environment
to crack your score. Remember your transition
not load checkpoint, then you want to learn
the logic here being that if you're loading
a checkpoint, you're probably evaluating.
If it's the case that you're just loading
a checkpoint to perform more training later,
then you're going to need to put that agent
dot learn outside of that if statement, just
get rid of the if statement. Okay, so that
you're, you know, gonna do what you actually
intend. No matter what you need to set the
current state to the new state. And at the
end of the episode, you want to append the
score to the score history, calculate an average
of the previous 100 games. If that average
score is better than your best score, then
set the best score to that average. So that
you know you're learning. And again, if not
load checkpoint agent does save models and
you want to print episode score average score
and then when all the games are over. Go ahead
and what's your learning curve? Okay, moment
of truth. Let's see how many typos I made.
Hello. That's easy. plog replay buffer as
attribute new size. It should be mem size
self dot new size. I didn't do that elsewhere.
I don't think Okay, once that I not call it
call that is interesting. So that is for my
value network. Oh, okay. replay buffer critic
and value network. Oh, ah. There we go. Sorry
about that. indentation error. As no attribute
sample. That's because it is sample normal.
That is in line 223. It does sample normal.
persistent. Because I forgot to t that is
in line 239. Verses 10. equals true that's
another typo perfect. Your ops is not callable.
See self dot gamma times value underscore
oh I'm missing a multiplication sign. That
is in line 240 Sorry about that. On 241 mine
is done. Good grief critic. One underscore
That is in line 251. Oh, because it's not
there's no self in there. Oh, okay. Good grief.
There is no self there. Yep. Okay. Greens
do not exist for one of my layers. Oh, okay.
means I have forgotten something in deed that's
problematic because it doesn't tell me. Well,
these warnings here, don't worry about those
those aren't a problem this Grady does not
exist is in fact a problem. So let me see,
I don't even though which network it is talking
about there doesn't tell me says dense kernel
zero. So that's probably whichever network
I made first. So I instantiate my Acura network
first. So here is value after a loss. Let's
check out our handy dandy cheat sheet here.
And so I have located the source of the issue.
And I will annotate this when I edit the video.
But the issue here is that I was passing the
state through both the first fully connected
and second fully connected layer, instead
of allowing the output of the first fully
connected layer defeat into the second layer.
So of course, that's how deep neural networks
work. And you won't get anything useful if
you don't feed things through properly. So
let's go ahead and try it again. Okay, so
now we got the same warning. And that's not
concerning. It's just has to do with the precision.
So that's something you can deal with on your
own. If you want, I'm not going to go changing
the backend settings, I could suppress the
warning, but it doesn't really bother me that
much. So I'm going to come back in a few minutes
and make sure that this is learning. And when
I do, I'll go ahead and show you the output
of a fully trained network. And if it doesn't
learn them to go back and debug it, and you'll
know because I'll tell you in a few minutes
from your perspective. Okay, so here we are,
it's just a few minutes later, by game 60,
we can see that the average score is improving
with each episode, meaning that it is in fact
learning and it's getting well over 100. And
so I'm gonna switch to the other terminal
where I have finished running it, I'm not
gonna wait for it to play all 250 games, because
it does take quite a while once it gets closer
to 1000 steps. So hold on one moment. So here
is the output of an earlier model I trained.
And just checking the other window to make
sure it is indeed still learning it is it's
saving models pretty much every game. So this
is an output of a model I ran earlier, where
you can see that for the last several games,
it gets a consistent score with 1000, with
a little bit of a low flyer here at 751. Still
a respectable score. It doesn't have 100 games
1000. So the average score hasn't hit 1000.
But it is trending well up on its way. So
I consider that to be a fully trained model.
If it ran another 50 games 100 games, then
the average score would be 100 new, and we
would have beaten the environment. So that
assault after critic in TensorFlow two, I
left out the re parameterization. Because
I don't think it's entirely necessary. I don't
know why it was put in the original paper,
I have implemented it in pytorch. You can
see that video, if you want to see how that
works. It's just you passing in a parameter
to your sample function. And if that parameter
is true, then you use the our sample instead
of the sample function from your distribution,
if you want to implement that in TensorFlow
to leave that as an exercise to the viewer,
but you would have to do some simple calculation
of adding in some spherically sampled noise
to it. So that's not you know, what's not
particularly difficult, which is something
I didn't want to bother with. So this agent
learn that is sought after critic in TensorFlow
to an incredibly powerful algorithm. Indeed,
state of the art I prefer TD three, I think
it tends to perform a little bit better, but
this is certainly no slouch in and of itself.
I hope that was helpful. Leave a comment a
question? Subscribe, certainly, if you made
it this far, and I will see you in the next
video. If you give me about 45 minutes of
your time, I will show you how to code a fully
functional asynchronous advantage actor critic
agent in the pytorch framework starting from
scratch. We're gonna have about 10 to 15 minutes
of lecture followed by an about 30 minute
interactive coding tutorial. Let's get started.
Really quick if you're the type A person that
likes to read content, I have an associated
blog post where I'm going to go into much
more detail, check the link in the description.
Deep reinforcement learning really exploded
in 2015. With the development of the deep
q learning algorithm. One of the main innovations
in this algorithm that helped it to achieve
such popularity is the use of a replay buffer.
The replay buffer solves a very fundamental
problem in deep reinforcement learning. And
that problem is that neural networks tend
to produce garbage output when their inputs
are correlated. What could be more correlated
than an agent playing a game where each time
step depends on the one taken immediately
before it, these correlations cause the agent
to exhibit very strange behaviors, where we'll
know how to play the game and suddenly forget,
when an account or some new set of states
that have never seen before, the neural network
really isn't able to generalize from previously
seen states to unseen states. Due to the complexity
of the parameter space of the underlying problems.
The replay buffer fixes this problem by allowing
the agent to randomly sample agents from many
many different episodes. This guarantees that
those time steps taken are totally uncorrelated.
And so the agent gets a broad sampling of
parameter space and is therefore able to learn
a more robust policy with respect to new inputs.
As I've shown before on this channel problems
arise when you attempt to simply bolt on a
replay buffer onto the actor critic algorithm,
it doesn't really seem to work. And in fact,
it's not very robust. Actor critic methods
in particular, suffer from being especially
brittle. And so adding on a replay buffer
really doesn't help to address that problem.
In 2016, a group of researchers managed to
solve this problem using something called
asynchronous deep reinforcement learning.
It's a totally different paradigm for approaching
the deep reinforcement learning problem. And
in fact, the technology can be applied to
a wide variety of algorithms, and the original
paper they detail solutions for deep q learning.
And step sarsa. Excuse me, instead, q learning
as well as sarsa, and actor critic methods
as well. So what is this big innovation? Well,
instead of having a replay buffer, we're going
to allow a large number of agents to play
independently on totally separate and self
contained environments. Each of these environments
will live on a CPU thread, in contrast to
a GPU for most deep learning applications.
This has the additional benefit that while
if we don't use a replay buffer, we don't
have to store a million transitions, we were
trivial environments really doesn't matter.
But if you're dealing with something like
say, the Atari library, a million transitions
can take up a significant amount of RAM, which
can be a limiting factor for enthusiast. So
having the agent play a bunch of different
games in parallel on separate environments,
only keeping track of a small number of transitions,
vastly reduces the memory footprint required
for deep reinforcement learning. So in what
sense exactly is this algorithm a synchronous?
Well, this means exactly in this context is
that we're going to have a large number of
parallel CPU threads with agents playing in
their own environments, they're going to be
acting at the same time, but at various times,
they're going to be deciding what to do as
well as updating their deep neural network
parameters. And so we're not going to have
any one agent sitting around waiting on another
agent to finish playing the game to update
its own set of deep neural network parameters.
Each one will be totally independent, and
learning on its own. Now, we're not going
to be simply throwing away the learning from
each agent average finishes the episode, rather,
we're going to be updating the network parameters
of some global optimizer as well as some global
actor, critic agent to have one actor, critic
agent that sits atop all the others, and the
local agents that do all the learning by interacting
with our environments. So what is the advantage
part of a three scene. So the advantage essentially
means what is the relative advantage of one
state over another, it stands to reason that
an agent can maximize his total score over
time by seeking out those states which are
most advantageous or have the highest expected
future return. The paper gives a relatively
straightforward calculation for this, all
we have to do is take the discounted sum of
the rewards received over some fixed length
trajectory, and then add on an appropriately
discounted value estimate for the final state,
the agent saw in that trajectory. Please note
that this could be some fixed number, like
say five steps, or it could be three steps
that the agent encountered in Terminal Terminal
state along the way, we're then going to go
ahead and subtract off the agents estimate
of the value of whatever current time step
it's in, in the trajectory. So that way, we're
always taking the value of the next state
minus the current state. That's what gives
us the relative advantage. So what does the
actor critic portion of a three c mean? Specifically,
this refers to a class of algorithms that
use two separate neural networks to do two
separate things. So the actor network is responsible
for telling the agent how to act kind of a
clever name, right? It does is by approximating
a mathematical function known as the policy,
the policy is just the probability of selecting
any of the available actions for the agent
given it's in some state. And so for discrete
action space, it's going to be relative probability
selecting one action over another. So in our
car pool, it's going to be say 60%, move left
40% move right, so on and so forth. We're
going to facilitate this by having two separate
networks, the actor network will take a state
or set of states as input and output a softmax
probability distribution that we're going
to be feeding into a categorical distribution
from the pytorch. framework, we can then sample
that categorical distribution to get the actual
action for our agent. And we can also use
that to calculate the log of the probability
of selecting that action according to the
distribution, probability distribution. And
we use that for the update rule for our actor.
Now, the critic has a little bit of a different
role, the critic essentially criticizes what
the agent the actor did, it said, you know,
that action you took gave us a pretty lousy
state that doesn't have a very large expected
future return. And so we shouldn't really
try to take that action given that state any
other time that we encounter it. So the critic
essentially criticizes what the actor does,
and the to kind of play off of each other
to access more and more advantageous states
over time. Before we go ahead and talk about
the specifics of each class, let's get some
idea of the general structure and flow of
the program. The basic idea is that we're
going to have some global optimizer and global
actor critic agent that sits on top that keeps
track of everything the the local agents learn
in their own individual threads. Each agent
will get its own specific thread work and
interact with its own totally distinct and
separate environment, the agent will play
either some fixed number of time steps or
until it encounters a terminal state, at which
point it will perform the loss calculation
to do the gradient descent on the global optimizer.
Once it calculates those gradients, it's going
to upload it to the global optimizer, and
then redownload the parameters from that global
optimizer. Now keep in mind, each agent is
going to be doing this asynchronously. So
while one agent is performing its loss calculations,
another agent may have already finished that
loss calculation and updated the global optimizer.
That's why right after calculating the gradients,
we want to go ahead and download the global
parameters from the global actor critic. So
that way, we make sure we are always operating
with the most up to date parameters. After
each time the agent performs an update to
its deep neural network, we're going to want
to go ahead and zero out its memory so that
it can start fresh for another sequence of
five or until it encounters a terminal state
number of steps. So now let's talk implementation
details. We're gonna have a few separate distinct
classes for this, the first of which is going
to be overriding the atom optimizer from the
base pie torch package. So we're gonna have
a shared atom class that derives from the
base torch optim, Adam class. And this will
have the simple functionality of telling pytorch
me want to share the parameters of a global
optimizer among a pool of threads, it's only
going to be a few lines long, and it's much
easier than it sounds, and I'll show you how
to do it in code. Our next class will be the
actor critic network. Now, typically, we would
use shared input layers between an actor and
critic where we simply have one input layer
and two outputs correspond to the probability
distribution pie and the value network meme.
But in this case, we're going to host two
totally separate distinct networks within
one class. It's a relatively simple problem,
the car pole. And so we're going to be able
to get away with this. The reason I'm doing
it this way is because I frankly could not
get shared input layers to work with the pytorch
multi processing framework. Our agent will
also have a memory which we're just going
to use simple lists. For that, we're going
to append states actions and rewards to those
lists, and then go ahead and set those lists
back to empty lists. When we need to clear
the agent's memory, we're going to have a
function for calculating the returns where
we're going to use the calculation according
to the algorithm presented within the paper.
So the idea is that we're going to start at
the terminal step, or the final step and the
trajectory. If that step is terminal, the
R or the return gets set to zero, if it's
not, it gets set to the current estimate of
the value of that particular state, then we're
going to work backward from the T minus one
time step all the way to the beginning. And
we're going to update our as r sub i plus
gamma times the previous value of r, I'm going
to do a calculation in the video, the coding
portion to show you that these two are equivalent,
meaning this calculation as well as the earlier
advantage description I gave you, I'm going
to make sure that you understand that those
are actually equivalent. And it's just a few
lines of mathematics. So it's not really that
difficult. And I've taken the liberty of doing
it for you, then we're going to be calculating
the loss functions. And these will be done
according to the loss functions given in the
paper. So for our critic, we're going to be
taking the delta between those returns and
the values and taking the mean squared error.
For our actor, we're going to be taking the
log prop of the policy and multiplying it
by the advantage. And with the negative one
factor thrown in there as well. Now that's
a really cool way of calculating the loss
for the actor because it has a pretty neat
property. So when we multiply the advantage
by the log of the probability, what we're
actually doing is waiting at probabilities
according to the advantage they produce. So
actions that produce a high advantage are
going to get naturally weighted higher and
higher over time. And so we're going to naturally
evolve our policy towards being better Over
time, which is precisely what we want, right?
Our final class will be the agent class. And
this will derive from the multi processing
process subclass. So here's where all of the
real main type functionality is going to happen.
So we're going to be passing in our global
optimizer as well as our global actor, critic
agent, instantiating 16, in the case of 16,
threads for a CPU, local critics with 16 separate
environments. And then each one of those is
going to have you know, two separate loops,
where it's going to go up until the number
of episodes that we dictate, and it's going
to play each episode, as I described earlier,
within each episode is going to play some
fixed sequence number of steps. And then it
is going to perform some update to the global
optimizer and then download the parameters
from the global actor, critic agent. Our main
loop is basically going to set everything
up, we're going to go ahead and define all
of our parameters, create our global actor,
critic, our global optimizer and tell pytorch
if we want to share the memory for our global
actor, critic, agent, and then we're going
to make a list of workers or agents. And then
we're going to go ahead and send each of those
a start command as well as a join command
so that we can get everything rockin and rollin.
So what are some critiques of this algorithm
overall? Well, one is that it is exceptionally
brittle. Most actor critic methods require
a fair amount of hyper parameter tuning. And
this one is no exception. I tried to use the
lunar lander environment, but couldn't really
get a good set of parameters to make it run
effectively and get a you know, a consistent
score of 200 or above, or Heck, even a consistent
score of over 100 out of call that good enough
for YouTube. Another one is that there is
a significant amount of run to run variation.
So it's highly sensitive to initial parameters.
You can solve this by setting global seeds
for the random number generators so that you're
getting consistent random numbers over time,
and so you're going to know exactly how you're
starting. But to me, it's a little bit kind
of like cheating. So I don't do it in this
video, but it is something to take note of.
And in the original paper, I think they do
something like 50 different runs of each evaluation
some large number to get a pretty tight or
to get a pretty solid distribution of scores.
And that is, I think, because of the high
degree of run to run variation. Okay, I have
lectured at you enough. Again, if you'd like
to read written content, I have a link in
the description to a blog post, where I talk
about this in a little bit more detail. But
nonetheless, let's go ahead and jump right
into the coding tutorial. Let's go ahead and
start with our inputs to the gym for our environment.
They are based towards package we'll need
torture, multi processing, to handle all the
multi processing type stuff, we will need
torch penon to handle our layers 
will need nn functional to handle our activation
functions. And we're going to need our distribution
as well. And in this case, we're going to
need a categorical distribution. All this
does is takes a probability output from a
deep neural network maps into a distribution
so that you can do some actual sampling to
get the real actions for your agent. Now I
want to start with a shared Adam class. This
will handle the fact that we are going to
be sharing a single optimizer among all of
our different agents that interact with separate
environments. All we're going to do here is
called the base Adam initializer. And then
iterate over the parameters in our parameter
groups, setting the steps exponential average
and exponential average squared to zeros effectively
and then telling it to share those parameters
amongst the different pools in our multi threading
pool. And this will derive from the base atom
class. Our default values are going to be
I believe, identical to the defaults for the
atom class. And then we want to call our super
constructor. Now we're going to 
handle setting our initial values 
And then we're going to tell torchiere, we
want to share the memory for our parameters
are for our gradient descent. And note the
presence of the underscore at the end of memory
there. Okay, that is it for the shared atom,
pretty straightforward. Next up, we want to
handle the actor critic network, which will
also encapsulate a lot of the functionality
I would typically put into an agent class
because of my understanding of the design
principles of object oriented software programming.
In this case, I do shimmy a few things around,
because the agent class is going to handle
the multi processing elements of our problem.
And so it doesn't really make sense to stick
this, like the Choose action, or memory, or
memory functionality in the agent class. So
we're gonna stick it in the network class,
it's not a huge deal. It's just a departure
from how I normally do things. And certainly
not everybody does things the same way I do.
So our initializer takes input dims from our
environment, number of actions from our agent,
and a default value for gamma of 0.99. We
also have to save our gamma. And the next
thing we want to handle is writing our actual
deep neural network. Now, this is also a little
bit different than the way I normally do things.
Normally, I would have a shared input layer
that branches out into a policy and evaluate
network as two separate outputs with that
shared input layer. When I tried to do that,
I found that the software doesn't actually
run, it doesn't handle the threading aspect
very well. In that case, when you have shared
input layers from a deep neural network, I
don't know exactly why that is, if you know,
please leave a comment down below, because
I'd be very curious to hear the explanation,
and simply what I found out through my own
experimentation, so we're gonna have two separate
inputs, one for the policy and one for the
value network, as well as two separate outputs.
So they're effectively two distinct networks
within one single class. We're only going
to be using 128 neurons here and not a very
large network. And our output will take those
128 hidden neurons and converted into number
of actions. And our value function will take
likewise, 120 in hidden layers, hidden elements
and convert it to a single value. Or if you
pass in a batch of states a batch of values.
The agent also, excuse me, the network also
has some basic memory, so rewards, actions,
and states. These we will handle just by appending
stuff to a list and then and each time we
call the learning function, we're going to
want to reset that memory. So let's go ahead
and handle that functionality first. So the
remember just appends a state action reward
to the relevant list. And a clear memory function
just zeros out all those lists. Pretty straightforward.
Next, we have our feed forward function that
takes a state as input. So we're going to
pass that state through our first input layer
for our policy and perform a value activation
on that and do something similar for the value
input layer. And the outputs of those two
are going to be passed to the roles to the
relevant policy and value outputs. And then
we just returned pi and V. Pretty straightforward
yet again. Next, we're going to have our function
to calculate the returns from are a sequence
of steps. So this will only take a single
input, and that will be the terminal flag.
Recall that the return for the terminal step
is identically zero. So we need the terminal
flag or the done flag to accommodate that.
So we want to go ahead and convert the states
from our memory to a torch tensor of T dot
float data type, because it is a little particular
about the data type, you don't want to pass
it in double it gives you an error. So best
to take care of it. Now, we're going to go
ahead and pass that through our neural network.
And we're not going to be concerned with the
policy output at this stage, we just want
to know what the value evaluations The Critic
has for that set of states. So our return
is going to be is going to start out as the
last element of that list, so the terminal
step, or the last step in the sequence of
steps. And we're going to multiply that by
one minus done so that if the episode is over,
one minus done is zero, so you're multiplying
by zero, you get zero. Pretty handy way of
handling that, then we're going to handle
the calculation of the returns at all the
other time steps. So we're going to go ahead
and iterate over the reversed memory. And
say that our return is the reward at that
time step plus gamma times are and then just
return actually append that return to the
list of batch returns. And then finally, at
the end, you want to go ahead and reverse
that list again. So that's in the same order
in which you encounter the states. This calculation
reverses it up as you're starting at the end,
when you know the value of the final state,
or at least the estimate of the value according
to the critic, and then reversing it to get
it back in order for passing it into our loss
calculation function. Now, this may be a strange
form to you. If you write it out by hand,
maybe I can show you something here where
I did it for you. If you write it out by hand,
this particular chunk of code, you can see
that it's identical to what they tell you
the calculation is in the paper. So you can
do that exercise on your own to convince yourself
or I can just show it to you so that I can
convince you of it. But this is indeed, the
return calculation from the paper, everything
is as it should be. And then I want to convert
that to a tensor and return. Next, we have
to handle the calculation of our loss function.
And this, again, is only a single input a
terminal flag from the environment, we're
going to go ahead and get the value, excuse
me the tensor representations of our state's
actions right at the beginning. And then we're
going to go ahead and calculate our returns.
And then we're going to perform the update.
So we're going to be passing the states through
our actor critic network to get the new values
as well as then a distribution according to
the current values of our deep neural network,
we're going to use that distribution to get
the log problems of the actions the agent
actually took at the time it took them. And
then we're going to use those quantities for
our loss functions via squeeze. Now, this
squeeze is very important. If you don't squeeze
here, it won't trigger an error, but it will
give you the wrong answer. The reason it will
do that is because the actor loss and the
critic loss, I believe, will come out as a
shape of five by five. And that is not the
shape we want, we want something in the case
of five time steps, team x equals five, so
it'll give you a five by five matrix instead
of a five element vector. So you have to perform
the squeeze here to get the five by one output
of the depot network into some advantages
five, a list of five elements or vector of
five elements instead of five by one. So definitely
that squeeze. If you don't believe me, by
all means, raised a line or commented out
and print out the shapes of things to the
terminal. I always recommend doing that. It's
a good way of solidifying your understanding
of how all this stuff works. So then our credit
loss. It's just the returns minus values,
squared, pretty straightforward. So now let's
go ahead and say We want the softmax activation
of our output. And that has a property that,
of course, the softmax guarantees that every
action has a finite value. And that's the
by the probabilities add up to one as all
probability distributions, distributions should.
So then we use that output to create a categorical
distribution, and calculate the log probability
distribution of our actions actually taken
than our actual loss, minus log probs times
the quantity turns minus values that's from
the paper, and then our total loss is just
predict loss after loss. That mean, we have
to sum the two together, because of the way
that the backpropagation is handled by pytorch.
And, of course, I did forget to choose action
function. But that is not a big deal, we'll
just go ahead and handle that now. So that
will take, we're going to call it observation
as input, because we're gonna be passing on
the raw observation from our environment.
So we have to convert that to a tensor right
off the bat. And we have to add a batch dimension
to that, for compatibility with the inputs
of our deep neural net. And we're gonna call
it a D type of float, we pass that through
our neural network, get our policy and value
function out, perform a softmax activation
on our policy along the first dimension, then
we're going to create our distribution based
on those probabilities and sample it and convert
it to a NumPy quantity. Take the zeroeth element
and return it. So that is it for our actor
network class. It encapsulates most of the
functionality I would associate with the agent,
but it does everything we're going to need
each of the independent actors within their
own thread to do now we're going to move on
to the agent classes going to handle our multi
processing functionality. So I'm going to
call this agent, you will sometimes see it
referred to as worker and that is that fine
name, that is kind of precisely what it is.
I'm just using agent to be consistent with
my previous nomenclature, it acid derived
from the NP dot process class. So we get some
access to some goodies there. So we will need
our global actor predict that is what is going
to handle the functionality of keeping track
of all the learning from all of our environment
specific agents. The optimizer that is going
to be the shared atom optimizer that we wrote
earlier. Input damns number of actions, gamma
in case you want to use something other than
0.99 a learning rate, a name to keep track
of each of the workers from our multi processing,
global episode, index. So this will keep track
of the total number of episodes run by all
of our agents. It's not as easy as our content
is it may seem because you're doing asynchronous
processing, as well as an environment ID.
So the first thing we want to do is call our
super constructor. And go ahead and start
saving stuff. So our local actor critic, is
just going to be our new actor critic, with
inputs of input demos, number of actions and
gamma. You want to save our global actor critic.
So that we can update its parameters. We're
going to have a name for each worker and its
own independent thread. And that's just going
to be just work or a number name. And then
we'll have an episode index. Well, welcome
so that it backs our environment. So environment
it is a string here if that isn't clear. And
our optimizer, I spell that correctly. Yes,
I believe I did. So we have to define a very
specific function. So that stuff actually
works and that function is the run function.
This gets called behind the scenes. By the
worker dot start function that we're going
to handle in the main loop. But this handles
effectively all the main loop type functionality
of our problems. So our global time step is
going to get set to one, while our episode
ID x dot value, so episode ID x is a global
parameter from the multiprocessing class,
and that we want to get the value by using
the dot value dereference. And while that's
less than number of games, some some global
variable we're going to define. In other words,
what while we have not completed all of our
games, set your terminal flag to false, reset
your environment and set the score to zero.
And go ahead and clear the agent's memory
at the top of every episode. And then while
you're not done, so go ahead and play your
sequence of episodes. I'm sorry, a sequence
of steps within an episode. Action is chosen
by a local actor, critic to pass the observation
into each local actor critic. So each of the
16 in this case, threads will get its own
local actor critic and that'll synchronize
took a global but you never actually use the
global network directly to do things like
choosing actions. So get the new state reward
done and a bug info back from your vironment.
Keep track, other award received as part of
the total score. Remember that observation,
action reward, then we have to say, if the
number of steps modulus, the maximum number
of steps is zero, in other words, if it is
every if it is every fifth time step, or we
have finished the episode with that last time
step, then we're going to go ahead and perform
our learning step. Last T, Max, or we're done,
then we're going to go ahead and handle the
learning functionality. So we'll say loss
equals local actor predict loss. Of course,
we need the most recent terminal flag as the
parameter for that function, the argument,
go ahead and zero your gradient and back propagate
your loss. So we're going to set the parameter
for the global gradient to the local agents
gradient at this step. And then, after doing
that, we can tell our optimizer to step and
then we can go ahead and synchronize our networks.
And we do that by loading the state dictionary
from our global actor critic to our local
one. And then we want to go ahead and clear
the memory for each learning step, and then
tell the algorithm to increment t step by
one the global time step and set the current
state to the new state. Then at the end of
every episode, we have to handle the the fact
that we may have finished an episode from
another agent while one thread was running,
because this is running a synchronous asynchronously.
So we say with self dot global sorry, self
dot episode ID x get locked. So we want to
make sure that no other thread is trying to
access that variable right now. And if we
can go ahead and increment that episode value
by one. Then we do the usual print the debug
stuff to the terminal. We're going to print
out the name of The worker, the episode that
is self episode, Id x and the reward the total
score from the episode, Matt, is it for our
agent class. So this is relatively straightforward.
All we're doing is handling the fact that
we have our local parameter, our local agent
that uses actions, we're going to perform
the gradient descent optimizer update using
the gradients calculated by the actions of
that agent. And then upload that to our global
network. So that every every agent uploads
this learnings to the global network, and
then we're going to go ahead and download
that from our global network as well so that
we make each agent in its own thread better
over time. Now we have our main loop. And
we're going to do some stuff like declaring
our learning rate, or environment ID. Card
poll, the zero number of actions for that
environment is just to left and right, input
them. So it's just a four vector, we'll say
number of games 5000. You know, we can actually
get away with 3000, I believe, team Max, every
five steps that comes from the paper, a global
actor critic, gets initialized here. And that
takes input demos and number of actions. Want
to tell that global actor critic object that
we want to share its memory? And we have our
optimizer? What are we going to be optimizing
and sharing the parameters of a global actor
critic network learning rate defined by learning
rate and betas? 0.920999 I get that from more
banjos stuff, I haven't experimented with
that too much. A little bit of cargo cargo
called programming here, I apologize for that.
Now we have our episode nine is just going
to be a value of Type II, that means unsigned
integer, doesn't mean I as an account or variable.
You could also have D for a double, if you
wanted to keep track of like a score or something
like that. I just mean unsigned integer, or
no, it's a it's a signed integer can be negative
as well as positive. Now we have to create
a list of workers. And that's going to be
a list of our agents. And this is what's this
is the construct that we're going to use to
handle the starting and running of our specific
threads. So that's an agent, we have to pass
on our global actor, critic, or optimizer,
our input number of actions, a gamma learning
rate, it's helpful to use commas, our name
is just going to be the the name is just going
to be the AI in our list comprehension. Our
global episode ID x is going to be that global
variable we just defined. And we're going
to pass in our environment ID for i in range,
multi processing dot CSV count. Now, I haven't
noticed a huge difference. And running this
with different numbers. So something other
than CPU count and the range, I have 16 threads,
if I tell it eight, it still uses all 16.
So I'm probably missing something here, I
could do a deeper dive into the documentation
to figure it out. But for the purposes of
the video, I'm just going to go ahead and
roll with it. So now we want to go ahead and
start all of our workers and then go ahead
and do our join operation. Okay, so now moment
of truth, I want to find out how many typos
I made in the recording of this. Let's go
ahead and do a right quit. I already have
an invalid syntax. So I say for reward in
that's an easy one. No others. So let's say
a through c.pi. Okay, name, envy is not defined
that's because it is self thought envy. So
that is in line 114 self thought can be reset
cannot assign torch flow tensor as child module
pi one torch and in module or non expected
so that is when I handle the feed forward
in our choose action function who this is
an interesting one. I haven't encountered
this before let's take a look cannot assign
as child module pi one. Oh, it's because interesting.
Oh, of course. Yeah. Obviously, I don't want
that self here. odd to annotate that one I
am editing the video, that is obviously going
to be problematic. So you'll probably have
seen that already that I put in a no self.in.
There. Okay. As the beauty of doing things
for an O scoters. Log problem, that log probs
that's in 85. Problem. Oh, by Remember, all
these non type object has no attribute backward,
I probably forgot to return my loss. Yeah,
that's right. There we go. All right, now
it's running. Hopefully, it's still gonna
record, even though it is running across all
of our threads. Looks like we're still good.
Now you see that executed really, really fast.
It was so fast, I didn't even have time to
get up the thread count the Resource Monitor
to show you guys. But you can see here that
it achieves a non negligible scores that indicates
some learning is going on. Now. Let's go ahead
and run it again. And you can see now this
time, it's not doing any learning at all.
So there is a significant amount of runner
run variation. So I'll let it run and finish.
And we'll try one more time. And maybe we'll
get lucky. And it will actually approach a
score of 200. Now, that's one of my big criticisms
of this algorithm is that there is a significant
amount of run to run variation. And it is
pretty brittle with respect to some other
algorithms that came later. Basically, the
main advantage here is that it's a synchronous.
And this paradigm can be employed on many
different types of algorithms. As you read
in the paper, you know, it can be applied
to deep learning, as well as and step learn
stuff like that. Okay, so it's getting a little
closer. Okay, so this one took a little bit
longer, because the episodes actually achieve
scores of 200, which is the max, not every
thread manages to be a winner, not all of
us can be winners in life. That's a nice little
microcosm of the universe. But you can see
that for the most part, some of the workers
actually one throws consistency amongst so
worker 11, not 200. And if I scan down here,
another worker 11 140 to 161. Interesting,
so you do get actual learning across your
agents. It's just not super consistent. And
that is my biggest criticism of this algorithm
is that it has a high degree of render and
variation, as you can see, but it's still
pretty good. It's an interesting take on deep
reinforcement learning. And I do enjoy the
algorithm. It's just not what I would categorize
as the top two or three algorithm and actor
critic methods. I hope this was illustrative
for you. I hope you found it very helpful.
If you have made it this far, please consider
subscribing, leave a like a comment down below
and I'll see you in the next video.