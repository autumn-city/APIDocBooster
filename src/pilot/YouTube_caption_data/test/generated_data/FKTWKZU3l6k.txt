okay let's get
started last session we talked about
the first paper that started it all
started the
field of deep learning as we know it
today
we talked a little bit about
convolutions
we talked about the type of loss
function that we are going to use and
how to evaluate our models
there are some methods that i'm not
covering from the paper
i will cover them in other papers
through the means of studying other
papers
for instance they were using drop out
for the fully connected layers
and they were using activation functions
like relu
rectify rectified linear units
they were using data augmentation which
i didn't cover from the paper
but i will cover them through the means
of other papers
let's talk about dropout first
uh so what is dropout
there are two objectives
when you do drop out in deep learning
one is to prevent your models from
overfitting
and what is overfitting over fitting
happens when you are
[Music]
basically your model is too powerful
that it's gonna memorize your training
data so it's going to do a perfect job
on your training data but who cares
about the training data
we want to take our algorithm the one
that comes out of
deep learning and put it into production
so our algorithm should be able to do a
good job actually on the
test data
and actually in production
so if your model does a great job
on your training data but does very
poorly on your test data
then that's called overfitting
and usually convolutional neural
networks
are less prone to overfitting because of
the weight sharing idea
so you are sharing the same filter
which is for instance three by three or
eleven by eleven
and you're sliding it over your image
because of the weight sharing idea those
are less prone to overfitting
but the densely connected ones
the ones that you usually put at the end
of your
neural network to extract your features
those are prone to
overfitting so dropout is going to take
care of that
another objective is to combine
multiple models together
in a smart way so usually
when people report their accuracy
for their evaluation metric
they usually combine multiple models
together they have an
ensemble of models so ensembling is a
great idea
for your model to be able to generalize
to your test data and
actually in production the idea is that
you're gonna have
multiple models maybe thousands of
models
do the same task and then they are gonna
vote
is this image a cat is this image a dark
and then the consensus is gonna be
the output of the ensemble model
so dropout is a way of ensembling
in a smart way we're gonna see why it is
smart
so you have two objectives when you do
drop out
the network on the left is a standard
neural network
these are the inputs and these are the
neurons
so the input could be
that particular portion of the image
that you have or it could be
the dimensions so think of this as input
dimensions
and the neuron is gonna
be the output of multiplying a bunch of
weights
by your input dimensions adding them up
basically it's a dot product and then
putting and putting the output here
after pushing it through a non-linearity
so this is going to give you a neuron
that's going to give you another neuron
and so on and they have different
weights as you can see
okay that's how to visualize a neural
net
what dropout does is gonna drop
some of your inputs
and it's gonna drop at random
some of the neurons it means that now
you have
less weight to do your summation on
what is the idea the idea is that a
single neuron
should be able to do its job
as independently as possible
this is what people call co-adaptation
you don't want your neurons to co-adapt
you don't want them you want them to be
independent
as independent as possible
so that's why it's going to help with
overfitting
the other aspect was ensembling
each one of these guys
is going to be a neural net on its own
with less weights and biases
so it has much fewer rates and biases
and less neurons
it's definitely a weaker neural network
compared to the one on the left
but at each iteration you're gonna
kill some of these neurons at random
so you have thousands of
uh or millions of these uh
small neural networks that are being
trained at the same time
and now you need to on to form your
ensemble in the end
and how are we going to do that is
coming up
so there are two objectives one is
preventing overfitting
in that sense it's just a regularization
technique
the other one is you want to combine
exponentially
many different neural nets and these are
the type of the neural net architectures
that you are combining
can i ask a quick question sure um
so when you say like these are if you're
viewing each neuron as sort of an
independent
uh neural network you mean that in the
sense that
each is its own model with many
different
layer types and it's like own sort of
structured neural network not just like
one sort of
type of layer connected like over and
over again if that makes sense
uh like it's not just one of these
neurons it's not like a single neuron is
just
only convolutional layers and then
another neuron is only
like fully connected layers so what i'm
talking about here in this slide is very
generic
so it could be a convolutional neural
net it could be recurrent it could be
lstm
it could be fully connected it could be
whatever
okay and it could be any combination of
any of those layer types in one model
yes exactly okay cool thank you but the
idea is general
okay some point in your training you're
gonna kill
your neurons okay basically perish data
point that goes in
that data point is going to see a
different model
okay at random
[Music]
so here is another way of looking at the
same topic
so this neuron for instance this one
is gonna be present with probability p
perish round of training
so each data point is going to come in
and it's either c this
is that is either going to see this
neuron or is not gonna see this
neuron basically it's either gonna go
through
it or it's absent it has to go through
another route
when you do your forward propagation
that's during training but at testing
you don't want things to be random
you don't want to combine you want to
combine multiple
neural network architectures together
and the way that the paper is doing it
is keeping all of these neurons so this
neuron is going to be always
present so all of the neurons are going
to be present
in the end but then you are going to
make your weights smaller
by the same ratio by the same
probability ratio it's sort of averaging
the question is is applying dropout to a
large
neural network equivalent to the idea of
ensembling
yes so it's equivalent to the idea of
ensembling
but it's approximate ensembling and the
ensembling is happening here
when you're keeping all of your neurons
but then you are scaling down your
weights by probability
so it's the expectation of your
neural network but you're doing it
in an approximate way so it's
approximate expected value of
your neural network so in the end what
you get
is an ensemble and as i said the idea of
a dropout
is that each hidden unit basically each
one of these guys
has to be able to work independently
it shouldn't rely on the neural network
on the neuron next to it to do its job
think of it this way if i were to form a
group
in your class in each session
i would randomly associate you with
another team
and team members this way you don't
learn to rely on person x all the time
to do your work
or you do some of the work the other
person is going to do
some part of the some other parts of the
work but you depend on each other
that's called co-adaptation
in our neural network we don't want that
to happen
during a single back propagation pass
are the weights of the dropped neurons
still
updated or
they are excluded so they are going to
be excluded
because they are absent for that
particular mini batch of data
they are absent it means that their
rates are going to be zero
and they are not going to get updated
does that answer your question
john okay perfect
so now let's get into the math how do
you actually
write down the math and how do you
implement it
let's say l denotes the number of hidden
layers
in your neural network basically this is
one layer two layers three layers
so that's l and
little l is gonna say this is the first
layer this is the second layer this is
the third layer
that's the index of your layer
what you are seeing here in neuron
is a non-linearity applied on
a hidden z
so that's yl it's a non-linearity
applied on zl
and what is zl it's a dot product
between your rates and your inputs
so if these are your inputs and these
are your weights the arrows
you're multiplying them one by one
and then adding them up together
that's going to give you a single number
and then you can add
a scalar to it that's your bias
that's gonna be your neuron sitting here
that's the eighth neuron basically here
this is the first neuron second neuron
third neuron
fourth neuron fifth neuron and
l is gonna be in this case one
and that's what you're gonna get
that's a typical neural network on the
left
how do you do dropout
at layer l you're gonna sample
from a bernoulli distribution basically
you have a coin
that you flip it and then with
probability p
it's going to be one with probability
one minus p is going to be zero
that's your bernoulli distribution and
they are independent
and at layer l you get
for instance one two three four five
numbers
some of them are zero some of them are
ones
the idea is that you multiply your input
by these random numbers so it means that
the neuron is going to be
some of them are going to be absent some
of them are going to be present
and that's going to give you y tilde l
the question is how do you pick p
usually a good number
for p is 0.5
[Music]
and
for the intermediate layers a good p is
0.5 basically half of the times
you keep the neuron half of the times
you
exclude it
for the initial layer
a good one is probably 0.8.85.9
so it's not that sensitive to the choice
of p
that's why i'm saying a good number is
usually
0.5 but the way that you
these are hyper parameters of your model
the way yet the way that you set them is
usually using cross validation
you take a look at your validation data
and choose the best p
that's doing that's giving you the
least error or the most accurate model
so now that you killed some of your
neurons and you kept
other ones the rest of it is the same as
before
and because some of these guys are zero
we go back to your question what happens
in back propagation
you are multiplying your weight by zero
so that weight doesn't exist
so in your back propagation that way it
is not going to update
because it is absent for instance this
weight is absent
so it's not going to update
and the rest of it is just applying the
non-linearity
the question is can you explain point
two a bit more
and how this is accomplished by the
dropout technique
so yes that one we haven't yet covered
so damien is asking about
this part how do you approximately
combine exponentially many
i will explain that later so so far we
are doing training
and each data point is gonna see a new
model
when it goes through our neural network
so during training each one of these
guys learn to be
independent or as independent as
possible
let's take a look at it visually this
is the l before the activation
zl is going to be a linear combination
of 1 and your
previous neurons coming from the
previous layer
this is exactly what i'm writing here
and the one here stands for because
you're multiplying b
by one so that's how you get your
neuron is neuron at layer l
plus one that's your standard neural
network
and visually you're gonna create some
random numbers here
they are either one or zero and you
multiply it
to get your y tildes so that's how you
kill your neurons
if this number is zero y three hat is
gonna be
zero it means that you're crossing it
out
so it doesn't exist during training
the question is how do you average it
out
how do you average multiple neural
networks out
you do it in expectation
and the way it happens is that half of
the times or
p ratio of the times p
percent of the times
some of the weights are present and one
minus p percent of the time it doesn't
it's not present
the expected value is gonna be p times
the weight during testing you don't want
things to be random
and that's how you do the averaging and
that's how i answer your question damian
this is how you combine exponentially
many
different neural network architectures
and you're doing it in an efficient way
and i'm gonna show you empirically why
this is efficient
there is another technique in the paper
and that's max nor normalization
it's basically weight clipping you don't
want your rate to become
too big that's another technique for
regularizing neural networks so i highly
encourage you guys
to read these two papers especially
the second one
there is a nice i would say
there's a nice page in the paper
that tells you how drop out when you
apply it
to linear regression is to be equivalent
to reach regression and range regression
is basically
regularizing your your weights
by its l2 nor
so it's actually doing a regularization
for us
and that's why it prevents overfitting
so they apply the method drop out
to multiple types of neural networks and
in different domains
for instance they applied in vision
which is the topic of our course so far
they apply to speech text and genetics
that's why i mentioned that these types
of methods are
universal you can apply to any type of
neural network that you have
and these are different data sets and
list
is the digits data set street view
house numbers are the numbers that are
coming from google
taking images google earth
c510 and c4 100
these are low resolution images and they
have 10 labels and 100 labels
respectively there is imagenet
which is a huge data set and the
resolution is bigger
it's 256 by 256. there is timid it's a
data set for
speech and reuters
is a data set for text and the other one
is for genetics
this is the size of the training data
this is the size of the test data
and they apply the method on all of
these
and as you can see some of these
[Music]
data sets are really small compared to
one and a half
million 1.2 million these are small data
sets
so it's very important for us to
regularize our method otherwise neural
networks are gonna overfit
and this is what you get this is the
classification error
the classification error during
training for
different types of models
is gonna end up being a better number
and it's gonna converge faster
with dropout compared to beat up
dropouts
so it's a very effective way of
regularizing your models
is there ever a reason not to use
dropout
sometimes you want to take derivatives
of your neural network with respect to
inputs
then dropout is gonna mess up
with those derivatives but other than
that i would say
no it's a good idea to use it all the
time
is there an advantage of starting
with a higher number of neurons and
using dropout
versus a lower number of neurons
that's a good question i would say the
advantage
is gonna be the second point here
your question is yes sort of addressing
problem one maybe a smaller model
is gonna not overfit
with a lower number of neurons probably
it's gonna be less prone to overfitting
but then you are not combining many
different models if you use a single
model
with a smaller number of neurons the
question is
how are you approximately combining many
different models
one way of combining it was
during testing perish data point
you look at a random
model take a look at the predictions
then get another data set get another
data point push it through another
random model that's going to give you
another
prediction and keep doing that for i
don't know
maybe 10 times and then average the
output
and then your test classification error
on the average is going to be this
number
and this is basically a monte carlo
model averaging
rather than doing this
we are just monte carlo averaging our
outputs our predictions
and yes if you do it enough times
maybe 120 times
you might get a better model compared to
the average
weight scaling because this line is
great scaling
but as you can see this is within the
margin of error
so this is actually a very good way of
averaging neural networks
in the end you keep all of your weights
but then you
scale them down with your probability
any questions i just i had a quick
question
um so when we're referring to
when we're talking about dropout as a
method for
combining different models
we're saying that we have individual
network
models that internally
may also use dropout and then we're
using dropout
to combine these models into some like
ensemble supermodel is that correct
uh that's sort of correct yes
so you start with a huge model that's
capable of overfitting
but then you keep killing neurons
if you do that that's going to give you
multiple smaller networks
and during training these guys have to
do good jobs
this is smaller neural networks and they
are random
in the end you average them this way
does that answer your question yeah i
guess i guess i'm a little confused if
we're saying that each neuron
inside of each neuron we might have like
three convolutional layers
and like two dense layers no no no each
neuron
this whatever you do whether it's a
convolution or anything
okay okay each neuron is a linear
combination of
the input to the neura okay that makes
sense to me i guess i was just getting a
little confused with the word
model and no here it is a model it's
just a
this is just a neuron okay and
mathematically this is what it is
yeah yeah that is exactly so there is no
other way of interpreting it
that's the implementation this is not a
recurrent neural network this is not a
convolution this is
this is just linear combinations of your
inputs
that's the bias so there is a question
my understanding is that applying
dropout inherently
creates lots of models because they are
not the same
since each of them have randomly dropped
neurons
exactly so that's a correct
interpretation
so any other questions feedback
do you have to maintain independence
between neurons
or does drop out prevent co-adaptation
due to drop neurons
so dropout is actually gonna prevent
co-adaptation
so it's gonna maintain independence for
us and as you can see the title of the
paper is improving neural networks by
preventing
co-adaptation of feature detectors so
that's exactly what it's doing
does that answer your question perfect
any other questions
is there a way to reproduce neural
networks
trained using dropouts is there a way to
measure
co-adaptation quantitatively
i'm gonna answer john's question first
i'm not sure probably you can take a
look at one of the neurons
and see how it behaves during training
with dropout versus without dropout
and now let's go back to parker's
question
is there a way to reproduce neural
networks trained using dropout
i don't know what you mean by reproduce
okay i'll expand on that so let's say
i'm using dropout i'm randomly killing
off
neurons to create a model and i'm
happy with that model but now somebody
else wants to take the
data set that i started with and
reproduce
that model so is there a way since the
dropout is a random process is there a
way
to reproduce that process
using like a random generator seed now i
see what you're saying
so there are two things that are
happening here
one is during training things are random
you are correct but during testing
it's when you want to give your model to
somebody else
things are deterministic so you keep all
of the
parameters but then you weight them
by the probability
okay so the model that you share with
others and you put in production is this
after the training is done this is what
you share with others
okay so i would share my weights and the
the probability that i chose
exactly okay thank you
so no during testing things are not
random you're deterministic
for how many iterations do you train a
certain dropout network
before moving to another
every single data point that goes in
your model
is gonna see a random net
so every time you push a
data point inside your neural network
a random number is gonna generate
and then a random set of your neurons
are going to be
dropped out of your neural network
so what i want you to take home
from this slide are the two points that
i
started with many people think of
dropout as a regularization technique
and they don't know about the second
point that you are actually
approximately combining exponentially
many different neural networks together
and the key point is here
and many of you are asking questions
whether this is actually deterministic
or no
is it a stochastic neural net no
in the end what you're gonna get
is an averaged out neural network
so that one is gonna be deterministic so
if there are no other questions
i will move on to the next slide