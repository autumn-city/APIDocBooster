if you are here to put big models on
small phones you are in the right place
uh if you want otherwise i'm sorry we've
locked the doors you have no choice now
this is client-side deep learning
optimization with pytorch i am shane
caldwell this is tyler kirby
let's get started
so who are we why should you listen to
us talk about putting models on phones
for the last couple of years tyler and i
have been working at a local st louis
startup handled
therein we made a bunch of machine
learning solutions for the moving
industry
we were recently acquired by unigroup
who you might know as the largest
interstate mover in the united states
they're behind mayflower and united if
you see those vans
um so since then we've been in charge of
technical direction and execution of the
data science machine learning team at
uni group
one of our bigger projects over the last
year has been taking those sort of deep
learning capabilities we built wallet
handled and then moving them on to the
mobile device unit group has a lot more
customers than handel did it would be
very expensive to keep those models
running up for server purposes
they just service cost money
and secondarily there are a lot of use
cases in moving wherein you're going to
not want to rely on an internet
connection to do things because you know
maybe the router got put away
yeah so and to give a little more
background of why it matters that a
moving company can put models on the
phone
this is something we tried back in 2017
we had a application that
truck drivers used and they often don't
have internet connection right we had a
text classification model that was of
interest
uh so we tried to put that on the phone
that was a disaster uh it was in
javascript the android got really hot i
got really concerned
so we walked away from that came back a
year later
this time working with object detection
trying to put that on the phone
tensorflow lite came out we were very
excited we thought we'd give it a try
wasn't everything we were hoping for
accuracy-wise so that was a little
disappointing
and then last year we really got
involved in real-time video streaming
for various applications this is very
compute intensive we got deep into pi
torch figuring out how to execute
multiple neural networks within
milliseconds
again as jane mentioned that doesn't
scale very well on the server side when
you have lots of quad clients so
we backed away one more time and this
past year really focused on putting
these deep learning models on the mobile
side to improve the user experience to
improve the cost of the application
and this is sort of a narrative of what
we found to be most useful on that yeah
2021 is a great time for this as it
turns out the rest of the world is
questionable but the client side deep
learning technology is great
so the first question is why pie torch
you know why not tensorflow
when you're running a sort of uh you
know a practice a data science practice
at a company in general i'd recommend
just picking one so you could go with
tensorflow or something like that but we
stuck with pi torch
pytorch has an eager execution model so
you could say that semantically it looks
a lot like python
it feels like python and for software
developers you might be hiring who have
never written deep learning code before
but have definitely written python
before it helps them be productive
pretty rapidly both in the actual
development of the model but also like
when you're using it downstream they can
be pretty helpful
tyler and i have something of a soda
fetish he doesn't like when i say that
but when we have a problem that's
presented to us we like to look for you
know what the state of the art solutions
are so we'll go on to archive and google
like hey how do you do this but make it
harder and
we can't like re-implement all these
papers because there's not enough time
for that
so in general we'll only go with
solutions that have code published and
we found anecdotally if there is code
published with the paper that code is
probably written in pi torch and though
that anecdotal evidence that feels good
is backed up by data from papers with
code that makes my brain believe it's
true
so we stick with pi torch for that
reason
so this was us about a year ago
server side it was it was expensive it
was kind of slow for the applications we
were working on where fps really counts
let's put it on the phone tyler is
endlessly confident he says how hard
could it be it could take one week could
take two weeks we told that guy it would
take that long we were wrong
this is more what it looked like
so we spent a long time expecting that
it was going to be easy and then more
time looking at stack traces
um i initially had a sea turtle here and
the people i had sort of previous talk
told me that sea turtles are actually
pretty fast so i'd replace it with the
tortoise anyway the point of this talk
is to get you from sad baby to happy
baby in terms of putting uh models on
phones
so if you or someone you love has
written a video you'll know that this is
basically what it looks like this is an
eager module the sort of most vanilla
pie torch
it's really easy to work with you just
inherit from this nn.module class
you will have a forward function that
takes some input and applies all these
nice tensor operations to it you'll
return it
that's convenient because python is not
going to do any of that math right in
general if math is being done we're
going to pass that off to someone else
usually c plus plus
but what pytorch makes really convenient
is passing up those kernel level you
know code executions back up to python's
process so after each of these things is
done you can sort of step through it in
your you know standard debuggers like
pdb and that makes you know unit testing
and debugging super easy so another
reason we love eco execution
but if you want these things to run
really fast then you sort of have to
look under the hood so pytorch is great
for people who are getting started to
focus on data sciencey things so like
how am i loading my data set how am i
going to handle my learning rate
scheduling what's my hyper parameter
search look like all that stuff that
they the kind of the level of
abstraction they prefer to think on so
for prototyping that's great but to go
to production you have to think a little
bit more about the context in which this
code is getting executed and sort of the
theoretical underpinnings of like the
the actual math getting done uh and that
sort of this talk is about getting you
from that research and production
context that data scientists usually
handle to the production deployment one
that's more ml engineering e
okay
so as a data scientist you have choice
unlimited in many different things how
you optimize how you choose your
algorithms how you go about
your work in general and it's useful to
have sort of guiding abstractions to
more quickly lead you to the right
method so we've used this axis of
optimization abstraction to sort of
think about uh what direction we need to
take our optimization project next right
so beginning with inference speed this
is typically the most important if
you're putting on a phone you're going
from a high compute environment to a low
compute environment
the model is usually going to be
slower on the gate
you're expecting that and that's what
you're trying to solve here
accuracy obviously if you optimize a
model and it does worse and you don't
solve your problem you have new problems
and
that's not great
so we need to be keeping track of
are we still able to solve the task with
the sufficient threshold of accuracy
that's required
while
ruthlessly optimizing for performance
and then finally binary size this may
seem a little
out of the blue here but it's a really
good metric to be tracking as they're
optimizing your model and we'll talk
about these interactions for why that's
true
so let's talk about inference time a
little more
this is your primary metric to
understand
the efficacy of your optimization
methods
it has a
adversarial relationship at times with
accuracy as you're able to improve your
inference time it might be at the cost
of accuracy
so be mindful of that
there are some things out of the box
that are going to make your model faster
and not worse and we'll talk about those
those are pretty few
you are going to have to make some
trade-offs ultimately
and how you measure accuracy is also
super important right so
you need to be aware of the available
threads on the computer that you're
benchmarking your models you need to be
aware of background processes
all these things that could affect the
validity of your measures of computation
and
because of that we typically throughout
the first few rounds burn in rounds
in general to sort of
ignore those effects
right and then a little more about
accuracy
this is pretty straightforward but we do
want to call out that it's important to
have multiple metrics for accuracy so
one thing we found when working with an
object detector for example if you only
use some aggregate method like the mean
average precision and then you do
various optimization methods that may
affect the
performance of your regression action uh
your regression accuracy unevenly
compared to your classification accuracy
you're not going to catch that so
readily right so you need as many
accuracy and evaluation metrics as
possible to understand how
optimization methods can unevenly affect
the performance of your model
and then finally binary size right
big models are typically more accurate
and slower small models are faster but
less accurate this doesn't quite work
out
that neatly but in general that's a good
heuristic for thinking about it
it's also useful to think about how many
parameters you need to solve your
problem typically when we're talking
about how big a problem is we're talking
about or how big a model is we're
talking about how many weights do you
need to represent that model
if you're able to solve your problem
with fewer weights do that right i know
a lot of modern deep learning encourages
extreme over parameterization but you
don't really have that luxury on the
client side so you need to be using
architectures that make the most
effective use of the parameters
available to you
so uh eager pi torch that's great
but it won't work for production so
eager modules have a couple of problems
one being they're bound to a python
runtime environment so python while we
love it it's not available everywhere
and it's not exactly desirable
everywhere it is available so we can
think of mobile devices or cars or
toasters i guess
the python runtime environment is
crucially bounded by that global
interpreter lock which makes a lot of
things easier but means you will not
have true multi-threaded inference so to
get the same throughput you would get
without the gill you're gonna have to
scale up more sort of server wise and
that again gets expensive so these eager
modules are bound in portability where
does python run and then in performance
because of the global interpreter lock
so we want to escape these sort of
restrictions in order to keep using our
pi torch models
particularly in context like mobile
devices
we are not the only ones with those
problems pytorch considered it i'm sure
facebook was very interested
so your pi torch model if it subclasses
from nn.module is an eager model module
as we've discussed
and pytorch allows you to take that
eager module and create an intermediate
representation of it and that's called
torch script that runs in a separate
interpreter that runs a subset of python
operations but crucially with no global
interpreter lock so you will be free
this gives us access to what is called
graph execution mode and the interpreter
is just in time meaning that
optimizations are going to be performed
over the runtime of the program and
we'll get into what that means a little
bit more
so the just-in-time compiler allows you
to serialize your eager execution model
modules in pi torch into torque script
but that process is not automated so you
as the practitioner have to make a
couple decisions in terms of how you're
going to do that
there are multiple techniques and we'll
cover them quickly here there is tracing
scripting and a combination of the two
so first is tracing you're going to
declare an input to your neural net you
will push it on through there's this
data structure called gradient tape
that'll look at all the tensor
operations that you're applying
so you'll think of like the eager module
i showed you earlier
that would be them
so it'll just take a look at that
gradient tape at the end and say okay
that's the tour script that's what it is
so if there's no control flow uh there's
no if statements then this is sort of
the right call and if you're not using a
lot of python data structures you can
imagine because we are doing again one
input to tracing if there's any if then
that would cause it to go down two
separate routes well you're only going
to trace the one so then you're going to
start you know your your intermediate
representation won't represent eager
module
scripting is different it handles that
control flow and it handles a whole
bunch of python operators and i will say
over the course of my time using torch
script
it continues to sort of include more
which is great
that means it's like mostly python
though so it's not that much faster than
just python except for you get rid of
that global interpreter lock so it's
still worth the price of admission and
it doesn't require sample argument
because it's going to use recursive
static analysis in order to get this
done
so if your model is not just a function
this is true of like many sort of modern
techniques that are going to use more
python data structures this is where you
should go and there is nothing legally
stopping you from combining the two so
if you really want to get more
performance out of your model
these neural networks people talk about
them like legos and that's like not
entirely incorrect these are pretty
composable functions so if you break off
the parts that require scripting you
script those take the rest that don't
need it trace those
you can kind of stick them back together
i will say the process in reality is a
little more complicated than that so if
you can avoid it just stick with
scripting but if you need that
performance you can you can get it
so intermediate representations they
come from compiler theory uh so
obviously we want them to fully
represent the sort of program uh that
was written in python if our inputs and
outputs are not the same then we're not
interested right
the format is generally going to be
simple so if you have a big compound
statement it's going to break it up into
simple ones that are easy to
automatically optimize
so this is sort of a just a generic ir
example
in pytorch you can look directly at your
intermediate representations this is
useful because pi torch is going to have
a bunch of different optimizations that
it can do but if you want to know what
is actually happening you're going to
want to interrogate your serialized
script at multiple points so you can see
like hey this is what actually changed
if you call doc code on a serialized
python or pytorch model you'll get this
sort of python looking syntax which is
nice but a little detail light
on the other hand if you want to get
really into it you can call that graph
and that's going to give you a lower
level representation and this is the
level on which optimization is actually
going to be performed
so if you want to see what your
optimizations are actually doing then
you know call.graph
do some optimizations call it again do
that diff with your eyes and then you
understand okay this is what's actually
changed on the torch script level
you can also write torchscript raw if
you want
there's nothing stopping you i don't
recommend it keep in mind that
eventually an interpreter is going to be
optimizing this code as it runs and it's
a way better at
you know optimizing the stuff that pi
torch generates itself than the stuff
you would write by hand
so now you have the serialized uh module
you're away from the gill but you're
still in python that makes you mad so
we're gonna export a dot pt file and
that's just going to be a zip that
essentially gives you everything you
need to get out of the python runtime so
everything the code the weights the
constants
debugging information
that's going to be all available in this
what is literally a zip file this is
what it looks like if you unzip it
there's nothing you can do with that but
it's kind of interesting
so now you have everything
you need to run the model without python
so now you want to run your torch script
it's going to run in c plus plus so
the dirty secret or the not so dirty
secret was that python was already
having all of its tensor operations and
its auto grad engine that was all
happening in lib torch anyway which is c
plus
so
now that you're away from python you
have this pt file anything that can run
c plus plus which last time i checked is
basically everything uh can now run this
sort of deep learning model
so all that stuff like you know torch js
go torch has torch root torch that i'm
sure exists like all that stuff is
actually just languages that have
written these bindings to talk to lib
torch
a couple of speed tips
do not forget these if you don't call
model.eval so that is putting your model
in evaluation mode so you're saying hey
we're not training
if you don't do that they're going to
start tracking like gradients and doing
all those things that you need when
you're training a deep neural network if
you do all these optimizations but you
leave your model in training mode at
runtime you're just leaving speed all
over the floor and i did this for like
months because i didn't even think i was
like why would you train in c plus plus
you know
you also want to warm up your model
right so these optimizations are
performed just in time
so those first couple of runs through
the model you say you're sort of
teaching the interpreter like hey this
is what my input's gonna look like i
don't need um you know my my gradient or
what have you and it's gonna learn okay
i can throw some of this stuff away i
can optimize that torch script we saw
earlier
so that compute graph is going to look
real nice
also
cache that
keep that in cache once you've got it
all nice and warmed up and loaded
because you just did a whole bunch of
work and you don't want that to get
garbage collected because then you got
to do it again and you're going to get
slow for no reason and if you're me
that'll take you a month to figure out
then if you want to get really nitty
gritty right you're really caring about
size your footprint
if you're shipping an app to the app
store it's going to ship with like lib
torch included and that can get sort of
hefty
but torches thought of that as well
because they're very nice to us you can
export the literal operator names of
what is going to need to get executed
for your model specifically into a yaml
file
then you can use a static linker to
prune literally everything besides the
operators you need to execute your
specific model so that means when you
ship your app it's going to you know
take up less space it's going to give
you a smaller footprint and slightly
faster execution for your treble
this is sort of a weird inside baseball
thing hopefully you guys remember this
later
keep in mind that while you're running
this stuff in c plus the actual
interpreter is not in c plus right so
you when you're loading your model and
running inputs through it you're not
going to get compiler time warnings all
the compiler is going to look for is
that you're passing it a vector of
interpreter values
so it
basically regardless of what the
actual model is expecting at runtime it
needs to be a vector of these
interpreter values
here you can see that in this sort of in
this rcnn example that is expecting a
tensor list this is a little sort of
novel and in terms of you know inputs
models expect i have to sort of jump
through all these hoops to make sure
that i like create a tensor and then i
put it in a vector and then i put that
in a tensor list and then i put all that
in in a vector of i values which
actually expected at that point xcode
stopped yelling at me and let me like
run this
so you don't need to know the specifics
just know you're gonna you're gonna need
to look at the actual runtime errors to
determine what you should be doing to
your input to let it run through the
model
and there's a light interpreter so i
hate to break this to you but if you're
watching this talk in the future
some of the stuff is going to have
changed so pytorch has recently released
in beta a light interpreter for mobile
so
we didn't use it in our experiments
because we don't put things in beta in
production come on when we can help it
uh but going forward it's likely to be
the preferred method so you're probably
going to get a lot of like you know the
binaries are going to get smaller and
the interpreter is going to get faster
and that's all going to be free so if
you can wait maybe just wait
you'll be able to recognize those
artifacts because they'll be called ptls
because they're light instead of pts
our experiment showed that they were
essentially like equivalent to the jit
slightly slower in some cases but the
binaries were a lot smaller so
if you're watching this on youtube
you go you use the light interpreter
okay
so let's talk about quantization so now
we know that we can take our pi torch
model and serialize it for a run time on
the client side how do we go about
actually making the model faster
as accurate as possible etc quantization
is the primary method in machine
learning nowadays we're doing this and
the idea really comes from digital
signal processing where we have some
continuous signal and we want to
approximate it with some discrete lower
memory representation and that's exactly
what we're going to do
in
this context of machine learning
so pytorch very nicely has
supported the most common methods of
quantization since about march of 2020
they have two primary methods of
quantizing models
they're going to work on both the eager
mode representations that we discussed
earlier as well as the traced methods
working
on the intermediate representations in
fx graph mode fx graph mode is pretty
new we didn't try it a whole lot because
it only came out in the last month or so
so we primarily worked with the eager
mode
so in the future when you're trying out
tensorflow lite interpreter i'm sorry pi
torchlight interpreter
you might as well try out this fx graph
mode as well
so let's get into what these methods and
algorithms actually are
dynamic quantization static quantization
and quantized so we're training under
three big ones we're going to discuss
today
they are sort of
mutually exclusive in that you wouldn't
do both dynamic and static quantization
they have different required effort from
the developer we're going to see dynamic
quantization is literally one line of
code whereas quantized aware training
can take about a week
to retrofit your code depending on what
architecture you're using
and
the choice of architecture is going to
be very important here as well because
of how these quantizations are performed
language models may prefer some methods
and vision models may prefer other
methods
all right so let's start with dynamic
quantization
there's really two things happening here
that are important the first is that our
weights are represented as
32-bit floating points
and that takes a lot of memory so the
first thing we're going to do is reduce
the precision of these weights to 8-bit
integers the way that we do this is
pretty simplistic it's pretty much
mid-max scaling we can do this because
our weights are finite static sets right
they're not going to change at time of
inference we've already done the
training we know the minimum maximum of
our weights
we can do this scaling
the other thing that we need to account
for though are activation functions when
you take the output of
quantized weights and inputs
to a non-linear activation function your
output may not be sufficiently quantized
right so we need to learn a scale factor
for the output of these activation
functions this is where dynamic comes
into play because
in this case that scale factor is going
to be determined at runtime as it sees
new data
because at time of inference that set is
not finite or static you can have data
across the whole
spectrum in theory
you need to be updating that scale
factor based on the data you see so
static quantization is really most
useful when the burden of compute is
coming from loading lots of weights and
batches off of disk loading that into
ram right so this is typically going to
be seen in language models transformers
lstms rnns
they benefit greatly from this method
they have a lot of weights typically
you're working with batches in language
context even at time of inference
and luckily it's the easiest method to
apply literally these four lines of code
up here will take your language model
your lstm whatever you want in this case
we looked at a mobile net
and
sufficiently quantized it
all right so just to illustrate what we
mean by min max scaling the weights it's
kind of a quick example we have some
random gaussian distributed matrix here
pretend these are your weights they are
32-bit
and floating point representation that's
taking up a ton of memory
we know the men and we know the max and
we know those aren't going to change
so
you know very easily in a line of numpy
we're able to
reduce the precision of this to 8-bit
integers
all right so next we have static
quantization
we're still going to quantize the
weights like we did in dynamic
quantization but now we're going to take
a closer look at how we quantize
the outputs of the other activation
functions so rather than waiting to time
of inference to have some running
determination of the scale factor based
on the data that we see we take the data
set that we already have available to us
and use that to calibrate the scale
factors of these activation functions
this can also be done on a per channel
basis which is really helpful in a
vision context right your vision models
are going to have
many different channels your inputs will
have many different channels
and if you learn a scale factor per
channel you're going to get a better
representation
of those weights in the 8-bit integers
right
so
this is going to benefit vision models
more than
language models we found
it does require a little more work this
isn't a one-liner that's why we didn't
show it it's a lot of lines of code
depending on what you're doing and
that's because
you need to stub out functions
in your custom pipe torch code if they
don't have some
quantized corollary
so
if you're working with a very custom
architecture that's very large this
could take a long amount of time if
you're designing a network from the
beginning knowing you're going to
quantize it that you can have
that selection of quantized operators to
make sure that you're not going to have
to retrofit this later what we would
actually recommend is if you know you're
going to need quantized
static quantization here and you are
doing a vision model pick one of
pytorch's pre-quantized architectures
like mobilenet v2 or mobile.v3 these are
going to be great options to start with
to fine tune
they outperform some of the more exotic
models that we've tried in the past
and it's going to save you a lot of pain
in the quantization process
so quantized aware training takes this
idea of using the data set to learn
these scale factors for our activation
functions one step further and ask you
know can we
do a couple forward and backward passes
in our network in a fake quantized
environment we force rounded integers or
something to cohorse the model
into
behaving better under quantized
conditions right can we fake quantize
put these constraints on the model and
see if it
can perform better in low compute
environments it's kind of similar to
regularization right we're putting
constraints on the model to make it
learn better in this case can i learn to
work with is 8-bit integers
this method is really only necessary for
when
you tried static quantization at the per
channel level you didn't get the results
you're hoping for
accuracy dropped dramatically
in the
pytorch
tutorials you'll see that they use a
mobile v2 to kind of illustrate these
ideas
a mobinevv v2 on imagenet classification
typically scores 72 percent after static
quantization that dropped to 57
pretty bad after doing the per channel
quantization that jumped up to 67
better and then quantized where training
brought that to 71 or so so almost back
where you started
well worth it if you're not getting the
accuracy you're looking for
all right so we did some
experiments uh with a couple different
architectures here
really to just show the benefits on real
models so we took an lstm with a pretty
large embedding layer a lstm cell with a
ton of hidden layers and then a final
linear layer for some classification
text the base model was 169 megabytes
and its inference speed on a batch size
of 32
was about five seconds
so after dynamic quantization we reduced
that to 134 megabytes and the inference
speed dropped to 1.6 seconds and then
looking in the vision context we took a
mobile net already a great architecture
for a compute constraint environment
like a phone
its baseline is 22 megabytes in size
0.13 seconds of inference in
classification and then it has a
imagenet accuracy of 74 percent
uh and then we perform static and
quantize where training on this got this
down to
5.6 megabytes
0.03 seconds of inference and finally 73
accuracy so losing one degree of
accuracy but dramatically
uh increasing performance of the model
here's a brief flow chart if you have to
make a decision and you really don't
want to to kind of guide you to the
right quantization method
if you are working with a neural network
that you want to quantize ask yourself
where is the burden of compute coming
from if it's coming from loading weights
off a disk and passing big batches like
a language model do dynamic quantization
you'll probably be fine that's really
all you need to try in a vision context
you'll probably want to start with
static quantization
if the accuracy was reduced too much to
solve your problem now i'd suggest
trying quantize aware training
all right and then very quickly we have
additional optimizations these are kind
of freebies in here
so the first of which is module fusion
the idea here is pretty basic there are
some operations you do all the time in
the neural network like batch norm and
then a rail you activation function
you can fuse these into a single
instruction so that when it's executed
in torch script it's one instruction
instead of two
the optimize for mobile method in
pytorch will do this for you
if you have large
convolutional block networks like a
resnet or something this is going to
give you
a little more performance
channel last format is a really cool
idea uh it's really useful in this
context where you're working with images
in batches at time of inference
but typically a image and pie torch is
going to be a tensor with four axes
batch height with channel right
imagine you had an image with a large
black patch
the pixel intensities in that area for
each channel is going to be the same
right so if you have the channel in this
second position
that's not the most efficient
representation to hold this tensor in
memory if you move the channel to the
last position however
areas of the image where the pixel
intensities are the same can be stored
more effectively
so now pi torches mobile apis and ios
and android support this channel last
format when you're working with image
tensors
okay and a quick addendum
some things that we've mentioned
in the abstract but didn't quite cover
yet
because they're not problems anymore
like operators so when we first work on
this talk in may or so we're working
with the network it used non-maximal
suppression like most object detectors
do this was not supported in pytorch's
mobile apis it was incredibly painful to
load up
that
operator intentionally in the mobile
application he wrote his own
implementation in java yeah it was a
disaster it was really bad very painful
not a problem anymore though so torch
vision on mobile supports these
operators natively and in general we'd
recommend that
don't get too novel when you're going to
the client side right architectures
matter mobile nets are really good
don't be using the activation function
of the month and then writing a custom
operator for it and trying to load it
into pipe torch
stick with what they support they
support a lot more now
and that will generally make your
transition to the client side less
painful
in conclusion
so we uh we threw a lot at you uh you're
all very brave
but some takeaways so in your home
thinking about this stuff
make sure you're using torchcrypt for
your modules right so if you're not if
you're running eager execution in prod
go home serialize that make that torch
script there you go your life just got a
lot easier i just saved you a bunch of
money on server costs you can demo me
when in doubt use
torch.jit.script so if you're using a
model that you're not sure sort of how
to serialize it script is basically
going to do the job particularly if
you're taking a model off a model hub
somewhere script is the way you want to
go as we've said architecture is going
to be greater than optimizations in this
case so there are some models that just
aren't going on the phone gpt it's not
going there
in those cases you're better off looking
around for sort of model architectures
that were designed for low compute
environments like mobilenet it's got
mobile in the name
that's going to do a lot more for you
than these optimizations assuming you've
picked a good architecture for your use
case and you're still needing that
performance quantization is like if
we're talking 80 20 rule quantization is
that 80. start there if you do that
right you're going to get a lot faster
for
relatively low work
and i just want to say like words saying
all the stuff like it's like oh super
novel but pytorch's documentation is
fantastic this stuff is all there make
sure you understand what it's saying and
you implement all the tips they
recommend
that's going to save you a lot of time
and heartbreak and get you to have your
baby faster all that stuff's going to be
better than these sort of implemented
custom operators because the stuff
someone else wrote is going to be faster
than the stuff you wrote
so that's it thank you if you have
questions you can email me i'm on
twitter too much you can find me there
you can also tweet at him but he's on
there very rarely you can email him
though he will see it thank you all for
coming out you've been a wonderful
audience have a good one
[Applause]
[Music]
[Applause]
you