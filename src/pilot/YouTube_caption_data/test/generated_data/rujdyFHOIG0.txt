hello community today we're gonna do the
parametric you map the fast and furious
run so here we go runtime and run
everything beautiful so we start with
tensorflow we are operating on a colab
notebook so of course we have tensorflow
installed i hope to get a gpu yes back
and gpu initializing yes connected we
have a gpu that's great so we have
tensorflow probability
version.16
we have in the 22 uh upgrade our python
library gym then we do all the numpy
scipy our where's the panda scikit-learn
data shader whole of user number because
it's now running especially focused on
number
and then if we have installed everything
we have here the final command pip
installed youmap learn
and i'll include also the plot
functionality so we have to do nothing
about whatever the plot
is there
so
this is successfully installed it yes
you map learn
0.5.3 great so we have our standard
tensorflow keras dataset mnist you
remember those are
702820 times twenty eight black and
twenty our
white images
so our dimension is seven eight four
and then we simply have to train images
we have sixty thousand train images
twenty eight times 28 times 1 is the
dimensionality and then for the test
images we have 10 000 test images and
here we go we just import parametric
umap with one line of code
import parametric umap
and then
if you have seen my theory video on a
waiter wait a second yeah of course you
cannot go on with this
just have to start
your network yes here we go
this is what i want to show you yes come
on
no problem at all
um the parameters to parametric umab you
have a lot of parameters we are going
for the default version but i just want
to show you what default means so that
you know where to start to tune
at first the optimizer
typical keras optimizer would be an atom
optimizer choose whatever e2 you're
liking we have a batch size we have to
define depending on your ram you have
the dimension of course we have 28 times
28 times uh one
if you go for other images 32 32 2 and 3
color
channels then we have an encoder
remember we're working in tensorflow
with keras we have a sequential model
and we have a decoder the encoder goes
from the high dimensional data space to
the embedding space and the decoder goes
the other way around it goes to does a
reconstruct
and it goes from the low dimensional
embedding space back to the high
dimensional data space to our input data
space
then we have some parametric embedding i
hope this is set to true but the
parametric reconstruction so this means
the inverse functionality is set to
false for the moment because we go for
the minimal version
so the reconstruction loss
is of course boolean it is a binary
cross entropy and the loss weight is set
normally to one
now this
this would be the turbo version of our
little car for a little volkswagen
uh the auto encoder if you combine
umap with uh auto encoder functionality
we remember that and the autoencoder
itself is a dimensionality restriction
we would get even better performance but
for the moment we said set it to no
false reconstruction validation yes we
will do this in a later video the last
report frequencies we say one time per
epoch
the three epochs we set the global
correlation loss weight and eagerly yes
there was a tensorflow one but we forgot
about this
and just that you know here our encoder
and decoder network they are default
and if
in parametric um you use the default
what we're gonna do in a second we have
the default is a three layer 100 neuron
fully connected neural network and here
you have it this is our default encoder
you're not going to see because that is
done behind the scenes for you
you have our three layer
fully connected 100 neuron
neural network now for the decoder that
goes further embedding space back to the
data space we have now if a parametric e
construction is activated
we have a d code that is also a
tensorflow 2 cara sequential model
same structure same activation function
reload you know everything standard
class
now we say
this is the line more or less where we
say our parametric umap and all the
parameters and from all the parameters i
just showed you here all of this
we just say let's go with default we
just have to say the number of epochs is
50
and we want to see
the progress in the calculation and you
see after three minutes it's done um
bada did our fit transform on the train
images
uh it started the first step of course
was construct from uh data cloud in high
dimensional vector space to construct
the fuzzy simplicial set you know we
love topology here so go and read what
is a simplistic saturn about the
fuzzyness and how you can compute yeah
yeah this is look at my theory video and
then we is gonna build our
where we are where we are the nearest
neighbors network
we have 17 trees we have an iteration
and then we're gonna construct the
embedding on your network and it is
already done after three minutes oh yeah
i have to do that
and let's have a look at the loss
function
and here we look at the cross entropy
over the number of epochs we had 10
so you can imagine if we increase
another number of epoch to 200 let's
start it right now
let's go back so you see that beautiful
that cross entropy against epoch is
going down beautifully exactly as we
expected so this means
for the simple uh visualization
that our amnest data set from the
digits from zero to nine where we have
sixty thousand images in black and white
twenty eight times twenty eight pixels
um in our tensorflow and biting is
beautifully able to separate those 10
classes those then digits
and this is more or less mission
achieved but if you want to see
because this is of course you remember
interior i showed you
uh the step two after you construct a
fuzzy simplicial set
then you have to do in the
parametric umap you have now your neural
network functionality and then of course
you have an initial start
um
set in the embedding space but the more
you train it the more you optimize your
cross entropy your loss function the
better the result will be if you compute
for the minimal difference between the
probability distribution in the original
vector space and the probability
distribution in the embedding space both
topological spaces you know bernoulli
equation cross entropy i explained
everything in my theory video
so now if we do run this
oh yeah this takes definitely longer the
first did just take 14 seconds
and now we are running here close to a
minute 59 seconds okay
so this would be 10 minutes if we do now
a real
if you try to improve our performance
not by
leaving our default configuration our
default parameters but just by
increasing the number of epochs we try
to increase our
accuracy if you want of the model and
then what we do more or less
we do the same we want to see
if we plot now this is exactly where we
calculate now our embeddings and we want
to see our
uh classes our 10 classes if we plot the
embedding if they are clearly separable
in a two-dimensional visualization
so we go from where is it where is it
raised
we have here a 784 if you want uh
dimensionality on this axis and we go
down to two and this is exactly where
parametric q map is gonna help us
and as you can see
although
with number of epochs was 50 we have
some beautiful results already but we
have now to run for the number of equals
200 takes a little bit of time
what can we do anything else yes we have
three lines of code we can still do so
saving and loading yeah of course
if you want to save this it's not so
easy as review map because remember
you cannot save it simply by by picking
a umap object because you have a keras
network living inside
of this little buddy so therefore there
is to save now this parametric umap
model that you trained on
and you want to reuse the model later
maybe retrain it again
there's a built-in function
that tells you to do exactly this so you
have here from our parametric umab you
have import load parametric you map
unbelievable
so our embeddings are embedded you just
save and you give it a path and then you
have this beautiful load parametric umab
command
and a path
and the embedder will be reloaded your
embeddings will be reloaded
and everything will be available for you
to go on
and do whatever you want because
remember one of the beautiful things of
parametric u-map compared to the simple
well simple under quotation mark umap
was that with this
neural network functionality if you have
new data points it is easy to find and
very fast to find embeddings for it
so we are here
a number of epoch is four of ten i would
say i'll be back with you in a second
and for me it's gonna be about six
minutes time
and what felt about hundred thousand
years so we finally finished our collab
gpu finished doing the job we have now
the embedding calculated and if we um do
now plot our embeddings you can see
three
three three six one two three four we
have our 10 classes again
i don't know if the
increase in accuracy is so much better
already the old one looked perfect to me
three three and four so whatever we have
the number of epochs increased
and we receive a beautiful uh result as
i showed you showed you
we have here this load command so oh gee
we got a warning
compiled the loaded model has yet to be
built will be empty modeled to be safe
too temporary model encoder you have the
parametric model warning uh yes to no
training configuration was in the save
files it was not compiled manually yes
beautifully
and then we just load it up and it says
yes pickle loader yes tensorflow no
warning was found in the film was not
compiled compiled manually yes loaded
loaded so i showed you that we can
define our encoder and our decoder
network neural network with tensorflow
kara sequential and just as a sneak
preview to my next video since we are
doing here a vision job we have of
course a convolutional network and this
will be the layout of the type
that we use as an encode and a decoder
in our next example where i show you how
to tune parametric umap to have a
performance like a ferrari this was it
for today i say thank you and i'll see
you in the next video