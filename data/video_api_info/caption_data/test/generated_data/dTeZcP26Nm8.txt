right
hello everyone um welcome to today's
recitation which is going to be on
attention um can everyone see my slides
looks good yeah all right awesome
so today's slides are courtesy of last
semesters
the years jingle and parathu put
together a really good slide deck which
i'll be using again today so
let's get started with attention
so so far we've looked at how rnns
work and we've used rnns for various
tasks
and you'll also be using them in
homework 3p2 for the problem of
converting
a sequence of speech frames to the
corresponding
phoneme sequence and you might remember
from
lecture that this problem has
order correspondence but not time
correspondence
some other tasks that rnns can be used
for include
translating text from one language to
another for example on this side we have
the sentence i like cats more than dogs
in english
being translated to japanese
some other things that can uh be done
using rnns
are things such as given an image come
up with a caption for that image
or given a pair of question and document
select the correct span of the answer
from
that given document and these tasks
are generative in nature meaning that
the output itself is a sequence which is
generated from the input
and the output is dependent on the
entire input sequence
so you have to have seen the entire
input sequence to
be able to generate some output
and more concretely this is called
conditional generation
where each term in the output sequence
is
conditioned on the entire input sequence
shown in green here x1 to xt where you
have dt timestamps in the input
and all the timestamps of the output
until
that output timestamp
for deep learning applications
specifically
we come up with an encoder decoder sort
of architecture for this
where each of the encoders and decoders
are realized as deep neural networks
such as rnns
and though the encoder and decoder have
their own separate parameters they are
trained
end-to-end jointly so the decoder in
order to be able to produce
this sequence of output tokens y
needs to have some kind of encoding of
the entire input sequence
and this encoding is assumed to capture
the entire meaning of the input sequence
and should have access to the past
states of
encode information about not only that
particular time stamp but of all
previous timestamps as well
so we assume that the last hidden state
from the last input token will have
effectively encoded the information
from the entire input sequence so far
and now that we have this representation
of the entire input sequence how do we
tell the decoder how to use it
effectively we have two options
we can either pass in this hidden state
from the encoder to the first
hidden state of the decoder as the first
hidden state of the decoder
or we can repeatedly pass this encoding
to the output network
at every time step but as you can
imagine there are some challenges
while doing this first it's hard to
train
a single input encoding vector
a single encoding vector having all the
information from the input
is difficult to achieve and earlier
inputs might
get forgotten in this process it's also
hard for the decoder to focus
if you're giving it the same thing
repeatedly at every time step
and so there is a need to come up with a
more flexible input encoding
which is determined by that particular
step of the decoder for example
if we look at a translation example from
english to say german if
the input sentence is
i eat an apple for example and you're
translating that to german
it's unlikely that you're going to look
at all the words
at every time step for example if i want
to translate only i
into german then it's not really useful
to pay attention
to the other words in the input such as
eat and apple
and so we need a more flexible encoding
type which
can dynamically look at different parts
of the input
so that's basically the intuition
of attention that at each time step the
decoder focuses
on a specific segment of the input to
produce the current output
and more specifically we let
the network learn how to focus on
the input segments and this is done
in the same way that we've been training
neural networks so far
through back propagation
so the concrete formulation is such that
this input encoding which you feed into
every time step of the decoder has to be
a function
of the decoder hidden state itself and
the encoder hidden states
at each time step from the input and not
only the last
input
so the general attention mechanism is
such
um you construct a query called qi from
the decoder hidden state
you construct a key and a value from
the encoder hidden states and you
calculate
some attention score so for now let's
just assume that attention
is some function um some similarity
function
that gives you a score between the query
q
and the key j and this is going to tell
us
how much attention or how much focus we
should give
to the jth input term at
um the if output stage
once we have um all the attention scores
for each of the input tokens
we can then construct a weighted sum of
the values of these
inputs by multiplying these attention
scores with
the corresponding values and
finally feeding in that new
representation
into the decoder state
so one of the variations of this general
mechanism is the dot product attention
where the attention score is calculated
as a soft max or the dot product between
the query queue and all the keys
okay
another variation is something called
bilinear attention
and this is when
your queries and your keys and values
are not in the same vector space so they
have say different vector dimensions
um then you can introduce another
parameter matrix
w between the queries and the queues to
make sure they're all
in the same space before you compute the
dot product
tension but i think it might be useful
to visualize these
these operations a little bit clearly so
i'm gonna switch to
my ipad now
all right can you guys see the ipad
screen
yep yep all right awesome so
um in keeping with the
same notation that the slides follow the
blue is
the encoder and the orange is the
decoder and your input sequence is i eat
an
apple and the corresponding german
output
and is is your typical encoder decoder
without
tension you're taking the last um
hidden state from the encoder which is
h3
and feeding it to start off the decoder
feeding it as the hidden state of
the first time step of the decoder
so how do you compute the keys and the
values from this
when we say construct keys and values
from encoder
hidden states we basically mean that for
each of those hidden states you project
them into
the keys and values using new parameter
matrices
called wk and wv
so now you've essentially ended up with
a set of key vectors and value
for each of those hidden states
similarly for the decoder
you take the hidden state of the decoder
and pass it through a different
parameter matrix
wq which gives you a query vector
and even though i've shown all four
timestamps here
typically you only do it one at a time
because h1 is going to be dependent on
h0 h2 on h0 and h1 and so on
so that's something to keep in mind but
just for the sake
of convenience i've shown all four
queries being generated currently
well what next now you compute the
attention score so you take the query q
and you multiply it with the entire key
matrix
and take a soft max after that which
gives you the attention scores
when we say scale the values by the
attention scores we mean
simply multiply those attention scores
with the corresponding
value vectors and sum them all together
to get this final representation
and this final representation is only
for one decoder state
since we used only um q0 at the top to
come up with the attention scores and
the representation
and finally you feed in that
representation in red
into your decoder state of the
corresponding timestamp and this is what
the complete picture looks like
almost complete um i didn't draw all the
queries and keys and vectors because it
would have become
a little more messy but this is what
your general attention pipeline is going
to look like
right
let's go back to the sides
all right i guess you guys can see my
slides again
so another version of attention is
additive attention where instead of
taking a dot product between the queries
and the keys you first project them into
a common space and then
sum over them take a tan edge multiply
that again with a different parameter
matrix and
take a soft marks over all the j input
tokens
this is a another variation of attention
called
scaled dot product retention and i think
this was introduced in the paper
called attention is all you need and the
difference here
is that similar to what we saw
on the ipad diagrams you generate the
queries keys and values
by passing the hidden states through
separate mlps
instead of having only a single weight
matrix you can have an entire mlp to
pass
through and another difference is
that the dot product in the soft max is
scaled by a factor of one over the
square root of the hidden dimension and
the authors justify the use of the
scaled dot product by saying that
this is in order to normalize the dot
product
so that when the hidden dimensions
get too large the soft max
isn't pushed into regions where
the gradients become too small to
propagate back
and they compare in the paper they
compare this with additive attention and
they find that
it performs um just as well if not
better
i believe
so this is great and this will already
uh help us generate outputs which are a
lot better than
uh simple encoder decoder architectures
but there's still a few problems with
simple rnn based attention
and that mainly stems from the
sequential natures of rnns
which makes them quite difficult and
almost impossible to fully parallelize
on gpus there's
still a step-by-step left-to-right
computation
where the output is always dependent on
the previous time steps
and so you cannot leverage the full
power of gpus
they also struggle with long-term
dependencies and even if you use
lstms you still cannot hold all the
information across very long input
sequences
also another use case is that in nlp
tasks the same word may have
different meanings based on different
contexts
so how do you deal with these problems
this is where transformers come into
play again in the paper attention is all
you need
the author has proposed a revolutionary
architecture called
transformer networks and in this paper
instead of using
rnns to do sequential computation
you do something called self attention
which is basically
computing the queries keys and the
values
from just the encoder
and doing that allows you to compute
attention scores in parallel across all
the queries
all the keys and all the values in the
input itself
and this allows the encoder and the
decoder to see the entire sequence
all at once and since there is no
uh particular left to the right
dependency
you can leverage parallelism to a much
greater extent
so to clarify this architecture a little
more
we'll talk about multi-hair retention
and
multi-head attention is just i believe
a fancy name for having multiple
key query and value parameter matrices
so if you look at the diagram on the far
right
you have the keys values and queries
which are produced
from the hidden state of the encoder as
usual
and instead of having one parameter
matrix you have
something like 8 or 16 or even more
parameter matrices
to come up with more keys and queries
and values for
a single encoder state and
on the diagram on the left is the
sequence of operations that
the paper proposes to use so as we saw
before
you do a scale dot product attention
between the query and the
q key in the query you optionally mask
it
out so we look at the purpose of the
mask which comes into play in the
decoder in just a second
but after that you can do a soft max and
multiply it with the value
as usual and the intuition for using
multiple attention heads
is that each of these heads can
focus on different parts of the sequence
and each of these heads
can have keys queries and values that
lie
in different vector spaces
so this is what a complete transformer
encoder looks like
all right contains a small question yes
go ahead
in the last slide the uh the modifier
for the modified
attention there's a concat block so is
that
does that mean flooded
um i wouldn't say it means
flattening it's just that
when you compute so from the diagram
uh if you remember that we had those
red uh representations coming out of our
final attention state
and you have each number of those coming
out
now since you have h tension heads and
so you simply concatenate them end to
end
and again project them through a
different linear
matrix to project it into the
vector dimension that you want at the
end
so when you do contamination it's still
a two so you will be
be a 2d matrix
it will be a vector i believe for a
single
for a single example yeah
okay
because each of those attention heads is
giving you one vector representation and
then you concatenate all of those
right so the transformer encoder
contains
uh multiple blocks in the paper they use
six of these blocks which are
visualized on the right and
the way uh the sequence of operations
happens in this
is that let's just talk about the gray
box at the
top for now you get some hidden states
as inputs as we saw before you compute
multi-head attention and then there's a
skip connection
around multihead retention into the add
and norm layer
and this add a norm layer is a layer
normalization
of the sum of these inputs and the
output of multi-header tension
similarly there's a second sub layer
which is the field forward layer
where you take the output of the first
add-in norm
simply feed forward through an mlb
and compute another added norm with
another skip there
skip connection and you repeat
this entire computation um n times where
n
in the paper is 6. so
you can do simply this
uh and probably get away with getting
decent results but the authors also
propose using
positional embeddings uh positional
encodings
and the intuition for using positional
encodings is
that now that we've gotten rid of all
the left to right computation
in our network how will the network
figure out what the relative ordering
between each of those input tokens is
and so you come up with something called
positional encodings where you add some
value to your input embeddings to let
the network
learn that there is some inherent order
in the input sequence that you are
passing it and
one proposed positional encoding
is to use a sinusoid so you can just
model a sine wave which goes over
all the input sequence tokens and add
the values to
the traditional word embeddings or input
embeddings that you have
or you can let the model learn its own
positional
encodings but there wasn't a big
difference in performance from what the
authors report
without positional encodings the two
sentences
shown here in the last bullet point
would be identical to each other
um so i like 11785 more than 10707
would be identical to the output for i
like 10707
more than 11.785
next we have the transformer decoder
this
is almost exactly the same as the
encoder
except that you have an extra layer here
sub layer here in the beginning called
the mask multi-hair retention
and the reason for using a masked
multi-head attention
instead of normal attention is that
for each of the output
sequence tokens you want to enforce
some sort of left to right dependency
and so you come up with a mask matrix
which is just a way of blacking out
some of the inputs at each of the output
states so
for example the
let's see an example would be that you
have some sort of a diagonal
matrix which at
output state j only pays attention
to the previous j minus 1
input states and that's the only
difference between
the transformer decoder and the encoder
and the rest of the block looks
almost exactly the same
so putting it all together this is what
the entire
transformer architecture looks like um
input sequence is used to compute the
keys and values
in the encoder master tension blocks out
blocks in the decoder transform the
output sequence until the current time
step
into the queries and multi-head
retention in the decoder combines the
keys queries and values and the result
result is projected
into output probabilities for the
current timestamp
so that's the end of my presentation
there's more resources that you can find
these two links and the slides are
already posted on
the website so i'll pause your
if you have any questions and shriek and
then start her part of the
presentation
do you have any questions
uh can you explain can you explain again
what is
the mask
sure so um
in the decoder you want there to be some
sort of
left right dependency especially when
you're doing language modeling
and a mask is basically a way of
telling um the decoder at each
step in the output sequence to not pay
attention
to the consecutive time steps so
for example if you have um five time
steps
in the decoder and you want to calculate
attention
over all of those five time steps
but you're only at the third time step
currently then
the mask is basically going to multiply
time step four and five with zeros
and only compute attention over time
steps zero
one and two does that make sense
uh yeah so actually it is going to
sort of uh like sort of um
obscure the outputs that
it's not supposed to look at yet right
maybe okay correct
uh any more questions before i start my
presentation
i have one question so the the last
diagram you
showed um putting it all together so
that only does that only work for a
single uh
embedding or does it work for the whole
sentence and you just
slide the uh the embedding so
um it's for the entire sequence let me
share my screen again
so it's going to take one entire
sequence of input embeddings so your
entire sentence
will have some uh matrix of input and
bearings that's fed into multihead
retention
right so did the input embedding contain
the entire sentence or would it be one
by one but
sequentially fighting with them the
entire sequence
so for example if you have five words
and
each word is a 300 dimensional word to
work
embedding then the input to this entire
architecture will be
five by three hundred
matrix of five cross three hundred
[Music]
so so if if the sentences are of
different lengths we have to
pad it all to the same plane
thank you no problem
uh so yeah so
uh and rav already gave you information
about what attention is and why do we
use attention
now next important question is how do we
use attention
so uh now
let's try to so before that i just
wanted to
give this information that like this
example is just for demonstration
you might not be using the same hyper
parameters or the same architecture for
your homework
upcoming homeworks so let's start with a
toy example
so uh we know that attention uh
basically the task is to you
to do a sequence to sequence modeling
and what right
in this example what we are doing
currently is that we are going from an
english uh um a representation to
to its corresponding pronunciation like
a word to its corresponding
pronunciation
so uh let's take an example
so for example we have an a data which
says
c and we see its corresponding feeling
phoneme
representation is like whatever this
character is
e so we have like 12 examples in which
every um every word has its
corresponding
phoneme representation um then we have
like a set of phonemes that we will be
using
in our example for this example we are
using
these this this uh s and this slash s
represents the start of a line and end
of a line
basically start of a sentence or end of
a sentence and this is basically your
dictionary mapping or the mapping that
you have for all the
phonemes present in your training exam
now
um so every target sequence would be
appended i mean prepended and appended
with
start of sentence and end of sentence
character
this is basically used for your encoding
and decoding purposes so for example in
your encoding you use
the part which only has start of
sentence and in the decoding part you
only
uh sorry in that in the decode in the
encoding part you only have
um starter end of sentence but in the
uh decoding part you have start of
sentence
how will uh how will that be used will
uh get to that
so you have your data and uh
now you want to pre uh you have to
pre-process it to make it uh
usable in the form of encoder decoder
architecture so what you do is
that if you see here in the example
um what could you observe about
the phoneme representation
do you have any answers on that
like do you see is there a certain
uh length like is there a fixed length
consistency in in the length of the
input and the output
no right i mean you see it's s e
but it's just two character
representation here and it varies around
all the
training examples that we have so what
we do in here
as part of preprocessing is that we take
uh first of all what we do is that
whatever we had
in our um dictionary phenome
phoneme mapping we map everything
as our x representation so for example
um this one um yeah
so i didn't talk about this so this
letters will also
represent the index mapping um
for example let's take this c
and we see that in here c s has
an index of three e has an index of zero
and zero
so your x vector would be representing
something like this
three zero and zero
and uh when you go for your y vector
which is basically your target vector
which is this phoneme representation you
will again do your index mapping
and uh um if you see
example of s e so
when you go there you get your y
representation to be
s which was 0 and i y which will
which will be um
0 1 2 3 4 5 so
it will be 5 here 2
and five so uh so before you
so you would also see that as part of
your uh target
you have appended and prepended start of
sentence and end of sentence character
which also has an index mapping
so you see here you have like start of
sentence which is zero
then you have uh s which is two then you
have e
which is five and um
and then you have like end of sentence
which is one
so this is how your um x and y would be
mapped to the index
then since you know that like all the
character
like all all the all the inputs are not
of similar length
so before passing it to the lstm layer
we will make them of similar length
and uh that's what we do in here
but but before that we also need our
info on our information about what was
the
input size of each sequence so we store
that
information using this this x lens
represent
the the length of each sequence in your
training example
and then what we do is that we pass this
to your pad
pad sequence so which basically pads
everything to the same
uh uh same uh shape so if you see here
uh zero is basically your start of
sentence representation
two is s representation five uh five is
your
e representation one is your end of
sentence representation
but since your maximum uh uh the length
of your uh
in the length of uh your input the
maximum length of your input
was uh coming out to be one two three
four five six so you had to append two
extra zeros here
so this is how you make your uh input
length uniform across all
uh the sequences present in that
training example
so uh any questions on this part
okay um
um anurag
yes yeah i'm just making sure
uh somebody has had a question do we
have to do it manually
can you speak up and ask
okay so yeah yeah uh for padding uh
i if you're talking about this part
uh you can just use pad sequence which
is one of the lstm libraries
and you can pad it to the to the maximum
length present in your batch
uh does that answer your question
i cannot see what's going on chad so um
i i assume it does because there is no
response okay cool
so yeah so before that uh so since we
are done with the preprocessing
uh part now uh let's
see now let's get into the model
architecture
and uh there's another question uh when
exactly are we finding
max of x lens
uh right so basically when you do here
length of sequence uh this is where you
take the maximum so basically what
happens is implicitly
you have your uh length of so you have
already saved each c
each sequence length in x lengths uh
when you pass this x to your pad
sequence
pad sequence will by itself check what
is the maximum length of the sequence
passed to it and then
accordingly uh pad all the uh
input sequences within that batch
there's another question uh which asks
is the data
transposed as compared to usual it seems
like each data point is represented as a
column as opposed to the usual vectors
yes yes for this example yes that's the
case here
another question sorry yeah so
i would say why that happened because
you have
batch as your second dimension in here
so that is why your
you see see your data as in this way but
you can do anything
like what you what you need depending
upon what kind of lstm configuration
you're using
yeah right there's uh another
option that you can pass to that called
batch first equal to true or false rate
sweetie
yeah great another question
is why are we computing the lengths
twice
if it happens in that sequence
um so yeah so you uh this is a very
important question
and uh this i would see how it is being
used
in the attention model so basically you
calculate a mask
and this mass depends on what what is
the input sequence length original input
sequence length because you when you
back propagate you only back propagate
through those
sequences which actually took part in
the computation
but not those which were useless which
were like uselessly padded as zero
i'll explain that in in the later stages
yeah so uh yeah so now your uh
um your whole attention sequence to see
sequence model has three major
components
uh basically it has two major components
which is encoder and decoder
but when you want the relative context
or the relative sequences uh sequencing
in your uh like basically when you want
your attention to come into picture you
add another component which is attention
component
so we'll see each one of them
individually encoded
attention and decoder and uh if you
might have started
your homework 3 you would have given a
task where you had to like
uh given an input sequence you had to
predict the label of it and for that you
did nothing but like
doing an encoding which was using an
lstm layer
so how would we do that is that we have
an
encoder uh and then first first thing
that we do
is we calculate the embedding of the of
the text
so for the example that we took was c
and we we want the corresponding vector
representation of that c we will also
uh provide an embedding size which is
like in your embedding space what what
exact what exactly is the dimension
that you want for your corresponding
input text
so for example just to make it clear um
yeah so you have like c
and using n dot embedding
in the vector or in the embedding space
you just
do a you do a vector representation of
this
of the c so this is what uh this
embedding layer would do
um then yeah then you pass
that particular input that particular
time series input to your lstm layer
which would be bi-directional because
you uh want like
forward and backward information and uh
yeah so these two part will
uh this is just a simple example of uh
encoding
where you have like embedding and
analysis lstm
so what you do is you get uh if you see
here we path
pass the length input here so first we
pass
x to our embedding layer after getting
the embedding
we again have to do that back padded
sequence thing before passing it to
the lstm layer and here you pass in your
lengths
of the vectors so depending upon this
your
sequence will be padded and whatever
padded uh
you padded sequence that you got you
will pass it to your lstm layer
now this lstm layer would you would give
you a hit
final state of a hidden state output
your output and a final state
final hidden state here now
using this output you will pass it to uh
at pac sequences which will basically do
your unpacking
and uh you will get your encoder input
and encoder input
lens corresponding lengths of each uh
input that you gave and you just return
that particular input so basically this
particular whole model
or whole component would give you an an
embedding of the
entire input and basically you do it to
um reduce the length of your input uh
you would see that
uh like being used in homework four
so this is what an encoder does
um then we want the information of
attention
and for this example we would
demonstrate how dot product attention is
being used
so your whole attention has like when
you
bring attention into picture you have
like key value and query into
uh these three things and apart from
that you have like so context vector
what each would have uh what like each
of these mean we'll explain that so
yeah so let's go to uh
attention and before that i just
wanted to explain i
think i would better annotate
yeah so
um for clarification we're not allowed
to use attention for homework 3 right
no got it
so this is your query this is your whole
p value that you get um
and what you that you get from the
encoder so what you do is that
for each query that you get you
calculate the similarity score
with every time stamp of your key vector
and then for example uh this
had a value of 0.8 this had
a value of 0.9 this had a value of
0.1 so we see that the maximum
similarity score of this particular
query which would be a particular time
stamp uh
by by query i mean that when when you do
your decoding
you will have like start of sentence
and then you have some some for example
ih
and then you have p or something or n
something like that
and this is what you pass in as your
query
and then you calculate the uh the score
similarity score with your key value and
you get
like uh for example you got 0.9
here so this this will be the so your
query will have a maximum score
of uh a maximum similarity score with
this
with the with the entry corresponding to
this particular timestamp
which means that it is more related or
your energy is more focused there
and then after you're done uh doing this
uh you will convert all this similarity
score is what
uh that you got into uh you know
probability representation so right now
i
explained it in 0.8 but the values that
you might get
might be 10 100 or something and
you have to convert all of them to a
probability representation which will
like bound the values from 0 to 1 and so
all your weight vectors this is nothing
but your weight vectors that you get
weight values that you get with each
time step and then
you do a summation like you combine all
take an uh weighted average or a
weighted
uh summation and then you calculate your
attention on top of that
so how would that done in port is
something like this um
yeah so you have your context and you
have your query
uh sorry i'm i meant context when i said
key
so um so this is your query and this is
your context
this torch.bmm is calculating the
context and query
the dot product between context and
query and then
uh as we have like somebody asked the
question about
why would we keep that lens so you know
uh so you have your input sequences
which has like
a padding for example
you had c
and then you have for example you have a
word
store
so you have like extra two zeros
when you do the index uh mapping you
will have extra two zeros in here
as part of your lstm computation but you
don't really need it to back propagate
or you don't really need it for your
attention layer
so uh what you do is that you keep into
account
the length of each sequence so this is
your
three the length of the sequence is 3
and this one is 5
so you want to take into account this
information and create your mask
which will only focus on the regions
which were really important for your
computation
so that's what we do here
we try to create a mask out of that and
whatever which was not
important or whatever which was padded
as zero would be kept as infinity
so this is how we create our attention
mask now
i told you that we have like so this
this particular these three
things basically computes the similarity
score between
the context and the query and after
you're done with that
uh you passing through a softmax layer
to convert it into a probability
representation
so at this particular point you have got
everything to be in the value of 0 to 1
for each weight vector for each uh entry
in the weight vector
and after you are given with the weight
vector then you again com
compute your uh uh like uh
weighted average uh or
attended attention weighted sum
which which you use this function again
torch.bmm
and uh you pass in your attention which
you got in from here
the probability distribution and the
context
and this is how what you go get as your
attention output which you pass
out from your um attention mod
attention component of your network so
this is how it is used uh another thing
here is
how do we use torch.bmm so i just
um
so what the dodge dot bmm does is that
you have
your in input in the in the in this uh
dimension
where first dimension is your b which is
your batch size
n and m and similarly you have like
the second matrix which has b which is
your bad size
m and p so after the after you apply
this function you have your final output
to be n cross b so whatever is common
would be taken
uh into consideration for like uh
doing the matrix matrix multiplication
and whatever the output that you get is
in the form of b cross n cross b
so this is basically batch matrix
multiplication
and this is how we do it so this will be
becoming very handy in your homework for
for calculating the attention and
yes so after we are done with this
then we come to the decoder so now
you have what what all information you
already have you have an information
where your input was converted to an
index mapping
and then you after doing that index
mapping you uh passed to
passed it toward encoder and then you
also after
passing to an encoder you also have like
uh
how to uh calculate attention on top of
that
now so basically we are at the stage
where we have like
um we are passing from um
you know uh text we have like text as an
input and we want
uh what is the corresponding phoneme
representation
uh of that particular input so that's
what decoder would do
you you would pass in c and it will give
you a corresponding representation of
that word
c so how we do it
is that the quotas architecture is a
little bit different than what
we have done in encoder so
yeah so if you see that you have like
another embedding
embedding layer which will be doing the
same thing as we have done
in the in the encoder now
after that another difference point that
you
need to notice is that we are not using
an lstm layer here rather we are using
an lstm cell
this is so because um
so if you see the listen attend and
spell
paper
this this is what your encoder does is
that
it encodes all the information from the
input and then
you get your hidden representation in
here and then you use like each
each particular hidden representation to
calculate
to like you know spell it or decode it
and
uh uh so as part of this you pass in
like
so
yeah so you pass in your so you have for
example you started with sos
and this was your uh query you
you calculated attention on top of that
and then you had a
hidden representation here so whatever
h0 that you
had here when you're doing it for the
second input you need to have that
particular information in here
so uh which is basically called c1
so this is c1 and then you pass into
to the next time stamp so you cannot do
it with
lstm a layer the normal lstm layer that
we use because you don't have the
liberty of
passing in the hidden state like one
particular hidden state
or the cell state of that to to the next
time stamp so that's why we use a lstm
cell here
[Music]
and
yeah so uh yeah so if you see in
in your decoder you have like uh lstm
cell and then you have attention which
will
calculate attention and then you have
you find a linear scoring layer
as as we use it um
in every model so um then
how uh how the forward would look like
is that you have your x
which is your input you pass into your
embedding layer
then you pass uh
your lstm like you have your lstm
for the first uh particular input you
will have
none as your state but uh like
in the in the uh as you proceed through
your timestamp if each
previous timestamp would would
correspond to the state that you have
so then you calculate uh use your x and
state
and you calculate your new state and
then you use your new state
um yeah so
yeah so this is your computation of new
state and then you use your new state to
calculate the attention
with the help of context vector and you
concatenate
uh the attention the the new state and
the uh
x attended and then you pass into
through your output layer
so this is how you calculate like decode
uh
this is how your decoder looks like and
yeah so after you're done with decoding
you need to
keep all those components together and
you need to start training
and for that like let's take this
example you have like your encoder
you initialize your decoder and you
whatever optimizer so as part of this uh
training loop
you will first call so you had your
input you will first
call your encoder you'll get a context
and a state
and uh uh after that
you will yeah so after that you will
in in here you will since you have
already
you are got like your decoder is already
taking into account
the attention uh it will output your
attention
the next hidden state and the output of
the network
and uh the last computation that you
would
basically do is that since i already
told that
uh we want our laws to propagate only
through the non-zero values of the
vector input vectors that we had
the padded value shouldn't be taken into
account so this through this
active you're basically creating a mask
to calculate losses
on those corresponding non-zero inputs
and
um the correspond by corresponding
nonzero inputs i mean that
non-padded inputs the original inputs
and this is how you calculate the loss
for those particular inputs and then you
uh
just do a backward so this is how we
train the network
and uh after you're done training
then you so this is not a very good
this is this was a toy example so this
is not a very good representation of an
attention plot
but if you are on the right track your
attention plot would come as to be
diagonal and that's how like you you
um i mean you make sure that your code
is running properly and all your
encoding and decoding is big
is happening uh in the right way and
then
uh yeah as part of decoding there are
various different techniques to decode
uh one of them is greedy decoding and
another one is beam search
beam search is uh is gives better
performance than greedy decoding
but for demonstration purpose i'm using
greedy decoding
beams search would be dealt in class by
professor
and uh so basically what you do is that
you
you have uh i told you that you have
like
um for every sentence you have start of
sentence and end of sentence
so you don't take uh your you make sure
that you
the input that you take doesn't have a
an end of sentence
you just have your startup sentence and
the corresponding input
and then you pass it through the decoder
and after
the output that you got from your
decoder you just taken out max or on
that decoder
and that's that's the probability of the
best class that you have for that
particular timestamp
and this is what you get so you just do
an art max that you usually do but
right now you're doing it for every time
stamp and
this is how we do greedy decoding um
any any questions that you have in the
entire uh
architecture
i see some questions i have a question
yeah
what is what is the context
right um yeah so
so i think and
yeah so if you see this diagram which uh
basically context is the previous hidden
state in
output that you get and you
so what you do is you you you pass that
particular
information so for example
yeah so um
yeah so for example you started with
your sos and you got
some uh some information in here you
pass that context information which is
basically your attention
context that you calculated and you pass
into
to the next time stamp so how i would
uh explain that
again
i can use the paper here to
explain that mathematically
yeah so if you see that you basically
quan cut it so
so here when you see um
you have your output here and you have
your attention here
so whatever you pass in for your not
here
sorry encoded input here and you have
your final state here
so whatever encoded input that you have
in here
you pass it as you take it as
the context for your first time stamp so
here if you so this context is nothing
but the output of the
encoder layer that you got uh does that
make sense
yeah i think so thank you
i have a question do you think the
encoder decoder are together or
separately
uh we clean it uh together
you have to train them together right
because there's no supervision for
just the hidden states from your encoder
right right because i didn't pay
attention to the training call
right
any other questions
all right if not then we'll keep an eye
on any questions that might come up on
piazza and post the recording as soon as
it becomes available
sriti do you have anything more to
no i think that's it thank you for
coming
all right thank you everyone bye
thank you