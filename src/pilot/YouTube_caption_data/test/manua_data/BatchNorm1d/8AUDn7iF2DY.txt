 All right, let's now talk about how we can use batch norm in
 practice. And also, we will talk briefly about how batch norm
 behaves during inference, because yeah, you know that
 during training, we have mini batches, but that's not
 necessarily the case if we want to use our network for
 prediction. Sometimes we only have a single data point. So how
 would that work? So but yeah, first, let me show you how
 batch norm works. When we use pytorch. I've actually just used
 the code from last week where I implemented dropout and just
 made a slight modification where I removed dropout and added a
 batch norm layer. So I was just accept that reusing the same
 code. So I don't want to walk through this in detail. But you
 can find the code here on GitHub. And here I'm showing
 you the main code, the multi layer perceptron that we have to
 modify. And yeah, so what we have here is one, two hidden
 layers, and the output layer at the bottom. And first, notice
 that I'm using again, flatten, that's because I was working
 with MNIST and MNIST is on n times one times 28, times 28
 dimensional, and flatten will essentially flatten this to a
 vector to n times 784 dimensional vector, where n is
 the batch size, and then it will work with the fully connected
 linear layer here. So yeah, I here insert batch norm after the
 linear layer, notice that there's a one D, it may be
 confusing. Why is there a one D? That's because there is a
 slightly different version of batch norm for convolutional
 networks. We will discuss this in the convolutional network
 lecture where this would be called batch norm 2d for the
 convolution networks. So to keep them apart, this is called batch
 norm 1d. This is essentially just the batch norm that we
 discussed here in the previous video. So the one is just to
 keep them apart. Yeah, and here I'm doing the same thing in the
 second hidden layer. Also, yeah, notice that I set the bias to
 false because it would be redundant, right? Because if we
 compute the net input as, let's say the weight times the feature
 plus B, and then batch norm, we had this plus beta values in
 that way, the bias becomes kind of redundant. So it's not
 necessary, but you can have it there. It doesn't really matter.
 I ran it with and without it, I didn't notice any difference in
 this case. Okay, so this is how your batch norm looks like a
 full code example can be found here. But there's really
 nothing, nothing really to talk about, because it's just two
 lines of code here. Um, yeah, so I was also just for fun running
 some experiments without the bias that I just showed you. And
 having batch norm before the activation, that's usually how
 yeah, that was originally how it was proposed in the paper. But
 sometimes people also, nowadays, it's even more common to have
 it after the activation. I will talk more about that in the next
 video. My phone some benchmarks, some more sophisticated
 benchmarks I wanted to show you. So but here in this case, when I
 ran the code of the multi layer perceptron that I just showed
 you, I enabled the bias. So I'm actually not showing it here. I
 don't know why I deleted it. But on by default bias is true if
 you don't set anything and I found it was the same
 performance. I then also inserted batch norm after the
 activation instead of before the activation like here or here. So
 I know, instead of here having it before the activation, I now
 have it after the activation in both cases. And I also didn't
 notice any, any difference really here. Um, yeah, and then I
 also ran experiments with dropout. In this case, also, I
 didn't notice much of a difference, except now the
 network was not overfitting anymore. The test accuracy for
 both dropout cases was slightly lower compared to no dropout. I
 think I use just too much dropout. But I could at least
 see there was no overfitting anymore. But either comparison
 here is inserting batch norm before the activation, and then
 dropout. And then after the activation, and then dropout. And
 I also did not notice any difference here. In practice,
 people nowadays, it's more common to actually recommend if
 you use dropout to recommend having batch norm after the
 activation. And yeah, one little fun memory aid to remember that
 is, if you consider this case, so you have batch norm, then
 you have the activation and then you have dropout, you may call
 it bad, it might be better to have batch norm after the
 activation, that's typically a little bit more common. In this
 case, I didn't notice any difference, it may make a small
 difference in other types of networks like convolution
 networks. So I would if you use dropout in a bedroom, I would
 probably go with this variant. But yeah, of course, it's
 something, it's a hyper parameter, essentially to
 experiment with. Alright, so I have one more thing about drop
 out in pytorch. So when we look again at our training function
 here, this is exactly the same training function that I used
 last week in dropout. But again, this is again, highlighting
 train and evil are important here that we during training set
 our model into training mode, because that's where batch norm
 will compute the running mean and the running variance, I will
 talk about this in the next slide. So here, batch norm will
 actually compute some running statistics during training. And
 these running statistics are then used in the evaluation mode
 Asian mode when we evaluate our model on new data. So during an
 evaluation, you have to imagine that you are mimicking the
 inference scenario. And in inference, you may only have a
 single data point, right? So let's say you have the Google
 search engine, and there's just one user running a query, and
 you have a network that has batch norm. So you have to
 normalize, but you don't have a batch of users. So only if one
 user. So how do we deal with that scenario? So there are two
 ways to deal with that scenario, the easy one would be to use a
 global training set mean and variance. So you would compute
 these means for the features and the variances for the features
 for the whole training set. That's something you would also
 usually do or could do when you compute the input standardization.
 But this is actually not very common in practice for some
 reason. So what's more common is to use an exponentially weighted
 average, or a moving average is just a different name for that.
 So usually, practice people keep a moving average of both the
 mean and the variance during training. So you can think of it
 as also as the running mean, how it's computed is by having a
 momentum term, it's usually a small value like point one. And
 this is multiplied by the running mean from the previous
 on epoch, or sorry, previous mini batch. And then what you so
 you have this, this term, this is like the running mean times
 momentum term, this is a point one value. And then you have one
 minus the momentum, this is like a point nine value, then plus
 yet plus point nine times the current sample mean. So that's
 the mini batch mean, and you just do the same thing also for
 the running variance. So here, essentially, this is just like a
 moving average or running mean. And you do the same thing for
 the variance. That's what you keep. And then during inference,
 you use that one to scale the data point that you do a
 prediction on you yourself don't have to do that. Yeah, yourself,
 by the way, by using model evil, it will actually happen
 automatically. But yeah, here's just like the explanation what's
 gonna happen under the hood. Okay, so yeah, that is how batch
 norm works in pytorch. And in the next video, I want to
 briefly go over some very brief, a brief rundown of all the types
 of literature that try to explain how batch norm works.