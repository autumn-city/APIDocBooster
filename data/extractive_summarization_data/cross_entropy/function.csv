"torch.nn.functional.cross_entropy function combines log_softmax(softmax followed by a logarithm) and nll_loss(negative log likelihood loss) in a single function, i.e.  it is equivalent to F.nll_loss(F.log_softmax(x, 1), y)."
"Yes, the cross-entropy loss function can be used as part of gradient descent. "
"In short, cross-entropy(CE) is the measure of how far is your predicted value from the true label. "
"The cross here refers to calculating the entropy between two or more features / true labels (like 0, 1). "
"And the term entropy itself refers to randomness, so large value of it means your prediction is far off from real labels. "
Cross-entropy is commonly used to quantify the difference between two probability distributions. 
"In the context of machine learning, it is a measure of error for categorical multi-class classification problems. "
"Correct, cross-entropy describes the loss between two probability distributions.  It is one of many possible loss functions."
Sparse functions are a special case of categorical CE where the expected values are not one-hot encoded but is an integer 
"Usually the ""true"" distribution (the one that your machine learning algorithm is trying to match) is expressed in terms of a one-hot distribution. "
"Note that it does not matter what logarithm base you use as long as you consistently use the same one.  As it happens, the Python Numpy log() function computes the natural log (log base e)."
"Cross entropy is one out of many possible loss functions (another popular one is SVM hinge loss).  These loss functions are typically written as J(theta) and can be used within gradient descent, which is an iterative algorithm to move the parameters (or coefficients) towards the optimum values. In the equation below, you would replace J(theta) with H(p, q). But note that you need to compute the derivative of H(p, q) with respect to the parameters first."
" Alright, so yes, again, the cross entropy,  again, recall that there are two sums.  So if I go back here, so  we have these two sums here, I kind of entangled them a little  bit.  So we have, this is a sum over the training examples.  And  this one is the cross entropy for the 100 encoding.  So this  one is the inner, the inner one here.  So let's compute first the  cross entropy for each training example.  So what I'm doing here  is I'm computing these terms. "
" So this function, the negative lock  likelihood loss expects the lock of the softmax values. "
 So you can see this  negative lock likelihood loss is the same as our cross entropy  here. 
" So where I mentioned  that the negative lock likelihood and binary cross  entropy equivalent in pytorch, it's actually the negative lock  likelihood and the multi category cross entropy  equivalent.  I mean, in a way, you can also think of it as a  multi category one, the multinomial logistic regression,  then this would be still true. "
" When we compute the cross entropy,  because we use the mathematical formulas, we compute first the  softmax.  And then from the softmax, we compute the cross  entropy in pytorch, they do all that work for us inside this  function, they do it for us. "
" And notice that I said, redact  reduction to none, which means it does not apply the sum or the  average, which is this outer one here.  So by default, when you  use this cross entropy, it will perform the average, you can  test it like this, see, it's the same same value.  If you wanted  to, you can also say reduction to consider reduction to some. "
" Actually, we had a seminar at UW  last week, where we also, yeah, it was briefly mentioned,  coincidentally, there was like a question whether it's the same,  the negative log likelihood and cross entropy. "
" So yeah, the  negative log likelihood and the binary cross entropy are  equivalent.  And in practice in deep learning, people just say  cross entropy, multi category cross entropy, which would be a  multi class version of the negative log likelihood, which  we will cover later in this lecture when we talk about the  softmax function.  So just to keep it brief, the negative log  likelihood that we just covered a few videos ago, is the same as  what people call the binary cross entropy, they were just  formulated in different contexts.  So negative log  likelihood comes more like from, I think it's like, it's probably  from a statistics context, I don't know the first paper, or  reference that mentioned that.  But this is something usually I  see in statistics papers, and the binary cross entropy thing  has originated from the field of information theory, or computer  science.  So we have actually seen that, or not, the cross  entropy, where we have seen the self entropy, or just entropy,  and statistics 451. "
" For those who took this class, in fall  semester, where we had used the entropy function in the context  of the information theory and decision trees, but we used a  lock to instead of the natural algorithm, but yeah, it's kind  of somewhat related, if you have taken any class where you  talked, for example, about the KL divergence, or callback  Leibler divergence, which measures the difference between  two distributions, the KL divergence is essentially the  cross entropy minus the self entropy. "
" The only  thing you have to know is or should know, because it's useful  to know, is that the negative log likelihood is the same as  the binary cross entropy, this is like a useful thing to know. "
" And  there's also a multi category version is the multi category  cross entropy, which is just a generalization of the binary  cross entropy to multiple classes.  So in order to make  that negative log likelihood or binary cross entropy work for  multiple classes, we assume a so called one hot encoding, where  the class labels are either zero or one for some reason, it was  cut off here. "
" So again, all I wanted to say here is the  logits in deep learning, usually refer to the net inputs of the  layer that just comes before the output. "
" We have   to use something called cross entropy loss,  and this is actually the loss function that   fastai picked for us before without us  knowing. "
 The first part of what cross-entropy loss  in Pytorch does is to calculate the softmax. 
" And so here is:   each of the part “y-i” times log of “p-y-i”,  and here is…(why did I subtract that's weird,   oh because I've got minus of both, so I  just do it this way, avoids parentheses…)   yeah, minus the are-you-not-a-cat times  the log of the prediction value not-a-cat,   and then we can add those together, and so that   would be the binary cross-entropy loss of  this dataset of five cat or not-cat images. "
" Basically it turns out that all of the loss  functions in pytorch have two versions – there's   a version which is a class, this is a class,  which you can instantiate passing in various   tweaks you might want, and there's also  a version which is just a function,   and so if you don't need any of these tweaks  you can just use the function. "
" All right   so that's all fine… we passed… so now when  we create a vision learner you can't rely on   fastaI to know what loss function to use, because  we've got multiple targets, so you have to say:   this is the loss function I want to use, this  is the metrics I want to use. "
There is just one real loss function.  This is cross-entropy (CE).
The goal of calculating the cross-entropy loss function is to find the probability that an observation belongs to a particular class or group in the classification problem. 
