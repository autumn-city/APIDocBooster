After the very first Conv3D, the output tensor is:
I realized that the self.conv3d was a single object being called for two different 3D convolutions with different input channel requirements.
self.conv3d1 and self.conv3d2 with exact same architecture.
For your implementation of Conv3D, if you want to implement ops, you can have look on   [custom operators (hyper-link)].
If my observations are correct, we can say that a single-layered d*h*w tensor going through a conv3d (1x3x3) kernel is the same as a d-layered h*w tensor going through a conv2d with d, d-layered 3x3 kernels.
However, when adding depth to your conv3d (layers could work as a 4th dimension), you won't be able to match that with a conv2d, as the name implies!
According to the [Conv3D docs (hyper-link)] in the latest version of tensorflow as of this post, the default shape is channels_last.
No, they are exactly the same, Convolution3D was renamed to Conv3D in Keras 2.0, so the difference is just which version of keras the code is targeting.
** Edit: updated the link
[Here (hyper-link)] is a custom imagedatagenerator for 5D input to Conv3D nets.
Each input to Conv3D must be a 4 dimensional data point.
The Conv3d class expects the batch_size to come first then the number of channels then the number of frames then the height and width.
You should change the shape of your input tensor to [5,3,1,126,126] Your conv3d parameters are also wrong.
The first number should be the number of input channels the conv3d is supposed to get which in your case is 3 because it is an rgb image.
To make sure you get the same dimensionality after performing convolution/maxpooling as you need to set padding=same while specifying Conv3D and MaxPooling3D layers.
If you have a conv3D, then you have have to be careful to chose the right axis for normalization.
As it is mentioned in the documentation, if you choose "channels first" for your conv3d, then you can do BatchNormalization with axis=1 on a 3D vector (since the position of the channels will be the same as for the conv2D)
The default values of the keras.layers.Conv3D are adjusted accordingly anyway.
However I have fork of lasagne that implements it:
[https://github.com/gyglim/Lasagne (hyper-link)] (Conv3DLayer)
Its outputs are identical with the ones from Conv3DDNNLayer.
You are using [tf.keras (hyper-link)] while you have added Conv3D layer from keras.layers.convolutional.Conv3D.
model.add(Conv3D(32, kernel_size=(3,3,3), strides=(1, 1, 1), input_shape=(64, 64, 10)))
Input_shape for Conv3D has 4 dimensions (time_sequence, width, height, channels)
Referring the link: [https://www.tensorflow.org/api_docs/python/tf/nn/conv3d (hyper-link)] the filter should be a tensor, to be specific a variable (trainable), so change the filter=[5, 5, 5, 3, 8] to filter=tf.get_variable(shape=[5, 5, 5, 3, 8], dtype=tf.float64, name='filter') then run it and should work.
So similarly for Conv3D the layer will accept a 5 dimensional tensor.
So replace the Conv3D with the following Conv2D:
You do not need to use Conv3D layer here.
The shape of your Conv3D filters would violate the calculation of size after application of a convolutional filter by reducing at least 1 dimension to less than 1 which is why you are getting your error (you can't multiple with a value that does not exist).
Similar to images with 3 color bands, these are typically not processed with Conv3d (maybe a different case with hyperspectral images, but that is not relevant here).
Conv3D is typically utilized in the case of 3 spatial dimensions, or sometimes 2 spatial and 1 temporal dimension.
Then start using:   

Conv3D if there are still spatial dimensions    
Conv1D if you eliminated the spatial dimensions     
In both cases, it's a good idea to increase the kernel size in the frames dimension, 3 frames may be too little to interpret what is happening (unless you have a low frame rate)
Finally, apply one or several [Conv3DTranspose (hyper-link)] layers to get to the desired output (e.g.
Essentially, the Conv3DTranspose layer goes in the opposite direction of normal convolutions - it allows upsampling your low-resolution images into high-resolution images.
I don't think there was any way to use conv2d or conv3d for the task.
I tried back then to use conv3d but didn't find a way to make it work.
If you are just looking to process the 3D image in terms of slices, then you can define a [TimeDistributed (hyper-link)] VGG16 network (Conv2D instead of Conv3D) would be the way to go.
If this is the basis of your application, then you cannot directly import VGG16 weights in to a Conv3D model, because the number of parameters in each layer now increases because the filter went from being a 3*3 to a 3*3*3 for example.
You can extract each layer weight from VGG16 and then construct a new numpy array for an equivalent Conv3D weight matrix and feed it to your Conv3D model.
According to your convolutional and pooling layer parameters, the tensor is downsampled after each Conv3D and MaxPooling3D.
The solution is to tweak Conv3D parameters: either use padding='same' (in which case the tensor shape is preserved after the convolutions and is halved only after pooling layers) or reduce the filter sizes from 3 to 2.
If you mean number of weights, if you define your layer as l (l = Conv3D(...) etc) you can access kernel and biase weights, and get its shape.
Your conv3d2d.py files seems to have been copied from an earlier version of Theano, and the syntax for @theano.gof.local_optimizer has changed since.
Applying that change alone may not be enough, though, so you may be better off importing conv3d2d from Theano rather than your repository, or updating the whole file.
You probably should read up about [differences between Conv2D, Conv3D (hyper-link)].
So You don't need Conv3D for images, you need Conv2D.
Note that -18 is the index of the conv3d_9.
This is supported by Conv3D and by LSTM with stateful=False.
(A Conv3D with kernel size = (frames, w, h) will work, but limited to frames, never understanding sequences longer than frames.
if you have 10 frames of size 64,64,3 each instead of doing conv3d, I did conv2d on 640,64,3 dataset and resulted in 86% accuracy on 16 classes for videos.
Add BatchNomalization after each Conv3D layer, it improves the convergence.
Try adding a smaller dropout after each Conv3D.
Conv1D, Conv2D, and Conv3D
I see there are calls to conv3d for which you have not provided the source code.
I believe it is not a mere wrapper around nn.Conv3d because the resulting size does not match (self.conv_5for example should not change the tensor shape with its given parameters)
However, when stride > 1, Conv3d maps multiple input shapes to the same output shape.
As far as i Know Conv3D need 5 Dimension input, (None,D1,D2,D3,D4).
which mean only 1st Con3D layer have (201,10,4) dimension, 2nd Conv3D layer will have (199, 1, 1), which is not optimal for 2nd Conv3D layer.
Long answer: The output of Conv3DTranspose layer (i.e.
Assuming you are using channels first data_format, your input_shape argugment to the first Conv3D layer  should be (CHANNELS, HEIGHT, WIDTH, DEPTH).
But your input shape tuple has length of 5, and that is not what Conv3D layer expecting.
Inspecting your last [nn.Conv3d (hyper-link)]'s output, you have a tensor shape of (-1, 16, 45, 54, 45).
you have misplaced an F.relu activation as your overall structure is [conv3d, relu, conv3d, relu, maxpool3d] + [conv3d, relu, conv3d, relu, maxpool3d] + [flatten, dense].
The problem is in your xtrain, it consists 1221 images of shape (50,50,1) however, the Conv3D is expecting images of shape (16,50,50,1).
Here is a custom imagedatagenerator for 5D input to Conv3D nets.
After first conv3d you have tensor with shape [None, 16, 16, 4, 32], therefore you have to use kernel with shape [3, 3, 4, 32, 64] in the second conv3d_s1.
No OpKernel was registered to support Op 'Conv3D' with these attrs means that the attributes you passed to the function call do not match any existing implementation for that function.
Node: Conv3D_1 = Conv3D[T=DT_INT32, padding="SAME", strides=[1, 1, 1, 1, 1]](Reshape, LoG_filter/read)] tells that for the Conv3D node in your graph that is raising the error, the input tensor has type int32.
tells that for the Conv3D operation there are available 2 implementations on your machine.
You don't want to implement Conv3D on your own, but rather change the input type to something that already has an implementation.
Please refer below sample code to implement Conv3D
It doesn't seem like you need Conv3D layers for this task.
If you insist to use Conv3d, your input shape should be: input_shape=(150,150,4,1) or (None, None, None, 1)
and trainset should follow shape like: np.random.rand(53, 150, 150, 4, 1)
It sounds like what you want to do is add a pooling step of some kind after your Conv3D layer, such that it flattens across the sequence dimension, such as with AveragePooling3D with a pooling tuple of (10, 1, 1) to average across the first non-batch dimension (or modified according to your specific network needs).
just after the Conv3D layer, and then made toy data:
As an alternative to ely's  Conv2D and AveragePooling3D solutions, you can set the last ConvLSTM2D layer's return_sequence parameter as True but change the padding of the Conv3D layer to valid  then  set its kernel_size parameter as (n_observations - k_steps_to_predict + 1 , 1 , 1).
Per above, I replaced Conv1D with Conv3D
The size of the dilated conv3d output is now (1, 8, 8, 8, 1) as expected.
