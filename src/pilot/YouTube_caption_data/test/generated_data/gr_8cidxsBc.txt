so
morning everybody ah
this we have less than
60 people in class okay so uh
does anybody recall where we left off in
the last class
anyone
anybody in the chat somebody who
remembers where we left off
anything on chat um nothing yet
nothing so we we stopped with the
the um back propagation of the
bi-directional
neural network vanishing gradients
people are saying vanishing gradients we
spoke of vanishing gradients and so we
left off there and said
the problem with vanishing it was not
just spanish ingredients it was also
the fact that everything was forgotten
and the reason for that
was what was the reason we stated
anyone remember
anybody what was the reason we stated
when the weights the weights the
singular values of the weights are
smaller than one they keep multiplying
with the
gradients of the divergence and then
that accumulates and
becomes lesser than zero and stops
learning yeah
so it doesn't stop learning so so we'll
get back to that yes
part of the reason right but we'll
revisit that briefly
we stopped off saying there was a
solution to this but it's a bit of magic
because of mystery and magic
long short term memory neurons and so
here is what we actually stated in the
last class
i'm going to draw things right to left
because that's easier
that's probably more intuitive so if you
had a network of this kind can you see
my screen
yes okay so if you had a network of this
kind
where everything is flowing right to
left right
then you had your first weight your
second weight you had
a bunch of some layer of neurons here
and then eventually you had some output
layer
and then you had an output right so this
was why
and so this was your first layer
activation this was your first layer
weights this is the second layer
activation second layer weights
third layer activation finally maybe the
output neurons so when we operated on
this i'm assuming
no bias because you can always assume
that there's an extra input
of one that have added and that the bias
are parts of the weights so this is just
for convenience
so the way we did this the first thing
you computed was
w1 x which was the input correct
and then you applied the activation on
it which was the first layer
then to this activation you multiplied
so
this is a function this is a function
right to this activation you multiply
the next layer weights
and then you apply the next layers
activation
do this activation you apply the next
layer weights and then you apply the
next layer activations
so this is a nested function
right
and what happens when you're going
forward through the network
what were the reasons things could be
forgotten anybody remember
out here
the gradients might vanish and uh but
you're speaking of going forward
remember we said we spoke a lot about
memory
the output is gonna saturate to some uh
some fixed value the output saturates it
to some fixed value that has nothing to
do with the input right and so
for the recurrent network the value that
it would saturate to depend it
depended on the weights the activation
the weights the activation the weights
and the activation
so if you're as you got through deeper
and deeper through the network if you
actually
it's the combination of weights and
activation at least in a recurrent
network
would make sure that uh the uh
the uh that's a wait let's put an
activation let me group it this way
what it would make sure that everything
eventually saturated that was going
forward
and so this was the issue that we had
the next network is actually
a nested function and in an
in the recurrent network the memory
depends on the
underlying underlying terms the weight
sort of either makes things blow up or
contract
and then the activation function makes
this saturate and so eventually
the term that you remember did that
depend on the parameters or did that
depend on the input
you remember it just depended on the
parameters it did not matter what the
input was
it was only going to depend on the
parameters and we have the same thing
going backwards now so when i'm going
backwards
i start off with the derivative of the
divergence
and then it was just a chain rule
remember
you had the uh you're going backwards
this way
so the derivative of the divergence then
you have the
uh the jacobian of this activation with
respect to its entire
input so this is the jacobian of the
activation
then you have the weights again then you
have the jacobian of this
activation with respect to its entire
input
and then you had these weights again and
then you had the jacobian of this
activation
and then you had these weights again and
so this was how the
back propagation went forward to inverse
again although i'm speaking of recurrent
networks over here
this was true regardless of the kind of
network and so once again
over here you had these pairs of terms
right
you had a jacobian of an activation
followed by a weight
and so what did we observe about the
jacobian of typical activations what
does it do
to the to this vector
do you remember anyone
shrinks it right
why did it shrink it
for the case of 10 10 h the derivative
is bounded between
0 and 1 so it can either leave it the
same or shrink it but usually
exactly right these whether it's tan h
whether it's
sigmoid whether it's a relu even if it's
a linear activation right
this jacobian is a diagonal where these
diagonal terms
are are capped at one
so these equilibriums are always going
to end up
shrinking the derivative going backwards
and similarly these weights what does
what do these weights do
anybody either shrink or
or either shrink or expand and so
typically they will just shrink it they
may sort of expand it in one or two
directions they're singular with
singular values greater than one
but a typical matrix has maybe you know
a small number of singular values
greater than one the majority are going
to be very small
and so the whole as you go back through
the network and you do your back
propagation
the derivatives are oh my good god
these as you go back through the network
the derivatives are going to
continuously keep shrinking
right so look at just these equations
how can you fix this
what if i don't want the memory to be
forgotten
how do i fix that over here what if i
don't want the
gradients to vanish how do i fix that
over here
anybody want to suggest something
somebody says use tray loop
use we saw it does things right it
half the time the derivative is zero
half the time the output is zero
well i think in the case of vanishing
gradients we could do
resonant we could just use skip
connections to
avoid the vanishing ingredients and in
the case of excellent
other issues you can yes that's a good
idea right you could have resonance
and you just what you did in a resonate
was you added things so which made sure
that the gradient had an
additional component of one but that
would cause things to blow up which is
why you had
things like batch moments such like
coming in so here is my suggestion
what if i remember i am speaking of
hanging on to memory correct
what if i got rid of this guy and made
it an identity matrix
would things expand our contract
anyone it would depend on the activation
right
correct so what if i make this an
identity function now
now what everything would say the same
everything would stay the same
nothing is either going to expand or
contract
correct neither going backwards nor
going forwards is anything going to
shrink
or expand so
this is the kind of kind of behavior you
want for memory
but then this is kind of naive right
what would happen is i have something
coming in
let's say time t equals 0 and
i learn something let me call this h
but then what you are suggesting by this
is that as time goes on
i neither i have no weights applying to
on it
and i have no activations applying on it
which means the memory is retained
forever
this works except you're supposed to you
know you don't want a constant memory
what was the kind of behavior we wanted
so we were speaking of
you know the example we were speaking of
was was was
trying to pass some code so when we were
trying to pass some code
uh the uh the idea was that
we would if you saw an
open brace you wanted the brace closed
and to close the brace you wanted to
recognize
that a closed brace actually matched
that open race
now to close the brace you didn't want
to forget that you had opened a brace
after
fixed amount of time you wanted to hang
on to that idea
until the brace was closed remember
when we were speaking of the little c
program example that we saw
or when we were looking at any other
kind of pattern when the pattern is open
you must wait for it to close and then
you close it
that's the kind of behavior we want so
uh
what we will do is now add some
corrections to these guys
at every input and what the correction
will do
is modify what is stored based
on the input and what is
already in the memory
does that make sense
questions guys
oh there's one that says so this is
equivalent to all weights equal to one
but
can't activation still shrink values we
have no activations either now so we've
gotten the activations are linear
activations the weights are identity
matrices
okay so what we're going to do is this
we're going to
have this little behavior here where
oh my god yeah so i'm going to start off
with some initially recognizing some
pattern
thereafter that each time i'm going to
modify the pattern in a multiplicative
way
or an additive way it can be either
which does something very simple
i'm going to say i i detected a brace
over here
so this one found an open brace then
this is time
so then i'm going to have some operation
over here that says
did i find that close do i have an open
brace
and does the input
include a closed brace
is this figure making sense to you guys
and here is the sentry direct so at this
time
i will just have a little pattern which
says
do i already have an open brace and does
my
input include a closed brace
if i have something of this kind then if
the include doesn't include
have a closed brace nothing changes this
memory is going to continue to the next
instant
but then at the next instant i repeat
the same text and now maybe
i find the closed base at which point
the memory is going to be killed and
there's nothing
being remembered going forward so now in
this kind of behavior
what is happening is that the what is
being remembered depends on the input
but how long it is remembered and how
you operate on it
also depends on the input and not just
on the parameters of the network
did that make sense to you guys yes
right the selection is actually part of
the input or it's a
part of the parameter here part of me
so the correction here is correction has
come
done completely separately right so here
is the basic idea behind
lstms that when you detect some pattern
the network does not have any intrinsic
parameters over here
that's going to modify the pattern the
only time the pattern is modified
is if you see something in the input
and of course based on what is already
being remembered
that says this pattern must be the
memory must be changed
okay i see thank you right and that
makes sense now this gets rid of this
whole business of the memory being
dependent only on the input only on the
parameters
and not responding to either what you're
trying to remember
or you know what are the triggers
that you need to forget it so this
very simple little idea extension is
what they call the constant error
carousel
in uh in the recurrent network so
here is the behavior you have some
memory pattern and the memory is just
sent through uncompressed no weights no
no non-linearities
the only thing had happens is that you
have some gating terms i'm calling them
scaling but there it gets you
but you need a uh there are different
ways of implementing this
right and the basic idea behind the
gating term is that
the gating term looks at the input
and what is already in memory and based
on that it modifies
what is in memory now of course this is
just the memory right
but a recurrent network is updating the
memory and then it's operating on the
memory to
to generate its outputs so the actual
non-linear work
is done by other portions of the network
these are neurons that compute a
workable
state from the memory so again you have
isolated the memory
from the state and so you have a memory
that's carrying everything
which is only changed based on what is
seen in the input
but then you have some additional
components of the network
that read the memory and perform
operations on it
and so and of course
what the uh how the memory is changed
must depend on a bunch of different
factors it depends on
the current input right the current
output
and once you begin looking for patterns
then you can add other stuff
because your ability to detect patterns
depends on many different factors as we
know right
so you can have other stuff and you can
even have uh you know
what's in the memory what's in the
hidden state all of these go into these
little pattern detectors
whose job is to determine
if the appropriate input pattern has
been detected such that what is in
memory
should or can be changed
so this structure is what we will call
the long short term memory it explicitly
latches information to prevent decay
and blow up and
the idea is that you want a memory
structure in these recurrent networks
where the memory depends on what is
stored
depends on the input you don't want to
remember every random thing
and when you're parsing code for example
the important things to remember are
opening braces opening loop stuff like
that right
you don't want to remember every odd
character so
what is stored depends on the input and
how long
the memory is retained and how the
memory is modified
also depends only on patterns detected
in the input that clear to anybody
questions
questions no okay we're not biasing the
patterns in any way right this is
through the corpus of data that we give
it it it learns the patterns from the
frequency of which they show up so
is that so how the patterns are learned
such and such like depending entirely
again this is learned behavior right
depending on the response everything's
learned the only key it's the structure
that we are here
that is important and the manner in
which we decide what must be stored and
how it must be forgotten is learned
does that make sense
yes no raise your hand
guys can you ask me a question yes thank
you yes sorry
yes because i do need a response so
there's i will wait that's how these
classes get
delayed right i can't see you so i can't
make out from your faces
i do need you to respond okay so
here is your standard recurrent neural
network things that we've seen
in the previous classes and each green
block
is basically one time instant the
computation of
one of the hidden state at one time
instantly this is what we will call
a recurrent neural network cell using
object oriented programming terminology
right
so within this hidden state what
typically happened was that you saw
at each time you read the input you read
the hidden state at the previous time
you operated on the com an alpha affine
function of the two using an activation
function
a tan h we saw was the best one and then
computed the hidden state again and this
went on to the next time step and also
went on
to the subsequent layers in the network
so this was your standard recurrent
network
this was happened this is what happened
at one time instant within one
computation what we will call
one cell of the recurrent network one
one rnn cell now just retaining the same
cell structure this is what the long
short term memory structure
ends up looking like it's a whole lot
more complex but then
although it looks very complex it really
isn't
uh there's i mean there's a lot of
unnecessary complexity over here but the
basic
idea is basically what we just saw
that you want the memory to go through
straight and the only modifications you
want made to memory must be based on
what is seen in the input
and not and must not depend on any
network parameters at least
on the memory line so the key
component to this uh lstm
is this memory backline backbone
which we call the constant error
carousel again i must
remind you that anytime i draw these
lines each of these lines is actually a
bank
going into the page remember this
notation i used where each square
represented an entire layer all
connections of you know
full connections and so on so you must
always imagine that this is just a bank
going into the screen so
this line means there's an entire bank
of memory lines remaining remembering
different patterns
but this is the key component which is
the
what we call the constant error
corrosion i'm not responsible for the
name so i can't explain it
but this is the memory structure which
just retains memory and
gets only modified by external gating
structures
so this cc carries the memory
at each time there's an incoming memory
ct minus one
and it may be modified or not depending
on
what has been detected in the input and
in the standard structure that has been
proposed to lstms you guys can simplify
it if you want and you would find that
you know
simpler structures often do work better
but uh
in the standard structure there are two
ways of modifying it a multiplicative
uh term and an additive term and
at the end of the time at that time
instant what comes out of the cell is
the modified memory
if no useful pattern has been detected
in the input ideally the modified memory
must be what went in itself
so what are these two operations these
are gates the first one
is the first here is is
a simple there again two kinds of
structures there is a multiplicative
correction
and an additive correction
multiplicative connections are what we
will call
gates so a gate is a simple sigmoidal
unit whose output lies in the range zero
one
so when you multiply any signal by the
gate
the output is either going to go through
unmodified or it's gonna shrink
right this just sort of controls how
much of the information is being led
through
and so the gate that modify multiplies
the cec line here is what is called
the forget gate it determines if the
signal and the cc
is to be retained or shrunk that is if
it's got to be forgotten a little bit so
you're remembering something
do i still continue holding the same
amount of memory or do i shrink it based
on
what has been detected in the pattern so
it allows for multiplicative shrinkage
and to decide whether memory must be
forgotten or not
we have to consider many factors
primarily the input itself
but also what was already in memory as
we just saw you know if you open the
brace
only then does a closed brace make make
sense
so this forget gate over here
is a sigmoid which operates on a
combination
concatenation in encoding terms
of the current hidden state ht
and the input xt you compute an affine
function of these two and then apply the
sigmoid activation to it you get a
multiplicative term between 0 and 1
that multiplies what's in the constant
error corrosion line
now the forget gate only allows you to
shrink the
input in a multiplicative way but then
you need you also need to be able to add
to it for instance
for for example if you were remembering
braces open braces if you've already
seen one
open brace and you see a second open
brace
you want to say the memory must say now
i have two open braces and none of them
has been closed
so for that you need some additive term
over here
and so this additive term is the second
component
uh and what this the second component
now has two parts again
there's a certain amount of almost
unnecessary complexity in the lstms
which while trying to implement the
basic concept we began with
so the first component over here is
this pattern detector this pattern
detector
says have i detected a pattern
which i must use which tells me i must
modify the memory so if i've seen an
open brace already am i detecting open
braces
such that i must actually increment this
the memory in this line but then
it doesn't just uh so
the uh that's the first part first term
that's this guy here the c tilde t
and then that's by the stan h but then
you also have
a gate which is this so-called input
gate
which decides whether what the pattern
that has been detected by this guy
is worth remembering so to increment the
memory here's what we will do
first we are going to find out if
there's a useful pattern and so the
output of the tan h is going to be some
value between -1 and plus 1
and then whether that pattern is worth
remembering which is this input gate
and what we will do is to multiply the
input gate
by the output of the pattern detector so
the entire modification of the memory is
as follows
first the memory is multiplied by the
forget gate
out here then you detect any additional
patterns new pad
new patterns multiply them by the input
gate
and uh and add it to this input now
although this whole thing looks very
complicated right
why do you want to uh
why do you want to find a pattern and
then multiply it by a gate
that's a rather complicated way of
saying i just have a fancier pattern
detector
so what happens is a tan h is an
activation of this kind which goes from
minus one to plus one
a sigmoid is an activation of this kind
which goes from
zero to one so
the product of a sigmoid and a tan h is
simply going to be something of this
kind
where initially it's all zero
because and then at some point it begins
to go
you know ends up right so this is a tan
h
times a segment okay
so although they like to explain things
in this rather convoluted way
if you think about it this is just a
slightly more complicated activation
function
but here's what happens look at this
it all goes negative it also goes
positive
so if i've seen an open brace
if i see a closed brace this allows me
to decrement the count of open braces
if i've seen another open brace then
this allows me to increment
the count of open braces so it allows
you to both
add and subtract to what is in memory
now the
these gates right the
the uh constant error corrosion over
here only
stores what is in is the raw memory
but then the network itself is a
recurrent network
which must apply some activation to the
raw memory
so before we begin operating on this raw
memory we will first compress it
using a 10 h activation and
so the 10 h activation is like so you
just
apply a 10 h to the memory component
twice
but then again to decide whether this is
even worth reporting
we're going to have a second gate called
an output gate
which considers everything including
current input pass memories and so on
to decide to sort of compute
a multiplicative term a gating term
which multiplies the
non-linearly transformed memory and so
the actual hidden state computed ht
is the product of the output gate
and the compressed memory
now all of these pattern detectors
we are trying they try to work on as
much information as possible because
again
we are very dependent on these pattern
detectors to decide if the memory must
be modified
and so typically the pattern detector
will look at the input
raw input it's going to look at the
hidden state from the previous time
but you can also have it look directly
at the raw memory itself
from the previous time so when you this
connection
which directly sends the raw memory into
these gates
is what is called a peephole connection
now this is a design choice these people
connections are only used by these gates
they are not used by these raw pattern
detectors over here
for whatever reason so
here is the complete lstm set with all
of the gates and connections
it looks pretty complicated but then if
you look at it from the
bit by bit as we just explained it it's
not very
uh it's not particularly complex the
previous raw memory
and the hidden state from the previous
time
and the input at the current time come
in
what goes out is the modified memory the
modified hidden state and the modified
hidden status of going to go into the
next cell at the same time and into the
uh cell immediate then the uh
it's going to go into a cell at the next
time at the same layer
and also into the next layer cell at the
same time
so here is the complete set of equations
for one step of computation
within a within an lstm cell first there
are three gates there's the
forget gate there's the input gate and
there's an output gate
all of these three must be computed then
there are
there's a pattern detector which detects
if there are patterns that are worth
remembering
so that's a c tilde and the actual
uh updated memory is going to be the
incoming memory times the forget gate
plus the output of this pattern detector
multiplied by the input gate that's this
term
and the ac the hidden state computed is
going to be
a non-linearly transformed version of
the latest memory
times an output gate and
so if you actually look at the manner in
which these
computations are performed the
first you have ct minus 1 and ht minus 1
coming in
so we would first compute this forget
gate
then we'd compute the input gate so
these two guys
and then we're going to compute this
pattern detector these three guys can be
computed
these three are combined to update the
memory
over here then the updated memory is
used
to compute the output gate so this guy's
used here
and then the updated memory is
compressed and the output gate
and the whole thing is multiplied by the
output gate
to produce the updated hidden state
value
so this is the entire computation of the
long short term memory cell now these
equations look kind of
a bit complex but then you can go
through it step by step
and you can pretty much uh reconstruct
these things
on your own once you understand what's
going on
so questions professor yeah couple of
questions on chat
uh first is is the people connection
kind of like the skip players
it's not really this is just there's
there's not it's not a skip layer right
i mean remember there's no notion of
skipping this is just a memory so all
that happened was
it said it's a certain degree of
unnecessary computation if you think
about it
you have the memory going over here i'll
call this c this memory is transformed
through 10h and maybe you have some
output gate over here
which multiplies it so now you have two
variants of it
you have the c and you have the h h
is basically just a transformed version
of the
memory it just so happens that in
uh neural networks the more stuff you
throw
into it the more information you're
providing for these
for these pattern detectors and these
gates so you're sending both the h
and the c in the initial design of the
neural network actually didn't have this
direct connection because they thought
they were getting it directly from the h
anyway but then they found that when you
actually add this extra line
the thing operates better so this is we
call this a people connection
it's just adding more and more inputs to
these various pattern detectors to
improve their ability to
uh detect patterns and and and remember
things properly
did that answer your question
yes no yes thank you
there was a second question yeah yeah so
what was the intuition for coming up
with lstms like how
cnns came from studies on the visual
cortex of the brain how did the first
lstm come up
so i don't know where the authors came
from came to it from but the intuition
behind the lstm is basically just this
right
i mean this makes sense to you does it
or does it not well whoever asked this
question
yes or no yes yes right so
this is the structure that's about it
after that all of the rest of this stuff
is
just basically making it building an
increasingly complex machine
to perform the simple operation that we
had
where we said that the memory must only
be modified based on what's in the input
okay right so the sigmoid and the tiny
they're basically based off of empirical
results that this
you could simplify it so the basic idea
of the memory
is what you must retain in your head
these are all details
which would vary with uh you know
whoever had come up with it first
the basic idea of the memory itself is
kind of clean
the various combinations of you know
multiplicative gates additive gates
complete that is a
at that point it becomes a bit of
engineering
sure so next one is so the difference
between carousel and people is whether
using c
or not so carousel is the constant error
carousel is the line c that's it
okay so the kerosene is see this line is
called the constant error corrosion
the the peephole is just an extra
so in the original lstm you know i had
to
present everything to you know but in
the original lstm
they just had this basic idea where you
said the memory goes in
the memory comes out and whether you're
going to retain something in memory or
not should depend on a forgetting factor
and an addition that decides whether new
things are being discovered or old
things must be subtracted
this this idea is very clean right
whoever asked the question yeah thank
you okay at this point all that happens
is what are the path things that this
guy considers
obviously it must consider the input or
obviously it must consider the previous
hidden state
and why not do this as well right so
this was the p column
this is the constant error charisma and
same thing over here
you know i wanted to take the input i
want to get the h
these for some reason they don't
actually use the p pole over here but
then they have the sigmoid
this gate and this multiplies things out
here but then
these are complexification of a rather
very intuitive basic idea
okay one last question so in these
diagrams uh how does the picture change
with more layers because this diagram is
for
just one layer haha okay so
this is how did we do it for regular
rnns so for regular rnns we just had the
hidden state and the tan h right
oh why
go away so why does this do that
okay so for the for the regular
arguments this is what we had
you had an input and you had a
something going in and then something
going out correct
this was your regular rnn
absolutely nothing changes this is
all that what changed was what was
happening inside this block
earlier you just had a simple this went
in
this went in and you had a 10h and this
came out
now you have a lot more complex
operation
and as a result you have two things
being transferred the raw memory
and the hidden state but other than that
so you know
out here you'd have the same structure
you basically retained replace this
little tan h with a more complex
function
it should become much simpler to
understand when you look at the
pseudocode
right so i'm going to talk about these
little cells as my lstm cell
and again remember that when we program
your
even your tanh based recurrent network
all the affine values and such like that
you compute they are local to this
computation and you retain them locally
right
so it's in some sense uh
because every location has its own
affine values and there's information
that's
which only the pre-activation
terms which are only uh required here
when you're going backwards and then the
actual
computed values which keep which are
which are moved forward right
so there's a natural notion of an
object-orientedness
in these networks so i'm going to assume
in our object-oriented program
each lstm unit is supposed to be an lstm
cell there's a new copy of the lstm set
at each time
at each layer and each cell retains all
of the
local variables so let me write down the
lstm cell
a professor poll time okay
there's a pole fox
i think it's like again like select all
versus
we can only select one that's
a bug
right that's a bug that should have been
fixed okay
stop now only 80 of you have replied
you have five seconds
okay stop the polling yeah so again
uh for those of you who couldn't see the
poll this was the
hole out here on top and
this is true they stabilize the memory
by eliminating the problematic recurrent
weights and activations so you know
everything is identity
they update the memory based on patterns
detected in the input
and on the current context of what is
already being remembered
so in the absence of external cues
ideally they must remember a pattern
forever
and so these are suited to building
pattern analyzers that require
long-term memory code parsers for
example now again
this third bullet which most of you
answered no to is actually true right
in the sense that if there's if there
are no external cues
then ideally the memory just must
continue and continue and continue
so here is the pseudo code what comes in
remember here is the operation of that
cell
come on here's the operation of the cell
and each time t the pre
the input or the output of the previous
layer x t comes in
the hidden memory h t minus 1 comes in
the hidden the hidden state the hidden
memory ct comes in
c c c t minus one c t goes out
h t goes out and so i'm sorry sort of
drawing
giving you a look inside the same hidden
state
goes up to the next layer so this is the
uh structure of the network right so
c the memory the state
and the input and the parameters weights
and biases come in
and there are only two outputs because
as you can see although
there are three lines going out two of
them are identical right
and so let's look at what happens within
the computation it's very
straightforward
here is the computation within a cell
first you would compute the affine value
for the
uh for the forget gate then you compute
the forget gate
you compute the affine value for the
input gate and then the input gate
and now you compute the affine value for
the input pattern detector
and then you compute the input pattern
detector itself so if you look at the
computation
what this means is that you have the cc
going in
ct minus 1 ct the first thing you
compute
is the sigmoid so the sigmoid first has
a z
and then it has the actual gate value
the forget value and this is multiplied
then you compute the input gate
which which has its own z and the input
gate
then you then you compute the pattern
detector
tan h which has its own z and the output
these two are multiplied and this is
added right this was the computation
that we had
and so uh the affine value the forget
gate
the final value for the input gate the
input gate the affine value for the
pattern detector when you detect the
pattern
and then the memory is updated by
multiplying the previous memory by the
forget gate
that's this term here that's this guy
here
and then you multiply the new pattern
detected
by the input gate which is this term
here
and then you add these two up so which
is this last
addition so this is the
updated memory the updated memory is now
used to compute the affine value for the
output gate
you compute the output gate you compress
what's in memory and multiplied by the
output gate and now you get the new
hidden state
and you return the memory and the hidden
state this is the
cell with computation within the cell
and then when you actually
perform the full forward computation
here's what happens you're going through
time
at each time you can say that the hidden
state value for the layer 0 is just the
input
then you go through the layers so at
each time
you are sort of
so h 0 is x t right and then you go
through
the layers like so and at each layer
what happens is you obtain as
input now each of these is an lstm cell
so each layer is going to get
as input the output of the previous
layer the state from the previous layer
and the state from the previous time and
the memory from the previous time
it's going to output the ct the new
memory
and the new
state which comes out and goes forward
right
so that's basically all that's happening
in this little cell
and so when you go to the next time you
already so the input is the state from
the
from the previous layer at the current
time state from the same layer at the
previous time
the memory from the previous time you
use these to compute the new state and
the new memory
and just loop through the layers and
then finally for the output layer you
just sort of
take the output of the final layer
compute the affine value and then
finally compute the output
so uh did this answer the question of
how do you stack these things up
whoever asked this question guys
yeah okay very straightforward right now
training the lstm is very simple it's
just like
a regular rnn except uh you have to back
propagate things through an lstm cell so
let's see how hideous it gets this
cooperation operation makes it really
nasty okay suppose i want to get the
divergence with respect to ct
what are all the influences that compute
affected first there's a
there's the influence of this guy
changes to h t c t of x h t
so the derivative of h t must be
propagated backwards to c t
but then there's also this line that i
must account for
and then there's this direct line that i
must account for
and then there's this line that must
that i must account for
and then there's this this line that i
must account for and then there's
so and there's several more right i can
just keep adding things when you
actually look at
what are all the terms that the ct
influences and what are all the terms
that i must back propagate the
derivatives through
so also what about the derivative for
this hidden state
there's this thing coming from up there
there's stuff coming from here
this stuff coming from here and so on it
just keeps
and then there's this more complex thing
so
i can keep adding more and more paths
of influence that uh that i must
consider when computing these
derivatives so
this rather ugly structure makes it look
like
computing derivatives must be devilishly
hard right
and so i'm not going to explicitly
derive the derivatives with respect to
these terms even with respect to the
weights this is just the
you know memory and the state but then
you also have the weights
right i won't do it if you want to do it
on your own i'll leave it as an exercise
but then i'm going to go back and say
how many of you remember homework
one part one raise your hands
expect to see 120 hands raised
because it was pretty painful right
only 74. guys are you asleep at the
wheel
and wait for 100 hands to be raised
because it
guys do you remember homework one part
one
those of you who are not raising your
hands to raise it
30 out of 90 120 people are just not
even
awake that's kind of a shame okay so
uh i'm going to call you out on piazza
bars so
anyways can you tell me how that would
apply
to simplify this problem anyone
what we learned in homework one part one
the auto gradient will just do it for us
we don't have to do anything
yes but exactly what does autograph do
so here is what autograph does
now could take a look at the slides how
to compute a gradient derivative
there's a very nice slide deck that we
put up on the course page
and that explains it all but then to
explain it
here's what happens right here is the
so we're going to show what how the
derivatives are performed within
a cell and i'm going to assume that all
the static variables the
fine values that you had when you were
performing the forward computation are
still
still available so here is the forward
code right we had these are the affine
values for the various gates
the forget gate the input gate the
output gate
and then this was the affine value for
the pattern detector we had the forget
gate the input gate and output gate
values
this is the raw pattern and then
we computed all of these terms in this
particular order
so what we will do is compute our
derivatives starting from the back
going front so c h and x came
in c and h went out right
so if i were to write this again i can
say the lstm step cell had
ct and ht
came from were obtained from the lstm
cell of
minus 1 h
t minus 1 right and x t
and then there were parameters so when
i'm computing derivatives
what will the incoming derivatives be
and what will the outgoing derivatives
be
anyone
the incoming derivatives would be with
respect to c
uh t and h t sorry
yeah and then the output ones should be
with respect to ct minus one and hd
minus one
is that it um xt
yeah i think xt yeah xd should be
included too maybe
these cells are layered yes so there are
three inputs going in you will always
have
three derivatives coming out right yeah
right so these so the derivatives for
these guys were growing and the
derivatives for these guys will come out
okay and so what the derivatives for the
terms that come out at the bottom
are going to go in and the derivatives
for the terms that
that yeah
go in are going to come out so just for
reference
so that was a forward code we're going
to start
the way we will do it we'll start from
the bottom we'll come
we have derivatives over here will work
our way through the derivatives
backwards
through this code if you can write the
forward code the backward derivative is
a trivial
now consider this uh i had the or
let's say i have the derivative with
respect to
the output and the derivative with
respect to c
output that went out right now the last
computation that i performed over here
was that the output was
the uh
what was it the c
this was h o right so
the l that i said ho
equals uh the forget gate
times tan h of
co right this was the last computation
so if i'm given the derivative for h0
what are the data what are the terms for
which i can compute derivative for
over here anyone
for this guy
okay let me help you with this one right
so there's a term to the left hand side
yes
tell me so h0 is we said is equal to o
dot with tan h of h0 tan h of c 0 right
this is the raw memory
okay so if we're given dh
the derivative of h0 with respect to the
yeah the gradient of h0 we could compute
it with respect to
we could say that's equal to
we could say the gradient of o is equal
to the gradient of h0
times um
yeah tan h prime correct and then okay
so now what i'm going to say is the
derivative of tan
h of c 0
this is the whole variable is going to
be the derivative of h0
times o correct yeah and
of course i can fl i can that's easier
okay
and then i saw oh come on don't fly away
and then i can go ahead and say
therefore the derivative of c
0 equals okay i'm going to leave the
symbol over here
the derivative of tan h of c 0
which is this term okay times
1 minus tan h squared of c0
correct this i'm calling a variable so
let me call this
whatever sum alpha so this is the alpha
right times the derivative of this
function itself
did that make sense wouldn't that be uh
shouldn't the gradient of c zero be
equal to gradient of
h zero uh times one minus
uh so here's what happened so i'm
calling tan h of c
0 a variable at this point okay
that's already taken care of here
yeah okay okay and now i'm just
factoring the
derivative of the activation right right
so i have not we put in the put in the
the uh
the uh relation over here this relation
is going to be plus
equals y because we already got
a derivative for c0 coming in right
and then we're incrementing it by what's
happening within the cell
so to explain this we had
c0 this became
h0 and then you had c0 going up
so what came in was this derivative
and now what we did was we computed the
derivative through this line
which is what the second computation was
doing so when you go back here the two
must be added
that makes sense yes by the way i have a
quick question in there
so we we multiplied
i shouldn't say multiplied we set the
output of c0 through a tan h
function and then multiplied it with an
output gate to get h0 right
correct so uh like c0 was already
already uh went through like tan
hyperbolic function
why would we multiply it why would you
send it again through
a non-linear hyperbolic function so no
no hang on hang on so this was a tan h
you had an a0 okay and then you had a co
okay
yeah so modifying this guy is going to
change
quarter both of these so what we are
really doing is computing the derivative
here
computing the derivative here and adding
the two
the derivative here is the derivative
here times the derivative of this
activation function
yeah i got this this uh the gradient
calculations but
i was asking why we did uh why we sent
the c0 to a time
hyperbolic function in the beginning
like what was the
idea behind that so the raw memory is
raw it's not compressed
so the initial so the tan h was that was
to explicitly compute a
compressed version of the raw memory
it's a very nice question and the answer
you will find in a few more slides that
it kind of seems silly to be doing it
twice over right
oh okay okay okay right why why have
these two terms separately it doesn't
make sense
and so there are variants of the lstms
which actually don't have those as
separate terms
i'm just giving you the basic lstm yeah
thank you right
so here's what happened we took the
first term at 0 equals 0 times tan h
and i can write this uh the what i wrote
is probably kind of ugly
so we had h o
equals o times tan h
of c o so this is probably better
written cleanly as saying
uh you know some uh some let me call
this
t equals tan h of co
right and then i h equals o times
t so given the derivative of
h i can say the derivative for o equals
the derivative of
h times t the derivative of t equals the
derivative of
h times o and so now i've got the
derivative of t
so the derivative of c 0 plus equals
the derivative for t times
the derivative of the activation
function itself
so does this make sense
i just rewrote this term i just rewrote
this in this map
[Music]
these two are the same guys questions
a couple of questions on chat professor
not related to this though uh the first
one is are we limited to only sigmoid
functions for the forget gates or can we
use other activation functions
the sigmoid is basically the forget gate
has is a compressive gate
something that lies between zero and one
right so it's a multiplicative term so
we are sort of restricted to sigmoids
you know any and you want things to
happen between zero and otherwise it
will blow things up
uh the uh ones that actually have the
ability to add
increment or decrement those are the
additive
input lines the ones with the pluses
cool and the next one is uh we can
define forward backward for cell
uh it should be faster than using
autograd
autograd is doing exactly this by the
way
right so look at what's happening here i
just went backward through this equation
i just sort of
i had the d o d h o
so from that i computed the d o and the
d c
o right then i go backwards through the
equations so now i computed the
derivatives for the right-hand terms
from the derivative of the left-hand
term
now i'm going to go back up i'm going to
compute the derivative for
i already have d o so i'm going to
compute d0
then now i have dzo so i'm going to
compute the derivative for the weight
this and then co is appearing yet again
the derivative for this weight this h
this guy this guy and this guy so uh
i'm so i had the derivative for o i
computed the derivative for d0
now i have the derivative for d0 so i
can compute the derivative for the
weight
i can compute the derivative for uh
the uh this h i can compute the
derivative for this
x i can compute the derivative for this
bias uh sorry
the weight the weights and the bias but
then i can also compute the derivative
for the c
over here and because you know this this
this is the same as the output
r but this is going to be incremented i
can compute
basically a compute i'm getting myself
tangled up but i can use this term to
compute derivatives for every one of
these terms because it's very
straightforward
now i have those derivatives so now i
can go down to the next line
which is the next line above which is
this guy now i have the derivative with
respect to this term
so now i can compute the derivatives
with respect to each of these four terms
below it
right very simple the derivative for c
is
the derivative for c 0 times f the
derivative for f
is the derivative time of c 0 times c
it's all very mechanical you don't even
have to think about it
and then i go back one line above in the
code
i have the derivative for the right hand
term you know this guy
i have the derivative for the left-hand
term i compute the derivative of the
right-hand term
then i go back to the previous line i
can keep repeating this process
till at the end finally i'm going to get
the derivatives for the
terms that went into the lstm namely the
c
the h and the x and in the process i
have also computed the derivatives for
the weights and the biases
so go back and take a look at this code
kind of carefully but you get the whole
idea of how the whole thing works
we just sort of worked our way backwards
and in fact
this is exactly what your autograph does
uh it's even more explicit than this if
you look at these slides which say how
to compute a derivative
take a look at it very carefully and
that explains
how you can actually simplify this
forward computation when you write it
down so computing the derivatives going
backwards is trivialized
and so the outcome of all of this was
that
the uh what looked like a very
complicated
piece of arithmetic became a fairly
mechanical
procedure for computing derivatives
going backwards
now that was for within a single cell
right
but then you have to do this for the
whole
network so now we're going to go back
this is the forward now the
computation you went from the beginning
to the end and through the layers
in the backward computation you're going
to go from the end to the beginning
for the output layer the computation of
derivatives is just like a standard
rnn or an mlp but then when you go
back through the layers now we are going
to use the derivative computation for
the
lstm cells that we just saw
and so what happens here is that
you have an lstm cell and from here
you're going to get
bc you're going to get the h
but you're also going to get the the x
coming in from the top
now we'll remember that what the lstm
cell
actually computed going forward was only
two terms it computed the c
it computed this h but then the h went
two ways it went to the right and went
to the top
so you know so back when you're going
backwards these two derivatives
must there are only two outputs so
these two guys get added to have two
derivatives going in
right here and so
this is the derivative going backward
you get the derivative for c from the
right
you get the derivative of h from the
right and from
x from the top and these two are summed
so you have these two derivative terms
going in plus all the other variables
and what it's going to return is the
derivative for
each of these three terms
that make sense to you guys
even if i didn't take a look at the math
it's it's tedious but it's not very
complete complicated
uh professor poltay okay
there's a pole guys
um
okay two seconds
all right stop it uh both of these
statements are true right
uh as you can see i think it's
so we compute derivatives from the right
hand side terms
for the right hand side terms from the
derivatives of the left-hand side terms
this is true for any equation
and the derivative computation
computation proceeds
from the end so this for those who are
we couldn't see it this was the
this was the call right and the
derivative computation proceeds from the
last equation in the forward routine
backwards to the first equation
so that was the gist of all of that very
complicated gobbledygook that i just
gave you
now there's the concept of the lstm that
the memory must go through
unmolested but
there's the implementation of it which
is you know there's so many little
things going on
does it really have to be that complex
not really and so there have been
various attempts at trying to simplify
the whole structure just to make the
thing a little more same
and one of these structures is the gated
recurrent unit which is actually quite
popular
it simplifies some of the issues uh
that that eliminates some of the
unnecessary
complexity first there's no need for a
separate
uh there's no there's no need for a
separate input gate
and a forget gate if a new input sb
is worth remembering then the old memory
is probably got to be forgotten
so they simplify this by saying that the
forget gate is 1 minus
the input gate then
as someone pointed out there's no need
to maintain a separate
memory and a hidden state why so just
have one line going out which is which
is both memory
and the hidden state the only thing that
you do is that
when you actually operate on this hidden
state within the next neuron
uh then you compress it using a tanh and
within the next neuron and then within
the next neuron you can
you will have an output gate which sort
of compresses
the memory that goes out so uh
there's still a certain degree of
complexity in the in the operation but
then you sort of
got rid of this whole anomaly of having
two outputs from this app
and so this is just a
variant on their stm you can come up
with other variants
which try to simplify the structure
so now once you define your lstm cells
and your gres you can use them in any rn
structure like this one
the only difference now is that each box
now
is an entire lstm cell or the
gru cell and if it's an lstm these
horizontal arrows are actually two
arrows one for the raw memory and one
for the state
if it's a gre it's just one one atom
and you can even have them in
bi-directional structures
like so and keeping in mind once again
that
these cells are now either lstm cells or
gre cells or their
equivalence and so here's the story so
far
we've seen that recurrent networks are
poor at memorization because
memory can explode or vanish depending
on the weights and activation
they can suffer from the vanishing
gradient problem during training
so error at any time cannot affect uh
parameter updates in the too distant
past and then so so you know
if you've forgotten about the open brace
then there's nothing
there's there's nothing important about
seeing a closed brace this kind of
uh issue happens with conventional
recurrent net neural networks so lstms
and their gate basically gated recurrent
networks which use this constant error
cruisal structure
with no weights or activations
they try to address this problem and so
they do not have
vanishing gradient problems they don't
have
you know memory just being forgotten in
the absence of input and so on
if designed properly and designed
properly is an issue that's going to
arise in this week's quiz
they do suffer from a different problem
which is that of
exploding gradients because regardless
of all of this
because things can get added and because
things get compressed going forward by
the
by the various gates when they are going
backwards they're going to get expanded
and so you can have exploding gradient
issues and so
it's not it doesn't solve all the
problems but sort of gets
halfway there and
so we've sort of defined the recurrent
neural network structure
but there's still still some significant
issues like the divergence
uh how do you compute these divergence
and how do you use these networks
so this uh still remains
and that is in the next set of slides
that i'm going to continue
continue at this point with
but any questions so far before i
jump to the next set
nothing on the chat nothing on the chat
okay
so let me see if
i've got to
oh no let's begin too big
let me see it in slideshow
okay so i'm going to continue with this
next
topic which is
and wait for a few seconds
for any more questions
uh professor quick question do you want
me to pause or stop the recording right
now or should i
know sir they just continue let this let
this recording continue right
so i'm going to start now and a lot of
these
topics are things that we are already
familiar with
let's continue at this point right so in
the key issue we've defined the
recurrent network we've
addressed the issue of stability we've
addressed the issue of
memory fading gradients vanishing and
things like that
but then there's a very important issue
in all of this the output the divergence
that we defined
was between the sequence of outputs of
the network
and the signal and the desired uh
sequence of outputs and how you define
this divergence
is going to really really influence what
the network learns and how we how we use
the network so
what we're going to look at in this
lecture and the next
is uh we're going to discuss network
architectures how to train recurrent
networks of different architectures
we'll discuss the issue of synchrony
between the network output and inputs
and uh how to train networks when the
target output is time synchronous with
the input
when the target output is order
synchronous but not time synchronous
and how to make predictions and
inference with social networks
now we've seen earlier there are many
variants to recurrent neural networks
so i've taken again i've taken these
images from android karpathy so
pink boxes represent inputs the green
boxes represent hidden layers
the blue blue boxes represent the output
layer
here each input is individually
processed by the
hidden layer and then finally you have
an output there's no recurrence so this
is just a conventional mmp
so this can be used to deal with time
series
but then you process each input and the
series independently
this one shows a recurrent network with
a time synchronous
output what this means is that the
network
processes the sequences of sequence of
inputs
and then produces one output for every
input
in synchrony with the input so this is
the kind of thing you'd need if you were
performing let's say
a stock market prediction every day you
see the input and make a prediction or
if you're doing part of speech tagging
you see a sequence of words and then you
assign a tag to every word
and then the third variant is full
sequence classification you see an
entire sequence of inputs then you make
a prediction like in
speech recognition or if you're looking
at text and doing sentiment analysis
this is the kind of network you'd have
and this
fourth variant is the key one this one
again this is all asynchronous
but not time synchronous and so what is
happening over here
i have a sequence of inputs i have a
sequence of outputs but i don't have one
output for every input
and rather i have an output the outputs
occur
intermittently the order of the outputs
is important
you can't change the order of the
outputs but
what you do have is when the is that
they don't necessarily you don't
necessarily have an output for every
input for example if you were doing
speech recognition
the input is going to be a sequence of
speech vectors
the output is going to be the sequence
of words that was recognized
and words you don't get 100 words a
second you get maybe four words a second
and so whereas your input is going to
come in at 100 words per second 100
feature vectors derived from speech and
that sequence of word feature vectors
relates
uh relates in terms of order to this
to the sequence of words so you can't
change this order of words
this portion of this word speech this
portion of the input
definitely corresponds to this word this
portion of the input would definitely
correspond to this word so there is
order synchrony between the input and
the output but there is no
time synchrony so did this distinction
make sense to you guys raise your hand
the difference between time synchrony
and order synchrony
yes makes sense yeah so just wait for 50
or 60 hands to be raised
i'll continue from there
wait how can i ask a quick question yes
okay so like
in code this would look like for all the
inputs
on the red thing you loop through them
you get for each one
like the output uh memory as well as the
hidden state
our output hidden state and memory and
then only
that would be for lstms let's not let's
not let's at this point i'm going to
i'm just going to treat these as blocks
okay it doesn't really matter right
whether it's an lstm or a gi or a
standard rnn is kind of not
immediately relevant uh this is just a
hidden state however it's computed
anyway continue oh yeah so like
basically like
how are you actually picking which
outputs to aha that was the bomb
point of this and the next lecture right
it makes con
and you're dealing with this already in
homework too right
so but continue right here are more
variants
this is what i call a posteriori
sequence to sequence conversion you have
to read the entire input and then
produce the entire output
so this is what you need if you are
doing machine translation
or you have the one to many you read a
single input and produce an entire
sequence of outputs so this is what you
would do for example if you were doing
something like uh image
captioning you get an input and then you
produce a sequence of what
right so these are all different
variants of networks
and let's look at this first
uh you know what it's 9 17
and i have to go through a whole bunch
of slides so
i will stop right here pause the
recording
uh pause or stop pause right yeah let me
cut
so let's start off by looking at this
simple model first right this is just a
regular mlp
i can use it to analyze sequences and
when i analyze sequences we just
we just sort of apply it repeatedly to
each input in the sequence and you're
going to get an output for it so
there are as many outputs as inputs and
so the
analysis is time synchronous now there's
no real recurrence in that
the hidden states are not being passed
forward to the next time
and so the output at any time is not
influenced by the outputs at the
previous
times and if you were going backward the
derivative at any time is not influenced
by what happened later
so because we have uh
one output per input
during training we will also have one
desired output per
input because you know you're treating
each input
independently and so the divergence is
going to be
so the output sequence is going to be
exactly as long as the input sequence
nevertheless you can still use this to
analyze time series
by actually computing a divergence
between two sequences
rather than just treating this as a
bunch of
individual divergences now to train the
network
so now we will need the divergence
between the output target sequence
and the input target output sequence and
the input sequence
and in this kind of situation a common
assumption is to say that this is just
some kind of a weighted
uh sum of the divergences of the
individual instances
where the weights are typically set to
one and now of course
uh we can just uh uh so in which case it
just reduces to a standard mlp
saying that you know i'm treating
although these are all sequences i'm
treating each observation and it's
corresponding target output as an
individual observation
uh each each each input and the sequence
and its corresponding target output as
individual
individual observations and then you can
just update your mlp
so uh now and of course
if you're doing classification the most
commonly used local divergences over
here
is the callback library dimensions
now let's consider this second variant
there is a poll here for those of you
who are going to take it
that's
these two questions
all right
stop in five
right
all right guys now so conventional mlps
can be used to model sequences yeah
as we just saw the way you connect up
these
uh you know mlps with the sequence is
through the divergence which actually is
between
uh sequences so now let's consider the
second variant
and while we're discussing it we'll also
sort of briefly detour into
language modeling but so
this is a time synchronous network the
network produces one
output for each input so again
there's a one-to-one correspondence
between the input
and the output output so this is a
genuine recurrent network which means
that the output at any time
is also related to what is computed at
other times so the fact that
i am calling this yellow and adjective
uh is uh dependent on everything else
that happened in the sentence the fact
that i'm calling the word
a noun depends on everything else in the
sentence
so this again
if your if your
network is unidirectional in this case
you're going to be processing the input
from the beginning of time to the end of
time
you can have bi-directional networks
where you have one component processing
it from the beginning of time to the end
of time
the other processing it from the end of
time to the beginning of time
it doesn't really matter what really
matters is that
for every input eventually there's one
corresponding output so there's a
one-to-one correspondence
between the inputs and the outputs and
for the purpose of explanation i'm going
to assume it's unidirectional but this
generalizes
so now when you train the network we
know exactly how to train the network
you're going to use
variance of gradient descent we'll use
bptt so we start off by defining a
a training set which could where each
instance in the training set is
an input sequence and a desired output
sequence
and so in the forward pass we just pass
the entire
input the data through the network
you're going to get one output at each
time
in the backward pass we're going to
uh compute divergence gradients by back
propagation
from the end to the beginning this is bp
back propagation through time
i mean this is stuff we've seen so i'm
not spending time
too much time on it as a first step we
compute the divergence between the
sequence of outputs and the sequence of
desired outputs
and uh then back propagation actually
uh now uses
the derivatives of this divergence to
the to train the network and again
this divergence need not be the sum of
local divergences for each of these guys
remember that
right and now
once we have this divergence we're going
to assume that
we can compute the derivative of the
divergence with respect to
the outputs at each time even though the
divergence is computed between the
sequence of desired outputs and the
sequence of
actual outputs it's still a function of
every one of these guys
and so you're going to assume that you
can compute the derivative of the
divergence
with respect to each one of these
outputs
and so this is the key
in order for us to be able to perform
back propagation
we really need to be able to compute
this
derivative for the divergence at
each time so otherwise you can't really
pass things
backward through the network and so that
ability to compute the derivative of the
divergence
sorry with respect to the outputs at
each time
that becomes our challenge
now uh although
so so the key is this one we need to be
able to compute the derivative of the
divergence with respect to y t
for every single time t now
although the divergence between is
between two sequences
at this point we will try to simplify
and you'll see why this simplification
makes sense
in the next class so one common
assumption we'll make over here
is to simply assume that the divergence
between these two sequences
is simply the sum of the divergences
for individual outputs
and why can we make this assumption
because this is time synchronous
for every time there is an output for
every time there is also a desired
output
and so i can just say that i'm just
going to compare the desired output at
this time with the actual output at this
time and computer divergence
i can do that at each time and then i
can sum up all of these divergences
and i end up with this sum divergence
which is the divergence between the two
sequences now this is not strictly going
to be true always
but in the case of the time synchronous
model this kind of assumption can be
made and is often valid so this is
basically what we're going to assume
now you might want to make modifications
to the model where you don't make this
assumption and a lot of the math will
change
but then any i mean some of the math
will change you'll see one
so here's what we have right
and now because the divergence between
these two sequences
is the sum of the divergences at the
individual times
if i want to compute the derivative of
this of this divergence
with respect to any given y t
i just need to compute the derivative of
the local divergence at that time
with respect to that yt because the rest
of them
don't don't actually this guy doesn't
actually influence
the local divergences at other times
now so and of course
for classification problems the typical
divergence that we will use
the local divergence at each time is
going to be the callback library
divergences at all of the steps
for example yeah there's one last poll
okay stop this all three are true
right
for time synchronous rnns there's a
one-to-one correspondence
uh the first and the third are the true
the second is not true
by the way i'm sorry there's a
one-to-one correspondence between every
input and output
and the divergence between the true and
desired outputs
can have an additive can just be added
up
but the computation itself can be
bi-directional as we saw you can have
both forward recursion and backward
recursion
so continuing on let's look at a simple
example of time synchronous modeling
time synchronous networks say trying to
model text
so i give you uh a sequence of
characters like l-i-n-c-o-l
and your job is to predict the next
character over here
or i can say i can give you a sequence
of words to be or not to
and your job is to predict the next word
so in both cases the problem boils down
to this
you are given a sequence of and this
this can be continuously done
so i can be you're given this w0 you
must predict w1
then you're given w0 and w1 you must
predict w2
then you're given w0 w1 and w2 you must
predict w3
and so on where w is either a character
or a word
so how do you actually do this here's an
illustration from andre karpathi so
which i've
borrowed for my slides now this is for
the word hello
here the input is a sequence of
characters h-e-l-l
and the network has to predict the next
character so after reading h
it must predict e after reading e
must predict l and so on but we have to
take a few things into consideration
first
words and characters are symbols whereas
the neural network requires
numbers as input so what we will do is
we will represent these inputs
as one heart vectors
and actually we are going to represent
them as embedding vectors we'll see how
but you start off with these
one hot vectors the actual
output is not going to be a symbol the
actual output is going to be a
probability distribution or over all of
the symbols here
so which is the output of our of a soft
max here
this this figure itself is showing the
logics but the actual output is going to
be a probability distribution
and so what we want to do is uh when
we've given
h and e as input which is to say they've
given the one hot
representations of h and e as input we
want the output probability distribution
at this time
to assign the greatest probability to
the character
l this is how we want to train the
network we want to
if you train this network given the one
hot representations for a sequence of
inputs it must produce an
output that assigns the highest
probability to the next symbol in the
sequence
and so to train the network
the input sequence of symbols goes in
and at each time the target output is
going to be the next symbol on the
sequence so at time 0
the target output is w1 time 1 the
target output is w2 and so on
and again remember what the network is
outputting at each time is a probability
distribution over
all of the symbols given all the
previous symbols so
at time t it's computing the probability
of
all symbols given all the inputs from
the 0 times 0
to time t minus one and
so if i'm computing a local divergence
at each t
the local divergence is going to be the
desire the divergence between the
desired
output at time t which is simply going
to be
the symbol at time t plus 1 and
the probability distribution at time t
and we know from this that this is
simply going to be the scale divergence
is simply
minus of the log of the probability
assigned to the next symbol
at this time so here the divergence is
going to be
the minus of the log of the probability
of w1
computed here minus of log of the
probability given
assigned to computed for w2 over here
minus
log of the probability of computed for
w3 over here and so on
and so that's going to be your sequence
of that's going to get total divergence
and this is what you'd minimize to train
the network now again this is a detour
we're really talking about modeling
language so
this is the slide detour interlap
modeling language using time sync
synchronous networks
and more generally language models and
embeddings the reason we're going to
spend some time on this is that it sort
of invokes a few concepts
that we will use in the rest of the
lectures so it's worth knowing about
now consider this general problem
for score and seven seven years what is
the next word
you want to be able to predict that the
next word is a go or if i gave you a b
r a h a m l i n c o l
what is the next character you want to
be able to predict that the next
character
is n and we want this guessing to be
performed by a regular neural network
so first off again we are going to
represent
all the symbols words if you are
representing words characters if you are
representing characters
as one hot vector so for example if
you're doing words
you'd have a predefined vocabulary of n
words in some fixed lexical order
and every word is going to become a one
hot vector according to these
lexically ordered words similarly
characters
if you want to represent english english
has about 100 characters including both
cases
and all the all the syntax hyphens
apostrophes and whatnot
so your one hot representation for the
characters would be 100 dimensional
vectors
and now if you want to predict words so
the task in language modeling is you
want there's that of prediction you're
given a sequence of words you want to
predict the next word
so this is the actual problem you'd have
you're given words 0 through n minus 1
you want to predict the nth word and
ideally
0 through n minus one are presented as
one hot vectors
you're going to output the one heart
vector for the nth word this is what you
really want to do
so if i want to write it out as a
in you know as a visualize it
i have words the 1 hot vectors for 0
through n
minus 1 going in there is some function
which computes the one hot vector for
the nth word
and what we want to do in language
modeling is to
learn this function now
the problem is all of these words
these one are represented by extremely
high dimensional
one-hot vectors for example if you have
a hundred thousand word vocabulary
which is what you need for even decent
english
then every one of these vectors is a 100
000 dimensions
if you want to do russian you need 400
000 words
so every one of these vectors is 400 000
vectors
a dimension so each of these vectors is
going to be
400 dimensional 400 k dimensional so the
number of parameters over here is going
to become
immensely large and this high
dimensionality occurs because we use
one hot representations now one art
representations are
extremely wasteful uh in an
n-dimensional one-hot representation of
words
you have this huge n-dimensional space
and of that huge n-dimensional space
you're using
only n points so think about this
three-dimensional case
suppose i have a vocabulary of only
three words
then there are only three valid vectors
in that space
one zero is zero which is one word zero
one zero which is the other word
and zero zero one which is the third
word but i'm using the entire space
to define this uh this
this uh just these three little words
and it's wasteful of space if i think of
it in terms of
you know the density of usage of the
space then
if each of these edges has some length n
a length
r the density of points is just n
over r raised to n so as the vocabulary
increases
the density falls exponentially
and then you have this third problem
which is that
uh you know i have this one dimension
one heart representation and this
one zero zero is a word but if i have a
point
just slightly shifted from here say
1 epsilon 0 this
point doesn't mean anything right
only these guys mean anything the rest
of them don't mean anything
so given this why would we be using
one heart representations does anybody
want to take a guess
anyone
it separates out the words so that
they're completely different from each
other
and orthogonal
kind of this there's actually a
suppose i didn't use one heart
representations right
then i'm going to have they're still
going to be vectors
so i'm let's suppose i have a
representation of this kind
but these are my words is there
any implicit implications from this
representation
yeah one is uh some might be more
similar than others depending on their
distance from each other exactly
when you use something that's not one
heart and if you decide these
a priori by yourself you are sort of
enforcing some notion of similarity that
might not hold
correct in the case of
in the case of a one heart
representation what's the difference
distance between any two words
always the same it's always square root
of
32 yeah right and the length of any
vector
is always one all right so you're not
making any a priori judgments
about the relative importance of words
and
or any judgment about the relationships
between words
so but then
this leaves us with the dimensionality
problem that's too hard
right so to solve this dimensionality
problem what we will do is we will
learn a subspace a plane
within this high dimensional space and
project
all the data points onto this plane
and when you project the data points
onto this plane
now each point becomes one point each
backward becomes a point on this plane
but the plane itself is being learned
and if you learn it properly now one of
the effects of
projecting things onto a plane means
that now the distance relationship
between words
changes some words will be closer some
words will be farther
but provided you learn this projection
matrix p
which multiplies the one hot
representation properly
these distances will actually be
meaningful
and one of the advantages of projecting
it down to a subspace is that now the
density of points
becomes much greater so if you project
it down from an n-dimensional space to
an
m-dimensional plane the density went
from you know 1 over
r raised to n to 1 over r raised to
m and
this basically can huge so
this can have orders of magnitude
improvement in the density
so if you had for example a hundred
thousand dimensional vocabulary
and then you projected it down to a
thousand dimensional plane
you got a factor of thousand
actually now you've got a factor of r
raised to 1000 improvement in
in the density it's huge so projecting
things down can really make the
usage of the space much more uh
meaningful and of course if properly
learned the distances between the
projected points
will learn semantic relationships
between words
and so that's what we'll do we're going
to go back to this problem
but i'm going to have this projection
matrix and each one of these one heart
representations
is going to be operated on by this
projection matrix so now this function
is going to operate it on operate on the
projected
down one hot words the output
is still one hot the output is not going
to be a projection you want to predict
the next word
but in order to simplify the job of this
function you're going to project these
words down to these
this lower dimensional subspace using
the matrix p
but then projection of course is just a
matrix multiply
and we know that a matrix multiplies
just a linear layer in a neural network
so if this were a neural network all of
these are also going to be just one
linear layer this whole thing is a
neural network
except that every word is locally put in
through the same
identical linear layer so this is a
shared parameter network this is a large
network
where each one of these guys is being
transformed locally
by the same subnet and then all of these
go in
through the larger network to predict
the next word
and so once you set it up like so then
back
you can use standard backdrop rules to
learn this shared parameter network
does this make sense to everybody
guys yes don't raise your hands if it
makes sense
so this this is what they call an
embedding layer
that is the embedding okay okay
make sense right now here's something
interesting that comes out of it
what you're really doing is performing a
matrix multiply right
but then suppose i have a matrix the
matrix is going to be a bunch of columns
and if i multiplied by a one hot vector
what is the outcome
the outcome is going to be i'm just
selecting
the one column corresponding to the
entry that is one
right
that makes sense yes okay so in other
words
you basically the columns of this matrix
themselves become your embedding vectors
it's actually just a matrix multiply but
the columns of them
matrix end up end up being your
comparing vectors and so
if i want to uh
perform language modeling then the
simplest model would be something like a
time delay neural network
uh which is not recurrent which is going
to predict each word based on the past n
words this was the first paper on
language modeling of this kind used by
benjio at
all so say i'm always looking at the
past
four words to predict the next word then
to predict
w5 i'll be looking at w1 w2 w3 and w4
but before operating on these words each
one of these words is going to be put
through this projection
matrix and then the rec and then there
and this
portion this portion of the network is
actually going to be
operating on the embeddings
now if you're training this model in the
process of
learning to predict the next word you
also automatically end up learning this
low dimensional this projection matrix
and therefore the low dimensional
representation of the words
now uh the load of it now
sometimes in many problems just this
business of
learning the low dimensional
representation becomes an
end in itself because the low
dimensional representation can end up
capturing a lot of
language structure so so
[Music]
people have suggested various alternate
models to learn predictions which are
neither recurrent
nor convolutional for example
this model here this is the soft bag of
words model
where you use the previous k words and
the subsequent k
words in any sequence to produce the
central word
and this is just a record this is just a
neural network where every word is put
through the projection layer
you minimize the kl divergence between
the prediction and the actual word
you end up learning these projection
matrices or here is the skip gram
where you use a word and you and you
predict all the previous k words and the
subsequent k words
from that one word so this is kind of a
mirror image of this guy
and that too will learn a projection
matrix and these days of course you have
far more complex
models that that you're probably going
to encounter in your
projects all of which are kind of
focused on learning these low
low dimensional representation of
words and why is it kind of useful to
learn these lower dimensional
representations
it turns out that the lower dimensional
representation can end up capturing a
lot of language structure
so here's this famous picture from
mikolov's 2013 paper
which got a lot of the recent work in
the area roaming
these are two dimensional pca
visualizations of word embeddings and
so this is the embedding for china this
is the embedding for
beijing and you know beijing is the
capital of china this is the embedding
for russia
and moscow japan and tokyo and what do
we see
the vector that connects china to
beijing is basically the same as the
vector that connects
russia to moscow in japan to tokyo
and poland to worship and so on so
basically
these vectors between words end up
capturing
semantic relationships they have these
other beautiful
examples where they show that uh king
minus
queen ends up being the same as man
minus woman
and so on so in the process of learning
these low dimensional
embeddings they actually end up learning
the representations between words
and the nice thing is that you can use
one task to learn these embeddings and
then use these embeddings in a
completely different task
they seem to transfer and so i'm just
going to spend a final five minutes
on going back to the thing that we began
with
modeling language so now
if i want to train a recurrent recurrent
network to predict the next word using a
time synchronous model
you wouldn't directly use the one hot
vector inputs
usually you'd use their embeddings and
then
you would you would actually make
predictions using these
embeddings the predictions of the next
word using these embeds now
simple rnns are really not great for
this task so typically you'd use one or
more layers of lstm units so these green
boxes
would be layers of lstm units
and you can train this network using
lots of text
and unlike other classification problems
you don't need
explicit labels because you already have
the text
the target label at any time is simply
the next word in the sequence
so just getting text is sufficient to
train the network and you can use bbtt
and now you can even use the model to
synthesize text
so let's say i've learned this model
where i gave it lots and lots of text
and for every at every time
the target output was the next word in
the text so i have the information
required
to train the network and once i've
trained the network
i can use it to generate text so
i can just give it say the first few
words in a sequence
and of course is one heart vectors which
might be converted to embeddings
and then when the last word goes in the
network will
output a probability distribution for
the next word
i can randomly draw the next word from
this probability
distribution and output it and now i can
out input that gen though that word that
was drawn from the distribution
as the next word in the sequence
this is going to be one hot word right
this is a word i'm going to present and
present it as a one hot vector over here
and get its embedding
and that is presented as the next word
in the sequence
and then the network will produce a
probability distribution for the next
word in the sequence
and from that i can draw the next word i
can feed that back in and generate the
next word and so on
until i've generated an entire sequence
of words
and i can continue this until you come
until you encounter some criterion that
you have chosen to terminate generation
like uh you know generate
n words or keep generating words till
you
encounter an end of sentence marker or
if you're generating programs keep
generating symbols till the final brace
is closed
and so if you go back to the uh the
original example i showed in class where
i started off by saying you know who
wrote this code
this was generated by a recurrent
network this was just
a recurrent network trained with lstms
trained on linux source code it was a
character level pro model
and then they uh
input the first sequence of characters
to the model and let it generate the
subsequent characters and you see that
this is what it's generated it's done an
amazing job
of generating a very cogent
piece of code or here's something that's
even more interesting i'll see if i can
play this for you guys
this is the last bit so i might as well
ah how can i love this
i think share computer service
let me try playing this so this one was
a neural network that was trained on a
bunch of midi files
and then they use the neural network
to generate more midi and convert it to
converted it to music and here's what it
sounds like
right let's can i play it
[Music]
and as you can see it's a really good
job of generating very coaching music
right
so
[Music]
anyway uh thanks for staying with me
guys i'll stop here that was the
that was the uh
termination of today's lecture we'll
pick up from the next topic in the next
class
but bharathi can stop uh can stop the
recording