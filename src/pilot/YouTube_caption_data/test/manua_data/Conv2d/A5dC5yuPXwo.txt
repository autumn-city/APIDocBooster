 All right, let me now show you how we can implement this all
 convolutional network that I discussed in the previous
 video. So here I have a code notebook, I will of course,
 shared as usual. And also, as usual, I want to run this
 because it took like 45 minutes to run this whole notebook. And
 yeah, there was actually an interesting question on Piazza
 regarding the runtime of the homework. So a student was
 asking whether it's normal that it takes like eight minutes to
 run the notebook. Yeah, that is totally normal in deep learning.
 It's actually pretty fast eaten. So yeah, deep learning is of
 course, a little bit different than traditional statistics
 where we usually work with small data sets like little tables of
 data. So here we have really large data sets and complicated
 models with many, many, many parameters. So yeah, it takes
 usually a long time. So some people actually use hundreds or
 1000s of GPUs for multiple weeks to train these cutting edge
 vision and language models. So you can actually get good
 performance also with efficient models that only run a few
 hours. But honestly, if you have a real world data set, sometimes
 you have to be patient, sometimes it takes a few days.
 Also, so this model here is relatively simple. We have the
 cipher 10 data set. So luckily, it only takes 45 minutes. But
 yeah, that is still a long time. And we probably want don't want
 to wait 45 minutes here during the video until it finishes
 training. So I'm not going to rerun this. But yeah, I will
 share the results. So everything here is exactly the same as in
 the VGG 16 notebooks. So I don't have to recap all of that. The
 only new part is really here this code of the all convolution
 network can make this maybe a little bigger. Um, yeah, I'm
 seeing I was writing this very verbose. So you actually, if you
 have the same height and width, just a small comment here, if
 you have the same height and width for kernel size and
 stride, you can also replace this by just three and this just
 by one, for example. So here, the first value always stands
 for the height. And this is for the width. And this is also for
 height and for the width. So yeah, because it's all about
 simplifying convolution networks, here, you will only
 see convolution layers and relu layers and the batch norm
 layer, no max pooling and no fully connected layer. So you
 can further of course, simplify it by not using batch norm. I
 think batch norm was not even around and not even invented or
 around when this architecture was proposed. But I didn't get
 really good results without batch norm. So I just added it.
 And the rest is just like the paper that I showed you this
 striving for simplicity paper. But yeah, except that I added a
 batch norm. Um, yeah, and all I can tell you here is that we
 have convolutions. And here, we have always like one convolution
 that increases the number of channels, but it keeps the input
 and the output size the same. So this is like the same
 convolution that we talked about, I use the padding of one
 to achieve that. So the input is 70 pixels, the output will also
 be 70 pixels. And then so here, there's always a conversion that
 increases the channels. Because each channel can be thought of a
 feature map from a different kernel or feature extractor.
 That is how the network learns to extract different features.
 And then there's always a convolution that keeps the same
 number of channels. And this one has the stride of two. And this
 is these are the equivalence of the max pooling with kernel size
 of two by two and stride of two. So this is here, this is the one
 for reducing the height and width. And then we have again,
 one that increases the channels with a stride of one, so it
 keeps the input and output height. And then we have again,
 one that here with a stride of two, just reduces the height and
 width by a factor of two. And we keep doing that a couple of
 times. And then in the end, we have 10 classes, right? So in
 cipher 10, we have 10 classes. So in my last convolution layer,
 I have now the number of output channels equal to the number of
 classes. And in pytorch, there is for some reason, no global
 average pooling, global average pooling is essentially just
 computing the mean over the channels. So also for each
 height and width for each channel, you would compute the
 mean, you could technically implement that very, yeah, in a
 very simple way. So that's probably why they didn't have a
 global average pooling layer in pytorch. But you can also,
 instead of just implementing averaging layer, you can also
 use this adaptive average pooling 2d with an input of one
 that has the exact same effect as this global average pooling.
 So adaptive average pooling is a layer that is quite versatile.
 So what it can do is, it will produce the size that you
 desire. So if you put a two here, it will produce two by two
 outputs. If you put a one here, it will produce a one by one
 output. So if I go back to my lecture here, so this global
 average pooling will be the same as this adaptive average
 pooling. So it will take the whole feature map, produce the
 size of one here. So that is what we are doing now. So we are
 reducing this one to a size of one, I actually don't even know.
 I probably should have prepared this. But I don't know what the
 size before this is, it's probably I don't know, maybe eight
 by eight or something like that. You can actually double check
 that by removing this part. And then just doing a print x size,
 and then you can find it out if you don't want to do the math.
 But I'm not doing that now, because then it will crash the
 output here. So that is how you can find out at home, you can
 wake and just make a copy. To be honest, let's do that. Why not?
 Let's do that. Why not? All right. So it's eight by eight,
 like I thought. So it was actually a pretty good guess.
 All right, so, um, back to the code here. So, um, yeah, so we
 have now this global average pooling or here called adaptive
 average pooling instead of the fully connected layer that we
 had in VGG 16. And then I'm training the network using very
 simple setup, the same as for VGG 16. It's training. It's a
 very simple network. So it doesn't really train that long.
 It doesn't have so many parameters. I mean, not as long
 as the VGG 16. So the VGG 16, like, double check here, it was
 probably long time, 90 minutes. So this only took half the time
 then 41 minutes, it doesn't get the same good performance, it
 has 80%. I think this one has 84 85%. So it's not as good, but
 twice as efficient. Alright, so yeah, this is how all conversion
 network works without max pooling and without fully
 connected layer. In the next video, I will show you how we
 can alternatively replace fully connected layers by convolutional
 layer. So here, we learned how to replace it by average
 pooling, but average pooling doesn't have any parameters. So
 it's replacing the fully connected layer, but it's not
 equivalent. It's of course different because now we replace
 it by a version without parameters. The next video, I
 will show you how we could technically if we wanted to
 replace the fully connected layer.