in this lecture finally we are going to
do spiders to implement linear
regression model in our previous lecture
we talked about how we're going to build
a computational graph using Python
variable the idea was pretty simple we
just create a variable using pilotage
variable and then after that we can just
compute we can perform any operations
that as usual Python program and then
the Python knows that all there is
variable then filled this completed this
kind of computational graph
automatically and the once we have this
computational graph basically we needed
two things first we want to do forward
paths and then second we want to compute
the gradients of the laws by doing
backward paths so how can a to top
forward which is here so basically we
need us to call the loss function for
given x and y value
so computer low value this is our
forward propagation in backward we want
to know in these cases the gradients of
loss so we identify the variable which
is L and then we call that L that
backward then perform the backward
propagations and then compute the
gradient loss with respect to all the
variable inside of the graph especially
in our case we are interested in this W
because other values are just given in
the dataset only we can change the W so
we are interested in the gradients of
loss with respect to W where is the
gradient so it's stored W tab great so
we are using these gradients computed at
the speck word we just updated our W
value this is basic concept of neural
Nets and then also backward and the
forward propagation but now we're gonna
go one step further using Python so
basically in pythons we have a rhythm in
three steps so first we cannot design
our model using class of course we are
going to use variables
and and then once we have this this
model with what we can do is that we can
construct loss and the optimizer using
pi touch API and then after that we just
run our training cycle which is forward
backward and update let's look at our
examples so in this case we are going to
use exactly the same data set but we
change the form a little bit of to the
metrics so here from now on we are going
to use everything to the metrics here
our data set is three by three by one
matrix which means that we have three
data sets three data and then each data
has one value of x x1 only x value here
this is one so in fact we can take n
data so it's n by one and by one matrix
it's sort of input so we can provide a
for data five there are six or ten data
but each data has one value X this is
how we define this tensor and then we
make them as variable and then we're
gonna go into the rhythm the first step
here is that we're gonna model our using
the class and in the Python way so the
idea is that we are going to create a
class you can just name anything so in
this case we call it model which is a
subclass of towards that and in the
module and inside of this class
basically we have two functions first
one is initializing this class and the
second one is you must have this
function called the Forge let's go one
by one in initialization first you're
gonna call this super init and then
after that you can do some initial
license chan task or you can create some
elements and components for your inner
net in our case which our linear model
so we cannot make a one linear block by
using torch that and end our linear and
we set the one by one this is our input
size so our input size is one because in
each data set the x value is only one x
value which one and output is also one
because
Just Breathe one Y value and then in the
fourth function it's like a freebase
example
it's just get X as an input and then try
to predict Y value inside here we're not
gonna use X W or something we're not
gonna use our own weight but we are
going to use the plot that we
initialized in in it so self that linear
and the we fed this x value and then
we'll generate Y value which has written
this ones and then once we create this
class we can just create so on one
instance to use this class and that's it
this is step one our step two is we are
going to define our construct loads or
select laws from the API so in our case
this is linear regression we are going
to use image zeroes which is already
implemented in the library it's called
criterion for the optimizer previously
we manually update these values but now
we are going to use stochastic gradient
descent algorithm is called
sgt algorithm and then when you define
any creative optimizer we need to pass
what parameters what variables we need
to update so in this cases easily we can
get model the parameters can provide all
the variables we need to be updated and
then when you create also you can
provide learning rate which is 0.01 in
this case you can change this rule in it
for your own problem the next step is
actually the most exciting step is
training the training is exactly
following the forward which is calling
this loss and then to backward and then
we update which in this case is called a
step so first of all we're gonna go run
this F walk cycle and then we're gonna
get the Y prediction value by passing
everything so please recall that is X
data his metrics we just pass everything
to metrics and then we got s metrics so
if you want to know what's the output we
just add this print
statements in here to see so this is
extremely simple to see watch the input
and watch out for if we want to see and
then we're gonna call this function
called create criterion by pissing our
predicted value was real data so the
computer differences loss using MSE you
know of course if you want if you can
print out the loss and then what we're
gonna do is that in this case what we
did is that we did a forward pass and
then our next step is wanted to backward
pass so we want to basically call this
loss that backward you know to complete
all the the gradients with respect to
all the variables before we just compute
these gradients we just initialize all
the gradients and then let's get all the
all the gradients we just call this
optimizers step to update variable what
kind of area was been adopted already we
defined it here so we pass the list of
the variables that we need to be updated
so this step can to update in our
previous example we actually went
through each there are one by one so in
these cases we have a three datasets so
we had roof of three and then we
complete the gradients one by one and
updated so in this case is no problem
but if you have millions and datasets so
you have to go million times one by one
so this is not very efficient so once
but once we are using stochastic
gradient descent we can deal with a
patch so in these cases we deal with
three terra's together and in computer
gradients at the same time update
variable advance so once we finish this
training we can of course test our model
just by calling model that forward like
before so just we provide number four
here and then try to get variable
predicted variable so in this case is
the input is variable so we have to
create variable for this number four and
then this is our result
so our training and basically as time
goes on the epic goes on this is our
loss the loss becomes extremely small
and then when you try to ask I want I
start the power but while we migrate
it's almost close to hate which is
correct points so everything works fine
so in click summary this is our entire
source code edit foodist just simply in
one slide using Python so this is how we
define our data with metrics and then
this is our model that using this class
and then we defined the criterion and
optimizer and your key matter we
provided what kind of variables we need
to update and then we run the training
cycle and then it's exactly following
the rhythm designer model this is linear
and they will construct lost and
optimizer and the readers run training
cycle forward backward and update in
this case we used to step from our
optimizer
so if you want to do something more
complicated classifications or I will
task for example here we want to
classify this image but don't worry we
cannot follow exactly the same step here
three kind of rhythm here right so we
first only we kind of design our model
using our class in this case is a little
bit complicated but initialize we just
define all the components which is
necessarily and then it forward for
Cuban X using this component we just
written this failure our prediction and
then our rhythm to what we're gonna do
is we're gonna define our criterion in
this cases we are using cross-entropy
loss because the different set of
problem we just choose different type of
loss and then in this optimizer we'll
just used STG and that time also gonna
pass this net parameters so that SGD
knows exactly what parameters they need
to update and then we're gonna just run
the cycle so run the epoch and then
inside here which is the completest
outputs using this net and they rerun
this loss and this is our fault cycle
once we finished loss what we do is they
really run the backward
and then this compute the gradients and
then we run the step that's updates our
parameters which is given here that's
all this is how we're gonna use Python
by following their our style and their
rhythm there are some exercises we want
to do in our lecture we introduced SGD
but there are many other optimizers some
are even much better than HDD so you can
try out other optimizers and then see
which one is better for your problem and
now you can basically understand the pi
torch concept so you can read more
examples to understand more about the
Python now our last lecture we're going
to talk about the basic block of neural
net which is called Rocha ste regression