["Tensorflow's contrib's API supports GRUCell only on later version, i.e.\n", "GRUCell or LSTMCell) in Keras directly.\n", "In short, the weights between compat.v1.nn.rnn_cell.GRUCell and keras.layers.GRUCell are not compatible between each other.\n", "Input to the GRUCell's call operator are expected to be 2-D tensors with tf.float32 type.\n", "This means that the ResidualWrapper adds the inputs to the outputs of the GRUCell it wraps.\n", "So you have to make sure that the last dimension of the input and output of the GRUCell it wraps must be the same.\n", "While searching for the documentation for these classes to add links, I noticed something that may be tripping you up: there are (currently, just before the official TF 2.0 release) two GRUCell implementations in TensorFlow!\n", "There is a [tf.nn.rnn_cell.GRUCell (hyper-link)] and a [tf.keras.layers.GRUCell (hyper-link)].\n", "From what I can tell, the GRUCell has the same __call__() method signature as [tf.keras.layers.LSTMCell (hyper-link)] and [tf.keras.layers.SimpleRNNCell (hyper-link)], and they all inherit from Layer.\n", "You should be able to just use the same RNN framework and pass it a list of GRUCell objects instead of LSTMCell or SimpleRNNCell.\n", "I can't test this right now, so I'm not sure if you pass a list of GRUCell objects or just [GRU (hyper-link)] objects into RNN, but I think one of those should work.\n", "[tf.nn.rnn_cell.GRUCell (hyper-link)] is initialized with num_units and activation etc, but not with inputs.\n", "The constructor of GRUCell doesn't add any nodes to the Graph.\n", "Function call() of GRUCell has a parameter of 'scope', I assign the same variable scope as the curgru to it.\n", "In that sense the original version is preferable since there the same GRUCell operate on the whole sequence.\n", "In my version the layers.GRU operate on the input sequence after which the state will be passed on to the layers.GRUCell.\n", "This has the drawback that the weights for the layers.GRUCell will have to optimised (learnt) separately and do not benefit form using the same weights as the layers.GRU, and vice versa.\n"]