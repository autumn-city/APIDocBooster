I dare you to think of anything
moving faster than Deep Learning.
But what if I tell you that behind all of it:
the complexity, the hard work, the data...,
behind all of that,
there is a simple piece that took us years to find.
Let me tell you the story about
the unreasonable effectiveness of the function
that made Deep Learning posible.
"The thinking machine."
It's 1958, and the New York Times publishes
an article about a device that "will be
able to walk, talk, see and write."
They just interviewed the scientist
that's about to change the world.
His name is Frank Rosenblatt,
an American psychologist
who published this report one year before, in 1957.
Here, Rosenblatt proposes the construction
of what many consider the first predecessor
of the neural networks we use today: the perceptron.
"Scientists built a few working perceptrons,
as these artificial brains were called."
April 1957.
This is a receipt from the lab
acknowledging the report.
And here is what it says about the project:
"... designing, fabricating, and evaluating
an electronic brain model."
We've been trying to figure out
neural networks for a long time.
Fast forward 50 years,
mid to late two thousands
and we still couldn't train a neural network
with more than a couple of hidden layers.
But to understand what's missing,
we first need to talk about something
we call activation functions.
Regardless of how big neural networks are,
by default, they can only solve linear problems.
Unfortunately, most things that matter
are more complex than that.
Here is one example.
Imagine you have two classes, orange and blue,
and you want to draw a function that separates them.
A neural network cannot solve this problem
unless we use nonlinear activation functions.
Number of layers, number of neurons...
None of that matters.
Here is what Wikipedia says about activation functions:
"only nonlinear activation functions allow such
networks to compute nontrivial problems."
We need these activations to create
some sort of bump,
disturbance that will allow networks
to solve all sorts of problems.
Like this one here.
But we've known about activation
functions for a long time.
But that wasn't enough.
Something was not right with these functions.
Sigmoid and tanh were, by far,
the two most popular activation functions back then.
Look at the blue lines
and don't worry about the red lines for now.
These functions checked every single box
we needed to train neural networks.
Well, almost every single box.
To tackle more complex problems
like image recognition, text generation
and audio translation,
we needed deeper networks.
But as soon as we try with more
than a few layers,
neural networks wouldn't work at all.
Let me show you this.
This here is the TensorFlow playground.
Anytime I want to play with neural networks,
I come here because I don't have to write any code
and it's really easy for me to try
any of my wacky theories.
Notice here.
This is configured to solve the same problem
I showed you before.
I'm going to add a few more
hidden layers to this network.
I'm going to change the activation to sigmoid,
and then I'm going to click play.
I did this before and I let it run
for a long time before I stopped it.
Over 5000 iterations and the network
could not solve the problem.
But wait, that's not necessarily an issue, right?
Maybe sigmoid is not good enough
to solve this particular problem.
Except, I ran the same experiment,
but instead of using six hidden layers,
I used just two and the network solved it.
This was the thing preventing
Deep Learning from becoming something.
We couldn't train deeper networks
because they wouldn't work.
"My feeling is: if you want to understand a really
complicated device like a brain, you should build one."
That was Geoffrey Hinton's voice.
He played a central role in
making Deep Learning a reality.
But to appreciate what happened,
we first need to understand
why these activation functions
didn't work with deep networks.
Time to look at the red lines.
Now, these are the derivatives of the functions.
We use these gradients during backpropagation
to update the weights of the network.
The deeper the network,
the more iterations we need.
I'm not going to get too deep into the math here,
but if the gradients are smaller than one
and you multiply a bunch of them,
the results will get smaller and smaller.
Look at the gradients of these two functions.
The maximum possible value
of sigmoid's gradient is 0.25.
That's really small.
And for tanh is one,
but that only happens at this particular point.
The gradient is very small everywhere else.
And that right there is the problem.
The deeper the network, the smaller the updates get
until they're so small that the network dies.
We call this the vanishing gradient problem, and
that's why these functions did not work.
"We should look at biology and we should try
and make systems that work roughly like the brain."
OK, it's 2010 and a new paper comes out.
A paper that proposes an idea so simple
that it looks ridiculous.
They show how a function they call
rectified linear unit
solves the problem they had
with the other activation functions.
Here's what the paper says:
"rectified linear units preserve information
about relative intensities as information travels
through multiple layers of feature detectors."
This was the function.
That's it.
This was the crucial missing piece.
Nair and Hinton wrote the paper,
and although they made this function popular,
I found references to it from decades before.
Like in this paper from 1975.
Fukushima doesn't give this function a name,
but that's the rectified linear unit
in the context of neural networks.
But here's the most surprising part.
This function that works so well
doesn't even meet one of the most basic
requirements of an activation function.
This function is not differentiable.
So how come the simplest function
that doesn't even meet the requirements
is the one that makes everything work?
Let's look at the laptop.
I have a very simple notebook here to plot
the rectified linear unit.
For short, we call it ReLU.
If I run this cell, we get the chart.
Here is the plot of the ReLU function.
The x axis is the input to the function,
while the y axis is the output of it.
Notice how ReLU returns zero
for any negative input
and it doesn't touch positive values.
At the point where x equals zero,
we cannot compute the derivative of the function.
And that's a big problem... IN THEORY.
It turns out that in practice
we can return a specific value for
that particular point
and everything works fine.
Here I'm returning zero,
and if I plot the derivative,
it's the red line on the chart.
One final thing for you to notice.
On this plot, the gradient is either zero
for any negative values
or one for positive values.
That means that ReLU doesn't suffer
from the vanishing gradient issue.
The signal propagating through the network
will never disappear.
That's a major win and the main reason
this simple function works so well.
By the way, the gradient is only one
of ReLU's advantages.
The function is very simple,
so it's really fast to compute.
And it doesn't saturate like sigmoid and tanh do.
It just works exceptionally well.
And thanks to it, we have Deep Learning today.
And by the way, when I think about this story
and how well ReLU works, despite its simplicity,
I can't help but wonder:
WHAT ELSE ARE WE MISSING TODAY?