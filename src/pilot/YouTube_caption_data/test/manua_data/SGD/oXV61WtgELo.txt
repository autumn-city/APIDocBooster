Hi, I’m Youhan lee. I’m Kaggle master.
So far, I have won 2 gold medals, 1 silver medals, and 5 bronze medals in vision competitions using deep learning.
In this video series, I want to share various techniques for wining medals.
I hope these videos help you in winning medals.
To win medals in vision competition, We need to make many good models to ensemble.
However, it takes a lot of costs to build multiple models.
It is expensive in terms of time and resources.
Stochastic weight averaging is one good solution for this.
Let’s call it SWA.
Briefly, SWA trains a single model.
After taking several snapshots taken during training, i.e, weights, do ensemble.
SWA was suggested by the collaboration of Cornell University, HSE laboratory and
Samsung AI center at Moscow state university in 2018.
The title is Averaging weights leads to wider optima and better generalization
This picture is from the paper.
Let’s call stochastic gradient descent as SGD.
As shown left figure, SWA averages the various points, bringing SGD to the central points.
As shown middle and right figure, It also places SGD in a large, flat region of optimal points.
SWA brings SGD to the centered point of the region,
Often, the test error is improved even though the train loss is worse.
In other words, generalization is well done.
Learning using SWA can be divided into two parts.
Before SWA, model is trained according to conventional learning schedule.
During SWA, learning is conducted in a different way than before.
There are two ways.
First, the fixed learning rate method.
In SWA, learning rate is fixed and several epochs are conducted.
Then, the weights corresponding to each epoch are averaged to obtain new weights.
Second, not fixed learning rate method.
During SWA, further learning is conducted according to various learning schedules.
Then, multiple weights are averaged to obtain new weights.
There is no prediction during SWA, so you need to do batch normalization update for new weight.
After SWA, update the batch normalization using the train loader.
It is an illustration representing the whole process.
I hope you understand well.
This table is from the paper of SWA.
SWA is easy to implement, improves generalization
no computational overhead, and effective on various tasks.
I have used SWA in many competitions and won many medals.
In fact, SWA is used for ANN, so it works well on NLP as well.
In the Toxic competition, 1st place used SWA.
How about using it?
I made an example notebook for this video using Bengali competition data.
You can see this notebook in our Github.
We will add SWA to the cutmix baseline created in the previous video.
To use SWA with PyTorch, you can install PyTorch contrib.
Ok, let's move the Optimizer part.
Torchcontrib's SWA provides auto mode and manual mode.
Automode makes it easy to do fixed rate strategies.
At first, let's use SWA auto mode.
At frist, Initialize optimizer.
And, put the optimizer into SWA and set parameters of SWA.
swa_start means the number of steps before starting to apply SWA in automatic mode.
swa_freq means the number of steps between subsequent updates of SWA running averages in automatic mode.
swa_lr means learning rate to use starting when SWA starts.
Next is simple.
After all learning is done, you can just call swap_swa_sgd (). That is all in automatic mode. Here.
In manual mode, You can call update_swa () fuction at any epoch you want
without specifying swa_start, swa_lr, and swa_freq.
Like this.
And at the end, you can call swap_swa_sgd function.
Here is an example. You call update swa() when this condition is satisfied.
After all learning, you can call swap swa sgd function.
Hope this code helps you.
To use SWA in tensorflow, install tensorflow-addons.
You need to install tensorflow 2.1 to use it.
Let's see how to use it with a simple fashion-mnist example
You can see this notebook in our github.
This code is from the official github of tensorflow-addons. All credits to them.
You can use SWA very conveniently.
First create an optimizer, put it in SWA and you're done.
There are parameters start_averaging and average_period, which match swa_start and swa_freq
in torchcontrib's SWA.
More details can be found in the tensorflow official documentation.
To store weights, you can use AverageModelCheckpoint, similar to ModelChecpoint.
And, create a model and complie with swa optimizer.
Put a callback in the fit function to learn.
This is all of the video.
Thank you for watching this video.