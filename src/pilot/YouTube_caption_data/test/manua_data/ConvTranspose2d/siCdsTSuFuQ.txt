Good morning, everyone.
I'd like to welcome
you to the first of
our two sessions
on AI at the Edge,
a practical introduction to
maximum integrated smack
78 thousand AI accelerator.
This is brought to you
by a partnership between
maximum integrated
and EBV electronic.
My name is Chris
artists and I lead
the business
development for
microcontrollers at
Maxim Integrated.
At my co-presenters
are Robert muscle,
who leads our
system architecture
for the AI products,
and Sean Brooks,
an Application
Support Engineer for
the AI products.
Most of the
presentation will
come from Robert and Sean.
So you have to listen to
very little of the
marketing guy.
Once I get done
with first couple
of minutes here,
you'll all get very
technical very quickly.
This is the first
session or two
that we'll focus on
practical aspects
of working with our new
AI accelerator
microcontroller
or the Mac, 78 thousand.
Across these two sessions,
our goal is to take you
through the entire
development process,
both the machine
learning portion
and the embedded
development side.
In the first session, we'll
start with some overviews.
After I talk really
briefly about
the device and its
benefits, rubber,
we'll go into much deeper
overview of the maxim
8 thousand device will
also take you through
the documentation on
the tools that are
available for,
for the device.
Once that orientation
portion is done,
we'll talk in
much more detail
about the tools for
the machine learning
side and how the
train networks
that can run on the
max and 8 thousand.
In the second session,
we'll pick up where we left
off with the
train network and
we'll talk about
how you take
that train that work
and bring it into
the embedded world in
a form that suitable
to run on our part.
And we'll show
this to you in
the context of the board
you see here on this slide,
the max and the 8
thousand feather board.
It's a nice small,
low-cost evaluation
kit that
still has a lot
of features,
but small enough that
you can prototype
small embedded,
wearable types
of applications.
So before we really
let the technical
team get going,
I just want to give a
brief introduction to
the max and the 8
first thousand.
Let me just start
off by talking
a little bit about the,
the market that
we're trying to
approach with the
maximum 8 thousand.
What we see at
maxim is that
there's a real gap between
big machines ability to
adopt AI technology and
little machines ability
adopt AI technology.
While we're all used to
the big equipment
on the left,
adopting AI technology
in terms of being
able to see and hear
much like humans,
to the point where they can
drive cars autonomously or
understand natural
language to
it to very great extent.
We see much more
limited capabilities
in the devices
on the right,
where at most
they tend to be
able to support maybe
a simple wake word.
And the real
reason for that
is because they are,
they're constrained by
the devices that
implement them.
The microcontrollers are
usually in these devices
can't really handle
in a realistic sense.
Ai applications much more
than just simple
wake words,
something like a hey Alexa.
And so what we're
trying to do with
our max and 8
thousand and it's
a follow on devices is
close that gap and let
the machines on
the right start
to see and hear.
Actually have vision
based applications,
but also here,
much more complex things,
maybe more sound or have
a bigger vocabulary.
And the reason that
we see that get
two days because the,
one of the main workhorses,
machine learning,
the convolutional
neural network.
It's computationally
expensive.
It takes billions,
multiplies
T2 to execute and
insight at which leads
to power consumption.
It leads to devices that
have to be very fast,
they run very hot, and
in some cases are
extremely expensive.
This is probably okay for
things like the
self-driving car.
They can adopt the cost
of an expensive
GPU or FPGA,
but doesn't really help
the embedded equipment,
the little machines
that you saw
on the right-hand
side of that slide.
To address this
maximum design,
the max 78 thousand.
What you see here
on the block
diagram that Robin
will get into
much more detail on is
really two sides
of the chip.
On the left-hand side,
you see things
that you might
expect from any normal
microcontroller.
Do you see actually
two micro-controller cores,
cortex and for
ethane or risk five,
you see a lot of external
memory interfaces
and you see
integrate a memory.
Well, on the right-hand
side, that's
the real magic of the part.
It's our own homegrown.
See in an accelerator.
It's really a big
state machine
that will run through
an entire scene and
computation once it's
configured and once
the data is loaded
and the weights are loaded,
we'll run through
computation,
run through an inference
on and on its own.
There's a lot of
the special sauce
in that accelerator
we've really designed
to try to lower
the energy
consumption to lessen
the data movement that
happens in those
CNN computation.
The microcontroller
sections you see on
the left really their
main job is not to
take part in that
AI inference
or in that CNN computation,
but really to get data
from the outside
world to the scene,
an accelerator, and there's
a low power away
as possible.
And then do something
with the result.
And you'll take the
result, put on a display,
set it to a radio,
whatever the appropriate
actions to take.
So some of the results
that we're seeing
with the maximum
8 thousand is
the high level
where we see that it's
making the energy
required to make
an AI inference
almost irrelevant.
And so what you
see here is you
see kind of the three
parts of making
an AI inference from a
system perspective and
the relative
energy consumption
of those in a
traditional micro.
You see that the energy
required for the AI
inference for kept doing
all those millions or
billions of calculations in
the CNN is dominant
and takes up so much
more energy than
things like.
Any data manipulation has
to happen before you run
the CNN or the any of
the input and
output getting
into data from the outside,
sending data back to
something outside.
What the max and the
1000 has done is
really lowered the
inference energy
required to
execute
a vision inference or
hearing inference.
And we've also taken
some steps to minimize
the amount of data
that's required for
data manipulation by
integrating a
low-power risk
five core and some
other features.
And so what we see is
that in many applications,
the energy of the inference
no longer matters.
And we put it
back in the camp
of the energy required to,
to input the data.
For example, to
access the camera
or taxes access
a microphone is now
again the dominant term.
So let's look a little
bit more real data and
maybe some comparisons on
the max and the 8 thousand.
We've got a couple of
demos that are part of
our evaluation
kit and are on
the GitHub repository
that we'll talk
about later.
That are pretty powerful
than those ones.
Keyboard spotter that,
that can spot 20 words.
And then what is a face ID?
And so they're the
keyword spotters and
the greenish teal color
there and the Face
ID is in yellow.
What we've compared
is our max and
8 thousand that has a C and
an accelerator that
runs at 50 megahertz.
And we compare
that to one of
our own very
low-power cortex.
And for F devices that has
a pretty big memory
footprint is
three megabytes of flash
and one megabyte of sram.
And then we've
also compared it
to an S T cortex.
I'm seven simply because
Mac and doesn't have
a cortex and seven
on the line card.
You can see that
the keyword
spotter on our device
executes in two
milliseconds
with a 140 microjoules.
Anthony see how
much longer and
higher energy the Cortex
M for f and the
cortex them 70.
And then the same
story on the
face ID we're arch.
It takes 40 milliseconds
and about 400 microjoules
compared to much
longer on the cortex.
And for f cortex it M7.
You also see
with the cortex
and 7 that there's
some additional
energy required
for accessing
external dram.
Because because the
maxillary to 650,
the middle product that has
a big memory
doesn't have to go
outside for the face
ID application,
but the STM 7 does.
And so you see
actually that
there's a
significant memory
burned when you have to go
the external memories to,
to execute any kind
of AI inference.
So that's all great.
The energy consumption
of our part
it is, is phenomenal.
So hopefully you see
that you can start
to think about
vision applications
for even battery-powered
devices.
But what's the catch there?
And there is a
catch regarding
AI technology in
the embedded world.
And one of the
first challenges is
that it traditionally
the people
who understand
machine learning can
build models and build and
train convolutional
neural networks
come from a very
different background and
very different education
than people who can
effectively build
embedded devices.
It's a very different
skill set and we see
very few cases where
those skills meet
in the same person.
Similar to this, the tools
used in machine
learning to,
to, to set up and
describe a model and
then to train it.
Are completely
different tools than
the tools that an embedded
designer is used to.
And so there's a chasm
here. There's a gap.
And we see that
machine learning
people are often not the
same as the embedded
people at our customers.
So there's definitely
a challenge in terms
of developing these
applications that you need.
Both of these expertise is.
So that's what we're
going to try to
approach today is
we're an end in
the next session
is we're going
to go through step-by-step,
the tools available,
the documentation
available for
both the machine
learning side and
then the embedded
development side as well.
So in today's session,
you will start off here
with a little bit
more detailed
overview of the max and
8 thousand will go through
our GitHub
repository and kind
of walk you through
some of the documentation.
The tools are there,
there's a significant
amount of content,
so a quick walkthrough
is probably called for.
I will talk
about setting up
your system for doing
machine learning,
for, for doing training.
Models for that can
eventually target the
maximum 8 thousand.
And we'll talk
about the over
an overview of
the models that
we provide in our
GitHub repository,
what those look like.
And then we'll actually go
through an example
of training.
Keyword spotter,
all the data,
all of the materials
are available
on GitHub and
we'll walk you
through it and we'll
close with some
further topics
on training before we
get ready for
session number two.
And with that, I'll turn it
over to Robert whole get
into an overview of
the max of 8 thousand.
Chris gave you
the overview from
a business
perspective of what
the max 78 thousand as.
So we thought we should
probably go a little bit
more into the
technical details
of how we achieve
what we can achieve
with this chip.
And thus slide, you can
see a lot of green bars.
That's the
inference energy.
So when you run a machine
learning algorithm,
most of its energy
at runtime is in
the inference,
meaning all the matrix
multiplications
and other
operations that are
needed to compute
the results.
On a traditional micro
here on the left.
As a lot of green bars
and that's to scale.
That takes out the
majority of all energy.
And there's a little bit
yellow of data
manipulation on the IO.
You might have to
rearrange your bites.
And of course
there is the blue,
which is the IO itself,
which you probably
cannot avoid
because how else would
you get to your data?
How do we go from
the traditional
micro to the max
78 thousand, which
is on the right.
So you can see the green is
now practically irrelevant
and the yellow,
if your eagle-eyed, you'll
notice it has shrunk it.
Well, That's a
block diagram
of the max 78 thousand.
And we draw it in a way
that makes it
obvious that really
half of the dye
is taken up by
the convolutional
neural network
accelerator that's
on the right.
You can also see there's a
lot more to this chip.
Embedded external
interfaces, power supplies,
some crypto and security,
and to micro-controller
cores though
there's a Cortex M4
with loading point,
as well as a risk
five core and
DMA and onboard
flash memory,
as well as lots of sram.
Looking at the CNN
accelerator on the right.
Notice there are 64
parallel processors
and lots of memory.
So when you
total it all up,
you will get two over one
megabyte of
memory that will
hold the network parameters
as well as the data that
you are processing.
When you remember the
big green bars earlier,
I said that we
optimize some of
the data manipulation
as well,
and that's what this
annotated slide shows.
There are additional
optimizations
in the chip that aren't
just the
convolutional neural
network accelerator,
but all kinds of
other measures that make
the chip lower power.
We're mostly inherited from
previous low-powered
chips that we
had that are used
in wearable devices,
for example.
But also specific
things such as
fifo is an optimized
data manipulation
on the interface between
the micro and the
scene, an accelerator.
Now we're getting
to the good stuff.
On the left side of
the slide you can see
a die shot of our
max, 78 thousand.
And you'll notice
at the top there is
a pretty large
yellow rectangle
is about half the die size.
That's the region
used by the sea
in an accelerator can
further see that
it's subdivided
into four smaller
rectangles.
There was other regions.
Basically.
They contain the
same resources,
eat their tiled together,
and moving on
to the center,
the colors correspond.
So the yellow shows
you four regions.
And then we've broken
out one red region,
which itself consists
of four processors.
So four times
four times four.
So we have 64 processors
total four processes
each share data memory,
but they have their
independent math engines
as well as the independent
parameter memory.
And they communicate with
one another and
bubble the result
through which then gets
accumulated at
the top level.
One other thing I
wanted to point out,
running at 50 megahertz,
which is the standard speed
that they see in an
accelerator runs.
Whereas the CPU
is twice as fast.
We get to about 28 billion
operations per second.
And an operation here
means multiply accumulate.
So not just an
addition or something.
The total memory, and
I mentioned
briefly earlier,
is over 1.5
megabytes of sram.
Everything's on died there,
as well as 512 kilobytes
of non-volatile
flash memory.
So we're trying to
keep the distance is
short and the memory local.
Can reason for
this is that the
further you have
to move data
anymore you have to pay
in terms of energy.
So obviously your
number one priority
is don't read at
all if you can,
number to reduce
the distance.
And then number 3, if
you did have to spend
the energy to
read something,
maximize the parallelism.
So this slight
we're trying to
show which way you
trying to do a
dataflow system.
You have an input, you do
the math on it
and the filter,
and then you
have the output
does is not a percolator.
Don't want to have the
data processed and
finished when it
comes out the bottom.
So the main optimizations
and the max
78 thousand are really
related to the datapath and
trying to minimize
movement of data.
And you will probably
realize that nothing
is ever free in life.
So while this system works
extremely well
and very fast or
very low power for
the operators it
understands, it
is hardware.
So you can't just go
in and add a piece of
software and make it
understand the
new operator.
The slide, we
briefly outline
the main operators that
are supported by
the hardware.
There's much more in-depth
discussion and the
documentation, of course.
But I wanted to
point out that
the main energy
efficiency is gained and
two dimensional
convolutions
with three by three kernel
starts the most
acceleration
that you can get.
We do support some
other things as well.
For convenience, mainly.
For example, you
can up-sample.
You can do one-dimensional
convolutions,
and you can do
linear layers.
We have mainly value
is for activation
that element-wise
operators pooling.
In some cases, you can
combine operators
into one layer.
For example, if
you do a max pool
and a convolution
that can't
shrink it to 1 layer
on the right side
of the slide.
Some other notable things.
One is you're not
limited to just
eight-bit weights,
so you can increase
the capacity a
lot by using For
two good weights.
For example, data is
always eight bits.
And then there's
some additional
hardware features
like every layer
allows you to do
a Shift before you
reduce the output
size to eight bits.
And we're using that for
our quantisation
where training,
batch norm and
other things.
One more thing
to point out,
maybe that's the
risk five core,
which we use as
a smart DNA.
The risk five is just much
smaller than the ARM core.
It lives in the same
clock and power domains
as the CNN.
And so it can
get to the data
quickly and we can
use it for smart
pre-processing of
your input data.
The last bullet.
So streaming mode
with fifo is i'm,
I'm going to get to
streaming in just a second.
Before I get to explaining
what three motors,
I should talk a
little bit about
the motivation behind it.
This slide is titled,
Why is VGA hard?
Could also have been tiled.
Why our image is hard?
And on the left side
there you see
these bubbles.
The bubbles scale
with the image size.
And it's hard
because usually
you scale both
dimensions so
it draws of the square.
So we can see
the little dot down there
in the bottom left,
on the left side.
That's a C 510
famous benchmark,
32 by 32 images.
And as it goes up
in term of videos,
that's the limit that
that hardware
accelerator can do.
Imagenet 224 by 224
quarter VGA, 320 by 240,
and then we have EDA at 640
by 480 should just
as shear area.
Lot bigger. Now
you might say,
well, a VGA image,
it's got three channels,
red, green, and blue, as
shown there in the center.
So that's about
900 kilobytes.
What's the big deal?
Well, that's the input
during a convolution,
it's very likely
that this will blow
up too many times
the upsized.
So here we're showing
a 128 intermediate channels
of 640 by 480.
And it could be
256 or even more,
and that's already
34 megabytes.
So this is not
going to fit into
an embedded memory
on die anymore.
So now you're going
to have to go off
guy and you're going
to pay the price for that.
Your distance grows,
means your energy grows.
You're getting off chip,
means you probably have to
use higher voltages.
Your energy grows.
So it's all in all, very,
very inefficient and very
undesirable to do that.
And what can you
do about this?
Now? Assuming you didn't
want to do the
obvious interests,
not support
images or video,
which would limit
the applications
quite a bit.
You have to come up
with another idea.
And what we are
implementing
is streaming mode.
Basically, we've taken
advantage of the fact that
any image sensor will
give you data on
a row by row
scanning basis.
And it turns out
that you can start
your processing before
you have a full image.
And this is true
for the image sensor
itself as well.
It does not store ethyl
image at anytime.
It may have a
couple of rows or
three or four rows
started buffers,
but it will not ever store
the full image itself.
And so we're feeding the
image sensor data in
real time through flavours
into our neural
network accelerator.
And it's really a
natural when you're
using an image sensor
on the right side,
you can see how
this might work.
So the green data
is all the data
that you had to
have to start
processing up to
the last layer.
Okay?
Notice the gray, the dark
gray in all these layers.
That's data that has
not been loaded yet,
that's not even in
the device yet.
It hasn't even been scanned
in from the MR sensor.
So once we get to
layer and on
the right side,
we consume the
first input data.
And we don't
need it anymore,
and that's marked in red.
We can throw it out
and reuse that buffer
inside IoT device
for new image data.
And there's
feedback between
these layers to
make this all work.
And you have to match
the data rates between
your image sensor
and the pricing
is processing speed up.
Do you see in
an accelerator
for this to work?
When it works, it
works pretty well.
And you can do
videos sized grains
with only 1.5 megabytes
of Islam in the device.
Before we get into details,
I'd like to quickly
walk you through
the maxim integrated AI
GitHub repositories.
We'll go over some of
the information found here
in more detail later
in the presentation.
But for now I
just want to make
you aware that
everything we're
about to present
is written down
and available
online at GitHub.
The URL is github.com
slash maximum
integrated AI.
So we have a few pen
repositories to the top.
This is where most of
the action happens.
The first repo you'll
want to take a look
at is the
documentation repo.
You can consider this repo
to be the top of
the hierarchy,
and you can use it to
navigate to the
other repos.
Here you will find
our evaluation kit
and feather board
documentation as
well as links
to the training set.
This is ribose.
The evaluation kit and
feather board links
take you to a quick start
guide specific to
each platform.
Let's have a quick look at
the feather board
documentation.
The information you'll
find here is aimed at
the traditional embedded
developer who was
interested in quickly
compiling and running code
on the MCU using the
Mac 78 thousand SDK.
The SDK has many
demonstrations.
See projects which use
pre-trained and synthesized
models such as m-nest,
keyword spotting and
face identification.
We'll come back to the SDK
later and I'll show
you how to compile
the demonstration
firmware and
get it running on
a feather board using the
Eclipse IDE on Windows.
For now, let's
go back and take
a look at the other
repositories.
This link will take
you to the max
of 8 thousand SDK,
which is going to be of
interest to firmware
developers.
And the next two links,
it'll take you to
the training repro
and the synthesis repo,
which will be of
interest to machine
learning developers.
And let's take a look
at the training repo.
The trainee repo
contains scripts
and other
collateral related
to creating ML models,
managing datasets
and model training.
The Ramy contains
topics on architecture,
tool installation,
workflow, and details
on tool usage.
Quickly jumping over
to the synthesis repo,
you'll notice the
Read Me is very
similar to what is
found in the training repo.
The synthesis repo contain
scripts that except
the output of
the training workflow
as an input in
order to generate C code
that can be run on the Mac.
70 thousand
functions such as
quantisation and
network loading
are provided here.
Also, you can find the max
78 thousand SDK as it
gets sub-module which is
referenced by the
synthesis tools.
We sometimes get
asked what PC to
buy and what operating
system to install.
And there's one
really important
thing that you
need to keep in
mind and that's
get an NVIDIA card.
Very recent with
lots of memory.
That's really the
most important thing
without the NVIDIA card
which supports CUDA,
which is and videos,
single instruction,
multiple data library
training will
take forever and just
won't be a lot of fun.
So in theory, you can do
it without that occurred.
It just be very,
very small.
And we do provide
pre-trained weights
for the networks
that we're talking
about today.
So you can get going
without that card.
And secondly, you should be
using Linux just because
most machine learning
development is
done on Linux and
there'll be a lot
less friction.
We would recommend
Ubuntu 20 or four.
That's a long-term
service release.
And you can install
the server or
the desktop image
depending on
whether you want
to sit in front
of it or whether it's in
your closet. And
that's really it.
With Linux and
a video card,
you're going to
be in business,
not just for the
max 78 thousand,
but for most every other
machine learning
system out there.
And while I really,
really recommend
cuda on a graphics card,
we did choose a model today
that's much smaller
and that can,
in a pinch, be trained
just on a CPU.
Assuming you have
enough cores,
I would expect on a
six or eight core CPU,
this thing to take
about six times
five or six times longer
than with a single
graphics card.
So it should be
possible, but again,
we're also going to provide
the pre-trained weights so
case it doesn't finish,
you'll have them
ready to go.
There are a few
ways you can set up
your development
workflow depending
on what equipment
you have available,
your development
team composition,
or just personal
preference.
It's possible to develop
entirely on
Linux or Mac OS,
or even Windows
with the help
of Windows Subsystem
for Linux.
However, if you want to
travel a well-trodden path,
and I'd recommend that
you use Linux for
your machine learning work
and Windows sphere
embedded work.
This is the most
well-documented
and the only officially
supported workflow.
So let's head over to
the training repo
on GitHub and take
a look at the
instructions for getting
your machine learning
system up and running.
There's a lot of good
information in the
read me that's worth
spending some time with.
But for a first look,
just scroll down to
the project
installation section.
These instructions are well
tested and you should
be able to just
copy and paste them and
be done and 20
minutes or so.
So I'll leave that
for you to explore offline.
Ultimately, you
want to be able to
successfully execute
one of the
training scripts.
So let's have a
look at that.
I've opened a
bash terminal to
the training repo that
I previously Cloud.
And before you do anything,
you'll want to run
source bin activate.
And when you do
that, you'll notice
that it changes
your prompt to indicate
that you're in
training mode.
And what this does
is just sets up
your Python
environment specific
to the needs of training.
So now we're ready
to do some training.
And for the purpose of
verifying the install.
The first training script
I like to run as amnesty.
So you can type in
train underscore
dot SH and kick that off.
This will take
quite a while on
a non could a
platform like mine.
So I'll just time
warp us ahead to
the finish line.
And finally we're done.
So once you get to
the end of training,
you should see
something like this.
The last line indicates
a log file and it gives you
the full path to us
subdirectory under
training called logs,
which is organized by date.
So each time you
train a model,
it'll get organized
into the logs folder.
So now we've we've
verified that
our install is
complete and correct.
At least as far
as training goes.
Now, more than
likely if if you
can train and
you're going to
be able to
synthesize as well,
but it's quick
to go ahead and
verify the synthesizing
functionality.
So let's do that
now. So remember
that we are running
in a training
environment right now.
And what we want
to do is switch
over to the synthesis
environment.
To do that first
you'll want to run
the deactivate command and
then change directory
to your synthesis
directory.
And here we can source
bin activate to
activate the synthesis
environment.
Now we can run Jin
demos max 78000 dot SH.
This will synthesize
all demos
using pre-trained data and
output to SDK examples,
max 78 thousand CNN.
And we're done. So let's
check the output directory.
Make sure that looks good.
Sdk. Examples like
70 thousand CNN.
And we see Oliver,
Oliver demos there.
So at this point,
install is good.
We've got output. All
of these demos Can,
can be imported into
Eclipse and burned into
the flash on a Mac,
70 thousand and run.
Let's take a closer look at
the contents of the
training repository.
You will see several
Bash scripts
associated with each of the
demonstration models.
These scripts, or for
convenience and
allow you to quickly
train and evaluate
models for
several demos supported by
the kit and the
feather board.
They can also serve
as a guide for
producing
customized training
for your own applications.
Let's cat one of the
training scripts
and you'll see that they
all look pretty similar.
They basically
call train PY,
train, the indicated model,
in this case 85
net five against a
specific dataset,
in this case m-nest.
Let's cat Kw 20 SH
for comparison.
You can see that has a lot
of the same options,
but as you would expect,
the dataset and
model are different.
So each of the training
scripts deals with
a specific demo supported
by the kit and feather.
Let me give you
a brief introduction
to each demo.
Cats and dogs is
an image classification
demonstration
that tries to
differentiate between
images of cats and
images of dogs.
That uses a 25000
image dataset
from kaggle.com.
By the way, all
the information
I'm discussing
now can be found
in the read me inside
each examples source
code directory.
And I'll show you exactly
where to find these
and other session.
Next we have the
CFR demonstration,
which comes in two main
flavors, CIFAR 10,
and see if our 100 CFR is
a labeled image
classification dataset
consisting of 10100
image classes,
respectively.
Each class has 6000
image of things
like vehicles and animals.
Cifar 10 consists of
low resolution
images which can be
useful during early
stage model development.
Since smaller
datasets generally
require less training
time and so reduces
iteration time as you make
the first few course
adjustments to your model.
Face ID is
another image classifier
that attempts to
differentiate
between a few faces
as well as identify
unknown faces.
It is trained with
the VGG face to data
set with the empty CNN
and face net models.
Next is kinda just 20
popular speech
centric application
for the Mac 78 thousand.
It's a 20 word
keyword spotter
which can identify
keywords such as left,
right, 0 through nine,
and several others.
It uses second version
of Google speech
commands data-set
along with AID five,
can you give us
20 net model?
This is a good example
to start with.
If you're interested
in keyword
classification
applications.
We have an antinode on
the Maxim
Integrated website
that provides a deep dive
into how the
model works and
its implementation
on the 78 thousand.
This is a very
good resource for
general information on
speech processing
models and methods,
as well as specifics
of the max 70 thousand CNN.
You can find it by
going to maximum
integrated.com and
searching for a and 7359.
Finally, we have Amnesty,
which is sort of the
hello world of
machine learning.
It's an image
classifier with
the purpose of classifying
handwritten digits.
It's relatively quick
to train and it's
a great candidate for those
who are new to
machine learning.
The m-nest dataset
consists of
60 thousand
handwritten digits
alongside a test set
of 10 thousand
additional images.
Each image is 28 by
28 pixels in size.
All of the demos in
this directory use
publicly available
data sets which are
downloaded as needed as
part of the
training process.
These datasets are stored
locally and the
data directory.
While we're here, I'd
like to point out
other aspects of the
training repository.
Alongside the
training scripts,
you will find correlating
evaluation scripts.
These can optionally
be used to
evaluate post synthesis
training model performance.
Brows and the tree
further, you'll
notice several
subdirectories.
Let me bring your attention
to a few of the more
important ones.
Data, as mentioned before,
is where datasets
get downloaded,
uncompressed, and
adjusted as necessary.
The dataset's directory
contains scripts that
facilitate interfacing
with publicly
available datasets.
The Logs directory is where
the output of each training
session will be stored.
And finally, the models
directory is
where you'll find
Python classes that
implement each of
the models used by
the demonstrations.
These Python classes
are a good guide to
creating your own
custom models.
Before we start
training our model,
I wanted to give
you an overview
of the development process.
And as you can see
on this slide,
I've divided it
into two sections.
The top is what we're
going to do today and
the bottom is what we're
going to talk
about tomorrow.
And the top you
can see the main biases
training nurses,
where we take our
model and training and
test datasets and create
the weights using
backpropagation.
That takes awhile.
So we split
it between the days
during training.
So basically you can
let it run overnight.
And tomorrow we're going to
go and take our
trained weights,
which ion floating point.
And we'll convert
them to integer using
a quantisation which
creates another
checkpoint file.
And that checkpoint
file and can be
run through evaluation
to see how good it is.
And when we're happy,
we're going to take
that checkpoint file,
run it through the eyes,
or to create
imbedded C code
using some additional files
that we'll talk
about tomorrow.
So that's the overview.
And I think we might be
ready to start
training now,
at this point, we're
ready to start
training our
example project.
And the example project is
keyword spotting,
20 keywords.
And the third model
that we have so
kw as 20 V3,
I'm sitting on
my Linux command
prompt with a huge
font that I chose.
So you can actually
see what's going on
inside the training
repo subdirectory.
And I have my virtual
environment active.
You can see that up here.
I'm correct. Git status.
So nothing left
to do there.
Everything is ready to go.
We have example scripts
for all of the model and
data set combinations
inside the scripts
directory.
So let's go look at that.
And unsurprisingly,
the training script
for Kw is 20.
B3 is called
train underscore
kw as 20 underscore
V3 dot SH.
I'm going to go start
that script right
now so you can see
what's going on and
what it should look
like when everything
is working correctly.
So scripts slash terrain,
underscore Kw, yes, 20
honors for V3 dot SH.
I'm also going to add
one more command
line option
that you don't have to.
I'm going to restricted
to a single GPU,
which is probably what
most of you have.
Dash, dash GPU 0,
I, and off it goes.
So you can see
it is creating
a log file and then it
does some data prep.
The data prep went
by real fast.
And the first time
you run this,
it'll probably
take a lot longer.
It will cache that and so
you don't have
to do it again.
Now we already training.
And you can see
there's 256 samples
per mini-batch and
It takes about
0.03 seconds.
So we can go into
another window and
type NVIDIA dash,
ask them high.
Net shows us the resources
that are currently
used and you
can see it's only
1.3 gigabytes on
the graphics card.
Out of the 24 gigabytes,
it has ans using
35 percent of
the GPU resources.
That's very little.
We chose that so that
the script can be
used by most anybody
regardless of GPU choice.
So we're already in
the second round here.
And I'm going to
pause this when it
comes and finishes
that applique.
So you can see what the
verification
line looks like.
So here it's doing
a verification
and then it's burning
a confusion matrix
that shows you how good
at it already is and
you can already see the
diagonal going there.
Now at the same time
that this was training.
And we're just looking
at the graphical
up at the command
line output here.
We also have
TensorBoard support.
So you could get a
graphical representation
and follow the
training progress
using your web
browser and just
refer to the documentation
if you want to do that.
I'm going to abort
this now and show you
the difference when we're
just using the CPU.
This model has chosen
specifically to make it
not too painful if
you don't have a GPU.
So I'm saying scripts
slash strain under spoke
AWS 20 underscore
V3 dot SH, dash,
dash CPU, which will
suppress the install GPUs.
Just to show you the
difference here.
And again, and
this particular
model is not going to be
so that it's much worse
with images than
with audio.
So you can see
it's 0.24.2.1.
So we're say
maybe factor 45,
which is the
lowest I've seen.
Again, the reason
why we chose
this model so that
anybody can follow along.
Okay, This is terrible,
so I'm going to stop this.
And I will start
my original again.
You 0.
Now you could go away,
have a coffee
and more coffee.
And it's set to
auto terminate
and everything.
But we also providing
the pre-trained weights
for you already.
So you don't
really have to go
through this if
you don't want to.
And that's how you
start training a model.
So while training script is
continuing in
the background,
Let's go switch
to a web browser
and take a look at
the code to see what's
actually going on
behind the scenes there
while we're training this.
If you've used by towards
before or any other deep
learning framework,
you're probably
going to be very
familiar with this setup.
There's one main
training loop.
In our case, it's
called train.py.
And that one's
really driving that
forward backpropagation
in a big loop.
So a trained up UI.
But it does radiate.
It has two main functions
and all kinds of
little extra stuff like
saving checkpoints
and such.
But it also in,
instantiates the
model and the data
set. It's all dynamic.
So you can just write
a new model or provide
a new dataset and
put those in the
respective directories.
So for the Kw is 20 v3.
We're using the
Kw as 20 data
set just in here in the
dataset's directory.
And we're using the
Kw years 20 v3 model,
which is in the
models directory.
Go open that model because
that is where
it begins to be
a little more interesting
than this model.
You have an instantiation
of the nn modules,
the top and
then just the standard
forward prop.
Like in any other Pi torch
system and PyTorch,
does the backprop
automatically
just need to specify
award product?
You'll notice the arms
cause up top here.
And it says fuse
car something
and always as an
AIX in front of it.
These are custom
sub-modules,
so they are inheriting
from the nn modules.
We're also augmenting them
with some knowledge
about our hardware,
primarily clipping
and rounding.
But some other ones sound
very normal like linear,
that's very similar
to an end our linear.
Whereas the fused ones,
we'll do both in this case,
a one D plus a
value in one layer,
or in this case, fused
max pool conflict.
Do you value it will do
max pool conformity and
value in one layer.
That's a feature of our
hardware that we can do,
pooling and convolution
in the same layer.
So this saves hardware
resources and make
things faster.
So we're providing
these fused modules.
Of course, you could also
separate them out if you
wanted to do that,
but why not?
And then here there's
a standard and
end our dropout.
Because that's really not
used during
inference anymore.
That just happens
during training.
And you look in the
models directory,
there are many
more that are
partially more
complicated than
this Kw is 20 V3.
But if the hardware
supports what
you want to do,
you're not going to
have any trouble
putting your
model to the max
78 thousand what we're
training our OK. WES model.
I wanted to give
you a few pointers
on what you
have to do to train
your own dataset,
the model.
And the data set also
includes the
data loader way
you would do data
augmentation.
For example, when
you're training images,
you can change
the background,
the image, or do any
number of these things.
In terms of audio, we would
add different
types of noise.
You look at the
Kw is 20 loader,
you will see that
there's just Gaussian
noise right now,
so that could be improved.
Now, I have already
pointed you to
the dataset and model
subdirectories.
But then the next
step would be
to take a look at the
big Read Me file.
And here there are two
versions of those.
There's a markdown and a
PDF, the mark down as many.
So you can see it on
GitHub when you
scroll down,
but it's missing,
for example,
a table of contents.
And it does not
show any equations.
So we'd say open up
the ReadMe file.
So here we are and
the ReadMe PDF,
and that's a
pretty big file.
But in our defense,
we're not just
talking about
the training process,
but also the hardware
resources as well
as how to convert
code into C,
which is the topic for
tomorrow's session.
Right after this,
we're also going to
be available for live Q&A.
So I'm hoping
to talk to you
now and then see you
again tomorrow. Thank you.