[MUSIC PLAYING]
NADAV EIRON: Hi, my
name is Nadav Eiron,
and I'm the VP of engineering
for Google's ML infrastructure
products.
Our mission is to
drive ML excellence
for Google and the world.
We create the tools, frameworks,
languages, and services
to power Google's
AI first strategy.
And we make them
accessible to all of you,
so you can build the
things that you care about.
So let's start with what we've
been working on with machine
learning at Google.
ML power's amazing things,
from night sight in Pixel,
to reconstructing thousands
of particles in one
go at the CERN Large Hadron
Collider with TensorFlow.
While it may feel
like magic at times,
it's simply advanced
technology, made possible
by tools that are
easy to learn and use
to solve your own problems.
The goal of any platform is
to let you build solutions
to your problems.
And we're working to build
one for machine learning that
can be used in your
business, your hobby,
or even cutting edge research.
When building ML solutions you
work with data, with ML models,
with conversions to
different run times,
with optimization, deployment,
ML ops, and a whole lot more.
So let's take a look at
some of the major parts
of the ecosystem,
like TensorFlow.
We open source TensorFlow to
accelerate machine learning
research and empower
developers around the world
to build AI applications.
That was six years ago today.
And we're so happy to celebrate
TensorFlow's birthday with you
all.
TensorFlow has expanded
into a vast ecosystem
of tools and frameworks
that help you
along your developer journey.
Let's take a closer
look at the ecosystem,
starting with applied ML.
We'll start with
the tools we have
to go from data to production.
From your first and simplest
Hello World machine learning
model, you can use the
high level APIs from Keras.
The same skills will transfer
to more cutting edge problems.
We often hear that machine
learning models can be opaque.
And we'd like to understand how,
for example, a computer vision
model determines what it sees.
With research into
class activation maps
there are techniques to see what
the model pays attention to.
So here you can see
simple Keras code
that shows us which
part of the image
the model paid attention to
when it classified a cat.
It was the cat's face.
Because of how it can
be used in problems,
from the simple learning one,
to the more complex research
oriented ones, as
I just showed, we
believe that Keras
is the best framework
in the world for applied AI.
And it's our primary high
level API for developers
using TensorFlow.
It's not just for
getting started.
Let's look at some of
the new advancements.
In TF 2.4 multi-worker
mirrored strategy was released.
Keras models now support
training on TPUs, GPUs,
multi-GPU machines, or
clusters of multi-GPU machines
by simply switching the
distribution strategy
between TPU strategy,
mirrored strategy,
and multi-worker
mirrored strategy.
In TF 2.4 as well the
Keras Mix Precision API
was released, allowing
faster training
on GPUs on float 16 mode,
as well as BFLOAT16 memory
optimizations on TPU, with
automatic [INAUDIBLE] scaling.
In TensorFlow 2.6 Keras
preprocessing layers
were released,
making it much easier
to add preprocessing
capabilities to your models.
They had multiple preprocessing
layers for categorical data,
replacing the featured
columns API from TensorFlow 1.
Preprocessing layers also
add a new comprehensive text
vectorization layer that will
make your MLP models much more
readable.
Additionally, as
of TF 2.6, Keras
has been modulized
as a separate pip
package on top of TensorFlow,
installed by default when
you install TensorFlow.
It now lives in
a separate GitHub
repository at the link shown.
This will make it much
easier for the community
to contribute to the
development of Keras,
and we welcome your PRs.
And we know that as developers,
efficient problem solving
doesn't equate to building
all your models from scratch.
Discovering these models and how
to use them isn't always easy.
And with that in mind, we
created TensorFlow Hub.
On TensorFlow Hub you can find
hundreds of pre-trained models
for video, image, text,
speech, and audio.
For example, here
is a simple model
we trained for common
spam detection.
You can find it in Hub,
and then test it easily.
Or this bird detection model
where we can upload a picture
and get it classified.
And, of course, when you
want to use the model
to solve your
business problem, you
don't have to do it by yourself.
TF Hub will show you how to
do it, which layers to freeze,
and which ones to use for
activations, et cetera.
We just saw browser
based TensorFlow models
with the common spam and
image classification.
And think about that.
It was a model running in the
browser, which means anyone,
anywhere can try your research
with the click of a link.
No dependencies to install
or environment to set up,
and usable by billions of
people around the world.
This is powered
by TensorFlow.js,
our JavaScript implementation
of TensorFlow that
enables you to run or
train models anywhere
JavaScript can run, which
is pretty much everywhere.
JavaScript can run
in the web browser,
on device, server side,
native applications
on both mobile and desktop,
and even IoT devices
like a Raspberry Pi.
As such, TensorFlow.js
has seen enormous growth
in the last few years.
You're not limited to just
running models in JavaScript.
You can also train them too.
And we've designed the APIs to
be very similar to the Keras
Layers API you saw earlier.
So your knowledge in
model architecture design
and programming skills can
be reused within the browser.
We also make use
of the GPU where
appropriate to speed up training
and inference in the browser.
We see a lot of companies
using TF.js in production.
For example,
IncludeHealth use it
to enable remote physiotherapy
at scale while maintaining
the user's privacy.
Here we see MoveNet single
person detection model.
We also just recently launched
a new multi-person detection
model of MoveNet.
This model can not only
detect multiple people,
but also track their movements.
As shown here, even if two
people exchange positions
the model can still identify
whether the person who moved
positions is the same person.
For more inspiration, check
out what the TensorFlow.js
community has made by searching
#MadewithTFJS on social media.
We've just seen deployment
of models to the browser.
But, of course, there
are many other places
you can deploy them, such
as to mobile devices.
We're going to continue to see
ML move from a niche market
to being the building
block of everything we do.
And I believe Google is
on the path to delivering
the technology that will
shape so many products
and research in the future.
With that, I'm going
to turn it over
to Kemal to discuss how we
help you be ready for mobile,
microcontrollers, and
other edge devices.
KEMAL EL MOUJAHID: Hi, my
name is Kemal El Moujahid.
And I'm a product director for
machine learning at Google.
While the number of edge
devices continues to increase,
they're also becoming
more powerful,
making it possible to build
ML powered apps that we would
have never imagined before.
And we want to put this
power in your hands,
no matter where you are
in your developer journey.
Whether you're an iOS
or Android developer,
we want to give you the
tools to be successful.
The first place to
start with is ML Kit.
For common ML use cases, ML Kit
offers easy to use, production
ready turnkey APIs.
From barcode scanning to
language identification,
ML Kit offers 13 unique
cross-platform APIs
which run well on both low
end and high end devices,
and are completely free to use.
We're continually
adding new APIs
and updating existing ones.
So you can be assured
that you will always
have the latest and greatest
that Google has to offer.
For developers with
more custom needs,
we provide TensorFlow Lite,
Google's open source ML
framework for on
device inference.
TF Lite is used
widely for powering
on device ML in Google apps, as
well as popular external apps
on more than 4 billion devices.
In addition to model
inference, we recently
launched on-device
training in TF Lite.
You can now train your TF Lite
models on mobile and other edge
devices just as you would
train your TensorFlow
models on server.
On TF Lite we continue to work
on making performance better.
XNNPack, a library for
faster floating point ops,
is now turned on by
default in TF Lite,
allowing your models
to run on an average
2.3 X faster on the CPU.
But once you've trained
and optimized your model,
the next step is to distribute
it as widely as possible.
To help with that, we're
adding the TF runtime
to Google Play services
so you don't have
to bundle it with your app.
This means no additional
APK size increase.
Plus, you get automatic
future updates.
Many apps have already deployed
this feature in production.
If you're interested
in trying it out,
you can sign up for an
early access program.
Attend the on-device ML
talk later to learn more.
And finally, you can now run
the same models in the browser
thanks to TensorFlow.js.
This means they work on
feature phones, as well
as desktop Electron apps,
or anywhere else JavaScript
can run.
This lets you use the
same model across billions
of mobile devices
running Android, iOS,
Linux, and other edge devices.
Let's take a look at an example
of an app powered by TF Lite.
Lookout on Android helps people
get details about objects
and text around them.
This is especially
important to help
people who are
blind or low vision
complete daily tasks
faster and more easily.
Lookout recently introduced
a new mode, Food Label.
With Food Label you can
quickly identify packaged foods
by pointing your phone's
camera at the label.
Lookout will guide you to
position the food product so
that it can be
properly identified
through its
packaging or barcode,
helping a user distinguish
between, say, a can of corn
and a can of green beans,
which would both feel
identical to the touch.
Lookout's different modes,
including its object classifier
and OCR models, are
powered by TF Lite.
We also know that you need to
run your models across vastly
different hardware, from the
latest and greatest phone, all
the way to tiny
microcontrollers.
There are now
hundreds of billions
of microcontrollers
in embedded systems.
And with TF Lite
for microcontroller,
you can run ML models
on microcontrollers
with only a few
kilobytes of memory.
So for now we've talked
about a simple workflow,
from building to
deploying a single model,
but when we move
to production ML,
we discover that workflows are
typically more complicated.
And there is a lot more that
is needed to be successful.
At Google we've measured
the ML model code
to be something like 5% of the
total code necessary to train
and deploy a model
in production.
All the other tools
and infrastructure
that you see here are critical.
Now, how do you manage
such a large operation?
With machine learning
operations or ML ops.
ML ops is an ML engineering
culture and practice
that aims at unifying ML
system development, dev,
and ML system operations, ops.
Practicing ML ops means that
you advocate for automation
and monitoring at all
steps of an ML system,
including integration,
testing, releasing, deployment,
and infrastructure management.
Like DevOps, we also need
processes and infrastructure
for continuous integration
and continuous deployment.
For that, we use TFX,
an open source solution
for the entire problem of
machine learning operations,
from storing and preprocessing
data, to model validation.
Let's take a look at an example.
Open X operates the world's
largest independent advertising
exchange.
At a basic level, the
exchange is a marketplace,
connecting tens of
thousands of top brands
to consumers across the
most visited websites
and mobile apps.
They used a TensorFlow
serving deployment
running on Google
Kubernetes Engine,
serving 2.5 million prediction
requests per second,
under 15 milliseconds each.
They've been able to make
dozens of improvements
to their model,
everything from changing
the architecture of
the original model,
to changing the way certain
features are processed
without support from any
other engineering team.
Now, as always,
with Google tools,
we're working hard to make
AI fair and responsible.
To that end, we've integrated
the fairness indicators
and the model cart
toolkit directly into TFX.
I'm also excited to announce
the 0.4 of our Language
Interpretability Tool, or LIT.
LIT is an open source
platform for visualization
and understanding of MLP models.
This new release includes new
interpretability techniques
like TCAV, Targeted
Concept Activation Vector.
TCAV is an interpretability
method for ML models
that shows the importance
of high level concepts,
for example, color, gender,
race for a prediction class.
Now, say that you're looking
for a managed system for ML ops.
Vertex AI is a new
managed machine
learning platform available
through Google Cloud
to help you with TFX at scale.
Vertex AI has tools that span
the entire developer workflow,
from data labeling,
notebooks, and models
within a single
console experience,
to prediction tools and
continuous monitoring.
All these offerings
may be familiar to you.
But what really distinguishes
Vertex AI is the introduction
of new ML ops feature.
For example, Vertex AI
gives you the flexibility
to use AutoML, which automates
model training so you can build
sophisticated models faster
using tabular text, image,
or video data, as well as custom
model training and explainable
AI.
And we've seen
exciting user adoption.
Since we announced general
availability in May 2021,
the volume of training
has increased by over 70%.
The volume of
predictions has doubled.
The number of GPU hours
for prediction has doubled.
And the total volume of
API requests has tripled.
Let's take a look at an example.
Waze is the world's
largest community based
traffic and navigation app.
As part of its offering,
it lets advertisers
put their businesses
on the Waze map.
Waze approaches improving
ad delivery as a learning
to rank ML problem.
They ranked candidate ads to
maximize the likelihood of ads
to be displayed on
the mobile client.
They used Vertex AI suite to
train and deploy a TF model
and easily integrate with
the rest of their data stack.
Waze has seen great
results, up to 19% lift
in pins displayed per
session in large markets.
Vertex AI delivered
low latency predictions
within their
performance parameters,
and helped smooth
scaling of resources
as ad traffic changed
through the day thanks to CPU
based autoscaling.
Data scientists at Waze noted
that using TFX and Vertex
AI for things like
data validation,
transform deploy to
data flow, monitoring,
et cetera felt like magic.
All these ML frameworks
and tools originally
came out of research.
And we're making them accessible
and easy to use for you.
Because ML is such
a fast moving field,
it's very important
for us to keep
helping researchers push
the state of the art
and keep flowing their
innovation into our ecosystem.
So I'm going to turn it over
to Laurence to share some
of our newest research tools.
LAURENCE MORONEY: Hi, my
name is Laurence Moroney.
And I lead AI and ML
advocacy here at Google.
Let's take a look at how
the Google ML ecosystem
tools are used to push
state of the art research.
And we'll start with how we
can do research at scale.
The Google TPU is
a piece of hardware
that's managed in the
cloud, and it deeply
integrates with TensorFlow.
It's designed from the ground up
to accelerate machine learning
and deep learning workloads
for both training models
and performing
inference with them.
With pods, TPUs reach
their full potential.
And when hundreds of
these individual boards
are connected together,
they form AI supercomputers.
To learn how TPUs ran novel
edge based graph convolutional
neural network
architectures to design
later generations of
TPUs make sure you
watch the Chip Floorplanning
with Deep Reinforcement
Learning talk a little later.
For your research friendly
framework needs, we have T5X.
T5X is a modular
composable framework.
It's used for high
performance, configurable,
self-service training,
evaluation, and inference
of sequence models.
And here's an example
using language
at many different scales.
The model has over 11
billion parameters.
And it was trained off of the
colossal clean crawled corpus
data set.
That's just one example
of how our tools are being
used in research,
and we welcome you
to try it for yourself through
the TPU research cloud program.
You can join the thousands
of other researchers
who've tried cloud
TPUs for free.
Our motivation is simple.
We believe there should
be no blockers in talent
being equally distributed
throughout the world.
And we want to make sure that
everyone has the opportunity
to access state of the
art hardware and software
for researching cutting edge AI.
And there's so much going on
in cutting edge research right
now.
But I'd like to highlight
one project that I
find particularly appealing.
It's called Pathdreamer.
And its initial goal is
in indoor navigation.
Consider what happens when
you walk into a dwelling
that you haven't
previously been in.
From visual cues around
your environment,
you can likely infer
which rooms might
be the kitchen, or the
dining room, or a bathroom.
But how would you train
an autonomous agent
to recognize those same cues?
Given just a single
observation of an environment,
Pathdreamer synthesizes
high resolution,
360 degree observations
up to 20 feet away
from the original location,
including around corners.
And here we can see video
that simulates reconstruction
of the interior space
of an apartment that
was previously unseen.
And the model infers what
various rooms may look like,
even though they were
hidden around corners
in the original picture.
It's been built using
TensorFlow and Keras,
and it's fully open sourced,
and available for you
to explore today on GitHub.
While Keras has a
great high level API,
sometimes you need to go
that little bit lower level.
And with that in
mind, we have JAX,
a Python library designed
for high performance
numerical computing.
Here's a sample of JAX code.
And you can see, it's just
ordinary Python plus NumPy.
But what's really
nice about this
is that your code will execute
on a variety of accelerators.
So you can run your JAX
code on a GPU or a TPU
without further modification.
And JAX libraries are
compatible with TensorFlow.
So for example, you can use
TensorFlow data pipelines
to input data into JAX.
If you want to learn more,
check out the talk, JAX,
Accelerating Machine Learning
Research on this stream later
today, or on demand at
youtube.com/TensorFlow.
You probably have heard
about the amazing work
DeepMind did with research
into protein folding that
stands to transform biology.
But did you know that they used
TPUs and JAX to achieve this?
So let's hear from Tim
Green at DeepMind who's
going to tell us all about it.
TIM GREEN: Hi, I'm Tim.
I'm a research
engineer at DeepMind.
And I'm going to tell
you today about how
we've used TensorFlow and JAX
to solve one of the biggest
scientific questions in
biology, how proteins fold.
Proteins are molecular
machines essential to life.
From our hair to
our immune system,
they're involved in every
aspect of our bodies.
Proteins are made of chains
of amino acids, which
fold into elaborate 3D shapes.
The exact 3D shape
that a protein makes
is important for
understanding what it does.
And so knowing the
3D shape of a protein
helps us to tackle
diseases like COVID-19.
You might have heard
of the spike protein
of the coronavirus, for example.
Normally the 3D
shape of proteins
can only be determined by
laborious experiments that
can take skilled researchers
a long time to carry out.
For 50 years it's been a
standing scientific grand
challenge to predict
protein structure
from their amino acids sequence.
AlphaFold changes this.
How does AlphaFold work?
AlphaFold uses deep
learning to predict the 3D
shape of proteins from their
sequence of amino acids,
almost like translating
between languages.
And as with many
modern language models,
it makes use of
attention mechanisms,
along with a number of other
neural network architecture
innovations.
If you want to know more, please
read our blog posts and paper.
AlphaFold makes predictions at
such a high level of accuracy
that in most cases it
matches experiments.
How do we know this?
So CASP is a blind assessment
of protein structure prediction.
The chart you are looking
at shows the performance
of the best entrant for
CASP from 2006 to 2020.
You can see the impressive
progress in the last few years.
After the 2020
assessment, the last line,
AlphaFolds recognizes
a general solution
to the protein folding problem.
Our research that led to the
development of AlphaFolds
involved trying many
different ideas.
The Google ML ecosystem allowed
us to iterate very quickly
and then to scale
up our research.
AlphaFold has trained
up 128 TPU V3 cores
over a couple of weeks.
This is actually quite cheap
compared to some modern models.
TPUs are very fast.
And training up
pod scale allowed
us to do things like running
with single example per core,
which is necessary due to
the high memory consumption.
When it comes to using AlphaFold
to predict a protein structure,
a prediction can
be made in seconds
to minutes on a TPU or a GPU.
This opens the door
to making predictions
at a scale that matches the
enormous number of known
protein sequences.
We developed AlphaFold
with TensorFlow 1.
As we were migrating
away from TensorFlow 1
we decided to use JAX.
JAX is also what we used when
we open sourced the model
for the whole world to use.
We also have a
version of AlphaFold
that runs using
Colab, allowing anyone
to run it using minimal setup.
So how can you access AlphaFold?
After sharing an
outline of our advances
at the CASP 14 conference
last December, in July
we published a detailed
paper describing our methods.
With this paper, we open
sourced AlphaFold within JAX
with the full model
parameters and code.
This allows anyone to run
AlphaFold on the protein
sequence of their
choice, and has already
been used by a huge
number of researchers
to study their proteins.
We also released predictions
for all human proteins,
plus 20 other biologically
important organisms
in the AlphaFold protein
structure database.
This is freely available
for anyone to use.
We're thrilled to see the
impact AlphaFold has already
had, excited to see
where people use it next,
and are looking forward to
the future advances enabled
by the Google ML ecosystem.
JEANINE BANKS: Hi,
I'm Jeanine Banks,
vice president of Google's core
third party developer products.
We're humbled that you
trust the Google ML
ecosystem with your
experiments and your products.
Which brings me
to the foundation
of what we're building.
And that's you, our community.
This past year, we found
ways to connect virtually all
around the world, like at
Google I/O, where you all
started over 9,000 chats in
the TensorFlow Adventure Dome.
We love connecting with you.
And we can't wait
until we can all
get back on the road together
for in-person events.
Until then, we want to
continue to provide ways
that we can all connect safely.
A question we often
get is, how do I
get started with the
community, or meet other people
to start projects?
Well, the first thing
you can do is reach out
to our group of 175 Google
Developer Experts for ML.
They give tech talks.
They organize workshops.
And they can help
you get started.
I want to give special
recognition to Sayak Paul,
a Google developer expert who
has been very active in our ML
community.
He's made contributions
to models and tutorials
in TensorFlow Lite and for dual
deployments on the Vertex AI.
Thank you to Sayak and all
of our Google developer
experts for their
amazing contributions.
Another great way to contribute
is by joining a TensorFlow user
group.
We have 87 user groups
spanning 37 countries.
And they hosted great
activities this past year,
like our group in Santiago,
Chile, which recently
gave a TED Talk
on how to generate
a model capable of being
in the top 1% of the Kaggle
leaderboard.
The community also
continues to grow
through the official
TensorFlow forum
that we launched
earlier this year.
We're delighted
to see developers
sharing ideas and best
practices with each other.
To date we've seen
over 2,000 users
join, making over 9,000 posts.
We'd love to have you
join the conversation
at discuss.TensorFlow.org.
We want to make the
community as vibrant
as possible, where
anybody is welcome,
and anyone can gain ML skills.
That's why we
invest in education,
and I'm extremely
proud to see we
passed the 1 millionth learner's
mark across our online courses.
Visit TensorFlow.org to explore
books, courses, and videos
to help you improve your
knowledge of machine learning.
We also have an annual
program of faculty awards
to support machine learning
courses, diversity,
and inclusion.
This past year, we gave
grants to 40 universities
in 20 countries to
help them develop
new machine learning costs.
When you're ready to showcase
your proficiency in building
models, you can take
our certificate exam.
A year ago we launched
the TensorFlow Developer
Certificate.
And since then, more than
4,000 developers from over 80
countries have been certified.
You can share your
expertise with the world
by displaying your certificate
on social media, like LinkedIn
and GitHub, or the TensorFlow
Certificate Network.
We're hearing great
feedback, including
from Tia, a certified
TensorFlow developer who
reentered the
workforce after years
of being a stay at home parent.
If you're looking to
apply your ML skills
or pick up a new
skill, we highly
encourage you to join Kaggle,
the world's largest data
science community.
It's best known for
its world class machine
learning competitions.
But Kaggle offers a
wealth of resources too,
for data scientists
of all levels,
including 110,000 public data
sets, and 500,000 data science
notebooks, also dozens
of free micro courses.
Be sure to check out this
notebook written by Kaggle
grandmaster, Andrada Olteanu.
She details all of the practical
resources and routines that
have helped her
grow from a beginner
to a grandmaster in
less than two years.
It's helpful for
learners at all stages.
Many of you have made our ML
tools what they are today.
And on behalf of the
ML teams at Google,
I want to say thank you.
As you saw today,
there are many ways
you can connect,
learn, and contribute.
We work together with
members of the community
so you all can build and
have an impact on the world.
And we hope what you
saw here today will
help you build the best ML
powered apps, or further
your research, no matter
what stage you're at.
Stay tuned for other
great talks today,
including a deep dive on
what's new in on-device machine
learning, an overview of
the responsible AI toolkit,
and a closer look
at TPUs, and how
edge impulse makes ML accessible
to embedded engineers.
Make sure you ask
all of your burning
questions in our live chat.
We want to hear from you.
Thank you all again for helping
build our wonderful machine
learning community.
It's your stories
from the community
that continues to inspire us.
Let me share one of my
favorite stories with you.
At Huridocs, machine learning
has become a vital tool
in the advancement of accessible
human rights information.
Take a look.
NICOLETTA ZAPILLE: UPR Info is
a non-governmental organization
that works to promote human
rights through the Universal
Periodic Review.
Every five years
UN member states
make recommendations
to other states
to suggest how to
improve the human rights
situation in their country.
And with 42 states being
reviewed each year,
there could be up to
8,000 recommendations that
need sorting.
This is where UPR
Info's team comes in.
We extract the recommendations.
And one by one,
categorize them by topic.
Having access to information
through our database
empowers human rights advocates
to put pressure on the states
and to make them accountable on
their human rights obligations.
All this work happens
with a very small team.
And the amount of
information is huge.
Let's say that we
felt the pressure.
[SLIDES CLICK]
FRIEDHELM WEINBERG: Huridocs is
a non-governmental organization
that was founded in 1982.
Using the power of
machine learning,
we're helping UPR Info to
better organize and mobilize
the thousands of
UPR recommendations
that are being made.
And coincidentally, we just
work right across the hall
from each other.
GRACE KWAK DANCIU:
We started off
with Bert, which is a
pre-trained natural language
processing model that's
available for anyone
on TensorFlow Hub.
That's really helpful, because
a ton of work and processing
power went into
creating this model.
And we could build on top of
it for our specific use case.
We used TensorFlow to
build a classifier that
would take human
rights related text
and assign human
rights topics to them.
And we had 60,000 hand labeled
examples from the UPR Info team
that we could use
as training data.
FRIEDHELM WEINBERG:
Right now UPR Info
has to explicitly accept
or reject a suggestion.
And that information is then
used to retrain the algorithm.
So over time, those
suggestions get better.
GRACE KWAK DANCIU: Updating
the database after each cycle
has gone from taking two to
three months to taking one
week.
NICOLETTA ZAPILLE: The
first amazing feeling
was to catch up
with the backlog.
That really was a good feeling.
This is so much more than
sorting out or reviewing
recommendations.
It's really providing tools
for human rights advocates.
GRACE KWAK DANCIU:
It's really been
amazing to see how a technology
like machine learning
can be applied to human
rights and have a real impact.
FRIEDHELM WEINBERG:
There's always more to do.
There's so much more to do.
There's so much more potential
for positive human rights
change.
So that's also what's next.
What more can we do?
That's the motivation, frankly.
YU-CHENG LING: Hi, my
name is Yu-Cheng Ling,
and I am a software
engineer in Google.
Today we are excited to
talk about on-device machine
learning and share some
of our newest updates.
But what is on-device
machine learning?
Google offers a range of
machine learning solutions
to enable running TensorFlow
models on your mobile phones,
including Android and iOS,
web, and even microcontrollers.
Turnkey solutions are easy
to use production ready APIs
to tackle common
machine learning tasks.
You should start with
turnkey solutions
and see if they meet your needs.
For more custom use cases, we
help you to train and optimize
your model, integrate
it in your app,
and deploy it in production.
We offer an easy to use
production APIs through ML Kit
SDK as turnkey solutions.
ML Kit offers a variety of
vision and natural language
APIs covering common cases,
such as barcode scanning
and language identification.
We are continually
adding new APIs
and updating our existing ones.
So you can be assured
that you will always
have the latest and
greatest features
that Google has to offer.
Let me go over a few
recent updates in ML Kit.
You can recognize
an [INAUDIBLE] test
from images using ML Kit's
text recognition API.
We added new language support
for Chinese, [INAUDIBLE],,
Japanese, and Korean scripts.
Text recognition now also
offers deep paragraphing support
to group texts into
meaningful blocks.
We have also
improved the quality
of ML Kit's barcode
scanning, including record,
[INAUDIBLE] latency, low
quality image tolerance,
and bounding box stability.
The ML Kit pose
detection API helps
detect human poses in real
time from a continuous video
or a static image.
We have updated the
pose detection model
to support 3D points
for depth analysis.
To support language
identification,
we added an unbundled
version backed by Google
Play service on Android and
reduced the impact on APK size
by about 500 kilobytes.
If you are developing
mobile apps,
I strongly recommend
you to try out ML Kit
and add some cool
machine learning features
into your app.
For users in need
of custom solutions,
we offer both a
pre-trained model and tools
to train, optimize,
and deploy your model
using TensorFlow Lite.
Let me quickly introduce
TensorFlow Lite.
TensorFlow Lite is Google's
machine learning framework
for mobile devices and
other edge devices,
such as browser,
microcontroller,
and it also runs on desktops.
A lot of features of ML
Kit that I just mentioned
are built on top
of TensorFlow Lite.
In the rest of the talk we will
share many recent improvements
of TensorFlow Lite.
The first thing I want to talk
about is on-device training.
TensorFlow Lite was
originally designed
for an on-device machine
learning inference framework.
I am proud to share
our recent progress
for the on-device
training support.
Now you can train your
machine learning model
on mobile and other edge
devices using TensorFlow Lite.
It enables important use cases
like on-device personalization
to customize models based
on individual user needs
without sending the
data to the server.
It is also an important
foundation piece
for advanced techniques
like [INAUDIBLE] learning.
To demonstrate the on-
device training capability
we built an Android demo app
for image classification.
In the app we can take pictures
from four different categories,
perform training, and use the
app to classify camera inputs.
Everything happens on device.
Let me show you how it works.
To use the demo app to train
an image classifier on device
we recommend to take
at least 10 photos
for each of the four classes.
In this demonstration, I want
to build an image classifier
for orange, plant, water
bottle, and scissors.
I'm taking a few photos on the
orange with different angles
using the triangle bottom
for the first class.
The same for the plant
with the circle pattern
for the second class.
Water bottle with the cross
button for the third class.
Scissors with the square
button for the fourth class.
After collecting
the training data
I can tap the Train button.
You can see that those is going
down with the training process.
After training is done I can
switch to the inference mode.
If I point the
camera to the orange,
the model knows it is the
first class by highlighting
the first button.
Plant is the second class.
Water bottle is the third class.
And scissors is
the fourth class.
The demo app we built
is pretty simple,
but you can use your creativity
to build other cool apps using
the on-device training feature.
The demo app and
its source code can
be found on the TensorFlow
Lite On-device Training page.
To perform on-device
training we need
to eschew different
functionalities
like training and inference.
To achieve this you can
build a TensorFlow model
with multiple
TensorFlow functions.
For example, in our demo app,
we defined a train function
that trains the model with
training data, an infer
function that invokes the model
inference, save and restore
functions that
saves and restores
the trainable ways
with the [INAUDIBLE]..
After that, you can convert
your TensorFlow model
to TensorFlow live format,
integrate the model
into your iOS and Android
app, invoke the model training
in the app.
That's very similar
to what you would need
to do with the model inference.
You can also find a complete
Colab example in the TensorFlow
Lite On-Device Training page.
Indeed, here the
device training is
supported by implementing a
bunch of low level features.
For example, we
support the variables
to be able to store
trainable weights.
We supported multiple
signatures to be
able to run different
functionalities like training
and inference.
We supported gradients for
computing the [INAUDIBLE] pass,
and optimizers to
update the weights.
The nice thing is many of these
features are useful [INAUDIBLE]
the on-device training as well.
You can now use variables for
[INAUDIBLE] for inference,
or ship a model with multiple
different functionalities
with the multiple
signature support.
Currently some of
these features are
achieved by reusing
TensorFlow kernels
with the select
TensorFlow apps feature.
And we are working on supporting
more features in TensorFlow
natively to deliver an even
better performance and smaller
[INAUDIBLE] size.
Thank you, and I will
hand over to Arun
to talk about more
exciting updates.
ARUN VENKATESAN: Thank you, YC.
Hi, everybody.
My name is Arun Venkatesan.
And I'm the product manager
for the TensorFlow Lite
and the TensorFlow model
optimization toolkit teams.
We have made it easier
for Android users
to deploy TensorFlow Lite.
To reduce Android app size and
enable automatic future updates
we announced at Google I/O
'21 that the TensorFlow Lite
runtime is being added
to Google Play services
on Android devices.
We are happy to share with
you that a few Google apps
and frameworks are already using
this feature in production,
while others are in the
process of integrating with it.
You can now sign up for
the early access program
to use this feature in
your own Android apps.
You will still continue
to have the option
to bundle TensorFlow
Lite inside your app.
Speaking of deployment, we
had earlier announced support
to run TensorFlow Lite
models on the browser
without having to
further convert
the model to another format.
This enables code and
model pipeline to use,
and eliminates the need to
maintain a separate JavaScript
code base.
We hope that you are already
leveraging this feature.
We recently extended support to
also run quantized TensorFlow
Lite models on the browser,
resulting in improved latency
and smaller model size.
We will talk more about
quantization in a later part
of this presentation.
Machine learning models
running on-device
need to be highly performant in
order to deliver a great user
experience.
We continue to offer
the best out of the box
performance on CPUs, as well
as peak performance using
on-device accelerators.
XNNPack is a highly
optimized library
that enables faster CPU
performance of TensorFlow Lite
models via a highly optimized
implementation of floating
point ops for Arm,
WebAssembly, and x86 platforms.
XNNPack is now
turned on by default,
starting with
TensorFlow version 2.7.
XNNPack also supports quantized
TensorFlow Lite models,
resulting in an
approximately 30%
speed up on Arm 64 mobile
devices, and a 5X speed up
on x86 laptop and
desktop systems
for certain image models.
Mobile and other
edge devices have
limited computational
resources, memory, and storage.
So it's important to keep your
application resource efficient.
You can optimize your
TensorFlow Lite models
to be smaller, run
faster, and be more energy
efficient using techniques and
tools in the model optimization
toolkit.
Pruning is one such technique.
Pruning removes
unnecessary connections
between layers of a
neural network that
produce a sparse model.
The pruning APIs have
extensive functionality
that provide you with tools
for model manipulation.
These enable control of multiple
aspects of model pruning,
such as how and when
to prune, and also
track the progress of pruning.
A new extension
to the pruning API
now allows you to specify
which parts of the model
to apply the pruning policy to.
Let's see this new
feature in action.
You heard about XNNPack
earlier in the presentation.
The example here shows how
to apply selective pruning
to the parts of the model
that can be accelerated
using XNNPack, while leaving
the rest of the model untouched.
Such selective pruning
allows an application
to maintain model
quality while targeting
parts of the model that
can be accelerated.
Weights and activations occupy
the majority of model size
and are usually represented as
32-bit floating point numbers.
Quantization is a technique
that represent weights
and activations
as 8-bit integers
or 16-bit floating point
numbers to help reduce the model
size with improved latency.
However, due to the
reduced precision,
quantization can potentially
result in an accuracy loss.
We have released
a new set of tools
to help you easily debug the
accuracy loss due to post
training quantization.
The Quantization
Debugger makes it
possible to do quantization
quality metric analysis
of the existing model.
The debugger logs differences
between the float Tensors
and the quantized Tensors
for the same op location,
and emits metrics such
as the mean squared error
between the float and
the quantized results.
You can easily
identify the layers
that have the
largest error metrics
by viewing the quantization
debugger's output.
Once you have identified
the problematic layers
you can skip quantizing
these layers using
selective quantization,
potentially
resulting in a model
with higher accuracy.
In order to run a
model on-device,
you first need to convert
the TensorFlow model
to the TensorFlow Lite
flat buffer format.
Further, to improve the peak
performance of your models,
TensorFlow Lite's delegates
provides a convenient mechanism
for hardware acceleration
of TensorFlow Lite models
on accelerators such as the GPU.
In other words, not only do the
ops in your TensorFlow model
need to be compatible
with TensorFlow Lite,
but your model also needs to
be compatible with the GPU
delegate to be able
to run on the GPU.
TensorFlow Lite's Target
Aware authoring tool
makes it easy to detect
not only TensorFlow model
compatibility with
TensorFlow Lite,
but also tells you
if your model is
compatible with a GPU delegate.
You just need to
add a decorator,
wrap your TF function model
to detect compatibility.
Surfacing both the model and
accelerator compatibility
at authoring time
helps model developers
avoid the extra work due
to the conversion errors,
and also provides an
opportunity earlier
in the model
development lifecycle
to use GPU compatible
ops in the model.
We are now working on extending
the target aware authoring tool
to support other
on-device accelerators.
You can also detect GPU
compatibility post conversion
of your TensorFlow model
to TensorFlow Lite.
Our new model analyzer
tool dumps a helpful log
of your model structure,
and highlights potential GPU
incompatibilities.
State of the art models
and reference app
help you get started
with TensorFlow Lite.
We have new models and reference
apps to share with you.
We recently released the MoveNet
model, a state of the art
pose estimation model
that can detect 17 points.
We also published reference
apps for reinforcement learning
and optical character
recognition.
As a recap, we learned about
some of the new updates
to our ready to use
turnkey APIs in ML Kit.
We saw a demo of
on-device training, talked
about TensorFlow Lite
in Google Play services
for your Android apps,
performance and optimization
updates, including techniques
and tooling, compatibility
tooling, and new state of the
art models and reference apps.
You can learn more by
visiting our website.
Do keep in touch with us via
GitHub and other public forums.
Thank you for your time.
TRIS WARKENTIN: Hi,
I'm Tris Warkentin.
ROBERT CROWE: And
I'm Robert Crowe.
And we're going to be talking
about production ML, TFX,
and what's new.
TRIS WARKENTIN: We're
really excited to share
what we've built
in TFX, as well as
some of what you, the
community, have created.
But first, a little
background for those of you
who might not be
familiar with TFX.
To begin with, as data
scientists or machine learning
engineers, we tend to
focus on our models.
That's natural, because that's
where the magic happens.
But when we move
to production ML,
we discover that there's
a lot more that's
required to be successful.
At Google we've measured
the ML model code
to be something like 5% of the
total code necessary to train
and deploy a model
in production.
But all of the other
tools and infrastructure
that you see here are
critical to success.
To put any piece of
software into production,
to use it in a
product or service,
we need a production
infrastructure and process.
First, we need all the things
that any ML development
needs, along with some
special considerations
because we're working in
a production environment.
But we also need all the things
that any production software
deployment needs.
In other words, in addition
to ML best practices,
we also need to apply DevOps
principles and practices
to machine learning.
We put tons of ML into
production at Google,
and we needed a
strong framework.
So we created TFX.
You've probably been
using it, or rather using
products and services that
are implemented using TFX.
It's used not only at Google,
but across all of the Alphabet
companies.
And since it's open source, it's
available for you to use too.
This is what I call the
Hello World of TFX, which
is the pipeline that you
start with when you just
do a pip install.
The components on top are
a pipeline for training,
and the ones on the
bottom are a pipeline
for running batch inference.
On the right side are
some deployment targets,
like TensorFlow Hub,
JS, and TensorFlow Lite.
Those are for use in a web
browser, mobile application,
or IoT device.
You can also use
TensorFlow serving
if you're going to serve your
model for online or batch
predictions, or vertex
prediction if you need
a cloud based managed service.
ROBERT CROWE: One of the key
aspects of TFX is portability.
We say that TFX runs
just about anywhere.
And here's what we mean by that.
You can run it in different
execution environments,
including just running
an entire pipeline
in a web browser using Colab.
Or on your notebook, or
in a Kubernetes cluster,
or in Vertex AI.
You can orchestrate it with a
wide variety of orchestration
engines, or just plain Python,
including Airflow and Kubeflow.
And you can perform all of
the processing on one machine,
like your laptop or a server.
Or you can leverage the
compute resources of a cluster
by using Spark,
Flink, or DataFlow.
This kind of portability
is incredibly powerful,
because it means that
you're not locked into one
way of running your pipelines.
And you can easily
move pipelines
to different sets
of infrastructure,
scaling up from a web browser
to a huge cluster with almost
no code changes.
OK, so that's TFX.
But what's new?
Well, we've only got 10 minutes.
So let's look at
a few highlights.
First, there's new stuff
in the core TFX SDK,
starting with
conditional control flow.
You can now test and branch,
or run optional components
based on the current state
of pipeline artifacts.
So a good example here is that
if you're training a model
and the accuracy falls
below a certain threshold,
you can optionally run
an alerting component
so that your team is
aware of the situation.
Another great addition to
TFX that a lot of people
have been asking for is
the new Exit Handler.
This allows you
to detect and act
on success or failure of
your pipeline and components,
and run custom code
based on the status.
At the time of this
recording, this
is scheduled to be
available in November 2021.
So it might already
be there, depending
on when you're watching this.
But remember, TFX is
an open source project,
so you can contribute too.
One of the best
ways to get started
is to join our special interest
group, Sig TFX Addons, which
brings together community
contributors to create
new components, tools,
libraries, examples,
and visualizations to
make TFX even better.
Let's take a look now
at some of the work
that the TFX Addons
team is doing.
TRIS WARKENTIN: A great example
is the Firebase Publisher
project, which is
a TFX component
to publish or update models,
usually TF Lite models,
from TFX to Firebase ML.
So you can train TF Lite models
from mobile apps with TFX,
and publish them to Firebase,
where you can dynamically
serve the latest
version of the model,
and update without pushing a new
version of your app to users.
Another great example is
the load testing project.
By adding a load test
component to your pipelines
you can run load tests as
part of training pipelines
before you deploy to production.
That allows you to measure
serving latency under load
to make sure
requirements are met
before it affects your users.
That helps avoid issues early,
before they become fire drills.
You can also use it
during development
to measure speed
improvements in your models.
Yet another great
example that really
shows the flexibility of TFX is
the XGBoost Evaluator project.
XGBoost is a very popular
framework for building gradient
boosted trees.
And this project
creates a component
to add support for evaluating
XGBoost models, which
you can train in TFX.
One of the things that
customers tell us is they
need to be able to train models
in more than one framework.
And this project extends
the standard TFX Evaluator
component, which helps expand
XGBoost support in TFX.
Another exciting
TFX Addons project
is the Feast Example
Gen component.
This project is
creating a TFX Addon
component which allows TFX
pipelines to ingest Feast data
sets.
Feast is the leading
open source feature
store for machine learning.
And it offers low latency online
data for real time prediction,
and offline data for
batch processing,
along with the other
advantages of using a feature
store for your data.
ROBERT CROWE: We'd like
to give a big shout out
to all of the project leaders
in the TFX Addons team,
and especially to the
leaders of the four projects
that we've just mentioned.
AI and machine
learning only advances
through the open
collaboration of researchers
and practitioners worldwide.
And community teams
continue to be incredibly
important to success.
Thanks, and don't
forget, you can help too
by joining TFX Addons.
But if you're just
getting started
in building ML for
production applications,
you can also sign up for a
four course specialization
that we're creating with deep
learning AI and Coursera,
working with Andrew
Ng, who is teaching
the first course in the series.
It covers TFX and
a whole lot more
to help you come up
to speed on the state
of the art of production ML.
For more information, don't
forget to visit our website,
where we have tons of
tutorials, examples, and docs.
And check out these other
resources for even more.
That's our talk.
Thanks for listening.
PARKER BARNES: Hi, everyone.
My name is Parker.
And I'm a product
manager at Google AI.
I'm here with my
colleague, Ludo,
to talk about how you can
build with the Responsible AI
Toolkit.
Today we're going to give
you a very quick overview
of Responsible AI at Google and
the responsibility AI Toolkit.
We'll then illustrate how two of
our tools, fairness indicators,
and the language
interpretability tool
can be helpful in a
real world example.
First, it may go without
saying that AI is transforming
the world we live in.
As creators of
new products, it's
staggering the range of
new opportunities created
by AI across so many
different sectors.
And yet, for AI to be
truly useful and valuable,
it needs to be built, tested,
and operated responsibly.
At Google, we have a set
of principles that guide
our approach to responsible AI.
These include the seven
listed on this slide,
as well as four applications of
AI that Google will not pursue.
We've developed the
Responsible AI toolkit
to operationalize these
principles at Google.
We open sourced the toolkit
a couple of years ago
and have been adding
resources to it ever since.
To better contextualize
the toolkit let's
take a look at a typical
machine learning workflow.
Building AI responsibly means
answering hard questions
at each step in the life
cycle, from problem definition,
through to model training,
testing, and deployment.
While the specific
questions that you ask
depend on your own
unique principles,
the tools we've developed
are designed to be flexible
and to help you answer them.
Our Responsible AI
Toolkit is a collection
of technical resources to help
you and your team at each stage
in the life cycle.
We encourage you to take a
deeper look at these resources
online.
They include Colab
tutorials, documentation,
and user guides, which can be
accessed via the QR code shown
on this slide.
Today we're going to
zero in on two tools that
can help you evaluate and probe
your models for unfair bias.
We'll introduce the tools in
a simple, real-world example,
which we spoke about in our
previous talk at Google I/O
this spring.
Imagine that you are the owner
of a new restaurant chain.
Here's the stylish interior
of one of your restaurants.
After eating, your customers
leave online reviews
about their experience.
As a diligent owner,
you decide to use
ML to analyze customers' online
comments and feedback to assess
customer satisfaction and help
you come up with new ideas
for how you can improve
your restaurants.
As a first step, you decide to
build a sentiment classifier
that assesses the degree
to which each customer's
comment is either
positive or negative
toward their experience
at your restaurant.
The inputs to the
sentiment classifier
are the text body of the
review, the review metadata,
and any associated images.
We have two main objectives.
First, we want to ensure that
our sentiment classifier has
high overall accuracy
for predicting sentiment.
Second, we want to make sure
that the classifier does not
disproportionately
misclassify reviews
based on sensitive identity
characteristics that may
be mentioned in the reviews.
For example, like
language, gender, or race.
We'll first demonstrate how
you can use fairness indicators
to perform a disaggregated,
or sliced evaluation
of your model's
performance to check
whether your model is achieving
your two main objectives.
But first, it's worth briefly
mentioning why analyzing models
for fairness is really hard.
First, fairness is highly
contextual, and almost never
reducible to a single
statistical definition.
In any real world setting,
it's important to acknowledge
and embrace this fact, and
explore multiple qualitative
definitions and statistical
measures of fairness.
Second, fairness
testing requires
iterating with a large
number of stakeholders.
So you need mechanisms
that make it
easy to share the results
of evaluations with others.
Thirdly, many real
world ML systems
operate at very large scale.
So you need tooling
that allows you
to evaluate your model
on large data sets
and as part of
repeatable pipelines.
Fairness Indicators can help you
solve some of these challenges.
First, Fairness
Indicators makes it
easy to zero in
on the performance
of your model for specific
subgroups or demographics.
It also comes preloaded
with some of the most
common fairness
metrics, which means
you don't have to commit to
a single definition up front.
Second, Fairness Indicators
provides an interactive UI
and dashboard for
exploring and sharing
the results of your
analyses with others.
Here is a snapshot of the
Fairness Indicators dashboard.
Within the dashboard you can
select one or multiple fairness
metrics, select which slices or
subgroups you want to review,
set different
decision thresholds,
and review and share output
as charts and tables.
Lastly, Fairness Indicators
is scalable and flexible.
It can be run as part of
TFX's evaluator component,
as a TensorBoard plug-in,
or as standalone binaries.
It also supports both TensorFlow
and non TensorFlow models.
With that background, let's
return to our restaurant review
classifier example.
We can use Fairness
Indicators to evaluate
our first objective,
which is accuracy.
We use the presence of
gender identity terms
as the category in question
for our sliced analysis.
We wouldn't want the
presence of these terms
to unduly influence the
outcome of the classifier.
Fairness Indicator shows us
that reviews containing terms
implying certain
genders actually
have a lower accuracy than other
reviews, which is concerning.
Now, we turn to our
second objective,
which is to minimize any
differences in error rates,
and specifically
false positive rates.
This graph shows us
that reviews containing
terms that imply
certain genders tend
to have a higher FPR
than other reviews,
especially for the male gender.
This means that
they are far more
likely to be classified
incorrectly as positive, which
could lead to biased, misleading
conclusions for your assessment
efforts.
Now that we've identified
a couple of issues
with our model, we can take
this slice of our data,
in this case customer reviews
that contain gendered terms,
and we can use another tool
from the Responsible AI
Toolkit, called the
Language Interpretability
Tool to do an in-depth analysis
of the model's performance.
Ludo, over to you.
LUDOVIC PERAN:
Thank you, Parker.
Now we can use the Language
Interpretability Tool, LIT,
to further investigate
these findings.
LIT is complementary
to Fairness Indicators
and serves a different purpose.
It's not about inspecting
average global metrics
on the whole data set.
But it allows you
to build intuition
about the mobile
prediction patterns
by interactively
inspecting the model,
often starting at
data point level.
Since the last demo at Google
I/O we have made good progress.
And we want to share
that with you today.
Let's keep building
on our I/O example.
This is the LIT UI, with
all modules visible.
It can be intimidating, but
the tool is fully modular.
And you can choose different
views with the module you need.
Think of it as a toolbox
to inspect your model.
For clarity, we'll only
zoom on specific modules
in the next slides.
The data table module displays
your data set in the UI.
And here we see all
the restaurant reviews
in your data set.
LIT also allows you
to tweak the text
and evaluate examples side
by side, a concept known
as counterfactual testing.
Let's try this for the
data point selected here,
a seemingly sarcastic,
negative review.
Let's change the gender
reference in this sentence.
In LIT you can edit the data
point directly in the UI
and add it to the
data set, and compare
the old and the new versions.
The bar graph shows that
the classifier considers
a sentence with waitress
more likely to be
negative sentiments than the
identical one with waiter,
even though these words
probably shouldn't bear impact
on sentiment.
LIT allows us to
use [INAUDIBLE] even
it was live or
integrated gradient that
indicate how much
each words account
for the model's prediction.
The identity term
waiter and waitress
are darker in color than
the surrounding words,
indicating that they have a
stronger prediction power.
This could be one of the hidden
causes behind the disparities
we surfaced in
Fairness Indicator.
But we can do further probing.
Beyond just comparing
single points,
you can edit a batch
of data points.
The model seems to be influenced
by the gendered noun waitress,
but does it react to
gender pronouns too?
We'll change all the
masculine personal pronouns
to feminine ones.
We can now look at the 19
pairs of similar datapoint
with different pronouns.
We notice that on
sentences with many tokens,
the prediction doesn't
change based on pronoun.
But when the sentence
is short, like this one,
with only three tokens
including the pronoun,
the model does make a difference
between the two examples.
This might be an
interesting insight when you
test this model before launch.
So you think of testing
with shorter sentences.
So far we looked
at how to interpret
a model using an input
feature like the non waitress.
But we know that there are
many other words that represent
this notion of gender.
What if you're more
interested in how
this notion of
female gender concept
influences the model in general?
TCAV is a method
developed at Google Brain
by Ben Kim and others
that can help you do this.
TCAV shows how
important a high level
concept like the female gender
is for prediction class.
And you can do
this after training
without even thought of
the concept of gender
before training.
To capture the impact
of a gender concept,
it creates a Concept
Activation Vector, CAV,
from our selection
of data points
exemplifying that concept.
Here, for instance,
we can group together
sentences with female
pronouns, as well as
various female oriented nouns.
It is important
to carefully think
of how you build
your vector for it
to be representative of
this high level concept you
are trying to capture.
Once you have this vector,
we can create a CAV score.
Here we can see that the concept
of female gender represented
by my CAV is indeed influencing
my model, because the CAV
score is high and
statistically significant.
Look at the blue scroll bar.
It's below the gray baseline.
This means that
the female gender
is contributing to the negative
sentiment classification.
TCAV is a powerful
method when your goal
is to get intuitive
and global behavior
characteristics of a model.
To build relevant
CAVs we recommend
that you read our website
and documentation.
We are pleased to announce
that we are releasing today
our new version of LIT.
On top of TCAV, it includes
some preliminary features
for image and tabular data.
And you can find more
in our documentation.
You can use LIT with
your own models and data
by installing the
pip package lit-nlp.
LIT is framework agnostic.
You can use it in TF
or other frameworks,
and in many surfaces,
like notebooks, or running
as a standalone server.
LIT is also model agnostic.
It works with classifier
regression, seek to seek,
and other model tasks.
To set it up with
your own model,
you need to define a lead
model that can make predictions
based on a given input, which
is defined in the input spec.
Which in this case includes a
text review and the ground true
sentiment level.
The output spec define the
model output, which in this case
are classification scores,
with 0 for negative sentiment,
and 1 for positive sentiment.
For your data, LIT has been
primarily designed for text.
But we have
preliminary features,
as I just said, working now
for image and tabular data too.
To set it up with
your own data, you
need to create a LIT
data set class that
loads your data into
memory, and specify
the name and types of all the
features in your data set.
There are many more tools than
the ones we mentioned today
in the Responsible AI Toolkit,
which collectively represent
the work of many teams
across Responsible AI.
We encourage you to
check them out and talk
about Responsible AI questions
at the new TensorFlow forum.
Please, don't hesitate
to reach out to us if you
have any questions or feedback.
Thank you.
JAKE VANDERPLAS: Hi, my
name is Jake VanderPlas.
I am a software engineer
at Google Research.
And today I want to
tell you about JAX,
which is a Python
package for accelerating
your machine learning research.
So at a glance what is JAX?
Basically JAX provides
a lightweight API
for a ray based computing.
That's very similar to NumPy.
So if you're writing NumPy
code in Python already,
JAX will probably
feel very familiar.
On top of that, what it adds
is a set of composable function
transformations for doing things
like automatic differentiation,
just in time
compilation, and then
automated vectorization and
parallelization of your code.
And then finally, what
you can do with this code
once you write it is execute
it on CPU, or GPU, or TPU
without any changes
to your code.
So it ends up being a very
powerful system for building
models from scratch and
exploring different machine
learning problems.
On top of this,
JAX is really fast.
In the recent ML
Perf Competition,
which pitted different
software and hardware
systems against each other on
common deep learning algorithms
and problems, JAX
outperformed other systems
on some of these
common problems.
You can take a look
at these results
and see how JAX
stacks up against some
of your other favorite systems.
So let's step back and
motivate JAX a little bit.
Thinking about
this, how might you
implement a performant and a
scalable deep neural network
from scratch in Python?
Usually Python programmers
would start with something
like NumPy, because it's
this familiar array based
data processing language that's
been used literally for decades
in the Python community.
And if you were trying
to create a deep learning
system in NumPy, you might
start with a predict method.
Here this is a
feedforward neural network
that does a sequence of
dot products and activation
functions to transform
the inputs into some sort
of outputs that can be learned.
The next thing you need once
you have this model defined
is a loss function.
And this is the thing
that'll give you your metric
that you're trying to optimize
in order to fit the best
machine learning model.
So here we're using a
mean squared error loss.
And now, what's missing with
this deep learning in NumPy?
Deep learning takes
a lot of computation.
And we'd like to run it
on accelerated hardware.
So you want to run this
model on GPU and TPU.
And that's a little bit
difficult with classic NumPy.
The next thing you
might want to do
is use automatic
differentiation,
which would let you fit this
loss function very efficiently
without having to do numerical
differentiation along the way.
Next thing you might want
to do is add compilation
so you can fuse together
these operations
and make them much
more efficient.
And finally, if you're
working with large data sets,
it's nice to be
able to parallelize
this operation across multiple
cores or multiple machines.
So let's take a look at what JAX
can do to fill in these missing
pieces.
The first thing you can do
is replace the NumPy import
with JAX.numPy.
And this has the same API as
the classic NumPy in many cases.
But it will allow you to do
some of these things that
were missing in the first pass.
So, for example,
JAX automatically,
via this XLA backend, will
target CPUs, GPUs, and TPUs
for fast computation of your
models and your algorithms.
On top of that, JAX
provides this set
of composable
transformations, one of which
is the grad transform, which
can take a loss function,
like MSE loss, and convert it
into a Python function that
computes the gradient.
And now, once you have
this gradient function,
you might want to apply it
across multiple pieces of data.
And in JAX, you no
longer have to rewrite
your prediction and
your loss functions
to handle this batch data.
If you pass it through
the VMap transform,
this will automatically
vectorize your code.
So you can use the same code
across multiple batches.
If you want to
compile this, you can
use the JIT transform, which
stands for Just In Time
compilation.
And this will fuse
operations together
using the XLA compiler to make
your code sometimes much, much
faster than it was originally.
And finally, if you want
to parallelize your code
there's a transform that's
very similar to VMap
that is called PMap.
And if you run PMap
through your code
this will be able
to natively target
multiple cores in your system
or a cluster of TPUs or GPUs
that you have access to.
So this ends up being
a very powerful system
to build up these
fast computations
without much extra code.
So the key ideas here
is in JAX, Python code
is traced to an
intermediate representation.
And JAX knows how to
transform this intermediate
representation.
And I'll tell you a little
bit about this in a moment.
The same intermediate
representation
enables domain specific
compilation via XLA
so you can target
different back ends.
It has this familiar user facing
API based on NumPy and SiPy.
So if you've been coding in the
Python data space for a while,
JAX should feel fairly familiar.
And on top of it,
it's this powerful set
of transforms, so grad,
JIT, VMap, PMap, and others
that let you do
things with your code
that you weren't
able to do before.
So I want to step back
a bit now and talk
about how JAX
works, because it's
interesting to use a
powerful black box like this,
but I think it's even
more fun if you know
what's going on under the hood.
And the gist of how
JAX works is that it
traces Python functions.
So just as a thought
experiment, let's
take a look at this function.
f of x return x plus 2.
What does this function do?
It may seem obvious, but Python
is such a dynamic language
that this function could
literally do anything.
Say if x is an instance of
this espresso maker object
that overloads the ad function
so that when you tell it
the number of
espressos you want,
it'll SSH to your espresso maker
and make those automatically.
This is a little bit silly, but
it just drives home the point
that we don't really know
what Python functions do
unless we know exactly
what's being passed to them.
And JAX takes advantage
of this Python dynamism
in order to figure out what's
going on in a function,
and to come up with
a representation that
can be transformed in the
way that we saw earlier.
So it does this by calling
the function on what's
called a tracer value.
So an example of this is a
shaped array tracer value.
And you can see
what happens here
is that the add function
not only returns a shaped
array with the result, but
also records the computation.
This isn't exactly what's
happening in the JAX code,
but it's the basic
gist of what's
happening under the hood.
So how does this work?
So let's say you have a
function like this that computes
the base 2 log of a number x.
How does JAX trace this and
understand what's going on?
Well, the first thing is,
all of JAX's operations
are based on operations
in LAX, which
is this set of primitive
operations that mirrors XLA.
And now, when we want to compute
the log base 2 of some array,
the first thing we do
is put in a shaped array
value in place of this x.
And once that shaped
array value is in there,
we can step through the
function and see what
operations are done on this.
So we see the log of x, and
we record b equals log a.
We see the log of 2, and
we record c equals log 2.
We see the division
operation between these two,
and we record d as
division of b and c.
And then we return this d value.
And now what's
left here is what's
known as a JAX spur, short
for a JAX expression,
or JAX representation
of the function.
And this encodes everything
that this function does
in terms of its numerical
processing of the inputs,
and how they lead
to the outputs.
And this is a well defined,
intermediate representation
that lets you do a
number of things.
JAX knows how to automatically
differentiate this.
Knows how to vectorize
and parallelize this.
And it knows how to just
in time compile this
by passing this to XLA.
And so the result is, you
have this nice pipeline
where you're writing
Python code on the left.
And JAX is tracing
it, turning it
into an intermediate
representation,
transforming that intermediate
representation in the way
that you specify in
your Python code,
and eventually JIT
compiling it to HLO,
which stands for High
Level Optimized code, which
is what XLA reads in.
And XLA can take this HLO,
compile it, and send it
to CPUs, GPUs, or TPUs.
And all that you
need to do as a user
is write this Python code
on the left hand side.
Everything else
is under the hood
and kind of happens
automatically.
So this has been really
powerful for use across Google,
and outside Google as well.
These are just a couple of
examples of applications
that JAX has powered.
On the top left, we
have protein folding.
This is the AlphaFold
from DeepMind.
The current version of
AlphaFold runs on JAX.
I won't say much more
about that because there's
another talk in this
session that dives into it.
But we've also seen JAX used
for robotic control, used
for physical simulations,
and other simulations
where you need to run
lots of computations
on accelerated chips.
And it ends up being an
incredibly powerful system
for exploring and building
these kinds of models.
So if you'd like to
get started with JAX,
you can go to our website.
There's a very nice
getting started
guide, a 101 tutorial to
help familiarize you with how
to get started with JAX.
And if you want to
dive in a little more,
there's a whole
ecosystem of tools
built around JAX, for
everything from deep learning.
There's higher level
deep learning libraries.
There are optimization libraries
for doing physical modeling
in other applications,
probabilistic programming.
There's graph neural
networks, and many,
many more that I don't have
time to highlight here.
And with that, I'd like to thank
you for listening to this talk.
If you want to learn
more about JAX,
take a look at the documentation
at JAX.readthedocs.io.
And we'd love to see what
you build with this tool.
JOSH GORDON: So with me
today are Jenny and Jan
from Edge Impulse.
And that's a company that
makes it easy for you
to develop with Tiny ML.
And one of my favorite
things about Edge
is that you have a very large
and active developer community.
Could you share
with us a little bit
about how many projects they've
completed with Edge Impulse
so far?
JAN JONGBOOM: So we just
crossed, about a week ago
during conference, we
crossed 40,000 projects,
which is an astonishing number.
And it's amazing
the type of stuff
that people build once you give
them kind of the tools, right?
JOSH GORDON: Congrats,
that's really fast growth.
And could you share how Edge
makes it easy for developers
to develop with Tiny ML?
JENNIFER PLUNKETT: So Edge
Impulse makes it really,
really easy for an
embedded developer
or any other developer
to take the sensor
data or any other
type of data that they
have such a strong knowledge
of, upload that into our tool.
And we have other
things involved,
like AutoML to help you figure
out exactly what type of model
you need to use in order to
successfully use that data
with machine learning.
And that's completely without
having to write any code.
So you don't need to be a Python
developer to use Edge Impulse,
especially if you really
only know something like C++.
JAN JONGBOOM: The biggest thing
is, we are an engineering tool.
And we want to put power into
the hands of the people that
really understand the data.
So for us, everything start--
I like to say it
when people on board
is that everything
starts with data.
So we try to help
these people actually
gather data from the field,
and use that first to get
insight in what they have.
And then they are
the domain experts.
They can actually tell us what
is good and what is wrong.
And the only thing we can do
is just give them the tools
to make that decision quickly.
So the big thing about
Edge Impulse that lets you
build applications and
machine learning models
from scratch with your own data,
rather than just downloading
them from a model zoo.
So with that, you can
build applications
that respond to your
voice, for example,
like whenever I say Tiny ML.
So that is actually trained
on 46 minutes of data
only that we actually
collected not in the office,
but live during a conference.
So during the Tiny ML summit,
we gave QR codes to people.
They recorded a little
bit of their voice.
We cut it up into data
samples and trained the model
with that.
So here we have
our whole data set.
46 minutes in the test set.
And we can actually show
our feature explorer
to see if that data separates,
and how well that actually
works.
So here we have the three
classes that we have,
noise, background noise,
unknown, different keywords,
and the Tiny ML sentence.
And you see a really nice
cluster of green data
here on the right with
all of our samples.
And once we see clusters
going into each other--
here we have some overlap of
blue noise and green Tiny ML,
those are samples we
probably want to look into.
But with a small
amount of data, we
managed to train a 96.6%
accurate on our training set,
error at 94% on
our test set just
with a small amount of data.
And the Feature Explorer
helps us here as well.
So we can dive in and see
exactly which of these samples
were classified correctly,
and which ones were classified
incorrectly, once again, giving
us some insight in the data set
quality.
And there's two ways of actually
programming a neural network
out.
So we have our visual nodes,
which is really convenient.
We'll load stuff that we know
is going to work for your data
set and your problem.
But if you want to have the full
power of TensorFlow and Keras
on your hands, just go to the
expert modes, as seen here.
And you'll have full
freedom in the way
that you design your
neural networks.
JOSH GORDON: Thanks,
that's really cool.
And where can developers go to
learn more about Edge Impulse?
JENNIFER PLUNKETT: Developers
can sign up for free
at EdgeImpulse.com.
And you can also get started
from our documentation
at docs.edgeimpulse.com.
JAN JONGBOOM: Yeah,
so we have end
to end tutorials on
anything from doing stuff
with vibrational
data, rebuilding
this keyword spotting
model that you
saw earlier, image
classification wherever
you want to run it.
And the cool part is that
we have launched not one,
but two Coursera courses
together with Shawn Hymel.
The first one I think has
over 10,000 already enrolled.
That literally guides
you to everything
you need to know about
embedded machine learning,
and the second one
about how you can
do embedded machine learning
for vision applications.
JOSH GORDON: Thank you both.
Your demos look awesome.
And I really appreciate
you joining us.
YUEFENG ZHOU: Hello,
my name is Yuefeng.
I'm going to talk about
tf.distribute, TensorFlow's
distributed training library.
tf.distribute requires
minimal code changes
to scale up your training.
It allows a unified workflow
across different hardware,
different training
architectures,
with great out-of-the-box
performance.
We released this
into library in 2018
with a mirrored
strategy, which provides
multi-GPU synchronous training.
It does data parallel
training, as it
replicates model variables
and the computations on all
the GPUs.
All GPUs run in lock
step, and these are all
reduced to aggregate gradients.
Multi-worker
mirrored strategy is
similar to mirrored
strategy, but it
supports synchronous training
on GPUs on multiple hosts.
We have also developed
TPU strategy.
It supports synchronous
training on TPU chips.
It has been a couple years
since our first release.
We are still actively
working on it.
So the next strategy
I'm going to introduce
is parameter server strategy.
It supports parameter
server training,
which is a common
data priority method
for asynchronous training.
When do we use parameter
server strategy?
It has its own use cases.
If you want asynchronous
training to get a larger
training throughput, or if your
model has large embeddings,
or if your workers can
be frequently preempted,
you should consider using
parameter server strategy.
So what is parameter
server training?
In this approach
variables are stored
on dedicated servers called
private servers, shared
by our training workers.
Most of the computation
happens on these workers.
In each training step, workers
put the latest available values
from primary servers,
run forward, backward
pass with their
own training data.
Then send back gradients
to primary servers.
Each worker completes a
training step on their own.
They update the variables on
primary servers independently.
This is why we call it
asynchronous training.
It is scalable, and it can
tolerate worker failure.
In TensorFlow 2 we've adopted
a very different approach
to implement parameter
server training.
We call it single client set up.
In this set up the user program
will run on the coordinator.
Workers in the primary servers
run a standard TensorFlow
server, which passively
executes requests dispatched
from the coordinator.
We have also developed a set of
new APIs to support this setup.
Let's see how we can use the
parameter server strategy
with customer training loop.
This is a customer training
loop with mirrored strategy.
We create our model and the
optimizer under strategy.scope.
We created a data
set, a training step
wrapped into a TF function
and to the training loop.
To switch it to a
parameter server strategy,
we need to make some changes.
We created a parameter
server strategy,
and a class.coordinator
with a strategy object.
Similarly, we created
a model and optimizer
under strategy.scope.
We created assets
using the coordinators
create.per_worker_dataset
method.
We still have the step function
wrapped into a chair function.
But we use coordinator.schedule
to dispatch this function
from the coordinator to workers.
The last part is to
do coordinator.join,
which waits until all scheduled
functions are executed.
Parameter server
strategy has also
integrated with Keras model.fit.
This is how we use a mirrored
strategy with Keras model.fit.
To use it with parameter
server strategy
we just need to replace mirrored
strategy with primary server
strategy.
So ref is exactly the same.
Here are some resources
for TF.distribute
and the primary server training.
We have also provided
guides in the tutorials
on how to migrate from
Estimator and the TPUEstimator
to TF.distribute.
Thank you.
SHAUHEEN ZAHIRAZAMI:
Hi, my name is Shauheen.
I'm an engineering
manager here at Google.
And today I'm going to tell
you about Google's Cloud TPUs.
One of many challenges
that companies
are facing to train
models is that the state
of the art in machine learning
models is growing very fast.
As a result, there is a
need for training models
faster than ever before.
In order to achieve
this speed up,
there are three approaches
that have proven to be useful.
First, using specialized
hardware for machine learning.
Google has developed the
TPUs with a custom design
particularly for machine
learning workloads.
Second, the ability
to scale your training
on many accelerators
in parallel.
Distributed training on
thousands of accelerators
has proven to be
a great technique
for reducing the overall
training time of ML models.
And third, the
software stack that
is able to take advantage of
such a scale of distributed
accelerators is
an important part
of success in training models
faster than ever before.
Let's look at a
TPU and TPU pods.
The Cloud TPU v3 became
generally available on GCP
in January 2019.
Since then many customers
have taken advantage
of these machines in the cloud.
Also, the TPU v3 Pod
has enabled customers
to use 100 petaflops of
compute power in GCP.
Sundar recently announced our
latest TPU generation, the TPU
v4 at Google I/O.
These machines are
powered by the v4 chip,
which is more than twice as
fast as the v3 chips.
Like the TPU v3, the TPU
v4s are connected together
into supercomputers called pods.
A single v4 pod
contains 4,096 v4 chips.
And each pod has 10X the
interconnect bandwidth per chip
at scale compared to any
other networking technology.
This makes it possible
for a TPU v4 pod
to deliver more than one exaflop
per second of computing power.
Next, let's talk about
the distributed training
using thousands of TPUs.
The latest results
of ML Perf, which
were published in
June this year,
included a submission
using TPU v4 pods
for ResNet-50, which
was the fastest time
to train the model,
at just 14 seconds.
As a comparison, when the
original paper was published
in 2015 the time
to train the model
was about 29 hours on GPUs.
The winning
submission from Google
is using more than 3,400
chips to train ResNet-50.
In fact, across many tasks in
ML Perf, including rankings
and recommendations, as well
as natural language processing,
TPU v4s have demonstrated speed
ups over fastest submissions
from the industry.
If you are interested in
the full ML Perf results,
I invite you to take a
look at our blog post that
goes into more details
about the submission.
Now, let's talk about
the flexible software
that enables interactive
supercomputing
on this platform.
Regardless of your ML
framework of choice,
cloud TPUs can help you
achieve the best performance
for training your model.
So whether you're using
TensorFlow, PyTorch, or JAX,
the best and easiest APIs are
available to train your model.
A typical Cloud TPU
development workflow
may consist of starting with one
of many open source reference
models, or developing a
model with your own code.
Next, you would use
a single TPU device
to test for functionality
and correctness of your code,
and validate your desired
performance metrics.
Once you have this
model ready, you
can start by moving to
a larger TPU pod slice,
say, a v3 32 by verifying that
your hyperparameters are still
good for larger scale
training, and to ensure
that you can obtain linear or
close to linear scalability.
Depending on your model and your
desired performance targets,
you can then increase the
slice size to a full pod
or to the limit that
your model allows.
TensorFlow has introduced
the Distribution Strategies
API, which allows you to
write cross compatible code
for a wide variety of
hardware configurations,
such as TPUs, TPU pods, GPUs,
as well as multi-GPU machines.
As you can see, with
only a few lines of code,
you can detect the
hardware, and select
the appropriate
distribution strategy,
TPU strategy for
TPUs or TPU pods,
mirrored strategy for GPUs,
or multi-GPU machines.
To use a distributional
strategy,
instantiate your model
in the strategy scope
and you are done.
The rest is regular Keras code.
You can also try it out
with the Colab sample
at bit.ly/keras-TPU.
While training, the
batches are automatically
split between the TPUs cores.
As you perform the
hyperparameter tuning,
you can use settings
from the strategy that
would allow the optimum batch
size, as well as learning rate.
Notice that the
compile fit calls
are the same as
normal Keras code.
To improve performance you
would set stepper execution
higher than one, say, to
hundreds of steps, knowing
that the per batch callbacks
would only be executed
every hundreds of batches.
Another major feature
of the cloud TPUs
that makes the user
experience incredibly smooth
is the new TPU VM experience.
In the past, for
using TPUs on cloud,
a network attached
architecture was used.
The user would connect to a VM
and then interact with the TPUs
through gRPC calls.
This was difficult to debug,
and sometimes introduced delays
in the experience.
With the all new
TPU VM architecture,
you have root access to
every TPU VM you create.
So you can install
and run any code
you wish in a tight loop
with your TPU accelerators.
You can use local storage,
execute custom code
in your input pipelines,
and more easily integrate
Cloud TPUs into your research
and production workflows.
I am excited to say that the
Cloud TPU v4 with the TPU VM
architecture will be
available in GCP this quarter.
If you are interested
in early access,
please contact
Google Cloud Support.
Remember, you can always
get in touch by visiting
g.co/cloudtpu.
ANNA GOLDIE: Hi, I'm Anna.
And Azalia and I are
very excited to tell you
about our work on machine
learning for chip design.
Our motivation in this
work was the observation
that advances in
systems and hardware
have fueled massive progress
in machine learning.
And we believe it's time
for machine learning
to return the
favor and transform
the way in which we design
systems and hardware,
and close the loop.
Some key takeaways
from this talk.
We're going to describe a deep
reinforcement learning method
that is capable of outperforming
or matching human expert
performance on the task
of chip floorplanning.
This method can generate
superhuman chip layouts
in under six hours,
whereas human experts take
weeks or months at an
extremely high cost.
And superhuman layouts
generated by this method
were used in Google's
latest AI accelerator
TPU, which was taped
out earlier this year.
And we recently published
this work in "Nature."
So what is the chip
floorplanning problem?
At a high level,
it's just a form
of graph resource optimization.
So we take as input a
graph of chip components,
memory components like
macros, and logic gates
like nand and nor, all of
which are connected by wires.
And the objective is to place
this graph onto a 2D chip
canvas such that we
minimize various costs,
like latency of computation
with the circuit,
power consumption, area, while
adhering to hard constraints,
like routing congestion
and cell utilization.
But this is an extremely
complex problem.
So compared to previous
towering challenges
in AI like chess and
Go, a simplified version
of a single instance of the
chip floorplanning problem
has 10 to the 9,000
possible states.
And there has been 60 years
of research on this topic.
Prior approaches fall into
three broad categories,
partitioning based
methods like MinCut,
stochastic or hill-climbing
based approaches,
like simulated annealing,
and analytic solvers,
like the prior state
of the art RePlAce.
And in this work we propose a
fourth category of approach,
learning based methods.
So more specifically, we take
a deep reinforcement learning
approach to the chip
floorplanning problem where
we train an agent to place the
nodes of this chip [INAUDIBLE]
graph one at a time
onto the canvas.
And after placing
all these nodes
we get an approximate
reward signal
that we use to punish
or reward the policy
and get better and better
at placing chips over time.
So just to make this a bit more
concrete, at each time step
the state is a representation
of the chip netlist,
a representation of the
current node to place next,
and a representation
of the canvas
onto which we need
to place that node.
The action is the decision
about where to place
that node onto this grid.
And the reward is, after
placing all the nodes,
a weighted average of
approximate wire length density
and routing congestion.
And in this work we
take a hybrid approach
to placement optimization.
So the RL agent places the
macros, or these larger memory
components onto the canvas.
And then we use a
force directed method
to place the standard cells so
that we can quickly evaluate
the quality of this placement.
So here are some results
on a TPU v4 block.
Unfortunately, we have
to blur these images
due to confidentiality.
But the white
regions are macros.
The green regions are clouds
of standard cells, or logic.
And as you can see
from this image,
the machine learned placement
has this more rounded, organic
looking placement, whereas
a human expert placement has
a very rigid looking shape.
And the interesting thing here
is that this rounded shape
actually allows the
machine learned placement
to minimize the
wire length needed
to connect the macros
or memory components
to the logic in the center.
And the human expert replacement
took six to eight weeks
to generate, whereas
the machine learned
placement took only 24 hours.
And we've actually
significantly reduced
the runtime of our
methods since then.
And we can now generate
placements in under six hours.
And Azalia's going to
tell you a little bit more
about how we achieved that.
AZALIA MIRHOSEINI: Thanks, Anna.
So one of our goals
in this project
was to be able to train policies
that generalize to new, unseen
chip blocks.
So basically, we want
to train policies
on a set of chip blocks.
And once they were
trained, we want
to use them to quickly
generate high quality placement
on unseen chip blocks.
To this end, we
started with just
doing the most basic thing.
And that was to train a
policy on a family of chips,
and then use it right
away on a new test chip.
And that didn't
really work well.
We tried other tricks, like
freezing different layers
of the policy, and fine tuning
it on the new chip block.
And that didn't work either.
What ended up working
in this case was
to take a supervised
approach to the way
we designed the encoder
architecture for this policy.
What we observed
was that the value
net that we had for
the policy was not
able to generalize
to new chips in a way
that it was not able to predict
the quality of placement,
such as wire length, congestion
and such, or the unseen chip
blocks.
So we tried to focus
on this problem
of supervised prediction of
the reward function first.
What we did was
develop a training
data set with 10,000 different
placements across five chip
blocks.
And these placements
had varying degrees
of quality in terms
of like wire length,
in terms of congestions.
Some were better,
and some were worse.
And then we developed a new
architecture, heavily focused
on the representation
of these chip netlists.
So basically on
the input side, we
would get a representation of
the canvas and the placement
so far, the new macro that
is going to be placed,
and also a representation of
the netlist, and all the edges,
and the connections between
the nodes themselves.
And on the outward
side, you were
going to predict congestion
and wire length metrics.
We tried existing graph
neural net architectures.
And they didn't work
really well for us.
And what we ended up
doing was developing
a new method, which
heavily focused
on edge representations.
So we explicitly
assigned weights
to be learned for
edge embeddings.
So we simultaneously updated the
node and edge representations.
Eventually this
graph representation
was able to predict wire
length and congestion well.
And in this slide, you can see
how these predictions work.
Basically, once we had
this representation
that was really well at
predicting the reward
function, what we did was
taking that architecture,
placing it on the encoder
side of the policy.
And then on the decoder
side, we used a multi-layer
of deconvolutions in order to
kind of predict the actions,
in this case was where
to place a new macro.
We also used a mask, which was
very important for the policy
to train.
And what the mask
function did was basically
to allow the policy
to only choose
actions or placements that
were feasible at that point.
For example, if a
macro is going to have
overlap with an existing
macro, that action
is no longer feasible.
So with this mask function
we could really enforce that.
In this animation
side, we are basically
showing how this
generalization works.
On the right side
we have a policy
that is pre-trained
on a set of netlists.
And we applied it
directly to a new netlist,
in this case a
RISC-V Ariane block.
On the left side, we
were training this policy
from scratch for the same block.
And what you can see here
is that the policy trained
from scratch, it takes a while
for this policy to come up
with optimized placements.
Whereas on the right side,
right in the beginning
the pre-trained policy knows
to assign a space in the middle
for the standard cells
and place the macros--
and those are the small
squares you see on this slide--
on the peripherals
of the canvas.
So what's happening here is that
the pre-trained policy actually
learned how to deal with this
new netlist really well, really
quickly.
So here is a slide showing
how this pre-trained policy
helps us achieving
better results,
and also achieving those faster.
In this case, for example, we
are getting optimized placement
more than 30 hours
faster than the policy
that is trained from scratch.
Here is another
slide that kind of
encourages us to
extend a pre-training
on more and more netlists.
The policy that is trained
on 20 blocks, in this case,
can achieve optimized
results much quicker
than the other two
policies that are trained
on a smaller set of blocks.
And this is really
a unique property
that deep learning
and representation
learning, in this case,
is helping us observe,
because none of the existing
methods to placement
can really become
better as they solve
more instances of the problem.
And the good thing is the number
of chips that we are placing
is only going to
increase over time.
In this slide we're
comparing our method
to prior academic state
of the art method,
and the true prior
state of the art, which
was the manual baseline,
a team of physical design
experts generating these
placements on TPU blocks.
What we see here is that
we compare favorably
to the manual design.
And we can do so in under six
hours using automated methods,
whereas the baseline
takes several weeks
and requires human
experts in the loop.
So to summarize, we developed
a new deep reinforcement
learning method for the
task of chip floorplanning.
Our method converges
in under six hours.
It was already productionized.
We recently published
this work in "Nature."
And this work generated
a lot of interest.
And we think this is
just the beginning.
And the whole process
of chip design
can really be transformed
with deep learning.
Thank you.
CRISTINA MAILLO: I'm a
student at the University
of Manchester.
And I have developed Yoga AI.
I have basically
classified six main poses.
So as you can see, if
you hold the Mountain
Pose for 10 seconds, the
counter starts going down.
And if it notices you're not
doing the pose, it stops.
And then once it detects
another pose, and if it's wrong,
the timer restarts.
VIVIEN TRAN-THIEN:
I use 3JS, which
is a JavaScript library, which
allows you to visualize a 3D
object and track
the face of the user
with a pre-trained face
tracking model in TensorFlow.js.
It moves as I move.
So it works left or
right, or up and down.
JASON MAYES: That really
pops out of the screen.
That's amazing.
It's so realistic.
FRANCISCO BAPTISTA: What we
created was a sports platform
to help teams, amateur
teams, such as myself,
to access sports performance
data and statistics
easily and affordably.
So we created a platform
that enables both coaches
and players to use artificial
intelligence and TensorFlow
models to generate the
performance statistics they
need, and to understand why
are they winning games or not
winning games?
And how can they improve
their performance,
whether it's from a conditioning
or from a skills perspective.
And that, in a nutshell,
is team sports.
JAMES SEO: What you're seeing
is a really short looping video
clip of a martial artist
doing his routine.
And what I'm doing
is using PoseNet
to extract the pose of
the different body parts,
and extruding it in both time
and space in augmented reality.
JASON MAYES: That
is really cool.
JAMES SEO: So you get the
3D form slash visualization.
DOUGLAS DUHAIME:
This project was
made as a sort of
demonstrator project
to show how can we
make autoencoders?
How can we use them
in TensorFlow.js?
We're sampling not digits,
the [INAUDIBLE] data,
but a different data
set, the CelebA data set,
which is a really great
data set of celebrity faces.
ANDERS JESSEN: We tried
to see how could we
make a website that
you could just interact
with with just your hand.
So you need to do like a drag
gesture to move things around.
And you need to like hover
your hand over an object
to kind of select it.
Yeah, and then spread
your fingers to click it.
CHRIS GREENING: So
TensorFlow.js, I
realized that all the building
blocks are in place now,
and I could actually
build an end
to end system that would solve
Sudoku puzzles in your browser.
And you could see it's
scanning the puzzle
and solving it as we scan over
the images in a newspaper.
SHAN HUANG: So a
couple of months
ago we open sourced this project
called Pose Animator that
allows you to use your body to
animate your SVG characters.
SVG, as it is, still requires
quite a bit of design expertise
to create.
But we want to lower
the bar even further
so that anyone can create a
character and animate them.
SEAN MCGEE: Recently I've been
exploring using TensorFlow.js
with imagery.
If you think about looking
at a map, and you see
it's almost like a photo
direct across the world.
So that's satellite imagery.
So we've been looking at using
satellite, as well as drone
and UAV imagery to
recognize objects.
And fundamentally, if we look
at the TensorFlow.js sample
website, we know we've got
the webcam sample where
you can show an object, and
it can show back what that is.
You can do the same techniques
with imagery in GIS.
JASON MAYES: OK.
RICHARD YEE: As you
can probably tell,
I'm a huge fan of
VTubers, which is
why I'm currently in this form.
I've been working on a
lot of VTuber web apps
that allow you to do real
time motion capture directly
in the browser.
ALEX BAKOUSHIN: I
was thinking, OK,
and could we do this in
JavaScript using TensorFlow.js?
And--
JASON MAYES: Here it is.
ALEX BAKOUSHIN: Here it is.
It works.
In the terminal we start
the driving script,
which uses the model we
have built to drive a car.
And you can see, it drives.
It knows only two commands.
The positive value
is turn right,
and the negative
value is turn left.
And the car drives pretty well.
[MUSIC PLAYING]
JOANA CARRASQUEIRA:
Hi, everyone.
I'm Joana Carrasqueira.
And I'm a developer
relations program manager
at the TensorFlow
team at Google.
We've been having
an amazing event,
and I'm very excited to
be here with you all today
and talk about how you can
get involved in the TensorFlow
community.
For the community team,
providing a good contributor
experience is at the
core of what we do.
We focus on creating systems,
resources, and documentation
to empower you as a contributor,
and help you scale your work.
We welcome all
contributors, either they
are beginners, intermediate,
or advanced professionals.
And regardless of where you
are in your machine learning
journey, we have a path for you.
I'm going to highlight some
of the ways through which you
can join the community
depending on your proficiency
with TensorFlow and desired
areas of contributions.
If you are getting
started with machine
learning, looking for a
mentor, or simply trying
to make your first
contribution, then
I invite you to
create an account
and join the TensorFlow forum.
The TensorFlow forum launched
in May at Google I/O,
and it's the first
central platform
that allows community members
to connect with project
leads and the TensorFlow team.
This is the right place to
socialize ideas and learn
the latest about TensorFlow.
More than 2,500 developers
from all over the world
have joined the
TensorFlow forum.
So join us today and follow
the Getting Started tag
in order to find
relevant discussions
and tips from those who are
also getting started with you.
This is what the
forum looks like.
We created a forum
experience that
enables you to follow the
tags and categories that you
might find interesting.
As I mentioned, if you
are getting started,
you may want to follow the
Getting Started and Community
tags.
Here you will find not only
the latest about our community,
but also tips and
recommendations
of where to start, as
highlighted in this slide.
There are very
exciting conversations
happening on the forum.
And I strongly encourage
you to be a part of it.
If you're looking to
contribute code to TensorFlow,
the RFC process is a
good way to get involved.
The RFCs, or Requests
For Comments,
are a great way to get feedback
on large open source projects.
Therefore, this
is the primary way
we communicate our design
rationale and receive feedback.
This way we give core
and non-core contributors
a chance to propose
an idea to everyone
before implementing a change.
If you want to propose
a design, we strongly
encourage you to socialize
your idea, recruit a sponsor,
write your RFC, respond to
feedback, and implement it.
Our contributor ecosystem
welcomes many different types
of contributions,
including code, known code,
and documentation.
For bigger projects in which
we have to work as a team,
we created the TensorFlow
special interest groups,
which are groups with a
defined project focus.
This is a program that
organizes contributors
into focus streams of work.
And these groups take
ownership of specific areas
to add and maintain
new features.
Currently we have 14 sigs.
And I would like to
highlight the latest addition
to the sigs program, the
TensorFlow Recommenders Addons
project, which makes large
scale recommendation easier
and widely adopted
in the community.
If you are interested in
contributing to any sigs
based on the parts of TensorFlow
that you enjoy or care
about the most, head to
GitHub for more information
on how to join.
You can also request a
new sig, and we are here
to guide you
through the process.
Another program that I
would like to talk about
is the Google Developer
Expert program,
which welcomes developers
from all over the world.
Our machine learning GDEs,
they play a critical role
in advancing machine
learning education worldwide.
And they act as
technology ambassadors.
Just this year so far
our machine learning GDEs
reached more than a
million developers
throughout tech talks,
workshops, blogs, and videos.
If you would like to learn
more about the GDE program,
head to developers.googl
e/community/experts,
or feel free to reach
out to any of the GDEs
on the TensorFlow
forum to find out
more about their personal
experience with the program.
Be an authority in the community
and increase your proficiency
in machine learning by taking
the TensorFlow Certificate.
Employers use this
network to find talent.
So showcase your skills and
maybe land your next ML job.
We also have stipends available.
And for more information about
the TensorFlow certificate,
please visit
TensorFlow.org/certificate.
We have many
educational resources
available to help you build
and train powerful models.
Learn how to design an
ML production system
end to end with our course
on ML ops on deeplearning.ai.
We also offer more
courses on topics
such as TensorFlow advanced
techniques and data
and deployment specialization.
Lastly, I would like to
highlight our Tiny ML Harvard
Professional
Certificate, where you
can learn how to
program in TensorFlow
Lite for microcontrollers.
Follow these links to find out
more about your desired course.
And last but not
least, for a chance
to be featured on the TensorFlow
social media channels,
please submit a
cool project or demo
that you created
using TensorFlow.
I'm always excited to see
what the community creates
every month.
And this is a great
opportunity for you
to get recognition and increase
your profile in the community.
We feature our community
spotlight winners
on Twitter and the
TensorFlow blog.
Thank you all for your
commitment to our community.
We couldn't have
done it without you.
Finally, I strongly
encourage you
to get involved in the
TensorFlow community.
Join the ML GDE program
or the sigs program.
Get certified with the
TensorFlow Certificate program,
or take any of our courses to
advance your machine learning
proficiency.
Don't forget to submit
your cool project or demo
to be featured on the
Community Spotlight Program.
And if your city or country
doesn't have a TensorFlow user
group yet, then you
have the opportunity
to start the first group.
Let's stay in touch
on social media.
We have a YouTube channel
with fantastic free
educational content,
a Twitter account
where we share the latest
news about TensorFlow,
and recognize the fantastic
work of our community members.
And lastly, check out
the TensorFlow blog
for technical content,
stories, and use cases
on how to use TensorFlow.
And, of course, for a
more personal touch,
and to stay connected
with the team,
I'll see you on the
TensorFlow forum.
Thank you.
[VIDEO PLAYBACK]
[MUSIC PLAYING]
- When I was 22 in
2012, I was involved
in an electrical
accident at work,
which resulted in the
amputation of my right hand.
Music and drums was my passion.
When I lost my arm I was very
negative about things, just
thinking about
what I couldn't do.
But after the first month,
I was like, you know what?
I'm just going to pull my
drum kit out of storage,
tape the drum stick to my
arm, and proceeded to play.
That was the turning
point for me.
It was letting me know
that it was possible.
My current prosthetic
is what you
call a body powered prosthetic.
It just uses basic mechanics.
But I use a lot of
my shoulder and elbow
to kind of exaggerate it to
get the hits that I want.
It also has limitations
on how fast I can play.
Yeah, it's a weird one.
So I had an idea of developing
a robotic drumming prosthetic.
So I knew Gil Weinberg was the
right person to reach out to.
- I understood immediately
what he wanted.
He wanted to sense
more of his muscles,
and be able to control the
bounce of the drumstick.
I was a little worried.
Can we provide him with what he
wanted in terms of the latency
in the control?
We didn't know
that it's possible.
Now, it's not a big problem
when you want to grab a cup.
So you wait a few milliseconds.
But when you play music,
you need to immediately hear
the sounds that you play.
And that's where we looked at
all kinds of different models.
And TensorFlow was
perfect for this project.
- So we basically
used TensorFlow
as a tool for the deep
learning trainings
based on the data we collected
through the EMG sensors.
- We need to see what kind
of electrical activity
happens when he's moving
his arm in different ways.
- And then through the
TensorFlow Lite, which
is actually a more
efficient tool to use,
we can apply that
to the Raspberry
Pi, a small device
that is implemented
as a general console.
And the TensorFlow
model will actually
predict Jason's gesture based
on how he moves his muscles.
[DRUMMING]
- TensorFlow Lite provided us
with exactly what we needed
much more streamlined, faster.
It also reduces the latency.
- The latency, it's like none.
- That's awesome.
Usually in university labs, you
make prototypes that can just
symbol what can be done.
And I think, fingers
crossed, that this arm
is going to last for
years, just like a product.
- I can actually feel the
feedback from the arm.
And it feels as
close to a real hand
as you can get without it
actually being a real hand.
[DRUMMING]
The possibilities of this kind
of technology with TensorFlow
and everything combined, you
could make almost anything
you can think of.
[MUSIC PLAYING]
[END PLAYBACK]
[MUSIC PLAYING]