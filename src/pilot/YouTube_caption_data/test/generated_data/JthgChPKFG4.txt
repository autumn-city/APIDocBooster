right so in this video we'll make a
simple stick classification using
scikit-learn
so for this example we'll be using a
data set called this BBC data set it
contains about 2200 articles and it has
five different categories let's look at
the data once you download the chip from
the link and then x rank you will see
the articles are organized into these
different folders into these five
categories so we'll make a simple night
based model using scikit-learn and make
the predictions from there so first
thing first yeah I'll keep the link in
the description so you can download the
data set so first of all let me define
the data directory you see then let's
import few libraries that I and that
we'll need in line 10
let's see import numpy this NP just in
case we plot something then
data sets load file so this this
function will help us load the data set
that on this format so it expects each
class to be in a separate folders which
we already have so it's handy function
that we can use so data equals to load
files from where from data directory
then we just want encoding to be utf-8
then in case of any errors while
decoding will just say replace so that
it doesn't throw any exceptions when we
run it so we already know how many
categories are there 1 2 3 4 5 but yeah
just for the sake of demonstration I
will I will show you how you can view
all of those so first word ok this is
still running
[Music]
anyways we'll see how many articles are
there for category so for that we'll use
calculate round so this would be labels
round circles can be dot unique there's
this unique function provided by numpy
and we can just pass this data to target
then we want this return counts to be
true so that we get the counts
associated we with eats cat away then we
want let's just print labels and count
see what it is
so labels is just 0 1 2 3 4 and we have
the counts but yes this scikit-learn
automatically converts the name of the
classes into this numeric indices
indices or index but lets us convert
these to string labels so for that we
need labels STR equal to NP array then
the target names or the classes have
saved as target names in the data this
is done by the load files function
automatically and then labels then let's
create a dictionary out of this so print
date ship levels STR and then ground so
this is basically creating a dictionary
by zipping the string levels and counts
so we'll get something like this so for
politics we have 17 what takes related
is 401 so
all in all they look okay I mean all of
the categories have some decent amount
of samples and none of them are heavily
not quite in balance I would say so I
think this would be this would work
pretty good so before we can make a
model what we need to do is convert our
text data into some sort of vector
representation and for this particular
purpose we'll be using something called
tf-idf so for that let's first we'll
split our data into training and testing
set and then we'll do the things as well
like vectorizing the text from selection
train test so this function will divide
the data set into training and testing
set and we can also specify what person
of the total data set we want to assign
to training set n well what for the
testing set so let's just say X train
extends
trend test
so data the data is in this data
property and then the classes are in
theta dot time so by default it will
split 70% of the data will go to the
training set and thought you will go to
the testing certain so this is the
format that this train test split will
return
so X train extension and white is
and let's also see how many samples we
have something is wrong so we have 1600
for training and 550 for testing um so
before we factorize the text what we
need to do is tokenize the text into
words
so basically tokenizing means converting
that these articles into words so
basically this would be one word this
would be another so first we need to
tokenize then what we new need to do is
convert all of these letters into
lowercase so that the world like this
capital da and this capital s are if
this door and this one are the same so
here the T is capital but they follow
here in this door all of the letters as
well so if we don't convert them to the
same case then these would be considered
as different words so we don't want that
that's why we convert usually convert to
do all lowercase but yeah we can also do
the operand then we need to remove some
stop words or basically the words that
are not important for example the world
like though or off
or N or something like that and the next
step what we will do is stemming we yeah
is so basically steaming means removing
this for example in case of investors
after steaming it would become invest so
going back to the fruit of the world so
biggest would become just big so and
after we have done all of those removed
our useless words and done this steaming
and some few others then we can finally
convert these in the whole takes into a
vectorized form so let's go through the
steps from SQL feature extract so dot
takes tf-idf factorial so we use this
one to vectorize our take so factorial
equals to DF right here then what we
want to do is so here I'm lot of
parameters that we can pass but for this
we'll sucide stop wars to P for English
or scikit-learn already provides or you
can also supply your own stock word list
yeah and then we also want max features
to be it's for this demonstration I'll
just use one thousand so what max
feature means is it will take the top
1000 most freaking frequent words and
only consider those and all the other
words that are not in the not seen in
the top 1000
we'll just replace it with some kind of
unknown token this we should try to keep
as small as possible because we'll see
later that the more are the bigger the
size of these features the more bigger
our matrix will be the training data or
for testing or training so yeah we
should try to keep this number as small
as possible while also in the
maintaining the accuracy that we want so
again we'll just say Tico tier or ignore
then vectorizer top fit extent the
reason we splitted our data into
training and testing set was so that we
can here was so that we can use the only
our training data to train that this
vectorizer so that the vectorizer only
sees the the text what was provided in
the training set and it will only
consider the words that was seen in the
training set we cannot it's not a good
idea to include all of your whole data
set to train this vectorizer so once the
vectorizer is trained now we can build
the model so so we'll just use a simple
name is from SK
my is import multinomial and B then
[Music]
we'll just use the default parameters
whatever they are now
CLS would field for this fig function we
need to pass our vectorizer to transform
and x train y trends so first we will
use our vectorizer to transform whatever
was in the extreme
so our all of our text would be
vectorized and then pass into this
classifier along with the labels so now
this is trained modern strength let's
see how it performs so we'll just import
a couple of helper functions from s
calendar metrics for classification
report and accuracy score so we predict
the pretty on our test cases or X test
then print accuracy score for accuracy
score needs this actual levels and what
was predicted by our model so my bread
then also will print
classification rapport with same thing
write test wipe read
and we saw that okay this our model
achieved a 96% accuracy in test set and
in this classification report we can see
that these are the categories 0 1 2 3 &
4 and we get more information rather
than just the accuracy we get all of
this precision recall f1 score so the
model looks good it could definitely be
improved like for example even though we
talked about stemming and all with him
to any stemming so we could also in this
vectorizer there is a function tokenizer
or preprocessor we can add more our own
custom function to do all this stuff
yeah for now
for this demonstration even with this
simple pipeline we managed to achieve
96% accuracy and it's not too bad so in
the next video we will look at how we
can use a great source of scikit-learn
to find the best parameters for for our
models for example here in multinomial
NV we just use the default values
whatever there was and also in the
tf-idf vectorizer we just said ok we'll
use max features 1000 but who knows
maybe we using 2000 would have would
have made the accuracy even better so in
the next video we'll look at how we can
automatically I tell the scikit-learn
to try out different values for these
these parameters here or here and give
us the best
best values that would make the highest
best performing model or in other words
that the model that gives the highest
accuracy see you next time
[Music]