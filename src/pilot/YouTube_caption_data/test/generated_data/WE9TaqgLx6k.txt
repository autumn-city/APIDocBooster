[Music]
okay so hopefully
um you've come back for the second
lesson
so in the first lesson we looked at pie
torch lightning and we also looked at
scikit-learn
so in this second lesson what we're
going to do is
we're going to try to improve our
performance just a little bit
uh hopefully you've had some time to go
ahead and build your own model
in in this kind of framework and have a
play around with some of the data sets
that you might have
so one of the ways that we can improve
this model is simply by adding more
layers so let's have a look at how we
can do that in pi torch
so to do that in pi torch all we need to
do is we need to define a new layer
like so and we need to make sure that
the
inputs and outputs of this layer match
what we expect
now in this particular case what we've
said
in is in the top layer input size is
size 4 output is size 3.
let's change this let's change this to
5 because we feel like it remembering
that the output still needs to be
the same size as the number of labels
that we have so let's change this to 5 3
as well so that means that the input has
size of 4 output of 5 and then in the
second layer
the input is of size 5 and the output is
of size 3.
okay so let's do this like that
that means what this means is when the
model is called it will first go through
the first layer
and then using the output of the first
layer it will go into the second layer
however this isn't where the strength of
deep learning lies
but rather what what you want to do is
you want to introduce some kind of
non-linearity
so we're going to use the relieve
function which is the rectified linear
unit
in order to do that so now it goes to
the first layer
applies a non linear activation function
and then uses the output of that to go
into the second layer
all right so let's go and try to run
through this whole example again
and see um whether we get an improvement
in our performance
so let's let's wait for this to
go for a little bit
so one of the really useful things with
um
with pytorches you can see how many
parameters it needs to optimize
uh compared with um
if you didn't have this so saying that
in the first layer there's 25
free parameters and in the second layer
there will be 18.
so every single time it's running an
earpock it is updating the parameters
used
for both of these all right so now you
can see that
by making a very simple change and
adding this non-linearity
we now have demonstrated we have uh
superior accuracy
to the psychic learn solution and we
have a lower
blog loss as well so you can see in our
torch solution 0.9
one and our loss is 0.38 our log loss
compared to our psychic loan which is
1.51
all right and let me try this for a
moment
just to double check that we get the
same number right 1.5
519. so there we have it we've
demonstrated how
you can extend your existing
pie torch model by adding an additional
layer
just by adding an extra three lines of
code and now we can build a model that
surpasses what we have in scikit learn
in a very easy
easy to use way at the end of the day
once we have defined our um our model
it's only three lines of code in order
to train it and that's very similar to
scikit-learn where we have three lines
of code except that
this model of course is we can't alter
this after we've we've built it
and we'll need to do a lot of work in
order to change
what actually happens as part of this
model
so hopefully you found this useful and
that you can go ahead and try lots of
different things
within the deep learning um deep
learning space
and remembering that it is isn't really
all that difficult in order to get your
um in order to dip your toes
and begin to build some deep learning
models