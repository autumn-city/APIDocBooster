one of the last tricks that this
sinclair paper introduces is the global
batch norm
and what that does is basically when you
have
a like very large distributed training
system so they do 128 tpu course in the
most general case batch learning is done
on each gpu where like batch statistics
are computed on a single gpu
and you divide so you subtract the mean
and divide by the standard deviation
over there
uh but what global bashform does is
you accumulate bad statistics acro in
the whole
world or across all possible gpus and
you synchronize those values and
basically
uh yeah do a batch norm for each image
using those
like the global statistics and what
the authors of this paper say is that
this is done to prevent a shortcut
uh because the positive pairs are always
on the same gpu so
there if you have a much smaller batch
their contribution to the bad statistics
is much
more like influential if you do a global
batch norm
their contribution diminishes by a lot
so
if the network finds a shortcut the task
becomes much easier to solve
and hence the representations learned
won't be that good
since i am dim i think there's a lot of
claims about
the ability for batch norm to
transmit statistics about the image
which will ruin the representations
right so
if for some if in some way the model can
kind of leak information that you know
this patch came from this image and so
on through the batch statistics
in batch norm then it's possible that
the training could collapse
and so that's a claim that the authors
make in amd
cpc sinclair and everywhere else as well
although
i i guess uh i'll probably push back and
say that there isn't anything concretely
or
yeah that i have seen that actually
prove that that's the case and in my own
experiments when i tried it with that
batch number i didn't get
um horrible results either