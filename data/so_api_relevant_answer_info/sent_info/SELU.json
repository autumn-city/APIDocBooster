["Selu is not in your activations.py of keras (most likely because it was added Jun 14, 2017, only [22 days (hyper-link)] ago).\n", "You can just add the [missing code (hyper-link)] in the activations.py file or create your own selu activation in the script.\n", "You can get the selu activation with:\n", "If you register selu with it's custom name, you would be able to use it for your runtime.\n", "First of all, I think there may no exist Activation(selu(x=dist)) such usage.\n", "For selu use in the Activation as a function not the output of the selu.\n", "The implement of the selu can be found below:\n", "In your case, I think the [article (hyper-link)] means to initialize the weights of the layers rather than selu.\n", "According to the official api [here (hyper-link)], I think selu can be used as below in your case:\n", "Here are some sample codes to show the trend of means and variances over 3 SELU layers.\n", "So even though SELU converges to a unit Gaussian distribution, it will not be a unit Gaussian if you do not have activation on the layer which generates the latent variables.\n", "The SELU will let the distribution of mean and variance to follow unit Gaussian, which does not make sense here.\n", "To give an intuition, the main property of SELUs is that they damp the variance for negative net inputs and increase the variance for positive net inputs.\n", "Therefore SELU networks control the variance of the activations and push it into an interval, whereafter the mean and variance move toward the fixed point.\n", "Thus, SELU networks are steadily normalizing the variance and subsequently normalizing the mean, too.\n", "After reading the original papers of batch normalization ([https://arxiv.org/abs/1502.03167 (hyper-link)]) and SELU ([https://arxiv.org/abs/1706.02515 (hyper-link)]), I have a better understanding of them:\n", "The inputs to SELU have to be normalized before feeding them into the model.\n", "As for the selu activation, you need to reshape the input to (n_samples, n_output):\n"]