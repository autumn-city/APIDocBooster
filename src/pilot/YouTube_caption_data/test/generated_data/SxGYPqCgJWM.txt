the callback library divergence is a
distance measure between distributions
which is widely used in information
theory
and particularly for machine learning
the equations come in two forms
one for discrete probability
distributions and one for continuous
distributions
on the surface however it's not exactly
clear why the kl divergence is so useful
or even how the summation or integral
measures the distance between
distributions in any way
to explain why the kl divergence is a
very natural idea let's back off and
consider what it is we're trying to
achieve
what we're trying to do is measure the
distance between probability
distributions
let's first give an example of a
probability distribution
let's imagine that we have a fair coin
which is equally likely to be on heads
or tails
this means that the probability of heads
is 0.5 and the probability of tails is
also 0.5
let's imagine that we also have a bias
coin where the probability of
heads is set at p and the probability of
tails at q
can we say anything about how similar
the distributions are to each other
there isn't a trivial answer to this
question we might be able to make
general statements about which examples
are closer
for example if p is 0.55 then coin 2
would clearly be much closer to the fair
coin than if p was 0.95
why well the results we'd see with 0.55
would be quite similar to the fair coin
and it would be very easy to confuse
them however if p was 0.95
then differentiating between the two
coins would be quite simple and could be
done in merely a few coin flips
determining the difference between two
extreme examples might be easy
but can we make any quantitative
statements about how different they are
and is it possible to put a numeric
value on the distance
previously we reasoned that a natural
way to think about the distance between
distributions was to look at how easy it
was to confuse them
a basic way of measuring this might be
by seeing if the distributions assign
similar probabilities
to the same sequences as if they assign
similar probabilities to similar
sequences
it implies that the distributions aren't
too different let's then propose an
algorithm
first we generate observations using
coin one we then calculate the
probability of coin two generating the
observations
after that we can compare this
probability to the probability of the
true coin generating the results
if the output probabilities are similar
then the coins might be similar
but if the likelihood is significantly
smaller for the second coin then the
coins might be very different
a natural thing to calculate would then
be this ratio here which compares the
likelihood of the observations for each
coin
let's go over an example of the proposed
method assume that we have a coin with
probability p1 for heads
and p2 for tails we then flip the coin n
times
and record the observations we then work
out the probability of coin one
generating the sequence
working this probability out is quite
simple whenever we see heads we just
multiply by p1
and whenever we see tails we multiply by
p2 let's now introduce coin 2
which has probabilities q1 and q2
instead
working out the probability of coin2
generating the data is done identically
where we just multiply by q1 for each
heads and q2 for each tails
these two expressions can then be
simplified by realizing it's the product
of the probability of each outcome
raised to the power of the number of
occurrences for the first coin
this is p1 to the power of the number of
heads multiplied by p2 to the power of
the number of tails
we then want to find the ratio of
probabilities so we simply divide the
first likelihood by the second one
although these calculations may seem
unrelated to the kill divergence
believe it or not under the hood the
kale divergence measures the exact same
thing
to see how let's normalize for sample
size by raising it to the power of 1
over the number of samples
then take the log of this expression
we'll then turn on the log rule
autopilot
and bring down exponents turn
multiplications and divisions into
additions and subtractions
again drop down the powers which gets us
to this stage
note that if observations are generated
by coin 1 then as the number of
observations go to infinity
we expect proportion of heads to 10 to
p1 and the proportion of tails to 10 to
p2
this allows us to say that in the limit
nh over n is p1
and nt over n is p2 after some final
logarithmic manipulation
we then arrive at our final expression
remember what this expression represents
the normalized log probability of the
true likelihood divided by the
likelihood of the second distribution
all we did was simplify this initial
expression for the coin example with
basic log rules
and we got to this equation if we look
at the discrete tail divergence equation
again
we notice that the two equations are
equivalent furthermore
every step of the proof holds true when
there are more than two classes
and the fact the kill divergence is a
general form of the normalized log ratio
when there are multiple classes
this then gives us intuition for what
the kl divergence is
a natural measurement of distance
between probability distributions
motivated by looking at how likely the
second distribution would be able to
generate samples from the first
distribution
since a lot of deep learning is all
about modeling true underlying
distributions
the kl divergence becomes a very useful
measure in fact
the cross entropy loss is equivalent to
the kl loss so by minimizing cross
entropy
we're minimizing the distance between
distributions that's everything for
today's video
hopefully you now understand what the
kill divergence is and have better
intuition for why it's used
to find out more about the kl divergence
and general fundamental machine learning
theory
make sure to subscribe to the channel
for updates on future videos