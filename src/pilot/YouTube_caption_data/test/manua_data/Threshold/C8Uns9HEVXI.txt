 Alright, let's now finally talk about training single layer
 neural networks, and in particular, the perceptron
 learning rule. So that goes back to the Rosenblatt perceptron
 that we briefly mentioned in the history of deep learning
 lecture. So the perceptron is kind of like a model. But yeah,
 thinking about it, the model was already proposed by McCulloch's
 and Pitts, the mathematical model of a neuron in the human
 brain. So at least it was inspired by this biological
 neuron. And the perceptron to be more, I would say specific, it's
 a learning rule for the computational mathematical
 representation of the neural model. So this is now offering
 us something that can help us find the weights automatically
 to make classifications, for example. So previously, we have
 seen that how we can manually implement end and or functions
 using specific weights of one and a certain threshold. Now
 with the perceptron, we can find automatic weights for different
 types of problems like classification problems in
 general. So here's a picture of how that looked like back then.
 So back then, the perceptron was actually a hardware device where
 people Frank Rosenblatt had to plug in and plug out certain
 cables to reach certain decisions and stuff like that.
 So but yeah, we are not in a museum here. So we don't have to
 go over how that works. So yeah, moving on. So I also should
 highlight just for correctness, there are multiple perceptron
 algorithms. And I don't know how many, but there are several
 variants. Here we are talking about, let's call it the
 perceptron. So basic version of the perceptron, which is now the
 commonly known variant. So when we talk about the perceptron, in
 theory, it's a perceptron, one of the classic Rosenblatt
 perceptrons. But yeah, there are multiple ones, we don't have to
 worry about it here. We just take the classic one that is
 commonly known and just refer to it as perceptron for short. So
 yeah, now we have a computational model of a
 biological neuron here. So for reference, on the left hand
 side, what is shown is the biological neuron. And on the
 right hand side, would be the computational or mathematical
 representation of that. So the one based on McCulloch and
 Pitt's idea, and that is also then the model that we are going
 to use in the perceptron. So the perceptron learning rule would
 be then learning the parameters of this model. So how does this
 model work? So what we have here are the inputs x one to x m. And
 you can think of it as a feature vector. For example, if you think
 back of the iris data set, so in the iris data set, we had a
 tabular data set where we had petal length, let's do it
 correctly, sepal length, sepal width, beta length, and beta
 width, the four features. And then we had the number of
 training examples of 1234, and so forth. And in this case, we
 had m features where m equals four. So here is x one x two x
 m, you can really think of it as a feature vector, for example,
 a feature vector corresponding to the first training example.
 And then each feature value gets multiplied by a weight. So you
 can see, each x here goes together with the weight. So
 they are multiplied to compute the so called net input, the
 weighted sum. So this is weighted sum, which you compute
 SWI times x i. And this is then what we usually use, or we
 usually use the letter z for representing that to have it
 more compactly. And then so this is an input here. And then we
 pass it to a threshold function. And a threshold function is
 returning a class label. So the class labels either zero or one,
 depending on the value of z. So if z is smaller than the
 threshold, then we return the class label zero. And if z is
 greater than a threshold, we return class label one. To be
 more precise, if z is smaller or equal to the threshold, but if
 we use floating point operations, like real value
 numbers in a computer, then the chance that something is equal
 is very small, because it could be also equally greater or equal
 to so in that case, we could remove it here. So it's, I'm
 trying to say is whether we use zero smaller than and greater
 equal to or smaller equal to and greater doesn't really matter.
 Either either way would work, because the chance that we have
 an equal is kind of very small anyways. So in our output, this
 is the either the zero or the one that is our class label. And
 now we can, yeah, then talk about a learning rule for this
 perceptron that can then be used to determine a good threshold
 and a good, good values for w, so that we can solve different
 classification problems, for example, classifying flowers in
 an iris data set. However, one shortcoming here is that this is
 only for binary labels, so zero and one. And in iris, we have
 three flower classes. So if we would want to use the standard
 perceptron for iris, we would have to simplify the data set to
 only two flower classes, but we are getting ahead of ourselves
 here. So also here at the bottom, I just see another
 equation included. So this is also abbreviating the threshold.
 So the threshold based on the threshold function based on the
 threshold theta, if it receives basically the weighted input, so
 this is the net input. It returns the predicted class
 label y hat. So y hat, the output is the predicted class
 label. I will show you how that looks like for a concrete data
 set later on. Yeah, here to recap some of the terminology
 that I already introduced in the previous slide, I made an
 overview slide because some of these symbols and words or terms
 will be used later on in this course, when we talk also about
 logistic regression and multi layer neural nets. So we have
 the net input, which is just another word for the weighted
 inputs. And we usually use the symbol or letter z for denoting
 that on the activations, these are values from activation
 function. So an activation function takes the net input as
 input. And we use the letter a and the activation function is a
 sigma, which takes in z. So this is something that we will be
 using over and over in this course. And then the output of a
 model, that is when we apply the threshold to these activations
 of the last layer of a neural network. And we can, for example,
 use y hat to denote the predicted label or the output.
 And F is the threshold function which takes an A. So the flow
 would be, you can think of it as first computing. So if we go
 from z, so we have z is the weighted net input. So this is
 computed based on x and w. And then we use sigma and activation
 function to compute these activations. And then we have F
 to compute the label outputs. So this will become also more clear
 clear later, because right now, we have a special case in the
 perceptron, where our activation function is the same as a
 threshold function. So the perceptron doesn't really have
 an activation function, they are here in that case, synonymous.
 And in linear regression, there's another special case. So
 some of you are probably already familiar with linear
 regression. So in linear regression, the activation is
 equal to the net input, and it's also equal to the output. So in
 that way, um, yeah, these are two special cases, and it will
 become more clear. So I can maybe quickly draw this. So if
 we have a linear regression model, recall what we had in the
 history of deep learning lecture, and also on the Piazza
 discussion, the relationship or the figure that I shown of the
 adder line, and then also how that related to the linear
 regression. So if we have then the model here, and a linear
 regression, we would have a linear activation or just the
 weighted sum, and then the output is just a weighted sum.
 It's our continuous value in a regular neuron, we have usually
 the steps, we have the weighted sum. So this is our net input.
 And then we have activation function. And then we have a
 threshold function. And then this produces our output.
 output. However, in the perceptron, so in the
 perceptron, these are the same, the sigma and the threshold
 function, they are one function. And in the linear regression
 model, so in the regression model, we don't have these. So
 the net input is basically our y hat. So there are these special
 cases. But let's not worry about it too much, because we will be
 talking about linear regression and logistic regression later on
 in more detail. So you will see how these concepts of activation
 functions and thresholds relate to these. So let's focus in now
 on the perceptron and only worry about the net input and the
 threshold, where in the bottom here, you can, again, see the
 summary where we have first weighted inputs as the net
 input, and then we apply the threshold to return either the
 zero or one, depending on the threshold value on theta. So I
 want to make one little change to that formula to make things a
 little bit more convenient. So I just told you that we return
 zero, if z is smaller or equal to theta threshold, and it's the
 output is one, if z is greater than theta, it's a little bit
 inconvenient to write it like this. For I mean, it's not that
 inconvenient, but for the sake of training the perceptron, it
 would be more inconvenient. So we will make things a little bit
 more convenient by rearranging the terms. So we are just now
 applying the mathematical operation minus theta on both
 sides. So we are bringing theta here onto the left side. And
 then what we get is we return zero, if z is smaller, if z
 minus theta is smaller equal to zero, and we return one, if z
 minus theta is greater than zero. So in this way, this is
 our, I would say more convenient notation here. And if you think
 about it like that, you can think of the negative
 threshold. So this part here, you can think of it as a as a
 bias. So this is like a common term in deep learning, or also
 in machine learning, a bias unit, it's a little bit weird
 to use the term bias, because it's a little bit overloaded,
 there are other types of bias. For example, there's an
 inductive bias, like this relational inductive bias that we
 talked about, there's also a fairness bias, and now we have
 this bias, mathematical bias unit here. There's also a
 statistical bias, if you think of decomposing a loss function
 into a bias and variance term. So it's a bit unfortunate. But
 in the context of deep learning, if you read the term bias unit,
 that would be in this case, this, this theta, we can treat
 it as a parameter will become more clear in a few moments.
 Yes, in this representation, we are now using the bias as a
 parameter, when we compute z. So we set the minus theta to be
 B is short for bias unit. So it's easy to memorize B for
 bias unit. So we set B equals to this minus theta. And then we
 compute the Z, the net input is shown here. So just to recap,
 what we had before was multiplying the W's and the
 X's. So this is from index one to M. And this was our Z. Now,
 when we compute the Z, let's call it. Let's make that right
 to denote this on UC. This includes now our bias unit. So
 net input is now including the bias unit, if we want to update
 the figure to reflect that I've drawn here, this B, this B goes
 now also into the net input. So the net input is really this
 computation here. And yeah, it's also the same as shown here. And
 now what we can do is we can also write then the activation
 function or here the threshold function. So I should have
 actually used F, but the perceptron is the special case
 where the threshold function is equal to the activation function.
 So I can use either sigma or F doesn't matter here to be
 consistent. And so now we can have the zero here on the right
 hand side instead of having the minus theta. So this will make
 more sense from a learning perspective. So in this way,
 it's easier to parameterize the network, if we don't have to
 learn the threshold here on the right hand side is just easier
 to write in code. Because then we have a threshold function
 that just simply checks whether some something is greater or
 smaller than zero. But of course, you can also implement
 it. Like shown on the previous slide with a minus theta, it
 doesn't really matter. But this is also the common notation. And
 this will be something that you will also encounter when we talk
 about multilayer perceptrons, convolutional networks,
 recurrent networks. And when we use existing code
 implementations, for example, in pytorch, this is really like the
 common way also, if you look at recent deep learning papers. So
 this is like the like I said, the common notation that you
 will find in most modern texts, like maybe not textbooks,
 because textbooks are usually a few years behind with the
 trends. But if you look at a modern deep learning literature,
 you will find that people use this extra this notation with a
 additional bias unit here. Yeah, however, it's slightly
 inconvenient for mathematical notation compared to a slightly
 different notation again. So on the right on the next slide, I
 will show you how we can modify this even a little bit
 differently to have the bias as part of the other inputs. So
 here, what I've done now is I'm setting B equals to weight, I
 call that W zero. And this is again, or minus theta or minus
 or negative threshold. Now I'm including this bias as a weight.
 So first, let's maybe start with the figure. If you look at the
 figure here on the left hand side, what I've now what I've
 done now is I have removed this B here, I've removed that
 B here, compared to the previous slide, and now have the value
 one, this is really like a value, an integer or float
 number of value one. And then now I have the W zero as weight.
 So this is essentially our bias, but I'm writing it as a W,
 because then we can write this notation more compactly. So
 instead of writing it on the previous slide with index over
 one to M, like this, with a W with a B, or you can also write
 it as this, instead of this, I can change this to zero, and
 then I can get rid of it as slightly more compactly. It's
 not that interesting to do it like this. But as one advantage
 of doing it as the advantage is that we can now use a dot
 product, just x transpose W. If we wouldn't have the modified
 version here, what we would have to do is we would have to write
 it as x transpose W plus B, which is just slightly more work.
 It's not that much more work. But yeah, mathematicians are
 sometimes efficient or lazy. I would say not lazy, definitely
 not lazy, but efficient. So the notation here shown would kind
 of help us to make things a little bit simpler. So why am I
 telling you this? So this is something this notation here on
 this slide is something you will find in some or most textbooks,
 actually, whereas this is I would say the more modern one. I
 mean, for things like this, it might be overkill to use this
 separate bias. So this looks simpler. But when we talk about
 multi layer networks, actually, this becomes more convenient.
 And this is also what most framework use. And the reason
 why this is more convenient is because I'm going I don't want
 to switch too much because it probably gets confusing when you
 watch this video, and I'm going left and right and left and
 right. But one more thing about this slide is, if you want to
 use this notation, you have to modify the inputs here, right.
 So if you think of the iris example, you have x one, x two,
 x three, x four as your feature vector for one training
 example. So this would be your, let's say, first training
 example. Now, if you have this as input, and you want to use
 this notation, what you might have to do, what you would have
 to do is to have, you have to have one here. So you have to
 modify this list. So in practice, usually you have fixed
 size array, and then you would have to make a new array. That's
 how it would work in a computer, you would have to make a new
 array that is a little bit bigger. And then you have to
 move all the values you have to let me use different colors,
 maybe you have to make an x one, x two, let's start with one, put
 the one here, and then the x one, x two, x three, x four. So
 you have to make a new array to make the one fit into this array.
 And making a new array can be expensive, of course, it's an
 additional computation, it's just much simpler. If we just
 add this B to it, if we compute it as x transpose w plus B,
 this is computation actually simpler than adding a new value
 to an array, because then we have to make a copy of the array
 move all the values over. And this is something it's a
 computational consideration, essentially. Yeah, I'm not
 exactly sure what I wanted to show on this slide. I think I
 just wanted to emphasize again, that we can now use dot product.
 Alright, so this is our general framework. And now we are going
 to take a look at the perceptron learning rule. So how it
 actually learns the weights to classify classes. So assume we
 have a binary classification task. So here we have two
 classes, these blue dots and the yellow or orange squares. And
 the perceptron finds the decision boundary to classify
 these two classes. If the classes are separable, by that,
 I mean, if there is a linear decision boundary, I should have
 maybe set linearly separable, that means there has to exist a
 decision boundary that can separate these classes
 perfectly, then the perceptron is guaranteed to find it. So
 here, just looking at it, you may guess that decision one, you
 could potentially look like that, and then it will perfectly
 classify these two classes. So this is a problem with only two
 features for simplicity. Because if I have more than two features,
 it would be very challenging to draw it in a slide. So in this
 case, we only focus on the simple example. And I actually
 made an animated GIF here, I will play it in a second. And
 then you can see how the network or the perceptron goes about the
 task of finding this decision boundary. So starting at a
 random place, it will basically update the rule in each
 iteration. So there will be an iteration counter moving up by
 and it will try to adjust decision boundaries to make them
 better, basically. So let's take a look.
 Actually, I forgot how many iterations they are, there might
 be 20 or 30. So you can see it's still making mistakes. And the
 circle that is shown here, that is indicating which data point
 it currently looks at. So you can see it's on this black
 circle, it's moving around, because we have shuffled the
 data set. And then it will look at one training example at a
 time. And if it finds that there's a misclassification, then
 it will move the decision boundary. So if it right now,
 you can see it always finds classes that are already on the
 right, correct side of the decision boundary. So there are
 no updates are necessary, but it has to find are on the ones
 where it makes mistakes like these here, if it touches these
 updated. So if it touches these, see, then it will update. So
 yeah, it's back to one. So it was a little bit faster. Let me
 move on though. So yeah, what you can see here in the lower
 left corner is the results. Yeah, it was 49 iterations. So
 here's the final decision boundary, how it looks like. And
 you can see it, um, yeah, it classifies those two classes,
 say, class, zero and class one perfectly. Now, though, it's
 gone only guaranteed to converge if a solution exists. So if
 there is no solution to that, for example, if I have a blue
 dot that is here, there would be no decision boundary, where you
 can classify this data set perfectly. And what will happen
 is, if the perceptron encounters this point, it will move the
 decision boundary more to this side. So it may even move the
 decision boundary, let's say to here, but then all the other
 points are wrong. So it's moving it back here. But then again,
 this blue point is wrong, and it's going back to the right
 hand side. So it's always like flipping back and forth. So if
 the data set is not perfectly separable, then it will never
 stop to converge. Later in this course, we will of course learn
 about algorithms where this is not the case, where it will
 always converge to at least some solution, it will at least stop
 updating. Because I mean, this is a big issue. If you have a
 real world data set, classes are rarely separable by linear
 decision boundary, there's usually always a case where you
 can't separate things perfectly. And it would be very annoying if
 your algorithm makes these big jumps back and forth. So the
 Adeline algorithm, for example, it will converge even if classes
 are not linearly separable. And we will learn about this and not
 to distant future. But for now, let's stick with the perceptron
 learning algorithm, just outlining how it learns. So
 there are three scenarios. It's the one scenario is if is if we
 make a correct prediction. By that, I mean, if the predicted
 label y hat is equal to the target label. So for that, we
 can also have two scenarios of the if y hat is one, and y is
 one, or if the predicted label is zero, sorry, someone jumped
 here, the predicted label is zero. And the actual label is
 also zero, then we are correct. And then we don't have to do
 anything. So if both the prediction of the output are
 equal to the target, I think this word is extra. Okay, and
 there are no two scenarios and be where things have to be
 updated. It's if we are incorrect. So one scenario is
 where y hat is zero, and y is equal to one. So in this case,
 what we do is we add the input vector to the weight vector. So
 we update it like that. So that means, let me draw this maybe as
 an example. So if we have data points here, data points here,
 or let me spread them a little bit. Oh, that's easier to draw
 the decision boundary. So let's say, blue is class zero, and
 yellow is class one. In this scenario here, what we have is
 we have the prediction is zero, although the true label is one.
 So for example, for a decision boundary like that, and we
 encounter this data point, what we have to do then is we have
 to move things over here. It's by adding the input vector to
 the weight vector. Why is that I will show you exactly why that
 is later when I go over the geometric intuition, and it will
 become clear why we have to add the input vector to the weight
 vector rather than subtracting it. The other scenario is if the
 output is one, and the target is zero, and then we subtract the
 input vector from the weight vector. So there are two update
 rules here going on. They are both very similar, we can write
 this mathematically very compactly, as shown here. So
 here, assume we have a data set D, this is our training data
 set. So we have n data points here from one to n. So x our
 feature vectors, and y our class labels. Yeah, so this is our
 data set. And now here, that's the perceptron learning
 algorithm. So how does it work? First, we initialize the weights
 or the weight vector to zeros, if we have m weight vectors,
 that would be a vector of zeros. And we are assuming the
 notation where the weight includes the bias, so we don't
 have to write the bias separately, we could do that, but
 it would be a little bit more work here. So it would be a
 little bit more verbose. So there's a little bit more
 compact here. So then we have a follow up here. So for every
 training epoch, what is an epoch epoch means pass over
 the entire training set. So if I have a training set, like shown
 here, then an epoch would be processing every data point in
 this data set. So for example, I consider the first data point, I
 do my computation, then I consider the second one, and so
 forth until I reach the last one. And then this is one epoch
 when I when I reached this last data point, and I have
 completed one epoch. And then I may go back to the beginning and
 do another sweep over the data set, that would be then the
 second epoch and so forth. So epoch really means pass or
 iteration over the data set. So then this is our auto loop. So
 for every training epoch is our auto loop, and then the inner
 loop is for every data point in the data set, we perform steps
 A, B, and C. So if we have multiple epochs, that means we
 can do this multiple times. So we would just go back to the
 beginning of the training set and do it all over again. So but
 the interesting part happens now for every data point. So note
 here, the perceptron processes, one data point, or training,
 let's say, one training example at a time. So for every training
 example, compute the prediction, this is our prediction. And it's
 computed by computing the weighted sum, this is our net
 input. So this is input z. And this is our predicted label. And
 then we compute the error. So the error is computed by the
 true label minus the predicted label. Why is that? So if this
 is one and this is y hat is zero, for example, then so let's
 first let's assume the simple case where we make no error. So
 if we have zero and zero, then the error is zero. And then if
 you look at the row C here, we have the weight update. So C is
 the weight update, then the weight update or the new weight
 is the old weight plus the error times the input vector. But if
 the error is zero, if the prediction is correct here, zero
 minus 00, then we can cancel this term and there is no weight
 update. So let me clear this a little bit here. If both are
 one, then the error is also zero. And then we can also
 cancel this term. So we can see the weight update only happens
 if we have an error if we make a prediction error. And then now
 let's consider the case where we have a one and a zero here. So
 then the error would be one here, so we would have an error
 of one into then we would be adding the weight vector. So if
 we have the case for the predicted label as a zero, and
 the case for the true label, a one, this is when we are adding,
 adding something, right? So we are adding the feature vector
 here. So this is exactly like the case that we have here
 where y hat is zero, and y is one, this is the same case here.
 Now, let me, I can't delete if I switched slides. Okay, so then
 let me write it like this, with a different color. So if we have
 the last scenario, zero, and one, then our arrow is minus
 one. And in this case, if this is minus one, we are subtracting,
 right? So w minus x one, so we are subtracting the feature x i,
 we are subtracting the feature vector, which is equal to this,
 sorry, to this case here, where we are subtracting the input
 vector. So here, this is really just a mathematical summary of
 the slide I've shown you before. Alright, so I think if this is
 confusing, maybe look at that more slowly. And then in the
 next video, I will show you vectorization in Python. And
 after that, I will also show you how to implement perceptron in
 Python using NumPy and pytorch, which will I think, make this
 video more clear. So what we just have discussed, if this is
 still complicated, I would look at it a little bit more slowly,
 like step by step. And then maybe don't worry about it too
 much, because we will implement it in code. And after that, I
 think it will be really clear.