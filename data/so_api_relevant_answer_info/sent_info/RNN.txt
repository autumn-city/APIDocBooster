RNN Decoder is sequence to sequence mapping model.
In General We use Normal RNN for one to one sequence learning setup.
Furthermore, is it possible to create a word prediction RNN but with somehow inputting words pretrained on word2vec, so that the RNN can understand their meaning?
I understand that the fundamental problem that you are facing is "how to train RNNs (LSTM is a kind of RNN afterall) using Keras".
To answer why you are getting that specific error undefined name 'SimpleRNN', it seems you forgot to import SimpleRNN.
Second, and actually most important in your case, after the RNN output you are applying sigmoid function.
The basic RNN equation is along the lines of state_t = nonlinearity(matmul(state_t-1, state_weights) + matmul(input_t, input_weights)).
To see whether your rnn is behaving appropriately, I'd just print the hidden state at each time step, run it on some small random data (e.g.
Pay attention to distinguish two different things: the number of layers of your recurrent neural network and the number of time this RNN gets unrolled by the Back Propagation Through Time algorithm to handle sequence length.
The MultiCellRNN is taking care of creating a 3 layers RNN (you are creating three LAYERS there, and MultiCellRNN is only a wrapper to make easier to deal with them)
The tf.nn.dynamic_rnn is taking care of unrolling this three layered network for a number of times related to your sequence length
How does RNN regularization work?
and "RNN: Depth vs. Width".
To understand RNN regularization, one must understand how RNN handles information and learns, which the referred sections describe (though not exhaustively).
RNN regularization's goal is any regularization's goal: maximizing information utility and traversal of the test loss function.
The specific methods, however, tend to differ substantially for RNNs per their recurrent nature - and some work better than others; see below.
RNN regularization methods:
Recurrent weights: default activation='sigmoid' 

Pros: linearizing can help BPTT (remedy vanishing gradient), hence also learning long-term dependencies, as recurrent information utility is increased
Cons: linearizing can harm representational power - however, this can be offset by stacking RNNs
Residual RNNs: introduce significant changes, along a regularizing effect.
See application in [IndRNNs (hyper-link)]
Layer Normalization: some report it working better than BN for RNNs - but my application found it otherwise; [paper (hyper-link)]
See relevant info on [stateful RNNs (hyper-link)]
Weights: [see_rnn.rnn_histogram (hyper-link)] or [see_rnn.rnn_heatmap (hyper-link)] (examples in README)
Each regularization method is unique, and no two exactly alike - see "RNN regularizers".
RNN: Depth vs. Width: not as simple as "one is more nonlinear, other works in higher dimensions".
RNN width is defined by (1) # of input channels; (2) # of cell's filters (output channels).
As with CNN, each RNN filter is an independent feature extractor: more is suited for higher-complexity information, including but not limited to: dimensionality, modality, noise, frequency.
RNN depth is defined by (1) # of stacked layers; (2) # of timesteps.
Specifics will vary by architecture, but from information standpoint, unlike CNNs, RNNs are dense: every timestep influences the ultimate output of a layer, hence the ultimate output of the next layer - so it again isn't as simple as "more nonlinearity"; stacked RNNs exploit both spatial and temporal information.
Here is an example of a near-ideal RNN gradient propagation for 170+ timesteps:
From the documentation I understand that what they are saying is that the parameter sequence_length in the rnn method affects the performance because when set, it will perform dynamic computation and it will stop before.
For example, if the rnn largest input sequence has a length of 50, if the other sequences are shorter it will be better to set the sequence_length for each sequence, so that the computation for each sequence will stop when the sequence ends and won't compute the padding zeros until reaching 50 timesteps.
This does not mean that dynamic_rnn is less performant, the documentation says that the parameter sequence_length will not affect the performance because the computation is already dynamic.
Also according to [this post about RNNs in Tensorflow (hyper-link)]:
Internally, tf.nn.rnn creates an unrolled graph for a fixed RNN length.
That means, if you call tf.nn.rnn with inputs having 200 time steps you are creating a static graph with 200 RNN steps.
tf.nn.dynamic_rnn solves this.
You may think the static rnn is faster than its dynamic counterpart because it pre-builds the graph.
In short, just use tf.nn.dynamic_rnn.
There is no benefit to tf.nn.rnn and I wouldn’t be surprised if it was deprecated in the future.
dynamic_rnn is even faster (or equal) so he suggests to use dynamic_rnn anyway.
Imagine an RNN as a stacked deep net with
DynRNN breaks with this constraint and adapts to the actual sequence-lengths.
To better understand dynamic unrolling, consider you would you create RNN from scratch, but using Tesorflow (I mean without using any RNN library) for 2 time stamp input
This is what Tensorflow does by dynamic unrolling
It creates these 50 outputs for you via tf.dynamic_rnn() units.
At each time step, you give an input of a certain size (that you are calling "the number of elements the RNN should process in sequence").
here num_units refers to the number of units in LSTM(or rnn) cell.
In general, when you shuffle the training data (a set of sequences), you shuffle the order in which sequences are fed to the RNN, you don't shuffle the ordering within individual sequences.
I am assuming that you want to mix the two outputs of the RNNs at the first Dense layer (with 64 units) because after that, there's no telling which input is which.
If nn.RNN is bidirectional (as it is in your case), you will need to concatenate the hidden state's outputs.
In case, [nn.RNN (hyper-link)] is bidirectional, it will output a hidden state of shape: (num_layers * num_directions, batch, hidden_size).
In the context of neural networks, when the RNN is bidirectional, we would need to concatenate the hidden states from two sides (LTR, and RTL).
[https://github.com/carpedm20/pixel-rnn-tensorflow (hyper-link)]
[https://towardsdatascience.com/summary-of-pixelrnn-by-google-deepmind-7-min-read-938d9871d6d9 (hyper-link)]
If you structure your code to use [tf.contrib.rnn.DropoutWrapper (hyper-link)], you can set variational_recurrent=True in your wrapper, which causes the same dropout mask to be used at all steps, i.e.
Your code indeed reuses the cell variables, because cells are initialized lazily and only once (see [RNNCell.build() (hyper-link)] method, which actually creates the kernel and bias).
Your current code creates two independent RNN layers (each one is deep), with same initial state.
That's why you should specify reuse=True before calling tf.dynamic_rnn as the question you refer to suggests, this will cause tensorflow share the kernels of all cells.
Suppose that RNN(.,.)
is an RNN (LSTM/GRU) cell that has a bunch of trainable parameters such as weight matrices and biases.
These parameters are all the same and being learned by each X[i] and hidden instances that are fed into the RNN cell at each iteration.
So back to your question, an RNN network is actually a multiple copies of RNN cells that gets trained as you proceed training.
You can just make the output size a list with any dimensions and then the RNN will track the outputs.
The class below also includes the use of constants in the RNN call because the previously mentioned paper passes an encoder latent space (z_enc) to the recurrent decoder:
That message means: the input going into the rnn has 2 dimensions, but an rnn layer expects 3 dimensions.
For an RNN layer, you need inputs shaped like (BatchSize, TimeSteps, FeaturesPerStep).
You can achieve this by simply calling your tf.rnn.LSTMCell object once.
Have a look at the documentation for [RNNCell.__call__() (hyper-link)] for more details on what the shape of input_single and state_single should be, if you have a good reason not to use cell.zero_state().
I suspect you're also confused by the use of tf.nn.dynamic_rnn.
It's important to note that the dynamic in dynamic_rnn refers to the way that TensorFlow unrolls the recurrent part of the network.
in tf.nn.rnn, the recurrence is done statically in the graph (there is no internal loop, it's unrolled at graph construction time).
In dynamic_rnn however, TensorFlow uses tf.while_loop to iterate inside the graph at run time.
Dynamic RNN's allow for variable sequence lengths.
You might have an input shape (batch_size, max_sequence_length), but this will allow you to run the RNN for the correct number of time steps on those sequences that are shorter than max_sequence_length.
In contrast, there are static RNNs, which expect to run the entire fixed RNN length.
In short, dynamic_rnn is usually what you want for variable length sequential data.
LSTM is the most common example of an RNN.
You usually need to pass all outputs to the next RNN layer or the last output for the last RNN layer.
The RNN documentation can be found [here (hyper-link)].
How to do a regression with a RNN in Keras it is very well explained here.
Since RNN takes sequential inputs, index of each element in the input sequence can be referred as a time step of that sequence.
According to my understanding, inputs to an RNN refers to individual inputs provided to RNN at each time step.
For example, in [1,2,3,4,5,....,100], each element is an input to the RNN at a particular time step.
If you are using [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py (hyper-link)]
GridRNNCell was created some while ago, at that time all the LSTMCells in Tensorflow was using concat/slice instead of tuple.
problem 2, GridRNNCell will not project the input if you pass None.
We can also use 2 input dimensions, by instantiate the GridRNNCell directly.
All I had to do was multiply the hidden size by 2 since output size of birrectional RNN is twice that of rnn.
I am not an expert at RNNs but giving it a try.
The dynamic_rnn handles the passing of states and inputs.
Like explained in the [doc (hyper-link)], Keras expects the following shape for a RNN:
follow the comments and see how very simple input is manipulated for using it 
as input in rnn/lstm
Note that by default, the second input dimension passed to RNN will be treated as the batch size.
Therefore, if the 420000 is the number of batches, pass batch_first=True to the RNN constructor.
I think you need to get the dynamic shape of x_rnn instead of the static one.
You can replace x_rnn.shape[0] as tf.shape(x_rnn)[0]
The short answer is that the code is now here: [https://github.com/tensorflow/models/tree/master/tutorials/rnn/ptb (hyper-link)]
In TensorFlow the RNN functions take a list of tensors (because num_steps can vary in some models).
Then you need to take care of the fact that your inputs are int32s, but a RNN cell works on float vectors - that's what embedding_lookup is for.
I think the ptb tutorial is a reasonable place to look, but if you want an even more minimal example of an out-of-the-box RNN you can take a look at some of the rnn unit tests, e.g., here.
[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/kernel_tests/rnn_test.py#L164 (hyper-link)]
I'm not sure on how to convert a 4D Tensor into a 3D one for it to be accepted by a dynamic_rnn but I think this might give you an idea on how to use GridLSTM:
The RNN will indeed only learn dependencies within one batch.
Note, that training of an RNN is not based on processing your data once, you do it multiple times - this should show you why duplicating dataset changes nothing (what is a difference between going once through 2 copies of a data and going twice through one copy?)
an rnn implementation like outputs = rnn(...)) the output of this computation is returned as a Tensor.
Told this, I think you need to take the final state of an RNN and provide it as initial state of a subsequent computation.
A) If you are using RNNCell implementations During the construction of your model you can construct the zero state like this:
How you actually construct your feed_dict depends on the implementation of the RNN.
Weights are shared in an RNN, and the activation response for each "recurrent loop" might have completely different statistical properties.
Batch normalization applied to RNNs is similar to batch normalization applied to CNNs: you compute the statistics in such a way that the recurrent/convolutional properties of the layer still hold after BN is applied.
For RNNs, this means computing the relevant statistics over the mini-batch and the time/step dimension, so the normalization is applied only over the vector depths.
The problem with doing this for the recurrent outputs of an RNN is that the parameters for the incoming distribution are now shared between all timesteps (which are effectively layers in backpropagation-through-time, or BPTT).
Why Yes, according to the paper [layer normalization (hyper-link)], in section it clearly indicates the usage of BN in RNNs.
In multiple-layer RNNs, you may consider using layer normalization tricks.
RNN/LSTM don't learn long range dependencies well.
For instance the [DA-RNN paper (hyper-link)] states
I came across the same question as you when reading the similar RNN code.
From my understanding the rnn_cell.zero_state actually returns you a tuple of tensors, which are feed-able.
To confirm the discussion in the comments: when constructing a static RNN using BasicLSTMCell,  state is a two-tuple of (c, h), where c is the final cell state and h is the final hidden state.
the RNN network use of hidden layer that update over time.
the LSTM is a type of RNNs that overcome to forgetting long-time dependencies.
RNN basically need some features and label at training phase to learn sequence in the data.
But in case of Image data, we use combination of CNN and RNN(CNN-RNN).
Here on the top of RNN layers (LSTM or GRU) Convolution layers are used to extract features from image and then we feed these features to RNN layers.
If you're intentionally calling recurrent_nn_model several times and combine these RNNs into one graph, you should use different variable scopes for each one:
We tried to use RNN to predict model with an basic idea that each word in a sentence will be affected by the former or the latter words(that's what we called context), so we use the sequential inputs of words in a sentence to represent words appear one by one with the time passes.
For example if you have n sequences, each sequence is of length m and each of your sequence data has d features the input of your RNN must be of dimension (n,m,d).
Here is a post by Danijar that implements the many to one rnn in tensorflow.
[https://danijar.com/introduction-to-recurrent-networks-in-tensorflow/ (hyper-link)] Regarding the dynamic time steps, the dynamic_rnn() allows you to specify any number of time steps.
by passing data into the dynamic_rnn(), this will create an rnn or a GRU or LSTM and will be unrolled time_steps automatically based on the second dimension in data.
A simple way to feed in an RNN state is to simply feed in both components of the state tuple individually.
First, in rnn class module, you'd better use "super(rnn,self).__init__()" to replace "super().__init__()".
And in rnn, the sigmoid function should be 1/(1+exp(-x)), instead of the logsigmoid function.
Third, the output should be activated by the softmax fucntion, if you use the rnn to classify.
I made a wrapper derived from RNNCell that accomplishes what I need.
Now your input is (batch_size=5,num_sentences=25,time_step=15, features=50) which is a invalid shape for any type of RNNs.
The example folder includes a lot of examples dealing with RNNs and there are built-in RNN estimators that you can just plug into your existing code.
From your variable name bidirectional_rnn/fw/gru_cell/w_ru you can see that the scope is not applied.
The other thing is that such a model cannot work as an encoder-decoder model because the decoder RNN cannot be bidirectional.
This means you only have the left context for the forward RNN, but you don't have the right context for the backward RNN.
In a recurrent neural network (RNN), however, your network functions a tad differently:
Most RNNs employ a forgetfulness factor in order to attach more importance to more recent network states, but that is besides the point of your question.
From the image, 'batch size' is the number of examples of a sequence you want to train your RNN with for that batch.
(in my case, my RNN takes 6 inputs) and finally, your time steps are the 'length', so to speak, of the sequence you're training
RNN's "batch size" is to speed up computation (as there're multiple lanes in parallel computation units); it's not mini-batch for backpropagation.
An easy way to prove this is to play with different batch size values, an RNN cell with batch size=4 might be roughly 4 times faster than that of batch size=1 and their loss are usually very close.
As to RNN's "time steps", let's look into the following code snippets from  [rnn.py (hyper-link)].
static_rnn() calls the cell for each input_ at a time and BasicRNNCell::call() implements its forward part logic.
Again, to predict next word in a text corpus with BasicRNNCell, small time steps might work.
From [this diagram (hyper-link)] and above code snippets, it's obviously that RNN's "batch size" will no affect weights (wa, wb, and b) but "time steps" does.
So, one could decide RNN's "time steps" based on their problem and network model and RNN's "batch size" based on computation platform and data set.
I am not sure the rest of your code is alright, but in order to fix this error, you can convert your rnn_out list to a torch tensor by adding the following line after the ending of your for loop:
The difference I noticed is that the code in the [second implementation (hyper-link)] uses tf.nn.rnn which takes list of inputs for each time step and generated the list of outputs for each time step.
If you carefully notice the way the inputs are being fed to the RNN in both the implementations.
tf.nn.dynamic_rnn(also tf.nn.static_rnn) has two return values; "outputs", "state" ([https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn (hyper-link)])
As you said, "state" is the final state of RNN, but "outputs" are all hidden states of RNN(which shape is [batch_size, max_time, cell.output_size])
You can use "outputs" as hidden states of RNN, because in most library-provided RNNCell, "output" and "state" are same.
Basic [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py#L347 (hyper-link)]
GRU [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell_impl.py#L441 (hyper-link)]
As you have said RNN only accept as input a Tensor like [batch_size, sequence_lentgh, features].
In order to use RNN from tensorflow you will have to extract the features with a CNN for each frame and convert your CNN output data to a tensor that follows [batch_size, sequence_lentgh, features] shape in order to feed it to the RNN.
Because the idea of the Bidirectional RNNs is to have two hidden states for each input, giving information about what should be (or is) before the current input, and what should be (or is) after the current input.
This effectively goes through RNN multiple times and will probably raise OOM, not sure what you are after here.
If that's the case there's not much you can do AFAIK as the RNN computations are simply too long to fit into the GPU.
[...] in an RNN you do multiple forward passes for each input step.
As you answered yourself - RNN are for sequences.
The main reason is that RNN can model process which is responsible for each conequence, so for example given sequences
RNN will be able to build a model, that "after seeing '1' I will see two more" and correctly build a prediction when seeing
Once again, such phenomena are much better addressed by RNN than non-recurrent models.
Second, RNNs are as "deep" as it gets.
Considering the most basic RNN, the Elman Network: During training with Backpropagation through time (BPTT) they are unfolded in time - backpropagating over T timesteps.
This makes RNNs so powerful for timeseries prediction (and should answer both your questions).
Understanding Elman Networks and BPTT is the needed foundation to understand any other RNN.
While it's tempting to say let's make T=infinity and give our RNN as much memory as possible: It doesn't work.
For an RNN you have to define what is the hidden state so you may set it to be the output of the CNN on the previous element in the sequence.
However, it doesn't do so in an RNN scheme, as RNNs are slower to train and evaluate due to their sequential nature.
Also note that RNNs are inferior (mainly) due to their problem of short-term memory.
Try this instead (I'm assuming batch_size=10 and rnn_size=16)
Say your first dense layer (or rnn layer) contains 100 neurons.
Most RNNs don't like data that don't have a constant mean.
An RNN is computing operations on a sequence of data, i.e.
But you can still use a RNN for 1D vectors, by interpreting them not as one n-dimensional vector but as a time series of n steps, each containing a 1D vector.
Assuming the embedding and the RNN dimension are the same, you can do:
This is used when you split long sequences into multiple batches and what the RNN to remember the state between batches, so you do not have yo manually set initial states.
You can get the final state  of the RNN (that you wany you want to use for classification) by simply indexing the last position from states.
The BRNN can be trained without the limitation of using input information just
  up to a preset future frame.
I guess you are talking about [tf.nn.static_bidirectional_rnn (hyper-link)].
For an example of how to do this, take a look at this RNN tutorial.
[https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/ptb (hyper-link)]
OK I solved the problem by copying code from Makefile in the Caffe installation to Makefile for caffe-crfrnn.
RNN-T has a transcription network (analogous to an acoustic model), a prediction network (language model) and a joint network (/function, depending on implementation) that combines the outputs of the prediction network and the transcription network.
The ALPHASIZE you're passing to the SimpleRNN layer is a clue: the size of the alphabet is one of the dimensions of the data that is expected.
You could put the construction of your rnn into 2 different variable scopes to ensure they use different internal variables.
or by using the scope argument of the dynamic_rnn method:
The formular in [Pytorch Document (hyper-link)] in RNN is self-explained.
I believe you can find a better method than RNN.
You can also try and yearly difference but that depends on your choice in window size(remember RNNs dont do well with large windows).
SO your inputs will be the preciding x values and preceding normalized y values of whatever window size you choose, and your target value will be the normalized output at a given time step t. Just so you know a 1-D Conv Net is good for classification but good call on the RNN because of the temporal aspect of temperature spikes.
Looking at examples from Element-Research/rnn or torch/nn is also very helpful.
Karpathy's [char-rnn (hyper-link)] is also interesting.
In the simple RNN case, a network accepts an input sequence x and produces an output sequence y while a hidden sequence h stores the network's dynamic state, such that at timestep i: x(i) ∊ ℝM, h(i) ∊ ℝN, y(i) ∊ ℝP the real valued vectors of M/N/P dimensions corresponding to input, hidden and output values respectively.
The RNN changes its state and omits output based on the state equations:
In the case of gated RNNs (GRU or LSTM), the state equations get somewhat harder to follow, due to the gating mechanisms which essentially allow selection between the input and the memory, but the core concept remains the same.
So far I also can't find API references about rnn functions on their site.
[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn.py (hyper-link)]
[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py (hyper-link)]
The bad news is that there is much complexity and layers of abstraction in the Python code, and that the code itself is the best available "documentation" of the higher-level RNN and seq2seq "API" as far as I can tell...thankfully the code is well docstring'd.
Here is a breakdown of the RNN seq2seq code from the top down as of version r1.0:
[ models/tutorials/rnn/translate/translate.py  (hyper-link)]
[ models/tutorials/rnn/translate/seq2seq_model.py  (hyper-link)]
...class Seq2SeqModel() sets up a sophisticated RNN encoder-decoder with embeddings, buckets, attention mechanism...if you don't need embeddings, buckets, or attention you'll need to implement a similar class.
Examples include one2many_rnn_seq2seq and models without embeddings/attention also provided like basic_rnn_seq2seq.
[ tensorflow/tensorflow/contrib/rnn/python/ops/core_rnn.py  (hyper-link)]
...provides a wrappers for RNN networks like static_rnn() with some bell and whistles I usually don't need so I just use code like this instead:
RNN docs for the current/master version of TensorFlow:
[https://www.tensorflow.org/versions/master/api_docs/python/nn.html#recurrent-neural-networks (hyper-link)]
RNN docs for a specific version of TensorFlow:
[https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#recurrent-neural-networks (hyper-link)]
For the curious, here are some notes about why the RNN docs weren't available initially:
[API docs does not list RNNs (hyper-link)]
Given (batch_size, timesteps, data_dim) the output of the SimpleRNN will be (batch_size, timesteps, n_units), and the output of Dense layer will be (batch_size, n_output).
You're not doing anything here that actually calls for LSTM (or any RNN), you're not actually using the time dimension, and you're basically just trying to learn addition.
Each unit of an RNN is able to receive values from all of the units in the previous timestep.
In other words, given a sequence x, consisting of T inputs, we are training the RNN language model h to estimate the following distribution: p(xt | x<t, h).
This sequence contains t=6 timestep inputs to the RNN: x1, x2, x3, x4, x5, x6
This is not a RNN, it's just a fully connected network (FC or Dense).
See a simple RNN model
The RNN models will create their variables with get_variable, and you can control the initialization by wrapping the code which creates those variables with a variable_scope and [passing a default initializer to it (hyper-link)].
Unless the RNN specifies one explicitly ([looking at the code (hyper-link)], it doesn't), [uniform_unit_scaling_initializer is used (hyper-link)].
How to initialize weight matrices for RNN?
I believe people are using random normal initialization for weight matrices for RNN.
I don't know if people use that in RNN, but I imagine you can even try those in RNN if you want to see if it helps.
The [dynamic_rnn (hyper-link)] function has a parameter called scope.
So you should create your own scope (using with tf.variable_scope('scope_name', reuse=True)) and set it when calling dynamic_rnn function.
The equation for Forward Propagation of RNN, considering Two Timesteps, in a simple form, is shown below:
To elaborate it, consider RNN has 5 Neurons/Units, more detailed equation is mentioned in the screenshot below:
[Equation of Forward Propagation of RNN (hyper-link)]
Back Propagation in RNN:
The Back Propagation in RNN is done through each and every Timestep.
[Flow of Forward Propagation and Back Propagation in RNN (hyper-link)]
The problem is the objective function with the Bi-RNN model in your application (next character predictor).
The unidirectional RNN (such as [ptb_word_lm (hyper-link)] or [char-rnn-tensorflow (hyper-link)]), it is really a model used for the prediction, for example, if raw_text is 1,3,5,2,4,8,9,0, then, your inputs and target will be:
But in Bi-RNN, the first prediction is really not just (1)->3, because the output[0] in your code contians the reverse information of the raw_text by use bw_cell (also not (1,3)->5, ..., (1,3,5,2,4,8,9)->0).
I think you can give me the right answer very easy, and this is also the reason why you getting an extremely low loss in your Bi-RNN model for the application.
In fact, I think Bi-RNN (or Bi-LSTM) is not an appropriate model for the application of next character predictor.
Bi-RNN need the full sequence when it works, you will find you can't use this model easily when you want to predict the next character.
2 questions asked: why RNN is slower to train, why it doesn't reach good accuracy
Generally speaking, RNN networks should perform relatively better on natural language data as they are superior to other NN networks at capturing long-term dependencies ––something that is seen commonly in textual data.
Some tips that you get the RNN network improved:
As for the training time, RNNs are being trained slowly because of their recurrent nature; a single RNN network cannot be parallelized to reduce the training time since each cell in the network does need the output from the previous cell for the computations.
As the Tensorflow documentation, 
[https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn (hyper-link)]
You can specify the initial state as an input for the RNN network,
Also, Tensorflow RNN networks output the final state.
This will allow you to create a RNN that can run in real-time ;)
So before thinking about an LSTM or an RNN, I would look at cleaning up or improving your data.
First of all for a GRU (RNN) to be efficient, you may need more data to train.
Otherwise, you can try [torch-rnn (hyper-link)].
It uses Adam for optimization and hard-codes the RNN/LSTM forward/backward passes for space/time efficiency.
Providing an initially trained state matrix by the initial_state= argument gives the RNN cell a trained memory of its previous activations.
If we set the initial weights which have been trained on some other model or the previous model, it means that we are restoring the memory of the RNN cell so that it does not have to start from scratch.
Is it only required in a single RNN cell and not in a stacked cell like in the example provided in the link?
I exactly don't know that why haven't they set the initial_state in the Stacked RNN example, but initial_state is required in every type of RNN as it holds the preserves the temporal features across time steps.
Maybe, Stacked RNN was the point of interest in the docs and not the settings of initial_state.
In most cases, you will not need to set the initial_state for an RNN.
In the case of seq2seq RNN, this property may be used.
Your RNN maybe facing some other issue.
Your RNN build ups its own memory and doesn't require powerup.
After each batch, the last state of the RNN is passed as the initial state of the next batch.
This effectively allows you to train the RNN as if it was one very long chain over the entire PTB corpus (and this explain why model.final_state is evaluated and why the state is passed into m.initial_state in the feed_dict).
Neural networks built out of CNN + RNN + CTC work on character-level.
Just one additional note: RNNs are used to propagate information along the sequence, e.g.
You can see example implementations at [https://github.com/pytorch/benchmark/blob/master/rnns/fastrnns/custom_lstms.py (hyper-link)]
For the SimpleRNN layer this is 128 * 128 + 128 * 28 + 128 = 20096 (see [this answer (hyper-link)]).
The [tf.nn.dynamic_rnn() (hyper-link)] or [tf.nn.rnn() (hyper-link)] operations allow to specify the initial state of the RNN using the initial_state parameter.
There is also a [tf.nn.state_saving_rnn() (hyper-link)] to which you can provide a state saver object, but I didn't use it yet.
Thanks to [this answer to another question (hyper-link)] I was able to find a way to have complete control on whether or not (and when) the internal state of the RNN should be reset to 0.
First you need to define some variables to store the state of the RNN, this way you will have control over it :
Secondly build the RNN and retrieve the final state :
So now you have the new internal state of the RNN.
So in the next batch the "initial_state" of the RNN will be fed with the final state of the previous batch :
My advice is to add this op every time you run the RNN.
The second op will be used to reset the internal state of the RNN to zeros:
For building an RNN with for classification, [the official pytorch tutorial (hyper-link)] is very easy to learn from.
I don't think you're making an unconventional use of a RNN/LSTM, and your idea makes sense.
If I understood it correctly, your idea involves using a many to one RNN:
[ (hyper-link)]
Source: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/ (hyper-link)]
Many to one RNNs are very common, and you wouldn't be making an unconventional use of them.
From the documentation of [tourch.nn.RNN (hyper-link)], the RNN is actually an Elman network, and have the following properties seen [here (hyper-link)].
So to fix this problem, I suggest using a network which is well known for handling long term dependencies well, namely the Long short-term memory (LSTM) rnn, for more information see [torch.nn.LSTM (hyper-link)].
Keep "h_state = None" and change torch.nn.RNN to torch.nn.LSTM.
Instead of using [rnn_cell]*3, I created 3 rnn_cells (stacked_rnn) by a loop (so that they don't share variables) and fed MultiRNNCell with stacked_rnn and the problem goes away.
I guess it's because your RNN cells on each of your 3 layers share the same input and output shape.
The way you stack up your MultiRNNCell probably implies that 3 cells share the same input and output shape.
I stack up MultiRNNCell by declaring two separate types of cells in order to prevent them from sharing input shape
If you want to use an RNN, go ahead, but this is a nice easy solution
I dont think so, you can choose between tanh and relu but is to be one of them when using nn.RNN as far as I know (and I dont think there is a work around).
But you could implement the RNN youself quite easily without using the implemented module and then use whatever activation you want.
No, the PyTorch [nn.RNN (hyper-link)] module takes only Tanh or RELU:
Removing non-linearity from RNN turns it into a linear dense layer without any activation.
Fundamentally, an RNN for timesteps works as below -
So the final form of the state of an RNN after 2 timesteps is simply Wx+b like the linear layer without activation.
In other words, removing the non-linearity from an RNN turns it into a linear dense layer without any activation, completely removing the notion of time-steps.
many to many vs. many to one: In keras, there is a return_sequences parameter when your initializing LSTM or GRU or SimpleRNN.
When you have return_sequences in your last layer of RNN you cannot use a simple Dense layer instead use TimeDistributed.
Refer this blog for more details [Animated RNN, LSTM and GRU (hyper-link)].
tf.nn.dynamic_rnn expects inputs of shape [batch_size, max_time, ...].
Naturally RNN's output contains 5 entries, one per input step: [None, 5, cell_units].
Your RNN output has the wrong shape for tf.summary.image.
In your code, you're calling tf.summary.image with rnn_outputs, which has shape [55413, 4, 100].
Assuming your images are 55413-by-100 pixels in size and that each pixel contains 4 channels (RGBA), I'd use tf.reshape to reshape rnn_outputs to [1, 55413, 100, 4].
I don't think I can help you visualize the RNN's operation, but when I was learning about RNNs and LSTMs, I found [this article (hyper-link)] very helpful.
You can plot each RNN output as an image with one axis being the time and the other axis being the output.
The problem here is that RNN layers expect a 3D tensor input of the form: [num samples, time steps, features].
I think that depends on how the RNN was trained and how you are using it.
While I can't comment on general practice for RNN state initialization, here is how we managed to force the initial state definition.
Finally, the RNN is defined in terms of the new Dynamic RNN:
I think they have already given a simple example on how to use RNN symbol to build LSTM (they have mode option for this, i.e., mode='lstm').
Here is the example, check it out: [https://github.com/dmlc/mxnet/blob/master/example/rnn/rnn_cell_demo.py (hyper-link)]
The problem we're trying to solve by gradient clipping is that of exploding gradients: Let's assume that your RNN layer is computed like this:
Overall, [LSTMs (hyper-link)] have better learning properties than vanilla RNNs, esp.
Update: From the comments of OP, I understand that the OP had problems understanding the output of PyTorch's RNN module.
An RNN update can be written (w/o bias and non-linearity) as:
Now, the RNN module implements this and outputs the hidden states at the last layer, i.e., h(t,L) for all t and the last hidden state at each layer i.e., h(N,1) to h(N,L) (as h_n).
A lot of problems that use RNN have output either the same size as the sequence (for example, POS tagging) or a single output (for example, classification).
Using only these outputs, it is possible to add more RNN layers (using the output) or continue processing the sequence (using the last hidden state).
This should be easy to do using Tensorflow's RNN cell library.
Each RNN cell type is implemented a subclass of RNNCell; you can easily add your own new subclass.
Take a look at the code that defines some of the common RNN cells:
[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py (hyper-link)]
A major problem with RNNs and with generative models in general is that during train time, they are not trained on their own predictions.
The idea is to allow the model to be trained on it's own outputs gradually using a decay parameter which increases the probability of a predicted token being fed into the RNN rather than the gold label.
Your RNN functions seems to be ok.
Since you are using RNNs for regression problem (not for classification), you should use 'linear' activation at the last layer.
So if you are working on video it's better to use RNN because RNN can remember previous states.
Your question is very interesting.The output of CNN is 4 dimension,but the input of RNN require 3 dimension.
Finally I can pass image tensor though RNN Layer
But the problem after is some how pytorch need the first hidden state as [1, 1, 1] only.
And now my output of RNN is [1, 20, 1] .
so far we randomly created a three tensor with different length (timestep in the context of RNN) , and we first pad them to the same length, then packed it.
In short, rnn would no even see the pad timestep with pack_padded_sequence
And now let's see what's the difference after we pass padded and packed to rnn
The values of padded_hn and packed_hn are different since rnn DOES compute the pad for padded yet not for the 'packed' (PackedSequence object), which also can be observed from the last hidden state: all three batch entry in padded got non-zero last hidden state even if its length is less than 25.
We discussed how to read from the CSV file and to form batches here: [[Converting TensorFlow tutorial to work with my own data (hyper-link)] There is detailed code there that works (not for RNN, but you can adapt it).
Below code & explanations cover every possible case of a Keras/TF RNN, and should be easily expandable to any future API changes.
Completeness: code shown is a simplified version - the full version can be found at my repository, [See RNN (hyper-link)] (this post included w/ bigger images); included are:
I/O dimensionalities (all RNNs):
Output: same as Input, except:


channels/features is now the # of RNN units, and:
return_sequences=True --> timesteps_out = timesteps_in (output a prediction for each input timestep)
return_sequences=False --> timesteps_out = 1 (output prediction only at the last timestep processed)
Plot reveals a lesser-known advantage of Bi-RNNs - information utility: the collective gradient covers about twice the data.
Plot color-codes each RNN unit consistently across samples for comparison (can use one color instead)
One simple approach is to compare distributions at beginning vs. later in training: if the difference isn't significant, the RNN does poorly in learning long-term dependencies
EX 7: LSTM vs. GRU vs. SimpleRNN, unidir, 256 units -- return_sequences=True, trained for 250 iterations
show_features_2D(grads, n_rows=8, norm=(-.0001, .0001), show_xy_ticks=[0,0], show_title=False)
LSTM, for one, bears the most parameters per unit, drowning out SimpleRNN
In this setup, LSTM definitively stomps GRU and SimpleRNN
For more convenient code, see repo's [rnn_summary (hyper-link)]
Thus RNN cannot be easily trained as if you try to compute gradient - you will soon figure out that in order to get a gradient on n'th step - you need to actually "unroll" your network history for n-1 previous steps.
This technique, known as BPTT (backpropagation through time) is exactly this - direct application of backpropagation to RNN.
People are creating workaround on many levels, by for example introduction of specific types of RNN which can be efficiently trained (LSTM, GRU), or by modification of training procedure (such as gradient clamping).
It would make more sense for your RNN to receive a sequence of n timesteps and 1 value per time step than the other way around.
RNNs process several sequences at the same time, in parallel.
The number of sequences processed in parallel is the batch_size, and the shape of the input to the RNN will be (batch_size, N).
(1) is also consequential for stateful RNNs, as Keras constructs batch_size number of independent states, which'll still likely behave as intended as all they do is keep memory, but still worth understanding fully - see [here (hyper-link)]
The reason that exploding\vanishing gradient is common in rnn is because while doing backpropagation (this is called backpropagation through time), we will need to multiply the gradient matrices all the way to t=0 (that is, if we currently at t=100, say the 100's character in a sentence, we will need to multiply 100 matrices).
However, feed forward neural networks usually don't have so many hidden layers, while the input sequences to rnn can easily have many characters.
CuDNN currently doesn't support INT8 RNN, we suggest you use FP16 input output to achieve better performance than float.
All supported config combinations of cuDNN RNN can be found here
[https://docs.nvidia.com/deeplearning/sdk/cudnn-developer-guide/index.html#features-of-rnn-functions (hyper-link)]
Let us know if you have any other issues!
It seems like trainr in the package rnn needs binary format to the input and output values.
Reference code like this, 
[https://github.com/BrotherJing/RNN_tabletennis (hyper-link)]
output is batched data from RNN.
You should create a variable for the state, then use tf.assign to assign the result from your RNN cell to that variable.
If you need an intermediate state, let's say you ran for timesteps 0:124 and you want step 10, you should split that up into 2 RNN cells, one that processes the first 10 timesteps and the second that continues processing the next 114 timesteps.
This shouldn't affect training and back propagation as long as you use the same cell (LSTM or other cell) in both static_rnn functions.
The 'tf.contrib.rnn.static_rnn' has the following signature:
To do this, we initialize the initial state of the RNN at window i with the final state of the window i-1 using the parameter:
initial_state: (optional) An initial state for the RNN.
[RNN Tutorial (hyper-link)] This is the official tutorial of tensorflow.
Take a look at [min-char-rnn (hyper-link)] by Andrej Karpathy.
From the pytorch doc [https://pytorch.org/docs/stable/nn.html?highlight=rnn#torch.nn.RNN (hyper-link)]
Edit : If you want to create a sequence of outputs of size 2, perhaps the best way would be to stack another RNN on top of your first one with input_size 10 and output_size 2, they should be stackable inside a Sequential without any trouble.
By the way, do we use tf.nn.rnn_cell.BasicRNNCell or tf.contrib.rnn.BasicRNNCell?
Yes, they are synonyms, but I prefer to use tf.nn.rnn_cell package, because everything in tf.contrib is sort of experimental and can be changed in 1.x versions.
Say, you calculate the "loss" after having (outputs, states) from dynamic_rnn call.
Multi-dimensional RNNs are well supported in Tensorflow, but not added to the legacy seq2seq interface.
Please use the current (dynamic_rnn) API and you'll see that multi-dimensional tensors work fine (there are even pre-made multi-dimensional RNN cells in contrib).
The BasicRNN is not an implementation of an RNN cell, but rather the full RNN fixed for two time steps.
An RNN cell is one of the time steps in isolation, particularly the second one, as it should include the hidden state of the previous time step.
The next hidden state is calculate as described in the [nn.RNNCell documentation (hyper-link)]:
In your BasicRNN there is only one bias term, but you still have a weight Wx for the input and the weight Wy for the hidden state, which should probably be called Wh instead.
This also means that you only have one calculation, corresponding to the formula of the nn.RNNCell, which was the calculation for the Y1, except that it uses the hidden state that was passed to the forward method.
In the tutorial, they opted to use nn.RNNCell directly instead of implementing the cell.
You will need to convert raw audio signal into spectrogram or some other convenient format that is easier to process using RNN/LSTMS.
It can not work with arbitrary python objects, which each of MultiRNNCell instances are, that's why API is forcing you to pass tensors as input and returns tensors as output.
Any RNN cell in tensorflow, MultiRNNCell in particular, is a factory.
If you mean to use the RNN layers that are already instantiated, these are normal tensors, not MultiRNNCell cells.
There are more weights as you call tf.nn.rnn_cell.LSTMCell.
They are the internal weights of the RNN cell, which tensorflow created it implicitly when you call the cell.
You need to get rid of the extra dimension because tf.nn.rnn takes inputs as (batch_size, input_size) instead of (batch_size, 1, input_size).
Retrieve the the indices specified by the x placeholder and get the rnn input, which is size (batch_size, num_steps, state_size).
tf.squeeze sqeeze them to (batch_size, state_size), forming the desired input format for tf.nn.rnn.
It had nothing to do with RNN nor TensorFlow.
For the units in keras.layers.SimpleRNN or any RNN structures that keras provide, you can consider it as the extension of the basic RNN structure that is in a single RNN cell, containing that many number of units for computing inputs.
As you know that the unit in RNN is tanh so if units=1 then it will be the graph on the left and for units=3 on the right
We get output states of every cell of the RNN as a list
Great intro to [RNN (hyper-link)]
[RNN + TensorFlow for noobs (hyper-link)]
[RNN + classification (hyper-link)] : take a look at the sequence classification in this article, which is the case of yours.
However, in LSTM, or any RNN architecture, the loss for each instance, across all time steps, is added up.
tensorflow/tensorflow/python/ops/rnn_cell.py
The implementations of the RNN cells have moved to [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py (hyper-link)].
I am not that deep into RNNs, but from your code it looks like you could refactor the inner tf.variable_scope into a [tf.Module (hyper-link)], which allows you to use xavier initialzation as follows (Not tested):
You might find these links useful:
[Variable Sequence Lengths in TensorFlow (hyper-link)], [Tensorflow RNN with varying length sentences (hyper-link)]
It turns out that I was not understanding the shape of the inputs to tf.nn.rnn() correctly:
Each sequence has input_size dimensions and has length T (these names were chosen to match the documentation of tf.nn.rnn() [here (hyper-link)]).
The latest version of TensorFlow (current nightly build/upcoming version 0.8) includes an experimental feature called [tf.nn.dynamic_rnn() (hyper-link)].
This function lets you build a RNN using a loop in the TensorFlow graph, which dramatically cuts the number of nodes used in the graph (and hence the size of the graph that must be built for gradient descent).
Your main mistake is misunderstanding how LSTM (or any RNN, actually) works and what it accepts as an input.
But you are using the following as the final layer in your RNN:
Contrary to what I expected the simpler model gave much better result that the other; even though RNN's supposed to be better in processing time series data.
A RNN + LSTM is for sure an overkill for such a simple problem.
After converting from raw text format to numeric vector representation, you can train a RNN/LSTM/CNN for text classification problem.
I could not validate without actual data, but I had a similar experience with an RNN.
If so, there is a problem because your data is 2-dimensional, but for an RNN you need a 3-dimensional input tensor.
However, in most of the cases, there is a single output from the RNN.
If the output must be a sequence, this output is usually fed into another RNN.
You may want to check deeplearning4j.org/usingrnns
You need to do some reading on the RNN equations and the [keras documentation (hyper-link)].
The whole point of using recurrent layers is that you will be folding over your sequence by applying the same function over and over, and this function is materialized by a single* RNN cell-- if each item of your sequence was processed by a different cell, there wouldn't be any recurrence in there.
To make things clearer, an RNN implements a function f: (x,h) → h. You give it the first item of your sequence, x0 and some pre-initialized h-1, it gives you back h0.
* In the case of a shallow RNN
A RNN is not the same as a R-CNN.
A RNN is a Recurrent Neural Network, which is a class of artificial neural network where connections between units form a directed cycle.
The following image shows a simple representation of a RNN Cell.
I think an RNN is not the right tool here since company names are short sequences.
I do not think that RNN would be the best options, but I am not sure about that
TensorFlow supports nested tuple as rnn input, see [doc (hyper-link)].
ctrl-f on [this page (hyper-link)] for RNN.
It seems like it should work if you can make the RNN static enough.
Re-implementing the RNN with a LSTM architecture solved the problem.
There’s no weight there because the PyTorch RNN doesn’t prescribe how to create the output from the hidden state.
When you apply the RNN to a sequence, it returns the sequence of hidden states.
(which you have used in cnn model, also use it in rnn model), there is spatial1D dropout for rnn, which drop one entire dim, which again helps to leverage to better generalization.
PyTorch 1.5 has completely fixed the issues with RNN training and DataParallel.
this is a fast procedure to create 3D data for LSTN/RNN without loops and involving this simple function
For calculating the number of params of simpleRNN 
[Number of parameters for Keras SimpleRNN (hyper-link)]
Also, since RNN blocks run in each time-step, you can add it to a variable time-step layer.
Each time step in SimpleRNN is the output Embedding.
In RNN there are two parameter matrix U and W
So in RNN layer the number of parameters is (32+32+1)*32=2080.
In fact, you made a mistake about the input shapes of static_rnn and dynamic_rnn.
The input shape of static_rnn is [timesteps,batch_size, features]([link (hyper-link)]),which is a list of 2D tensors of shape [batch_size, features].
But The input shape of dynamic_rnn is either [timesteps,batch_size, features] or [batch_size,timesteps, features] depending on time_major is True or False([link (hyper-link)]).
Could the solution be attained to switching to dynamic_rnn?
The key is not that you use static_rnn or dynamic_rnn, but that your data shape matches the required shape.
Other errors should have nothing to do with rnn shape.
If I understand your question correctly, you want a RNN where you input 3 features ([x0, y0, z0]) and output a vector of 50 numbers representing temperature over 50 time steps?
In that case, you need to input coordinates for each timestep you want to predict, as the whole idea of RNN's is that they can learn to predict time series by seeing examples of such series.
Assuming you have this dataset, you could construct a simple RNN like this:
The arrays returned by model.get_weights() directly correspond to the weights used by [SimpleRNNCell (hyper-link)].
2) I think it is not right, because RNN1 encoder and RNN2 decoder suppose to be connected by state, but in this code it is just initial all the RNN2 states by RNN1 states
When Tensorflow builds the computation graph, it automatically feeds the result of RNN1 into RNN2 because of the way your computation graph was created.
The second part is LSTM or RNN which are the core.
You can learn more in details about RNN and LSTM here [RNN and LSTM (hyper-link)]
Keras/TF build RNN weights in a well-defined order, which can be inspected from the source code or via layer.__dict__ directly - then to be used to fetch per-kernel and per-gate weights; per-channel treatment can then be employed given a tensor's shape.
Below code & explanations cover every possible case of a Keras/TF RNN, and should be easily expandable to any future API changes.
Also see visualizing RNN gradients, and an application to [RNN regularization (hyper-link)]; unlike in the former post, I won't be including a simplified variant here, as it'd still be rather large and complex per the nature of weight extraction and organization; instead, simply view relevant source code in the repository (see next section).
Code source: [See RNN (hyper-link)] (this post included w/ bigger images), my repository; included are:
EX 1: uni-LSTM, 256 units, weights -- batch_shape = (16, 100, 20) (input)
rnn_histogram(model, 'lstm', equate_axes=False, show_bias=False)
rnn_histogram(model, 'lstm', equate_axes=True,  show_bias=False)
rnn_heatmap(model, 'lstm')
EX 2: bi-CuDNNLSTM, 256 units, weights -- batch_shape = (16, 100, 16) (input)
rnn_histogram(model, 'bidir', equate_axes=2)
rnn_heatmap(model, 'bidir', norm=(-.8, .8))
EX 3: uni-CuDNNGRU, 64 units, weights gradients -- batch_shape = (16, 100, 16) (input)
rnn_heatmap(model, 'gru', mode='grads', input_data=x, labels=y, cmap=None, absolute_value=True)
Did you try pass it(RNN2) as a new layer like.
Can you elaborate on how i can use a Model(RNN2) as a Layer in RNN1?
Assume that you put RNN2.predict in the loss function.
If before you were using: MultiRNNCell([BasicLSTMCell(...)] *
  num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in
  range(num_layers)]).
I think it makes sense to talk about an ordinary RNN first (because LSTM diagram is particularly confusing) and understand its backpropagation.
When it comes to backpropagation, the key idea is network unrolling, which is way to transform the recursion in RNN into a feed-forward sequence (like on the picture above).
Note that abstract RNN is eternal (can be arbitrarily large), but each particular implementation is limited because the memory is limited.
Let's take a look at a classic example, [char-rnn by Andrej Karpathy (hyper-link)].
Here each RNN cell produces two outputs h[t] (the state which is fed into the next cell) and y[t] (the output on this step) by the following formulas, where Wxh, Whh and Why are the shared parameters:
Both passes above are done in chunks of size len(inputs), which corresponds to the size of the unrolled RNN.
LSTM picture and formulas look intimidating, but once you coded plain vanilla RNN, the implementation of LSTM is pretty much same.
Use stateful RNNs (stateful=True) and make them only read one timestep at a time.
Stateful means that they keep their hidden state between sequences, so it should effectively work like a "normal" RNN/LSTM, except that you only input a sequence of length 1.
Use [tf.contrib.rnn.AttentionCellWrapper (hyper-link)].
According to [Masking and Padding with Keras (hyper-link)], you won't need to manually set mask on the RNN layer, in the following code the RNN layer will automatically receive the mask.
The question is not easy to answer since it is not clear what you're trying to achieve (it shouldn't be the same using a FFNN or a RNN, and what works best depends definitely on the application).
Anyway, you might be confusing the training steps (say, the forward- and back- propagation over a minibatch of sequences) with the "internal" steps of the RNN.
What you can do is return sequences of outputs (one y_predicted for every internal time step) including the argument return_sequences=True inside SimpleRNN(...).
By concatenating the embedding inputs and pass through a single cell won't work, instead you use dynamic_rnn method for a sequence.
After digging through TensorFlow source code for the RNN and Bidirectional classes, my best guess for the output format of a stacked bidirectional RNN layer is the following 1+2n tuple, where n is the number of stacked layers:
[0] concatenation of forward and backward state across the RNN
You will have to put a convnet under the RNN, most likely, to get reasonable results.
As such, you feed the image to a convnet, then flatten the activation map and feed it to the RNN cell.
Hidden size is number of features of the hidden state for RNN.
However, num_layers is just multiple RNN units which contain hidden states with given hidden size.
num_layers=2 would mean stacking two RNNs together to form a stacked
RNN, with the second RNN taking in outputs of the first RNN and
computing the final results
I want to explain how RNN works with the image below.
Each RNN unit (blue rectangles) takes one h_n (hidden state) and one input.
At each timestep (t, horizontal propagation in the image) your rnn will take a h_n and input.
The parameter that controls the RNN cell size in keras is called units.
try [dilated RNN (hyper-link)]
Based on [this answer (hyper-link)] I believe that the state is set back to whatever state you pass in with the initial_state argument to tf.nn.rnn (or the other RNN creation functions).
Trying to overcome the problem, I updated the inference in such a way that at each RNN step I output the vector of embeddings instead of item_id.
Secondly, since I use LSTM/GRU cells, they minimize the probability to observe two absolutely similar outputs on different steps of RNN's inference.
I think you can get the initial states by performing the inverse computation of the RNN cell.
In this case, your are using a SimpleRNNCell and the forward computation is implemented in the [call method (hyper-link)].
Since you are using a stateful RNN, you need to provide the input/output pairs corresponding to the initial state.
So in short, yes, I agree that rnn:bin2int and rnn::int2bin appear to be wrong/broken.
Also, rather than trying to fix the rnn::int2bin function, I'd suggest R.utils::intToBin
I found an example implementation using rnn here: 
[https://www.r-bloggers.com/plain-vanilla-recurrent-neural-networks-in-r-waves-prediction/ (hyper-link)]
This implementation works.
Then pass this cell to tf.dynamic.rnn.
tf.nn.dynamic_rnn replaces elements after the sequence end with 0s.
* as far as I know, but you can get a similar behaviour with RNN(Masking(...) approach: it simply stops the computation and carries the last outputs and states forward.
You will get the same (non-padding) outputs as those obtained from tf.nn.dynamic_rnn.
Here is a minimal working example demonstrating the differences between [tf.nn.dynamic_rnn (hyper-link)] and [tf.keras.layers.GRU (hyper-link)] with and without the use of [tf.keras.layers.Masking (hyper-link)] layer.
Now here are the actual outputs from the m1, m2 and old_rnn architectures:
The old tf.nn.dynamic_rnn used to mask padding elements with zeros.
The new RNN layers without masking run over the padding elements as if they were data.
The new rnn(mask(...)) approach simply stops the computation and carries the last outputs and states forward.
Note that the (non-padding) outputs that I obtained for this approach are exactly the same as those from tf.nn.dynamic_rnn.
Since an RNN doesn't know anything about time deltas but only about time steps, you will need to quantify / interpolate your data.
Using a custom layer I was able to stich the Tensors manually before feeding them into the RNN.
When training RNN (LSTM or GRU or vanilla-RNN), it is difficult to batch the variable length sequences.
Moreover, if you wanted to do something fancy like using a bidirectional-RNN, it would be harder to do batch computations just by padding and you might end up doing more computations than required.
This is helpful in recovering the actual sequences as well as telling RNN what is the batch size at each time step.
This can be passed to RNN and it will internally optimize the computations.
And we do packing so that the RNN doesn't see the unwanted padded index while processing the sequence which would affect the overall performance.
Now, we pass the tuple packed_seq_batch to the recurrent modules in Pytorch, such as RNN, LSTM.
For Bi-directional LSTM (or any recurrent modules), it is even more cumbersome, as one would have to maintain two RNN modules, one that works with padding at the beginning of the input and one with padding at end of the input, and finally extracting and concatenating the hidden vectors as explained above.
Check out this [unit test (hyper-link)] for RNN.
Then there is this simple RNN model:
language modeling) in the [StateSavingRnnEstimator unit tests (hyper-link)].
I'm not sure exactly where your code is failing, but if it's failing at tf.nn.dynamic_rnn, maybe you just need to specify the dtype parameter:
tf.nn.dynamic_rnn(cell, inputs, initial_state=self.initial_state, dtype=tf.float32)
as discussed in the docs: [https://www.tensorflow.org/versions/r1.6/api_docs/python/tf/nn/dynamic_rnn (hyper-link)]
i can not access rnn package because it had some bugs reported
[https://cran.r-project.org/web/packages/rnn/index.html (hyper-link)] page.
Yes CNN and RNN can be combined
CNN is applied to Images
RNN is applied to Sequence Models
RNNs suffer from an exploding gradient, so you should clip the gradients for the RNN parameters.
where cell is one of RNN cells (BasicLSTMCell, BasicGRUCell, MultiRNNCell, etc).
Because of the way the RNN state is passed from timestep to timestep,
  the model only accepts a fixed batch size once built.
I got the exact same error trying to build bidirectional GLSTM using bidirectional_dynamic_rnn.
[Why are there no pre-trained RNN models (hyper-link)]
From my point of view, I think you could add RNN, LSTM layer to the Network#__init__,Network#forward; shape of data should be reshaped into sequences...
For more detail, I think you should read these two following articles; after that implementing RNN, LSTM not hard as it seem to be.
[http://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html (hyper-link)]
So from asking about and reading around, it seems RNNs might not be the best solution for financial / random walk data - at least with the setup I am using.
In my case, a good idea is was to provide custom collate_fn to DataLoader, which is responsible for creating padded batches of data (or represented as [torch.nn.utils.rnn.PackedSequence (hyper-link)] already).
I would advise you to create a more general model, where you can provide PyTorch's cells (like GRU, LSTM or RNN), multilayered and bidirectional (as is described in the post).
BTW: [Here (hyper-link)] is some info on concatenation of bidirectional output of RNN.
dynamic_rnn), there is always a sequence_length parameter which you need to set to the corresponding sequence lengths.
[https://github.com/j-min/tf_tutorial_plus/tree/master/RNN_seq2seq/legacy_seq2seq (hyper-link)]
The easiest way to do this is to make sure state_is_tupel is set to False for every RNN you use, and you will simply get a final state tensor back from the dynamic RNN function.
It uses the following two functions which should work for any type of RNN although I only tested it with this model.
But if you don't want to pass init_state of RNN Cell , it is a way.
For this part, as mentioned in chasep255 solution, we can use placeholder where we, ourself, passed the zero_state to RNN.
The RNN layer by definition could not have unequal input and output lengths.
However, there is a trick to achieve an unequal, but fixed, output length using two RNN layers and a RepeatVector layer in between.
