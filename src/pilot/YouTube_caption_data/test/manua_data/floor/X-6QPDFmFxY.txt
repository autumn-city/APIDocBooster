- Hi, everybody.
I hope you're enjoying re:MARS,
and thank you for taking the time
to join us today for our talk.
I'm Sree Ganesan, head of
software products at Habana Labs,
and today I'll be also joined
by Jonathan Luse and Erik Nieves.
We're gonna be talking about
the democratization of technology
and how Intel is working with our partners
to enable the same with some
of these emerging technologies.
Let's start with what we mean
by democratization of technologies.
We're talking about
both cloud and the edge.
In terms of putting the technologies
in the hands of the
users, the practitioners,
and those who are also going to be
the emergent practitioners,
there are several vectors here.
One is affordability.
You need to make it such that
they can get access to it,
that it's cheaper, that
enough practitioners
and enough businesses and developers
can actually play with these technologies
and start using them to
solve some of the problems
for larger scale and for the world
but also for their own smaller problems
which they're targeting
for their businesses
and for their people,
whether it's public sector,
private sector.
In terms of making it accessible,
it's not just the hardware.
We want to make sure both the hardware
and the software is accessible.
You're putting it, again,
in the hands of people
who are gonna use it in
the most effective way.
We don't have to stop with just those two
because we also have to train and empower
our developers and practitioners, again,
and that's where the
education aspect comes in,
and the last part is
propagation and making sure
that we're able to scale it across
to more industries, more businesses,
more organizations so
they can take advantage,
and that's how we see
democratization playing out.
I'm gonna start with focusing on AI.
We believe AI is one of the most
transformational technologies out there.
Intel has a lot of assets
across our portfolio.
I'm gonna be focusing on the
Habana AI training processor,
and we're gonna talk about
how we're democratizing AI
through cost efficiency using
Habana's Gaudi processor
as an example.
You will see here that a
recent IDC study reported
about 56% of the AI/ML customers
saying that the cost is their
most significant barrier
to adopting AI, and for them
to actually take and deploy it,
some of them are using retraining
and running their models,
like 74% of them are doing it weekly
or half of them are doing it daily,
so there's a lot of iterations of training
that are happening.
As an industry, it's our challenge.
How do we actually provide
customers and businesses
and organizations access to
more affordable training?
And this is where Habana Gaudi comes in.
It's been designed from the
ground up for cost efficiency,
and here you'll see that
this is just the picture
of our OEM form factor.
Gaudi is a purpose-built
AI training accelerator.
It's got a cluster of
tensor processing cores.
It's got a configurable
matrix math engine.
It's got a software-managed memory.
We have 32 gigabytes of
HBM that's available.
It's very different from a
general-purpose architecture
and it's designed specifically
for training efficiencies,
and more specifically
in terms of AI training,
it's for deep learning training.
The other unique thing that
you'll notice about Gaudi
is that it has integrated 10 ports
of 100-gigabit Ethernet
on the device itself.
For a deep learning training,
it's not sufficient to do
training on a single device.
Usually it's like a distributed training.
We're seeing a lot of growth
in large-scale models,
large-scale training, so
enabling scaling efficiency
is one of the most critical pieces
of enabling better, efficient training.
By integrating these ports on die,
we're allowing you to scale out
very efficiently with Gaudi.
The unique other factor is the fact
that we're using standard
Ethernet protocols.
This is using the RoCE v2 protocol.
It's a standard Ethernet protocol,
so it's not something that's
a proprietary protocol
you are using.
You can scale out and you're not locked
into specific vendor interfaces,
and in terms of building out a cluster,
if you are building out
a large number of nodes
and connecting them together,
all you need is standard
Ethernet switches.
What happens is that you have also reduced
the number of discrete
components, and therefore,
there's savings in terms
of both power and cost
and all of that cost savings
is basically translated back
to the end users.
Gaudi is built on a 16-nanometer process
and the fact that it's
purpose built in terms of
its architecture for deep
learning training efficiency
allows us to actually give
significant cost performance savings,
and we'll show you in a
little bit some of that.
In this slide, we've got
Gaudi-based EC2 DL1 instances
that we're comparing against
some of the other training instances,
and you'll notice that
we've compared it against
the A100 80 gig instance,
the A100 40 gig instance,
and the B100 instances,
and you have up to 40%
better price/performance
with the Gaudi DL1 instances
compared to the GPU-based instances.
On the left, you'll see the
pictures showing the chart
showing ResNet50 as a
proxy for vision models,
and on the right, you see
the picture of BERT-Large
as a proxy for NLP or language models,
and you can see that for the A100,
it's in the 45, 50% range,
and then as you go to the
B100 and compare against that,
you get up to 75% and 77%.
On BERT, I've got two pictures here.
Phase 1 and phase 2 have
different training regimes,
so we just wanted to highlight
that obviously depends
on the training regime
and the kind of training
that you're doing,
and you still get a lot of savings
from using Gaudi-based DL1 instances
and this is the cost
savings that you can expect.
This is on a single instance,
and if you have distributed
training, you get multiple.
You get the multiplier effect with that.
We are also very excited to announce
the availability of Gaudi2.
In May, we announced that we have
our next-generation processor
and it has leadership performance.
You'll see again two
charts, ResNet50 for vision
and BERT representing the
language models as a proxy,
and we are nearly 2X better
in terms of throughput
compared with the A100 GPUs,
and we are very excited that we have
this next-generation product.
It's building on the same
architecture. It has just more.
Matrix math engine is more.
The TPC cores are higher.
You have more scale-out ports.
We have more HBM memory available
and the same cost efficiency that you get
will be available on Gaudi2 as well.
I want to talk about a little,
a couple of examples of our end customers
who are using Gaudi and taking advantage
of the cost efficiency of Gaudi,
and this is just to show
you what's possible.
This is just a sampling of customers
that we are going to show you here.
The first one is Mobileye.
Mobileye is a company that
is driving the evolution
from assisted driving
to autonomous driving.
They're an Intel company and
we've been working with them
on enabling some of
their training workloads
with Gaudi on DL1.
You'll notice in the picture of the car
that there is a bunch
of cameras on the car.
They're capturing a lot
of data, image data,
and doing custom object detection,
2D and 3D models that are
getting trained on Gaudi,
and multiple teams are
now starting to use DL1
in the Mobileye team.
They have seen more than
the 40% performance,
price/performance improvement
that we reported earlier
with some of the vanilla workloads.
We have also started working with them,
some of the larger-scale models
to allow them to get
better time to market.
This allows them to actually
put their cars on the
road, which are safer,
allows us to make the transition
to autonomous driving more
faster and empowers more,
what we can get out of the AV space
by partnering with a
company like Mobileye,
and we hope that other AV
companies can take advantage
of the similar capabilities
that Gaudi offers,
and hopefully we'll see in the future
more excitement with the AV space.
The second example here is Leidos.
This is another partner.
They work a lot with
the government agencies
like FDA, NIH and CDC.
We focused on healthcare
applications with them,
which is very different from the AV space.
In this case, we took the scenario,
in the context of the
pandemic, there's a lotta work,
a lot of research and a lot
of activity that was going on
in terms of how do we handle,
diagnose and treat COVID.
They wanted to use frontal chest X-rays
to basically detect COVID
pneumonia using deep learning.
They were able to see 60% cost savings
with just switching from a P3dn
instance to a DL1 instance,
and so they were very excited
to then start taking this
and then scaling it out to more
of their end users and customers.
For us, for me personally
and for us as such,
I think this growth in AI
adoption in the healthcare space
was going to continue.
COVID is just, was the inflection point
where this just picked up.
It's actually very exciting and rewarding
to be able to offer cost-efficient
technologies like DL1
to be able to solve some of
these very challenging problems,
reducing the time to a
diagnosis, time to a cure,
and enabling the world
become a better place,
and it's actually rewarding
to be working on this.
I'm gonna show you a brief testimonial
from one of our partners in Leidos.
Hopefully you can hear this.
- For the past two years,
our customers in federal
health and medical community
have been racing against COVID,
from diagnosing the disease
to devising treatments and vaccines.
The race continues worldwide
as COVID continues to morph
and there's new data collected every day
to inform diagnosis and
cures for the new variants.
One key benefit that faster
time to train can provide
is early detection of the
presence of COVID in patients.
If physicians can detect
the disease at early onset,
they can prescribe oral medications
that can prevent hospitalization.
Early detection is the key to minimize
severe effects of COVID on patients
and getting them healthy
as quickly as possible.
For this reason, faster time to train
can mean faster time to recovery at home.
At Leidos, we are looking forward
to bringing Gaudi2's performance
to our federal health customers,
like the FDA, NIH and CDC,
and medical practitioners and researchers
to help them mitigate severe outcomes
and predict and prevent outbreak.
Thank you.
- The other one I wanted to highlight
is our work with San Diego
Supercomputing Center.
We have a Gaudi cluster called Voyager.
This was, it just went live.
There's a three-year testbed phase
and during which they're
opening it up to researchers
and institutions to
come in and start doing
some of their research with deep learning
on this Voyager cluster.
This is the next set of
solutions, next set of ideas
that these researchers
and teams are working on
and hopefully will be taking advantage of
the uniqueness of Gaudi
and being able to solve
these problems in a
different and better way.
It allows the next
generation of problem solvers
to take advantage of Gaudi.
I just got a simple example here of
how they've already used Gaudi here.
This is in the context
of wildfire management.
They've been using DL algorithms
to look at satellite images
and determine land cover
across different types of geographies
and address the threat of wildfires,
especially in terms of
where there's a lot of
populations and impact possible,
and they've been using Gaudi
to go and solve this
problem with deep learning,
and we're very excited to be partnering
with the San Diego Supercomputing Center
to enable researchers and scientists
solve the next set of
problems using Gaudi.
That talked about the hardware part of it.
The fact that, again, we have access
through a public cloud instance like AWS,
the fact that we have
Voyager for universities
and researchers to work on it
democratizes the access to the hardware,
but that's only one piece of it.
We have to also democratize
the access to the software.
I'm going to spend a
little time talking about
how we're making that happen.
The bottom of the stack
over here is SynapseAI.
It's purpose built, again,
to get optimized models running on Gaudi.
It takes the neural network
representation, that topology,
and we basically convert it into a recipe
that can run most efficiently on Gaudi.
That's very hardware specific
and all of these compiler optimizations
are happening specifically
for our hardware,
but at the end of the day,
the data scientists and developers
are working with some of
these open-source tools,
like PyTorch and TensorFlow.
These are the popular
frameworks they're using,
so we wanted to make sure that we empower
the developers and data
scientists to be comfortable
using the same programming
paradigms that they are used to,
not having to change anything on them.
SynapseAI, which is our software stack,
is integrated into TensorFlow and PyTorch.
Since then, we've also done more work
in terms of trying to meet
developers where they are,
because there's a lot of new ideas
and new research that's going on.
These get quickly turned
into Python libraries
or lightweight frameworks
and developer tools
and developers are taking
advantage of the productivity
that those tools and libraries offer,
so we've started working
with ecosystem partners
in terms of connecting
with these developer tools
and libraries and frameworks
in addition to just having
the deep learning frameworks,
like TensorFlow and PyTorch support,
and I'll talk to you in a moment about
the couple of partnerships
and highlight them.
I want to call out three
partnerships over here.
We're working with Hugging Face,
a very popular repository.
This is a very one of,
it's like they call it the
GitHub of the models out there.
They started out with NLP.
Now they've expanded
into many other areas,
computer vision and other areas.
This is a transformer-based
library, extremely popular.
They've democratized access to models
through their approach with
working with developers
and making it easy for them
to take advantage of these models.
We recently partnered with
them to integrate SynapseAI
into the Optimum library,
and we've got the Habana Optimum library
where you can basically quickly
take any transformer model
that's available on Hugging
Face and be able to deploy it
and run it and train on Gaudi.
That's something that we're very excited
to be continuing to work
with the Hugging Face team,
and this allows us to, again,
reach a vast number of developers
with tools and libraries,
again, that they're familiar
with and comfortable with.
The second one is PyTorch Lightning.
PyTorch Lightning is another
very popular framework.
It's an abstraction on top of PyTorch.
There's a lot of boilerplate code
that gets written in PyTorch.
Not every data scientist
needs to worry about
how these things get implemented,
so Lightning offers a wrapper on top of it
to make it simpler to
focus on the data science
and research aspect and worry less about
actually the details of how it
gets implemented in PyTorch,
and it's, again, a very
popular lightweight framework.
We've been working with
the PyTorch Lightning team.
We've integrated SynapseAI
into PyTorch Lightning as well.
Today, you can go and
install PyTorch Lightning
and start working with Gaudi
and take advantage of the
features of Lightning,
and the third one here is cnvrg.io.
It's an MLOps platform and
this is something where you,
whether you are a data
scientist or a developer
or an IT infrastructure owner,
there are different types of problems
you're solving in an enterprise,
and MLOps for AI is one of the key areas
where you can actually make it very easy
in terms of managing experiments,
in terms of versioning your experiments,
building up workflows
where you can then try
different types of models and
try hyperparameter tuning,
and build out your pipeline
all the way through
from your data ingestion
through to inference,
and we've also integrated
Gaudi with cnvrg.io.
It's both available on premises,
if you have Gaudi available on premises,
or you wanna cloud burst to DL1.
You can do both. It's agnostic.
Cnvrg.io allows you to continue
to use your existing infrastructure
and also take advantage of DL1,
so where it is appropriate, you
can take advantage of Gaudi.
The reason we're, again, to reiterate,
the reason we're working
with these partners
is because we're trying to be
as close as possible to the developers
and we'll continue to have
these types of partnerships
where we can actually empower
developers to take advantage
of whatever's available in the ecosystem,
in addition to taking advantage
of the benefits of Gaudi.
The other aspect in terms
of software accessibility
is not just integration of SynapseAI
and making it available.
It's also how easy it is to use,
because there is this barrier in terms of
taking advantage of a new architecture
and a new software stack.
We've spent a lot of time trying to reduce
the amount of investment
that we expect end users
and developers to make.
We focused a lot on
providing developer assets
on our developer site.
You can go to developer.habana.ai.
You can find a lot of resources on,
there's obviously user guides
and training materials.
There's small tutorials
which are easy to consume.
For those who like video tutorials,
we've got video tutorials.
Different types of learners
have different approaches
working with different assets.
We've also got a reference
model repository.
We've got 40-plus models
that are very popular CV
and NLP models that have
been ported over to Gaudi,
so you can go in there and start running
these popular models and start training
with your own data sets and
take advantage of Gaudi easily.
It's also a reference so you can look at
what kind of changes you need to make,
and we've got the developer site,
the forum and our GitHub for support.
The next question I usually get is,
what does it mean to actually
make it work on Gaudi?
I wanted to show you how simple
we've tried to make the changes.
This is a very simple example
of training an MNIST model
to use the MNIST data set with
a small neural network model.
You need to insert two lines of code
where you load the Habana
module and invoke it,
and that basically, this
is a TensorFlow example
where it tells the framework
that there's a device
called the HPU, which is
the Habana Processing Unit,
that's how we refer to
Gaudi in the frameworks,
and it allows you to start prioritizing
the operations to get executed on Gaudi.
Basically, our SynapseAI software stack
will take the computation
graph from the framework,
identify the sub-graph that
can get accelerated on Gaudi,
and then basically
schedule it for execution,
do the optimizations and schedule it,
and if there are operators
that we don't support,
it'll just fall back to the CPU
and you'll take advantage of
Gaudi wherever it's possible
and the rest of it is going
to execute without issues.
I wanted to also highlight
a couple of more use cases.
We have several customers.
I cannot describe details
about each customer,
but we have customers working on
industrial defect detection,
using Gaudi for fraud detection
and inventory management in retail.
Obviously we've talked about 2D
and 3D medical imaging use cases,
and in the autonomous vehicle space,
we've talked about segmentation,
and we've got several customers
using NLP BERT-like models
and vision transformers.
They're using it for
different NLP use cases,
sentiment analysis, question,
answer, subject matter query.
These are the four few areas
where we have customers
actually exploring and using Gaudi
and taking advantage of
the cost and cost savings
that they can get with Gaudi.
I'm going to then switch
to the third vector,
which was democratization
through education.
It is, again, not enough to
just have the democratization
in terms of affordability,
in terms of the access.
We also need to train the
next generation of users,
and the current generation of users
who are familiar with
a different platform,
make sure they're effective
in taking advantage
of these technologies.
We've been spending a lot of effort
in educating the developer community.
There's a lot of how-to trainings.
We've got open online courses.
We've got workshops, hackathons,
so lots of hands-on training
that we're making available
to our developer community
and ensuring that they are
effective and productive
with our Gaudi platform,
and on a broader scale,
I want to also touch upon
some of the work that Intel is doing
with other areas in AI,
where we're working with to
enable underserved students
and underrepresented minorities
to have that access with this.
Diversity is another
important piece for us.
It's very critical. We
care deeply about it.
AWS and Intel's DeepRacer Student League
is one of the ones that
I'm calling out here.
It's a fun way for underserved students
to come in and get started
and learn about Gaudi.
They get many hours of
educational material.
They get free time to kind of,
computer resources to
do their learn and train
and up-level their skills.
There's monthly prizes
that's available to them,
scholarships and they have devices.
This is how we hope it's a path
for these underserved students
to learn and grow skills
and for some of them can qualify
to earn a Udacity Nanodegree
scholarship as well
through the AWS AI/ML Scholarship program.
The second one I want to call out,
something that I'm very passionate about
is Intel continuing to
invest with Girls In Tech.
This is a nonprofit that
has been focused on STEM
and making sure that they're
closing the gender gap in tech.
The thing I invite you to is, please,
come to our Intel booth and participate.
When you do that, we could raise up to 10K
which we will provide to Girls In Tech
to fund programs like
hackathons and career fairs
and other STEM-focused
activities, boot camps.
I'd invite you to please come down
to the tech showcase and
stop by our Intel booth.
This is something that is very important
for us in the AI space,
to build out this diverse,
capable set of users
beyond who we have today.
With that, I'm going to switch over
and invite Jonathan to come over
and talk about automation and robotics.
- All right. Thank you.
Hi, everybody. Sorry
for being too loud here.
Is this, can you guys
hear me okay? All right.
My name's Jonathan Luse.
I work in the industrial
solutions division of Intel.
I run that group and so my
background's in manufacturing
and manufacturing tech.
I spend a lotta time with
plant manager discussions
and technology discussions
around manufacturing.
I wanted to touch on that a little bit,
about what's going on, and
I think I'd start off with,
probably, in the conversations I've had
with plant managers three years ago,
you ask 'em, "What's important?"
and they would say, "My
plant has to be secure.
It has to be reliable. Has to be uptime.
I have to have the safety of my personnel
and the equipment covered, reliability,"
and then they say, "Once that's covered,
how efficiently can I run my operation?"
and then this little thing
called the pandemic happened,
and for those that experienced
it, which we all did,
I learned about the
implications of the supply chain
in some unusual ways.
When I talked to plant managers,
I got some calls from a senior VP
of a pulp and paper company who said,
"Hey, people are doing
their business at home
a lot more now instead of at work,
'cause they're not at work,"
and so the implications of
that from a paper company
are big offices have big
rolls of toilet paper,
home, toilet paper is this big,
and the plants are designed
to produce one or the other,
and the whole demographic and
supply chain shift happened,
and this person called me up and said,
"We got a ton of these extra big rolls
and we can't produce enough
because of the great
toilet paper run of 2020
that everybody had and
we need to get agility
and flexibility to our
manufacturing plants,"
and that single phone call kicked off
a series of conversations
of very similar themes
around supply chain shortages
and the perspective around
what's important going forward.
When I talk to plant managers these days,
when they talk about technology
and how it can help 'em,
there's still security and reliability
and performance and safety,
but now agility and the
ability to address shortages
that happen now on a global scale
and on a local scale in
different ways is now a priority.
The technology that
normally would've happened
over the course of a decade or longer
is now happening a lot faster,
and so these supply chain interruptions,
these labor shortages, the
changing customer demands
as far as small versus
large rolls of toilet paper,
it really causes companies
to think about manufacturing
in different ways where
they have to be more agile
and resilient to these changes,
and the business is gonna require,
the manufacturing is gonna
require different sets of data
and different ways of doing things
to be able to accomplish those goals,
and so it's really interesting
how this transformation
has really impacted technology,
because the amount of data and analytics
to be able to do quality control
as they bring a new plant,
product up to speed in a plant,
anybody that's been in a
plant for a long time knows
that when you optimize
and train your production,
you tune it and it turns
into a well-oiled machine
and you just run it and run
it and run it and run it,
but when you go and start
switching product lines over,
you need really good data sets
to optimize and learn and
reduce defect detection.
These technology creates
huge opportunities
that we've been participating in
relative to the application
of new robotics,
new machine learning, new
AI that goes into that.
I'll talk about and touch on
that a little bit as we go,
and the other interesting thing
that that VP of pulp and
paper company told me
was his productivity
actually went up in his plant
when they got rid of the
non-essential employees,
which was another interesting
side effect of their learning,
was we don't wanna go
back to the way it was.
We wanna keep the non-essential
employees outta the factory
and keep the essential productivity
focused on the task at hand.
Very interesting trends
accelerating these automation adoptions,
and so when I think about the need,
it's primarily, for me,
it's focused on industrial.
I have sister groups in
retail and hospitality
that are also looking at
these types of things.
The approach really is
around factory automation,
factory supply, robotics.
When you get to robotics,
there's fixed and mobile robotics
that come in.
When you deal with robotics
into manufacturing,
the AMR stuff that you
saw, like Proteus today,
was a very interesting
next-generation perspective,
but the demand is really driven
by a couple of key operational themes.
These plant managers are
driven by labor shortages.
They wanna have the yields up.
They want defect detection.
They want to know and understand
how to produce the products
that they're chartered to
go produce in the least,
the most efficient way,
the least inefficient way,
and so when I thought about
some of the technologies
that go into this,
we really think about
these technology drivers,
and for me, it all starts with,
for me being a chip guy at
Intel, it starts with the silicon
and there's a real-time component of it
that goes in relative
to time-sensitive data,
having the compute engines to
do the management of the data
in a way that your production
latency is not compromised,
and what that ends up doing is creating
an interesting edge which
creates this private cloud.
When we talk about the cloud,
five years ago, I'd have said,
"Oh, all the data goes up
for manufacturing excellence
up to the cloud, they analyze it,
they figure out what they
should do differently,
solve the business conversations,
then move it back down to production."
The reality is there's a step
in between that's time sensitive,
and this time sensitive
is what we would call
this hybrid or edge private cloud
that is the host for a
lot of these data-sensing
and data-compute applications
that go into the technology,
but there is a regional and
public data center cloud
that goes in, and so for
me as a manufacturing guy,
the technology driver that really dictates
where all this compute is is
really the latency of the data
and the speed in which you can move it
up and out of the factory,
out of the sensor,
out of the control unit and
into the cloud and back,
and if you can get it there
fast enough, analyze it,
it's way more economical in the cloud
than it is at the edge
from an economies of scale,
as far as computational capabilities.
The optimization here is not only having
ubiquitous computing, the
ability to process the data
no matter where you are
in the edge to cloud,
but it's also having
pervasive connectivity,
where you have these
reliable, connected devices
from edge to cloud and go in.
Like anybody who knows
anything about factories,
the last thing you wanna
do is have an alarm
take any extra period of
time to get to the operator
indicating there's something
going wrong with their plant.
That is very time sensitive
and you wanna make sure
that your architecture and
everything that comes in here
goes in and absolutely
has a time-sensitive component to it,
and when you do that, it opens
up different applications.
When these customers come to us
from a technology point of view,
especially when you think about robotics,
I'll give a couple robotics examples
in this type of scenario.
Let's take welding automation,
welders, robotic welders in automotive.
You have a company like Audi
that produces 1,000 cars a
day on their production line,
and they say, "Here's my reality today."
Plant manager of Audi comes and says,
"We pull one car a day off the line
to do quality inspections of the welds
and the other 999 go downstream,"
and when they do further
quality assessments
of the cars as their normal production,
maybe they find faults or failures
and they do rework and everything
and that's very expensive,
and they came to us and said,
"We do 5,000 welds per car
and 1,000 cars per day."
That's five million welds a day,
and when you think of the different alloys
that make up a car,
different plies of aluminum
and other composites,
when they get welded together,
they're not exactly the same,
so a single welding application
is typically overdone,
and what they wanted to do was say,
"We gotta solve for this to say,
'How do we get that
quality inspection inline?
Don't pull it out of the
line to go inspect it.
How do we get it inline?'"
We worked with them on
some time series data,
and this particular case,
this welding application
started off as a robotic welder
that had a current and voltage flow
and it would go in and do the spot welds,
and the reaction of the weld
based on the dynamic of the
training of the AI system
would indicate whether it
needed to be adjusted or not.
It turned into an inline
adjustment of the control loop
that goes in, and long story short,
what ended up happening
was the welding situation
went from inspecting 5,000 welds a day
to five million welds a day
with adjustments on the fly
and defects went down by, I don't know,
something like 90% downstream reduction,
which was an incredible savings for them.
That started off and then
John Deere heard about this
and they hit us up and said,
"We've got robotic welding
for our John Deere farming equipment.
We've got an issue with porosity,"
and porosity are those little
tiny microscopic air bubbles
that go into a weld bead
that a human has a hard time detecting,
and that weakens the weld
and then obviously creates
quality issues downstream.
Then that came, how do we solve that?
That's where machine vision comes in.
Instead of having time series data,
you start getting into
machine vision data.
The whole point is as you
add these technologies in,
the data gets exploded.
There's new quality associations
that come outta here
and the net effect of the customers
is a tremendous improvement in
the quality of their output,
and so when you have that, that's great.
When you have to have
an agile manufacturing
that goes with it, you
now have to have a system
that's more software defined
than hardware defined,
and that's where these technologies are
in the journey today,
is having a more agile ability to adapt,
and that's where some of the things
that Sree was talking
about relative to training
and retraining and being able
to deploy those technologies
becomes so critical in these
technologies that go in.
Really, the full potential
of this entire conversation,
at least from my chapter,
really is all about
when you pull the technologies
of AI, machine vision,
time series data, you add
robotics, fixed and mobile,
you add the networking
components from the edge to cloud
and all the sensors for control,
you really have to pull that
together as an entire solution
when you think about solving
for some of these business drivers
and the technologies that go with it,
and to realize the full potential,
you really have to take
advantage of all those assets
that come in.
It's a great opportunity
and it's also very complex,
as I think the opening keynote
was saying this morning.
Was his name, Tye I think was his name.
Yeah, thank you.
Tye was saying complex is
hard, simple is harder.
Thank you.
I even screwed it up just like he did
(laughing) on that second time,
but basically we're in a situation
where you've gotta have
the tools that go with this
and be able to understand
what your data objectives are.
When you get into what
the effect of this is,
this creates enormous volumes
of data, and for an end user,
the biggest challenge they have for that
is the volume of data, the
variety of data, the veracity,
the ambiguity of the data.
Some of the data's good.
Some of the data's bad.
This creates huge problems
for an operational technology
to figure out how to deal with this.
One of the biggest things you've gotta do
from a technology point of view
is have a good data
management plan, if you will,
understanding what data's important,
what data needs to be
processed immediately,
what data needs to be put up to the cloud,
and that ends up being a
big part of the success
of applying technology in the future.
The last thing I really
wanted to talk about
was the analytics of defect detection.
When I mentioned the Audi
or the John Deere situation as examples,
the biggest and most
important part of that
wasn't necessarily just defect detection,
but it was also the
ability to put that inline
and act on it as a control
loop in the system.
It's very important
that you don't just use
this defect detection and
then not make process changes
to your production.
That is part of where the
gains are huge in this thing.
An ability to have a scheme that says,
"How do I take things that
were offline quality control,
put them inline,
take all the sensory data
that you have to go capture,
act on it, think on it, act
on it, make some changes,
improve your output of that?"
Lots of amazing technologies out there.
I think you've heard a lot about 'em.
I think if you ever wanna have some chats
about some of the manufacturing
technologies out there,
I'll be around.
For now, I'm gonna close with this
and then introduce Erik Nieves,
who is going to talk to us
from a Plus One Robotics perspective.
- Thanks, Jonathan.
Jonathan is right that
a lotta this started
in the manufacturing
space, and even today,
the vast majority of the
robots that have been deployed
in the last 40 years have
all been in manufacturing,
specifically in automotive,
but it's been in the last
five, six, seven years
that it's been clear
that robotics was gonna grow
well beyond manufacturing,
and that's the reason Amazon
can pull off a robotics conference,
because as Tye pointed out this morning,
they've deployed over a
half a million robots now.
Jonathan is right, that
it's all of these elements.
Everything in robotics is a
systems problem, everything,
and it is
perception, so the sensing.
This is just a example of our architecture
and you can see that
there's a industrial robot
and that robot's gonna
have an end effector
or a gripper of some
sort, but it needs to see.
This is the fundamental difference
between the manufacturing world
and the logistics and supply
chain automation world.
In the manufacturing context,
it needs to do the same thing the same way
every time for the next
four to eight years
or until there is some major change
of going from big rolls to little rolls,
but in the warehouse, you
can't plan on repeatability.
The warehouse automation problem
is one predicated on variability
because I don't know what's
gonna come down the line next
and we sure don't know what's
gonna happen next week,
so it's all built around perception.
You have to discover
everything every cycle,
and so yes, there is a sense.
You take those sensors and you do not want
to do computer vision in the cloud.
It's way too much latency.
You have to do that at the edge
and so that's what you see there.
There's gonna be an industrial PC.
It's gonna be GPU enabled.
There's gonna be accelerators.
We're doing all of the AI lifting there,
and then there's going to be
a cloud component as well,
which we're gonna talk about,
and then ultimately a human in the loop.
It takes all of those,
and the reason that you're seeing
warehouse automation become a thing
is because all of these technologies
have finally come to bear
in a cost-effective fashion.
The robot arm has been
available for 30 years,
but computer vision wasn't
and neither were the AI models necessary.
All right.
That's the reason that last
year, for the very first time,
more industrial robots were sold
outside the automotive market than within.
That had never happened before,
and I expect this is
gonna be the new normal.
It's the other spaces beyond manufacturing
that are gonna be the large
consumers of automation,
and it's not just gonna be the warehouse.
You see, I saw the Miso Robotics
people out on the floor.
There's gonna be food prep robots
and everything that we're talking about in
medical devices, et cetera.
Robot technologies are
gonna become embedded
in lots of different facets around us
to the point where the robotics is,
as a technology, is going to recede.
We won't think of them as robots,
but it is effectively these elements.
It's perception, it's some
sort of decision making,
and then actuation.
This is what I think
about in democratization.
We've talked about democratization
from accessibility and other things,
but the last thing Sree
talked about was propagation.
The robots have to go solve
problems in the world.
These are jobs to be done
and this is effectively
all the things that we've talked about.
This is an industrial robot
that's tied to a set of RealSense cameras,
and you'll notice that, look
at the one on the upper right,
it's picking from one tote,
what we'll call a donor tote,
and placing into the order tote,
and it might be segmented
so that there's four orders
in four different segments.
There are, in fact, two
sets of cameras here.
One of them is over the picking zone
and one of them is over the placement zone
because you need to validate
on both sides of that.
3D sensing has finally come
down in cost sufficiently
that you can afford to do a
closed-loop system like this.
Previous instances of 3D technology
were so cost prohibitive, if
you could afford it at all,
you would only be able to do
it one place, over the pick,
and you would never really know
if you did it the right
way on the place side.
Democratization, cost is a part of that.
The other is, as Tye pointed out,
you have mobile robots
and you have manipulators.
What are they, deployed
a half a million now AMRs
within Amazon facilities?
It's because every workflow in a warehouse
is some combination of
mobility and manipulation.
It's either somebody was dragging a cart,
pushing a cart somewhere,
and then a person unloaded
and packed, et cetera.
What we're beginning to see
is what you're seeing on the upper left,
which is a combination
of, yes, there's a robot,
and yes, there's a machine vision
and it's doing barcode scanning.
That's the sort of
motion that you're seeing
because you need to tie
the physical assets now
to their data footprint.
It'll scan and now it's
loading mobile robots
and then the mobile robots
go and sort into the world,
so reminiscent of what
you saw this morning
in the keynote of fixed robots
loading mobile robots, because at the end,
the job to be done is some combination
of mobility and manipulation,
which ought not surprise us
because we built our processes
around our capabilities
and we are mobile manipulators,
so that's what's happening here,
but I want to back up to this
because we said that there
was a cloud component,
but all the way to the left
is this human in the loop
and this is something that
we feel strongly about,
and that's this.
For all
of our work in AI,
for all the however clever we might be,
for all of the assets
that we're going to bring
to bear to this problem,
you are a better 3D sensor than any camera
and you are much better at inference
than any computer vision system,
so at some point, at some level,
in the real world of the messy warehouse,
everything from the robot to the sensors
to the edge compute to the AI
fails you.
You don't reach confidence threshold
and what are you going to do then?
You could just say,
"Okay, I guess we just had
an exception that time.
I hope to catch it the next time."
No, that's not the way the world works.
You have to be able to continue to do it.
Instead what happens is
when the AI does not reach
confidence threshold,
whether it's the AI pipeline
or the 3D pipeline or the 2D pipeline,
none of them said, "We
see and understand,"
then it raises its hand over the cloud,
and at the other end of that cloud,
there is a person and the person said,
"Oh yeah, that's a mess.
I think I would do that, pick that up,"
and the robot will say,
"Thank you very much,"
go back to work and you may
not hear from that robot again
for some duration.
What just happened?
You had a human in the loop
deal with the exception.
What are you gonna now, what
do you now have in your hand?
The most valuable piece
of data you could have,
an exception that was captured.
Obviously all of that goes back in
and feeds the reinforcement learning model
and you anticipate less and less instances
of that human intervention
for that use case.
When we talk about robots
and edge and cloud, add human,
because you really do wanna
have it be all four elements.
That's how you get the reliability
and scalability across the system.
With that, we have just
a couple of minutes,
but we're gonna invite you to come down
to the Intel booth downstairs
and you can get your hands
on all of these pieces.
Sree will be there and her
team talking about Gaudi.
You can see the RealSense
cameras that we use
and we'll be there.
I'll be there myself demonstrating
how we do this
human-in-the-loop intervention
and how it feeds the AI models,
and you'll be able to control a robot
2,000 miles away in Texas
from right here at the floor at re:MARS.
We're going to invite you,
if you do have questions,
just meet Sree or Jonathan
and I outside the door,
and we're going to invite
you to complete the survey
and come see us downstairs.
With that, we appreciate you. All right.
(audience applauding)