hello and welcome to today's ACM Tech
Talk this webcast is part of a CMS
commitment to lifelong learning and
professional development serving a
global membership of nearly hundred
thousand computing professionals and
students I'm Eric Meyer director of
engineering at Facebook and before that
founder and CEO had applied to ality I'm
also a member of a cm and ACM queue
editorial board you can find more info
on my background in the bio edit widget
on your screen for those of you who are
unfamiliar with ACM or what it has to
offer here's some more information is
your first educational and professional
development resources that bolster
skills and enhance career opportunities
our members can stay competitive in the
constantly changing world of computing
with a range of ACM Learning Center
resources at learning dot HTM dot org
you can see some of the highlights on
your screen
ACM recognizes the role of computing and
driving the innovations that sustain
competitiveness in a global environment
ACM provides timely computing
information published by ACM including
communications of the ACM and Q
magazines access to the ACM digital
library the world's most comprehensive
database of computing literature
international conferences that draw
leading experts on a broad spectrum of
computing topics support for education
and research including curriculum
development teacher training the ACM
Turing and ACM prize and computing
awards and the updated ACM code of
ethics a collection of principles and
guidelines designed to help computing
professionals make ethically responsible
decisions in professional practice ACM
enables its member to solve political
problems using new technology that
enriches our lives and advances our
society in the digital age before we get
started I'd like to quickly mention a
few housekeeping items shown on the
slide and from the view first the slides
will advance automatically throughout
the event on the bottom panel you'll
find some additional widgets and
resources if you're experiencing
problems with the slides or
audios press the f5 key in Windows
command + R if you're on a Mac or
refresh or browser on a mobile device or
you can close and relaunch the
presentation to control volume adjust
the master volume on your computer if
you have questions during this talk
please type them in the Q&A box at any
time and click the submit button I will
organize the questions as Adam speaks
and he will reserve time at the end of
the presentation to address all your
questions or most of your questions this
session is being recorded and will be
archived you will receive an automatic
email notification when it comes
available and check learning that ACM
org for updates on this and other
upcoming webcasts at the end of the
presentation you'll see a survey open on
your screen please take a minute to fill
out to help us improve our Tech Talks
you may also open the link to the survey
at any time from the resources window on
your screen you can use the Facebook and
Twitter widgets on the bottom panel to
share the presentation language or
friends as well as tweet comments and
questions to the hashtag hash ACM
learning we'll be watching for your
tweets we also have a community
community discourse page to continue the
discussion after this webcast including
questions we won't be able to get
through the Q&A today's presentation is
by torch a modern library for machine
learning by Adam pesky Adam is an author
and maintainer of Pi torch he has
already worked with large organizations
like Facebook AI research Google and
NVIDIA despite the fact that is only
recently graduated from the master's
program in computer science at the
University of Warsaw currently is also
finishing his second major in
mathematics his general interest include
graph theory programming languages
numerical computing and machine learning
and Asia Adam is also an ACM member Adam
without further ado take it away all
right thanks a lot ok hopefully you'll
be able to see my screen right now if
you can't and please just let me know
but yeah I think you know we can just go
ahead and get started so thanks everyone
for coming I'll be talking today as you
know about Pike perch and sort of
presented us you know in the context is
a library sort of for solving a lot of
problems associated machine learning but
ultimately a lot of the things that I
will be talking about today go much
further than machine learning and
hopefully like you will see that they
also apply in like a significantly wider
context as as you know just machine
learning yeah as Eric said I've been
with Piper since the beginning and so
I'm one of the original authors and I
still contribute as much as I can and
yeah I just wanted to take just a few
seconds to sort of thank ACM for
organizing this particular talk in 4x4
for moderating and introducing the
questions anyway so when you think about
the Piper series so it's it's sort of a
very wide term that you can think about
right and you can organize it into
essentially multiple layers right so
it's sort of at the very core of all of
it instead of the inner tear the there
are the critical components that sort of
implement the very fundamental
functionality of the libraries like maps
and mathematical kernels and also you
know things like automatic
differentiation we'll be talking about
all of those so don't worry if like you
don't fully you know get good what I'm
talking about right now and there's also
a just-in-time compiler that I will also
briefly talk about later then sort of as
a second tier this second tier is also
included whenever you download piperj
you also get a basically a bunch of
utilities which will help you you know
develop the the applications which is
usually used for so you know there is
the NN module for example which is used
to build machine learning models there
is the optimal for for gradient based
optimizers there's distributions for
things like prevalence of programming
distributed healthcare distributed
computing helpers multi processing
helpers fun things for quantizing your
machine learning models to make them you
know more and more efficient at
inference time and yeah so you know
Piper's sort of there's sort of a very
core on top of which everything is
implemented but it also comes from sort
of with some battery
included whenever you install it and
then there's like a whole ecosystem
essentially of Montford domain-specific
packages which then you can use to also
simplify the implementation of some
particular use cases so you know their
packages for things like audio models or
packages for vision models for text
packages for basically sort of secure
distributed learning with encrypted data
for Bazin approaches for graph learning
so quantum computing there's basically a
whole like long list of different
packages you can try out which are
really high quality so you can see some
of these some of the examples in here
but of course this is not like a
complete list so you know if you think
about papers you can really think about
this obviously like we want to be good
good citizens in the Python ecosystem so
that's just sort of the ecosystem around
pikers but as you'll see you should also
be able to easily integrate whatever we
provide with the rest of the Python
packages because there's just a lot of
things out there yeah and at the really
at the lowest level hyperedge if you
think about it that the sort of the core
component to piperj is pretty much the
same as numpy in many ways or if you if
you've done any sort of data data
analysis data processing in python then
you have surely using them pi if you
haven't really done this then well you
probably know you know software packages
like num like MATLAB they all basically
sorry they all basically provide the
same software abstraction right so the
very core software abstraction did
provide is a multi-dimensional array
multi-dimensional array is like a
generalization of a nested list
essentially where at every single
nesting level all of the lists have the
same length right so you can think of
you know a matrix just like a a - deep
list that sort of by a twice deeply
nested list right and similarly then you
can take a matrix and you can stack
matrices and get like a cubic numbers
essentially getting like a three
dimensional I right now and then you can
take a stack of cubes and you get it for
a dimensional array and so on and so on
right so this is exactly sort of the
primary object in the primary
action that all of those packages
provide the benefit that they give you
is that for example it is Python if you
just were to represent the data as
actual list of lists then it has two
major downsides one of them is that
whale encoding even very simple
operations become becomes very difficult
and the other downside is that is
actually very inefficient so numpy
MATLAB and my clerks they will store the
data in a way which is basically very
native in some sense to your computer in
which which basically enables those
libraries to do the math much more
efficiency than like a completely
general-purpose sort of programming
language which does not have which does
not have all of those abstractions built
in it and then - in particular it
doesn't have been built in in sort of
going some you know going back to the
first issue which is that well you would
have to like write out explicit for
loops the one benefit of this like array
there is like a whole domain-specific
language so like a DSL that's associated
with those arrays and that's like a DSL
of mostly mathematical operations right
so for example and one of the benefits
of this DSL is that essentially you
don't ever need to you pretty much never
have to write loops in your programs
right so in here for example the
statement C equals a plus B essentially
adds two to every single element of the
of the array a no matter how many
elements there are so like a lot of your
code sort of has implicit for loops
implemented sort of inside the operators
that are used but thanks to this even
though python is not a very efficient
programming languages even though it's
not very fast because those operations
and those loops are sort of implemented
in a in a in an efficient way using
probably some different languages as
well you know that you can basically get
pretty much the peak performance on most
of most of the workflows that he would
Express of course mathematical
operations are not everything then for
example the nice statement this deep
will see and then some indexing
expression this for example selects all
of the rows that's what they call in
means and then the first and the third
column of that of the array C right so
you know you do mathematics you can do
basically selecting subsets of those
arrays you can check their shapes which
is sort of the excellence in each
dimension
and then also the number of dimensions
you can you can learn this from the
shape as well and then there's like a
call basically range of of different
mathematical like statistical operations
like at the bottom you know I have a
matrix multiply and you know some
sampling from the normal distribution so
there are hundreds of different
operators on those arrays that sort of
compose this like domain-specific
language that you can use implementer
programs and as I mentioned at the very
bottom towards exactly the same we don't
necessarily call those things and and
the arrays like none Plato's we just
call them tensors but you know most of
the code stays exactly the same
some of the names that we use are
slightly different mostly because of
some legacy reasons I purchased did not
come out employed I will talk a little
bit about this at the very end of the
talk so you know for example some of the
names actually come from MATLAB instead
of instead of numpy but but yeah but you
know the general idea behind all of this
is pretty much exactly the same and yeah
just to sort of just to sort of convince
you that well you know you have an M PI
and now you have hyperedge and you know
now you it's the question that you know
when you start a project you just have
to pick one or the other and the answer
is that he don't really have to do that
you can sort of pick and choose them
based on whatever features to use and
we're really trying to make it very easy
right so even if you have an existing
project right now like a person can use
PI preferred like the new projects you
might be developing but even if you have
a project that's existing but does
something you can still start
integrating Pipers step by step which
you know also enables you to like have a
smoother transition essentially right
like it's always easier if you can just
like change the code function by
function to like to like do something
slightly different the except you know
maybe implement it in a semantically the
same way and and then you can let's say
run the tests at every single step to
like verify that your code still that's
the same thing but it does write as
opposed to just like rewriting the whole
codebase using completely different
library and they're just hoping but you
don't make an error somewhere along the
way and they just spend days debugging
the geeky issues because well debugging
numerical programs is not very fun and
yeah with per chicken you can
essentially do that relatively easily
right if you have a function that takes
in some number
and returned some men PI function like
at the top there are functions like
torches tensor and numpy is array but
you can basically use to convert an
entire race into torch tensors and the
benefit that you get from this is that
those conversions are essentially free
like there is no even though the arrays
may be huge the cost is entirely
constant actually they actually shared
the data in memory we use the same
representation as M pi and so we can
just like reuse the memory fee that
numpy array is actually use just
rewrapping them sort of in hydrogen
objects such as you can actually pass
them in to a hydrogen so but both ways
you can just sort of jump into the
system and out of the system really
cheaply and so you can like slowly start
transitioning start transitioning let's
say a áááhole implementation like this
so you just sort of caste and Piper's
let's say rewrite the Piper's code and
then cast the result back to none pi
before returning it and the whole system
providing you actually implement the
hydrogen code that does the same thing
as an empire code your Priora process
and does not lose any performance and it
to just perform exactly the same thing
so you know just to sort of calm me down
that it's not just like a framework
which will wall you off and you know
will force you into like a completely
traditional systems and you know
everything you can benefit from in
Python but you know obviously just
reimplemented everything but now I
already does would be kind of pointless
so obviously there's more and really
there are two crucial features I think
to what Hydra does the first one is
accelerator support so pretty much every
single device that we use today has an
accelerator of some sort for a lot of
numerical computing use cases it turns
out that it is much that you know GPUs
which have been developed for example
for completely different applications
and science at the computing they
actually excel in many machine learning
workloads and also just high-performance
computing workloads they turn out to be
machines to which this like array DSL
Maps perfectly because there's a lot of
parallelism this is our ADSL it's like
actually really easy to execute this on
the accelerators and so it's actually
one of the easiest ways to program and
to take advantage of the extra videos
like this and fighters in particular
supports and video DP use AMD GPUs and
and Google T pews but there will be
probably support for more hardware as it
comes out but for now those are sort of
three primary things and really
accelerators are one of the are one of
the reasons why sort of machine deep
learning in particular could really take
off for I'd like the 2012 breakthrough
on image net that that model was it was
in particular trains on a GPU and so we
do try really hard to have very good
support for all of those accelerators
because they can usually give you
basically an order of magnitude
improvement in a lot of the workflows of
people in code using fighters and the
second component is high-performance
automatic differentiation so automatic
differentiation is a technique which
essentially lets you write a program and
sort of you never have to do compute the
derivative sort of by hand okay so in
the past let's say if you if you had if
you just want it to write over that
function and then let's say get its
gradient for the purpose of let's say
optimization right you would maybe have
to sit down with a day with a piece of
paper and like you know figure out the
using the chain rule and like the
primitive the primitive differentiated
differentiation formulas what is what is
the gradient then just like code add the
code up the the derivative code
essentially in a completely separate
function right
fortunately differentiation is a
completely mechanic procedure and so a
computer can do this for you right and
so a long time ago people have already
figured out how to basically take pretty
much an arbitrary program and compute a
derivative out of this without you
having to necessarily than anytime sort
of figuring out what the derivative it
is right like as long as your program
implements a differentiable function the
system should be able to efficiently and
and easily let you take the derivatives
of the functions that you compute and
that's sort of the second second second
basically important use case let's say
over numpy
that Pipers provides and yeah that's
also really crucial for machine learning
right like the models that people
actually develop these days are
extremely complex and so if you were to
just sit down and try to work out the
equations for the derivatives
and that would just be extremely
complicated and so this is also really
one of the enabling factors that led the
deep learning in particular I think
take-up in the recent years and yeah so
you know we have those features but the
question is how do we actually expose
them and the answer is that we expose
them in a way which is I think very
familiar if you've ever used a library
like numpy so in particular the programs
you implement using numpy they're very
polymorphic in some way right so in this
case we have just a program which like
samples some parameters for a model we
will be implementing and the model is
just this linear model function so it's
just like an affine transformation of
the inputs and you know and then we can
just sample some sample some example
input and try to evaluate the model on
it right and so in some in some sense
this there's a lot of dynamism happening
in this code which is sort of not
visible statically you cannot just
resolve it statically and that's for
example because there's an infinite
number of different size configurations
you can evaluate the linear model
function right this piece of code will
work out for an infant for a very
infinite number of different shapes of
weight of course not every single not
every single configuration of like sizes
will be valid because well not it not
every two matrices can be multiplied
together like which is the operation
we're doing in here but there is so an
infinite number of different sizes of
matrices that you can multiply together
which you don't really specify in the
code of the other model that you
implement right it's only specified in
the place where you actually create the
parameters or create the inputs when the
sizes with which you will be evaluating
the code are determined and similarly
you know if you want to do mathematics
on a computer you also have to commit to
a particular precision in which you will
be computing you know every the
quantities of interest rate and so numpy
also similarly like with a single
function for this linear model you can
also use the same particular the same
code to basically evaluate evaluate this
function for any particular precision as
long as you like change the precision of
the inputs sort of the the precision of
the intermediate will be automatically
answered from the precision of the
inputs right and so you know there's
already a lot of polemarchus in this
linear model code first
you know in factors they mentioned the
base is the same so you can implement it
basically in the same way
you can change shapes you can change
data types and but so you know now the
question is have we actually use the
accelerator support and automatic
differentiation and it turns out that
both of those actually just extend this
same framework right so for example for
accelerator support you can see that the
code of the linear model actually did
not change at all right
the only thing we have changed is when
we were creating the inputs of the
function of the parameters to that model
we just specify that they are supposed
to be created on a particular device and
what this changes is that the actual
data of those of those arrays will
actually not be kept in the like main
memory of your computer in the RAM
memory but actually in the in the you
know global memory in the RAM memory of
the accelerator that are used so for
example in the G they will actually
reside in the GPU memory and it's also
interesting because a lot of those
decisions can be made at runtime so with
the line at the top for example this
will actually pick the GPU if you have a
GPU in your computer but otherwise it
will just fall back to the CPU
for example enabling you to actually
prototype on a only like less powerful
device like the laptop which may not
have like a GPU which is capable of
accelerating those workloads but once
you put it on a server which actually
does have a GPU the script will
automatically change to it right but the
really important part of it again is
that the code of the file of the
function you implement does not change
the only thing that you change is the
metadata of the inputs and the insert of
the whole computation reorganizes itself
around that decision and you know it
works exactly almost the same for
differentiation so for differentiation
the only thing you have to change is
again for the quantities okay so the
particular kind of differentiation that
packard supports because it's the one
that's really the most important for
machine learning it's reverse mode
automatic differentiation and that's
just really a fancy way of saying that
we can differentiate scalar functions
and we efficiently we can differentiate
scalar functions with respect to a lot
of inputs right and that's usually the
case you have in machine learning you'll
have a lot of produce a single number
which essentially will tell you it's
usually like the last function which
tells me how good or like how well or
how bad you actually did on a particular
you know instance of a task that you
were trying to solve right and you will
be optimizing
- - like let's say lower the law so like
lower the the badness of it make
yourself better in some sense at this
particular task and it works similarly
so the way you actually use it in
tighter is on the on the parameters of
your model you say that well they will
require the gradient right so the
derivative of a scalar function is
usually called the gradient and so if
you say that a different that different
cancers will require gradients that will
basically turn on the automatic
differentiation subsystem by default
it's disabled because it takes some
extra computational resources to
actually do this efficiently but you
know so you have to explicitly opt-in in
to that feature and so if you computer
enough but and if you compute some kind
of a scalar loss then you can just use a
function again linear model the code of
the function that you compute stays
exactly the same and then you have this
like magical function that's called
large autograph Brad where you give it
the scalar output of the function that
you have just computed and then you give
it the sort of like a list of inputs
with respect to which you want to get
the gradient and it will just return you
the list of the same length of tensors
the same shapes and devices and data
types which are the gradients of the you
know quantities with respect to which
you have differentiated and the really
important part to do in here is that
linear model can contain or like you
know the thing you differentiate the
function differentiate can essentially
contain arbitrary Python code there's
like no limit whatsoever to what
particular implementation you use as
long as it's like do all different
mathematical equivalent implementations
will give you pretty much pretty much
the same gradient up to some let's say
maybe floating-point differences
sometime but you can use let's say a
single wave function so you can use
generators you can basically go crazy
with any particular like Python language
feature and as long as the functions
that you have actually implements it is
differentiable we will give you the true
gradients of that particular function so
there is really nothing that limits you
and basically both of those features are
then started using it yeah and you know
as I said that's pretty much everything
you need to do ml right so if you just
want to do something like stochastic
gradient descent pretty much that's that
you have all the building components
right you if you have them data set of
like inputs and targets and let's say
model is parametrized by some vector
theta then you know you just iterate
over the data sets and targets we
evaluate the model on for each sort of
parameter right for each input and you
evaluate the model with the current set
of parameters you have derived then you
get some output you use the loss
function to compare it to some target
you get back to loss tells you how well
you did then you take the gradient of
the loss with respect to the parameters
that's a torture autograph grab a line
and then you just will change the
parameters right you sort of take a step
in the space of all parameters to sort
of optimize towards you know making the
bus smaller on this particular example
that's basically pretty much most of
supervised learning works right of
course this is not really the easiest
way to do this thing right so for
example if you have a really complex
model just keeping all of the parameters
in a single flat vector that's probably
a bad idea
right so it might just be better to have
like a better way to organize your model
and similarly if you look at a lot of
papers you will often see some diagrams
like the one on the left where you know
people let's say think of models and
some as some kind of a hierarchical
structure where you have those like
boxes then code different operations and
so you have you know let's say a boxed
in here that encodes a linear
transformation and another box that
encodes something called the you know
the attention layer essentially and then
you can also take those boxes and like
let's say add throw in some extra
auditions and also group them in a
bigger box which is the gray box in this
particular picture and you will say that
well okay so now you can use this gray
box that's a multiple times in your
model as like an epic unit of some kind
of computation right so naturally when
you think about machine learning models
you often have this hierarchical
structure and those different boxes like
linear attention they can actually have
different parameters and so sort of
transitively the gray boxes also can
have parameters because in some sense
they're parametrized by everything the
inner boxes are parametrized by and yeah
and hyperbola provides you some
interface to actually deal with this
right so the way you actually do this is
there is a torch and in package it's
usually import as just an N and then
there is a class which is called that in
module and so the way to implement for
example this gray box that you see on
the right
on the right of the screen here is to
actually implement a subclass of an and
module and then you have to implement
two functions an initializer and the net
forward function so the initializer
essentially what it does is it builds up
all of the sub boxes right so all of the
sub boxes and essentially you really
only should be building up the sub boxes
which have parameters so in this case
maybe the potential the player may have
some parameters inside and that also the
the linear transform also can it can
take can have some parameters so this is
where we will construct it and
everything you do is you just assign it
to an attribute of this class and
another important thing to note is that
the initializer can have some arguments
right so when you construct your models
there's some hyper parameters it's like
how many layers I will actually have
right or what kind of nonlinear function
I will actually apply in between of some
of my layers right and so for all of
those you can just take some extra
arguments to your initialization
function which will specify like how
your about you your your dislike gray
box may not always encode this
particular structure but it can also
encode like a whole class of different
structures that works like this right
so in this particular case the linear
transform for example it is parametrized
by how many input features we'll have
and how many output features is well it
will have right so there's an infinite
sort of number of different
configurations of this gray box
depending on how many outputs you
actually might want to have in the end
up for your computer right and so that's
the role of the initializer there all of
the forward function is to basically
implement whatever this particular box
does right including the delegation to
lower-level boxes so if you take this
like graph those arrows so they sort of
encode the data flow of different values
throughout this function right so at the
bottom from the bottom comes comes in
the input right and so what do you do is
you apply the attention layer to the
input and then he added to the just
original input that's sort of this like
skip arrow and then you get a quantity
which well in this case maybe we called
pre linear and then we do the same we
just apply a linear layer and then add
it to added again the the pre linear to
do it right and then we get a result
then we just return it so this is sort
of forward is the place where you
actually translate this graphical
structure into an actual model
all right and what model study do is you
you know when you create a model you
specify a little bit hyper parameter so
you have to actually give them given you
know give some numbers or some
configuration parameters for them but
the benefit you get is that now you
actually can access the hierarchical
structure right so for example if you do
the models of linear now you just get
the blue box from from this graph on the
right yes that's the only thing you get
right and similarly linear has a
parameter called weight linear is that
can define transform so you can do model
the linear the weight and you will just
get a just get a tensor which encode be
like that one of the parameters of the
linear layer right so it's not you have
a flat vector and you have to slice off
a particular range of indices you just
have them encoded as a completely
different multi-dimensional arrays with
potentially different data types
potentially being on different devices
all of this is legal and another
important thing that this gives you is
that that's essentially an abstraction
that lets you get easy access to
parameters of your model right so all of
it an end module so you can sort of
think of them as partial applied
function C well you will see a good
example of that in a moment but and but
they're partially applied in the sense
that they also know exactly to what sort
of parameters they have been partially
applied to so you can say model those
parameters and that will just give you
an iterator over every single parameter
in your model so in particular model
that's linear the weight will be one of
the things that would be returned or
like yielded from from the iteration
over parameters right similarly you can
say model the two CUDA and then will
take every single parameter in your
model and just move it to a completely
different device right so you can have
thousands of ten serves as parameters of
your model but this single call will
like find all of them automatically for
you and we'll just move into a different
device you can print them that that's
sometimes helpful but it doesn't include
all the information that's possible and
yeah and so you know the same local loop
looks very similarly using this
abstraction so you know you get you
again get some kind of a data set you
instantiate your model you in this case
we just take East uses the optimization
package again so that's opt-in but SGD
stochastic gradient descent and we just
pass in the model right and then we
iterate over over the data set and we
just again evaluate the model
on every single input notice that now we
don't pass in any parameters the model
like implicitly knows in which point at
the parameter space it is right now and
this is what I mean by a partially
applied function right if you've done
any fool programming it is as if
the model has been partially applied to
the parameters argument and now it's
just a function from inputs to outputs
yeah and then you can for example call
loss the backward and what it does is
it's very similar to something like
birth autograph grad except it's not
it's in some way much more convenient
because it will just automatically find
all of the parameters that you have used
to compute this particular quantity so
the in this in this in this case that's
lost and for each one of them the
gradient will be accumulating into their
death grad attribute so if you say for
example in this slide model that linear
way to the grad that would be the
gradient of the last backward call that
you know you've actually executed and
that would be the gradient with respect
to the you know this like lost quantity
that you differentiate it in the in the
last iteration of this loop then all of
the oddly all you have to do to actually
do the optimization is usually just to
optimizer dot step and you can swap out
different optimizers and because is
exactly the same and then in the end as
they've mentioned the gradients get
actually accumulated for that grad
attribute so if you called law step
backward twice all of the like dark Brad
attributes will increased twice because
you know you would just get the sum of
the two of the same values so it's also
important to remember to cause your Brad
from time stein some algorithms will
require the zeroing a different points
so it's not fully automatic but that's
essentially that's essentially what's
happening alright so you know that's
sort of about the this is about the
basic part of the library and this is
essentially how you would even go about
implementing a machine learning model
and piperj and piped really has been
created to sort of facilitate rapid
research and to make it really easy to
to express those models right but
ultimately like it takes time to develop
a model it takes a long iteration cycles
and so it's really important to be able
to change your model quickly and
validate your hypotheses quickly but at
some point we will arrive at like a
final product like BDD and the resulting
model that you're happy with and at that
point you'd like something you might
care about is actually taking it and
deploying it to ax
work in the real world example give you
some predictions for some particular use
case that you have in mind and this has
been traditionally relatively hard with
piperj but this is we've put a lot of
work into it like over the last
basically a year or like a year and a
half and so it's improved greatly and
now I'll talk a bit about how you do
this right so in tight church now we
sort of talk about two different
components right there is PI procedure
and that's exactly the thing that I've
been talking previously that's sort of
the numpy like API and then there is
Piper script which is essentially
completely in your programming language
but you should not be afraid of it
because as I'll show you it's actually
really easy to use and if you know
Python you know toward script already so
hi dirty girl the benefits of it are
it's well known because that's just the
number way of doing things it's easy to
write and simple to debug because it
just runs sort of as you execute the
program you just run the successive sort
of mathematical operations you can print
the intermediate variables and all that
the downside is that it's really hard to
optimize right as a in an eager mode
we're just a regular lab Python library
so we sort of never know what kind of an
operation will come next and for some
sequences of operations if we knew that
he will execute this sequence of
operations we could let's say execute
them in a slightly better way right
because maybe there are some
transformations we could apply they're
sort of annoying to write off manually
so you might not necessarily want to do
this by hand in your programs but like a
compiler a compiler hypothetically could
rewrite them to make them faster
similarly there is a downside in that
well if you want to take your model and
if you want to for example package it
and put it in a mobile app if you just
use the regular pykrete code which you
know encodes the model as Python code
you cannot ship your mobile app without
a Python interpreter and python
interpreter is a really complex piece of
software all the things these are just
like in the intents of megabytes
essentially so it will increase the size
of your mobile app significantly in many
cases right and it will also not run
very quickly so we would want to have a
faster runtime that would let us execute
those things on mobile easily and
efficiently and so this is exactly what
torch trip has been why tour trip has
been created essentially so as I said
it's a completely new programming
language but in some sense you
if your program it you will not even
realize that in many cases because it is
exactly the same syntax is Python and it
has exactly the same semantics is Python
so the benefits of it are that it's
optimizable and exportable so we can
both find the array code that you know
transparency arrays in some ways and we
can actually export it to run completely
without the Python interpreter the
caveat is that you know we use the same
syntax and semantics but it's not all a
pipe up right so there are some Python
constructs that will not be supported by
toward script having said this the range
of constructs that are supported
actually my first trip right now it's
really really wide and so for really the
vast majority of use cases that should
just be sufficient so you know the real
question is okay so we have this toward
script thing but how do we actually get
to it right we have an eager program we
worked we did some research we developed
some program how do we get it to execute
using toward script and so we take this
as an example we take this not very
machine learning specific function and
so if this is just a regular Python
function right but the way to actually
get it to work when torch script is you
just add an annotation on top and so
this annotation just hard to describe
what it does is it will tell us that you
want to you in this particular Python
function you just want to use hard trip
and what we'll do is we'll inspect the
source code and like every time you
execute this function it'll actually not
run through the regular Python
interpreter it will run through like a
custom runtime system that we've built
to actually execute to optimize and
execute this code the important thing is
that this does not throw an error then
the program that you know looks like
this and the program that looks like
this they're exactly the same
semantically there's no difference that
you can notice except from well maybe
some things may become faster right
because well we can optimize some things
but essentially semantically it should
always give you exactly the same answers
of course you know when you sprinkle the
torch it's connotations sometimes who
will complain and sometimes will reject
pieces of your code saying that it
doesn't so it doesn't support some of
the features that are used but if it
doesn't throw an error like this then
it's hundred percent guaranteed to
preserve all of the semantics of the
user codes you're just programming
and in some places you just add those
fortune script annotation and everything
magically sort of attempts to use the
different run time but you know you can
just think of it as if you're just
programming Python as long as that
purchase script is not complaining but I
use a feature it doesn't support it's
all good and you know the subset we
support is a relatively large one notice
that is important is that it's actually
a statically typed language so you need
to provide type annotations on the
inputs but the types of all the
intermediates are actually inferred
based on those so there are some type
annotations that I have to do in case of
function inputs but in the rest you
pretty much never have to actually
annotate any intermediates with type
information we should be able to infer
that and what you can see on this slide
is that like most of the built in for
example data structures like lists or
big optionals they are already supported
in part script and savior program uses
those it just you can just like slapped
or dude script on top of it and it
should be all good and similarly another
thing you can do is you can actually
implement all classes so tortured script
also support classes this is actually an
example taken from a research project
where you can see pretty much none
trivial and relatively convoluted pen
code but all of this is supported by
toward script and sort of works out of
the box the only thing you have to do is
add this torch in script annotation at
the top yeah the only sort of exception
to this rule are n n modules because
torch script classes don't support
inheritance right now and modules need
inheritance because well your models
will inherit from an end module so the
way you use those actually a little bit
special
the way you script those is you
instantiate the the module so you
instantiate the model right and then you
just pass it into the portrait script as
like just calling it as a regular
function and what you get back is a
model which behaves exactly like the
original model that you had in mind but
now it will also run through the like
optimized runtime on that that we have
built yeah and you know so again getting
back to the benefits that this gives you
you can export those things right so you
have a scripted function like added the
topping here or if you have a scripted
model just by you know passing in them
instant
of a model that he have just created you
can just call targe in save on this
object and specify a file name and I
will just dump all of the details like
all of the code and all of the
parameters in case it's a model in this
in this particular archive in this point
and what you can do later for example is
they are basically sort of default work
clothes now starting from version 1 1.3
point O which has been released this
October
there are basically default sort of
workflows for taking checkpoints like
this and embedding them in both Android
and iOS apps so in some sense you can
just take your Python programs and just
run them on mobile apps sort of through
a different interpreter which is of
course in some in some sense geared
towards executing code that focuses on
those monthly dimensional arrays it's
not just meant to like I surveyed any
kind of Piper of Python code so you know
just a small caveat and here it's really
mostly geared towards things that are
you know in substance do some numerical
computation yeah and you know it's it's
also just to like give you a quick note
the actual archives that are produced is
there there are just zip files with a
relatively readable structure you can
even like unzip a pipe like this and
there's a code directory and you can
just see all of the code that like the
whole implementation of your model or of
your function sort of printed out in a
human readable format you can even
modify it and reason fit again and that
would just be like you will just get a
different checkpoint with like a
different semantics in some way so you
can even easily inspect those
checkpoints after you have actually
produced them and you can also alter
them if that's something you really want
and yeah and you know it's not just for
mobile like we starting from 1.0 I think
we actually provide also like just a
pure C++ library which you can use to
evaluate those checkpoints as well right
so if you just link this library to your
project you can just a target load
giving a path to the checkpoint then you
can create you know some input and then
like as a one-liner you can basically
evaluate the functions the functions on
the oppose you created by cancer's right
so pretty much anywhere where C++ can be
piperj you can also use PI codes and use
also exported Python programs in those
places and also
a side note starting from 1.0 we also
provide a pure C++ interface so
everything all of the API is that you
just saw in Python they're pretty much
one-to-one in correspondence with C++
API so if your project for some reason
calls for C++ like I don't know you're
interfacing with the game engine maybe
and game engines are usually in C++ so
maybe just adding Python to your project
might be annoying you can just use C++
to implement this right and in this way
you can also get rid of some limitations
of Python like you know the global
interpreter lock if that's really what's
slowing you down so I will not be
showing you a lot of examples for this
just to sort of give you anything just
to give you an idea that it's really
pretty much the same except for some
syntactic differences in your models so
this is an example of a Python model and
if you were to implement it as a C++ it
would look like this so you know if I
switch back and forth it's pretty much
the syntax is slightly different but the
lines are pretty much one-to-one with
like how you actually describe things
right so once you know one API it's
really easy to actually learn the other
one
similarly pretty much all of the helpers
that we have in Python land so in here
you see an example training loop in C++
the same training loop would look like
this so again like pretty much
one-to-one abstractions are exactly the
same it's just the cemented the
syntactic differences between two
different languages but the overall code
that you implement it's really easy it's
really easy to there are no templates
involved so compiler error messages
should be readable and yeah it should it
should hopefully work work that
relatively nicely yeah one interesting
component that I didn't have time to
talk about today is the whole data
loading pipeline which we also provide
as part of the data package both in C++
and in Python it's the second line both
of those examples yeah and you know as I
mentioned most of the packages like the
neural network library which also comes
pre-built with a lot of those intend
modules built in there both in Python
C++ the option packages there the data
file loading packages there in the
serialization utilities there are some
utilities to exchange data between C++
and Python programs so you can actually
mix both languages any single project
really easily and then there is also the
gem namespace which gives you access to
this to this just
some time optimizing compiler to execute
the models that you have like the
authored in Python yeah okay so and you
know so that was a lot of sort of
content about using the jib as a means
as a mean of exporting the code that you
wrote in Python maybe as a result of
some research project and getting it
into into an environment which cannot
really work with Python for some reason
but I have not talked all that much
about optimization yet so just to sort
of attach the optimization aspect of it
you know we we have not we have
definitely spent more time optimizing
for actually making it easy to like
support a large language stop set than
actually building out the full
optimization pipeline for script but one
of the use cases we have actually looked
at our required neural networks so
regarding real networks are models that
the rough outline looks like this so you
have some kind of a function which you
know and here I omit for simplicity
which is called a cell and then you have
essentially the whole model is like
takes a sequence of inputs usually to a
sequence with outputs and it uses the
the cell function to like which is
applied to a step at every input and to
some particular state right and so the
the cell is like a function which takes
the input and at a current step and they
like input state and produces an output
for this particular step and a step and
a state for the next time step right and
then you can imagine just like passing
in as you progress over the over the
input sequence you also progress sort of
over the states that the model is
producing so in some sense it gets some
kind of let's say memory or some kind of
context about the about the inputs that
it has seen in the past so those kinds
of models are for example useful for
things like translation or yeah or
things like that or just text yeah and
if you just take this function right if
you just add this you can notice that
really the only thing you have to do to
get it work with script it's just add
the annotation on the lsdm function the
lsdm cell function is scripted
automatically just by the means of it
being sort of used from the body of lsdm
like if you call STM cell directly this
will not go through our script but if
you just call a cm the actual calls will
stem cell will be replaced like a call
to an optimized version of OS DM cell
which will also use toward script and
yeah then you can actually get some
pretty nice results so if you just use
eager mode you could just use the code
that looks like this it will take on
some you know particular input size of
course those are not representative and
like the speed-up that I show you here
it's not necessarily representative of
every single use case but you know the
speed-up you can essentially get is like
an ear mode let's say it takes 23
million milliseconds to actually
evaluate this model over some particular
sequence right you can compare this it
could be an encode enn is like a
handwritten library by Nvidia where they
basically provide handwritten kernels
for like many different kinds of very
popular machine learning bottles and so
one of them is for analyst TM recurrent
neural network and so you can see that
it's pretty much you know 4 times 2 up
to 5 times faster if you just use the
handwritten kernel than just their
regular like array based code like in
here but just adding this tortured
script annotation basically lets you
bring down this those 23 milliseconds to
somewhere around 6 milliseconds so
that's pretty much a 4x speed up you're
saying much closer to the like 10
written kernel regime in this case the
through to the eager mode regime right
and so you might wonder why is this
important right like if we have a
handwritten kernel for this model then
why is it impressive that we can get a
slightly worse performance in the head
written kernel right and the the real
reason why you want to be able to do
that is code enn only has a finite a
finite number of kernels implemented
right so if you're doing research on
some let's say you want to change you
want to play around with what the cell
function computes if you just change the
cell function slightly there's no longer
it could be in an implementation that
will compute it for you right but the
chip is more general it can actually
take a lot of different cell functions
that have implemented and hopefully it
should be equally good at optimizing
them as it is optimizing as it is good
at optimizing this particular case so in
some sense it is applying all of those
things automatically so making the code
much more convoluted but it doesn't
matter because it's a it's an automatic
tool so you yourself does not have to go
through the optimization process but it
allows you to experiment with models
with which you normally
let's say might not want to experiment
because it takes a huge toll on your
basically time budget for research if it
takes like if slows you down let's say
5x alright so I have just a couple of
points to touch upon before we finish
because we're running out of time so
pipe or did I come out of nowhere it's
actually synthesis of different packages
like torch 7 there's been 7 torch
packages before this Hipps autograph
entertainer we initially took a lot of
inspiration from other packages as well
we're very much community oriented all
the development is happening on github
through issues pull requests you can
also be involved you can comment and
submit your own patches we're very happy
to accept more people developer velocity
is critical to us to the point that is
in some cases it's more important than
performance we're not really aiming to
like top every single performance
benchmark it's really much important
more important for us that it's actually
pleasant a system that is pleasant to
use but of course we don't want to like
sacrifice it's a orders of magnitude of
performance if we don't have to in
particular we do focus very much on
building composable abstractions really
wants to make the abstractions we use
sort of easy to use and also we want
that to behave well when you compose
multiple of them and also especially we
want to be able to give you the ability
to actually pick and choose the
abstractions you use so in particular we
don't want to be a framework which like
forces your problem into a particular
way of thinking about it we just want to
provide you with tools to actually
express whatever you want and the tools
that you use which you which we have
provided should behave like in a very
predictable way in particular right so
that they should not they should not
surprise you all that often yeah what's
the close integration with other
libraries is really important for us I
sort of touched upon this what I talked
about numpy integration but there is
also integration with things like Q PI
so that's sort of like numpy bottom GPUs
we also work with with authors of those
libraries to enable easy sort of
exchange of data between those libraries
too and as I've mentioned a lot of the
features that I talked about today so
things like you know accelerator support
and automatic differentiation they're
not only useful for machine learning
there's a lot of other domains like you
know physics simulations or finance
which actually might benefit from having
ax
those things too right like numpy is
great and vampires used for a lot of
different things in particular not so
much for machine learning but it is used
a lot for other use cases and pretty
much every single of those use cases it
can benefit in many cases from from from
having access to an accelerator so it
might be interesting I would actually be
very interested in expanding the scope
of piperj to other domains as well all
right so that's almost everything I have
for today so I just wanted to take a
moment to like thank all of the sort of
organizations that are supporting piperj
and also to all of the contributors it
is very much as I mentioned we're very
community focused and we do care a lot
about the contributors to our project so
that's just a quick way of saying thank
you to the long list of people who have
contributed to it over all this time and
yeah thanks for joining us that's
everything I have for today and if you
have any questions I'm happy to take
them now all right Thank You Adam let's
move on to the questions and answers and
so the first question there's a quite a
few questions but I'll pick some like
you know and see how much we can get
through in the remaining time and here's
one question and what does it take to
add support for a new accelerator such
as an FPGA based circuit right that's a
good question so right now it's not it's
not ideal for sure and it's usually
right now it requires some modifications
basically for the code base so in some
sense you would have to you would have
to fork the code base and in some sense
if you wanted to do that which it's not
it's not something that I'm very happy
about and we are sort of moving slowly
to decouple like we basically have like
a hard-coded set of devices right now
and it's not a place where we want to be
so we are working to resolve this but
for now it's not like oh you just like
implement like some interface and it
magically works even though the end goal
is to have it work like this all right
thanks so a similar question is what
decides if a component of Python is
supported by torch script
well it's mostly like a there's no like
fixed decision process it's like we try
to we try to look for the idioms I guess
and for the constructs that people
usually use when they're trying to store
trip or like we get you know sometimes
we get backward reports from people who
are saying like look this it would
probably make sense to include this
particular Python constructs and then we
just you know try to make a decision
you know just deciding if this falls in
scope or out of scope let's say for now
in many cases most of most of those
constructs are relatively regular and
should not be very difficult to support
so times sometimes just takes some time
to actually implement this support but
for most of them we should be relatively
open to actually adding them alright so
here's another question it's similar to
the first one so it's by torch available
on google teepee use yeah from what I
know you can use the GPUs that are in
the cloud with Piper there is if you go
to github.com slash Piper /xl a that's
the repository where the way the TPU
support lives and it essentially goes
through exabyte which is like a piece of
stuff for google provides for interface
and see with GPUs and you can just use
this and it works similarly to GPU and
CPU support you just you just cast
change the device instead of saying CUDA
our CPU just say X LA and that will move
it that will move all the tensors to a
GPU and if you run them through your
code they will actually execute on the
TPU I'm pretty sure that the images that
are available on Google Cloud if you
actually spin up some instances they may
already have the GPU support built in so
they should be relatively easy to use
I'm pretty sure some people at Google
have worked on that thank you
so here's another one it's a low level 1
2 and we have any tips on profiling part
or code and so they're Storch that you
tell the bottleneck and and vvp or
line profiler and Python so how do you
can pick between these different tools
yeah that's a very good question I think
it really depends on it's not always
easy to like it's very hard to have a
single tool which like does everything
you need from for your like swabs all of
your profiling needs so I would say that
for two littles bottleneck is actually a
good sort of starting point if you just
want to explore whatever model that
stays doing and maybe you will get some
potential insights it is also a it is
also a sort of program which in some
sense try to tries to summarize and just
like automatically figure out things
which means you know it's not super
smart so it can also give you some very
valuable insights if you're running if
you just want to figure out if you're
let's say CPU bound so you know it there
is there there are different sort of
ways of be a fighters code being slow
right so in some ways you're Piper's
code can be slowed because well the
operations you execute are really
expensive and if you run them on a GPU
it just takes a long long time to run
them on the GPU right similarly it can
be slow because you're running a lot of
smaller operations so even though
they're really quick to run on the GPU
the cost of actually like the cost of
communication between the CPU and GPU
it's like large enough that you're
actually it's actually CPU doing like
most of the job and not like most that
most of the job is just the
communication part and not the
computation part and those are really
the two important answers that you have
to get so what I usually do is there are
use sort of a cross of the Nvidia
profiler if I use in an NVIDIA GPU I
will use the Nvidia profiler to see GPU
time line if it's actually packed with
like math with like mathematical kernels
then that's a good sign that well you
should probably you know spend some time
optimizing your program or I don't know
worst case scenario is you write some
custom kernels maybe but do keep in mind
that at the Nvidia profilers will slow
down your program a little bit so if
there are gaps in the program if they
are relatively small then this means
that you know it's probably just a
profiler overhead
if they're really large then the space
are probably bound by the CPU GPU
communication and in this case it's
probably better to use we have search
that autograph profiler which is like a
very low overhead profiler which can
which you know lets you build up
timelines of operations but nothing too
said without actually looking into what
the accelerators are doing which you
know adds up to a lot of profiler
overhead so you know it's not maybe it's
super satisfying answer but it really
depends on the use case and that's what
I usually do all right next question
it's automatic differentiation
implemented for complex numbers and
complex activation functions yeah that's
another good question it's so complex
support does not work great right now
there have been a few attempts from
people sort of outside of the spar team
to actually add a complex number support
and with some of the pour requests that
have blended recently it should actually
be a lot easier to start adding it so it
does not work great right now and I
really want to get complex number
support because that's for example
what's blocking let's say some physics
use cases to actually use piperj so but
yeah I mean once we once we clear out
those that should be a lot better so
again that's another thing which is not
working great right now but I would I
would I would think that you know
hopefully they should be implemented in
a relatively near future as well all
right I think we have time for one last
question so let me pick this one where
do you start to become a contributor to
PI torch for example is there select
community that you can join or like what
like how do how do people get involved
right yeah I know don't remember if we
actually have so I know that a lot of
projects actually do something like if
you go to their issue pages they will
have labels with a good first issue I
don't actually remember if we're doing
that right now but you know something
that I would recommend
is go through the even if the labels are
not there just go through the issues
look if something like looks relatively
approachable to you if something does
not necessarily look approachable and
you have some you know you might want to
do this but he needs to make sure our
context you can also comment on the
issue and someone will get back to you
and hopefully give you some additional
information for how would you go about
solving this or if these give you some
pointers for like where you should look
or like a full request which was doing
something similar so maybe you can
follow similar steps and also if you
have any issues if you go we have very
active user forums which you know are
slightly more oriented towards the use
cases and not necessarily hacking on an
improving piperj but there are a lot of
people who know and understand deeply
how Piper works so you can also ask the
question is there and hopefully someone
will be able to help you alright well
that was it I'm afraid we have to we
have run out of time today I would like
to thank Adam again for his great
presentation and insightful answers to
the many questions and special thanks to
each of you for taking the time today to
attend and participate and please
remember the talk was recorded and will
be available online in a few days at
learning dot a cm dark you can find
announcement of upcoming talks and other
ACM activities learning the day's event
dot orc and ACN dot org and also please
fill out our survey where you can
suggest future topics or speakers which
you should see on your screen right now
and on behalf of the ACM at the pesky
and myself Eric Meyer thanks again for
joining us and I hope you will join us
again in the future this concludes
today's talk