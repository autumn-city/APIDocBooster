"Tanh seems maybe slower than ReLU for many of the given examples, but produces more natural looking fits for the data using only linear inputs, as you describe.  For [example a circle (hyper-link)] vs a [square/hexagon thing (hyper-link)]."
"Most of time tanh is quickly converge than sigmoid and logistic function, and performs better accuracy [[1] (hyper-link)]. "
"A good neuron unit should be bounded, easily differentiable, monotonic (good for convex optimization) and easy to handle.  If you consider these qualities, then I believe you can use ReLU in place of the tanh function since they are very good alternatives of each other."
"Update in attempt to appease commenters: based purely on observation, rather than the theory that is covered above, Tanh and ReLU activation functions are more performant than sigmoid. "
"For example, try limiting the number of features to force logic into network nodes in XOR and [sigmoid rarely succeeds (hyper-link)] whereas [Tanh (hyper-link)] and [ReLU (hyper-link)] have more success. "
" In the Deep Neural Network, which I mentioned earlier, this affine and tanh process is repeated three times.  This combination of Affine and Tanh can be used once to function as a Neural Network for one layer. "
"From documentation, [https://pytorch.org/docs/stable/nn.html (hyper-link)] "
"Usually, you can use torch.cat to concatenate two tensors. "
"However, recently rectified linear unit (ReLU) is proposed by Hinton [[2] (hyper-link)] which shows ReLU train six times fast than tanh [[3] (hyper-link)] to reach same training error. "
