If you're using a MultiRNNCell, concatenate the states for each layer first:
Look in the TensorFlow source code under python/ops/rnn_cell.py too see how to subclass RNNCell.
You can use "outputs" as hidden states of RNN, because in most library-provided RNNCell, "output" and "state" are same.
And BasicRNNCell need a Tensor other than a Variable as inputs.
I think the problem is that the value of Agent's self.histogram1 never got updated to reflect that summary assigned in RNNCell.
When RNNCell's __call__() method is invoked, it updates the RNNCell's value of histogram1
From what I can tell, the GRUCell has the same __call__() method signature as [tf.keras.layers.LSTMCell (hyper-link)] and [tf.keras.layers.SimpleRNNCell (hyper-link)], and they all inherit from Layer.
You should be able to just use the same RNN framework and pass it a list of GRUCell objects instead of LSTMCell or SimpleRNNCell.
Look like RNNCell.LSTMCell and write your own with changes you want.
LSTMBlockFusedCell is inherited from FusedRNNCell instead of RNNCell, so you cannot use standard tf.nn.static_rnn or tf.nn.dynamic_rnn in which they require RNNCell instance (as shown in your error message).
Also, notice that the inputs to any FusedRNNCell instance should be time-major, this can be done by just transposing the tensor before calling the cell.
If before you were using: MultiRNNCell([BasicLSTMCell(...)] *
  num_layers), change to: MultiRNNCell([BasicLSTMCell(...) for _ in
  range(num_layers)]).
The BasicRNNCell has a parameter called activation function.
1.1 The Decoder instance is  from:
seq2seq.BasicDecoder(cell, helper, initial_state,output_layer)
The inputs are:  cell (an RNNCell instance),  helper (helper instance), initial_state (initial state of the decoder which should be the output state of the encoder) and output_layer (an optional dense layer as outputs to makes predictions)
1.2  An RNNCell instance can be a rnn.MultiRNNCell().
MultiRNNCell/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_2/lstm_cell/bias:0
MultiRNNCell/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_2/lstm_cell/bias:0
MultiRNNCell/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel:0
  MultiRNNCell/rnn/multi_rnn_cell/cell_2/lstm_cell/bias:0
  NewMultiRNNCell/rnn/multi_rnn_cell/cell_0/lstm_cell/kernel:0
  NewMultiRNNCell/rnn/multi_rnn_cell/cell_0/lstm_cell/bias:0
  NewMultiRNNCell/rnn/multi_rnn_cell/cell_1/lstm_cell/kernel:0
  NewMultiRNNCell/rnn/multi_rnn_cell/cell_1/lstm_cell/bias:0
  NewMultiRNNCell/rnn/multi_rnn_cell/cell_2/lstm_cell/kernel:0
  NewMultiRNNCell/rnn/multi_rnn_cell/cell_2/lstm_cell/bias:0
You'll see this same mechanism used in tensorflow [rnncell wrappers (hyper-link)]
The hidden size of your layers: it is the units attribute of the RNNCell constructor.
I recommend you to have a look at the TF tutorial on Recurrent Neural Networks [here (hyper-link)] and maybe this answer [here (hyper-link)] to understand what a RNNCell is w.r.t.
cell = tf.nn.rnn_cell.MultiRNNCell([drop for _ in range(num_layers)])
cell = tf.nn.rnn_cell.MultiRNNCell([drop])
