[Music]
good morning and a very warm welcome to
all the people attending this workshop
today organized by intel in association
with analytics india magazine
and today we are going to understand and
learn how to accelerate python
applications using intel one api toolkit
to give you guys a brief background
developers often struggle with high
computational complexity
algorithmic challenge and inference of
large neural networks while accelerating
deep learning applications
pytorch facilitates building these deep
learning projects by allowing the
creation and training of deep neural
networks and the ability to perform
accelerated mathematical operations on
dedicated hardware
however deploying complex models with
thousands of millions of parameters and
large number of layers require heavy
gpus
and that's where intel extension for
pytorch ipex comes and comes in handy
it's a part of intel's one api ai
analytics tool kit and is designed to
improve the pytorch cpu experience while
achieving good performance
so this workshop here is going to give
us a better hands-on understanding of
how developers can accelerate pytorch
applications using one api toolkit
before we delve into the session i would
request you to register yourself on
intel devcloud the link is shared with
you over the chat box please also use
the event code one api 18 august it's a
one word
so please register yourself to get
started with this workshop
we are also running lucky draw contest
wherein 10 lucky registrants on intel
devcloud will get a chance to win amazon
vouchers worth inr
1000 each
so please hurry up and register yourself
on intel devcloud with the even code
the workshop session will run from 9 30
a.m to 12 30 p.m and all the attendees
can share their questions on discord the
link will be shared on the chat window
and on that note i will hand over the
session to kavitha the developer
marketing manager apj at intel
over to you kavitha
thank you sujithi for that wonderful
introduction uh and hi everyone this is
kavita root i welcome you all to the
intel one ebay hands-on workshop uh
where we bring in the accelerated python
application using the one api ai
toolkits
um i have some brief ground rules for
today's event uh ellen if you can go to
the next slide please yeah um
we we have the intel dev cloud access
and if you have been participating in a
workshop previously you would know the
devcloud access is mandatory sign in to
devcloud with one api 18 in august as
the event code uh we might have some
internet connectivity issues or go to
webinar tech glitch right so please bear
with us and we will come back to you
very soon
like sujiti explained the q a's are
going to be hosted on discard so feel
free to join us there and we have some
of our moderators who can help you with
your relevant questions regarding the
workshop today we will have some poll
questions and trivia to keep this
workshop not just you know from a
learning perspective but also fun
element is included there so participate
and you know uh look forward for
surprises to be won through the contest
okay the next slide
uh as as you all know that you know one
of the key element for developers you
know when you're working on your
projects is to you know see how
innovation you can bring it from the
workshops we are organizing today right
so we're trying to introduce the
platform to you devmesh is a community
portal for like-minded individuals like
you and developers who can create and
share the innovation story with intel
and the other network available there so
all you need to do is sign up to
devmesh.intel.com create a profile for
you and start sharing your stories
sharing your projects uh you know
respect of the segment right whether
you're working on ai iot hpc or one api
for that matter go ahead and share your
projects there
some of the key benefits for you for
parties participating on the deaf mesh
is you can you know be our intel
software innovator you can also uh you
know create your own blog stories which
will get amplified in promoterangel.com
and i intel developer zone as well you
can connect with uh you know your peers
and community members on the devmesh
community you can share your projects or
collaborate there we have our ai builder
network there we have most of our
innovators and student ambassadors
participating in
in this community portal so sign up
there the other benefit you have by
signing up there is uh you can be a
speaker at events uh like this right you
can participate and you know be a key
speaker for summits and events and
workshops coming up so sign up and look
forward for more information there and
reach out to me for any further
information you might need on deaf mesh
we are organizing a
contest for devmesh
and this is specific
yes please the next slide
so
we we're organizing a contest just for
you guys who are participating in the
workshop today all you need to do is
sign up to devmesh.intel.com
submit your projects if you have uh your
current projects that you're using for
python for right it could be replaced
with either intel's extension of python
or intel distribution of openvino or any
other relevant components or toolkits of
one api along with dpc plus plus after
you replace the components you know you
can share the performances compare the
performance performances with your stock
counterparts and the same can be
submitted for intel's evaluation uh the
contest ends by 30th of this month 30th
of august to send in your project so you
need to go ahead and submit those in
devmeasureintel.com
and send me the link to
kavita.organial.com
the prizes will be announced by
analytics india magazine on on 30th and
and the top few winners are eligible for
uh some exciting amazon cash watchers
next slide please
we are also running a short contest
today uh and uh the aim team will add in
more details of this on on the chat
window and you will also see most of the
um you know the contest details been put
across on discord so look forward for it
what we're doing is on the contest we'll
have the trainers uh take you through
the modules being presented during the
hands-on session you have to submit the
optimized python code for both training
and inference um and you can share with
me the screenshot before we close the
day um
or by end of tomorrow which is 19th of
august so send me the screenshots to my
email id kavitha.intel.com
all you need to do is participate in the
hands-on sessions sign up to intel dev
cloud be active on the dev cloud for for
this three or three and a half hours of
the workshop and then stand a chance to
win the goku
goki smart watch and that will be
announced by analytics with their
analytics india magazine again
with this um you know i would like to uh
bring in the dev summit to you all the
dev summit is basically focused on one
api data panel c plus plus ai ml right
for accelerating computing across mixed
architectures whether it's cpu gp or
fpga or any other accelerators
uh so this summit is specially defined
for hpc and ai developers if you are
working on a similar background you know
you can participate and register today
we will send across the register link on
the discord and the chat window shortly
what you would learn from the summit is
basically it's it's all for the ai and
and the hpc developers here to connect
and listen to some of our industry
experts academia uh and intel speakers
come in and share their one api case
studies right so we have some excellent
uh case studies on dpc plus plus and ai
ml platform so come in and join and you
know try to explore what the community
is talking about from the innovation
perspective so uh the registrations are
on today feel free to register and
participate at this event which is
happening on 15th of september with this
i would like to introduce our speakers
for the day
we have uh
lakshmi nassim and who's going to be our
first speaker for the for the event he's
going to talk about the what api uh you
know portfolio of offerings as well as
the introduction uh to dev cloud lakshmi
leads the consulting team for developer
products and consulting division with an
intel software group um and in his role
right he's basically giving a lot of
expertise to our key accounts um from
from intel hardware across iot client or
hpc segments he has also been working
with some of our key customers to
address some of these major challenges
and issues he has been working with
intel for nearly about 16 plus years now
and he has a bachelor's degree in
computer science and has completed his
senior leadership program from nc he
loves to read write and teach in his
spare time
so he's available on linkedin and
twitter and feel free to connect and
network with him
our next presenter for the day is aditya
and jingsan
is a senior technical consulting
engineer working at uh intel he has been
with intel since about five years now
and has a very rich experience in
enabling the developer community and our
key enterprise accounts right and has
expertise over hpc uh you know deep
learning uh performance of deep learning
frameworks and performance libraries for
high performance for performance
libraries for mkl his research includes
machine learning deep learning and
optimization performance optimization
and data analysis so look forward for
him uh to get enhanced on uh you know
and and and theory session today for for
the event
our next presenter for the day is aditya
aditya survey is the ai technical
consulting engineer
at intel and he was a bachelor's degree
in engineering physics from iit delhi
and master's degree in computer science
with ai specialization from iit mumbai
apart from his work he loves to watch
play and you know play football and
cricket so he's our uh trainer today for
the hands-on lab um and he will
uh talk more on that front end in some
time now we have two of our moderators
present today uh we have um bhandar
gurov who is going to be a moderator on
discord uh he comes in with a very rich
parallel programming background and you
know he's helping organizations to
accelerate the applications on gpu and
cpu he's been working as a technical
consulting engineer at intel and is
responsible for enabling the internal
and external customers to extract the
best performance on on gpu
um and and we have mary as well
in the team who's going to help us and
support uh some of your questions on
discord so look forward for
these two key tc's who's going to help
you all on the discard front
with this i hand over the session to to
lakshmi uh lakshmi over to you thank you
thank you
can you all hear me okay can you see me
okay
yeah yeah lakshmi yes
awesome okay thank you so much kavita
thank you for that wonderful intro about
the workshop as well as the introduction
to the speakers
i hope all of you
in whichever part of the globe you are i
know we have attendees from india and
outside of india across apac many many
thank you for joining this workshop and
spending your time uh you know valuable
time in this workshop so here here is
the rough uh agenda for the next uh
next two to three hours that we have
today so i will introduce you and expose
you to one api i know some of you who
have been part of our previous workshops
probably might have already been
introduced to one api so i will keep it
a little brief so that you know for
those who are new to one api you know
they are not left out so i will keep it
really brief and make sure that you're
able to grasp what on api is and also
i'm sure a lot of you would have access
to dev cloud but in case you didn't have
access to dev cloud probably i will just
walk you through uh the instructions for
dev cloud and we will also have a hand
handbook which will basically guide you
in case you missed the
dev cloud setup part of it right so we
will post it on discord also we will
post it in the chat window to make sure
that
you have the required infrastructure
because this is very important because
this is where you're going to do all
your labs hands-on labs on right so
that's why it's important and then we
will talk about first we will give you a
theoretical overview of the intel
hardware features for ai this is
important because a lot of
a lot of times we missed that basic
understanding of what really intel has
in the cards for from an ai standpoint
right so we will talk about the critical
features that enable ai on intel
hardware right so after that we will
jump into pytorch and how
the intel extensions for pytards really
use the hardware features that we will
speak uh in the previous session
after that we will jump into the
hands-on labs for both training and
inference we will have uh
like almost a close to 40 45 minutes for
the lab where uh we will have we will
walk you through the how how to use the
intel extension for pytorch for both
training as well as inference and then
uh we will conclude with uh showing you
a demo of using intel's openvino toolkit
which is primarily a toolkit for
inference right we will also show you
how to use a pytorch model um using and
undo inference using intel's openvino
toolkit right
so
this would be the pretty much the
summary uh for or the agenda for uh
today
with that let me uh straight jump into
the introduction to one api
so
it is very interesting that i i am
actually at my
sister's place and literally there are
eight people
logged into the internet at this point
in time right so i have four of our kids
uh each of them logging into their
schools uh school networks for their
online classes and uh you know uh i'm
logged into this meeting and uh i have
my brother-in-law who's also logged into
a different you know his own corporate
network and my sister is a teacher so
she is teaching the kids right at this
point in time this is one of those uh
you know
critical workloads which has actually
shifted in overnight literally after the
pandemic heated right so this has been
online learning and online meetings etc
has been a critical workload that has
over over the last one to one and a half
years it has just transformed right
meaning it just it was a bubble right
and now it is more or less it is going
to be part of our life right and similar
to that when it comes to the pandemic
and the drug discovery or the vaccine
discovery
right that's another set of workload
which really requires so much of compute
and
every side every corner of the world
scientists and researchers are really
using all the compute available
to make the drug and also the vaccines
of course they are available just
because
uh you know we had this excellent
compute access to compute
we are lucky to have the vaccine right
and i'm sure you know maybe in the next
few months we also have the drug that
can cure
the coronavirus right so all of this the
back end is the compute and today
compute cannot just be one device right
it cannot be just cpus or gpus it is a
mix of all these different hardware
right now
given that the hardware is
is going to be multi-fold in terms of
cpus gpus
not a single hardware is going to be
good at everything right so that's why
for some workload cpu may be good some
more loads gpu may be good some more
codes fpga may be good and also we have
a lot of other accelerators that intel
has got and also the industry is
innovating and now the biggest struggle
is okay we have these excellent hardware
that can do this excellent computing
right but now the biggest challenge is
there are all these developers who are
really trying to figure out hey you know
how do i program to all these different
hardware right that is one of the if
some of you have been programming to
fpgas you know how difficult it is to
really program to an fpga right so the
whole idea of one api is to create a
common framework that can help you
program to these all different hardware
right so essentially uh the the
interface the developer interface to all
these different hardware is going to be
unified through one api right so that is
the whole idea of one api so in in
natural how are we accomplishing this
right so how are we accomplishing this
is we
just like how you have traditional
languages like c c plus plus or cpus we
have something called as dpc plus plus
or data parallel c plus plus which
essentially what it does is it is going
to
really be talking to all these different
hardware in a unified fashion right so
that is what this basically this uh
framework allows us so you see here that
dpc plus is one of the backbones
for one api
so um
so you will see that now of course right
uh data parallel c plus plus so it is it
is available in different forms and
shapes uh one is data parallel c plus as
a native programming language but you do
need
uh associated libraries associated uh
debuggers profilers to profile the code
that is being written on all these
different hardware right so with this so
what we need to really do is we need to
have those uh have those bundled in a
way that is more developer friendly so
that is where
you you could see that i have
highlighted some of these in red that's
where especially for ai there are some
critical components that are packaged in
the intel's base toolkit so if you
google for intel's one api this is
probably the structure or the package
that you will get on google right
intel's one api is available in these
different forms i mean different tool
kits the base toolkit hpc toolkit for
the base toolkit is common for all these
different toolkits and what happens is
that it is the base for
really
doing this heterogeneous programming
right when when it comes to programming
all these different devices this forms
the base and then you have these domain
specific toolkits which is ai hpc the dl
framework which is for more for the
framework developers like tensorflow
pytards etc and a analytics toolkit for
people like you who are data scientists
for ai developers who can use the
optimizations that are there in the base
toolkit i will i will talk about more in
the next file and also i did spend
briefly uh when i spoke about the agenda
i also spoke about the open v toolkit
right so that is another toolkit for ai
inference right so what we will do is as
the day progresses you will get exposure
to
these different
you know toolkits and also you will get
hands-on for these different toolkits
right so that is the whole idea
so now if you look at what the intel's
base toolkit is all about right so again
i have uh i have probably
highlighted the ones which are really
which are really uh essential for ai in
in red right you can see that you know
these ones which are in red clearly
those are the ones which are which forms
the base for accelerating our ai
workloads intel distribution of python
i'm sure all of you uh python is
literally the the language uh you know
for ai developers right so we have our
own distribution of python where what we
have done is we will we have optimized
all these different packages like numpy
scipy psychic learn etc and then we have
packaged it into intel's distribution
for python and apart from these we also
have these libraries like math kernel
libraries the data analytics library the
collective communication library and the
deep neural network library so these are
different libraries that basically
accelerate your ai workloads on intel uh
intel hardware right so that is one of
the you know key messages that i want to
leave you with when you use these
libraries
that's where the concept of one api
comes in where you use these libraries
these libraries are already optimized
for intel's gpus
fpgas and you know cpus so you use these
libraries you already get the best
benefit if you feel that hey you know i
have to write the code natively you have
the choice of using data parallel c plus
plus and to the right you do see that
you know we have these
profiling tools which basically give you
insights into what's going on inside the
hardware right so this is in nutshell
the base toolkit and we will
we will be using these libraries and
in the ai toolkits and then we will get
the best out of the intel hardware
so now moving on to the ai analytics
toolkit
i'm sure some of you here have already
attended one of the previous workshops
that we had conducted so the previous
workshop focused on the intel's
optimizations for tensorflow and also we
touched upon we had hands-on labs for
the machine learning part using
scikit-learn and also the one the data
analytics library right so
now what we want to focus uh in this
particular session is about the intel's
extension for pytorch if you have a
pytorch
framework if you have a model that's
developed on pytorch how can you
accelerate how can you make it run
better on intel hardware is what we will
see and we will focus on
uh in this workshop in nutshell ai
analytics toolkit is all that ai
developer needs we have these different
frameworks which are optimized for intel
hardware which is the intel's
optimization for tensorflow pytorch and
also we have
already the standard models which are
already optimized on intel right that is
part of the model zoo and also we have a
low precision optimization tool which
converts your fp32 and two models to
into eight and so on so that you get
better performance maybe with a little
bit of trade-off on the accuracy right
so this is a deep learning part of it on
the machine learning we have a lot of
these libraries like you know packages
like scikit-learn xgboost optimized on
intel and
eventually some of these all use the
math kernel library that i just spoke
about in the base toolkit right and the
same with for data analytics we have the
intel distribution for model which
basically helps you speed up the
pre-processing for the data which again
using the omnisi backend and also a lot
of the core python packages have been
optimized to run based on intel whether
it's numpy sci-fi number pandas all of
these have been optimized to run based
on it
okay so with that uh uh let's just have
the first poll uh committee on team can
we just have the first poll on the
screen please
how many of you have access to dev cloud
so
so is the pole up kavita
yes ellen the pole is up
you can also see the responses in the
control panel
um
i can i can see the poll
sure okay okay
so i can see roughly around
60 percent of saying yes
yeah can we just uh have
everyone answer the poll do you have
debt cloud access or not please
okay we see a 60 40 mix now
okay so those who don't have access
please don't worry we will walk you
through the process in any way so that
you don't have to really worry about hey
no i don't have access what do i do
right so we will walk you through the
process so that you are registered and
you have access to devcloud and you can
do your hands-on session right so don't
worry
i'm closing the poll
sure no problem yeah
okay thank you so uh for some of you who
are not aware so intel's uh we
what is intel dev cloud it is just a
sandbox for you to play an experiment
with one api on your own workloads right
it will just help you try out uh your
code on the on the servers that we have
in the cloud environment it is it is
just a sandbox for you to clearly test
and uh evaluate the performance using
the one api tools right so uh
maybe uh kavita as as i just flashed the
foil if you can just put this link as
well in the uh
in the chat window that would be great
so please make sure that uh you know you
have this uh you click on this link and
then make sure that you put in the dev
cloud access code it is one e small and
api
all caps
one eight aug caps i repeat o n e is all
small
one api api is caps all if api is caps
one 8
18 august aug again caps
so if once you click on this
link and use this just keep this dev
cloud access code handy
because this will be needed as a you try
to register
and uh
okay so you should be in this when you
click on this link you should have this
enrollment form you click on register
and you would get this form
you just have to enter your personal
details your name email ids password you
give a password and then a country in a
phone number is optional
please make sure that
whatever email id you give here you have
immediate access to it because an email
would be sent to it immediately
for the confirmation
are there any questions either on
discord or the chat window kavita and
manda
are we good until here
i don't see any specific questions
good now okay so
for those uh
you can also follow these instructions
in the dev load handbook that is already
also it is available on discord as well
as
in the chat window please make sure you
can you can refer to that as well so
once you uh once you click on the next
button
you will
get this
option for about you know the details on
all the company information etc so and
then you submit
please make sure that after you submit
you check your email
and ensure that you have received the
email with the login id and the email
address make sure that you know you have
received this email
can uh can i have a quick show of hands
as to i will just pause here for a few
two minutes two to three minutes just to
make sure that all of you are with me
can i can i just have a quick show of
hands as to how many of you are here
until this stage
please use the show hand feature and you
can just let me know if you're there
with me
and once you have received this email
then you have to register for the uh
the dev cloud so this is where you have
to key in the
uh
the code that we just uh said it's only
one small api all gaps and 18 august
right so
when you click on the activate
you will
activate link you will get to this
particular
form
all that you have to do is enter the
you know
the country
the region etc and then use this code
one api 18th august and then submit
[Music]
um but there's no specific questions of
you know if they're stuck in the process
so you can um so everyone you can please
uh type in your questions on the discord
channel if you're stuck in any of the
steps we can help you uh you know get
access to deaf cloud
but uh to answer your questions ellen we
don't have any specific questions as
such
okay all right so uh in case you just
joined recently this is going through
the dev cloud sign up process uh we also
have the dev cloud setup guide in the
link in the chat window here and also we
have that in the discord as well so
please make sure that you use the
devcloud handbook and get access to
devcloud so that you can do and you can
get ready and
do your hands-on lab in the dev cloud
which is a sandbox environment for doing
all your testing and benchmarking
okay so
[Music]
so you will be presented this screen for
accepting the terms and conditions and
then click on submit
so then
you will
so from here probably i will just show
you
so kavita can you see my screen
yes yes
okay
so you have this anyway the link in the
uh
when you go to this link
devcloud.intel.com slash one api get
started
we can just put this also in the chat
window if they are struggling
it's on the discard right now so anyone
who has
access to discord can view this pdf
thanks
sure thank you
um
so
most of you should be able to get to
this page get started
and here you would have this option
called connect with jupiter lab
right so when you see this screen you
just click on the jupyter lab launch
jupyter lab
so you may not be presented with this
because i have two logins but you may
have just one login so you it will just
lock you into the dev cloud
you may not get this screen
so i'm just switching switching between
the foils and the
uh on the screen to just show you books
so you some of you may get if the server
is not running i think i may also see
this okay your server is starting up so
you may see that launch server if your
if your server is not running you just
click on launch server
and it will take you to this
particular screen
okay so can we just
check out if there are any issues under
on the discord or in the chat window are
people able to follow the instructions
any questions
yeah nothing specific we are answering
okay all right
so you have to connect with the jupiter
lab so if you are in this
webpage get started with the intel one
api dev cloud you will have to scroll
down to the bottom of the page and you
will see this connect with jupiter lab
click on the launch jupiter job and then
okay
it may take a
few minutes depending on the network
speed that you may have so just give it
some time and then you should be
you know in this in this kind of a
screen
so i will just stay here
just to make sure that all of you are
able to get to this point
and can i probably just have a show of
hands as to how many of them are able to
get to this point
so that you know we can get on to the
next part of the presentation
in the meantime if someone wants to just
follow the handbook and then get to the
dev cloud that would also be great
so for all those who were not able to
follow i just will repeat the process so
that you know you are with us
so you have the link that is posted in
the
chat window and discard you click on
register
you enter your personal details name
email
password create a password etc and then
you click on
submit here
you will get an email for activation
and then once you
verify your please verify your email
address by clicking this link when you
click on this link it will basically
take you to this
form all that you do is you enter the
code
and then click on submit right that will
take you to
the one api
and then click on submit it will take
you to the one api getting started page
scroll down to the bottom and then you
will have the launch jupiter lab once
you launch the jupiter lab you may get
if your server is not running you may
get this or
you may not get this
if your server is not running so and
then click on launch server
and then you should be at the screen so
that is where i am currently
please give it a few minutes uh
depending on your network speed etc it
may take a few minutes
extra
so i also just want to kind of introduce
you to the jupiter notebook sometimes
you know
you may not you may be new to jupiter
notebooks so what i will just do is
i'll just do a quick uh you know show of
one jupiter notebook and then say hey
you know how you're going to run your
lab right so i just picked this resnet50
training dot ipy nv this is the
extensions for a jupyter notebook so all
that i do is i double click this
uh don't worry our team will
help you
get to this point where you zip unzip
the required files and folders etc so
i'm just picking a sample jupiter
notebook and then telling you how to run
right
so if you're new to jupiter notebook
all that you do is there are these
multiple boxes right these gray boxes
you will see there are several of them
right so all that you do is you just go
inside the box and then either hit this
play button or the run button or yes you
just do a control enter right
when you do a control enter
it will when the activity is still
happening you will see a star and once
it is complete you will see the number
appearing back again so a star is an
indication that it is still executing
right so you have to wait
until
the first box completes and then you
move to the next box right so there are
essentially these are just you know a
sequence of things that needs to be done
for your lab sessions so just make sure
that you don't miss any of these boxes
go one after the other do a control
enter inside the box you can see a star
here
right and so that you know that it's
running
especially when you're doing the
training etcetera it may go on for a
while right may take two three minutes
or four minutes depending on your data
set and workflow
so you will see a star appear and then
you can just move on right so you just
have to repeat the same step for
all the boxes that are there in the
jupyter notebook that our team will
guide you
okay um
any questions
i just showed you them
there on the kernel right now in the lab
section so do they wait for the hands-on
session or they can just select the
python kernel
for now no i don't think they might have
uh they might have uh you know this
folder so i am inside the python
workshop folder so if they want what
they can do is further they can just uh
you know follow the guide i think our
folks will guide them into the you know
copying of the model etc right the
exact folder so for now we will just
stop here and then we can get into the
theory part
uh
if they are here until this point where
they are at the terminal or this screen
right the jupiter
screen if they are at this screen i
think we are good i just wanted to show
them how to run a jupyter notebook as a
just as a demo uh so that because if
they are unfamiliar with jupiter
[Applause]
and then i think the folks who are
asking this question in discord maybe
they have to wait until the hands-on
session so hold on to that page
i would recommend that to the team who's
on the kernel page
so hold on till there until monsanto
starts
thanks yes yes yes so as long as you see
the screen that i am i have here i think
we are good if somebody is not here
let me let me let me know we will just
you know guide them through the process
anyone having trouble please post your
questions on discord we will make sure
that we address your question
so aditya and jing you will basically
guide them through the copying process
right from the github
yeah okay yeah
yes and i'll cover in my
okay thanks
thanks okay uh kavitha then i think uh i
am pretty much done i think uh i can
hand it over to jane for the theory part
if there are no other questions on the
setup anyone stuck with issues etc i can
just hand it over to jing
yes ellen i think we will we will be on
discord to help someone if you know
someone is stuck on on deaf cloud we can
help those guys
we can pass them
okay thank you
i will just stop my screen share julie
please uh go ahead and share your screen
yeah okay
yeah can you see my screen
we can see
okay okay
so
um yeah from
from this slide i will
uh introduce
uh briefly about the intel
i hardware features and also
how this
hardware feature features
uh effect
the usage pattern of
the
python
to achieve better performance
um
so
this is a
graph
shows how intel
cpu architecture looks like
um
basically for xeon servers we have
two sockets
two sockets on motherboard so each
socket you can consider to be
one
cpu chip
and each of so here the right part is
one socket of
the graph of one socket so each
cpu chip
it has
several cores and also it has
on-board cache
and also for
cpu and cpu communication
we have
intel upi this is interface
so this is a bus which
takes
account of the
communication data communication between
between these two cpu chips
um and also
each cpu has its own
memory
uh so this i will introduce later so
basically
for the intel xeon architecture on one
motherboard we have two cpu chips
and each cpu chip is communicated is
attached to its own
memory chat memory slot
and cpu communicates with another one
with its counterpart
via intel upi
okay so we call this kind of
architecture to be the non-uniform
memory access shot for numero numa
neumann
for traditional architectures
the processor is attached directly to a
bus
and all memories are attached also
attached directly to the bus so you can
see that
on
one bus we have several
processors so as the cpu
processors that we say
uh and also the memories all of the
memories are attached to this box
so this this architecture called humor
it's the uniform memory access the
advantage the disadvantage of this uh
architecture is that the bus will be
very you know high highly occupied for
data trans
transmission
um
so this could be a block bottleneck for
the performance to solve this problem
the newer architecture is introduced
so
still we have this boss
but
all the memory the memories are split
into
several parts so each part is attached
to one
one processor
so we call this memory
as a local memory to this
uh cpu
uh so in numa architecture we have local
memories for all of each
processor
uh it is not
uh that the the pro this let's say this
processor can directly access its
attached memory
and also it can access
other memories
via the bus so it is not not say that if
we attach
memory to one processor the process
cannot
access other memories they can but
it will go through the bus
for local memory it is a direct access
so this uh so we recall if if this
processor access its local memory then
we call this as a local memory access if
this processor access other memories
then we call this as a remote memory
access
so obvious obviously that the local
memory access is much faster than the
remote memory access
for using
uh for the you used scenarios on intel
xeon servers we need to avoid
remote memory access as much as possible
also a data type is
a second biggest
component of the
hardware architecture
so
for
fp32 it is simplified for float 32
it is
stored as
uh like in this format so it has
one bit for the sign signature
for sine and then eight so if this is
set to zero then uh it shows this number
is a positive number if this
bit is set to one then it is a negative
number
and then uh followed by 8-bit
exponential part this shows for the
the
exponential part
and for fp32 the rest of the bits rest
of 33 bits
23 bits
uh uh mantissa mentioned bits so this
shows for the decimal part
uh for b float 16 to
because we need to consider the
compatibility to fp32
we keep the same for
sign bit
the same bit wise for the exponential
part
and we use only seven bits for the
medicine part
so
um for the flow 16 data set
for flow 16 data type
they use 5-bit for the
exponential part and 10-bit medicine for
the minister part so fp16 could have
a higher relative higher
precision compared to the
b float 16 but
uh the compatibility from compatibility
perspective b flow 16 is much
uh better than the fp32
fp16
if we take the float 32 as the base
and for end quantization uh for ins
eight it is one sign bed uh with seven
beds for the medicine so each one
uh so so for instance there are eight
bits
the fp32 is
usually the default training and the
inference numerical precision
this is fp32 generally is the base data
type
b float 16 shown
here
it provides virtually the same accuracy
for training and influence as fp32
it's simulated on various
workloads and achieves virtually the
same accuracy
and there's no hyperparameter change
required
for fp32
workloads compared to 2b float16 so
there are
some
specific
hyper parameters required to use fp30
fp16
for training
and int 8 shows
shown here
it provides similar accuracy of
inference
as fut32 for various of models
um
vni is
vani instructions is a
instruction set
which is designed specifically for deep
learning
deep learning workloads on xeon intel
xeon platforms
so it supports int8 and efrod16 dot
product operator
on intel xeon the second generation of
intel xeon scalable processor which is
the pro product code is cascade lake
the
vni instruction set
was introduced as part of avx 512
instruction set
uh so the cascade lake has four
uh channels for ins eight uh peak
computation uh compared to the float
32
and
in xeon the third generation of intel
xeon scalable processor which
uh called
cycle rapid or cooperate uh the b float
16 instruction
sub instruction set
of avx 500 cloud
is introduced was introduced
so it has two
uh channels for before 16 computation
versus the float32
uh the right side is a uh
illustration of
of the b float 16
uh dot product instruct the this new
instruction
uh for the b flow 16
and
the
vp dpv usd uh this is the new
instruction specifically for the int h
dot product
so originally we need to use several uh
instructions to do the b flow 16 and 8
but with this two we by taking advantage
of this
two instructions uh we can just use one
single instruction to do the computation
so we can achieve a
performance boost
one dnn is
a
library that
was designed specifically to accelerate
the performance of
most popular
most widely used deep learning
algorithms or operators
so 1dm provides the cross-platform
performance
boost
on intel xeon servers on
intel
graphics cards gpus
um so one dnn support
uh convolution deconvolution the inner
product
rn
like the vanilla iron and lstm cell the
giu
and also it accelerate the
gem
the matrix multiplication
so the for for the memory band with
limited operations it
supports the pruding operation it's for
the badge norm
some kinds of normalizations
and also it
supports the element-wise
operations binary
and twice operations like the relu
um the activation functions
uh also for the soft mag some concave
shuffle it also uh all these operators
are supported in 1dn
[Music]
because we have so intel
architecture has its favorite data type
the
the memory layout for storing the data
the
nchwhwc
used for
uh used in tensorflow and pyrotorch
might not is not always the
the best data layout for intel
architecture so in one dnm we have some
uh
best tuned
data layout which
has best performance on you know
very good performance online
architecture so one dna has the
functionality to reorder
to reorder the
data from the external
hwc or nchw
into its internal data layout format
so it will handle the order conversion
from and to
external and internal data layouts
automatically
um
then
let me i'll cover i would introduce
intel optimization for pytorch
so
what is intel optimization for pytorch
it is
a mixture of it is a combination of
stock pipe torch
and intel extension for pi torch we call
we we call this inter extension for
pythog
ipax short for ipax
us
optimization for python intel regularly
upstream our
optimizations or let's say most of the
cpu optimizations to stockpile torch so
that
users can have directly have better
performance best performance with the
high touch that you installed in normal
uh in your normal way like
via condor or installed in pip
but
there are some specific optimizations or
further optimizations which is not has
not been merged in stockpile touch so we
provided
those kind of optimizations
in
intel extension for pytorch
which is called ipex
so basically speaking uh intel
optimization for pytorch equals
stock pi touch which includes
uh upstream cpu optimizations plus intel
extension for pytorch ipex
so both stock and
ipad stockpile torch and ipacs are
combined into one package called intel
optimization for pytorch
and we made we made it available via
multiple distribution channels through
intel one api ai analytics toolkit
so if you install air analytics toolkit
of one api then you will
have both of those two binaries by
default
alternate alternative player you can we
also release the intel extension for pi
torch as a real file as real files so
you can install
this separately rather than use a
text toolkit so if you install this ipx
real file you will also get
ipads and the stock python installed
automatically
so pytorch is a open source
machine learning library based on torch
library
which is used for applications such as
computer vision or
natural language processing
it is primarily developed by facebook
and it is free and open source software
under modified vst license
although the python interface is more
polished
uh you can also use the c plus plus
interface
for the deployment
so
similarly
we also provide pi title python
interface and c plus plus interface for
ipex intel extension for python
um here we will show a simple
uh
usage
of pytorch
so how pythora's code looks like
normally we will
first need to import torch and then
import torch.nn as nn
then we need to define
a model it will be heritage
from
from the air dot module
and inside this definition we will
define
uh an iit-inet function
and we define the some operators here
like we will use convolution comp2d
and we'll use softmax
then we need to define a forward
function forward function is the main
function
during the
training and the inference
so in forward function we put we we
put the
input this data
parameter this is the input data
uh we put that into the com2d operator
and then get its output then we
put its output into the softmax
for the
for the computation and then we would
return the output of softmax as the
output of of this model
so when we use we do the inference we
need to first
get the input data and then we
instant size
this model class
and then we put this input data into
this model and then gets the final
result
so this is the
default usage pattern of pi touch
cpu optimizations are
upstream to stock python so you can
enjoy the
vectorized kernel
for intel avx2 or ebx 512 instructions
then
and also the 1dn had been
integrated into pi touch so you can also
even you use the stock high touch you
can
get acceleration by 1dm
uh the support of
hwc the
vpfload16.8 has
all been merged into stockpile touch so
you can also
use these
data types and memory layout
in stock python
so intel extension for pi python
short for ipax is a buffer
of prs for stockpile torch
it provides users with up-to-date intel
software and hardware features
[Music]
so and also this ipax will unify the
user experience on intel cpu and gpu
uh currently there are two separate
branches but uh
later in the future we will merge them
together so
for users of ipax
you can
get performance boost
on both intel xeon cpus and
intel gpus
so we have several
optimization methodologies in ipads the
the the topology
looks like this so the core part is pi
torch we use pytorch as the base and
then applied several optimization
methods like
optimizer operator optimization
or mixed precision
use this
methodologies to
implement the entire extension for
pytorch
so in ipax we have the operator
optimization which involves the
some customized operators
and also
it can do the automatic graph
optimization
so maybe you have known that the
stock python at
the default
scenario usage scenario is imperative
mode
which means
one
when you execute one operator you don't
know the next operator
like when you run convolution you don't
know the next co the next operator is
whether it is a batch norm or it is a
radu
so
from performance perspective we have to
optimize individual
operators to achieve better performance
but
we don't we we cannot have higher level
uh optimization methodologies
uh so the graph mode of pi touch is
uh
it provides the functionality to
let's say scan the entire topology first
so we can know
okay the convolution is followed by
batch norm and then followed by radu
so we can have a high you a bird view
of the entire topology
uh with the uh with understanding
the entire topology then we can apply
further optimization methodologies like
we can do operator fusions
because you know
for deep learning
operations
there are several patterns
which are most widely used like
convolution followed by batch norm or
convolution followed by radu
or convolution followed by batch norm
and then relu
so
most widely these patterns are used
so we can consider to fuse this
operators like two operators or three
operators into one into a single one
to reduce the overhead time
between the
operators
so with this methodology we can further
we can you know
have
further performance boost uh based on
the single operator uh optima
optimizations
um for the
mixed precision part we accelerate the
pythagoras operators by low precision
data types like before 16 or into 8
and
we can we also simplified the data type
conversion because you know for from
converting from the flow 32 to b float
16 or into eight uh takes extra effort
so with ipacs we can
do this automatically so simplify this
daily data type conversion
uh
furthermore we have the
optimizer so here the optimizer is not
from the performance perspective it is
for training like adam
sgd
so we also optimize this training
optimizers in ipads like we introduced
split optimizer and also we
have some fused optimizers
inside ipex
um we provide easy of use ipax user
interface api
for python
interface
so what you have to do is just
first import intel python extension this
python module to register ipax
optimizations
for operator level and graph level into
python
if you need to
use
b flow 16 then you have you need to
call this ipac start
enable
auto mixed precision function and set
the mixed d type to touch.bloat16
with this single
api function invocation
the ipex will
do the automatic
mixed precision
computation with before 16.
so the automatic data layout conversion
will be
enabled by default
if you don't use bitflow 16 or if you
use before 16 and you use float 32
you can
what you need to do is just invoke the
two xpu function
on both the modules and the input
tensors
to
run
to run the code in with ipax
optimization
all the ipex optimizations are
implemented on xpu rather than the
python vanilla cpu
so here is the diagram that shows how
ipax xp integration
integrated into the architecture
um
so
the top is the python level
operators
so pythorg provides the touchscript to
do the judge graph
and also it provides the a10
operators 18 library for
deep learning operation operators
and also there are some device
specific runtimes
defined in pytorch so
ipacs use this interface provided by
pythog to
provide further optimization for the
graph integration
uh for the you know the the kernel level
optimization
with 1dn or 1cl
and also
we provide xperia device we
register this executed device into the
device runtime package device runtime so
that
the optimizations will be applied
for
for gpu we use zero device runtime
to
maximize the performance on intel
gpu
hardware so this architecture works for
cpu gpu and other hardware accelerators
from the software user perspective
the code change to the original
python code was is very simple
first you need to import entire
extension for python
intel python extension and then again uh
just to define the module as you have to
do for the pi torch
then
after you initially
get the input data just call dot 2 xpu
function
on the data and also apply this to xpu
function on the module
so if
these two steps are taken are done
then
at the final line model input this
influence we are running into
into our optimization property
where you use all the entire
optimization methodologies
from
graph
optimization perspective
as i mentioned that we can do operator
fusions to reduce
overhead for memory operations
furthermore
in ipads we can
we have the batch normalization folding
so which means if you run convolution 2d
or you run convolution
plus batch norm
then the batch norm will be merged into
convolution so it will uh
reduce the operat operation execution
time
uh and and further if you use this
convolution plus bias plus add or or
plus relu
or you use convolution plus relu or or
lu this kind of activation function or
linear plus value
all these kind of patterns will be
merged into
will be fused into a single operator
which brings performance boost
furthermore
by by reducing the overhead
um
for other
operators there are
users for if you use python you are
always
able to create your own
operator rather than just use the atm
the patos 18 operators
so for some most widely used topologies
like most gaussian or ssd resonant 34 or
drm
they are customized operators
so
we have also implemented these operators
in ipax
so if you run this you can directly
replace the original
customized operator with the optimized
one in ipex
for the
training training part
if we train with because you know the
beefalo 16 has lower
precision so it performs it its
performance is accelerated compared to
fp32
but the
um the problem that we have with before
16 is that compared to fp32 the accuracy
is much is lower so because it has uh
less bits for the mantissa part of the
data flow data so the accuracy is
relatively lower than fp32
um
for training part we
the the for training a model the biggest
problem the the most thing the
biggest thing that we
focus on is the accuracy of training you
know uh compared to performance training
accuracy is always the first priority
uh so to overcome this low precision
uh
issue brought by before the 16
we still keep
the
floats 32
weights in memory
so let's say uh if you run
training with b flow 16 like this uh the
input is before 16 the weight
is before 16 the bios is before 16. so
if you do
the forward pass
for the inference or the forward pass
then this is okay
but if you need to do the
backward propagation later uh in
training part uh
the
stadium the the stochastic gradient uh
this sgd
operator the optimizer will still use
one copy of fp32
so fp32 copy of the weights original
weights
let's say
to keep the to make sure that there will
be no
accuracy loss
brought by the fp32 so this is called
the
split sgd
this is one optimization methodology
that we used uh in apex to run with fp32
fp16
to make sure that we
keep the accuracy of the training but we
still enjoy the performance boost with
the low precision data type
okay now we have
we are coming to the pool
um
so
so could you yeah could you show the
pause screen that
we can ask
oh okay
uh hi actually are you still seeing my
presentation window or
all the power window
okay ginseng i can see the pole
yeah the pole is up here
so the
question is
which
components are included in the intel
optimization for python so we have abc d
for
choices
so let's see
how how we should choose from this form
uh using you can also
see the responses of the poll uh in the
control panel
if you would like foreign
[Music]
so um jensen i think 15 percent have
opted in for intel extension for python
and 85 percent
in z vote yeah
yeah i see uh 58 has been loaded
so
okay
yeah let's wait for a couple of minutes
okay so yeah i i see that 72 percent uh
has voted uh and uh among which the uh
eighty percent uh
show
chose the both of above
this is the correct answer so as i
introduced in the previous slides the
intel optimization for python actually
contains the stockpile touch because we
are always upstreaming our intel
optimizations to
stockpile torch
and
the rest of the optimizations which is
pending on the you know the merge
pr merge or
some specific optimizations are included
implemented in intel extension for
python
so
the answer of this question is c it can
it includes both
both two
components
okay
okay so
let's continue
uh you all see my
screen
or the poll window
we see the screen
okay okay
okay so uh yeah i will show some case
studies uh of the usage of um ipacs uh
intel optimization for
high torch to show
its performance boost
so here is the result of one training
example with bfloat 16.
we conducted this training
on the third generation of xeon
scalable processor
so we used three topologies
one is drim facebook drm and another is
the bird large
the third is the resnex101
with 32 by 4d topology
so we run
one
instance for training so here the second
column we run one instance for training
and each instance uses 28 cores
we compare the performance
with uh between fp32 and the b float 16
and we can see that
the performance boost that brought by b
float 16 is around 101.55
for drrm
1.81 for bird large and
over 2
2.4 times better performance
for the cv
computer version task
and also meanwhile we tested
the
beautiful 16 with the inference workload
again the topology are the three
so we run it with 224 instances and each
instance will use only one core
um
so we run it in a throughput mode so we
at this simultaneously we run
224 instances
and
uh and we can see that as the result the
b float 16
achieved
uh 1.4
times the performance
compared to fp32 uh yeah similar number
for the large and again uh over four
times better performance
uh for the image
uh field red next
topology
so if you are interested on this uh
numbers more detailed information are
provided in this blog so you can open
this and learn more from this article
um for the int 8
topology in eight data type we conducted
another
work load with prrm so we run one
instance of the drm and use 28 instances
we
sorry we run 28 instances and each
instance use only one core for the
inference
uh this is the number that we got for
ins eight and this is the number that we
got for
uh with fp32 stereotype
uh comparing comparing these two
we can see that we achieved 2.85
times better performance with
in quantization
with into eight
also this
table more detailed information of this
experiment can be found in this article
okay
um
there are
other two
case studies
one is
a
topology called um
the you used fast speech and multiband
uh male gain
topologies which are
used which are
provided in espnet
we used no more than three quarters we
we used actually four cores
uh for one instance
on this workload
originally we didn't get performance
idea or performance so we
used pytorch
the piotog
or pathogens profiler default profiler
and
this is the table that we've shown
we see from the from the provider so we
can see that the first the the this uh
operators are sorted uh based on its uh
cpu its execution time uh in descent
order
so the top one is take took the most
longest time but since uh it is
convolution this has already been
optimized by intel
uh with with intel optimizations
so we focus on the second
the top two
operator and we see that
it took
around 12 percent of time
for the duration for the computation uh
compared to the third one it's uh
twice twice long
so we did some investment investigation
on the on-screens this is the sunscreens
operator and then
did some applied some further
optimizations
after
after this analysis and the
corresponding
optimization we achieved performance
boost
with our optimizations
uh and another workload is a uh ocr
model so
so this model uh
this workload
converts the image find the text strings
inside the
inside image
with with pytorch
we
tried because by default the python use
gnu openmp as its openmp backend
but at the same time intel has intel pmp
library
sometimes intel opmp library could
achieve
performance boost
and also
there are
third-party
memory
related like malloc related libraries
like je maillock or tc matlock
with these two
metallog libraries
they optimized for the cache the memory
cache usage so
from memory usage efficiency perspective
the ge method and tc methox sometimes
has better performance
it could result resulting some
better performance
to use these library third-party
libraries are very simple we just need
to
use the ld preload this environment
variable to load them
and then everything could work
uh also
je maloc and iomp
had some run you know their specific
environment variables like camp infinite
kmp setting
camp infinity they are metal config they
can be block time
so without using them this libraries
with the default python libraries
we can see that less than 90 of the cpu
time was used
and also we see this long red part so
this indicates that there are some
low efficiency
operations
were running inside the linux kernel so
after the code after the
we applying this
libraries we're running it with python
we can see that the cpu usage raised
to
rise to
around 100 percent and also we reduce
this red part we don't see the red part
in this in this running so which means
that the execution efficiency is
much higher than than before
so with these two
methodologies we also
we again achieved performance boost
compared to the uh previous runs
so
to summarize uh we can the the stock
python profiler helps us a lot to
do the you know to understand where is
the hotspot of our codes and
uh so we can take
actions to to reduce these overheads
and also if we apply
other libraries like iomp we switch the
gnu openmp to iomp
or we use we apply the jmel octc matlock
we can
also it is also possible to achieve
performance boost compared to the
original one
so here
is
the
a flow that
we have we provided uh in ai analytics
toolkit so in a analytics toolkit we not
only have the intel optimizations for
python change optimization for
tensorflow
we also have openvino
toolkit which was designed specifically
to
accelerate the inference part of deep
learning
also the traditional machine learning
the classical machine learning tasks are
also included in uh supported in the ai
nanotech toolkit
so
uh depends on your workload how you
would like to run your workload
there are several
usage models that you can consider
so first if your workload is about the
data
analytics like data pre-processing
so you can use the
data analytics related softwares
in
ai analytics toolkit like pandas
the intel optimized
optimizations for pandas numpy skype
if you would like to accelerate your
classical machine learning workloads
then you can use the intel optimizations
for sky kit learn for xg boost
and also
dial the inter-specific
machine learning acceleration library
provided in the ai kit
for the deepening part
if you would like to
implement a
framework like tensorflow or python
let's say if you are not that satisfied
with tensorflow or python you would like
to write your own framework then you can
consider to use
1dn or 1ccl
as the
acceleration library
if you would like to do the training or
you know
yeah if you would like to do training
with
your model
then you can use the intel optimization
for tensorflow and the intel
optimization for pytorch in ai kit
yeah it's the same for the fine tune if
you want to retrain or fine-tune a model
with customized data then you can also
choose tensorflow and python in airkit
if you would like to do inference only
if you if you would like to do inference
or if you would like to
use a pre-trained model
option that is optimized by intel then
you can use the ai
kit ar moto zoo
and see if
the topology the model can be converted
to
to
open vino format or not if the model
cannot be converted to openvino then you
still need to use
framework tensorflow or pytorch for
inference
if the model can be converted to
openvino and the performance shows
better then you can
choose to you know do the conversion
first to open vino format and then
use openvino inference engine to round
on
dedicated devices
so you can choose which device to run or
i mean to run on cpu or you can choose
to run on gpu
or other acceleration libraries
okay uh here comes to
another
poll
um
yeah let's switch to the poll window
okay the question is
which
data type is
supported in
uh ipacs in inter optimization for
python
uh
um yeah i see around 60
has voted
okay so uh
the majority of
uh the votes are
on
d uh all of the above
this is the correct answer
because yeah in ipads in intel
in intel
optimizations for pytorch
we
include
we include all of this uh data layout
for ins
uh
the you know the uh for indeed for float
32 for bfloat 16
yeah
um all of them all these data types are
included as ported in
intel
optimization for python
uh okay so
yeah so let's move to the hands-on lab
um i'll change the
screen to add it here
uh if we want to take a five-minute
break we will probably start in five
minutes uh if people want to take a
five-minute break we can just
give them a five-minute break yeah let's
take yeah let's take a break
yeah we will uh so we are at 11 13 india
time 1 3 11 1 3 india standard time
indian standard time we will start
exactly at 11 20 2 0 11 2 0 ist indian
standard time we have six more minutes
to go
we'll take a short break kavithas are
okay
yes olivia good thanks
okay
meantime if any of you have any
questions that need to be addressed or
any doubts you have you could just post
it on discord uh we can help you with
the answers for those questions
yes and for those of you who still want
the dev cloud access or the
the lab file it's pinned on the discard
channel so go to the print section and
download these files
messaging there is one question on
discord related to the batch training
optimizations and overfitting problem
so i see that some of them are already
you know catching up with the labs so
thank you so much for your active
participation
we'll just give a
few minutes for the rest of the folks to
catch up
can you hear me
yeah
yeah i think yeah
okay maybe jing is on a break okay let
him come back
um
okay
okay
okay
so other day you can just get ready you
can start sharing your screen in the
meantime let let's just join
sure i think let's just have a quick
poll and see how many of them have
actually logged into a dev cloud that's
a good point uh
yeah
can we have a quick poll just to check
how many of them have logged into
devcloud and they are able to start on
the exercises
can we just have a poll
before we start we just want to make
sure you know it is good to check
do you think we can repeat the poll
number one
yes correct one second
sure
[Music]
are you back
okay so there was one question on
discord related to uh
just
related to batch training how it
optimizes the overfitting problem
um
could you put that in the chat window
yeah i'm just
yeah
okay
okay we can see your screen aditya now
uh yeah zing i have put the uh question
how come batch training optimizes
overfitting problem can you please
explain i'm not able to follow the same
with respect to sgm
oh okay okay so yeah uh we're
our optimization of focusing on the
performance
uh performance optimization so not
uh
not for the uh
the not for solving the overfilling or
those problems
uh but uh of course from the accuracy
perspective we will
make sure that
uh
while we achieving better performance we
were still
keeping the accuracy
so
does this answer the question
okay let's just check
uh uh should i relaunch the poll kavita
yeah just give a second
um erin i think he wants this to be
repeated
jingsan could you repeat the
answer please
yeah yeah yeah
yeah so
our optimization focus on the
performance perspective so
we will not directly
uh touch the accuracy part so our target
is to uh we will make sure that
while we achieve performance boost
with intel optimizations
we will still keep the accuracy training
accuracy loss
okay thanks uh thanks ring i think uh
the focus is more on
the performance than the accuracy that's
what uh jinx response is i don't know
did that answer your question
um
okay all right can we have the poll on
screen please
okay we can see that at 85 percent of
europe yes can we have the rest of them
also vote please we see that 50 percent
almost 55 percent have voted if
we can have the rest of them also just
confirm that would be great
i hope all of you are able to
uh sign into and register to develop
if you could just let us know that would
be great
yeah and if if any you're facing any
issue you can post on discord channel
okay so yeah 62 percent have voted and
85 almost 85 percent are saying yes yeah
okay okay
almost 50 percent okay
yeah we just started
okay
okay okay so go ahead
okay
so uh
if you have logged in and uh i think
most of you
will be seeing this screen after login
so uh
from this uh
pdf uh
of the instructions
uh you can open this and uh
and follow
and copy the instructions from here
and on dev cloud first of all like you
need to
launch the terminal from here so in the
top left
this new launcher
you will see all these uh kernels and
these tabs so here you can open the
terminal by clicking on it
and
on the terminal we we need to like
follow the instructions from this pdf
and i'll be i'll be also
going through all these instructions
so
first like we are we are downloading all
the
uh dataset and notebooks
by cloning
the repository
uh
okay it's taking a little okay
so
so
now uh like we have this pythage
workshop folder in
in our home home directory
so
we can check
so now we have cloned our
the folder
which is having all the data set and
notebook
uh now we can just uh
you can talk check it with like ls
so you can see
pytorch workshop somewhere
okay so you can see this this folder
created by touch workshop
and
and
you can
now we need to
go into this folder by
just cd by dodge workshop
and we
will be able to see these two zip files
listener 50 inference and uh
listener 50 training dot zip
and now we need to
unzip both of these files
so you can follow this
unzip
command
[Music]
and similarly for
inference also
we can
unzip
now we should see
uh
these two extra holders
unzipped
and uh
okay so
after this uh
from the left
we can open our pythons workshop folder
and then
now we are we are going to start with
python
resonate 50 training
one so let's open this one
and the training notebook
just double click on the notebook you'll
be able to
open that
[Music]
so here you should be able to
see by dodge kernel python 1.4.0
uh
actually it's a it's a newer version but
the name of the kernel is
little different it's not correct
but but
we are using the latest version of
python
okay
so if you are not seeing this this
kernel you can
click on this and
you can just click on this and then
scroll down and select this one
python
1.4.0 yeah you get
and
for the benefit of others maybe what we
could do is uh yeah
maybe you can just repeat the steps what
you showed until now uh just to make
sure that people are with us
we will also just check the discord if
they stuck anywhere we will let them
know
you can just repeat the steps that you
have done till now on the terminal as
well as how you open the notebook
so that people are able to follow
yeah i'm sure sure
so
for for launching the terminal just need
to click on this plus plus button
here it will launch the
this page and then you can click on the
terminal
to
to open the terminal for you
so here here you need to like copy all
the steps from this pdf i think
this pdf is has already been shared on
discord or
or did we share on
[Music]
here also in the chat
um
looks like we can't insert a file on on
the chat window here or go to webinar
okay okay
is there a direct link from gitlab
github if we have a direct link we can
also put that in the chat window
to the notes option
i can add like a pdf file
as a handout
okay
yeah this is a pdf question
let me send this
quickly sure
okay once once you have launched the
terminal
you need to just follow these
set up instructions on one by one
so the blue highlighted ones are the
the scripts we need to copy and paste in
the terminal
so i have like
clone the
depository and then
unzipped all these both of these folders
and uh
yeah i'm here like i opened the training
training notebook
so
let's wait for one more minute and i'll
and if you have any questions you can
post
scott yeah is everyone caught up on the
with the process
sorrow are you with us are you able to
follow
okay all right
yes
download the file
okay even on the go to meeting right now
yeah
okay thanks
okay
so
so once you have like opened uh
these notebooks we don't need uh the
terminal now
we can just uh
run through these notebooks
so this one that's not 50 training
notebook is basically for
cleaning our uh
rest net 50 model
uh
so what
we are doing is we are not doing full
training here
of the
resnet 50
we are just doing the
last denser
layers part training
because if we do the full training it
will take so much time
so for the training part we are just
doing a last few layer training
uh
and this this used case is basically
classification of
our image into
two classes one is the alien and
other is predator
so
you can see these left two are alien
images and uh
right
to our predator images
so these are the images we have taken
from the data set itself
and this data set and the baseline of
this notebook is present on this
link you can check more there
so
in this data set we have like
347 images training images for alien
category and 347
for predator category
and 100 images for
each of these
classes
for validation
uh so this was about data set
and uh
and let's let's go
go through all of these cells
and run them so here like we are
importing all the like record libraries
common libraries
modules
so
let's import them
so numpy will be needing and by
matplotlib and time and
pl
so we have done this and then by pythog
specific uh modules we are
importing so torch and then from touch
vision we will be like importing the
resnet 50 model detailed model so yeah
this one models data sets and transforms
for data manipulation and
our data creation
so
and then we are importing neural network
uh
one from torch and then
other
its
neural network
functions and then this opt-in for
optimization or
updating the weights this will be used
for optimization of the weights
and this one is our
the module which we needed for ipex
so
with with the use of this
we will be able to uh
use our ipex optimization
further for normal parties python
so so here like in the normal python
model or notebook
we just need to add this
import line
so i think i have
done this
and uh here we are like generating the
test data set and uh
transforming it
so so we are like this so since the
images are of like different size and
dimensions
uh we are we need to do this
we need to resize this to 224 by 24 and
then converted to tensor and then
normalize
so we are doing this for both training
and
validation
so this is just
[Music]
for training we'll be doing this this uh
transformation and for validation we'll
be doing this one just transform
uh here both are same but still like
we can change it according to our needs
so this was the date like uh defining
our data transform
and uh
here we will give the path to our
training data set and uh
validation data set and apply the
transform function
to all the images we have
and then in the data loader we
will get the
our data set into uh
into our tensor
and we which we can use later so
it will give
uh iterated kind of
thing which we can
take one by one
so here here we are taking a patch size
of 64. we can change it according to our
needs
but here we have we picked 64.
so
i think let's run this
and uh in this one we are creating our
network
uh neural network and here so
from models we are importing
a snake50 model pretend one
and here what we are doing is basically
we are
we are making
um
gradient uh off for all the
parameters
because we we are not going to train the
full full network
we will just train the
this fully connected layers in the end
so we are defining that
since we have two classes
we are just giving two as the output
so let's run this one also till it will
download the pre-trained model so it may
take
a little bit more time
so till now like uh we didn't need
to do anything
apart from this import for ipex
so it's
downloading the model
so when when the notebook is running you
can see star mark between these two
square brackets
instead of this number
so i can see it's completed now
and
now we are like defining the
loss function
which we like backtrack
using which will backtrack our
our error functions to change the
weights of the
network so we are defining this this uh
cross entropy loss function
and uh uh
for optimizing and uh changing the
weights we are using this atom
optimizer
so this also we can we are taking from
optim
which we imported from torch
and and one thing to note here is uh we
are only
going to change the
weights for this fully connected
parameters module
so you can see the model dot fc
parameters
so
so here we we defined this
model.fc according to our data set
okay so
we have defined the
loss function and optimizer
now here
we will see like
which device we are on so cpu
we can select the device as cpu
using this storage.device
so
this one
and now uh here's the training part
so
here in this function we are taking the
model
model from here or the main model
resonant pretend model
and then this loss function then
optimizer and the number of epochs and
so this is the difference here
with this ipex
ipex
parameter what we are doing is
so we can change it according to like
what we want to use if we want to use
the ipex then we can
make it true
in our usage of this function
so these are all the parameters
uh
and uh here we like defining the time
and uh
because we will check like how much time
did it take
for for this training
so
so this is the content of the function
what we are doing is we are
uh taking each epoch and then
um
if it's training then we will like
activate the training mode of model if
it's a validation we'll activate the
evaluation
so it's in the so
initially for each epoch we'll
put the weight as zero
and
sorry loss as zero
and now we
will take the important labels for
from our data loader
and uh
for let's say this phase is training or
validation so for training phase
uh
it will it will
take the inputs from our training data
set or
if it's validation it will take the
inputs and labels from
validation data data set
so here uh this is the main thing here
we are going to
change
so if ipex is activated like if this is
true
there is a question
uh basically about should we use a
separate model for training using ipex
because uh
uh i think this is asking this question
we are already training the weights
using the default method and we are
reusing the model object with the
trained weights right so he's asking
should we use a separate model for
training using ipex
right so
yeah like
yes you can do
all these uh changes and use different
models later
yeah we uh so yeah i think probably the
answer is maybe
feel free to try it out and you know you
should be able to get similar results
right so that's what our estimate is
yeah feel free to do that and maybe you
know if you still are stuck we are there
to help
okay cool thanks
so this is the like uh main
regarding the ipex
so
since we have all these data set and
import
input data set and validation
in our cpu device uh we need to like uh
if ipx is true we need to
like move it to
this
ipex device using this this function dot
2 ipex dot device
so this is the
thing which which will be done when
when ipex is true
and uh and if if ipex is not true then
and then it's already in the device
it's uh else part is just to show like
that
it's in the cpu device already
so
so once we have model and input and all
the data files in
data onset in ipex device we it will use
our ipex optimizations
so here it will calculate the output
from our model
uh with that with the training data set
and then
calculate the loss
using the labels we have
and uh
if it's the training phase then then
we'll like
get this we'll put this loss backward
and optimize our weights for the
network
so this is the training part
and uh here we are like calculating the
results
loss function and how many of
um
images are correct or not
so this is for calculating the loss
and
so for for full epoch
here the loss is calculated
and
yeah this was the
part in the training and here we are
calculating the time required from
this
from starting our epoch 1 to number of
epochs we have
so here we started our time
and
we are
checking the time
till this point like when we our
training is completed
and we are storing this in time
and
so let's run this function
now
and now what we do is uh
we'll use this function
whatever we defined here
for our
ipex falls and ipex2
so we will store the time
required for the training in
with ipex falls in the do dur underscore
n
and with type x true
store in d u r i underscore i
so
so it will take some time and we will be
able to see the results
so it's uh initially it's using
the
normal pythons
so
right now epoch one is going on
and uh
in this cell what else we are doing is
we are
we will see like how much better ipex
training
is
and the speed up
so let's wait for this
so
so as also we can see loss is decreasing
as as
the epochs
as we go further in the box
since in initially we are having 0.4 for
loss
and accuracy 0.78
and
after the training has been done and the
weights been updated
uh from after the epoch one
till
now loss has been decreased and
accuracy is increased a little bit
so
yeah this this is
training is going on now
again with
epoch 3 again
loss decrease and accuracy increased
we are not like worried about uh
loss and accuracy right now
because we are just comparing the ipex
and uh normal pythons
time
for training
so
so training with the normal pythagoreans
completed it took 2 minutes 25 seconds
and uh
with ipex it's not started
just wanted to check how others are
doing i we do see a few of them are able
to
kind of progress on the hands-on
sessions
if you are struggling or are not able to
keep up feel free to just you know post
your queries
and
make sure that we be able to help you
please
so at least we have seen three or four
people already post their results out on
the discord so thank you so much for
doing that
uh and others i know
sauro and others who are probably not
had some issues in case you
have still some issues please let us
know
uh yeah
so
for my case it's completed now
and and we can see the training uh
with ipex doc one minute 27 seconds
and uh with the pi dodge stop by dos it
took two minutes 25 seconds
and and if we increase the box also we
will see this kind of
uh time reduction and it's it's scalable
so
i think that was what it was
that was one question in discord aditya
so probably what uh i think it was
indrajit who felt they were probably
ipex could need a little bit more number
of epochs
so yeah so you may want to just add on
to that
uh yes so even if you take like more
epochs you will see
such kind of accuracy
increase
so sorry performance increase and speed
up
but but it depends like what what batch
size you are taking and all these
parameters also
so for training this is the case like
according to our hyper parameters
it may change
but still like you you'll see like uh
from our experience we see at least 1.5
or
1.5 x speed up at least
mostly
yeah especially uh
this is a very very small workload right
for training typically the data sets are
so huge and it takes hours and hours of
training either from the cloud or or on
your local machines so just imagine
maybe a 30 to 40 percent uh you know
decrease in the reduction in the time
for training right it's going to be huge
yes yes
so now so this notebook or i've also
like added the
inference part also
uh
with the same data set
so because it's quick we can go through
it also
so here we don't need anything like uh
optimization like because we don't need
training
related things like calculating the loss
and uh
updating the weight so we don't need
criteria not loss criterion or
optimization variables here
so here just we need the model the
pre-trained model and the ipex
if we want to use ipex or not
so for the inference again the time we
are starting the time and then
activating the
evaluation mode of the model
then
here also we will calculate the loss uh
because how much uh accuracy and how how
how is the loss with uh validity
validation data set
again the same things like uh
if 5 x is true we are
we are uh using this ipex device
and if not
the device is already used
so
yeah here we are calculating the outputs
for all the input images and the loss
uh and
yeah so
similarly for each epoch we
will calculate the loss and accuracy
again
the only thing is like we don't need to
uh
up update the weights
so
again we'll see the time how much time
it
it required to
do all this inference part
so let's uh let's define this function
by running it
and now uh
similar to the
training notebook comparison training
time comparison
we are doing
i know inference time comparison
for
if if x is false or i fix is true
so
so it it will it will be very quick for
both
uh apex false or true
uh yeah so we can see inference time
with uh
stop by touch it took 14 seconds
here here we had 100 images for the
alien and alien category and 100 for
predator
so total images were 200 so it took 14
seconds so
uh to get the results from our model
so it is inference time
with stock pythons and uh
it took
six seconds with
ipex
so
yeah we can see like 50
almost 56 percent of uh
decrement in time
which was required for this infinite
and yeah more than like 2x of speed up
so this was the inference part and
here like we are using just
our trained models to
to get the results for some sample
images
from our data set
um
so
yeah like we can see here for this image
it's uh it's alien and
it's predicted hundred percent
uh
so it's unlike
very ideal case uh
for here also it's it's alien uh it's in
the alien category we can see here
so validation alien
model predicted correctly almost 82
percent
probability of
being alien it predicted
and here also like almost ideal case
predicted uh radiator
so this was uh like full
training and influence uh notebook
and uh
we have another notebook uh which has
like on some custom images and uh we
will be using the
uh infini lesnar 50 trained model
again to infer for those custom images
so
again like we can
go to this
python workshop
folder and then open lesson 50 inference
folder
and just double click on
inference notebook
so you will see like uh this pythons
1.410 is automatically selected
if if not you can just uh click on this
and uh
and select python
so here we have
we have all these six images
we have taken from
some sources like on internet and uh
and we have put
on this data set into like three
category so this one is uh like
for face
uh first face fact category and these
two are for
car category and uh
this one
this is like some other any like it has
some
id so here basically
the resonance 50 model is already
trained on trend on imagenet dataset
so
we are not doing anything else we are
just
we have taken these images custom images
and we will do the inference and and
in imagenet data set we already have uh
some id for each object like for this we
should be having some id
in imagenet data set and for car also we
we have we have some id
to differentiate and and for this
also like
we have some id
so uh let's run this uh
um
inference it's it's pretty
straightforward just uh
no no need offer training or anything
just uh
the same same thing we added here
import import intel python's extension
and ipex
and
we are just
using the ipex
in the function
so we are what we are doing is we are
transforming the input images
and preparing our data loader
and here we are like
getting the or getting our pre-trained
model there is no 50
and we activate the evolution mode of it
and we
we are using again the same thing that
we did for
the other notebook
just uh
did the two ipex device
so it's now available in the ipex device
uh
and again like uh what we are doing is
since since these are like uh
very few images six images so we are
doing this for like 10 times we are
doing the inference with 10 times and
then we leverage them
here we calculated all the time required
and then here i i took uh i took a
batch size of three we have six images
so
any message is fine
just
so let's let's run this and define this
function
for inference
and here we can
see the similar comparison
of time required for the inference of
uh
with ipex and without the apex so let's
run this
so again this will take some more time
because
we had like
uh
not many images but we did
we are doing inference multiple times on
the images
so
youtube wait for some time here
meanwhile like i can check
[Music]
so i know many of them have been sharing
the results when you are sending an
email to kavitha please make sure you
attach the screenshot as well as the
actual numbers
that you get on the
on the text right you can just copy the
or take a screenshot of both the
bar graph and also the actual numbers
that you get on your screen it will be
good so that we can correlate it
along with the code
uh so it took like
186 seconds
uh but but there's a mistake here like
we have added
this
m
so you can just assume or edit this ms2s
so second
so
so it's still fine because we
consistent with both of these
ipex or without ipex
so here here what happened is uh
we had like two batches and uh
as we can see in in the images uh data
set we had we had three
three
images for one category that much fish
so it had the id
0 in
a data set
and one was for this animal's
image so it has 336
and for the last one of car we had in
imagenet uh
it it should have
one five one one as id
so we can see like results are just
consistent with uh with and without apex
and in the time taken here you can see
so here it took 186 seconds for the
influence and here it with type x it
took 89 seconds
so it's again like more than two weeks
of speed
and with this notebook like you can uh
experiment more and
take other data set and check
or maybe other
models so this was the inference part
mainly
um
[Music]
so now i'll i'll
move to uh
openvino part also
uh
so uh ellen like i'm taking like two
minutes of
break here because i need to set up this
sure no problem yeah in the meantime if
there are any questions et cetera please
do let us know uh so what we are what
azithya is trying to showcase now is so
we did uh currently we did inference
using uh the pytorch extensions so the
next demo so this is not uh we we didn't
have the hands-on plan for this he will
just show a quick demo of
how openvino can use can be used to do
inference right openvino is a toolkit
from intel which is used for
inference and
let's say you already have a trained
model right
and you are not interested to let's say
make changes to your code with i with
ipex etc
then uh openvino can come in very handy
you just feed this existing model into
openvino
and then it will generate
some intermediate representation and you
can use that intermediate representation
for uh doing inference right so this is
assuming you probably don't have control
over the code and you just have the pair
model so in that case what do you do
right it could be pytorch it could be
tensorflow it could be any other model
so is going to show a demo of how you
use a ready-made model that you already
have and then do inference using open we
know so that openvino can optimize that
model for the underlying hardware
whichever is going to be the hardware
whichever intel hardware it will
optimize
so in the meantime any questions
yeah yeah
yes
just give me one more minute
in the meantime our every is everyone
able to get or reproduce the results
that aditya had shown on his screen at
least for those who have shared the
results on the discord we could see it
definitely
but or everyone is everyone able to
consistently reproduce the results
okay see there's a question on base
toolkit and separately install analytics
toolkit yeah you can you can directly
install ai analytics toolkit and it
should have all the dependencies covered
so you can just directly install the ai
analytics toolkit but in case you want
to do some profiling etc then you may
need the base toolkit where you have the
profilers like vtune etc right but if
you just want the pair
uh
the only the analytics toolkit you can
directly use the analytics toolkit and
even further if you want to just install
specific components like
ipex or even intel distribution of
python etc you can directly use any of
the channels like pip conda etc and
directly install just that component
which will also give you the intel's
optimized versions
so uh
question i think uh on ipex library work
only on intel versus ryzen can also be
used is it is that that's the question
uh
so uh so it's our sarah question uh so
uh sorrow so whatever extensions we have
for pytorch etc we have validated that
on the intel cpus
typically you know we don't guarantee
any performance on any other processors
but if
let's say
amd or nvidia have done some work on
this they can definitely you know they
you can leverage because uh for us one
api is a is kind of an open standard we
are just you know defining the standard
and
already we have uh
some
some part of you know the one api
already working on nvidia cpus etc but
as such for ipex specifically
we don't really guarantee any
performance on competitive cpus we
validate it only on intel processors
yeah yeah one clarification is that
currently the ipx only runs on avx 512
machines
yeah good clarification i think thanks
thanks for that so avx 512 is one of the
insta instruction set
uh that is needed for the pie charts
extensions to work so that is available
on the intel uh first generation
scalable xeon processors
so that is something that's a good
clarification thanks
okay and energy that's some question on
analytics toolkit
based includes analytics
so energy this is all your question and
answer any
okay
all right yes they are okay all right so
are you ready
are you able to yes yes okay
we can see you are not seeing
yeah we can see it we can see it yes
uh
so yes like what open mino toolkit is
basically
for
speeding up the inference of our
deep learning applications
and
it supports all the like uh
all the
main frameworks like pythons
like tensorflow pytorch and mxnet cafe
so all of these are supported uh
directly by
open mino
uh
but uh so all these are but only thing
is
with python we need to convert it to nx
so that's the thing it has
so we are going to show how how we can
convert it
because
now with the latest version uh
we don't need to
do the conversion manually it's it's
already included in the open miner
we just need to run the scripts and it
uh
it does all the conversion to on nx
and then use the open we know
so with open we know like we have
two steps like let's say we have uh one
pre-trained model of any framework deep
learning framework
here let's say uh
pytorch model
now
it uh
it needs to convert it to one nx that's
what uh
uh it uh it has been included in the
openvino
and once we have the onenext model
with the use of
model optimizer it gets converted into
ir format
so if format is basically
two files xml and bin file
xml file
will be having all the networks related
details
and bin file will have
our retained
weights for the network
so that is ir part which is done by
model optimizers
so we will see all this in
this
demo so let me
show the
directory structure of the open we know
so i have so this
open window 20 21.4 has been installed
and in this one we have this bin data
processing deployment tools and all this
so bin is here to just activate the open
you know so that i have already done
uh
it just sets the
just activate the open we know
and uh
so let's let's do the inference of uh
lesson 50 pre-trained model
i touch
so let me
go to this
connect folder
so here in the deployment tools uh we
have
open model zoo and then tools and then
downloader
so
we are just going to that
and here we can see uh
all these
python scripts which we can use directly
so the downloader one
let's check this
you can see like how it can be used with
edge
oh sorry i think i have not activated
the environment
so let me do that first
so this is my like
a python environment in which i
installed
some dependencies
requirements for the open you know
and
here like i'll
activate the open window itself
using this source optical bin setup once
so now open window is activated
now we can see
so we can see like
download as functionality what it does
so
the arguments that it is showing
so
here we can see uh if you want to
download some pretend model available in
open know model zoo we can just to give
that tester's name and
after that we can provide the name of
the model which we are
going to use
and uh and print all will show like what
are the what are all the models which
are available with the open window let
me
show this
so so yeah we can see like all of these
uh pre-trained models are available with
uh open you know and optimized with open
we know
so here we can see like all types of use
cases and multiple models for each use
case
so this was uh
like uh how to use the downloader just
just we need we don't need to do
anything just python downloader and uh
and give the name name of the model
which we want to use
so let's do this step
so
here okay
so we are going to download the pythos
z950 python
so here also in the list we can see
[Music]
less net 50 a python
this one
so from this list you can download any
model from here
and i have downloaded the model and uh
we can see where it uh
stored the model
like this
it will show
and after uh we have downloaded the
model uh
next step is to like convert this model
to
uh
ir format which will be ultimately used
by
inference engine of the open we know
so
let's do this like
use
model optimizer
so basically in the downloader
[Music]
downloader we have all this converter
downloader so converter.pi is used for
converting our
model
so just we need to provide
the model name
from the list
so what i have done is uh
[Music]
i think a quick time check we have
you can take maybe another five minutes
so so for converting our downloaded
model to if format what we did is
basically
uh we used that uh
converted.pi
file from our downloader folder and then
give the name of our model
uh
or like uh
name may not be required if you are
doing
some custom model
but we need to provide the model right
so
so here like mo and it's given
so first it will convert it to since
this is a python model it will convert
it to
one nx
so all these uh
arguments and
script is given this this we can use
manually also to convert the python's
model to nx since it's a pre-trained
model available with openvino
we can do directly we can use the
converter.i
directly
so
we can see here
it uh it converted our model to uh
to this xml and bin files and this is
stored somewhere in downloaded public
here
so we got now xml and pin files
and these files we can use
for our inference
let's say
okay
so we have like with open we know we
have multiple demos for the
different use case
let me show one of the demo food
directory
so these are all the like uh demos
available with uh
open we know so these are different use
cases so classification and uh
different different object detection
and these are like use cases
and we have like
demos also for all these different place
that uh
i think you can check later
but let let me run one demo here
with uh let's net
50
so since we have this retained model
and we converted it to ir format using
model optimizer
now we need to run this uh run the
inference using inference engine which
is the part of uh openvino
so
this classification sample.async it this
is the like
a demo
script which uses inference engine
inside
and we'll run this to check the uh
results from our
inference so
jsnx50 model
so let me run this
so what i did is basically i so i had
the data set of uh
uh data set which was like we used for
inference uh using
using ipex so that's the data set i use
i have like six images in that
so what i did is basically what i
there is this script
sample script classification sample
and then we gave
the path to our
xml file
and the input file here so we have like
six images i gave all the input files
and the device here so this is what the
demo looks like this is what the script
for learning the inference looks like
so what we did was we used this uh
script python script
which has all the inference engine code
in it you can just add it according to
your news this one
and then we gave the model file xml file
with m
flag and then
gave the input images
and then device cpu
so this rain our
inference
so we can see like we had the six images
uh and
first if you remember from ipex
inference we had the first three images
is classified as zero so we can see like
zero uh
first image got classified as zero
since it has the highest probability
and then similarly for the second image
first three image was classified as zero
so we can say like probability
of zero is high
similarly for uh fourth image and fifth
and sixth image we can see the results
are consistent with with open winner
infinite also
so this is like a quick uh
demo of insurance uh
uh on on that data set which we used for
ipex also
with that networking
model
so
[Music]
so yeah like like this we have
more demos
here i can i can show you
me
i think in the interest of time probably
we are just three minutes away so
probably if you're okay we can just you
know kind of summarize and wrap it up is
that okay with you
oh yeah just this one
time if there are any questions on open
we know uh if you can just post it on
discord we could just answer it thanks
for
so in this directory if you see
uh demos in open model zoo demos we have
like so many demos for each uh
each use case like you can see
classification and image
segmentation uh
so all all the like used cases are here
you can use these demos
first but just you need to have the
train models you can download the pt
model using
direct downloader and then
convert it to
ir using model optimizer
and then just use these demos
to get the inference from that model
uh and and all these uh steps required
for this for running the demo is also
available on
open window documentation
so
oh yes
no
okay thank you aditya
zing do you want to just show that
the final foil and then maybe
you can just hold the foil
if you can start sharing
uh
yeah yeah you can start sharing those
for i i don't have specific for you to
show
okay all right so let me just uh
uh
okay let me just
all right
can you all see my screen
did you want to cover this
i've already covered this okay awesome
awesome so let's just have one final
poll and then i will just wrap it up uh
can we have the predict can we have the
next poll please
we have 45 percent of the words
so which of the deep learning frameworks
are not supported with openvino
okay we have 57 percent votes so far 65
percent of the thing none of these and
i think we can stop the pool
yeah but if i think we can stop the pool
other than i want to clarify the
response
uh yes yes so
so
so all the models like uh all the
frameworks tensorflow python mx all are
supported with open we know so answer
should be like none of these
so i already showed with pytorch it
needs conversion to one nx but uh but
with tensorflow and mxnet
even like uh
even direct so like we don't need to
convert it to an nx or something like
that
so they are
they are even directly supported
and then
apart from
these three also like other other
frameworks are also supported like cafe
and one linux
so
okay thank you thank you
thank you okay thank you all i just want
to kind of summarize i think you saw you
know
through this workshop you probably saw
how to use the intel pi torch extensions
and extract the best juice out of your
code with
you know with simple changes to your
code you can see how much performance
you can actually extract right so
this is what this is the magic that some
of these extensions and the
optimizations can really help you out
and you also saw even if you don't want
to change any code you saw how we could
get almost the same performance with
openvino right so essentially what we
want to leave you all with is these
small changes can actually go a long way
to basically get get the best
performance uh you know out of uh
internal hardware and intel architecture
so currently what you saw is available
on cpu under for the one api framework
we eventually will expand these
gradually into gpus we already have some
internal versions that are supported on
gpus etc so by using these optimizations
you can actually minimize the amount of
effort that you put to
change the code to work on different
hardware and extract the best tubes
right so that's what i just want to
leave you all with before i just
conclude please do participate in the
contest
make sure that you send the screenshots
for all the exercises that we did today
and also if you have any projects
interesting projects that you feel that
you can that can benefit from neither
openvino or the intel pythons extensions
please do submit them on the intel dev
mesh and also you can send it or send it
across to kavitha you already have email
id so that would be great and uh with
that maybe kapita if we have the final
uh you know feedback survey uh please do
feel free to post it we can you know get
the feedback and
if there is any feedback you know feel
free to let us know any questions we
would still be hanging around on discord
for a few more minutes uh of course even
if you have we will be we will be
watching this card in case you have any
questions at a later point as well we
will uh answer it um i know at least in
the previous workshop there were people
who were actually doing the exercises
until late in the evening so we will uh
support you with any issues that you may
have executing these
these labs etc right so thank you so
much really appreciate all your time i
hope it was good good use of your time
the last three hours
thank you so much
and thank you all the presenters jane uh
aditya and pavita and mander and mary
you know for helping on the discord and
also the presentations and that's thank
you so much
excellent presentation thanks team uh
over to you sajuti
may be for
summarizing yeah uh thank you kavita
thank you lakshmi it was a great session
thank you zing and aditya for explaining
the hands-on sessions and it uh
definitely gave us a lot of insights
into using intel extension for pytorch
so and i'm sure that nds had a lot to
learn from from this session so thank
you so much from uh for the workshop
uh before we uh wrap up the session and
the workshop for today uh i would
quickly uh like to
uh shout out the names of the 10 lucky
winners of the intel dev cloud
registration contest
uh each one of them is going to receive
amazon vouchers worth thousand rupees so
the winners are
abhash
so these are the 10 names so
congratulations to all of you all the
winners will receive their prizes over
email by analytics india magazine
and uh i would also like to quickly
remind you guys like lakshmi mentioned
please participate in the devmesh
project contest uh
participants just needs to submit their
projects
uh using intel devcloud and one api
toolkit and you have time till 30th
august and please uh share the project
link to kavitha
kavitha.intel.com
and top eight projects will stand a
chance to win amazon vouchers so don't
forget to participate in the contest we
would also like your feedback the
feedback link has been shared with you
over the chat so please share your
feedback for the session
and uh that will be all for today and
thank you all for joining us at this
workshop thank you kavita once again for
organizing this thank you so much
yeah thanks thanks to the air team this
was a great collaboration uh and we had
some wonderful audience you know
participating and engaging
like lakshmi mentioned we are still on
discord for next few minutes so if you
have any questions uh please post in
your q a's on discord and will help
address those
thank you and see you all soon
unanswered questions in discord
yes
uh can we hang around there for a few
more minutes
yes you have any questions
yeah if there are any questions that
need to be addressed we can address it
thank you
thank you thank you lakshmi thank you
kavita thank you all
thanks guys
you