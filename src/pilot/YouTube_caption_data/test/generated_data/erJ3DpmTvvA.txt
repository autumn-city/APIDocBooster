hey everyone and welcome back to this
class modern deep learning and Python
deep learning in Python part 2 in this
lecture we're going to implement back
norm in pi torch as you'll see this is
going to be very very simple as with
dropout there will be only three main
changes the irrelevant file for this
lecture is PI torch Bachelor I in the
course repo so firstly again in order to
make this script I copied and pasted
everything from PI torch example to PI
so the script is going to be very
similar to that and all we are
interested in in this lecture is what
changed
so just like the dropout lecture all I
need to do is add some bachelor layers
since this is a feed-forward neural
network and our data is one dimensional
we use the batch norm 1b class in later
courses we'll be using different kinds
of neural networks like convolutional
neural networks which work on images and
images are of course 2d objects so we'll
have a different kind of batch norm for
those classes and also remember that
bash norm happens after the linear
transformation but before the activation
function so people have experimented
with different orderings so this is not
the only possibility but it's the most
common in any case this is still very
easy we just add some batch norm layers
to the neural network
next just like drop out we have a small
change to our train function remember
that Bosch norm also has two different
modes of operation one for train and one
for tests
so in train mode we update the running
mean and variance but in test mode we do
not so we call the train function on the
model and this sets it to Train mode and
now every layer within the model also
knows it is in train mode and can behave
accordingly
and also like before I want to call
model eval in my get cost and predict
functions so that I can switch to
evaluation mode after this everything
remains the same so let's run this and
see we get
you