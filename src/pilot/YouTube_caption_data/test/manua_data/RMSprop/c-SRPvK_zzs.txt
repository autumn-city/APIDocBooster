 Alright, let's now talk about how we can use these optimization
 algorithms in Python. And I can promise you this is going to be
 a very short video because it's super simple to do in pytorch.
 So the most common optimization algorithms are still SGD, the
 regular SGD with momentum. And Adam, there are of course,
 other optimization algorithms, and I will mention some of them
 in the next video. But yeah, these two are usually the ones
 that most people use in practice. And if you are also
 interested in using other types and exploring them, you can also
 find them here under this website. And there's also a
 more detailed description of the parameters that we can use. So
 how we use them here, you have already used optimum SGD before
 and previous code examples and the homework, I think. So right
 now, the only difference is really that here, I added the
 momentum term to it. Usually, I find that using a learning rate
 of point or one or point or one works well. But you have, of
 course, to experiment with it, it really depends on the data
 set and weight initialization and data normalization, batch
 norm, and so forth. There are many, many things that affect
 finding a good learning rate. So it's something you have to try
 out in practice. When I use SGD, I usually also use a learning
 rate scheduler, I will show you or give you the code for
 reference later in the slides. And if I use a learning rate
 scheduler, I usually also use a learning rate of point one. For
 Adam, I usually don't do anything I use usually a learning
 rate of point or one or point or five. These usually work well
 for me. But again, it depends on the problem I recommend also
 exploring some different learning rates. But most of the
 time, something like point or five works well for me. And it's
 sometimes also easier to find a good learning rate with Adam
 compared to SGD with SGD, I have to try out many more things to
 get it to work with with whereas with Adam, usually, multiple
 learning rates can work well in practice. So if you're using any
 of these like the momentum or Adam versions, also the momentum
 version of SGD or Adam, then yeah, and you want to save your
 model for continuing training later on, you also have to save
 and load the optimizer because there is now a given state. So
 there's a state for the momentum and also for the adaptive
 learning rate component, the RMS proper component. But yeah, I
 talked about saving and loading optimizers. And in earlier
 videos, I think it was like two or three videos ago. So if you
 work with the momentum version or the Adam and you want to
 continue training at a latest time point, make sure you know
 how to save and load the optimizer states too. So just a
 little bit more about Adam. So in the previous video, I showed
 you that there are two parameters. So the alpha
 parameter for the momentum term and the beta parameter for the
 RMS prop term. Yeah, they are also available to modify in the
 Adam optimizer. So here, they are both called betas, like in
 the paper. So this would be beta one. And this one would be beta
 one, two. And this one would be then beta two and beta two. So
 just so you know what they mean. So the first value is for the
 momentum beta. And the second value is for the RMS prop beta.
 Personally, I never change these that usually works well for me
 just keeping them as they are by default. And also in many
 papers, when you read deep learning papers, almost no one
 changes these parameters, they always say they use the default
 parameters because they are they usually work pretty well in
 practice. Here are two examples. I'm so of using these different
 optimization algorithms. So here I was using an SGD with a
 learning rate scheduler having the learning rate when it
 plateaus, and also momentum term of point nine. So the momentum
 term here was point nine, the learning rate was point one, and
 it was halved. Every time the validation accuracy was
 plateauing when it was not improving. So it was
 fluctuating. And then I was having the learning rate to
 stabilize the training. So that's then more like the fine
 tuning stage. And you can see in the beginning, it's rather
 noisy. And then probably after this having it stabilizes more
 because there's no improvement anymore. So we can have the
 learning rate and then remove that oscillation. So that's what
 I was doing here. And with this setup, I reached accuracy of
 test accuracy of 97.34%. On the right hand side, I was just
 using Adam with default parameters, I think I used the
 learning rate of point 005 here. And I got also essentially the
 identical performance. But here you can see, because we are not
 we're not using the learning schedule here, probably, we see
 it's a little bit noisy here around that region. But to be
 honest, both work perfectly fine in practice. And I find usually
 Adam works a little bit better for me. But I'm also a very
 impatient person. I think if I would toy around with the SGD
 more, I could probably also get a similar good performance or
 even outperform it. But yeah, Adam is something if you are
 lazy, like me, it usually always works pretty well. And I kind of
 like it for that reason. So in the next video, I want to just
 briefly go over some, I would say I am more advanced concepts
 regarding optimization algorithms, I want to talk about
 them in detail, I just want to mention them by giving you
 references for future study if you're interested.