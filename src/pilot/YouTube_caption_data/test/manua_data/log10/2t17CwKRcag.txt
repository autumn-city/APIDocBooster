It's Ryan Soklaski here.
I'm very excited to welcome you
to the fifth year of Cogworks.
We have a lot of exciting
things in store for you
from processing audio data to
making sense of visual data
through the computer, and
even quantifying language.
We're going to be
working together
to create some very
exciting projects
and ultimately craft cognitive
technologies that we're
going to be presenting together
at the end of this year.
Hello, everyone.
Welcome to the live session
for the Cogworks final
presentations.
We're so excited to be
sharing with you all
of the exciting work that
we've been collaborating on
over the past week.
We have six teams who
attended the main Cogworks
course, all of whom will be
presenting to their projects.
And then we also have an
international team for Mexico
and we're very excited to
show their work as well.
So without further ado,
I would like to start off
by introducing our team
Cog* It Like You Mean It led
by their TA, Jacob.
Hello, everyone.
We are team Cog* It
Like You Mean It,
and my name is Kathryn Le.
My name is Dharshini Anand.
My name is Hyojae Park.
My name is Joseph.
And I'm there TA, Jacob.
You wanted to help
students in their day
to day activities, hobbies
and chores by creating
artificial intelligence
programs that can work
with audio, images and text.
Our first project
is called Work,
which helps students answer
prompts that have images.
Our second project
is called Green
which helps students
sort their trash.
Our final project is Cog*ify
which helps students generate
music.
First off, we created a
program called Cog*Work!
that can generate full textual
responses to prompts that
require the context of an image
such as the quest question that
asks the students to explain
what a historical image might
mean.
A program works by
taking advantage
of pre-existing and
models and tools
to simplify the work needed.
This complete pipeline
works to answer questions
with images such as an
AP US History for prompt
using a large captured
segment of the internet
as a source of data.
We set out with the goal of
providing GPT-3 and Advanced
Language Model with semantic
understanding of an image.
In order to describe
the provided image
and gather relevant context, we
leverage the Google Image API
in order to make the best
guess at what the image is.
The Google Image API looks
at a massive image database
and tries to identify
similar images.
It then uses these
similar images
to obtain headlines
for these results.
We also leverage
a Headless browser
in order to perform a
reverse image search
to find additional headlines.
You also make use
of Google Vision,
a computer vision
model made by Google,
in order to label particular
objects in the image
and perform optical
character recognition, which
looks for text and image.
These labels and a transcription
of the text and the image
are combined with the
acquired headlines in order
to provide image context.
We then use this
context and the question
to prompt GPT with the hoops
that the context provides
GPT with a semantic
understanding of the image.
GPT-3 stands for Generative
Pre-trained Transformer 3.
It is an autoregressive
language model, a type of model
that predicts the future word or
token based on the tokens given
in context previously.
The GPT-3 model
in particular was
mostly trained on a
publicly available data
set called Common Crawl.
With this, GPT-3
can create anything
that consists of some form
of language structure.
It's generative ability combined
with its large knowledge base
makes it the best
option for a problem.
Here is a program outline
of that entire process
to summarize our
images convert it
into a description with the help
of reverse image search Google
Image API and
Google Vision data.
We then give the description
and the original question
into GPT-3 to get
a final answer.
Finally, here are some of
the resources from Cogwork.
Here, we created a
React web application
where you can insert any
image in a question related
to the image.
When you press Ask, it'll
query our express server
which will generate an essay
or answer the question using
the pipeline described before.
At the end of the
presentation, we'll
show the link of
the web application
so you try asking some
questions yourself.
Next, we created a program
called Cog*Green that helps
people with distinguishing
waste as trash, paper recycling,
or other recycling.
Recycling is important
and this program
will help you do it right.
Our first motivation
for making this program
is because many people don't
recycle since they don't know
how to, or it's too much work.
Since it's very harmful
to our environment,
especially with climate change
becoming a bigger problem
today.
The second motivation is as
some people try to recycle
but they do it incorrectly,
which is actually worse
than not recycling at all.
Lastly, this program can help
others learn how to recycle.
The data we used was from
a project named Trashnet.
This data set contains thousands
of images of glass, paper,
cardboard, plastic and metal,
each with a size of 512
by 384 pixels.
We categorize paper
and cardboard images
as paper recycling, and glass
and plastic and metal images
as other recycling.
Some limitations and problems
we initially ran into
were mainly because of
the size of our data set.
For instance, we
had to use Memmap
which allowed us to store
our data on our disk
and work with batches of
data in memory at a time.
We also had a hard time
transferring large files
so we use one drug
to store our files.
We also had to change
the way we train
our model by loading batches
of data into memory at a time.
When we finish
preparing our data,
we made a model that consisted
of two convolutional layers
and two dense layers.
A convolutional neural network
is a type of neural network
that is often used
for visual data.
It consists of many filters
that go around images looking
for certain features,
and these filters
can learn to detect specific
features like hair, color,
facial features, and many more.
These filters
constitute one layer
of a convolutional
neural network.
And by using several
of these layers,
CNN can learn to interpret
visual data very well.
After training our
data for 10 epochs,
we achieved a 74% test
accuracy, 84% train accuracy,
and a loss of 0.4.
And the picture on
the right, we see
that a glass bottle
was correctly
predicted to be other
recycling, which
is metal glass or plastic.
Finally we created a
program called Cog*ify!
that helps people create music.
Now, we wanted to create a
neural network that could
generate music all by itself.
So we looked online for help.
We found a blog that provided
a bottle at a data set of piano
music from old video games.
We prepared this data set
for our neural network
by using Music21 which is
a Python library for music.
We were able to turn the song
files into individual notes
and chords as you
can see on the left.
We had a few main
challenges when
we were designing our model.
Since we were using
a different library
than the ones used
online, we had
to closely look at
the documentation
to implement our own version.
Another challenge was finding
a data set large enough
to ensure a model wasn't
memorizing the music,
but still able to generalize
on broader patterns.
Lastly, our initial model
do not train on rhythm,
so we have to change our
methods of pre processing
to account for this.
For our neural
network, our layers
consisted of two LSTMs layers
followed by a dense layer.
This type of neural
network layer
is able to use context to make a
prediction, which is preferable
when working with sequential
data such as music and text.
We trained our data
using this model,
and we were able to achieve
a loss of around 0.742.
LSTMs, which stands for
Long Short Term Memory,
is a type of recurrent
neural network
that is exceptional at
retaining information.
LSTMs work by taking an
input data and memory
from previous LSTM blocks
and outputs to things.
A result and its memory that is
then used by subsequent elastic
blocks to create like
a chain of LSTMs.
And here's a sample result
from the model that was
trained on video game music.
[PIANO PLAYING]
Although it isn't the
best music or the music
you would hear on
the radio, you can
tell that the
model is definitely
picking up some patterns
and nuances in its music.
You also converted this
music into piano sheet music
as you can see on the slide.
Some ways that we can
improve our project
are to use more data.
Here are the references,
and thank you for listening.
We'd also like to thank our
incredible instructors and TA
for supporting us
through these projects
and helping us explore the
field of machine learning.
We also want to give a special
shout out to Python Like You
Mean It and Cog*Web.
Please let us know if you have
any questions about our work.
All right, that was wonderful.
Thank you team Cog*
It Like You Mean It.
All right, next up, we have
Team Ryan's Sandwich led
by the TA, Sam, take it away.
We are Ryan's Sandwich and we
would love to introduce you
to our project Cog*Shop, the
AI-powered Grocery Assistant.
To start off with, we'll
introduce our team.
I'm Jamie.
I'm James.
I'm Lara.
I'm Tobechi.
I'm Henry.
And we also have our TA, Sam
and Audrey in this picture.
I'm Sam, the TA for the team.
So why did we build this
particular assistant?
Well, grocery
shopping is a process
that many people have
to endure regularly.
Automating at least
part of this can
save time and make shopping and
meal planning less stressful.
In addition, artisan makes
meal planning more accessible
to those who may
have busy schedules,
or difficulty finding recipes
that suit their needs.
We'll begin by giving a
brief summary of our program
functions as you can see here.
We will be explaining
each in detail
as we go along with our
presentation and share demo
videos.
To start off with, here the data
sets that we use for Cog*Shop.
We have an image-based
grocery data
set for image
classification purposes
as well as text-based
recipe and nutrition
data sets for
retrieving information.
Yeah, so starting off with
our object detector model.
In order to recognize food
from an image, such as, maybe
a photo of groceries in
our fridge or pantry,
we first needed to be
able to detect objects
from the background of an image.
To do this, we use a
pre-trained object detection
model from Pytorch called
Faster-RCNN Resnet50 in order
to return coordinates
of detected objects
for an input image.
Faster-RCNN is trained on or
learns from a large image data
set called Coco which
provides the model with object
coordinates for a specific image
beforehand through learning
how to reproduce the
matching coordinates.
For a specific image
from the database,
model is then able to
learn how to return
coordinates for a
completely new image.
Next slide?
In the image on the
left, which happens
to be a picture
of my fridge, you
can see a demo of
the object detector.
And the object detector can
also classify certain objects
of food if they're
in the Coco data
set like the oranges, or
apples at the bottom my fridge,
or like cake.
However, in the case that they
detect an object an image isn't
in the Coco data set, it gets
sent to the image classifier
that my teammates James
into they save it done.
To classify an object
as a certain food,
we use a strategy called
Transfer Learning, in which we
used a primary model that is
already adept at recognizing
patterns and even further
on our data to get results
more efficiently.
We use the model Resnet18, which
was trained on an image data
set called ImageNet, and
we trained it further using
our grocery data set.
The image classifier takes in
the coordinates from the object
detector model and
determines if the object
is a type of grocery item.
How does Resnet18 work?
Resnet18 is an 18-layer
convolutional neural network,
which is a type
of neural network
that excels at
recognizing features
and patterns in an image.
Using transfer
learning, we train
Resnet to identify features
and images of our data set.
These features were then
passed into our dense layers
which were trained to classify
an image based on its features.
Here are some examples
of this model in action.
You can feed in an
image of an object
from various angles and
lighting conditions,
and the model will be able to
classify which type of grocery
item it is.
For our speech-to-text
application,
we used Python's included
Speech Recognition library.
We use this to
generate a transcript
from our voice that recognizes
keyword specific to food.
We could then use our transcript
to search for recipes,
or create a hands
free shopping list.
So to go into some
strategies that we
can use with these
models and techniques,
first, to find the nutrition
of food from an image,
we took in a list of the food
items discovered in an image
by our models.
We then use speech recognition
to have the user decide
what food item they wanted
to analyze, and then
search the nutrition data
set for its information
accordingly.
The process for finding
recipes was similar,
except instead our
function took in a speech
to text list of
ingredients, and then
search the recipe data set for
recipes that most closely match
these ingredients.
And we can show a demo video
of retrieving nutrition here.
Nutrition.
I can now insert a
file path to any image
that I would like to
check nutrition facts for.
The image will then be sent
to object identification
to identify each individual
grocery item in the image.
After identifies each
object in the photo,
ask me which specific object I
would like to nutrition facts
for.
Here, we have our recipe
retrieval system in action.
Find recipes.
Chocolate, vanilla,
strawberry, yes.
So as you can see, it generates
a recipe containing chocolate,
vanilla, and strawberry.
Our next function, our
shopping list function
informs users what
groceries they still
need to buy by checking the
contents of a pantry or fridge
against a shopping list.
To accomplish this,
our models identify
the groceries found in
an image, checks them
against a shopping list
taken from the user using
speech-to-text, and then prints
out what items are missing.
Here's our shopping list demo.
Now, let's use the same
picture of the fridge
to identify objects
in the fridge
and compare them
to a shopping list
to see what we're still missing.
Shopping list.
Again, the images passed
through object identification
to identify each specific
object in the photo.
Milk, Apple, pasta.
Yes.
Using voice recognition,
I generate a shopping list
and the Cog*Shop will return
what's missing in my fridge.
Now, normal old recipes
do well most of the time,
but what if a user is looking
for something more creative?
Coffee shop has just the
thing recipe generation.
To do this, we use something
called an n-gram model that
finds patterns in
how text appears
and the probabilities of certain
words appearing after others.
We trained this model
using our recipe data set.
By using the probabilities and
patterns found from this data
set, we created a
function that could
generate a completely random and
sometimes nonsensical recipe.
Here it is an action.
Generate a joke recipe.
Here, it generates a random
recipe using text generation.
We experienced some problems
when classifying fate.
Our accuracy well far better
than randomly guessing
was about 54% for new data.
This may have occurred
because we only
had 200 images per class
in our grocery data set.
The fact that our
data set was also
composed of German products
might have also been a problem.
The second model
also few drawbacks
primarily due to the data
sets, which trend on.
As you can see on the
image on the left,
the model has trouble
detecting all the objects
in the crowded space.
This is likely due to
the fact that most data
sets like Coco, which
the model was trained on,
have images where the
objects are distinct
and all spread out.
If you were to make
improvements to the model,
we would use data with
objects to put together.
The picture on the
right shows a model
that was trained using a
data set called SKU110K whose
objects are close together.
And as you can see,
the performance
has drastically improve.
And finally, we would like to
finish off by giving a huge
thank you to Cog*Works for
teaching us and for making this
project possible.
All right, thank you so
much, team Ryan's Sandwich.
Folks, I'll tell you
what the sandwich
is, it's peanut butter,
pickle and honey.
Don't knock it until you try it.
I'm telling you,
it's good stuff.
All right, so next, we
have the team Oh my Gauss.
Team Oh my gosh please welcome.
Oh, yeah, there you go.
All right.
Oh my gosh here,
my name is Ozan.
I'm Sophia.
I'm William.
I'm Koa.
I'm Clarice.
OK so to kick it off
within our final capstone,
we decided to pursue three
different projects each
inspired by Cog*Works week,
but still all within the theme
of music.
For the audio module, we
have music synthesis and note
detection.
For the language module, we have
sentiment analysis on lyrics.
And then for the visual module,
we have sheet music reading.
So you may ask yourself, music
synthesis with no detection?
What?
My goal behind this
project was to extract
clean coherent strings of
notes from WAV or MP3 files,
then use these notes to train
a model to synthesize music one
note or rest at a time.
My intention behind this
was to bridge my interest
in spontaneous music
improvisation and computer
programming.
I play a lot of
guitar so I thought
it would be fun to teach
a computer to make music
similarly to how I do.
But first, some
background information
within the domain
of note detection.
For those unfamiliar,
this is a spectrogram.
A spectrogram is a
visual representation
of the spectrogram of
frequencies of a signal
as it varies over time.
It works by decomposing
the signal into frequencies
and the respective
amplitudes by taking
discrete Fourier transform
at every given time step.
My approach to no detection is
premised around spectrograms
as they allow me to search
through the list of frequencies
and amplitudes for
the fundamental notes
that I want to retrieve.
Overtones are
musical tones which
are part of the harmonic series
above a foundational note.
Often when a single
note is played,
there are numerous
other higher frequencies
that resonate along
with it, which
make up the overtone
or harmonic series.
To the right, you can see the
spectrogram of "Never Gonna
Give You Up" by Rick Astley.
In this, the only
notes that are actually
being played occurring near
the bottom as you can see
indicated by a more
saturated shade of yellow.
The remainder of
the frequency shown
are overtones as depicted, it
is possible for these overtones
comparable amplitudes the
foundational played notes,
this brings up a fundamental
challenge in this note
detection and note detection.
In order to retrieve the
notes that were played,
one will have to
weave out overtones
and other possible
frequencies which
may be more readily visible
than the fundamental notes.
In computer-generated music,
this is less of a problem
so I implemented the
procedure as I normally would.
First, I converted the
audio to digital samples
then I passed my digital
samples to a spectrograph.
Then identified the frequencies
with the highest amplitude
for a time step.
Following this
identified frequencies
that persisted throughout
multiple time steps,
and finally I converted these
frequencies and time stamps
into a note object
that I had verified.
Here is the process being
done on the Mii Plaza theme.
[GUITAR PLAYING]
As you see in the program
is able to derive the notes
from the original signal.
However, natural sounds
contain many overtones.
I approached this issue
with three solutions.
First, I took the moving
average of the raw data signal
which smoothed out some of the
roughness in the recording.
Then I implemented
algorithms which
would search for undertones
below it's located frequency.
In case the frequency that
the program had was a node
was itself an overtone.
I also implemented
further methods
to remove erratic jumps in
the recovery signal, which
occurred due to imperfections
within the original sample.
Here is an example
with a guitar clip.
[GUITAR PLAYING]
We're quitting you could see
the frequencies fluctuate pretty
excessively, and
it's pretty clear
that those are not a
very good representation
of the original sample.
After cleaning the
notes, visually
impaired much more consistent
with the original signal.
Accordingly, the audio is pretty
similar to the original data.
Armed with data,
it's modeling time.
And N-gram model is
a model that operates
by generating an Nth item
given a history of N-1 items.
This is by randomly selecting
an item using the probabilities
that each subsequent item which
is determined through analysis
of a large body of data.
Models like these are commonly
used in natural language
processing, but are
also applicable to music
due to the sequential.
Nature Sophia and
Clarice wrote the code
for this model, thanks
Sophia and Clarice.
I trained it by recording my
own guitar improvisations,
driving the notion of
recording, computing
the intervals between
notes in semitones,
and passing these semitone
intervals into the model
so that generates
notes as intervals.
This way, the
model is not bogged
down by discrepancies and
key within the training data.
I ended up using
a 12-gram model.
[BEEPING]
So the next project is Lyrical
Sensitive Analysis and the goal
of this project was to
classify a song by having
a positive or negative
sentiment based on its lyrics.
And we use a data
set of 1,000 songs
from many different genres,
and each of the songs
were labeled as
"happy" or "sad".
In order for the computer
to classify each song,
we need to encode the
lyrics using TFIDF.
TFIDF will return
an array of numbers
that gives a numerical value
to each word in our lyrics
based on how important they are.
The first part of 2
is to take the lyrics
and create a list
of vocab words.
And from them, we
tokenized by filtering out
stop words, white
spaces, punctuation,
and we lowercase everything.
Then we take that vocab
list that we just created,
and we sort by the top
end words by occurrence.
And lastly, we calculate
the frequency of each word
with the vocabulary.
For the next part of TFIDF,
we use the equation log10
by N over nt for each word
in our vocab list, where
N is the number
of songs and nt is
the total number of documents
in which term teethered.
We decided to use
K nearest neighbors
since it is a
classification algorithm.
And K nearest
neighbors just looks
at a certain number
of news data points
and finds some distance between
them in relation to the query.
We used the TFIDF array to
find the cosine distances
by normalizing our
test in training data
and then transposing the
said training data to find,
then using the product to
find distance between them
and tracking it all by one.
So, in this picture
in the top left,
it's an example of how K
nearest neighbors works.
The green data point is the
point we want to classify,
and if we were looking
at the three nearest,
we could see that there's
two blues and an orange.
So we would classify that
green data point as a blue.
And we got around 72%
accuracy with our model
which isn't that great, but it's
better than randomly guessing.
Our second method was using
a dense neural network.
So as you could see,
you have an input size
of 400, which is the
length of a TFIDF factor
for a particular song.
And we feed that
into the input layer
and it goes through
two hidden layers.
And in between the hidden layers
or updatable model parameters,
and lastly there's
an output layer,
which includes two nodes.
Basically, two
outputs that represent
the negative sentiment score and
the positive sentiment score.
And the way we
trained this model
is with cross entropy loss,
and we also applied softmax
in addition to the
Adam optimizer which
allows the model
to have parameters
that would teethers my data.
And the train
results of this show
that the model
performs quite well
on training data, which is
800 songs within the training
batches.
So it kept on seeing
the train data
and it updates its
parameters accordingly.
However, the model wasn't
able to generalize super well
on testing data.
As you can see, the testing
accuracy was around 75
to 79% which shows
signs of overfitting.
So the limitations of
this project was that--
So one of them was we only use
a data set of only 1,000 lyrics
so that limited the
model in terms of words
and a range of
patterns and nuance.
And in addition, the
meaning with different songs
can be it isn't always obvious,
because many songs would
use metaphors as opposed
to obvious statements.
For example, because
I'm happy versus I'm
a hot air balloon that
could go to space.
They're both positive but
it's not always obvious
to the model.
And lastly, sentiment
can be quite subjective
and it isn't always like go
one way or another as opposed
to something like
restaurant reviews.
The final part of
our capstone project
is reading sheet music
using object detection.
The motivation
behind this project
is to help music
lovers hear what
they're composing or playing.
The task for algorithm is
to read notes on a page
from left to right
and play them out
in the correct order,
duration, and pitch.
So how did we
approach this task?
Well, we started off
with some research
and we found that current
methods of sheet music reading
employed purely
algorithmic approaches
such as template matching
and semantic reconstruction.
However, upon testing
those approaches,
we discovered their inaccuracies
and inconsistencies.
That brings me to
our approach of 1,
using bug detection
as illustrated
by the diagram on the right.
2, using a custom data
set for training, 3,
using two convolutional neural
networks or classifying.
And finally, 4, a
file generation method
to play our detected notes.
So first the blob detection.
When we get a lot of music, the
first thing our algorithm does
is it tries to figure out where
the notes are on the page.
In order to find
the notes, we use
a Python function from
the OpenCV library
called blob detection.
A blob is a group of connected
pixels such as the note
heads of the notes.
But in order to better
emphasize the heads
and improve our
overall accuracy,
we first applied erosion
filters to remove the noise
on the image, such as the
stop lines and bar lines.
We also used a dilation
filter in order
to ensure that the half
notes in whole notes,
which more closely
resemble empty circles,
are also picked up by
the blob detection.
Finally, using the coordinates
of the blobs or note heads,
we crop around the entire
note and return the results
for the next step.
As mentioned earlier, we
created our own custom data
set for this project.
And to elaborate, this data
set includes 14 pitches
ranging from C4 to A5, including
the rest pitch and 0 frequency.
It also includes
durations of notes
ranging from 16th
note to whole notes,
and then also six locations
of the note in an image.
In total, that made our data
set have a size of 1,122 images.
After acquiring our data, we
pivoted to the data preparation
stage in which we
applied to approaches -
padding and resizing
- before feeding
the images into our model.
So once we obtained the
individual images of notes
and rests from the
blob detection,
we used a convolutional
neural network
to classify both the
duration of the notes
as well as the name of the note.
This image here on the
slide is a RestCNN used
to identify images of vehicles.
But the structure of
our CNN is very similar.
Convolutional neural
networks are particularly
useful for image classification,
especially in our case
where the note and
staff in each image
is never in the
exact same position.
Because they
utilize convolutions
which are a special
type of layer
that are useful in picking out
important features and images
through applying filters.
Once we input our
images into this model,
it outputs a
probability for each
of the possible
classifications, and we then
take the highest probability
as our final prediction.
Given that we have two
models and each outputs
an array of classifications,
we needed some functionality
that would match those arrays
to the appropriate values
when being played
as a MIDI file.
So unlike the MP3 files,
you're probably familiar with,
MIDI files are an easy way to
store sequences of audio data
which is perfect
for our project.
Using two functions, write
music and play music,
the arrays at the
model's output are
passed through their
respective MIDI conversion
dictionaries written into a
MIDI file and then played.
Here's a demo of how this works.
[MUSIC PLAYING]
OK, moving on to the evaluation.
After adjusting the parameters
of our blob detection,
we were able to consistently
pick out all the notes
except for the occasional
half note, which
is particularly difficult
shaped as a thin donor.
Our classification models
achieved a relatively good
accuracy at around 65% to 70%.
However, the training
data accuracy
reached over 90% for
both models indicating
that there's a sizeable amount
of error due to overfitting.
We plan on improving this
classification accuracy
by training our model on
a larger amount of data
in the future as well as trying
data augmentation techniques,
such as randomly cropping,
rotating, and adding jitter
to our images.
And that wraps up our sheet
music reading project.
Considering we only had a
week to do research and work,
there are many
improvements we can
make including what
Sophia mentioned
about data augmentation.
But also for extensions
of the project,
we can implement more
classification abilities,
such as a larger pitch
range, the ability
to request key signatures, time
signatures, dynamic symbols,
staccato, slurs, and
so on, and the ability
to identify sound source from
a variety of instruments.
So while today marks the
end of PW side this summer,
we're very excited to continue
this project in the future.
Thanks for tuning
into our presentation.
Feel free to leave any questions
you may have in the Q&A box.
We'd also like to take a moment
to thank our TA instructors
and VWSI staff for enabling
this incredible educational
experience.
And if you like
learning Python, you'll
love Python Like You Mean It.
Thank you.
All right, thank you so
much team Oh, my Gouts.
For the record, folks, I did
not put them up to the website
there.
But can we also appreciate
their wonderful background
that they created with the
water flowing through pipes
and douse, good stuff.
OK next up, we have a group of
pandas led by their TA, Andrew.
Hi, we're team A Group Of
Pandas, I'm Akshata Tiwari.
I'm Kylen Williams,
I'm Michael Huang.
I'm Adrianna Peng.
And I'm Joe Lin.
Oh, sorry.
I'm Adrew, I'm their TA.
The first project SiLT is
a sign language interpreter
and translator.
Our project helps to facilitate
a conversation between someone
and another person who is
hard of hearing or deaf.
The software uses
neural networks
to recognize lives and
language from a webcam.
It is then able to
detect these signs,
transcribe them into text
and convert them into speech.
It also conducts a
conversation in reverse
by interpreting the speech and
converting it sign language.
This is presented to
the hard of hearing user
as a sequence of videos
of sign language.
So what is a neural network?
A neural network is essentially
a programmers attempt
at writing or quantifying
how humans learn.
So the way a neural
network works
is it takes in an
input like an image,
makes a prediction on
what is in that image,
and then uses what's called
a loss function to determine
how well that neural network can
predict what's in that image.
Then we use gradient
descent to determine
what parts of the network
correspondent what
parts of the output,
and then iteratively
optimize towards the
predictions we want it to make.
Let me tell you about the
pipeline for our project.
First, the software
detects signs
the user is making with their
hands through the webcam,
and then translate these into
corresponding sign language
symbols.
The science isn't
recognized and transcribed
into text which is later
converted into speech.
Similarly, another option is to
first have speech as an input
and then have the software
convert it to text.
The text is transcribed into
sign language, which is then
stitched into a single
video and presented
to the hard of hearing user.
The first step is detecting
the hands in an image.
To detect the hands
in an image, we
used a pre-trained neural
network called MediaPipe.
It was built by Google
and it draws points
at each of the joints
in someone's hand.
We then give these landmarks
to our neural network
to figure out the sign that
person is trying to make.
MediaPipe works
in two main steps.
First, it draws a box around
the hand in the image,
and then it places
points at each joint
in that image using what's
called a single shot detector.
A single shot
detector draws boxes
over the objects and an
image, and then resizes them
depending on how well that box
fits the object in the image.
All right, so now,
we got the input
of the hand and the landmarks.
Now, let's show you how to
detect signs based on that.
Yes, please play the video.
So here is a video of me trying
to spell the word "Hello"
and I'm spelling each word
very slowly because I don't
know the language very well.
And as you can see, we
recognize each letter each
signed as a letter
and then we are
able to concatenate
them together
using some filtering
technique to stitch together
the word "Hello".
So basically, to recognize sign
based on image and landmarks,
we try to approach this the
first a convolutional neural
network approach, whereby
we just pass in the image
and try to process
image and recognize
and sign based on that.
And the second approach we try
is a densely connected neural
network approach, where we pass
in the landmarks, positions
of the landmarks and try
to recognize a sign based
on the locations
of the landmarks.
Eventually, we
combine the two models
to make our final
prediction because we
find that this increases
the accuracy of test time.
And so here is a diagram of
the entire system of our SiLT
detector.
First, we obtain a
crop image of the hand
as well as the
landmarks position.
We transfer the
landmark position
to a polar representation
to help facilitate
the recognition of
the sign, and then we
pass it in through a densely
connected neural network
as shown on the right.
And on the other hand, we
also pass in the entire image
through a convolutional
neural network.
And combining the
feature generated
from the crop image as
well as the landmarks,
we are able to generate
our final sign prediction.
Next slide, please.
So a little bit about the
convolutional neural network
we use, here's a diagram
showing the convolutional neural
network.
The first step is a
pre-trained MobileNet V3 model,
and after that,
we stitch together
a couple of convolutional
neural network blocks.
And MobileNet V3 is
basically a chain
of what they call bottleneck
blocks stitch together.
Bottleneck blocks
is just a fancy way
of doing convolutions,
such that it saves memory
and basically, they're
using depthwise conclusion
and pointwise convolution that's
shown on the right hand side
to achieve similar results
as a normal convolution
while saving tremendous memory.
We chose this model
because it's very efficient
and it can run on a
web app very well.
So in the process of converting
sign to text and then
to speech, for the first
part, from sign to text
we're parsing the
sign into the text
and we're using the Google
text-to-speech software,
and this is API powered by
Google's machine learning
technology.
And the text-to-speech
API converts the text
into something called SSML
or Speech Synthesis Markup
Language.
And it turns into
audio data using CNNs.
Onto the audio or
text sign portion,
similar to converting
text to speech
to convert speech and text to
utilize the speech recognition
module.
That use Google
speech recognition API
to translate audio
input into words.
Roughly, how this
works is that you
start with an
analog signal, which
is, for example, your voice.
And that gets converted
into a digital signal
through a microphone
and that gets
passed into Google's
model, which
is a RNN-T model or a recurrent
neural network transducer
model.
And our non-models are good at
processing the sequential data
because they even maintain
like a history of all
of its past inputs and results.
But a RNN transducer
model is particularly
good for this purpose because it
can process them in real time.
So the model analyzes
distinctions,
it predicts the
text and outputs it.
So going from text
to sign language,
we use a very large American
Sign Language Database,
and we basically query each
word in our string of text.
So we query the words from
the database and the database
basically stores
from word to video.
We're pulling each video
corresponding to each word
and we're concatenating
them using
Python library called MoviePy.
Our next project is
called RoshamBot.
It's a rock paper
scissors detection game.
So RoshamBot is a
game that allows
you to play rock paper
scissors against the computer
with your real hand.
It uses a dense neural
network and hand detection
similar to our sign
language detector,
or a sign language translator.
And the computer
generates a rock paper
scissors to play against you.
So here's how the pipeline
for RoshamBot books.
The users first prompted to
generate a play either rock
paper scissors with their hands,
which goes through our model
and the model detects it.
So after that, the computer
generates its own move randomly
and the score for each
round is calculated
based on the results.
Here's a demo of our model as
you can see if you can play it.
If you pay attention
to the terminal, that's
the thing on the
bottom outputting text,
you can see that it
works in real time
and it's like really flexible.
It can recognize like the
symbols of rock-paper-scissors
at various angles and
various positions too.
It's really low latency.
We also created a
graphical user interface
for this game using Py game.
At first, the user is directed
to a start screen, which
includes a help button, which
leads to the instructions page.
But rules for the
game, the Start Button,
directs the player to
the live detection game.
The instruction page
basically provides
the user with steps and details
when the computers plays
as well as information
about the score.
And on the game
screen, the user is
prompted to generate a shape of
a simple either a rock, paper,
scissors with their own hands.
This leads to the camera popping
up and through the webcam,
the software detects
the user symbol.
The computers play is also
revealed right up to detection,
and is used to calculate
the score for the wrap.
We also gave the user an
option to play as many times
as he or she would like.
Here's another
demo, my hand again.
You can see me going through
the interface right now.
Here's instructions pages
of mentioned and start.
So if you pay attention
to the choices, where
it says you picked
blah, agent picked blah,
you can see the results
for each round I play.
All right, so some disclaimers,
all of the people involved
in this project, do not
regularly use sign language
and we're not very familiar
with it beforehand.
And this project was just
meant as a general ASL
to English translator.
We understand that not
all people who sign
are deaf or hard of hearing.
And not all people who are
deaf and hard of hearing sign.
So its purpose is
not exclusively
between those parties.
A potential future
application will not feature,
but a potential application
would be using this as a tool
for people to learn
sign language.
To throw in a couple of
words for future works
for the project,
as you can see, we
have a lot of
directions to expand on.
And one direction
that we can expand on
is as you can see during my
demo that currently we recognize
each sign as a letter.
Basically, we are only
recognizing 26 signs
corresponding to 26 others.
We are hoping to expand it
to a world level translation
from sign to English.
So currently, our letter
recognition system
achieved an accuracy
of 98%, which
is a little bit short of the
99% of the state of the art.
And in the future,
we hope definitely
to make a world
level translation.
World level
translation is actually
the state of the art of
research at the moment,
but we are definitely
planning to do that.
The other thing
that we plan to do
is to use an NLP models to
stitch together a sequence
of letters together into words.
So currently, we are
just using a Parser
that basically parsed a
sequence of letters recognized
from the sign into a word.
And in the future, we will try
to make a more sophisticated
sequence or sequence model.
And Joe, do you want
to speak on the, yeah.
Another thing, is,
as you may recall
for our pipeline
for text design,
we pretty much just
did a simple method
for querying to
a large database.
So instead for one
of our future works,
we plan to implement some
transformer-based network
architecture so
that we can better
convert text to sign language.
And we'll also be working on
perfecting the UI which we
unfortunately cannot show
today because my screen share
apparently doesn't work.
All right, thank you so
much Group of Pandas.
Next up, we have team Wow
led by their TA, Darshan.
Hi, everyone.
I'm Chris Gu.
I'm Shruti Sharma.
I'm Svanik Sharma.
I'm Evelyn Zhu.
I'm Darshan, the
TA for the team.
Our team's project
focused on music analysis
with machine learning and
deep learning algorithms.
The analysis that
we focus on where
sentiment analysis, lyrics
generation, audio generation,
and genre classification.
We'll start off with
sentiment analysis.
Sentiment analysis
is essentially
the process of an
algorithm usually
a machine learning
model determining
the emotional
facets of language.
Now, this is difficult
because language is subjective
and not quantifiable.
For our project, we
decided to gather
a database of our
favorite songs and then
divide them into
two groups based
on if they had a happy
or sad theme to them.
And the goal was to have our
deal models correctly classify
the songs into these two
groups when only given
their lyrics or their choruses.
For the DNN, we use the IMDB
Movie Review Sentiment Analysis
Dataset.
There were 10,054 unique
words seen across all reviews.
The labels of the reviews
were two dimension
and all other
statistics are posted.
For pre-processing, we
first counted the frequency
of each word in the review
and removed any stop words
which are words that didn't
carry any significant meaning.
Next, we converted
those frequencies
to TF-IDF values, which
is a weighting mechanism
to determine the
importance of each word.
Finally, we normalize
those values.
So for our model, the
DNN had two layers
without any
activation functions.
Our representation of the
model is shown on the right
and train statistics
are listed on the left.
Through our training, we end
up with a strong validation
accuracy towards the
end and is a little bit
of overfitting because the
training curve might minutely
eclipses the testing curve.
Our results, the DNN
achieved a decent accuracy
when the song's choruses
were used to classify.
So the confusion
matrix, we can see
that the classification
was pretty even
and other classification
metrics are listed as well.
The second model we created was
a Convolutional Neural Network
or a CNN.
This model was trained on a
publicly available Twitter data
set which was pre-processed
to keep everything
clear and consistent.
We also use glove
embeddings which
essentially translates human
understanding of language
to machines.
The reason we wanted
to test out of CNN
is because of the
model's ability
to learn how words
group together
can indicate a pattern.
CNNs are usually used on
computer vision problems,
we can also use
them on language.
In our case, we only have
one dimension, which is time,
so we use a 1D CNN.
Now, listed here is the
architecture of our model
as well as some hyperparameters.
Something important to
note is that we only
have one output, which is
a value between 0 and 1
to indicate the sentiment.
And after training our
model, we tested it out
on our self-created data
set, and the results
are depicted in this
confusion matrix here.
And overall, we achieved
74% test accuracy.
So RNN or Recurrent
Neural Networks
are DL models that
are specifically
designed to learn sequential
data like language.
Through the diagram
below, you can
see how the calculations are
carried into future ones.
But one major limitation
of basic ordnance
is that they have
trouble remembering
earlier inputs and
later calculations
as indicated for
the nodes in red.
The LSTM or Long Short
Term Memory mechanism
correct this drawback by
adding additional complexity
into each calculation
of the model
to keep earlier inputs
and calculations
accessible to
later calculations.
For the RNN, we
use the same data
set but with a fifth of all
reviews dedicated to testing.
And with pre-processing,
we limited the word count
of reviews and did a
sequential integer word
encoding based
structure, which meant
that each word was
converted to an integer
ID in the same sequence.
The Y Value this time was
one dimensional as shown.
The RNN was composed
of three components.
The first layer was a
trainable word embedding
layer which converted
those integer
word encoding into a vector.
The second layer
was the RNN itself.
And then the third and final
layer was just a dense layer.
The training statistics
are listed below as well.
For training, while there
was a high testing accuracy,
the RNN experience
substantial overfitting
because of the disparity
between the training
curves and the testing curves.
And for the results, the RNN
LSTM had the worst performance.
The results show that more
complex doesn't always
mean better.
But we do have to keep in mind
this objectiveness of sentiment
analysis and the unusual data
set choices for this project.
Our second project focused
on generating song lyrics.
We approach this like a
text generation problem.
Our goal was to produce text
that could resemble lyrics.
The first model we trained
on was an RNN LSTM,
which you now know about.
The results of this model
on generating characters
aren't too bad.
Since here, we can see
some actual English words
being generated.
We also went ahead
and tried to generate
lyrics using an n-gram model.
What an n-gram model does is
it predicts the probability
of seeing the nth
word based on history
of n minus 1 previous words.
So we are basically trying
to statistically create text
based on how frequent
certain sequences are.
We train the n-gram model
on a corpus of lyrics
from our self-created dataset.
The output of the
model is a dictionary
where the key is a word that
corresponds to the distribution
of possible words.
Our n-gram model
does a decent job
actually of creating lyrics.
With a 2-gram model, we can see
there are certain phrases that
do make sense.
Now, one of the limitations
which you might have noticed
is that our model is highly
dependent on the training
corpus.
With a 3-gram
model, for instance,
you can see that
it actually starts
to copy the exact
lines of songs,
because our model was trained
on such a small dataset
without enough variety.
So to improve on this
model, we would definitely
want to train on a
larger set of data.
And now, we move on
to audio generation.
So the objective
of this project was
to generate music that
sounds good to the human ear.
When deciding what type
of music to generate,
we decided to use
classical music
since it's more
structured compared
to contemporary genres like
rock pop, hip hop or metal.
And the approach we used was
similar to n-gram in the sense
that each note that
was produced was
based on the previous
sequence of notes.
The model that we use to learn
is probabilistic distribution
was the WaveNet model.
It's a deep
autoregressive model,
which means that it can predict
future values such as note
values based on past values.
The architecture of this
model is pretty complicated,
but it be dissolved into
three specific layers.
The first layer that really
matters in this model
is the causal convolutional
layer, which is basically
what makes the model
autoregressive, and is
what allows the model to
predict future note values based
on past values like an RNN LSTM.
The next layer that is very
important for this model
is the dilated
convolutional layer.
Which basically increases the
receptive field of the model
without actually
increasing the kernel size.
It also preserves the
audio input resolution,
so future layers can
actually use the original,
or they can basically
use the audio
as is from previous layers.
The final layers that
really matter for this model
are the residual blocks
and skip connections.
They preserve information
learned from other layers
and pass it on to
the layers up top.
So that means that layers
can learn from features
that were extracted previously.
They also stopped the problem
of vanishing gradients, which
is basically when a
neurons activation is
so low that the model basically
stops learning altogether.
So in terms of experimenting,
we tested the model
using several different
number of input notes
to generate a unit.
And we found a correlation
between that hyperparameter
with the sophistication
of the generated music.
I'll now play some snippets
of the music produced
from the WaveNet model.
[MUSIC PLAYING]
Our next audio technology
is Genre Reclassification.
Next slide, please.
In this project, we used
Convolutional Recurrent Neural
Networks to predict
the genre that most
correlates with the song sample
based on its spectrogram.
For our project, we utilize
GTZAN, a data set widely used
for music genre classification.
With good sound, we were able
to correlate wave files of song
clips to their genre labels.
After preprocessing, we trained
and tested our model over
with 10,000 song
samples over 10 genres;
80% of the data
used for training,
and 20% used for testing.
While CNNs are majorly used
for image classification,
they can also be used
for audio classification
as we can represent sound
over time with the spectrogram
and represented as a vector
or multidimensional array,
since the spectrogram is
a numerical representation
of song frequencies
occurring over time.
When using this type of
data, CNNs are very useful,
because representation
of features in input data
can be made regardless of
where those features occur.
Because our input
data is sequential,
as we have music
notes over time,
the input data was fed to
LSTM after going through 3 one
dimensional
convolutional layers,
so that both long and short
structures among music genres
could be identified.
And LSTM was used because it
can remember earlier sequences
better than a basic RNN.
After 100 iterations
of training,
our model produced a
training accuracy of 48.2%
and a validation
accuracy of 46.6%.
There are various reasons
for this model's performance.
Next slide, please.
Shruti was saying
there's a lot of reasons
as to why the model
didn't perform
as well as we thought it would.
One of the reasons is
dataset distribution.
Here, I use principal
component analysis
to break down the audio
features of each audio file
from both of the datasets
that we tried to use.
And I plotted it in
two dimensional space.
You can see that there's a lot
of overlap between the genres.
For example, you can see that
blues music and jazz music
tend to overlap left and
so do metal and rock.
This intuitively makes sense
because a lot of genres
are generally just
mixes of other genres,
and a lot of songs that
tend to combine elements
from multiple genres.
So after thinking
about it, we actually
did some last minute
research and we found out
that we can actually just
use a deep neural network
model and not any
fancy convolutional,
or recurrent models.
You might be asking
how is this possible.
Well, key feature
here that matters
is that instead of using
the raw audio or Mel
spectrograms that we produced
from the audio files,
we actually use the audio
features such as chroma,
spectral bandwidth,
and harmonic mean
to produce a higher accuracy.
So these features were actually
available to us in the dataset,
but we hadn't actually
thought of using it
for predicting the
genre, we were just
simply using the raw audio.
So once we took these
audio features that
were extracted for us
and normalize them,
we then just simply used a
couple layers of, or a couple
of dense layers with drop
out and batch normalization,
and we obtained a much higher
accuracy as you'll see.
The results of the
deep neural network
were extremely impressive.
The training
accuracy was 96.89%,
and the validation was 93.35%.
What this basically shows
is that sometimes just using
the raw audio or Mel
spectrogram data, or really,
in any project, just using
the raw data given to you
isn't always good.
The lesson we
learned was basically
that you should always
exploit your dataset
and that you should always
try to extract features that
are meaningful from your
dataset that are given to you
if possible.
So in terms of future
planning, we definitely
could have expanded
upon extensive sentiment
by building and testing
more machine learning models
using TF-IDF or RNN LSTM
model, and ensemble learning,
which is combining the
power of multiple models
to create more powerful
prediction methods.
Both sentiment analysis
and lyric generation,
it would also be
interesting for us
to try using a transformer
model or an attention mechanism
for the RNN LSTM models.
For audio generation,
we'd like to make
our music less
repetitive and we'd also
like to try other models
like RNN LSTMs TMS and GTZANs
For genre classification, we
are interested in researching
the nuances in how
music is classified
and the different
audio features that
can be used to more accurately
classify songs by genre.
We would also like to research
different classification
techniques, such as
K nearest neighbors
or K means clustering.
Next slide, please.
Thank you for listening
to our presentation.
And we would like to
give a special thank you
to all of TAs, our
lead instructors
runs a class named Peter
Griggs, and our group TA leader,
Darshan Krishna Swami
for the guidance
and during the course
and our final project.
And we would also like to shout
out Python Like You Mean It
and Cog*Work.
Citations and
links for resources
are on the last slide
of our presentation.
And thanks again for watching.
All right, thank you very
much team Wow, that was great.
So for the last team from
the main Cog*work class,
we have Team while Luigi
lovers' and I will be playing
their video that
they pre-recorded.
Hello, everyone.
This is team Waluigi
Lovers 2398729.
My name is Pranav.
I'm Bhargav.
I'm Nick.
I'm Mindy.
And I'm Isita.
Today, we will be
presenting our final project
utilizing the invaluable
instruction and mentorship
from our instructors in space.
Each component of
our final portfolio
involves intersecting CS
techniques with music.
As music is such an important
and fun aspect of all
our lives, we thought it
would be the perfect field
to blend with
computers - another fun
and important aspect
of all our lives.
So let the symphony commence.
Our first project
focused on utilizing
a convolutional neural network.
A popular type of
machine learning
model that is extremely good at
finding patterns between images
to determine which musical
instruments were present
in a given audio.
Musical pieces
from IRMAS dataset
compiled by UPF Barcelona
were used for analysis.
With over 6,000
seconds long clips,
our convolutional
neural network was
trained to classify audio into
11 different instruments based
on the patterns found in
the spectrograms, which
are a visual method of
representing audio files using
frequencies.
We took inspiration
from previous research
done in classifying instruments
sound with the IRMAS dataset
by Dominick Hing
and Connor Settle
from the Stanford University
Department of Computer Science.
Though our convolutional
neural network structure
was similar to theirs, we
utilize the Python migrant
library introduced
to us by Cogworks
as well as other processing
insights from the program
for this project.
For our machine
learning model, our team
chose to use a convolutional
neural network.
This is a type of
neural network that
has special layers
called convolutions
in its structure, where a filter
is applied to an input data
point that results in some
features being activated.
The input layer
collects input data
which in the case of the
convolutional neural network
would be pixel information.
Hidden layers extract patterns
that cannot be observed
by humans.
Eating an increasingly
accurate predictions
of the neural network learns
from more training data.
The output layer
returns probabilities
for each of the
different classifications
possible in the model with
the highest probability
being insufficiently answer.
This network did its
best to identify features
of our spectrograms
allowing it to learn which
graphs correspond to
which instruments,
and give us a prediction
based on its learning.
After the model trained,
the training accuracy
came up to be about
95%, or receiving
a test accuracy of 75%.
This clearly shows
signs of benefiting
which is another word
for when a model seems
to be performing
well at training
but actually does
poorly at testing.
Here's a short clip
doubling our model in act.
[MUSIC PLAYING]
Along with exploring
the different models
and tuning typekit
parameters further
to increase the accuracy,
one future extension
of this project that can
and should be explored
is the classification of
non-European instruments.
So the incidental music
from all over the world
is studied and preserved in
the form of digital data.
So a critical part of
all our audio analysis
is the fourier transformer
to the mathematical principle
where any close function
can be perfectly
recreated as a sum of sorts.
We decided to have a
little bit of fun with this
by taking a graph of
the Dow Jones Industrial
average from 1990 to the present
day predicting into chunks
and decomposing these chunks
into their most prominent
frequencies, and then
playing that back
through the speakers as
a sequence of pure tones.
So we can see that
in action right here.
[BEEPING]
Our genre classification
project uses
a convolutional neural
network to analyze
a piece of music and output
a probability distribution.
Among nine genres, these being
blues, classical, country,
disco, hip hop, metal,
pop, reggae and rock.
This project was inspired
by Kunal Vaidya's article
on Towards Data
Science regarding
his genre classification model.
Our nine genre
categories originally
came from the GTZAN
dataset, which
has been widely used in machine
learning projects concerning
music genre recognition.
The dataset consists of
1,030 second wave files,
each containing a clip
of audio corresponding
to one of 10 different genres.
In both our project and
Kunal Vaidya's project
from which we took
inspiration from,
only nine out of the 10 genres
were used due to an issue
with processing
jazz music files.
We pre-processed our data
by splitting each audio file
into five second long
clips to increase
the size of our dataset
and to train our model
to be able to recognize genres
with shorter clips of audio.
After splitting each clip,
we generated Mel spectrogram
through each audio sample.
A Mel spectrogram plus
frequencies at different times
of audio samples on a
scale that is normalized
to how humans perceive sound.
Since differences between
higher frequencies
are more difficult for humans
to detect than differences
between low frequencies,
a Mel spectrogram plots
sounds based on a
logarithmic scale.
Mel spectrograms
received as nampai
arrays representing
decibel units
and stored in respective
directories for input
for our model.
In our project, we used a
convolutional neural network.
We had both an
implementation of the CNN
that use scare us which was
what the tourist data science
article used and one that
we built with Pytorch.
Both models made use of
three convolutional layers
to analyze the image
data of each spectrogram.
And also a dense layer
to have the model
output a list of probabilities
for how likely the audio fits
into each genre.
The final version
of this project
uses the camera's implementation
since our data processing
and query functions really were
originally based around it.
But our Pytorch version
trained similarly as well as
seen in this slide,
and the query functions
can be easily adapted.
Even though we resulted
in a training accuracy
of 60% on Pytorch
and 70% on Karas,
one of the ongoing issues we
faced in both of our model
versions was overfitting.
Essentially, our model was
learning specific quirks
in the training data that can
be generalized to our testing
or validation data.
And we dealt with
this problem partially
by reducing our model complexity
and the number of layers
from 5 to 3.
We also created a dropout
layer which randomly
set input units to 0
with a frequency of 0.3
at each step of training.
Both of these changes
made positive impacts
on overfitting but if we
extended this project further,
we would likely look
into the datasets impact
on overfitting as it was
made between 2000 and 2001,
and may have more narrow
or outdated music samples.
We train our model using
a batch size of 100
and Epoch count of 25, and
both of those parameters
were calculated through
trial and error over time.
Now, using a query function, we
were able to input audio files
and view a corresponding genre
probability distribution.
Even with overfitting
on our model,
you'll see that our general
classification program
accurately dissects the
different styles of music
in an audio sample.
For example, testing disco, pop,
country, and classical songs,
all return accurate results.
And since we omitted the
judge genre from our project,
jazz songs generally
had a higher probability
of being the blues since both
styles are inherently related.
Moving forward, there are
definitely some functions
to fix and features to add.
We would want to experiment
more with resolving
overfitting and adding
audio sampling and song name
recognition functions to
enhance our genre classification
program.
So in another small project,
we used an n-gram probability
model to generate
a sequence of notes
based on probabilities generated
from data and analyzed.
So this normalized
function down here
will take in counts of
occurrences of notes
and use it to associate
notes with their probability
of occurring, which we can
see in this graph down here.
Down here, this
function is going
to act as a tradeable model
and keep a record of a history
with specified number
of notes and keep
track of how many times
a certain note follows
that history in
our training data.
And then it makes use of
the normalized function
to associate each history
with a list of probabilities
for the next note.
And now, we can just
generate a random note,
keep track of the history
of what's being generated,
and use the corresponding
probabilities
to select the next name
for how many other notes we
want to generate.
After straight on a
few songs from a minor.
[MUSIC PLAYING]
And for our last
project, we decided
to pay moj sound of
Waluigi, our team mascot,
by utilizing Python to
transform simple piano
pieces into melodies
with the Waluigi
wah sounds as the instrument.
With the help of the MIT
Music21 library and the Librosa
library, we were able to convert
medley files of simple piano
pieces to repeat the same
features in the wah sound
effect.
Here are short clips
of our released
and Hot Cross Buns in
the instrument of wah.
So here, I'll be quickly showing
you what for at least in wah
sounds like.
[MUSIC PLAYING]
And here, to show the
differences in beat length
is Hot Cross Buns.
[MUSIC PLAYING]
Thank you so much for
listening to our presentation.
We are grateful to BWSI Cogworks
for giving us the opportunity
to learn so much about
computer science and the chance
to apply in such an amazing way.
The course itself gave us
such an in-depth knowledge
about techniques in many
different areas of computer
science.
And the speakers
of the Bo's seminar
is truly inspired us to
keep exploring our passions.
Please let us know if you have
any questions about our work.
I think we can all give a
big wah to that, am I right?
OK, folks, we are very
strapped for time.
I'm going to play the
final video, which
is from the Cogworks
team from Mexico.
Good evening.
My name is Mani Virick Nielsen
and here is my partner.
Hello, my name is
Alberto Tamayo.
And we are the representatives
of the High School property
Esmeralda in Mexico.
And unfortunately, our
partner Diego Perez Rossi
couldn't be present here
due to COVID complications.
Yet, we are going to present our
Cogworks Autonomous Cognitive
Assistance which we
name NUDARPEC, which
stands for Neural Ultra-Dynamic
Autonomous Role-Playing
Polyhedrons Entropic
Classifier, which is our project
and we would like to
present it to you.
So firstly, we would like to
talk about the dataset which
contains more than 16,000 images
of dyes used for role playing
games such as Dungeons
and Dragons, or Warhammer.
And we decided that
our neural network
should be able to
classify the shapes,
the different polyhedrons
or figures of these dice.
As a project for the
future, we could improve it
by having the neural network
identify the numbers in device
and some of them.
So my partner, Alberto,
is going to talk
to you more about the dataset.
OK, thank you very much.
Well then, as my classmate
Mani said earlier,
the dataset includes
16,000 images
of guys for playing games.
And we decided to reserve
30,000 images for training data;
around 2000 for validation
data, and about 1,404 test data.
And then the data is split
into six categories which
are basically the six
types of dices, which
are d4, d6, d8, d10, d12, d20.
Now, about our
pre-processing of the data,
we will mention what we did.
Firstly, we normalized the
data so the vector operations
were much smaller and much
faster also for their training.
We also had data augmentation
although this was mostly
seen already in the data set
as the data set contained
images of the dice
but also taken
from many different
angles which counts mostly
as the detailed information.
And we decided to reduce the
dimensions of the images which
were in RGB, and we decided
to make them grayscale.
Well, the color wasn't
really as important
because we were more
focused on identifying
the shape the
polyhedron run itself,
and it was just taking many
operations more and much more
training effort.
So we decided to make it
one channel one dimension
and it would be much faster.
So our model is
actually pretty simple.
It is a sequential
model composed
of many different
convolutional layers.
The reason we
chose convolutional
is because it was very
good for identifying images
even if there were some
different translations in them,
or there were different space
variations even scaling.
Actually, all the
images in our dataset
have different scale and sizes.
So we decided to scale them
all to images of 100 times
100 pixels.
So convolutional was the
best answer for this.
So for our loss function,
we chose categorical press
entropy.
The recent mainly being
that it was much easier
to take the labels from
the folders their names
as positions in the vector.
So these vector would be
much easier and more tangible
for use in the training.
And my partner is
going to tell you
more about this training
we have with our model.
OK, thank you very much, Mani.
As you can see, our training
process consisted of 10 epochs
and we use the batch
size of 32 basically
to optimize the
training process.
So that it was like
efficient and quick
so that we could also have
the highest accuracy possible.
And well, by the 10th
epoch, basically,
our accuracy for all
three sets of data
were actually very, very high.
For the training data,
we obtained an accuracy
of 99.99% almost perfect.
For the validation data, we
got an accuracy of 97.95%,
and then for the test data
we got an accuracy of 99.92%.
Well, after making the model, we
decided to take our project one
step further so that
we could use the OpenCV
library to be able to capture
images with our computer
camera.
So that we can actually
grab the image of a dice
that we could show
in the camera,
and then maybe our
model could possibly
give us the answer to
what kind of dice it was.
Unfortunately, based on
the type of resolution
and the capabilities
of our camera,
it was really
difficult to really get
appropriate predictions.
And we were getting many errors
like the predicted levels
didn't really match
the true label.
So then we decided
to use some samples
that we collected
from the internet that
were completely
different to the data
that we use to train our model.
And so, this image as my
partner was talking about,
will be used for a demonstration
of how our model works,
and how it predicts
labels for images
that have never seen before.
So here we go.
We throw it an image
of dice many sites
and the network should tell us
how many sites the dice has.
The first one is 10-sided
dice and the network tells us
it is actually a 10-sided dice.
Well, here, we have an
8-sided dice and d8,
this is another
d8, d8, this is d6.
The model says this is a
d6, and another d6, d6, d8,
and other d8s.
Here, we have d20,
20-sided dice,
and the model says
it's 20-sided dice
so that's actually pretty good.
Here, we have
another 8-sided dice
and it says it's 8-sided,
6-sided, 6-sided, 8-sided and
8-sided.
Actually, it seems that our
model has done pretty good
with our results.
Right now, it seems
to have trained well
and it isn't all of
it so it's actually
predicting labels very good for
images have never seen before.
Well, I think that's all we
have to show for our model.
It's not very complex but we
have a lot of fun doing it.
We want to thank you for
the space you have given
us to make this
project and the courses
you have given us to
learn as much as possible.
Well, I don't know if Alberto
has anything more to add,
but as well for my
part, that's everything.
Thank you.
Well, thanks.
Yeah, well, pretty
much the same.
I also want to thank
you for watching
over there at MIT for the
courses and help, also
the challenges that they
provide the problems we
had to solve those.
It was very fun, it was very
satisfying to see your code run
and then you see
like the old test pad
that passed in the notebook.
It was a very fun and
really challenging.
And then the information
given was just very clear,
very concise.
It was just very
straight to the point
that is very clear and general.
So thank you very
much at the end,
and I think that's it for me.
Thank you.
Yes, thank you very much.
Yes, see you guys.
Thank you very much.
That was an awesome project from
the Cogworks team from Mexico.
Really cool seeing the dice
detector or dice classifier
there, that was an
awesome application.
I'm sure that some of the
other Cogworks students
would love to play some role
playing games and Dungeons
and Dragons with all of you.
All right, well, we
are all out of time.
My TAs and co-instructors have
been furiously responding,
or sending the Cogworks students
your questions from the Slido
session, and have been
responding in Slido.
So if you ask a
question, please check
to see if we've responded.
We're sorry that we cannot
do a live Q&A session since
we're running low on time.
But we will have a send off
during the closing ceremony.
So this is not
parallel quite yet
but everyone give a
virtual hand to all
of the wonderful work that was
done by the Coggies of class
of 2021.