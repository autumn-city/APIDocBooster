Adam is the most used Optimizer in deep learning.
No matter what type of data you have, you
can just blindly use Adam and it will just
work out.
This video assumes that you have seen the
previous video of this series in which I have
introduced Optimizers.
Before, going into Adam you must know the
Adaptive Gradient method or AdaGrad for short
which is also one of the optimizers.
When we deal with the update rule of Gradient
descent this learning rate parameter is always
constant if you set it to 0.01 so for the
whole training process it would be the same
so researchers came up with an idea to make
an optimizer that can change learning rate
as per the situation so it can converge faster...
and thus AdaGrad was born.
In AdaGrad, we divide the learning rate with
the square root of the past gradient and we
can calculate past gradients as follows.
First, we will initialize a value 'alpha'
as zero, and in every iteration, we will keep
adding the square of gradient in this alpha.
So, the alpha will contain all the history
of past gradients and then we can divide the
learning rate with the square root of alpha...
this thing here is called epsilon and it is
a very small number like 10 raised to -8 to
avoid division by zero error if in case our
alpha became zero.
That's why now our learning rate will not
be the same with every iteration it will keep
changing according to the knowledge of past
gradients.
But what change this has made?
Can it do better than normal gradient descent?
Short answer is yes...
The long answer is
I have created linear data in which the input
array contains numbers from 1 to 51 and the
output array or y_true is a transformation
of x.
You might have guessed that here both weights
and bias are 2.
And then I have created this function which
is taking weights and biases as input also
known as learnable parameters because during
the training process we will be finding optimal
values of them which can give minimum error
rate and the function is returning the mean
squared error loss function we will be giving
this function to an optimizer so it can reduce
this function so as to decrease the error
rate.
This 'gradients' function is returning a list
in which the first element is the partial
derivative of the above function with respect
to weights and the second element is with
respect to bias.
We have discussed this thing in the previous
video as well.
Then we will give function and gradients to
Gradient descent and AdaGrad so they can find
optimal values of weights and biases.
I have created both optimizers from scratch
with python so if you want to learn more you
can find this on my GitHub.
Links are in the description.
After training, I am printing the loss of
both optimizers and some matplotlib magic
to show us good-looking graphs.
If I run the program, you can see that Gradient
descent's mean squared error loss is about
0.78.
But AdaGrad's loss is so small that it is
almost zero.
If we see the training process plot here you
can see the x-axis is weights, the y-axis
is bias and the z-axis is MSE loss.
The plot is a convex bowl-like structure and
in this plot, both optimizers are finding
the minimum point.
The actual minimum is at (2, 2) for which
mse loss is zero.
Gradient descent wasn't able to find that
point as you can see it is way off from the
actual minimum.
But AdaGrad was able to find that point in
just 2 iterations.
This was the benefit of using the adaptive
gradient method!
But it still has some weaknesses.
Because in every iteration we are adding square
of gradients in alpha so with every iteration
alpha is increasing and then if we divide
this increasing alpha with learning rate this
term becomes very-very small.
And a time comes when this number becomes
infinitesimally small and training stops.
As you can see from these plots the alpha
is increasing with every iteration and the
learning rate is decreasing so this is quite
problematic.
To overcome this issue AdaDelta was born.
In the AdaDelta paper by Matthew Zeiler, he
talked about this drawback of AdaGrad saying
"due to the continual accumulation of squared
gradients in the denominator, the learning
rate will continue to decrease throughout
training, eventually decreasing to zero and
stopping training completely.
We created our ADADELTA method to overcome
the sensitivity to the hyperparameter selection
as well as to avoid the continual decay of
the learning rates.".
The second good thing about AdaDelta is you
don't have to choose the learning rate, it
automatically computes it.
Let's see how.
I know there's too much on the screen but
let's understand this step-wise.
Instead of just alpha, now we are also initializing
delta x as zero.
Delta x is the accumulated update this is
used to calculate the learning rate so we
don't have to choose it.
After this, we will update the alpha term
unlike AdaGrad there is one more term in this
equation known as 'rho' this is used to avoid
infinitely increasing alpha value.
As we saw in AdaGrad, the alpha value was
increasing with each iteration but in the
case of AdaDelta, the alpha first increased
till 50 iterations and then continuously decreased
so now there is no problem of learning rate
decay.
This was made possible due to this rho hyperparameter.
It is also known as 'Decay Constant'. and
is typically set to 0.9 Now, we will calculate
the update.
Instead of writing it with this equation,
I have written it separately because we will
need delta theta in this equation.
So, this term over here is the learning rate
calculated by AdaDelta.
We are dividing square root of delta x with
the square root of alpha... again this epsilon
is a very small number to avoid zero division
error.
By doing this we will get a learning rate
which will then get multiplied by the gradient.
The final answer will be the total update.
Then we can sum it with our theta term...
normal gradient descent stuff!
At last, we will update our initialized delta
x term by rho multiplied by delta x of the
previous iteration plus one minus rho into
update square.
DONE!
If you have gone side-track just remember
one thing we are going into so much hassle
because normal gradient descent has its learning
rate constant for the entire training phase...
which is so LAME!
So the optimizers like AdaDelta have some
techniques to vary the learning rate with
every iteration that's it.
Now, let's visit our Adam again...
Adam's main motive was to add the concept
of momentum into the previous optimizer like
AdaDelta.
In Adam, we will initialize two variables
namely 'm' and 'v' as zero.
'm' is known as the first moment vector, and
'v' is known as the second moment vector.
Now, iteration starts... the first equation
is to update initialized m variable and the
second equation is to update initialized v
variable the only difference between these
equations is in the first one we are multiplying
one minus beta1 with gradient but in the second
equation we are multiplying one minus beta2
with gradient square.
This is just like AdaDelta... instead of rho,
we are writing beta 1 and beta 2.
Typically, beta1 is set to 0.9, and beta2
is set to 0.999 as suggested by the author
of adam's paper.
But there is a slight problem with these two
equations as they are initialized with zero
that's why they are "zero-centered".
In the paper, the author said this as "these
moving averages are initialized as vectors
of 0â€™s, leading to moment estimates that
are biased towards zero, especially during
the initial timestep.
The good news is that this initialization
bias can be easily counteracted, resulting
in bias-corrected estimates".
Meaning, we will write two more equations
two correct this biasness.
The first equation is to correct the biasness
of m_t and the second of v_t.
In the first equation, we are dividing m_t
with one minus beta1 raised to t... here t
is timestep meaning at which number of iteration
we are at.
By doing this we will get bias-corrected m_t
now known as m_t hat.
Same with the second equation just instead
of m_t here it is v_t and in the denominator
instead of beta1 it is beta2.
Now, it's time to write the final update rule
of Adam...
Our learning rate parameter is back because
in Adam you have to set a learning rate, unlike
AdaDelta in which it is automatically computed.
And then we can compute gradient by dividing
bias-corrected m_t or m_t hat with the square
root of v_t hat.... again this epsilon is
a small value to avoid zero division error.
Yeah!!!
This was all about Adam.
If you are interested to see how to implement
this with python from scratch check the link
in the description.
So that's it for today's video.
Till next video see my other videos and if
you are from the very future I may have already
uploaded many videos in this series so you
can get this playlist link in the description
and many other useful resources.
See you in the next video.