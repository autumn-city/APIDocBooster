Vanilla adaptive gradients (RMSProp, Adagrad, Adam, etc) do not match well with L2 regularization. 
Usually batch sizes around 40 gives better results, as for my experience training with 40 batch size for 3 epocs using default RMsprop gives around 89% accuracy. 
In   particular, when combined with adaptive gradients, L2   regularization leads to weights with large gradients   being regularized less than they would be when using   weight decay.
There are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running average of first and second moment of the gradient. 
The best algorithm is the one that can traverse the loss for that problem pretty well.
