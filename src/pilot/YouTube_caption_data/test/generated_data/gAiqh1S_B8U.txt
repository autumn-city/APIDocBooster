hello there and welcome to this
collection of notebooks and tutorials on
machine learning for audio signals in
python
this is a course offered by professor
schulich at the illinois university of
technology
i am renato the instructor for this only
my online materials
and on this notebook we'll talk about
neural network detector
for the mnist digit recognition
so let's get started
in the last part in part three we saw
how to use a classifier network
for the mnist digit recognition
at this time we will change our strategy
and we will use
a detector for the mnist digit
recognition in such case it makes sense
to use a detector as in part one for the
recognition
because it has the advantage that we can
train the recognizer
of each digit class to output a one
or a value close to it when it
recognizes
digits or a zero or a value close to it
when it's not recognizing its digit
if no digit is present all detectors
will output values close to 0
which is something that the classifier
cannot do for that we are going to use
the same neural network structure
as in part 3 for the mnis classifier but
with a different loss function
instead of the cross entropy loss we'll
use the mean squared error loss
but we need to take into account that
the mean squared error loss
expects a different target
representation
for the cross entropy loss we need the
class label
number as a target which means we have
one
integer or a float input digit the mean
squared loss expects a float number for
each class
which means 10 float numbers for each
input digit
where the correct class should have a
one and
all other classes should have a zero
and then we we can do this by
initializing a torch array with zeros
and setting it to one at the correspond
correct class
and this is called a one hot
representation
pi torch has a function
it's the one hot that takes a long
tensor with the index value
of shape star and returns a tensor
of shape star and num classes that have
zeros everywhere
except where the index of the last
dimension matches the corresponding
value of the input tensor so it does
exactly what we want to do
this is also an example from the machine
learning mastery
but it was modified and translated to pi
torch
by professor schuler to use a simple
fully connector
network and i prefer that ported and
modified
to run inside jupiter notebooks we start
importing
our usual libraries so we are using
numpy
pipelot by torque torch fission
and here we are defined in the neural
network model and we see that we are
using
a very similar structure as we used
before
and we have two fully connected layers
the first layer has the input
features equals to the number of pixels
the output features also
equals to the number of pixels and the
second layer
as the input features equals to the
number of pixels and the output feature
as the number of classes like we do
we did before this time here we are
using a relu
after the first layer so we're using the
yellow activation function after the
first layer and then
we have the second layer and we return
the output of the second layer
at this part again we are using 100
images for the training set and 10
images for the testing set
we are also defining the 10 classes
according to the 10 possible digits
from 0 to 9. this is the same
what we've done in previous examples
so in the beginning our example will
print
the one hot vector y train one hot
where we can see that each row contains
the values for the 10 classes
and indeed only one entry is equal to
one
and all other are zeros
and this is what we are doing here we
are
using the one hot function and we are
converting
the target to the one hot
format and here we see it was the
original target seven two
nine nine zero eight and then here's the
one
hot format let me see this is the first
so it's equivalent to seven so zero one
two three
four five six seven and there is one one
at the position
this is a two so zero one two and so on
so now we've converted our target
it was a number an integer
that describes the digit and now we are
using the one hot
format that also describes the digit but
we have a one at the position
of the digit representation
[Music]
again here we are flattening the images
so it was 28 times 12 by 28 pixels and
we have
now of 784 samples vector for each
image and we see that for our training
set we have
100 vectors with 784
samples and for the testing we have 10
vectors with 784 samples so this is
our training set and our testing set we
also
did this before here we are defining
our model using the network
structure that we previously defined
here
we are using now the mean squared error
loss
and the optimizer we still use
the adam optimizer
at this point here we are training the
network on the mean squared error loss
using just 10
iterations like we did before
with the same procedure here and we have
here
the losses after every second
epoch and now we get also
the final evaluation of the model
we get the loss on the test set and it's
uh
higher than the
loss on the training set as we expected
and as we've seen
before
at this point here we are doing the
interference like we did also
in previous examples so we are feeding
our model
with an image from the testing set
and then the model will predict at this
time using the detector
which a digit it is so this is the image
we are fitting
and it looks like an 8 and the detector
predicted that it's yes indeed it's an
eight
and we can look here these are the
values
for all the classes
and here is the value for eight
so is the um highest value
and this is with the arg max we pick
the highest value in predicted the
correct
digit so it's interesting notice that
now we have some values of the 10
classes
and we need to observe those numbers and
we'll see
if they show some confident detection
and finally we're also feeding an
image of random pixels as input and
we see that it's not a digit but the
prediction our model thinks it's a zero
and if we see here we have a value that
is
higher than this threshold for the
confidence that we set so
the model thinks and predicts
it to be a zero so we need to be
very careful interpreting results and
setting
correct thresholds and of course this is
a very
simple example a very simple network
architecture
we use just an iterations but we can get
a lot of interesting concepts
and basic intuitions using
this kind of simple examples
you