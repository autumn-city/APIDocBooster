Vanilla adaptive gradients (RMSProp, Adagrad, Adam, etc) do not match well with L2 regularization. 
Usually batch sizes around 40 gives better results, as for my experience training with 40 batch size for 3 epocs using default RMsprop gives around 89% accuracy. 
Have you tried adjusting the learning rate of RMsprop optimizer ?  try with a very small value first ( default is 0.001, in keras implementation) and try to increment it with factors of 10 or 100.
Adam is a recently proposed update that looks a bit like RMSProp with momentum.
The best algorithm is the one that can traverse the loss for that problem pretty well.