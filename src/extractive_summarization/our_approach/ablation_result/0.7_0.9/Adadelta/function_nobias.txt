 We created our ADADELTA method to overcome the sensitivity to the hyperparameter selection as well as to avoid the continual decay of the learning rates.".  The second good thing about AdaDelta is you don't have to choose the learning rate, it automatically computes it. 
Adaptive gradient algorithms have learning rates for each parameter.  This is very helpful when you have models where some parameters might be more sparse (increase its learning rate) or not sparse (decrease its learning rate).
Adadelta is an adaptive learning rate method which uses exponentially decaying average of gradients. 
Adadelta optimizer has a way to adaptively change learning rate but still, it needs an initial value of learning rate. 
At step 0, the running average of these updates is zero, so the first update will be very small. 
