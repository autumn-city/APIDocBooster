All right. Welcome to lesson 7, the penultimate 
lesson, of Practical Deep Learning for Coders  
Part 1, and today we're going to be digging 
into what's inside a neural net. We've already  
seen what's inside a kind of the most basic 
possible neural net, which is a sandwich of  
fully connected layers, or linear layers, and 
values. And so we built that from scratch,  
but there's a lot of tweaks that we can 
do, and so most of the tweaks actually  
that we probably care about are tweaking the 
very first layer, or the very last layer.
So that's where we'll focus. But over 
the next couple of weeks we'll look  
at some of the tricks we can do inside as well.
So I'm going to do this through the lens of  
the Paddy… the Rice Paddy 
competition we've been talking about,  
and we got to a point where — let's have a look…
So we created a ConvNeXt model, we tried a few 
different types of basic pre-processing, we added  
Test time augmentation (TTA) and then we scaled 
that up to large images, and rectangular images.
And that got us into the top 
25 percent of the competition,  
so that's Part 2 of the 
so-called Road to the Top series,  
which is increasingly misnamed, since 
we've been presenting these notebooks,  
more and more of our students have 
been passing me on the leaderboard, so,  
currently first and second place are both 
people from this class: Kurian and Nick.
“Go to hell, you're in my target, 
and leave my class immediately!”
And congratulations, good luck to you.
So in Part 3 I'm going to show you a really 
interesting trick —a very simple trick—  
for scaling up these models further.  
What you'll discover if you've tried to use larger 
models… so you can replace the word small with the  
word large in those architectures, and try to 
train a larger model: a larger model has more  
parameters, more parameters means it can find 
more tricky little features, and broadly speaking  
models with more parameters, therefore, ought 
to be more accurate. The problem is that those  
activations — or more specifically — 
the gradients that have to be calculated  
chews up memory on your GPU, and your GPU is not 
as clever as your CPU at kind of sticking stuff  
it doesn't need right now into virtual memory 
on the hard drive — when it runs out of memory:  
it runs out of memory. And it also doesn't do such 
a good job as your CPU at kind of shuffling things  
around to try and find memory, it just allocates 
blocks of memory and it stays allocated until you  
remove them. So if you try to scale up your models 
to bigger models, unless you have very expensive  
GPUs, you will run out of space, and you'll get an 
error, something like “CUDA out of memory error.”  
So if that happens, first thing I 
mentioned is: it's not a bad idea to  
restart your notebook because they can be 
a bit tricky to recover from otherwise,  
and then I'll show you how you can 
use as large a model as you like.  
Almost it's, you know, basically you'll be able to 
use a x-large model on Kaggle. So, let me explain.
Now, I want… when you run something on Kaggle 
— like actually on Kaggle — you're generally  
going to be on a 16 Gig GPU. And you don't have 
to run stuff on Kaggle, you can run stuff on  
your home computer or Paperspace or whatever, but 
sometimes you'll have — if you want to do Kaggle  
competition— sometimes you'll have to run stuff 
on Kaggle because a lot of competitions are what  
they call “Code Competitions” which is where the 
only way to submit is from a notebook that you're  
running on Kaggle, and then a second reason to run 
stuff on kaggle is that, you know, your notebooks  
will appear, you know, with the leaderboard score 
on them, and so people can see which notebooks are  
actually good. And I kind of like, even in things 
that aren’t code competitions, I love trying to  
be the person who's number one on the notebook 
score leaderboard because that's something which,  
you know, you can't just work at NVIDIA, and use 
a thousand GPUs and win a competition through  
a combination of skill and brute force. Everybody 
has the same nine hour timeout to work with,  
so I think it's a good way of keeping 
the, you know, things a bit more fair.  
Now… so, my home GPU has 24 Gig so I wanted to 
find out what can I get away with, you know,  
in 16 Gig, and the way I did that is, I think, 
a useful thing to discuss because, again, it's  
all about fast iteration. So I wanted to really 
quickly find out how much memory will a model use,  
so there's a really quick hacky way I can do 
that which is to say: “okay, for the training set  
let's not use… (so here's the value counts 
of labels, so the number of each disease…)  
let's not look at all the diseases, let's just 
pick one, the smallest one, right?, and let's  
make that our training set. Our training set is 
the “bacterial_panicle_blight” images, and now I  
can train a model with just 337 images without 
changing anything else. Not that I care about  
that model but then I can see how much memory it 
used. It's important to realize that, you know,  
each image you pass through is the same size, 
each batch size is the same size, so training  
for longer won't use more memory, so that'll 
tell us how much memory we're going to need.
So what I then did was I then tried training 
different models to see how much memory  
they… they used up. Now, what happens if we 
train a model? So obviously Convnext_small  
doesn't use too much memory. So here's something 
that reports the amount of GPU memory just by  
basically printing out cuda's GPU processes, 
and you can see Convnext_small took up 4GB.  
And also this might be interesting 
to you, if you then call Python's  
garbage collection gc.collect(), and 
then call Pytorch's empty_cache()  
that should basically get your GPU back to 
a clean state of not using any more memory  
than it needs to, when you can start training 
the next model without restarting the kernel.
So what would happen if we tried to train this 
little model, and it crashed with a “cuda out  
of memory error”. What do we do? We can use a 
cool little trick called gradient accumulation.  
What's gradient accumulation? So what's gradient 
accumulation? Well I added this parameter to my  
train method here. That's my train method, 
creates my data loaders, creates my learner,  
and then —depending on whether I'm fine 
tuning or not— either fits or fine-tunes it.  
But there's one other thing it does… it 
does this gradient accumulation thing.  
What's that about? Well, the key step is here. 
I set my batch size (so that's the number of  
images that I pass through to the GPU all at 
once) to 64, which is my default. Divided by…  
(slash-slash means integer divide in Python..). 
divided by this number. So if I pass 2,  
it's going to use a batch size of 32. If 
I pass 4, it'll use a batch size of 16.  
Now that obviously should let me cure any memory 
problems, use a smaller batch size. But the  
problem is that now, the dynamics of my training 
are different, right? The smaller your batch size,  
the more volatility there is from batch to 
batch. So now your learning rates are all  
messed up. You don't want to be messing around 
with trying to, you know, find a different set of,  
kind of, optimal parameters for every 
batch size, for every architecture.  
So what we want to do is find a way to run just, 
let's say accum is two, accumulate equals two,  
let's say we just want to run 
32 images at a time through.  
How do we make it behave as if it was 64 images? 
Well, the solution to that problem is to consider  
our training loop. This is the… basically 
the training loop we used from a couple of  
lessons ago, the one we created manually. We 
go through each (x,y) pair in the data loader.  
We calculate the loss using some 
coefficients based on that (x,y) pair,  
and then we call backward() on that 
loss to calculate the gradients,  
and then we subtract from the coefficients, the 
gradients times the learning rate. And then we  
zero out the gradient. So I've skipped a bit 
of stuff like the… with torch.no_grad() thing.  
Actually, no, I don't need that because I've 
got .data, no that's it. That should all work  
fine. That skipped out printing the loss, that's 
about it. So here is a variation of that loop…
where I do not always subtract the 
gradient times the learning rate. Instead I  
go through each (x,y) pair in the data 
loader, I calculate the loss, I look at  
how many images are in this batch. So initially I 
start at zero, and this count is going to be 32,  
say if I've divided the batch size by 2. 
And then if “count” is greater than 64,  
I do my gradient… my coefficients update. 
Well it's not, so I skip back to here,
and I do this again. And if you remember there 
was this interesting subtlety in Pytorch,  
which is if you call backward() again 
without zeroing out the gradients,  
then it adds this set of gradients to the old 
gradients. So by doing these two half size batches  
without zeroing out the gradients between 
them, it's adding them up. So I'm going to  
end up with the total gradient of a 64 image 
batch size but passing only 32 at a time.
If I used accumulate equals four. It would go 
through this four times, adding them up before  
it subtracted out the coefficients.grad 
times learning rate, and zeroed it out.  
If I put in accum equals 64, it would go 
through and do a single image one at a time,  
and after 64 passes through eventually, 
count would be greater than 64, and we  
would do the update. So that's gradient 
accumulation, right? It's… it's a very  
simple idea, right, which is that you don't 
have to actually update your weights every  
loop through, for every minI batch. 
You can just do it from time to time.  
But it has quite significant 
implications, which I find  
most people seem not to realize, which is if you 
look on, like, Twitter or Reddit or whatever,  
people could say “oh I need to buy a 
bigger GPU to train bigger models”.  
But they don't. They could just 
use gradient accumulation, and so  
given the huge price differential between, 
say, a RTX3080, and an RTX3090 Ti, huge  
price differential… the performance is not that 
different. The big difference is the memory. So  
what? Just put in a bit smaller batch size, and 
do gradient accumulation. So there's actually  
not that much reason to buy giant GPUs.
John?
JOHN: Are the results with gradient 
accumulation numerically identical?
JEREMY: They're numerically identical 
for this particular architecture.  
There is something called batch normalization, 
which we will look at in Part 2 of the course,  
which keeps track of the moving average 
of… standard deviations and averages,  
and does it in a mathematically slightly incorrect 
way. As a result of which, if you've got batch  
normalization, then it could… it basically 
will introduce more volatility. Which is not  
necessarily a bad thing, but because it's not 
mathematically identical, you won't necessarily  
get the same results. Convnext doesn't use batch 
normalization, so it is the same. And in fact,  
a lot of the models people want to use, really 
big versions of which, is NLP ones, Transformers,  
tend not to use batch normalization, but instead 
they use something called layer normalization,  
which, yeah, doesn't have the same issue. I 
think that's probably fair to say. I haven't  
thought about it that deeply. In practice I found 
adding gradient accumulation for Convnext has not  
caused any issues for me, and I don't have 
to change any parameters when I do it.
Any other questions on the forum, John?
JOHN: Tamori asking shouldn't it 
be count>=64 if bs=64, I haven't…
JEREMY: No, I don't think so, oh yeah, is that… 
so we start at zero, then it's gonna be 32,  
then it's gonna be… yeah yeah, probably yeah, you 
can probably tell I didn't actually run this code.
JOHN: Madav is asking: does this mean that  
lr_find() is based on the batch 
size set during the data block?
JEREMY: Yeah, so lr_find() just 
uses your data loaders batch size.
JOHN: Edward is asking: why do we need 
gradient accumulation rather than just  
using a smaller batch size, and follows up 
with how would we pick a good batch size?
JEREMY: Well just, if you use a smaller 
batch size… here's the thing right,  
different architectures have different amounts of 
memory, you know, which they which they take up,  
and so you'll end up with different 
batch sizes for different architectures,  
which is not necessarily a bad thing but each of 
them is going to then need a different learning  
rate, and maybe even different weight decay 
or whatever, like, the kind of… the settings  
thats working really well for batch size 64 won't 
necessarily work really well for batch size 32.  
And you know, you want to be able to 
experiment as easily, and quickly as possible.  
I think the second part of the question was “how 
do you pick a optimal batch size?” Honestly,  
the standard approach is to 
pick the largest one you can,  
just because it's faster that way – you're 
getting more parallel processing going on.  
Although to be honest I quite often use batch 
sizes that are quite a bit smaller than I need  
because quite often it doesn't 
make that much difference,  
but yeah, the rule of thumb would be, you know, 
pick a batch size that fits in your GPU, and for  
performance reasons I think it's generally a good 
idea to have it be a multiple of eight. Everybody  
seems to always use powers of two, I don't 
know, like, I don't think it actually matters.
JOHN: And look there's one other, 
just a clarification or a check,  
if the learning rate should be 
scaled according to the batch size?
JEREMY: Yeah so generally speaking the rule of 
thumb is that if you divide the batch size by two,  
you divide the learning rate by two, but 
unfortunately it's not quite perfect.
Did you have a question Nick? If 
you do you can, okay cool yeah.
JOHN: Yeah no, that's us all 
caught up, thanks Jeremy.
JEREMY: Good questions thank you.
So gradient accumulation in fastai 
is very straightforward, you just  
divide the batch size by however much you 
want to divide it by, and then add a… you  
got something called a callback, and a callback is 
something which changes the way the model trains.  
This callback is called GradientAccumulation, and 
you pass in the effective batch size you want,  
and then you say, when you create the learner 
you say, these are the callbacks I want,  
and so it's going to pass in GradientAccumulation 
callback. So it's going to only  
update the weights once it's got 64 images.
So if we pass in accum=1 it won't 
do any gradient accumulation,  
and that uses 4GB. If we use accum=2 about 3GB, 
accum=4 about 2.5GB, and generally the bigger  
the model the closer you'll get to a kind of 
a linear scaling because models have a kind of  
a bit of overhead that they have anyway. 
So what I then did, was I just went through  
all the different models I wanted to try. So I 
wanted to try convnext_large at a 320 by 240,  
vit_large, swinv2_large, swin_large, and on each 
of these I just tried running it with accum=1, and  
actually every single time for all of these I got 
an “out of memory” error. And then I tried each of  
them independently with accum=2, and it turns out 
that all of these worked with accum=2, and it only  
took me 12 seconds each time, so that was a very 
quick thing for me. Then okay, I now know how to  
train all of these models on a 16GB card. So I 
can check here, they're all in less than 16GB.  
So then I just created a little dictionary 
of all the architectures I wanted,  
and for each architecture all of the resize 
methods I wanted, and final sizes I wanted.  
Now these models vit, swinv2, and 
swin, are all Transformers models,  
which means that, well, most Transformers models, 
nearly all of them, have a fixed size – this one's  
224, this one's 192, this one's 224. So I have 
to make sure that my final sizes are square,  
of the size required, otherwise I get an error.  
There are… there is a way of working around this 
but I haven't experimented with it enough to know  
when when it works well, and when it doesn't, 
so we'll probably come back to that in Part 2.  
So for now we're just going to use 
the size that they ask us to use.
So with this dictionary of architectures, and for 
each architecture, kind of pre-processing details,  
we switch the training path 
back to using all of our images,  
and then we can loop through each architecture,  
and loop through each item transfer– transforms 
and sizes, and train the model, and then  
the training script, if you're 
fine-tuning, returns the tta predictions.  
So I append all those tta predictions, 
for each model for each type, into a list,  
and after each one it's a good idea to do this 
garbage collection, and empty cache that… because  
otherwise I find what happens is your GPU memory 
kind of, I don't know, I think it gets fragmented  
or something, and after a while it runs out 
of memory even when you thought it wouldn't.  
So this way you can really do as much as you like 
without running out of memory. So they all train,  
train, train, train, and one key thing 
to note here, is that in my train script,  
my data loaders does not have the seed= parameter, 
so I'm using a different training set every time.
So that means that for  
each of these different runs they're using also 
different validation sets, so they're not directly  
comparable, but you can kind of see they're all 
doing pretty well, 2.1%, 2.3%, 1.7%, and so forth.  
So why am I using different training 
and validation sets for each of these?  
That's because I want to ensemble 
them, so I'm going to use bagging,  
which is I am going to take the average of 
their predictions. Now I mean really, when  
we talked about random forest bagging, we were 
taking the average of, like, intentionally weak  
models. These are not intentionally weak models, 
they're meant to be good models, but they're all  
different – they're using different architectures, 
and different pre-processing approaches, and so  
in general we would hope that these different 
approaches… some might work well for some images,  
and some might work well for other images. And 
so when we average them out, hopefully we'll get  
a good blend of, kind of, different ideas, 
which is kind of what you want in bagging.
So we can stack up that list of different… of all 
the different probabilities, and take their mean,  
and so that's going to give us 
3469 predictions, that's our  
that's our test set size, and each one has 10 
probabilities, the probability of each disease.  
And so then we can use argmax() to find 
which probability index is the highest,  
so that's going to give us our list of indexes.  
So this is basically the same steps as we 
used before to create our CSV submission file.
So at the time of creating this analysis that 
got me to the top of the leaderboard, and in fact  
these are my four submissions, and 
you can see each one got better.  
Now you're not always going to get 
this nice monotonic improvement,  
right, but you want to be trying 
to submit something every day,  
to kind of, like, try out something new, right, 
and the more you practice the more you'll get a  
good intuition of what's going to help, right. So 
partly I'm showing you this to say, it's not like  
purely random as to whether things work or don't, 
once you've been doing this for a while, you know,  
you will generally be improving things most of the 
time. So as you can see from the descriptions my  
first submission was our convnext_small for 12 
epochs with TTA. And then ensemble of convnext,  
so it's basically this exact same thing 
but just retraining a few with different  
training subsets. And then this is the same thing 
again, this is the thing we just saw, basically  
the ensemble of large models with TTA. And then 
the last one was something I skipped over, which  
was I… the VIT models were the best in my testing, 
so I basically weighted them as double in the  
ensemble – pretty unscientific but again it gave 
it another boost, and so that was that was it.
all right, John!
JOHN: Yes, thanks Jeremy. So in no particular 
order Kurian is asking, would trying out cross  
validation with k-folds with the same architecture 
makes sense as an ensembling of models?
JEREMY: Yeah, so a popular thing is 
to do k-fold cross-validation. So  
k-fold cross-validation is something 
very very similar to what I've done here.
So what I've done here is I've trained a bunch of 
models with different training sets, each one is  
a different random 80% of the data. Five-fold 
cross-validation does something as similar,  
but what it says is rather than picking, like say 
five samples out with different random subsets,  
in fact, instead, first like… do all except for 
the first 20% of the data, and then all but the  
second 20%, and then all but the third, and so 
forth, and so you end up with five subsets each  
of which have non-overlapping validation sets, 
and then you'll ensemble those. You know in theory  
maybe that could be slightly better because 
you're kind of guaranteed that every row is…  
appears… four times, you know, effectively. 
It also has a benefit that you could average  
those five validation sets because there's no kind 
of overlap between them to get a cross validation.  
Personally, I generally don't bother, and 
the reason I don't is because this way  
I can add and remove models very 
easily. I don't, you know… I can just,  
you know… add another architecture and whatever 
to my ensemble without trying to find a  
different overlapping non-overlapping 
subset. So yeah, cross-validation is  
therefore something that I use probably less 
than most people or almost… or almost never.
JOHN: Awesome, thank you. Are there 
any… just coming back to gradient  
accumulation… any other kind of drawbacks or 
potential gotchas with gradient accumulation?
JEREMY: No, not really! Yeah, like, amazingly 
it doesn't even really slow things down much,  
you know, going from a batch 
size of 64 to a batch size of 32.  
By definition you had to do it because your GPU 
is full so you're obviously giving a lot of data.  
So it's probably going to be using its processing 
speed pretty effectively, so yeah, no it's just…  
it's just a good technique that… we should all be 
buying cheaper graphics cards with less memory in  
them, and using you know, have like… I don't know 
the prices, I suspect it's like you could probably  
buy like two 3080s for the price of one 3090Ti 
or something, that would be a very good deal.
JOHN: Yes, clearly you're 
not on the Nvidia payroll.  
So look this is a good segue then, we did have 
a question about sort of GPU recommendations,  
and there's been a bit of chat on that 
as well. (JEREMY: I bet) So any… any,  
you know, commentary… any additional 
commentary around GPU recommendations.
JEREMY: No, not really. I mean obviously at the 
moment Nvidia is the only game in town, you know.  
If you buy… if you trying to use a you know Apple 
M1 or M2 or an AMD card you're basically in for  
a world of pain in terms of compatibility and 
stuff, and unoptimized libraries, and whatever.  
The Nvidia consumer cards, so the ones that start 
with RTX are much cheaper but are just as good  
as the expensive enterprise cards. So you might 
be wondering why anybody would buy the expensive  
enterprise cards, and the reason is that there's a 
licensing issue that Nvidia will not allow you to  
use an RTX consumer card in a data center – which 
is also why cloud computing is more expensive than  
they, kind of, ought to be, because everybody 
selling cloud computing GPUs is selling  
these cards that are like, I can’t remember, I 
think they're like three times more expensive  
for kind of the same features. So yeah, if you do 
get serious about deep learning to the point that  
you're prepared to invest, you know, a few 
days in administering a box, and you know,  
I guess depending on prices, hopefully will start 
to come down, but currently a thousand or two  
thousand dollars, and buying a GPU then you know, 
that'll probably pay you back pretty quickly.
JOHN: Great, thank you. Let's see, 
another one has come in. If you have a…  
back on models, not hardware… if you have 
a well functioning but large model, can it  
make sense to train a smaller model to produce 
the same final activations as the larger model?
JEREMY: Oh yeah, absolutely. I'm not sure 
we'll get into that this time around but  
yeah we'll cover that in Part 2, I think, but 
yeah basically there's teacher/student models,  
and model distillation, which 
broadly speaking there are ways to  
make inference faster by training small 
models that work the same way as large models.
JOHN: Great thank you, all caught up.
JEREMY: All right, so… that is the actual real 
end of Road to the Top, because beyond that  
we don't actually cover how to get closer to 
the top – you'd have to ask Kurian to share his  
techniques to find out that, or Nick, to get the 
second place from the top. Part 4 is actually  
something that I think is very useful to know 
about for learning, and it's going to teach  
us a whole lot about how the last layer 
of a neural net works. And specifically,  
what we're going to try to do is we're going to 
try to build a model that doesn't just predict  
the disease but also predicts the type of rice.  
So how would you do that? So here's the 
Data Loader we're going to try to build –  
it's going to be something that for each image 
it tells us the disease, and the type of rice.  
I say disease, sometimes normal, I 
guess some of them are not diseased.
So to build a model that can predict two 
things, the first thing is you're going  
to need data loaders that have two dependent 
variables, and that is shockingly easy to do  
in fastai thanks to the DataBlock. So we've seen 
the DataBlock before. We haven't been using it  
for the Paddy competition so far because 
we haven't needed it – we could just use  
ImageDataLoader.from_folder(). So that's like the 
highest level API, the simplest API. If we go down  
a level deeper into the DataBlock we have a lot 
more flexibility. So if you've been following the  
walkthroughs you'll know that as I built this 
the first thing I actually did was to simply  
replicate the previous notebook but replace the 
ImageDataloader.from_folder() with a DataBlock to  
try to do, first of all, exactly the same thing, 
and then I added the second dependent variable.
So if we look at the previous 
ImageDataLoader.from_folder() thingy,  
here it is. We were passing in some item 
transforms, and some batch transforms,  
and we had something saying what 
percentage should be the validation set
So in a DataBlock if you remember we have to 
pass in a “blocks” argument saying what kind  
of data is the independent variable, and what 
is the dependent variable. So to replicate what  
we had before we would just pass an ImageBlock 
comma CategoryBlock, because we've got an image  
as our independent variable, and a category, 
one type of rice, is the dependent variable.
So the new thing I'm going to show you here,is 
that you don't have to “only put in two things.”  
You can put in as many as you like, so if you put 
in three things we're going to generate one image,  
and two categories. Now fastai, if you're saying 
I want three things… fastai doesn't know which  
of those is the independent variable, 
and which is the dependent variable,  
so the next thing you have to tell it is how 
many inputs are there, “number of inputs,”  
And so here I've said there's one 
input, so that means this is the input,  
and therefore by definition two categories will 
be the output because remember we're trying to  
predict two things the type of rice, and the 
disease. Okay this is the same as what we've  
seen before, to find out… to get our list of 
items we'll call get_image_files(). Now here  
is something we haven't seen before – get_y is 
our labeling function. Normally we pass to get_y  
a single thing such as the parent_label function 
which looks at the name of the parent directory,  
which remembers how these images are structured, 
and that would tell us the label. But get_y  
can also take an array, and in this case we want 
two different labels. One is the name of the  
parent directory, because that's the disease. The 
second is the variety. So what is get_variety()?
get_variety() is a function. So let 
me explain how this function works.  
So we can create a data frame containing 
our trainings... our training data that  
came from Kaggle. So for each image it 
tells us the disease, and the variety.
And what I did is something I haven't shown 
before. In pandas you can set one column to  
be the index, and when you do that, 
in this case image_id, it makes this  
series... this... sorry… this data frame, kind 
of like a dictionary. I can index into it by  
saying tell me the row for this image, and to do 
that you use the “loc” attribute, the location.  
So we want in the data frame, the location of  
this image, and then you can also say optionally 
what column you want – this column. And so  
here's this image, and here's this column, 
and as you can see it returns that thing.  
So hopefully now you can see it's pretty 
easy for us to create a function that takes  
a row... sorry, a path, and returns 
the location in the data frame  
of the name of that file, because remember these 
are the names of files, for the variety column.
So that's our second get_y  
okay, and then we've seen this before, 
randomly split the data into the 20% and 80%.
And so we could just switch them 
all to 192 just for this example,  
and then use data augmentation to get us down 
to 128 square images just for this example.  
And so that's what we get when we say 
show batch. We get what we just discussed.
So now we need a model that predicts two things.  
How do we create a model that predicts two 
things? Well, the key thing to realize is we never  
actually had a model that predicts two things. 
We had a model that predicts ten things, before.  
The ten things we predicted is the probability 
of each disease. So we don't actually now want  
a model that predicts two things, we 
want a model that predicts 20 things:  
the probability of each of the 10 diseases, and 
the probability of each of the 10 varieties.
So, how could we do that? Well let's first 
of all try to just create the same disease  
model we had before with our new data loader. So 
this is going to be reasonably straightforward.  
The key thing to know is that since we told 
fastai that there's one input, and therefore  
by definition there's two outputs, it's going to 
pass to our metrics, and to our loss functions,  
three things instead of two: the predictions 
from the model, and the disease, and the variety.
So if we're gonna... so we can't just use 
error rate as our metric anymore because  
error rate takes two things. Instead we have 
to create a function that takes three things,  
and return error rate of the two things we 
want, which is the predictions from the model,  
and the disease, okay? So this is 
predictions of the model, this is the target.  
So that's actually all we need to do to define a 
metric that's going to work with our new dataset…  
with a new dataloader, and this is not going 
to actually tell us anything about variety,  
first we're just going to try to replicate 
something that can do just disease.  
So when we create our learner we’ll 
pass in this new disease error function.
Okay, so we're halfway there. The 
other thing we're going to need  
is to change our loss function. Now we never 
actually talked about what loss function to use,  
and that's because vision_learner guessed what 
loss function to use. vision_learner saw that our  
dependent variable was a single category, and it 
knows the best loss function that's probably going  
to be the case for things with a single category, 
and it knows how big the category is. So it just  
didn't bother us at all. It just said okay “I'll 
figure it out for you”. So the only time we've  
provided our own loss function is when we were 
kind of doing linear models and neural nets from  
scratch, and we did, I think, mean-squared-error. 
We might also have done mean-absolute-error.
Neither of those work when the dependent variable  
is a category. Now how would you use 
mean-squared-error or mean-absolute-error  
to say, how close were these 10 probability 
predictions to this one correct answer.
So in this case we have to use a 
different loss function. We have  
to use something called cross entropy loss, 
and this is actually the loss function that  
fastai picked for us before without us 
knowing. But now that we are having to  
pick it out manually I'm going to explain to 
you exactly what cross-entropy loss does, okay?
And you know these details are very important 
indeed. Like... remember I said at the start  
of this class, the stuff that happens in the 
middle of the model you're not going to have  
to care about much in your life, if ever, but 
the stuff that happens in the first layer, and  
the last layer including the loss function that 
sits between the last layer and the loss, you're  
going to have to care about a lot, right? This 
stuff comes up all the time, so you definitely  
want to know about cross-entropy loss, and so 
I'm going to explain it using a spreadsheet.
This spreadsheet's in the course repo, and so 
let's say you are predicting something like  
a... kind of a... mini imagenet thing where you're 
trying to predict whether something... an image,  
is a cat, a dog, a plane, a fish or a building. 
So you set up some model, whatever it is,  
a convnext model, or a just a big bunch of 
linear layers connected up, or whatever, and  
initially you've got some random 
weights, and it spits out at the end,  
five predictions, right? So remember, to predict 
something with five categories, your model will  
spit out five probabilities. Now it doesn't 
initially spit out probabilities. There's nothing  
making them probabilities, it 
just spits out five numbers;  
could be negative, could be positive, 
okay? So here's the output of the model.
So what we want to do is: we want 
to convert these into probabilities.  
And so we do that in two steps. 
The first thing we do is we go…  
EXP, that's e-to-the-power-of. We go 
e-to-the-power-of each of those things,  
like so... okay? And so here's the mathematical 
formula we're using. This is called the softmax,  
what we're working through. We're going 
to go through each of the categories.  
So these are our five categories – 
so here k is five. We've got to go  
through each of our categories, and 
we're going to go e-to-the-power-of  
the output, so zj is the output for the j-th 
category. So here's that, and then we're going to  
sum them all together. Here it is... sum up 
together, okay? So this is the denominator,  
and then the numerator is just e-to-the-power-of 
the thing we care about. So this row.
So the numerator is e-to-the-power-of cat on 
this row, e-to-the-power-of dog on this row,  
and so forth. Now if you think about it, since the 
denominator adds up all the e-to-the-power-ofs,  
then when we do each one divided by the sum, that 
means the sum of these will equal 1 by definition,  
right? And so now we have things that can be 
treated as probabilities. They're all numbers  
between 0 and 1. Numbers that were bigger in 
the output will be bigger here. But there's  
something else interesting, which is, because we 
did e-to-the-power-of, it means that the bigger  
numbers will be, like, pushed up to numbers 
closer to one, like we're saying, like, “oh  
really try to pick one thing” as having most of 
the probability, because we are trying to predict,  
you know, one thing. We're trying to predict 
which one is it, and so this is called softmax.
So sometimes you'll see people complaining about 
the fact that their model, which they said...  
let's say, is it a teddy bear or a grizzly bear or 
a black bear, and they feed it a picture of a cat,  
and they say “oh the model's wrong because 
it predicted grizzly bear but it's not a  
grizzly bear“. As you can see there's no 
way for this to predict anything other than  
the categories we're giving it – we're forcing 
it to that. Now we don't... if you want that,  
like, there's something else you could do which 
is you could actually have them not add up to one,  
right? You could instead have something which 
simply says: what's the probability it's a cat,  
what's the probability it's a dog, what's the 
probability it’s a plane, totally separately  
they could add up to less than one, and in that 
situation you can cert... you know... or or more  
than one in which case you could have like more 
than one thing being true or zero things being  
true. But in this particular case where we want 
to predict one and one thing only, we use softmax.
The first part of the cross entropy formula...
The first part of the cross entry formula... 
in fact let's look it up, nn.CrossEntropyLoss.
The first part of what cross-entropy loss 
in Pytorch does is to calculate the softmax.  
It's actually the log of the softmax but don't 
worry about that too much, it's just a... slightly  
faster to do the log, okay? So now for each 
one of our five things we've got a probability.
The next step is the actual cross-entropy 
calculation, which is we take our five things,  
we've got our five probabilities, and then we've 
got our actuals. Now the truth is the actual, you  
know, the five things would have indices, right? 
Zero, one, two, three or four, and the actual  
turned out to be the number one, but what we tend 
to do is we think of it as being one-hot-encoded,  
which is we put a one next to the thing for 
which it's true, and a zero everywhere else.  
And so now we can compare these five numbers to 
these five numbers, and we would expect to have  
a smaller loss if the softmax was high where the 
actual is high. And so here's how we calculate...  
this is the formula... the cross-entropy loss.
We sum up… (they switch to M this time for some 
reason, but the same thing…) we sum up across  
the five categories so M is 5, and for each one we 
multiply the actual target value, so that's zero…  
so here it is here the actual target value,  
and we multiply that by the log of 
the predicted probability… the log of  
(red) the predicted probability. 
And so, of course, for four of these  
that value is zero, because see 
here: y-j equals zero, by definition,  
for all but one of them, because it’s one 
hot encoded. So for the one that it's not  
we've got our actual times the log softmax, 
okay, and so now actually you can see why  
pytorch prefers to use log softmax because then it 
kind of skips over having to do this log at all.
So this equation looks slightly frightening but 
when you think about it all it's actually doing  
is: it's finding the probability 
for the one that is one,  
and taking its log, right. It's kind of weird 
doing it as a sum but in math it could be a  
little bit tricky to kind of say, oh look this up 
in an array, which is basically what it's doing,  
but yeah basically, at least in this case for 
a single result with soft max this is all it's  
doing, it's finding the 0.87 where it's 1 for, 
and taking the log, and then finally negative.
So that is what cross-entropy loss does.  
We add that together for every row.  
So here's what it looks like if we add it 
together over every row, right, so N is the  
number of rows. And here's a special case, this 
is called “binary cross-entropy”. What happens  
if we're not predicting which of five things 
it is but we're just predicting “is it a cat?”  
So in that case if you look at this approach 
you end up with this formula, which it's  
exactly… this is identical to this formula but 
in for just two cases, which is you've either:  
you either are a cat; or you're not a cat, 
right, and so if you're not-a-cat, it's one minus  
you-are-a-cat, and same with the probability 
you've got the probability you-are-a-cat,  
and then not-a-cat is one minus that. So here's 
this special case of binary cross entropy,  
and now our rows represent rows of data, okay, 
so each one of these is a different image,  
a different prediction, and so for each 
one I'm just predicting are-you-a-cat,  
and this is the actual, and so the actual 
are-you-not-a-cat is just one minus that.  
And so then these are the predictions 
that came out of the model,  
again we can use soft max or it's 
binary equivalent, and so that will  
give you a prediction that you're-a-cat, and the 
prediction that it's not-a-cat is one minus that.
And so here is:  
each of the part “y-i” times log of “p-y-i”, 
and here is…(why did I subtract that's weird,  
oh because I've got minus of both, so I 
just do it this way, avoids parentheses…)  
yeah, minus the are-you-not-a-cat times 
the log of the prediction value not-a-cat,  
and then we can add those together, and so that  
would be the binary cross-entropy loss of 
this dataset of five cat or not-cat images.
Now if you've got an eagle 
eye, you may have noticed that  
I am currently looking at the documentation 
for something called “nn.CrossEntropyLoss”  
but over here I had something 
called “F.cross_entropy()”.  
Basically it turns out that all of the loss 
functions in pytorch have two versions – there's  
a version which is a class, this is a class, 
which you can instantiate passing in various  
tweaks you might want, and there's also 
a version which is just a function,  
and so if you don't need any of these tweaks 
you can just use the function. The functions  
live in a… I can’t even remember what the 
sub module called, I think it might be like  
torch.nn.functional but everybody including the 
pytorch official docs just calls it capital-F,  
so that's what this capital-F refers to.
So our loss, if we just care about disease 
(we're going to be passed the three things)  
we're just going to calculate cross_entropy 
on our input versus disease. All right  
so that's all fine… we passed… so now when 
we create a vision learner you can't rely on  
fastaI to know what loss function to use, because 
we've got multiple targets, so you have to say:  
this is the loss function I want to use, this 
is the metrics I want to use. And the other  
thing you can't rely on is that fastaI no 
longer knows how many activations to create,  
because again it… there's more than one target, 
so you have to say the number of outputs to create  
at the last layer is 10. So this is just 
saying what's the size of the last matrix.
And once we've done that we can train it, and we 
get, you know, basically the same kind of result  
as we always get, because this model at this 
point is identical to our previous convnext_small  
model – we've just done it in a slightly more 
roundabout way. So finally, before our break,  
I'll show you how to expand this 
now into a multi-target model.  
And the trick is actually very simple, and you 
might have almost got the idea of it when I talked  
about it earlier – our vision learner now requires 
twenty outputs – we now need that last matrix  
to have to produce twenty activations not ten. 
Ten of those activations are going to predict  
the disease, and ten of the activations 
are going to predict the variety. So you  
might be then asking, like well, how does the 
model know what it's meant to be predicting,  
and the answer is with the loss function, 
you're going to have to tell it.
So for example disease_loss – 
remember it's going to get the input,  
the disease, and the variety – this 
is now going to have 20 columns in.  
So we're just going to decide, all right, we're 
just going to decide the first 10 columns,  
we're going to decide are the 
prediction of what the disease is,  
which is the probability of each disease. 
So we can now pass to cross_entropy  
the first 10 columns, and the disease target. 
So the way you read this colon means every row,  
and then colon 10 means every column up to the 
10th. So these are the first 10 columns, and that  
will… that's a loss function that just works on 
predicting disease using the first ten columns.
For variety, we'll use cross_entropy 
loss with the target of variety,  
and this time we'll use the second 10 
columns, so here's column ten onwards.  
So then the overall loss function is the sum of 
those two things disease_loss plus variety_loss.  
And that's actually it! That's all the model 
needs to basically… it's now going to… if  
you kind of think through the manual neural nets 
we've created, this loss function will be reduced  
when the first 10 columns are doing good 
job of predicting the disease probabilities  
and the second 10 columns are doing a good 
job of predicting the variety probabilities  
and therefore the gradients will point in an 
appropriate direction that the coefficients  
will ~(be getter and…) get better and better 
at using those columns for those purposes.
It would be nice to see the error rate 
as well for each of disease and variety,  
so we can call error_rate passing in the first 
10 columns and disease, and then variety,  
the second 10 columns and variety. And we may 
as well also add to the metrics the losses.  
And so now when we create our learner 
we're going to pass in as the loss function  
the combined_loss – and as the metrics, our list 
of all the metrics and n_out=20 and now look what  
happens when we train! As well as telling us the 
overall train and valid loss, it also tells us the  
disease and variety error and the disease and 
variety loss and you can see our disease error  
is getting down to a similar level as it was 
before. It's slightly less good but it's similar.  
It's not surprising it's slightly less good 
because we've only given it the same number  
of epochs and we're now asking it to try to do 
more stuff, which is to learn to recognize what  
the rice variety looks like, and also learns 
to recognize what the disease looks like.
Here's the counterintuitive thing 
though if we train it for longer  
it may well turn out that this model 
which is trying to predict two things  
actually gets better at predicting disease than 
our disease specific model. Why is that?... like  
that sounds weird, right, because we're trying to 
do more stuff, [and] that's the same size model.  
Well the reason is that quite often it'll turn out 
that the kinds of features that help you recognize  
a variety of rice are also useful for recognizing 
the disease. You know, maybe there are certain  
textures, right, or maybe some diseases 
impact different varieties different ways,  
so it'd be really helpful to know what variety 
it was. So I haven't tried training this for  
a long time and I don't know the answer is… In 
this particular case does a multi-target model  
do better than a single target model at predicting 
disease, but I just want to let you know sometimes  
it does, right. So for example a few years ago 
there was a Kaggle competition for recognizing  
the kinds of fish on a boat, and I remember 
we ended up doing a multi-target model  
where we tried to predict a second thing… (I can't 
even remember what it was, maybe it was a type of  
boat or something…) and it definitely turned 
out in that Kaggle competition that predicting  
two things helped you predict the type of fish 
better than predicting just the type of fish.  
So there's at least, you know, there's two reasons 
to learn about multi-target models: one is that  
sometimes you just want to be able to predict 
more than one thing, so this is useful;  
and the second is, sometimes this will actually 
be better at predicting just one thing,  
than a just one thing model. And of course 
the third reason is it really forced us to  
dig quite deeply into these loss functions and 
activations in a way we haven't quite done before
So it's okay, it's absolutely okay, if this 
is confusing. The way to make it not confusing  
is… well… the first thing I do is, like, go 
back to our earlier models where we did stuff  
by hand, on like the titanic dataset and built 
our own architectures, and maybe you could try  
to build a model that predicts two things in the 
titanic dataset. Maybe you could try to predict  
both sex & survival or something like that, 
or class & survival, because that's going to,  
kind of, force you to look at it on very small 
datasets. And then the other thing I'd say is  
run this notebook and really experiment at 
trying to see what kind of outputs you get,  
like actually look at the inputs and look at the 
outputs and look at the data loaders and so forth.
All right let's have a six minute break, so 
I'll see you back here at ten past seven.
Okay, welcome back. Before I continue, I 
very rudely forgot to mention this very nice  
equation image here is from an article by 
Chris Said called “Things that confused me  
about cross-entropy.” It's a very good article, 
so I recommend you check it out if you want to  
go a bit deeper there. There's a 
link to it inside the spreadsheet.
So the next notebook we're going to be looking 
at is this one called Collaborative Filtering  
Deep Dive, and this is going to cover our 
last of the four major application areas,  
collaborative filtering.
And this is actually the first time I'm going 
to be presenting a chapter of the book largely  
without variation because this is one where 
I looked back at the chapter and I was like,  
“oh I can't think of any way to improve this.” So 
I thought I'll just leave it as is, but we have  
put the whole chapter up on Kaggle, so that's 
for the way I'm going to be showing it to you.
And so we're going to be looking at a 
dataset called the MovieLens dataset,  
which is a dataset of movie ratings,  
and we're going to grab a smaller version 
of it, 100 000 record version of it,
and it comes as a csv file which we can read in,  
well, it's not really a csv file, it's a 
tsv file, this here means a tab in Python.
These are the names of the columns. So here's 
what it looks like. It's got a user, a movie,  
a rating, and a timestamp. We're not going to 
use the timestamp at all. So basically three  
columns we care about. This is a user id, so 
maybe 196 is Jeremy, and maybe 186 is Rachel,  
and 22 is John, I don't know. Maybe this movie 
is Return Of The Jedi, and this one's Casablanca,  
this one's LA Confidential. And then this rating 
says how did Jeremy feel about Return Of The Jedi,  
he gave it a three out of five. That's how we 
can read this dataset. This kind of data is very  
common. Anytime you've got a 
user and a product or service,  
and you might not even have ratings, 
maybe just the fact that they  
bought that product, you could have a similar 
table with zeros and ones. So, for example  
Radek, who's in the audience here, is now at 
Nvidia doing, like, basically just this, right,  
recommendation systems. So recommendation 
systems, you know, it's a huge industry,  
and so what we're learning today is, you 
know, a really key foundation of it. Um…
So these are the first few rows. This is not a 
particularly great way to see it. I prefer to kind  
of cross tabulate it, like that, like this. This 
is the same information. So for each movie, for  
each user, here's the rating. So user 212 never 
watched movie 49. Now if you're wondering, uh…
why there's so few empty cells here, I actually 
grabbed the most watched movies and the most movie  
watching users for this particular sample matrix. 
So that's why it's particularly full. So yeah,  
so this is what kind of a collaborative filtering 
dataset looks like when we cross tabulate it.
So how do we fill in this gap? So maybe 
user 212 is Nick, and review 49… what's  
a movie you haven't seen, Nick, and you'd 
quite like to? Maybe, not sure about it,  
the new Elvis movie, Bez Luhmann? Good choice. 
Australian director, filmed in Queensland. Yeah,  
okay, so that's movie two… that's movie number 
49. So is Nick gonna like the new Elvis movie?  
Well, to figure this out, what we could do, 
ideally, would like to know: for each movie…  
what kind of movie is it? Like, what are the 
kind of features of it. Is it like actiony,  
science fictiony, dialogue 
driven, critical acclaimed,  
you know. So let's say, for example, we were 
trying to look at The Last Skywalker –maybe  
that was the movie the… Nick's wondering 
about watching. And so if we like, had,  
three categories, being science fiction, action, 
or kind of classic old movies, would say The Last  
Skywalker is very science fiction. Let's 
see this is from like negative one to one,  
pretty action, definitely not an 
old classic, or at least not yet.
And so then, maybe we then could say, 
like, okay, well, maybe like Nick’s tastes  
in movies are that he really likes science 
fiction, quite likes action movies,  
and doesn't really like old classics, right. 
So then we could, kind of, like, match these up  
to see how much we think this 
user might like this movie  
to calculate the match, we could just 
multiply the corresponding values  
user1 times The Last Skywalker, and add 
them up, point nine (0.9) times point nine  
eight (0.98), plus point eight (0.8) times point 
nine (0.9), plus negative point six (-0.6) times  
negative point nine (-0.9). That's going to give 
us a pretty high number, right, with a maximum  
of three. So that would suggest Nick probably 
would like The Last Skywalker. On the other hand,  
the movie Casablanca we would say 
definitely not very science fiction,  
not really very action, definitely very old 
classic. So then we'd do exactly the same  
calculation and get this negative result here. 
So you probably wouldn't like Casablanca. This  
thing here, when we multiply the corresponding 
parts of a vector together and add them up,  
is called a dot product in math. So this is 
the dot product of the user's preferences and  
the type of movie. Now the problem 
is, we weren't given that information.  
We know nothing about these users or about 
the movies. So what are we going to do? 
We want to try to create these 
factors without knowing ahead of time  
what they are. We wouldn't even know what 
factors to create, what are other things  
that really matters when it says people 
decide what movies they want to watch?
What we can do is we can create things called 
latent factors. Latent factors is this weird  
idea that we can say: I don't know what 
things about movies matter to people,  
but there's probably something and let's just 
try, like, using SGD to find them. And we can  
do it in everybody's favorite mathematical 
optimization software: Microsoft Excel.
So here is that table.
And, what we can do –let's head over here 
actually– here's that table. So, what we  
could do is we could say: for each of those 
movies –so let's say for movie 27– let's assume  
there are five latent factors –I don't know what 
they're for–, they're just five latent factors,  
we'll figure them out later. And for now I 
certainly don't know what the value of those  
five latent factors for movie 27, so we're going 
to just chuck a little random numbers in them,  
and we're going to do the same thing for movie 49 
–pick another five random numbers– and the same  
thing for movie 57 –pick another five numbers. And 
you might not be surprised to hear we're going to  
do the same thing for each user, so for user 14: 
we're going to pick five random numbers for them,  
and for user 29: we'll pick 
five random numbers for them,  
and so the idea is that this number here 0.19 
is saying –if it was true– that user id 14  
feels not very strongly about the fact that for 
movie 27 has a value of 0.71. So therefore in here  
we do the dot product. The details of why don't 
matter too much but, well, actually you can  
figure this out from what we've said so far: if 
you go back to our definition of matrix product  
you might notice that the matrix product of a row 
with a column is the same thing as a dot product  
and so here in excel I have a row and a column 
so, therefore I say matrix multiply that by that:  
that gives us the dot product. So here's the dot 
product of that by that –or the matrix multiply,  
given that they're row and column. The only other 
slight quirk here is that if the actual rating  
is zero –is empty– I'm just going to leave it 
blank, I'm going to set it to zero actually.
So here is everybody's rating, 
predicted rating of movies.  
I say predicted –of course these are currently 
random numbers so they are terrible predictions.  
But when we have some way to predict things 
and we start with terrible random predictions,  
we know how to make them better, don't 
we?, we use static gradient descent.  
Now to do that we're going to need a 
loss function, so that's easy enough,  
we can just calculate the sum of x 
minus y squared divided by the count,  
that is the mean squared error –and if we take the 
square root, that is the root mean squared error–  
so here is the root mean squared error, in Excel, 
between these predictions and these actuals.
And so now that we have a loss function, 
we can optimize it. Data solver,  
set objective (this one here) by changing 
cells (these ones here) and (these ones here).  
Solve. Okay, and initially our loss is 2.81 –so 
we hope it's going to go down– and as it solves  
–not a great choice of background color– but 
it says 0.68 so this number is going down. So  
this is using… actually an Excel it's not quite 
using stochastic gradient descent because excel  
doesn't know how to calculate gradients, 
there are actually optimization techniques  
that don't need gradients: they calculate them 
numerically as they go but that's a minor quirk.  
One thing you'll notice is 
it's doing it very, very slowly  
–there's not much data here and it's still going– 
one reason for that is that… it's because it's  
not using gradients it's much slower and 
the second is Excel is much slower than  
Pytorch. Anyway, it's come up with an answer, 
and look at that: it's got to 0.42. So it's  
got a pretty good prediction and so, we can 
kind of get a sense of this, for example,  
looking at the last three,  
movie user 14 likes, dislikes, likes. Let's see 
somebody else like that. Here's somebody else,  
this person likes, dislikes, likes. So, based 
on our kind of approach we're saying: okay,  
since they have the same feeling about these three 
movies, maybe they'll feel the same about these  
three movies. So this person likes all three 
of those movies and this person likes two out  
of three of them so, you know, you kind of… this 
is the idea, right?, as if somebody says to you:  
“I like this movie, this movie, this movie” and 
you're like: “oh, they like those movies too” what  
other movies do you like? and they'll say: “oh, 
how about this?” There's a chance, good chance,  
that you're going to like the same thing. That's 
the basis of collaborative filtering, okay, it's…  
and mathematically we call this matrix completion. 
So this matrix is missing values, we just want to…  
complete them. So the core of collaborative 
filtering is, it's a matrix completion exercise.
Can you grab a microphone?
NICK:  
Is that better? Okay, my question was, 
is, with the dot products, right?, so,  
if we think about the math of that for a minute 
is, yeah, if we think about the cosine of the  
angle between the two vectors that's going 
to roughly approximate the correlation,  
is that essentially what's going 
on here in one sense? with…
JEREMY: Okay so is the cosine of the 
angle between the vectors much the same  
thing as the dot product? The answer is yes, 
they're the same, once you normalize them so.
Is that still on…
NICK: It’s correlation what we're 
doing here at scale, as well?
JEREMY: Yeah, you can, yeah, 
you can think of it that way.
NICK: Okay cool…
JEREMY: Now,  
this looks pretty different to how Pytorch looks.  
Pytorch has things in rows, right?, 
we've got a user, a movie rating,  
user movie rating, right? So, how do we 
do the same kind of thing in Pytorch?  
So, let's do the same kind of thing in Excel, 
but using the table in the same format that  
Pytorch has it. Okay. So to do that in Excel the 
first thing I'm going to do is I'm going to, see,  
okay, this… I've got to look at user number 
14 and I want to know what index –like how far  
down this list is 14. Okay, so we'll just match 
means find the index. So this is user index one.
And then what I'm going to 
do is, I'm going to say the…  
these five numbers is, basically I want to find  
row one over here, and in excel that's called 
OFFSET. So we're going to offset from here by  
one row, and so you can see here it is 0.19, 0.63, 
0.19, 0.63 et cetera, right? So here's the second  
user: 0.25, 0.83 etc. And we can do the same 
thing for movies, right? So movie 417 is index 14,  
that's going to be: 0.75, 0.47 et cetera. And so 
same thing, right?, but now we're going to offset  
from here, by 14, to get this row which is 0.75, 
0.47 et cetera. And so the prediction now is… the  
dot product is called SUMPRODUCT 
in excel, this is a SUMPRODUCT  
of those two things. So this is exactly the 
same as we had before, right?, but when we  
kind of put everything next to each other we 
have to, like manually, look up the index.
And so then for each one we can calculate 
the error squared prediction minus  
rating squared and then we could add those all 
up and –if you remember– this is actually the  
same root mean squared error we had before 
–we optimized before. 2.81 because we've  
got the same numbers as before and 
so this is mathematically identical.
So what's this weird word up here: 
“embedding”. You've probably heard it before  
and you might have come across the impression 
it's some very complex fancy mathematical thing,  
but actually, it turns out, that it is 
just looking something up in an array:  
that is what an embedding is. So 
we call this an “embedding matrix”,  
and these are our “user embeddings” 
and our “movie embeddings”.
So let's take a look at that in Pytorch, and, 
you know, at this point if you've heard about  
embeddings before you might be thinking: that 
can't be it. And yeah, it's just as complex  
as the rectified linear unit which turned 
out to be: replace negatives with zeros.  
Embedding actually means: “look something up in 
an array”. So there's a lot of things that we use,  
as deep learning practitioners, to try 
to make you as intimidated as possible  
so that you don't wander into our territory 
and start winning our Kaggle competitions.  
And unfortunately, once you 
discover the simplicity of it,  
you might start to think that you can do 
it yourself and then it turns out you can.  
So yeah, that's what basically, it turns out 
pretty much all of this jargon turns out to be.  
So we're going to try to 
learn these latent factors,  
which is exactly what we just did in 
Excel, we just learned the latent factors.
All right, so, if we're going to learn things 
in Pytorch we're going to need data loaders.  
One thing I did is, there is actually a movies  
table as well, with the names of the movies, so 
I merged that together with the ratings so that  
then we've now got the user id and the actual name 
of the movie –we don't need that, obviously, for  
the model, but it's just going to make it a bit 
more fun to interpret later–. So this is called:  
ratings. We have something called 
CollabDataLoaders –so collaborative filtering data  
loaders– and we can get that from a data frame, 
by passing in the data frame, and it expects a  
user column and an item column. So the user column 
is what it sounds like: the person that is rating  
this thing, and the item column is the product or 
service that they're rating. In our case the user  
column is called “user” –so we don't have to pass 
that in– and the item column is called “title”  
–so we do have to pass this in– because by 
default the user column should be called “user”  
and the item column will be called “item”. Give it 
a batch size, and as usual we can call show batch,  
and so, here's our data loaders… a batch 
of data loaders –or at least a bit of it–.  
And so now, since we told it the names, we 
actually get to see the names which is nice.
All right, so, now we're going to create 
the user_factors and movie_factors -ie-  
this one and this one. So the number of 
rows of the movie factors will be equal to  
the number of movies and the number of rows of 
the user factors will be equal to the number of  
users. And the number of columns will be whatever 
we want: however many factors we want to create.
John!
JOHN: This might be a pertinent 
time to jump in with a question:  
any comments about choosing the number of factors
JEREMY: Uhm… not really. We have defaults 
that we use –for embeddings– in fastai.  
It's a very obscure formula and people often 
ask me for like the mathematical derivation of  
where it came from, but what actually happened 
is, it's, I wrote down how many factors I think  
is appropriate for different size categories on a 
piece of paper in a table –well actually in Excel–  
and then I fitted a function to that and 
that's the function. So it's basically a  
mathematical function that fits my 
intuition about what works well.  
But it seems to work pretty well, I've 
seen it’s used in lots of other places now,  
lots of papers will be like: “using fastai's rule 
of thumb for embedding sizes… here's the formula…”
JOHN: Cool, thank you.
JEREMY: It's pretty fast to train 
these things so you can try a few.
So we're going to create… so the number 
of users is just the length of how many  
users there are, number of movies is 
the length of how many titles there are;  
so create a matrix of random numbers of 
users by five and movies: of movies by five.
And now we need to look up the index of the 
movie in our movie latent factor matrix.  
The thing is, when we've learned about 
deep learning, we learnt that we do matrix  
multiplications, not lock-something-up 
in a matrix –in an array. So in Excel  
we were saying OFFSET, which is to say, 
find element number 14 in the table;  
which, that's not a matrix multiply, 
how does that work? Well actually it is,  
it actually is for the same 
reason that we talked about  
here, which is: we can represent 
–find– the element number-one-thing  
–in this list– is actually the same as multiplying 
by a one hot encoded matrix. So remember how,  
if we –let's just take off the log for a moment.
Look, this has returned 0.87 –and particularly if 
I take the negative off here, if I add this up–  
this is 0.87, which is the result of finding 
the index number-one-thing in this list.  
But we didn't do it that way, we did this by 
taking the dot product of this (sorry) of this  
and this, but that's actually the same thing, 
right? taking the dot product of a one hot encoded  
vector with something is the same as looking 
up this index in the vector. So that means that  
this exercise here of looking up the 14th thing 
is the same as doing a matrix multiply with a  
one-hot-encoded vector. And we can see that here: 
this is how we create a one hot encoded vector  
of length and users in which the third element 
is set to one and everything else is zero.
And if we multiply that –so 
“at” (@) means, to remember,  
matrix multiply in python– so if we 
multiply that by our user_factors,  
we get back this answer. And if we 
just ask for user_factors number three  
we get back the exact same answer –they're the 
same thing. So you can think of “an embedding”  
as being a computational shortcut for multiplying 
something by a one-hot-encoded vector.
And so if you think back to what we did with 
dummy variables, right, this basically means  
embeddings are like a cool math trick 
for speeding up doing matrix multipliers  
with dummy variables. Not just speeding up. We 
never even have to create the dummy variables.  
We never have to create the one-hot-encoded 
vectors. We can just look up in an array.
All right, so we're now ready 
to build a collaborative filter…  
a collaborative filtering model and 
we're going to create one from scratch.  
And as we've discussed before, 
in Pytorch a model is a class.
And so, we briefly touched on this, 
but I've got to touch on it again.  
This is how we create a class 
in Python. You give it a name,
and then you say how to initialize 
it, how to construct it. So in Python,  
remember they call these things “dunder 
whatever”. This is dunder init, these are magic  
methods that Python will call for you at 
certain times. The method called dunder init  
is called when you create an object of this class. 
So we could pass it a value, and so now we set the  
attribute called “a equal to that value”, and so 
then later on we could call a method called “say”,  
that will say hello to whatever you passed 
in here. And this is what it will say. So,  
for example, if you construct an object 
of type Example(), passing in Sylvain,  
self.a now equals Sylvain. So if you say use the 
dot method, the say method “nice to meet you”,  
x is now “nice to meet you”. So it will 
say hello Sylvain, nice to meet you.  
So that's… that's kind of all you need 
to know about object-oriented programming  
in Pytorch to create a mode. Oh, there's one 
more thing we need to know, sorry, which is  
you can put something in 
parentheses after your class name,  
and that's called the superclass. It's basically 
going to give you some stuff for free, give you  
some functionality for free. And if you create 
a model in Pytorch, you have to make Module your  
super class. This is actually fastai's version 
of Module, but it's nearly the same as Pytorch’s.
So when we create this dot product object, it's 
going to call dunder init, and we say well how  
many users are going to be in our model, 
and how many movies, and how many factors,  
and so we can now create an 
embedding of users by factors  
for users, and an embedding of movies 
by vectors for movies, and so then
Pytorch does something quite magic, 
which is that if you create a
dot product object like so, it then… you can 
treat it like a function. You can call it  
up and calculate values on 
it. And when you do that,  
this is really important to know, Pytorch is going 
to call a method called “forward” in your class.  
So this is where you put your calculation of 
your model. It has to be called “forward”,  
and it's going to be passed the object itself, 
and the thing you're calculating on. In this case,  
the user and movie for a batch. So this is your 
batch of data, each row will be one user and movie  
combination, and the columns will be users 
and movies. So we can grab the first column,  
right, so this is every row of the first column, 
and look it up in the user factors embedding,  
to get our users embeddings. So that is the same 
as doing this. Let's say this is one mini batch.
And then we do exactly the same thing for the 
second column, passing it into our movie factors  
to look up the movie embeddings, and 
then take the dot product. dim equals one  
because we're summing across the columns for each 
row. We're calculating a prediction for each row
so once we've got that
we can pass it to a learner, passing 
in our data loaders, and our model,  
and our loss function mean squared 
error, and we can call fit.
And the way it goes.
And this by the way is running on cpu. Now 
these are very fast to run. So this is doing  
100 000 rows in 10 seconds, which is a whole 
lot faster than our few dozen rows in Excel.
and so you can see the loss going 
down. And so we've trained a
model. Um…
it's not going to be a great model,  
and one of the problems is that, let's 
see if we can see this in our Excel one…
Look at this one here. This 
prediction is bigger than five.
But nothing's bigger than five. 
So that seems like a problem.  
We're predicting things that are bigger 
than the highest possible number.
And in fact these are very much movie 
enthusiasts that… nobody gave anything a one.  
Yeah nobody even gave anything a one here. So…
do you remember when we learned about Sigmoid. 
The idea of squishing things between zero and one.  
We could do stuff still without a Sigmoid, 
but when we added a Sigmoid, it trained better  
because the model didn't have to work so 
hard to get it, kind of, into the right zone.  
Now if you think about it, if you take something 
and put it through a Sigmoid, and then multiply it  
by five, now you've got something that's going 
to be between zero and five. We used to have  
something which is between zero and one, So we 
could do that in fact we could do that in Excel.
I'll leave that as an exercise to the 
reader. Let's do it over here in Pytorch.
So if we take the exact same class as before  
and this time we call sigmoid_range and 
so sigmoid_range is something which will  
take our prediction and then squash it into our 
range and by default we'll use a range of zero  
through to 5.5. so it can't be smaller than zero, 
can't be bigger than 5.5. Why didn't I use five?  
That's because a sigmoid can never hit one, 
right? and a sigmoid times five can never hit five  
but some people do give things... movies a five so 
you want to make it a bit bigger than our highest.
So this one got a loss of 0.8628
86 oh it's not better. Isn't that always the way?  
All right, didn't actually 
help; doesn't always. So be it.
Let's keep trying to improve it. 
Let me show you something I noticed.  
Some of the users, like this one... this 
person here just loves movies. They give  
nearly everything a four or five. Their worst 
score is a three, all right? This person... oh,  
here's a one. This person's got much more range. 
Some things are twos, some ones, some fives.
This person doesn't seem to like movies very 
much considering how many they watch. Nothing  
gets a five. They've got discerning tastes, I 
guess. At the moment we don't have any way in  
our kind of formulation of this model to say this 
user tends to give low scores and this user tends  
to give high scores. There's just nothing like 
that, right? But that would be very easy to add.
Let's add one more number to 
our five factors, just here,  
for each user and now, rather than doing 
just the matrix multiply let's add...
Oh it's actually the top one. Let's add this 
number to it h19 and so for this one let's  
add i19 to it. Yeah so I've got it wrong. 
This one here, so this... this row here...
We're going to add to each rating and then 
we're going to do the same thing here.
Each movie's now got an extra number 
here that again we're going to  
add a 26. So it's our matrix multiplication 
plus, we call it, the bias. The user bias plus  
the movie bias so effectively that's like making 
it so we don't have an intercept of zero anymore.
And so if we now train this model...
Data -> Solver -> Solve.
So previously we got to 0.42, okay? and so 
we're going to let that go along for a while  
and then let's also go back and 
look at the Pytorch version.  
So for Pytorch, now we're going to 
have a user_bias which is an embedding  
of n_users by 1, right? Remember there was just 
one number for each user and movie bias is an  
embedding of n_movies also by 1 and so we can now 
look up the user embedding the movie embedding,  
do the dot product and then look up the 
user_bias and the movie_bias and add them.
Chuck that through the sigmoid. Let's 
train that, see if we beat 0.865.
Wow we're not training very well, 
are we? Still not too great. 0.894.  
I think Excel normally does 
do better though. Let's see.
Okay Excel... oh Excel's done a lot 
better. It's gone from 0.42 to 0.35.
Okay so what happened here? Why did it get worse? 
Well, look at this the valid loss got better  
and then it started getting worse again. 
So we think we might be overfitting,
which you know we have got a lot of parameters in 
our embeddings. So how do we avoid overfitting?
So a classic way to avoid overfitting 
is to use something called weight decay.
Also known as L2 regularization, 
which sounds much more fancy.
What we're going to do
is when we can compute the gradients, we're 
going to first add to our loss function,  
the sum of the weights squared. This is something 
you should go back and add to your titanic model,  
not that it's overfitting, but just to 
try it, right? So previously our gradients  
have just been and our loss function has 
just been about the difference between our  
predictions and our actuals, right? And so 
our gradients were based on the derivative  
of that with respect to the derivative 
of that with respect to the coefficients  
but we're saying now “let's add the 
sum of the square of the weights  
times some small number”. So what 
would make that loss function go down?  
That loss function would go 
down if we reduce our weights.
For example if we reduce all 
of our weights to zero...  
I should say we reduce the magnitude of our 
weights. If we reduce them all to zero, that  
part of the loss function will be zero because 
the sum of zero squared is zero. Now, problem is  
if our weights are all zero, our model doesn't do 
anything, right? So we'd have crappy predictions.  
So it would want to increase the weights so 
that's actually predicting something useful.
But if it increases the weights too much then it 
starts overfitting. So how is it going to actually  
get the lowest possible value of the 
loss function? By finding the right mix.  
Weights not too high, right? But high 
enough to be useful at predicting.
If there's some parameter that's not 
useful, for example, say we asked for  
five factors and we only need four,  
it can just set the weights for the fifth 
factor to zero, right? And then problem solved,  
right? It won't be used to predict anything but 
it also won't contribute to our weight decay part.
So previously we had something calculating the 
loss function. So now we're going to do exactly  
the same thing but we're going to square 
the parameters, we're going to sum them up,  
and we're going to multiply them by 
some small number like 0.01 or 0.001.
And in fact we don't even need to do this  
because remember the whole purpose of the loss is 
to take its gradient, right? And to print it out.  
The gradient of parameters squared is two times 
parameters. It's okay if you don't remember that  
from high school but you can take my word for 
it. The gradient of y equals x squared is 2x.  
So actually all we need to do 
is take our gradient and add  
the weight decay coefficient 0.01 or whatever 
times two times parameters. And given this is just  
number... some number we get to pick, we might as 
well pull the 2 into it and just get rid of it.
So when you call fit you can pass in a wd 
parameter which does... adds this times the  
parameters to the gradient for you. And 
so that's going to ask the model...it's  
going to save the model... “please don't make 
the weights any bigger than they have to be”.
And yay! Finally our loss actually improved, okay? 
And you can see it getting better and better.
In fastai applications like vision we 
try to set this for you appropriately  
and we generally do a reasonably good job. Just 
the defaults are normally fine. But in things  
like tabular and collaborative filtering, we don't 
really know enough about your data to know what  
to use here. So you should just try a few things. 
Let’s... try a few multiples of 10. Start at 0.1  
and then divide by 10 a few times, you know, and 
just see which one gives you the best result.
So this is called regularization. So 
regularization is about making your bottle...  
model no more complex than it has to be, right? 
It has a lower capacity and so the higher the  
weights, the more they're moving the model around, 
right? So we want to keep the weights down but not  
so far down that they don't make good predictions 
and so the value of this if it's higher, will keep  
the weights down more, it will reduce overfitting 
but it will also reduce the capacity of your model  
to make good predictions and if it's lower it 
increases the capacity of model and increases
overfitting. All right,  
I'm going to take this bit for next time. Before 
we wrap up, John, are there any more questions?
JOHN: Yeah there are. The... there's 
some from... from back at the start of  
the collaborative filtering. So we had a bit of 
a conversation a while back about this... the  
size of the embedding vectors and you talked 
about your fastai rule of thumb. So there was  
a question if anyone has ever done a kind of 
a hyperparameter search, an exploration for…
JEREMY: I mean people often will do 
a hyperparameter search, for sure.
JOHN: I beg your pardon.
JEREMY: People will often do a hyperparameter 
search for their model but I haven't seen a...  
I haven't seen any other rules 
other than my rule of thumb.
JOHN: Right, so not not 
productively to your knowledge?
JEREMY: Oh productively for an 
individual model that somebody's built.
JOHN: And then there's a... there's a question 
here from Zakia which I didn't quite wrap my head  
around so Zakia if you want to maybe clarify in 
the... in the chat as well but “can recommendation  
systems be built based on average ratings of users 
experience rather than collaborative filtering?”
JEREMY: Not really, right? I mean if you've got 
lots of metadata, you could, right? So if you've  
got, you know, like lots of information about 
demographic data about where the user's from and  
you know what loyalty scheme results they've 
had and blah blah blah and then for products  
there's metadata about that as well then sure 
averages would be fine but if all you've got is  
kind of purchasing history then you really want 
the granular data otherwise how could you say  
like... they like this movie, this movie, and this 
movie therefore they might also like that movie  
or you've got... it's like oh they kind of like 
movies. There's just not enough information there.
JOHN: Yeah great. That's about it.
thanks okay great alright thanks everybody 
see you next time for our last lesson