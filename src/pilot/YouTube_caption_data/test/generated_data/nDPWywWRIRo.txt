Stanford University hello hi so I want
to get started welcome to CS 231 on
lecture 11 we're going to talk about
today detection segmentation and a whole
bunch of other really exciting topics
around sort of core computer vision
tasks but as usual a couple
administrative notes so so last time you
obviously took the midterm we didn't
have lecture hopefully that went okay
for all of you but so we're going to
work on grading the midterm this week
and but as a reminder please know don't
make any public discussions about the
midterm questions or answers or whatever
until at least tomorrow because there
are still some people taking makeup
midterms today and throughout the rest
of the week so we just ask you that you
refrain from talking publicly about
midterm questions
why don't you wait until Monday okay
great
so we're also starting to work on
midterm grading we'll get those back -
as soon as you can as soon as we can
we're also starting to work on grading
assignment - so there's a lot of grading
being done this week the TAS are pretty
busy
I'll also reminder for you guys
hopefully you've been working hard on
your projects now that your most of you
are done with the midterm so your
project milestones will be due on
Tuesday so any any sort of last-minute
changes that you had in your projects I
know some people decided to switch
projects after the proposal some teams
reshuffled a little bit that's fine but
just your your milestone should reflect
the project that you're actually doing
for the rest of the quarter so hopefully
that's going out well I know there's
been a lot of worry and stress on Piazza
wondering about assignment three so
we're we're working on that as hard as
we can but that's actually a bit of a
new assignment it's changing a bit from
last year so it will be out as soon as
possible hopefully today or tomorrow
although we promise that whenever it
comes out you'll have two weeks to
finish it so don't try not to stress out
about that too much
but I'm pretty excited I think
assignment 3 will be really cool it has
a lot of cool it'll cover a lot of
really cool material so another another
thing last time in lecture we mentioned
this thing called the train game which
is this really cool thing we've been
working on sort of as a side project a
little bit so this is an interactive
tool that you guys can go on and use to
explore a little bit the process of
tuning hyper parameters in practice so
we hope that so this is again totally
not required for the course totally
optional but if you do but we will offer
a small amount of extra credit for those
of you who want to do well and
participate on this and we'll send out
exactly some more details later this
afternoon on Piazza but just a bit of a
demo for what exactly is this thing so
you get to you'll get to go in and we've
changed the name from train game to
hyper quest because you're questing to
solve to find the best hyper parameters
for your model so this is really cool
it'll be an interactive tool that you
can use to explore the training of hyper
parameters interactively in your browser
so you'll log in with your student ID
and name you will fill out fill out a
little survey with some of your
experience on deep learning then you'll
your
you'll read some instructions so end up
in this game you'll be shown some random
data set on every trial
this data set might be images or it
might be vectors and your goal is to
train a model by picking the right hyper
parameters interactively to perform as
well as you can on the validation set of
this random data set and it'll sort of
keep track of your performance over time
and will be a leaderboard it'll be
really cool so every time you play the
game you'll get some statistics about
your data set in this clay in this case
we're doing a classification problem
with ten classes you can see down at the
bottom you have these statistics of our
random data set we have ten classes the
input data size is three by 32 by 32 so
this is some image data set and we can
see that in this case we have 8,500
examples in the training set and 1,500
examples in the validation set these are
all random they'll change a little bit
every time based on these data sets of
statistics you'll make some choices on
your initial learning rate your initial
network size in your initial dropout
rate then you'll see a screen like this
where it'll it'll run one epoch with
those chosen hyper parameters show you
on the bottom on the on the right here
you'll see two plots one is your
training and validation loss for that
first epoch then you'll see your
training and validation accuracy for
that first epoch and based on the gaps
that you see in these two graphs you can
make choices interactively to change the
learning rates and the hyper parameters
for the next epoch so then you can
either choose to continue training with
with the current or change hyper
parameters you can also stop training or
you can revert to go back to the
previous check point in case things got
really messed up so then you'll get to
make some choice so here we'll decide to
continue training and in this case you
could go and set new learning rates and
new hyper parameters for the next epoch
of training you can also kind of
interesting here you can actually grow
and you can actually grow the network
interactively during training in this
demo we there's this cool is this cool
trick from a couple recent papers where
you can either take existing layers and
make them wider or add new layers so the
network in the middle of training while
still maintaining the same function in
the in the network so you can do that to
increase the size of your network in the
middle training here which is kind of
cool so then you'll make choices over
several epochs and eventually your final
valid
and accuracy will be recorded and we'll
have some leaderboard that compares your
score on that data set to some simple
baseline models and depending on how
well you do on this leaderboard will
again offer some small amounts of extra
credit for those of you who choose to
participate so this is again totally
optional but I think it can be a really
cool learning experience for you guys to
play around with and explore how hyper
parameters affect the learning process
also it's really useful for us you'll
help help science out by participating
this experiment we're pretty interested
in seeing how people behave when they
chew when they train neural networks so
you'll be helping us out as well if you
decide to play this but again totally
optional up to you any any questions any
questions on that
hopefully at some point but it's a so
the question was will this be a paper or
whatever eventually hopefully but it's
like really early stages of this project
so I can't make any any promises but I
hope so but I think it'll be really cool
yeah so the question is how can you add
layers during training I don't really
want to get into that right now but the
paper to read is net to net by Ian with
Ian's good fellows one of the authors
and there's another paper or paper from
Microsoft called Network morphism so if
you read those two papers you can see
how this works okay so last time a bit
of a reminder before we had the midterm
last time we talked about recurrent
neural networks we saw that recurrent
neural networks can be used for
different types of problems in addition
to one-to-one we can do one-to-many
many-to-one many to many we saw how this
can apply to language modeling and we
saw some cool examples of applying
neural networks to model different sorts
of languages at the character level and
we sampled these artificial math and
Shakespeare and C source code we saw
also saw how similar things could be
applied to image captioning by
connecting a CNN feature extractor
together with an RNN language model and
we saw some really cool examples of that
we also talked about different types of
RN ends we talked about this vanilla RN
and I also want to mention that this is
sometimes called a simple RN n or an
element RN n so you'll see all of these
different terms in literature we also
talked about the long short term memory
or LST M and we talked about how the
gradients the LS GM has this crazy set
of equations but it makes sense because
it helps improve gradient flow during
back propagation and helps this thing
model more log returned dependencies in
our sequences so today we're going to
switch gears and talk about a whole
bunch of different exciting tasks we're
going to talk about so so far we've been
talking about mostly the image
classification problem today we're going
to talk about various types of other
computer vision tasks where you actually
want to go in and make say things about
the spatial pixels inside your images so
we'll see segmentation localization
detection a couple other different
computer vision tasks and how you can
approach these with convolutional neural
networks so as a bit of refresher so far
the main thing we've been talking about
in this class is image classification so
here we're going to have some input
image come in that input image will go
through some deep convolutional network
that network will give us some some
feature vector of maybe 4096 dimensions
in the case of Alex tete or V Gigi and
then from that final final feature
vector will have some fully connected
some final fully connected layer that
gives us
one thousand numbers for the different
class scores that we care about we're
one thousand is maybe the number of
classes in imagenet in this example and
and then at the end of the day the whole
what the network does is we input an
image and then we output a single
category label saying what is the
content of this entire image as a whole
but this is kind of the maybe just the
most basic possible task in computer
vision and there's a whole bunch of
other interesting types of tasks that we
might want to solve using deep learning
so today we're going to talk about
several of these different tasks and
step through each of these and see how
they all work with deep learning so
we'll talk about these more by more as
more in detail about what each problem
is as we get to it but this is kind of a
summary slide that we'll talk first
about semantic segmentation we'll talk
about classification and localization
then we'll talk about object detection
and finally a couple brief words about
instance segmentation so first is the
problem of semantic segmentation so when
we're the problems in the problem of
semantic segmentation we want to input
an image and then output a decision of a
category for every pixel in that image
so for every pixel in this in so this
input image for example is this cat
walking through the field he's very cute
and in the output we want to say for
every pixel is that pixel cat or grass
or sky or trees or background or or some
other set of categories so we're going
to have some set of categories just like
we did any image classification case but
now rather than assigning a single
category label to the entire image we
want to produce a category label for
each pixel of the input image and this
is called semantic segmentation so one
interesting thing about semantic
segmentation is that it does not
differentiate instances so in this
example on the right we have this image
with two cows where they're standing
right next to each other and when we're
talking about semantic segmentation
where we're just labeling all the pixels
independently for what is the category
of that pixel so in the case like this
where we have two cows right next to
each other the output does not make any
distinguishing does not make its not
distinguish between these two cows
instead we just get a whole mass of
pixels that are all labeled as cow so
this is a bit of a shortcoming of
semantic segmentation and we'll see how
we can fix this later when we move to
instant segmentation but at least for
now we'll just talk about semantic
segmentation first
so you can imagine maybe using a classic
so one potential approach for attacking
semantic segmentation might be through
classification so there's this you could
use this idea of a sliding window
approach to semantic segmentation so you
might imagine that we take our input
image and we break it up into many many
small tiny local crops of of the image
so in this example we've taken maybe
three crops from around the head of this
of this cow and then you could imagine
taking each of those crops and now
treating this as a classification
problem saying for this crop what is the
category of the central pixel of the
crop and then we can use all this we
could use all the same machinery that
we've developed for classifying entire
images but now I'll just apply it on
crops rather than on the entire image
and this this would probably work to
some extent but it's probably not a very
good idea so this would end up being
super super computationally expensive
because this because we want to glaybels
in the image we would need a separate
crop for every pixel in that image and
this would be super super expensive to
run forward and backward passes through
and moreover we're actually we're
actually if you think about this we can
actually share computation between
different patches so if you're trying to
classify two patches that are right next
to each other and actually overlap then
the convolutional features of those
patches will end up going through the
same convolutional layers and we can
actually share a lot of the computation
when applying this to separate passes or
when applying this type of approach to
separate patches in the image so this is
actually a terrible idea and nobody does
this and you should probably not do this
but it's at least kind of the first
thing you might think of if you were
trying to think about semantic
segmentation then the next idea that
works a bit better is this idea of a
fully convolutional net work right so
rather than extracting individual
patches from the image and classifying
these patches independently we can
imagine just having our network be a
whole giant stack of convolutional
layers with no fully connected layers or
anything so in this case we just have a
bunch of stat a bunch of convolutional
layers that are all maybe three by three
with zero padding or something like that
so that each convolutional layer
preserves the spatial size of the end
and now if we pass our image through a
whole set a whole stack of these
convolutional layers then the final
convolutional layer could just output
output a tensor of something by like C
by H by W where C is the number of
categories that we care about and you
could see this tensor as just giving our
classification scores for every pixel in
the input image at every location in the
input image and we come and we could
compute this all at once with just some
giant stack of convolutional layers and
then you can imagine training this thing
by putting a classification loss at
every pixel of this output taking an
average over those pixels or in space
and just training this kind of network
through normal regular back propagation
question
oh the question is how do you develop
training data for this it's very
expensive right so the training data for
this would be we need to label every
pixel in those input images so there's
tools that people sometimes have online
where you can go in and sort of draw
contours around the objects and then
fill in the fill in regions but in
general getting this kind of training
data is very expensive
you
yeah the question is what is the loss
function so here we since we're making a
classification decision per pixel then
we put a cross entropy loss on every
pixel of the output so we have the
ground truth category label for every
pixel in the output then we when we
compute a cross entropy loss between
every pixel in the output and the ground
truth pixels and then taking either a
sum or an average over space and then
sum or average or the mini-batch
question
yeah yeah
you
the question is do we assume that we
know the categories so yes we do assume
that we know the categories up front so
this is just like the image
classification case so an image
classification we know it at the start
of training based on our data set that
maybe there's 10 or 20 or 100 or a
thousand classes that we care about for
this data set and then here we are we
are fixed to that set of classes that
are fixed for the data set so so this
this model is relatively simple and you
can imagine this this working reasonably
well assuming that you tuned all the
hyper parameters right but it's kind of
a problem right so in this setup
since we're applying a bunch of
convolutions that are all keeping the
same spatial size of the end of the
input image this would be super super
expensive right if you if you wanted to
do convolutions that maybe have 64 128
or 256 channels for those convolutional
filters which is pretty common in a lot
of these networks then running those
convolutions on this high resolution
input image over a sequence of layers
would be extremely computationally
expensive and would take a ton of memory
so in practice you don't usually see
networks with this architecture instead
you tend to see networks that look
something like this where we have some
down sampling and then some up sampling
of the feature map inside the image so
rather than doing all the convolutions
at the full spatial resolution of the
image will maybe go through a small
number of convolutional layers at the
original resolution then down sample
that feature map using something like
mike max pooling or strata convolutions
and sort of down sample down sample so
we have convolutions and down sampling
and convolutions and down sampling that
look much like a lot of the
classification networks that you see but
now the difference is that rather than
transitioning to a fully connected layer
like you might do in an image
classification setup instead we want to
increase the spatial resolution of our
predictions on the threat in the second
half of the network so that our output
image can now be the same size as our
input image and this ends up being much
more computationally efficient because
you can make the network very deep and
work at a lower spatial resolution for
many of the layers at the inside of the
network so we've already talked about
we've already seen examples of down
sampling when it comes to image class
but when it comes to convolutional
networks we've seen that you can do
strata convolution
or various types of pooling to reduce
the spatial size of the image in inside
a network but we haven't really talked
about up sampling and the question you
might be wondering is how what are these
upsampling layers actually look like
inside the network and what are our
strategies for increasing the size of a
feature map inside the network sorry was
there a question in the back
yeah so the question is how do we up
sample and the answer is that's the
topic of the next couple of slides so
one so one one strategy for up sampling
is something like uncool
we have this notion of pooling in to
down sample so we talk about average
pooling or max pooling so when we talk
about average pooling we're kind of
taking a spatial average within a
receptive field of each pooling region
one kind of analog for up sampling is
this idea of nearest neighbor uncool so
here on the Left we see this example of
nearest neighbor uncool Inge in put is
maybe some 2x2 grid and our output is a
four by four grid and now in our outputs
we've done a 2x2 stride to nearest
neighbor uncool Anor up sampling where
we've just duplicated that element for
every point in our 2x2 receptive field
of the of the unpooled region another
thing you might see is this bed of nails
on pulling or bed of nails up sampling
where you'll just take where again we
have a 2x2 receptive field for our
uncool in regions and then you'll take
the in this case we you put that you you
make it all zeros except for one element
of the uncool in region so in this case
we've put all we've taken all of our
inputs and always put them in the upper
left hand corner of this uncool in
region and everything else is zeros and
this is kind of like a bed of nails
because the the zeros are like very flat
then you've got these like things poking
up of the for the values at these
various nonzero regions another thing
that you see sometimes which was alluded
to by the question a minute ago is this
idea of max uncool so in a lot of these
networks they tend to be symmetrical
where we have kind of where the we have
a down sampling portion of the network
and then an up sampling portion of the
network with sort of a symmetry between
those two portions of the network so
sometimes what you'll see is this idea
of max on pooling where when for for
each uncool enforce am player it is
associated with one of the pooling
layers in the first half of the network
and now in the first half in the down
sampling when we do max pooling will
actually remember which element of the
receptive field during max pooling was
used to do the max pooling and now when
we go through the rest of the network
then we'll do something that looks like
this
of nails upsampling except rather than
always putting the elements in the same
position instead we'll stick it into the
position that was used in the
corresponding max pooling step earlier
in the network I'm not sure if that
explanation was clear but hopefully the
picture makes sense
the vs so then you just end up filling
the rest of zeros so then you fill the
rest with zeros and then you stick the
elements from the slow resolution patch
up into the high resolution patch at the
at the points where the max cooling took
place at the at the corresponding max
pulling there okay so that's that's
that's kind of an interesting idea sorry
question
oh yeah so the question is why is this a
good idea why might this matter so the
idea is that we when we're doing
semantic segmentation we want our
predictions to be pixel perfect right we
kind of want to get that should get
those sharp boundaries and those those
tiny details in our in our predictive
segmentation but now if if we're so now
if you're doing this max pooling there's
a sort of heterogeneity that's happening
inside the feature map due to the max
pooling we're in from the low resolution
image you don't know like with like
you're sort of losing spatial
information in some sense by like you
don't know where that where that feature
vector came from in the local receptive
field after max pooling so when you do
if you actually unpooled by putting the
vector in the same slot you might think
that that might help us handle these
fine details a little bit better and
help us preserve some of that spatial
information that was lost during max
pooling question
you
the question is the same thing is easier
for back prop and yeah I guess you don't
you'll I don't think it changes the back
prop dynamics too much because storing
these indices it's not a huge
computational overhead they're pretty
small in comparison to everything else
so another thing that you'll see
sometimes is this idea of transpose
convolution so a transpose convolution
so for these for these various types of
unpooled that we just talked about these
bed of nails this nearest neighbor this
Maxon pooling all of these are kind of a
fixed function they're not really
learning exactly how to do the how to do
the up sampling so if you think about
something like strata convolution
striated convolution is kind of like a
learnable layer that learns the way that
it the way that the network wants to
perform down sampling at that layer and
in by analogy with that there's this I
there's this type of layer called a
transpose convolution that lets us do a
kind of learn about sampling so it will
both up sample the feature map and learn
some weights about how it wants to do
that up sampling but and this is really
just another type of convolution so to
see how this works remember how like a
normal 3x3 stride one pad one
convolution would work that for this
kind of normal convolution that we've
seen many times now in this class our
input might be four by four our output
might be four by four and now we'll have
this three by three kernel and we'll
take an inner product between will plop
down that kernel at the corner of the
image take an inner product and that
inner product will give us the value in
the activation in the upper left-hand
corner of our output and we'll repeat
this for every every receptive field in
the image now if we talk about strided
convolution then striated convolution
ends up looking pretty similar however
now our input is maybe a four by four
region and our output is a two by two
region but we still have this idea of
taking of there being some three by
three filter or kernel that we plopped
down on the corner of the image taken in
a product and use that to compute the
value of the activation in the output
but now with striated convolution the
idea is that we're moving that rather
than stepping every rather than plopping
down that filter at every possible point
in the input instead we're going to move
the filter by two pixels in the input
every time we move the filter by one
pixel every time we move by one pixel in
the output right so this stride of two
gives us a ratio between how much do we
move in the input versus how much do we
move in the output so when you do a
stride
convolution with stride - this ends up
down sampling the image by the feature
map by a factor of two in kind of a
learn above a and now a transpose
convolution is sort of the opposite in a
way so here our input will be a two byte
maybe a two by two region and our output
will be a four by four region but now
the operation that we perform with
transpose convolution is a little bit
different now rather so rather than
taking an inner product instead what
we're going to do is we're going to take
the value of our input feature map at
that upper left-hand corner and that
there will be some scalar value in the
upper left-hand corner we're going to
multiply the filter by that scalar value
and then copy those values over to this
3x3 region in the output so what so
rather than taking an inner product with
our filter and the input instead our
input gives weights that we will use to
weight the filter and then copy those
cuz and then our output will be weighted
copies of the filter that are weighted
by the values in the input and now now
we now we can do this sort of the same
ratio trick in order to up sample so now
when we move one pixel in the input now
we can plop our filter down two pixels
away in the output and it's the same
trick that now the blue pixel on the
input is some scalar value and we'll
take that scalar value multiply it by
the values in the filter and copy those
those weighted filter values into this
new region in the output the tricky part
is that sometimes these receptive fields
in the output can overlap now and now
when these when these receptive fields
in the output overlap we just sum the
results in the output so then you can
imagine repeating this everywhere and we
repeating this process everywhere and
this ends up doing sort of a learn about
sampling where we use that these learn
convolutional filter weights to up
sample the image and increase the
spatial size by the way you'll see this
operation go by a lot of different names
in the literature sometimes this gets
called things like D convolution which i
think is kind of a bad name but you'll
see it out there in papers so from a
signal positive processing perspective
deconvolution means the inverse
operation to convolution which is which
this is not however you'll frequently
see this type of layer call
a deconvolution layer in some deep
learning papers so be aware of that
watch out for that terminology you'll
also sometimes see this called up
convolution which is kind of a cute name
sometimes it's called fractionally
strided convolution because if we think
of the stride as the ratio in step
between the input and the output then
now this this is something like a stride
1/2 convolution because of this ratio
this ratio of 1 to 2 between steps in
the input and steps in the output this
also sometimes gets called a backward
strided convolution because if you think
about it and work and work through the
math this ends up being the same the
forward pass of a transpose convolution
ends up being the same mathematical
operation as the backwards pass in a
normal convey this looks like I think
it's maybe a little easier to see in one
dimension so if we imagine a so here
we're doing a 3 by 3 transpose
convolution in one dimension oh sorry
not 3 by 3 a 3 by 1 transpose
convolution in one dimension so our
filter here is just three numbers or
input is two numbers and now you can see
that in our output we've taken the
values in the input use them to weight
the values of the filter and plops down
those weighted filters in the output
with a stride of two and now we're these
receptive fields overlap in the output
then we sum so you might be wondering
this is kind of a funny name where does
the name transpose convolution come from
and why is that actually my preferred
name for this operation so that comes
from this kind of neat interpretation of
convolution so it turns out that any
time you do convolution you can always
write convolution as a matrix
multiplication so again this is kind of
easier to see with a one dimensional
example but here we've got some some
weight sum so we're doing a
one-dimensional convolution of a weight
vector X which is three elements and an
input vector a vector which has four
elements ABCD so here we're doing a
three by one convolution with stride one
and you can see that we can frame this
whole operation as a matrix
multiplication where we we take our our
convolutional kernel x
and turn it into some matrix capital X
which contains copies of that
convolutional kernel that are offset by
different regions and now we can take
this giant weight matrix X and do a
matrix vector multiplication between X
and our input a and this just produces
the same result of convolution and now
transpose convolution means that we're
going to take this same weight matrix
but rather than but now we're going to
multiply by the transpose of that same
weight matrix so here you can see the
same example for this stride one
convolution on the left and the
corresponding stride 1 transpose
convolution on the right and if you work
through the details you'll see that when
it comes to stride 1 astride 1 transpose
convolution also ends up being a stride
one normal convolution so with there's a
little bit of details in the way that
the border and the padding are handled
but it's fundamentally the same
operation but now things where things
look different when you talk about a
stride of two so again here on the Left
we can take a stride - convolution and
write out this stride - convolution as a
matrix multiplication and now are the
corresponding transpose convolution is
no longer a convolution so if you look
through this weight matrix and think
about how convolutions end up getting
represented in this way then now this
this transposed matrix for the stride -
convolution is something fundamentally
different from the original normal
convolution operation so that that's
kind of the reasoning behind the name
and that's why I think that's kind of
the the nicest name to call this
operation by sorry was there question
it's very possible there's a typo in
this and
point out on Piazza and I'll fix it but
I hope the idea was clear is there
another question okay thank you yeah so
oh no lots of questions
yes the issue is why do we some and not
average so the reason we some is due to
this do this this transpose convolution
formalism so that's that's the reason
why we some but you're right that you
actually this is kind of a problem that
the magnitudes will actually vary in the
output depending on how many receptive
fields were in the output so actually in
practice like this is something that
people have started to point out like
really recently and somewhat switched
away from this stride so using three by
three stride to transpose convolution
for up sampling can sometimes produce
some checkerboard artifacts and the
output exactly due to that problem so
what I've seen in a couple more recent
papers is maybe to use 4x4 stride 2 or
2x2 stride to transpose convolution for
up sampling and that helps alleviate
that problem a little bit
yeah so the question is whether the
weather
stride hath convolution and what does
that terminology come from I think that
was from my paper so that was actually
yes that was that was definitely this so
at the time I was writing that paper I
was kind of into the name fractionally
striated convolution but after thinking
about it more a bit more I think
transpose convolution is probably the
right name so then then this at this
ends up this idea of semantic
segmentation actually is pretends up
being pretty pretty natural that you
just have this giant convolutional net
work with down sampling and up sampling
inside the network and now our down
sampling will be by strata convolution
or pooling our up sampling will be by
transposed convolution or unfair sex of
fun pulling up sampling and we can train
this whole thing and end with back
propagation using this cross entropy
loss over every pixel so this this is
actually pretty cool that we can take a
lot of the same machinery that we
already learned for image classification
and now just apply it very easily to
extend to new types of problems that's
super cool so the the next sort of task
that I want to talk about is this idea
of classification plus localization so
we've talked about image classification
a lot where we want to just assign a
category label to the input image but
sometimes you might want to know a
little bit more about the image in AD in
addition to predicting what the category
is in this case the cat you might also
want to know where is that object in
image you might so in addition to
predicting the category label cat you
might also want to draw a bounding box
around the region of the cat in that
image and classification plus
localization the distinction here
between this and object detection is
that in the localization scenario you
assume ahead of time that you know
there's exactly one object in the image
that you're looking for or maybe more
than one but you know ahead of time that
we're going to make some classification
decision about this image and we're
going to produce exactly one bounding
box that's going to tell us where that
object is located in the image so that's
that we sometimes call that task of
classification plus localization and
again we can reuse a lot of the same
machinery that we've already learned
from image classification in order to
tackle this problem so the architect
kind of a basic architecture for this
problem looks something like this so
again we have our input image we feed
our input image through some giant
convolutional Network and
Alex this is Alex not for example which
will give us some final hidden some
final vector summarizing the content of
the image then just like before we'll
have some fully connected layer that
goes from that final vector to our class
scores but now we'll also have another
fully connected layer that goes from
that vector to four numbers in this
where the four numbers are something
like the height the width and the X&Y
positions of that bounding box and now
our network repute will produce these
two different outputs one is this set of
class scores and the other of these four
numbers giving the coordinates of the
bounding box in the input image and now
during training time when we train this
network we'll actually have two losses
so in this scenario we're we're sort of
assuming a fully supervised setting so
we assume that each of our training
images is annotated with both a category
label and also a ground truth bounding
box for that category in the image so
now we have two loss functions we have
our favorite softmax loss that we
compute using the ground truth category
label and the predicted class scores and
we also have some kind of loss that
gives us some measure of dissimilarity
between our predicted coordinates for
the bounding box and our actual
coordinates for the bounding box so one
very simple thing is to just take an l2
loss between those two and that's kind
of the simplest thing that you'll see in
practice although sometimes people play
around with this and maybe use l1 or
smooth l1 or they parameterize the
bounding box a little bit differently
but the idea is always the same that you
have some measure some regression loss
between your predicted bounding box
coordinates and the ground truth
bounding box coordinates question
sorry go ahead
so the question is is this a good idea
to do all
same time like what happens if you miss
classify should you even look at the box
coordinates so sometimes people get
fancy with it so in general it works ok
like it's not a big problem you can
actually train a network to do both of
these things at the same time and it'll
figure it out but sometimes things can
get tricky in terms of miss
classification so sometimes what you'll
see for example is that rather than
predicting a single box you might make
predictions like a separate prediction
of the box for each category and then
only and then only apply loss to the
predicted box score to the predicted box
corresponding to the ground truth
category so people do get a little bit
fancier with these things that sometimes
helps a bit in practice but at least
this basic set up it might not be
perfect or it might not be optimal but
it will work and it will do something is
there a question in the back
ah so that's the question is do these
losses have different units do they do
they dominate the gradient so this is
what we call a multitask loss so
whenever we're taking derivatives we
always want to take derivative of a
scalar with respect to our network
parameters and use that derivative to
take gradient descent steps but now
we've got two scalars that we want to
both minimize so what you tend to do in
practice is have some additional hyper
parameter that gives you some weighting
between these two losses so you'll take
a weighted sum of these two different
loss functions to give our final scalar
loss and then you'll take your gradients
with respect to this weighted sum of the
two losses and this is really this ends
up being really really tricky because
this weighting parameter is a hyper
parameter that you need to set but it's
kind of different from some of the other
hyper parameters that we've seen so far
in the past right because this this
weighting type of parameter actually
changes the value of the loss function
so one thing that you might often look
at when you're trying to set hyper
parameters is you might make different
hyper parameter choices and see what
happens to the loss under different
choices of hyper parameters but in this
case because the loss actually because
the hyper parameter affects the absolute
value of the loss making those
comparisons becomes kind of tricky so
setting that hyper parameter is somewhat
difficult and in practice like you kind
of need to take it on a case-by-case
basis for exactly the problem you're
solving but my general strategy for this
is to have some other metric of
performance that you care about other
than the actual loss value which then
then you actually use that final
performance metric to make your
cross-validation choices rather than
looking at the value of the loss to make
those choices question
so the question is why why why do we do
this all at once why not make it why not
do this separately
yeah so the question is why don't we fix
the big Network and then make the maze
and just only learn separate fully
protected layers for these two tasks
people do do that sometimes and in fact
that's probably the first thing you
should try if you're faced with a
situation like this but in general
whenever you're doing transfer learning
you always get better performance if you
fine-tune the whole system jointly
because there's probably some mismatch
between the features that because if you
train on image net and then you use that
network for your data set you're going
to get better performance on your data
set if you can also change the change
that networks but one one trick you
might see in practice sometimes is that
you might freeze that network then train
those two things separately until
convergence and then after they converge
then you go back and jointly fine-tune
the whole system so that's a trick that
sometimes people do in try and practice
in that situation and as I've kind of
alluded to this big network is often a
pre trained network that is taken from
imagenet for example so a bit of an
aside on this idea of predicting some
fixed number of positions and the image
can be applied to a lot of different
problems beyond just classification plus
localization one kind of cool example is
human pose estimation so here we want to
take an input image as a picture of a
person and we want to output the
positions of the joints for that person
and this will actually allows the
network to predict like what is the pose
of the human where his arms or his legs
stuff like that and generally most
people have the same number of joints
that's a bit of a simplifying assumption
it might not always be true but it works
for the network so for example one
parameterization is that you might see
in some datasets is define a person's
pose by 14 joint positions their feet
and their knees and their hips and
something like that and now when we
train the network then we're going to
input this image of a person and now
we're going to output like 14 numbers in
this case giving the x and y coordinates
for each of those 14 joints and then you
apply some kind of regression loss on
each of those 14 different predicted
points and just train this network with
back propagation again yeah so so
commonly you might seal an l2 loss but
people play out other regression losses
here as well question
so what the question is what do I mean
when I say regression loss so I mean
something other than cross entropy or
softmax right when I say regression loss
I usually mean like an l2 Euclidian loss
or an l1 loss or sometimes a smooth l1
loss but we're in general classification
versus regression is whether your output
is categorical or continuous so if
you're expecting a categorical output
like some like you ultimately want to
make a classification decision over some
fixed number of categories then you'll
think about a cross entropy loss off max
loss or these SVM margin type losses
that we've talked about already in the
class but if your expected output is to
be some continuous value in this case
the position of these points then your
output is continuous so you tend to use
different types of losses in those
situations typically an l2 l1 different
kinds of things there so sorry for not
clarifying that earlier but the bigger
point here is that for anytime you you
know that you want to make some fixed
number of outputs from your network if
you know for like for example maybe you
knew that you wanted to maybe you oh you
knew that you always are going to have
pictures of a cat and the dog and you
want to like predict both the bounding
box of the cat and the bounding box of
the dog in that case you know that you
have a fixed number of outputs for each
input so you might imagine hooking up
this type of regression classification
plus localization framework for that
problem as well so this idea of some
fixed number of regression outputs can
be applied to a lot of different
problems including pose estimation so
the next the next up the next task that
I want to talk about is object detection
and this is a really meaty topic like
this is kind of a core problem in
computer vision you could probably teach
a whole a whole seminar class on done
just the history of object detection and
various techniques applied there so I'll
be relatively brief and try to go over
the the main big ideas of object
detection plus deep learning that have
been used in the last couple of years
but the idea in object detection is that
we have again we again start with some
fixed set of categories that we care
about maybe cats and dogs and fish or
whatever but some fixed set of
categories that we're interested in and
now our task is that given our input
image every time one of those categories
appears in the image we want
draw a box around it and we want to
predict the category of that box so this
is different from classification plus
localization because there might be a
varying number of outputs for every
input image you don't know ahead of time
how many objects you expect to find in
each image so that's the sense of being
a pretty challenging problem so we've
seen grabs so this is kind of
interesting we've seen this graph many
times of the imagenet classification
performance as a function of years and
we saw that it just got better and
better every year
and there's been a similar trend with
object detection because object
detection has again been one of these
core problems in computer vision that
people have cared about for a very long
time so this this slide is due to Ross
Kirk who's worked on this problem a lot
and it shows the the progression of
object detection performance on this one
particular data set called Pascal vo C
which has been relatively used for a
long time in object detection community
and you can see that up until about 2012
performance on object detection started
to stagnate and slowed down a little bit
and then in 2013 was when some of the
first deep learning approaches to object
detection came around and you could see
that performance just shot up very
quickly getting better and better year
over year one thing you might notice is
that this plot ends in 2015 and it's
actually continued to go up since then
so the current state that we are on this
data set is well over 80 and in fact a
lot of recent papers don't even report
results on this data set anymore because
it's considered too easy so it's a
little bit hard to know like I'm not
actually sure what is the
state-of-the-art number on this data set
but it's off the top of this plot sorry
if you have a question never mind okay
so this is this is as already said this
is different from object from
localization because there might be
different numbers of objects for each
image so for example in this cat on the
upper left there's only one object so we
only need to predict four numbers but
now for this image in the middle there's
three there's a there's like three
animals there so we need to we need our
network to predict 12 numbers for
coordinates for each bounding box or in
this example of many many ducks then you
want our your network to predict a whole
bunch of numbers again four numbers for
each duck so this is quite different
from object detection
because sorry object detection is quite
different from localization because in
object detection you might have varying
numbers of objects in the image and you
don't know ahead of time how many you
expect to find so as a result it's kind
of it's kind of tricky if you want to
think of object detection as a
regression problem so instead people
tend to work use kind of a different
paradigm when thinking about object
detection so this so one one approach
that's very common and it has been used
for a long time in computer vision is
this idea of sliding window approaches
to object detection so this is kind of
similar to this idea of taking small
patches and applying that for semantic
segmentation and we can apply a similar
idea for object detection so the idea is
that we'll take different crops from the
input image in this case we've got this
this crop in the lower left hand corner
of our image and then we take that crop
feed it through our convolutional net
work and our convolutional network does
a classification decision on that input
crop it'll say that there's no dog here
there's no cat here and then in addition
to the categories that we care about
we'll add an additional category called
background and now our our network can
predict background in case that it
doesn't see any of the categories that
we care about so then when we take this
this crop from the lower left-hand
corner here then our network would
hopefully predict background to say that
no there's no object here now if we take
a different crop then our network would
predict dog yes cat no background no we
take a different crop we get dog yes cat
no background no or a different crop dog
no cat yes background no does anyone see
a problem here
yeah the question is how do you choose
the crops so this is a huge problem
right like because these opposite
because there could be any number of
objects in this image these objects
could appear at any location in the
image these objects could appear at any
size in the image these objects could
also appear at any aspect ratio in the
image so if you want to do kind of a
brute force sliding-window approach
you'd end up having to test thousands
tens of thousands many many many many
different crops in order to tackle
tackle this problem with a brute force
sliding-window approach and in the case
where every one of those crops is going
to be fed through a giant convolutional
network this would be completely
computationally intractable so in
practice people don't ever do this sort
of brute force sliding-window approach
for object detection using convolutional
networks instead there was there's this
cool line of work called region
proposals that comes from this is not
using deep learning typically these are
slightly more traditional computer
vision techniques but the idea is that a
region proposal network kind of uses
more traditional signal processing image
processing type things to make some some
lists of proposals for where so give it
an input image a region proposal network
will then give you something like a
thousand boxes where an object might be
present so it you can imagine that maybe
we do some local we look for edges in
the image and try to like draw boxes
that contain closed edges or something
like that these various types of image
processing approaches but these region
proposal networks will basically look
for blobby regions in our input image
and then give us some some set of
candidate proposal regions where objects
might be potentially found and these are
relative these are relatively fast ish
to run so one common example of a region
proposal method that you might see is
something called selective search which
I think actually gives you two thousand
region proposals not the one thousand
that it says on the slide so you kind of
run this thing and then after about two
seconds of turning on your CPU it'll
spit out two thousand region proposals
in the input image where objects are
likely to be found so there'll be a lot
of noise and those like most of them
will not be true objects but there's a
pretty high recall if there is an object
in the image then it does tend to get
covered by these region proposals from
selective search so now rather than
rather than applying our classic
in network to every possible location
and scale in the image instead what we
can do is first apply one of these
region proposal networks to get some set
of proposal regions where objects are
likely located and now apply each of the
apply a convolutional Network for
classification to each of these proposal
regions and this will end up being much
more computationally tractable than
trying to do out all possible locations
and scales and this idea is was all came
together in this this paper called our
CNN the from from a few years ago that
does exactly that
so given our input image in this case
we'll run some region proposal network
to get our proposals these are also
sometimes called regions of interest or
ro eyes
so again selective search gives you
something like 2,000 regions of interest
now one of the problems here is that
these these input these regions in the
input image could have different sizes
but if we're going to run them all
through a convolutional network our
classification our convolutional
networks for classification all want
images of the same input size typically
due to the fully expected net layers and
whatnot so we need to take each of these
region proposals and warp them to that
fixed square size that is expected as
input to our downstream Network so we'll
crop out those region proposal those
regions corresponding to the region
proposals will warp them to that fixed
size and then we'll run each of them
through a convolutional Network which
will then use in this case an SVM to
make a classification decision between
for each of those to predict categories
for each of those crops and then I lost
MN and then I lost the slide but it'll
also not shown in the slide right now
but in addition our CNN also predicts a
regression like a correction to the
bounding box in addition for each of
these input region proposals because the
problem is that your input region
proposals are kind of generally in the
right position for an object but they
might not be perfect so in addition our
CNN will in addition to category labels
for each for each of these proposals
it'll also predict four numbers that are
kind of an offset or a correction to the
box that was predicted at the region
proposal stage so then again this is a
multitask loss and you train this whole
thing
start with our question
the question is how much does the change
in
Specht ratio impact that impact accuracy
it's a little bit hard to say I'm not
you I think there's some controlled
experiments and some of these papers but
I'm not I'm not sure I can give a
generic answer to that question
question is is it necessary for regions
of interest to be rectangles so they
typically are because it's tough to warp
these non region things but it once you
move to something like instance
segmentation then you sometimes get read
proposals that are not rectangles if you
actually do care about predicting things
that are not rectangles so another
question
yes so the question is are the region
proposals learned so in our CNN it's a
traditional it's a traditional thing
these are not learned this is kind of
some some fixed algorithm that someone
wrote down but we'll see in a couple
minutes that we can we can actually
we've changed that a little bit in the
last couple of years is there another
question
the question is is the offset always
inside the region of interest the answer
is no it doesn't have to be you might
imagine that like suppose the region of
interest like put a box around a person
but missed the head then you could
imagine a network inferring that like oh
this is a person but people usually have
heads so the network shows that the box
should be a little bit higher so
sometimes the final predicted boxes will
be outside the region of interest
question
you
yeah the question is you have a lot of
our why's that
correspond to true objects and like we
said in addition to the classes that you
actually care about you add an
additional background class so your
class scores can also predict background
to say that there was no object here
question
so the question is we still what kind of
data do we need and yeah this is fully
supervised in the sense that our
training data has each in it has it
consists of images each image has all
the objects categories marked with
bounding boxes for each instance of that
category there there are definitely
papers that try to approach this like Oh
what if you don't have the data what if
you only have that look at that what if
you only have that data for some images
or what if that data is noisy but at
least in the generic case you assume
full supervision of all objects in the
images at training time okay so I think
we've kind of alluded to this but
there's there's kind of a lot of
problems with this our CNN framework and
actually if you look at the figure here
on the right you can see that that
additional bounding box head so I'll put
it back but this is a little bit this is
this is kind of still computational
pretty computationally pretty expensive
because we've got if we've got 2000
region proposals we're running each of
those proposals independently that can
be pretty expensive there's also this
question of like not of like relying on
this fixed region proposal Network fixed
region proposals we're not learning them
so that's kind of a problem and just in
practice it ends up being pretty slow so
in the original implementation our CNN
would actually um dump all the features
to disk so take hundreds of gigabytes of
disk space to store all these features
then training would be super slow since
you have to make all these different
forward and backward passes through the
image and it might take it took
something like eighty four hours is one
number they reported for training time
so this is super super slow and now a
test time it's also super slow something
like roughly 30 seconds minute per image
because you need to run thousands of
four passes through the convolutional
network for each of these region
proposals this ends up pretty being
pretty slow thankfully we have fast our
CNN that fixed a lot of these problems
so when we when we do fast our CNN then
it's going to look kind of the same
we're going to start with our input
image but now rather than processing
each region of interest separately
instead we're going to run the entire
image through some convolutional layers
all at once to give this high resolution
convolutional feature map corresponding
to the entire image and now we still are
using some region proposals from some
fixed thing like selective search but
rather than putting those rather than
cropping out the pixels of the image
corresponding to the
proposals instead we imagine projecting
those region proposals on to this
convolutional feature map and then
taking crops from the convolutional
feature map corresponding to each
proposal rather than taking crops
directly from the image and this allows
us to reuse a lot of this expensive
convolutional computation across the
entire image when we have many many
crops per image but again if we if we
have some fully connected layers
downstream those fully connected and
where's are expecting some fixed size
input so now we need to do some some
reshaping of those crops from the
convolutional feature map and they do
that in a differentiable way using
something they call an roi pooling layer
once you have these these crop these
warped crops from the convolutional
feature map then you can run these
things through some fully connected
layers and predict your classification
scores and your linear regression
offsets to the bounding boxes and now
when we train this thing then we again
have a multi task loss that trades off
between these two constraints and during
back propagation we can back prop
through this entire thing and learn it
all jointly this this ROI pooling I it
looks kind of like max pooling I don't
really really want to get into the
details of that right now and in terms
of speed if we look at our CNN versus
fast our CNN versus this other model
called SP P net which is kind of in
between the two then you can see that at
training time fast our CNN is something
like 10 times faster to train because
we're sharing all this computation
between different feature Maps and now
it's test time fast our CNN is super
fast and in fact fast our CNN is so fast
at test time that it's this
computational computation time is
actually dominated by computing region
proposals so we said that computing
these two thousand region proposals
using selective search takes something
like two seconds and now once we've got
all these region proposals then because
we're processing processing them all
sort of in a shared way by sharing these
expensive convolutions across the entire
image that we can process all of these
region proposals in less than a second
all together so fast our CNN ends up
being bottlenecks by the computing of
these region proposals thankfully we've
solved this problem with faster our CNN
so the idea in faster our CNN is to just
make the so the problem was that
computing the region proposals using
this this fixed function was a
bottleneck so instead we'll just make
the network itself predict its own
region proposals and so the way that
this sort of works is that again we take
our input image run it run the entire
input image all together through some
convolutional layers to get some
convolutional feature map representing
the entire high resolution image and now
there's a separate region proposal
network which works on top of those
convolutional features and predicts its
own region proposals inside the network
now once we have those predicted region
proposals then it looks just like fast
our CNN where now we take crops from
those region proposals from the
convolutional features pass them up to
the rest of the network and now we
talked about multi task losses and multi
task of training networks of to do
multiple things at once
well now we're now we're telling them
that work to do four things all at once
so training so balancing out this
four-way multi task loss is kind of
tricky but because the region proposal
network needs to do two things
it needs to say for each potential
proposal is it an object or not an
object it needs to actually regress
these bounding box coordinates for each
of those proposals and now the final
network at the end is to do these two
things again make final classification
decisions for what are the class scores
for each of these proposals and also
have a second round of bounding box
regression to again correcting errors
that may have can't take come from the
region proposal stage question
so the question is that sometimes
multitask learning might be seen as
regularization and are we getting that
effect here I'm not sure if there's been
super controlled Studies on that but
actually in the original version of the
faster our CNN paper they did a little
bit of experimentation like what if we
share the region proposal network what
if we don't share like if what if we
learn separate convolutional networks
for the region proposal network versus
the classification Network and I think
there were minor differences but like it
wasn't it wasn't a dramatic difference
either way so in practice it's kind of
nicer to only learn one because it's
computational cheap computationally
cheaper all right question
you
yeah the question is how do you train
this region proposal network because
you're you don't know you don't have
ground truth region proposals for the
region proposal Network so it's a little
bit hairy I don't want to get too much
into those details but the idea is that
anytime you have a region proposal which
has more than some threshold of overlap
with any of the ground truth objects
then you say that that is a positive
region proposal and you should predict
that as a region proposal and anything
any any potential proposal which has
very low overlap with any ground truth
objects should be predicted as a
negative but there's a lot of sort of
dark magic hyper parameters in that
process and it's a little bit hairy
questions
yeah so the question is what is the
classification loss on the region
proposal Network and the answer is it's
making a binary class so I didn't want
to get into much of the details of that
architecture because it's a little bit
hairy but it's making binary decisions
so it has some set of potential regions
that it's considering and it's making a
binary decision for each one is this an
object or not an object so it's like a
binary classification loss so once you
train this thing then faster are CN n
being ends up being pretty pretty darn
fast so now because we've eliminated
this overhead from computing region
proposals outside the network now faster
our CN n ends up being very very fast
compared to these other other
alternatives also one interesting thing
is that because we're learning the
region proposals here you might imagine
like maybe what if there was some
mismatch between this region this fixed
region proposal algorithm and my data so
in this case once you're learning your
own region proposals then you can
overcome that mismatch if right if your
region proposals are somewhat somewhat
weird or different than other data sets
so there's this whole family of fast of
our CN n methods the R stands for region
so these are all region based methods
because there's some kind of region
proposal and then we're doing some some
processing for each some independent
processing for each of those potential
regions so this whole family of methods
are called these of region based methods
for for object detection but there's
another family of methods that you
sometimes see for object detection which
is sort of all feed-forward in a single
pass so one of these is yellow for you
only look once and another is SSD for
single shot detection and these two came
out somewhat around the same time but
the idea is that rather than doing
independent processing for each of these
potential regions instead we want to try
and treat this like a regression problem
and just make all these predictions all
at once with some big convolutional
Network so now given our input image you
imagine dividing that input image into
some some coarse grid in this case is a
7x7 grid and now within each of those
grid cells you imagine some set of base
bounding boxes here I've drawn three
based bounding boxes like a tall one of
a wide one and a square one but in
practice you would use more than three
so now for for each of these grid cells
and for
these bass pounding boxes you want to
predict several things one you want to
predict you want to predict like an
offset off the bass bounding box to
predict like what is the true location
of the object of off this bass bounding
box and you also want to predict
classification scores so a confident
maybe a classification score for each of
these bass bounding boxes how likely is
it that an object of this category
appears in this bounding box so then at
the end at the end we end up predicting
from our input image we end up
predicting this giant tensor of 7x7 grid
by five B plus C so that's just where
where we have be based bounding boxes we
have five numbers for each giving our
offset and our confidence for that based
bounding box and C classification scores
for our C categories so this so then we
kind of see object detection as this
like input of a three input of an image
output of this three dimensional tensor
and you can imagine just training this
whole thing with a giant convolutional
Network and that's kind of what these
single-shot methods do where they just
and again matching the ground truth
objects into these potential of these
potential base boxes becomes a little
bit hairy but that's that's what these
methods do and by the way the region
proposal network that gets used in
faster our CNN ends up looking quite
quite similar to these where they have
some set of base bounding boxes over
some grid in the image and now the
region proposal met but network does
some regression plus some classification
so one so so there's there's kind of
some overlapping ideas here whether we
want so we in faster our CNN or kind of
just do it treating the the object the
region proposal step as kind of this
this fixed end to end regression problem
and then we do the separate per region
processing but now with these single
shot methods we just do we only do that
first step and just do all of our object
detection with a single forward pass
so object detection has a ton of
different variables there can be
different based networks like bgg ResNet
we've seen different sort of meta
strategies for object detection
including this like faster RC n n type
region based family of methods this
single shot detection family of methods
there's kind of a hybrid that I didn't
talk about called RFC n which is
somewhat in between there's a lot of
different hyper parameters like what is
the image size how many region proposals
to do
use and there's actually this really
cool paper that will appear at cbpr this
summer that does a really controlled
experimentation around a lot of these
different variables and tries to tries
to tell you like how do these methods
all perform under these different
variables so if you're interested I'd
encourage you to check it out but kind
of one of the key takeaways is that the
faster our CNN style of region based
methods tends to give higher accuracies
but ends up being much slower than the
single-shot methods because the
single-shot methods don't require this
per region processing but you to check
out this paper if you want more details
also was in as a bit of a side i had
this fun paper with andre a couple years
ago they kind of combined object
detection with image captioning and did
this problem called dense captioning so
now the idea is that rather than rather
than predicting a fixed category label
for each region instead we want to write
a caption for each region and again we
had some data set that had this sort of
data where we had a data set of regions
together with captions and then we sort
of trained this giant end-to-end model
that just predicted these captions all
jointly and this ends up looking
somewhat like faster are CNN or you have
some region proposal stage than a
bounding back then up some per region
processing but rather than an SVM or a
soft next loss instead that those per
region processing has a whole RN n
language model that predicts a caption
for each region so that ends up looking
quite a bit like faster our CN n there's
a video here but I think we're running
out of time so I'll skip it so but but
the idea here is that once you have this
I want what you can kind of tie together
a lot of these ideas and if you have
some new problem that you're interested
in tackling like dense captioning you
can recycle all of the components that
you've learned from other problems like
object detection and image captioning
and kind of stitch together one
end-to-end network that produces the
outputs that you care about for your
problem so the last task that I want to
talk about is this idea of instant
segmentation so here instant
segmentation is in some ways like the
full problem we want to we're given an
input image and we want to predict one
then the locations and identities of
objects in that image similar to object
detection but rather than just
predicting a bounding box for each of
those objects instead we want to predict
a whole segmentation mask for each of
the
objects and predict which pixels in the
input image corresponds to each instance
to each object instance so the so
this is kind of like a hybrid between
semantic segmentation and object
detection because like object detection
we can handle multiple objects and we
differentiate the identities of
different instances so in this in this
example since there are two dogs in the
image an instance segmentation method
actually distinguishes between the two
dog instances in the output and kind of
like semantic segmentation we have this
perp this pixel wise accuracy where for
each of these objects we want to say
which pixels belong to that object so
there's there's been a lot of different
methods that people have tackled for
instance segmentation as well but the
current sort of state of the art is this
new paper called NASCAR CNN that just
actually we just came out on archive
about a month ago so this is not yet
published this was like super fresh
stuff and this ends up looking a lot
like faster our CNN so it has this
multistage processing approach where we
take our whole input image that in that
whole input image goes into some
convolutional net work and some learned
region proposal network that's exactly
the same as faster our CNN and now once
we have our learned region proposals
then we project those proposals on to
our convolutional feature map just like
we did in fast and faster our CNN but
now rather than just making a
classification and a bounding box
regression decision for each of those
boxes we in addition want to predict a
segmentation mask for each of those
bounding box for each of those region
proposals so now it kind of looks like a
mini in like a semantic segmentation
problem inside each of the region
proposals that we're getting from our
region proposal network so now after we
do this ROI aligning to warp our
features corresponding to the region
proposal into the right shape then we
have two different branches one branch
will come up that looks exactly and this
first branch at the top looks just like
faster our CNN and it will predict
classification scores telling us what is
the category corresponding to that
region proposal or alternatively whether
or not it's background and will also
predict some bounding box coordinates
that regress off the region proposal
coordinates and now in addition we'll
have this branch at the bottom which
looks basically like like a semantic
segmentation
mini network which will classifies for
each pixel in that input region proposal
whether or not it's an object so this
this mask our CNN problem kind of this
mask our CNN architecture just kind of
unifies all of these different problems
that we've been talking about today into
kind of one nice jointly end-to-end
trainable model and it's really cool and
it actually works really really well so
when you look at the examples in the
paper they're they're kind of amazing
they look kind of indistinguishable from
ground truth so in this example on the
left you can see that there's these two
people standing in front of motorcycles
if drawn the boxes around these people
it's also gone in and labeled all the
pixels of those people and it's really
small but actually in the background on
that image on the left there's also a
whole crowd of people standing very
small in the background it's also drawn
boxes around each of those and grab the
pixels of each of those images and you
can see that this is just like it ends
up working really really well and it's
relatively simple addition on top of the
existing faster our CNN framework so I
told you that that masks our CNN unifies
everything we talked about today and it
also does pose estimation by the way so
math so we talked about pote you can do
pose estimation by predicting these
joint coordinates for each of the joints
of the person so you can do mask our CNN
to do joint object detection pose
estimation and into the segmentation and
the only addition we need to make is
that for each of these region proposals
we add an additional little branch that
predicts these of these these
coordinates of the joints for the
instance of the current region proposal
so now this is just another loss like
another layer that we add another head
coming out of the network and a dish and
then the end an additional term in our
multitask loss but once we add this one
little branch then you can do all of
these different problems jointly and you
get results looking something like this
where now this network like a single
feed-forward network is the test is
deciding how many people are in the
image detecting where those people are
figuring out the pixels corresponding to
each of those people and also drawing a
skeleton estimating the pose of these
people and this works really well even
in crowded scenes were like this
classroom where there's a ton of people
sitting and they all overlap each other
and it just work it seems to work
incredibly well and because
it's sort of built on the faster our CNN
framework it also runs relatively close
to real-time so this is running
something like fight like five frames
per second on a GPU because this is all
sort of done in a single fordpass of the
network so this is again a super new
paper but I think that this will
probably get a lot of attention in the
coming months
so just to recap we've talked a sorry
question
question is how much training data do
you need so all of these all of these
instant segmentation results were
trained on the Microsoft cocoa data set
so Microsoft cocoa is roughly 200,000
training images it has 80 categories
that it cares about so in each of those
200,000 training images it has all the
instances of those 80 categories labeled
so there's something like 200,000 images
in for training and there's something
like I think an average of five or six
instances per image so it actually is
quite a lot of data and for Microsoft
cocoa for all the people in Microsoft
cocoa they also have all the joints
annotated as well so this actually does
have quite a lot of supervision a
training time you're right
and actually is trained with quite a lot
of data so I think one sort of really
interesting topic to study moving
forward is that we kind of know that if
you have a lot of data to solve some
problem at this point we're relatively
confident that you can stitch up some
convolutional Network that can probably
do a reasonable job at that problem but
figuring out ways to get performance
like this with less training data is a
super interesting and active area of
research and I think that's something
people will be spending a lot of their
efforts working on in the next few years
so just to recap today we had kind of a
whirlwind tour of a whole bunch of
different computer vision topics and we
saw how a lot of the machinery that we
built up from image classification can
be applied relatively easily to tackle
these different computer vision topics
and next time we'll talk about we'll
have a really fun lecture on visualizing
CNN features we'll also talk about deep
dream and neural style transfer