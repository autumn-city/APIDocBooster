 Yeah, in the previous videos, I showed you how the softmax
 regression model looks like. Now let's talk about the
 multicategory cross entropy loss that we will need for training
 such a model. And for that, we will also need the one hot
 encoding scheme for representing the class labels in a way that
 is compatible with the multicategory cross entropy
 loss. So yeah, this is just a recap a slide I already showed
 you in the previous video. It's how the basic setup looks like
 where we have an input feature vector and then we have multiple
 sets of feature of sorry of weight vectors, we have multiple
 sets of weight vectors. If we have h classes, we have h weight
 vectors, which we combine into a matrix W. So we discussed that
 before, I don't need to recap this, I think. But yeah, what we
 are focusing on now is how we compute this softmax activation
 that comes from these different net inputs. So here's an
 overview of how the softmax activation function looks like.
 It's fundamentally not very complicated. It's essentially a
 normalization function. So it's in a way you can think of it as
 a generalization of the logistic sigmoid function for making the
 probabilities to sum up to one. So let's say we have a class
 labeled t, so the possible class labels are one to h. And with a
 softmax activation, we can compute the probability that a
 given output belongs to class t. So if I go back here, this is a
 z one, z two, up to z h, we have h net inputs here. And from that
 one, we can compute this a one, which is essentially the
 probability that class label belongs to class one, given the
 input vector, or here in this case, let's write this as z one
 as the net input, because here we have written this as a
 function, softmax activation function. And how this works is
 there are two parts, there's it's essentially an exponential
 function. So you have e to the power of the net input as the
 numerator. So I is the index over training examples. And here
 in the numerator, we sum over all exponential terms. So we
 have these exponential terms, j, also one to h. And if we have h
 class labels, we have h activations, and h net inputs. So
 here we are summing over h, sorry, j, you were assuming from
 j to h. So this is essentially a normalization term. So if we
 apply this, the probability will be smaller than one between zero
 and one. And the probabilities from all the net inputs will
 sum up to one. So consequently, if I go back one more time here,
 so the sum, let me write this on the right hand side, the sum of
 all these is should be one after this normalization function. So
 yeah, this is essentially how this softmax activation works.
 Now, let's talk about the loss function. So before we can talk
 about the loss function, there's one more thing, we have to talk
 about the one hot encoding, which is an encoding scheme for
 categorical variables. And we can also apply this to the class
 label variable. So you can use that in different contexts, you
 can do that for encoding features, you have probably seen
 that in the context of traditional machine learning. But
 yeah, we can also apply this one hot encoding scheme to the class
 label variable or the class label vector, which is
 essentially also categorical variable. So imagine we have a
 data set that consists of different class labels here, each
 row represents one training example. So we have the first
 training example with class label zero, the second one with
 class level one, then class label three, and then class
 level two, and the possible class labels are three. So let's
 say we have class labels on in between, it's a set between
 012 and three, these are possible class labels. Now we
 can convert this into a one hot encoding on the right hand side
 here. So we have now a column for each possible value. So
 class zero, one, two, and three. And each row still represents
 one training example. So here, this is still the first training
 example, the second row is still the second training example, the
 third row is the third, and this is the fourth, I can maybe write
 this down. So this still refers to the training examples 123 and
 four. And then we can look at all these four features here. So
 for training example one, let's focus on training example one.
 So training example one has class level zero, right? So it
 has a one and indicator one in the column class zero, and all
 the other columns are set to zero. So here, this is a zero,
 zero, zero, because it's class zero. So the true class has a
 one in its position, and the other ones are all zero. And
 consequently, if we look at class level two here, sorry,
 training example two, then this one has class level one, and it
 has a one in the second column belonging to class one. And then
 let's do one more. So training example, this is class three, it
 has one here in the last column. So this is how the one hot
 encoding works. You can think of it as an indicator variable. So
 you have like a one in the right column and the other ones are
 zero. Yeah, and here's now how the multi category cross
 entropy looks like for the age different class labels. So if
 class labels, one to H, and essentially, it's a
 generalization of the binary cross entropy. So if you recall
 the binary cross entropy is essentially the negative log
 likelihood, which we discussed in the last videos about
 logistic regression. Now, so if you recall what we had was then
 this negative lock of the activation, that's the same
 thing here. See, we also have a negative lock. But there's one
 more thing. Now we have this y i y j here. And the y j is either
 one or zero, right? According to the one hot encoding, this is
 either a one or a zero. So this whole equation assumes a one hot
 encoding. I will show you an concrete example in the next
 slides to further clarify that. But yeah, just to focus in on
 this overview here. Also, I, again, is over the training
 examples. So let's only focus on this inner term here. So what we
 have is we have for this y either one or zero. And then we
 are summing over these negative locks for these different
 activations. So the activations each activation is a value
 between zero and one. So we apply, so it should be actually
 larger than zero. So we apply minus lock of this term between
 zero and one. And then we sum over all of these for the for
 the output node. So we have h output nodes, and we sum over
 those. So if we use the one hot encoding, we have h classes, but
 only one class, the true label for one class is only one right,
 according to the one hot encoding, all the other ones are
 zero. And I will clarify this with a concrete example, because
 I think it's hard to visualize if you don't have a concrete
 example in mind. But yeah, one more thing than here, this first
 sum is then summing over the training examples. Alright, so
 let's take a look at a more concrete example. Okay, I have
 one more slide in between, and then I will show you the concrete
 example. So here's just the analogy to the binary cross
 entropy at the top. So at the top, you have the binary cross
 entropy. So here you have also the negative term, so bring it
 inside if you like. So you have this negative term, negative
 cross entropy, negative log likelihood. And this term is
 essentially the same as this term, if the class label is one.
 Right. So this is the one, this is the one that's the same. And
 this one is when we had a class label zero. So if we have a
 class label zero, then we have one minus zero, then this is a
 one. So essentially, we have also this term here, one minus a,
 we don't have that here, here, we have minus log for all these
 classes. But essentially, it's the same thing. It's just a
 generalization, because if we have a point eight here, for the
 activation, then we would have a point two here for one minus the
 activation, right, because they have to sum up to one, we set
 with a softmax function, it's the same thing, all the
 activations sum up to one. So if we have two class labels, zero
 and one, if our class labels are either zero and one, these
 these equations are identical. Because it's just a different way
 of writing this here, we write this as the sum, but the sum is
 for binary classification, basically the sum between those
 two terms. So it's essentially exactly the same thing. But now
 with writing it like this, using the one hot encoding, we can
 generalize this to more than two classes. So here's a concrete
 example. So let's consider this example where we have four
 training examples. Each row here represents one training
 example. Number one, number two, three, and number four. And the
 column index represents which class label it is. So we have
 three possible classes. So the first column is class one, then
 this is class two, and this is class three. So we can see class
 one, the true label is class. Sorry, training example one, the
 true label is class one. Because the one is in the first column,
 this one has the one in the second column. So it's class
 two, this should be class three, because the one is in the third
 column. And this is also class three. Here on the right hand
 side, I generated some arbitrary softmax outputs. So notice that
 all the columns should sum up to one. So the sum of the columns
 should be one in each case.
 Can double check this with a calculator if you like, they
 should all sum up to one because we have mutually exclusive
 classes. This is what we get when we apply the softmax
 outputs. I will also show you a code example how we can actually
 compute that in pytorch. But let's continue with this example.
 So now let's compute the loss. So here at the bottom, so we are
 focusing on let's say we are focusing on the first training
 example, training example one. So this corresponds to this
 first row in the activation matrix. And now what we are
 computing is we are computing the negative lock of the
 activation. So let me use, let me use blue here. So we compute
 one times the lock of point three, seven, and then zero
 times the lock of point three, one, and zero times point three,
 one. So this is what I'm what I've written down here. So minus
 one times lock three, seven, and then the other terms here. So
 this is this is this one here that we are computing when we
 are summing. So for the first training example, we get a loss
 of point 96.969692. So this is our loss for the first training
 example.
 Okay, I hope that makes sense. So we can then do the same thing
 for the other training examples. So here I use a color, a
 different color for each training example. So here, this
 one is computed here, then we have the red ones, or that we
 have this box here. And then lastly, for this row, we have
 this box here, I don't have to go over the computation again,
 I think, because it's the same as for the first training
 example. Yeah, and then one more step, we can put everything
 together. So now the outer sum. So we are summing over all these
 losses here. And the sum of these is sorry, it should be
 actually the mean, right? One over. And so the mean is point
 nine, three. Otherwise, the sum would be four times that. Okay,
 so usually, like I said, it's more stable in a deep learning
 context. In practice, if we compute the mean instead of the
 sum for the mini batch, because then it's easier to find a good
 learning rate, because let's say you find a good learning rate
 for a given mini batch, and then you make the mini batch larger,
 the loss will then be also if you make mini batch 10 times
 larger, the loss will also be 10 times larger. So you have to
 decrease the learning rate by a factor of 10 to get
 approximately the same performance. So it's usually I
 find it easier to deal with the mean instead of the sum. Okay.
 And yeah, I will show you now a code example, how we can do this
 in pytorch. But this will be more like exactly what we have
 done in the slides. In practice, there are a few more tricks that
 will be the nature shown when we talk about the softmax
 implementation. But let me make a pause now. And let me record
 this code example in a separate video, because I think this
 video is probably already pretty long.