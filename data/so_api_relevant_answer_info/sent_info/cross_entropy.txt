torch.nn.functional.cross_entropy function combines log_softmax(softmax followed by a logarithm) and nll_loss(negative log likelihood loss) in a single
function, i.e.
Read more about torch.nn.functional.cross_entropy loss function from [here (hyper-link)].
softmax_cross_entropy_with_logits takes logits (real numbers without any range limit), passes them through the softmax function and then computes the cross-entropy.
This is correct, and both should give similar but not the same results, softmax_cross_entropy_with_logits is superior because of numerical stability.
Therefore, rightly, you expect to see the value of cross_entropy everytime something flows across the cross_entropy node.
Thus, in practice, the cross_entropy variable is an identity node that "points" to another variable, that is effectively evaluated.
This same process happens for y_, y_clipped, and cross_entropy.
So in the end you have a graph which connects cross_entropy to W1.
The problem is that optimizer.compute_gradients(cross_entropy) seems to return a single gradient, even though cross_entropy is a 1d tensor of shape [None, 1].
I unstack cross_entropy to get cross_entropy per instance, then I call compute_gradients per instance.
If you are doing ordinary (binary) logistic regression (with 0/1 labels), then use the loss function [tf.nn.sigmoid_cross_entropy_with_logits() (hyper-link)].
[1, 0, 0], [0, 1, 0], ...) and use the loss function [tf.nn.softmax_cross_entropy_with_logits() (hyper-link)]
1, 2, 3, ...) and use the loss function [tf.nn.sparse_softmax_cross_entropy_with_logits() (hyper-link)]
[What's the difference between sparse_softmax_cross_entropy_with_logits and softmax_cross_entropy_with_logits?
The problem I think is that nn.sigmoid_cross_entropy_with_logits expects unormalized results, where as the function you replace it with cross_entropy = tf.reduce_sum(- y * tf.log(y_) - (1 - y) * tf.log(1 - y_), 1)
print(sess.run([optimizer, cross_entropy]) instead of run print(c)
Instead of just running the training_step, run also the cross_entropy node so that its value is returned to you.
You can fetch the value of cross_entropy by adding it to the list of arguments to sess.run(...).
Let's say, in addition to the value of cross_entropy, you wanted to print the value of a tf.Variable called W, you could do the following:
Ok, after two days research, I found the issue, this is caused by inproper cost function, I tried tf.nn.softmax_cross_entropy_with_logits as the cost function to do the binanry classification, that caused the very high cost.
after changed that into tf.nn.sigmoid_cross_entropy_with_logits then the cost reduced to 0.2 which is much lower, and to 100% training accuracy, I find another [post (hyper-link)] indicated the issue, so after I changed "correct_prediction = tf.equal(tf.argmax(AL), tf.argmax(Y))" into "correct_prediction = tf.equal(tf.round(AL), Y)" then it's fixed.
If you use tf.reduce_sum() in the upper example, as you did in the lower one, you should be able to achieve similar results with both methods: cost = tf.reduce_mean(tf.reduce_sum( tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))).
I increased the number of training epochs to 50 and achieved accuracies of 93.06% (tf.nn.softmax_cross_entropy_with_logits()) and 93.24% (softmax and cross entropy separately), so the results are quite similar.
Whatever, you can find on my [GitHub (hyper-link)] the implementation of numerically stable cross entropy loss function which has the same result as tf.nn.softmax_cross_entropy_with_logits() function.
You can see that tf.nn.softmax_cross_entropy_with_logits() doesn't calculate the large numbers softmax normalization, on only approximate them, more details are in [README (hyper-link)] section.
Use [tf.nn.sparse_softmax_cross_entropy_with_logits (hyper-link)] instead of [tf.nn.softmax_cross_entropy_with_logits (hyper-link)].
Since you have a single class, you should use [tf.sigmoid_cross_entropy_with_logits (hyper-link)]
Try print(train_step) and print(cross_entropy).
So if you're looking for the value of the cross entropy that was calculated on the forward pass, just do something like _, loss_value = sess.run([train_step, cross_entropy])
Try this cross-entropy instead:
print(cross_entropy(q, p))
real_loss = cross_entropy(tf.ones_like(real_output), real_output) evaluates to


real_loss = -1 * log(real_output) - (1 - 1) * log(1 - real_output) = -log(real_output)
fake_loss = cross_entropy(tf.zeros_like(fake_output),fake_output) evaluates to


fake_loss = -0 * log(fake_output) - (1 - 0) * log(1 - fake_output) = -log(1 - fake_output)
When you define your loss you are passing the operation stored in cross_entropy, which depends on y_ and y. y_ is a placeholder for your input whereas y is the result of y = tf.nn.softmax(tf.matmul(x, W) + b).
The operation loss contains all the information it needs to build the model an process the input, because it depends on the operation cross_entropy, which depends on y_ and y, which depends on the input x and the model weights W.
TensorFlow has [tf.nn.sigmoid_cross_entropy_with_logits (hyper-link)] for independent, multilabel classification.
From the documentation* for tf.nn.sparse_softmax_cross_entropy_with_logits:
*[https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits (hyper-link)]
Use tf.nn.softmax_cross_entropy_with_logits instead of sparse_softmax.
[sparse_softmax_cross_entropy_with_logits (hyper-link)]
For soft softmax classification with a probability distribution
  for each entry, see softmax_cross_entropy_with_logits.
[softmax_cross_entropy_with_logits_v2 (hyper-link)] and [softmax_cross_entropy_with_logits (hyper-link)]
To calculate a cross entropy loss that allows
  backpropagation into both logits and labels,  see softmax_cross_entropy_with_logits_v2
here is the same implements of softmax_cross_entropy_with_logits_v2
tf.nn.softmax_cross_entropy_with_logits performs softmax internally.
It is explicitly documented here: [https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits (hyper-link)]
This is a sampled softmax_cross_entropy_with_logits, so it takes just a few samples before using the cross entropy rather than using the full cross entropy: [https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/ops/nn_impl.py#L1269 (hyper-link)]
I think the comment about softmax_cross_entropy_with_logits is incorrect because you have a multi-label, (each label is a) binary-class problem.
I assume you mean that the value of out (i.e., the first return value from sess.run([train_op, cross_entropy], ...)) is None.
However, to avoid some edge cases TensorFlow already has its own version of cross-entropy: for example tf.nn.softmax_cross_entropy_with_logits.
This is an easy fix: softmax_cross_entropy_with_logits() has three arguments of relevance (in order): _sentinel, labels, and logits.
There are two dimensions because cross_entropy computes values for a batch of training examples.
I changed it to sparse_softmax_cross_entropy_with_logits , which doesn't need labels in one hot encoding format.
You have the tensor cross_entropy which computes the loss, and train_step, an optimization operation that updates the trainable variables in the model (by default, the variables you create are trainable unless trainable=False is specified) according to some rule depending on the value of cross_entropy.
Run train_step with your training data until cross_entropy has a "good enough" value.
Binary cross_entropy is for two-class problems or for multi-label classification.
First at all, you have to understand what the cross_entropy is meaning.
You could keep the labels as integers 0 and 1 and use [tf.nn.sparse_softmax_cross_entropy_with_logits() (hyper-link)], as suggested in [this answer (hyper-link)].
I modified the problem here to implement a solution that uses sigmoid_cross_entropy_with_logits the way Keras does under the hood.
The optimizer minimizes your loss function (in this case cross_entropy), which is evaluated or computed using the model hypothesis y.
In the cascade approach, the cross_entropy loss function minimizes the error made when computing y, so it finds the best values of the weights W that when combined with x accurately approximates y.
Because the cascade approach ultimately calls cross_entropy which makes use of the placeholders x and y, you have to use the feed_dict to pass data to those placeholders.
"minimize by gradient descent") is connected/depends on the result of cross_entropy.
cross_entropy itself relies on the results of y (softmax operation) and y_ (data assignment); etc.
Yes there is tf.nn.softmax_with_cross_entropy but this is too specific (what if you don't want a softmax?).
The softmax_cross_entropy_with_logits should support your use case, according to documentation.
Tensorflow assumes the input to tf.nn.softmax_cross_entropy_with_logits_v2 as the raw unnormalized logits while Keras accepts inputs as probabilities
In your case, you're defining the partial derivative of cross_entropy with respect to x (and extracting the first (and only) element, since tf.gradient returns a list).
I would also recommend using the built-in tf.losses.softmax_cross_entropy instead of doing it yourself.
By default, when you
call [Optimizer.minimize() (hyper-link)], TensorFlow creates operations to update all variables on which the given tensor (in this case cross_entropy) depends.
To search for some information in the documentation of a previous version you can use: [https://www.google.com/search?q=site:https://www.tensorflow.org/versions/r0.12+sigmoid_cross_entropy_with_logits() (hyper-link)]
To avoid all this calculations, I will suggest you to make use of Tensorflow's in-built functions like tf.nn.softmax_cross_entropy_with_logits.
In case you want to look into [tf.nn.softmax_cross_entropy_with_logits (hyper-link)]
[rrao's suggestions (hyper-link)] to add a bias term, and switch to the more numerically stable [tf.nn.softmax_cross_entropy_with_logits() (hyper-link)] op for your loss function are good ideas as well, and these will probably be necessary steps to get reasonable accuracy.
Your cross_entropy function seems to work fine.
I don't know what kind of loss you want to measure, but I think you are looking for something like tf.exp(tf.nn.sigmoid_cross_entropy_with_logits(y_true, y_pred)) or tf.exp(tf.softmax_cross_entopy_with_logits(y_true, y_pred)).
You should remove the softmaxOutput variable and use tf builtin softmax_cross_entropy loss function, it does apply softmax activation and handle the cross entropy loss.
In case you have less than one thousand of ouputs you should use [sigmoid_cross_entropy_with_logits (hyper-link)], in your case that you have 4000 outputs you may consider [candidate sampling (hyper-link)] as it is faster than the previous.
The latter I think it supported by sigmoid_cross_entropy_with_logits.
Finally you call your accuracy or cross_entropy ops on a batch or on the whole training set and TF doesn't care which it is!
It looks like you are just trying to get training loss out of your loop, thus you are passing your training batch to your cross_entropy function.
The value it returned is the same as F.binary_cross_entropy value.
You can just feed cross_entropy_sum = tf.reduce_sum ... to minimize.
By the way: tf.nn.softmax_cross_entropy_with_logits() expects unscaled logits ([https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits (hyper-link)]), so you should not perform a tf.nn.softmax() before using this option.
I always have the tf.nn.softmax_cross_entropy_with_logits() used so that I have the logits as first argument and the labels as second.
It seems the problem is that softmax_cross_entropy_with_logits_v2 needs more than 1 output class: [Cost function always returning zero for a binary classification in tensorflow (hyper-link)].
...instead of tf.nn.softmax_cross_entropy_with_logits.
The [tf.nn.softmax_cross_entropy_with_logits (hyper-link)] operator expects the logits and labels inputs to be a matrix of size batch_size by num_classes.
If you look at the [implementation of the operator (hyper-link)], the backprop value for tf.nn.softmax_cross_entropy_with_logits is:
tf.nn.softmax_cross_entropy_with_logits wants unscaled logits.
For example, tf.losses.sigmoid_cross_entropy.
The key to the problem is that the class number of you output y_ and y is 1.You should adopt one-hot mode when you use tf.nn.softmax_cross_entropy_with_logits on classification problems in tensorflow.
tf.nn.softmax_cross_entropy_with_logits will first compute tf.nn.softmax.
Although I am not sure why it is automatically flattening when you call softmax_cross_entropy_with_logits...
train_op = slim.learning.create_train_op(cross_entropy, optimizer)
The model is not converging, and the problem seems to be that you are doing a sigmoid activation directly followed by tf.nn.softmax_cross_entropy_with_logits.
In the documentation for the tf.nn.softmax_cross_entropy_with_logits it says:
Hence no softmax, sigmoid, relu, tanh or any other activations should be done on the output of the previous layer before passed to tf.nn.softmax_cross_entropy_with_logits.
I'd encourage you to use tf.nn.sigmoid_cross_entropy_with_logits, and removing your explicit sigmoid call (the input to your sigmoid function is what is generally referred to as the logits, or 'logistic units').
Also see this question: [What is logits, softmax and softmax_cross_entropy_with_logits?
because softmax_cross_entropy_with_logits is designed to work with the input to softmax, not the output.
You should instead use tf.nn.sigmoid_cross_entropy_with_logits() (which is used for binary classification) and also remove the softmax from Y_obt and convert it into tf.sigmoid() for Y_obt_test.
In this case, you should use tf.nn.softmax_cross_entropy_with_logits(), but remove the tf.nn.softmax() from Y_obt, since the softmax cross entropy expects unscaled logits ([https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits (hyper-link)]).
Another thing: It might also help to take the mean of the cross entropies with cross_entropy = tf.reduce_mean(tf.sigmoid_cross_entropy_...).
(sess.run([accuracy,cross_entropy], feed_dict={x: data, y_: labels})
You can use a loss that is already implemented for multiclass logistic regression instead of your loss: [sigmoid_cross_entropy_with_logits (hyper-link)].
The application of this function is a bit hidden, because it happens at the same time as the cross_entropy is calculated.
As side notes: 1) [tf.nn.softmax_cross_entropy_with_logits (hyper-link)] expects logits as input, that is, the values before being passed through the softmax function, so if the Softmax tensor in the loaded model is the output of a softmax operation you should probably change it to use the input logits instead.
2) Note that [tf.nn.softmax_cross_entropy_with_logits (hyper-link)] is now deprecated anyway, see [tf.nn.softmax_cross_entropy_with_logits_v2 (hyper-link)].
You are calling softmax_cross_entropy_with_logits on the output of softmax.
softmax_cross_entropy_with_logits should be called on the linear output of the last layer, before applying softmax:
According to the tensorflow 1.3 docs, the usage of nn.sparse_softmax_cross_entropy_with_logits is:
When your using softmax on your last layer and then calculating cross_entropy, combine together to a numerically stable tf.softmax_cross_entropy_with_logits.
I have tested your code and it seems to be caused by numerical instabilities in the simple cross_entropy calculation (see this [SO question (hyper-link)]).
Replacing your cross_entropy definition by the following line, you be able to resolve the issue:
By also visualizing the returned cross_entropy, you will see that your code returns NaN, whereas with this code you will get real numbers...
The complete working code which also prints out the cross_entropy per iteration:
Avoid the three lines of code and use the tensorflow version 
tf.nn.sparse_softmax_cross_entropy_with_logits
You got this problem because the arguments you passed into tf.nn.softmax_cross_entropy_with_logits are not what it wants.
From the [doc of tf.nn.softmax_cross_entropy_with_logits (hyper-link)]:
2) your learning rate is too high 
train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy) change this line to
train_step = optimizer.minimize(cross_entropy)
First you have tf.nn.relu and then you use softmax (with tf.nn.softmax_cross_entropy_with_logits_v2()).
Question 1:
tf.losses.sparse_softmax_cross_entropy
I managed to run tensorflow.nn.softmax_cross_entropy_with_logits_v2() with labels and logits with shape [None,1] where this tensor is index of action (category).
you probably want [tf.nn.sigmoid_cross_entropy_with_logits (hyper-link)].
@Aaron is already covering well the error regarding the numerical meaning of your loss, and why you should consider native Tensorflow losses such as tf.losses.sparse_softmax_cross_entropy().
When it happens,  tf.log(neural_network_model(x)) will output NaN (as the logarithm is undefined for null and negative values), and so will your cross_entropy loss.
cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(predicted_probabilities), reduction_indices=[1]))
The cross_entropy should be aggregated to a single, scalar value before minimizing it, using for example [tf.reduce_mean() (hyper-link)]:
[code snippet]
When I have changed loss function to loss = tf.nn.sigmoid_cross_entropy_with_logits(pred,y) and normalize input, then the net start to learn the patterns.
The computed cross_entropy value doesn't make sense, hence the result.
You're using square loss, not cross entropy, for classification use tf.nn.sigmoid_cross_entropy_with_logits(...), not tf.losses.mean_squared_error
Remove the softmax on y before feeding it into tf.nn.softmax_cross_entropy_with_logits
