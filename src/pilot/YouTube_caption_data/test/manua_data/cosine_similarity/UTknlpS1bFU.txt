Hi, My  is Abhilash Majumder.
I am a research scientist for Morgan Stanley.
Previously I used to work for HSBC holdings as a machine learning NLP engineer.
And I, I've also worked with Google research for the
language models and I've contributed and maintained.
Bert and Albert before that I have been mentoring students and
professionals from different organizations, like a Udacity.
Udemy, upgrad on, on different, on different machine learning projects natural language processing.
So in general, so I have been working in this mentorship array for a long time
and you can find the links to connect with me in the description given below.
So welcome to the Co-Learning lounge YouTube channel.
Today I will be explaining you the word embeddings from Jay
Alamar's, a blog post tech illustrated, a word embedding.
The Illustrated Word2vec also make sure to subscribe to this YouTube channel, because
we're going to explain all the important concepts of NLP in a series of videos.
In suppose in the next video, we're going to cover about
different topics like attention transformers, BERT, GPT
so let's get started with the illustrated, Word2vec.
So in this, in this post to everybody to cover the concept of
what to, how it was created, what are the different aspects of it?
So in general, when we talk about embeddings it is a very fascinating
idea because all the language models that we see today rely on embeddings
and beddings naturally give meaning to the words that are currently used.
And there have been several advancements in this, in this context to recent developments like BERT.
GPT 2 to transform based models.
But the base of all these models comes from Word2Vec, which is one of the oldest.
And it has been around 2013, since 2013, and most of the big
organizations like Alibaba, Spotify, they still use them.
And we're going to talk about how, Word2Vec embeddings are generated.
And how they are created and you know, by different different logic, like skip prom
and common buy before it's in this, in this current block written by Jay Alammar.
So the first thing how we can what, what do we mean by Embedding?
So let's say support.
What was we a hair?
Jay has given an example of that personality, right?
What are the different characteristics of a particular personality?
So it can have agreeableness contentiousness, negative emotionality, extroversion.
So let's say we take into consideration only introversion and extroversion.
So, and let's say for instance we have an introversion score of 38 out of a hundred.
So when we plot this into a scale we can see that you know
38 will be placed somewhere between zero and a hundred.
Now, if we normalize that that score over here from minus one to one we
can see that that gets translated to a probability value of minus 0.4.
It gives her a probability value of minus 0.4.
Now.
If we were to find another person with similar to me having the same
kind of introversion score then we need this particular score, right?
Because other than that, how were we able to identify
someone else with almost similar characteristics?
So in this case, we have only one characteristics that is the extroversion
or introversion score, but are but is that sufficient for our use case?
Because with the same introversion or extroversion
score, there may be thousands of different people.
Or are similar or maybe the similar to me.
So in that case, we need to expand those features.
We need to include more features.
And that is where the concept of embeddings comes into the picture.
So let's say in this, in this next example, we are
increasing the number of features from one to two.
And in this case, we are taking two different features.
So there, there will be two different values associated with it.
So suppose for instance Jay the author is, has taken two different values as minus 0.4 and 0.8.
Right now, if there are, if he wants to compare between two persons
with this on the basis of the same different characteristics he will
be analyzing them based on those two scores, which they generate.
For a person when the score is minus 0.3 and 0.2 that you see
over here and for a person to that is minus 0.5 and minus 0.4.
Now based on these two scores, how can we say the Jay is similar to person one or person two now?
In this, in this condition, it is very important to mention that in the space of Word
vectors in the concept of algebra, we see that there's a Cosine distance similarity.
Now, when we measure Cosine distances of any two different vectors The the less, Cosine distance.
The more similar that is, and the Cosine distance similarity is also
an important metric of finding the similarity between the vectors.
And we find that vectors which are along the similar direction,
similar lengths are more, are more likely to be similar semantically.
So.
Keeping that in mind we can compute the semantic similarity by using the
Cosine similarity of each of these individual you know features that we have.
And we find that, you know, a person, one has a similarity with Jay of around 0.7 and
eight seven, whereas person two has a negative Cosine similarity score that is minus 0.20.
So person one is more similar to Jay as we can see by this.
And this is where we have determined that, you know,
a word embeddings are very, very much important.
And this is where the concept of Cosine similarity in Word vector space comes into the picture.
So if we were to increase the number of features from two to let's, say five
or six automatically the vectors would increase in dimensions and to configure
or to find out a similarity between different people based on these features.
We have to compute the whole Cosine distances between all of them.
And so let's say in this particular example, the Cosine similarity
between Jay and person one taking five different features is 0.6, six.
Whereas between Jay and person two is minus 0.37, So the end of
this section, there are two different ideas and two central ideas.
The first idea is anything can be presented in the terms of vectors.
All the features can be presented as vectors, vectors, which are
effectively numbers or decimal values or floating point numbers.
And the second thing is that we can calculate how similar the vectors are, how how,
what is the distance between them and to what extent they are dissimilar to each other.
Based on the Cosine similarity score that we have.
Now, if we move forward to what embeddings, we can see that we will be entering
a new concept where we are going to analyze the features of a particular word.
Now Word2vec the traditional algorithm is trained on, on a billion Wikipedia corpuses.
And in this end we are taking it.
And similarly glove is another different model, which is also trained on a larger Corpus.
So if we are going to analyze the word embeddings for the word King, as, as, as mentioned
over here, we can see the float array of different values, some positive, some negative.
Now, if you were to represent it, represent it in a, in a, in a presentable manner.
By by plotting some diagrams and some colors on it.
We can see that a typical representation looks like this.
You know, this is the representation of a glove embedding of the word king now why this is
important because we may not be able to perceive you by numbers, how similar to vectors are.
But by colors, we are easily able to perceive what is the degree of similarity between two different
words or two different or the degree of this similarity between two or three different words.
So in this case, in this example, if we see.
That King man and woman are three different words.
And we can see this color contrast of each of these different words.
Now, if we observe very, very closely, we can see that the man, the color changes
of the man are very much similar with the color patterns observed in the woman.
These two words are hence very semantically similar as compared with King.
So men is closer to woman.
And the vector for man is closer to the vector for women as compared to King.
And this is represented you know, this is represented if we are analyzing different words.
So in this case, if we see this cluster of words, like, what are we in King, man, boy.
And if we analyze this colors, we will see that some
colors remain almost constant, like this red hue over here.
It's almost constant.
And this blue here overhead is almost constant.
These signifies certain characteristics, which are almost similar to all of them, but if we were to
observe queen and King, this, this first this, this first row and, and this row over here, the King,
we can see that the color contrast is not much, the difference is not much, which implies that they.
Both may have a sense of royalty but it is, it is.
For us, it is easier to imagine royalty because they are similar in
context, but for a machine, all of these are translated into decimals.
So our float arrays, so you can buy with the help of this only weekend, we
can convert these you know words into Word vectors, a few things to point out.
Is that as I mentioned, that there's a street rate column, two different ports.
They are similar because there may be some features which are common to all of them.
And also we see that women and girls are similar to each other in lots of different
places, which also implies they're there, that they are semantically similar.
Now, if we have boy and girl, they are also similar, but different from a woman or a man.
So this can be a cause of ambiguity.
And you may think that how can this, this happened because why?
Boy and girl also have places where they are similar
to each other, but they are not that much similar.
You know, as we compare women and men and the last few important things are.
There are you're, this is working and we are similar to each other and distinct from all others.
So that is what I was trying to imply that whether there has been a sense
of loyalty being implied in these words, in the embeddings of these words.
Now, if we move forward, we can see that just like we mentioned, we can also plot
these embeddings are vector space embeddings into, into multidimensional vector spaces.
Now plotting, this helps us to understand and visualize them clearly, and we can compute these are
the result and vectors of each of these vectors with the help of simple algebra vector algebra.
So if we were to formulate a famous example, which is given by King minus man
plus woman we would be left with something which is similar to almost similar to.
Queen, this is because King will have a different vector space, a different set
of array values particularly decimals man will have another array of decimals.
Women we'll have another array of the symbols.
So all of these three different vectors that we see this will be an a kind of a numeric
or an additive form of adding these different vectors to get a result and vector.
So in the example, given over here, we can see the color contrast of each of these.
So King has a color contrast like this, which the presence, the King vector man
has this man vector color contrast, and woman has this woman vector color contrast.
Now, if we were, if I went to subtract man from King,
we can see the, an intermediate layer of color contrast.
Where the colors are not matching or are not in phase.
And if you were to add queen with it, or I mean women with it,
we can see that the woman would get superposed on these colors.
And the resultant that we see over here is King minus man,
plus woman with all the super post and removed you know colors.
We can see the resultant as this color slot over here.
Now, if we were to compare this equation, that king minus man plus
woman, this, this final vector space with the word queen, with the vector
space of what queen we see that most of them have the same pattern.
Some of the, hue was at a bit darker and some of them are a bit
lighter, but in most of the cases, the pattern is almost the same.
This we can see a very important conclusion in this sense is that vector
algebra and the loss of vector algebra, particularly vector addition, and vector
subtraction becomes equally important in this current context to where, when we are
determining semantic inference or semantic, meaning between the different words.
And this is how the word embeddings are used.
Now the next concept is about language modeling.
Language modeling is very, very important in the sense, because whenever
we are typing and using the keyboard where you want auto correct, or, you
know, auto type features, this, this is a very important feature to have.
And the concept behind this is next sentence prediction.
Popularly known as NSP in NLP.
So NSP means that given two or three different words,
I want to predict what the next word is going to be.
And this is particularly what a language model does.
It takes two or three words and tries to predict what follows them in this
example that you see, we have input feature one, which is this word Thou.
We have input feature two, which is shall.
And what would be the output label?
No, based on the different train and the pretrained, Word vectors
that we see the word having the highest probability value.
Yes.
Predicted as the output.
Now, how does this happen now?
If we, for the time being, if we consider a traditional trained or a
pre-trained language model as a black box, we can have the input features
like Darwin, Charlotte, and we can have the predicted output, which is not.
This is actually in foot from the train corpus right now,
but in practice, the model does not output only one word.
It outputs rather a score of our probability score for all the
different words that are present in the pre-trained models.
Vocabulary.
Now, this can be a very large vocabulary because Word2vec glove and
all these kinds of embeddings are trained on thousands of different
words, preferably, you know, arguably Wikipedia and millions of context.
So in that case we can have different You know you know, we gotta have different features.
So when we are considering this fact, the most important thing
is having a probabilistic output for all the different words.
The keyboard application knows when which one to pick
which one to pick based upon this probability scores.
The ones with the highest scores get picked up.
If we see the figure below, we can see that this trained language model gets,
gets redirected to the output prediction with it is, you know, a list of different
probability values referring to all the words in the for model vocabulary.
And after being trained, the early language models, which were, which was discovered by
Bengio what calculate the prediction in three different steps, which are very, very important.
So for the first step is look up the embeddings.
The second step is calculating the predictions and
the third step is project to the output vocabulary.
The third step is very, very important and it's very memory intensive and computationally intensive.
So we will be looking at this towards the end when there are special features available on it.
Right now moving forward, the first step is discussing about
embeddings, the most important part, which we're discussing.
Now during the prediction, we use a lookup embedding table, or look up embedding
metrics where we have a, in this example that we see we have an embedding metrics
created with all the words in the vocabulary from aardvark containing sharp Dow.
And also in current words, slides, I said, why sense and white.
VA.
And there are millions of words in this vocabulary or the embedding metrics that we see now,
if we were to find or use this lookup embeddings, we would be returned with a prediction
value or a prediction probability score, and we would be able to extract the most meaningful
or the most matching word w way by taking the maximum value from this predictions.
Now, how do we train this language model now?
As a very famous quotation by Jr fit is you shall know award by the company.
It keeps, which is very, very relevant for the next topic that we'll wait to look into in
this context there are some important things when we are training, when we were trying to.
Train a language model.
There will be lots of text.
The input features are going to be enormous because we are going to train it on
all Wikipedia articles, all new news corpuses the second important vector is we
are going to use a sliding window logic where we are going to select a window size.
The window size is very, very important for for
selecting two or three or five different words at a time.
And we analyze.
What we analyze with the help of the lookup embedding that we saw earlier,
what prediction values are, what probability values that we are getting with
those sliding window values, sliding window words, and the sliding window is
a very important model because it generates training samples for our model.
And we'll be seeing that just know.
Now, if we take this example though, shall not make a machine in the likeness of a human mind.
This is a quotation by noon.
Now when we start the sliding window, if we take a window size of three, the
window is on the first three words of the sentence that implies Thou Shall, not.
Now in the data set that gets formed we can have two inputs that
is input one and input two and a corresponding output labels.
So if we were to order or progress like this, that we take the first two words in a particular
window as our input input one and input two, we want to predict us or of the output word.
That is the third word.
Now, if we were to go following this logic, we can say that thou shall as colored by
green, they are input one input two features respectively as not is an output feature.
Right now, we have generated the first sample of the dataset.
Now, if we progress along the similar logic, we will, we
will be generating a huge and very big data set of windows.
three, where we take the first two words of the window size.
As our input features and the last word that is a third word as our output.
Now a larger data set can be like this, where we have  thou shalt not make a machine in dub.
And if we take up to this, we can, we can see that make a machine.
If we take the law, the last two, the last example that
is a machine, a machine becomes input one and input two.
Whereas the output becomes in.
No language models in practice, they are trained by the sliding window technique.
Now there have been several advancements by using neural networks and various complex
models, but there is another, another class of models which are called n-gram models.
And these n-gram models rely on these kinds of sliding window techniques
for creating a CBOW and skip gram that we'll be able to look into.
Now, these are very extensively used and there is a block from Shift key that you can
find the link that is provided here on how the Android keyboard uses this n-gram, logic
and ngrams logic to create their own beautiful dataset comprising of sliding window.
Now if we move forward we can see that there was one of the important
feature is that as Jay has mentioned, that Jay was hit by a blank.
Now in this blank, we have to predict a certain word.
Now, given the context if we are given the context that Jay was hit by
a bus, we might have guessed that Jay was hit by a bus in the blank.
The word pursuit have been there.
But suppose if there was another blank in front of the word bus that that would change the
semantics of the sentence entirely, and that would indeed change the result and outcome
that is going to be predicted by the lookup embedding look by the lookup embedding.
No in this case, if, if we have, if we are given by, you know, any advert or any adjective, right?
So let's say red bus in this case, then red would have been the correct answer.
But what would have been like if, you know, if we are not aware of this, if another black was given.
So it turns out that accounting for both the directions
that is what's to the left and also the words to the right.
Leads to the better word embeddings.
And this is quite true from an intuitive label, because if I were to just
consider the words, which are just forward of me, I would not be able to
retain the information for the words which I have observed previously.
So that is why it is always important.
And all the language models use this concept.
We, there should be a bi-directional approach where we have a look
ahead and look back where we are using the words on the statistics, the
semantics of the sentence from both forward and in the backward direction.
That is an important concept of skip gram comes to the picture.
Now, instead of looking just two words before the
target word, we can also look at two words after it.
So this is another very important feature.
So if we were to just say that Jay was hit by blank, and then we have bus and
we have a continuation of a different sentence, then by probability value.
Well from a range of, you know, property values, we can get that the answer is red.
Given the reward red is in the vocabulary.
Now the data set we are for sure, building up by the model that we saw that is the ngram model.
It would look very similar to the example shown here.
Well, we have input.
One is byte input two as a input three as bus input, four as N and the output as red.
Because as we have mentioned, we are just looking to words before the target world.
So we can also look for towards after it.
This would not only improve the model.
As I mentioned before, it would also help to create better embeddings.
Now, this, this particular thing, this particular thing is called is continuous bag of words, model.
And this was describing is in one of the first Word2Vec papers by Mykonos.
And this architecture is shown to provide great results.
Now, instead of guessing a word, right, just you know, just the word by guessing the
context just before it, and after it, this architecture actually uses a very different
concept by predicting a set of different words or phrases when we are given a particular.
When we're given a particular phrase or a particular words.
Now, if we, if we get this you know, example Jay was head and we leave a blank after that,
then by or red bus, this particular phrase, we'll get the Mo the most the highest probability.
Are the, will be the result of the higher having the highest probability event.
Now this is the CBOW model where we have given a set of
words or a set of words, or a given order or a single word.
Right.
And we are made to predict what the context of it.
That means The resultant probabilistic, contextual vectors, right?
So in this case, Jay was head and we leave a blank.
So by your dead bus in this becomes our context vector.
So this becomes the CBOW result right now.
This is very you know diverse of this is a very universal of the skip gram model, where
we are given a context and we are way to predict a single word, a CBOW model, actually.
Tries to predict the context that is a phrase or a
group of words, given a particular word or a phrase.
Now, how is, how is this created is very important because it
is, it is very it is one of the most important concepts there.
So if you see this example by a red bus, and so if we have the input vector as red.
So the output can be all these values, right?
So first we have by.
Then we have a, then we have red, again, red will
not be taken because red, we are taking as the input.
So the next word comes in bus and the last one is in.
So, and we look and we go to the lookup embedding table, right.
And we go, and we see the probability values, and we find the maximum probiotics from them.
So, this is how the the CBOW gets calculated.
So we are using a sliding window concept, right?
So the sliding window gets translated at each step by window size.
Right?
We are taking some input features.
In this case, we are taking just the input feature.
That is a single word.
And we are trying to predict the context from it.
That is we are trying to look up the embedding metrics, right.
For our fixed window size.
And then we are finding the maximum value.
Probabilistic value output from that particular center.
So this is called the countinous bag of words model.
That is the CBOW model.
Now, if we were to move back to your previous you know, a single word,
given a particular context, we would go back to the skip gram architecture.
Skip gram Architecture is very important for predicting a single word, given a particular context.
In this example, thou shall not make a machine in the likeness of a human mind.
So if I were to have an input word in this example that we see overhead, that is not,
not, not, not, we're not taken as the input word that the target word is going to be
So these words in this vicinity in for this particular
window range, This would have very high probabilistic values.
And from them, when you slide our window to the next position, then
we see that the next one which gets, which gets created is a me.
That is the input word, which should be rather than not.
So, because we are taking the median position.
In this example, we're taking the median position as the input.
So with me, we have different words like shell, not a machine.
So we have different target words.
So if we move forward like this.
This is the combined example.
So the, in the first case, we have the example with not as our central
input word and all the words of the window target words, right.
And in the next case we have, the input has make, as I mentioned, and we have the
surrounding or the other, or the other words in the window as our target words.
Now, if we were to translate this sliding window one step at a
time, or depending on what the window slide, that is how much.
The window size is then we would be able to generate a big Corpus, right,
a big Corpus, where we have a, particularly a dataset where you have a
particular input word and we have the target words associated with it.
So this target words based on the probabilistic values associated with the lookup embedding table.
Now, if we were to this is this entire thing, right?
Creating the the, the, the data set from the from the sliding window concept.
This falls under the train, the language model.
Now, if we were to have our this sliding window is skip gram model.
And if we were to train it then we were to have.
And untrain model, right.
And we were to predict the next neighboring ward or the neighboring neighborhoods right now.
This is our data set and we can predict our neighboring words.
No.
We first start with the first sample of our dataset, right?
The first word is not, as we see over here in this example, they put
what is not, and in its vicinity, there are different words, right?
There is thou shall make a right.
There are four different words in its vicinity of window size.
Four or five, if we did not as one of the words that we used, the rest of the words are 4 number.
So from now, the next step is the lookup embeddings that I mentioned.
We look up the embeddings to generate the different probabilistic values that we have.
Right.
And we select the maximum of those . So in this way, we can just predict the
next word that is using the skip gram, you know skip sliding window model.
The next thing is calculating the prediction.
Right?
calculated by taking the maximum probabilistic value.
Now this prediction is, is then, you know, initially when we train a language model
or we, when we are doing a training based on a supervise task, right, there is
always going to be an error, which is, which gets a computer in the initial stages.
Now this error is due to many things.
This can be new to not incorrect parameters, incorrect output, right.
Incorrect weights, and in supervised learning.
We this, this learning is moderated with the help of wait update rules.
And in this case, the weights are nothing but the prediction vectors and the arrow vectors.
Right?
So in this case, we have an actual target vector of you know in this case we see on the left
as, as you know, and 00 & 1 that is binary, Right now, what, why, why binary is very important?
I will come to that later, but if we just consider in this
example, the original one, that is the probabilistic values.
In many cases, we see that we may get different probabilistic values and during
the initial training stages, now there is bound to be some error in the training.
So we have to mitigate this error.
So, how do we mitigate this after we get the prediction vector, right?
And we, and we subtract it from the actual, actual vector that we have.
Now, if it is similar to this, now this is the actual target vector.
Imagine this as zeros and ones for now, just for simplicity, right?
And this is the model predicted vector.
Now, when we subtract the actual target vector from the model vector, right?
Predicted vector, we get the edit vector.
Now, this is the weight that is good.
Getting that is that, that is analyzed as the weights in the supervised learning model.
Now the language model gets trained like this.
They get the error vectors and the updates.
The weights are particularly the vectors of each of these words, right?
And this is how the training goes on.
iteratively epoch and epoch until we get a proper balanced
you know, predicted vector with the actual vector.
Now the importance of this actually, you know, actually target vector being in binary is because
the logistic function has a very simplistic, you know being in binary has a very simple application
in the sense that it is computationally very fast because there are only two labels, right.
Zeros at once.
So there are no intermediate values either.
It is correct, or it is wrong.
And this increases the you know the speed and the
efficiency of any machine learning tasks that we see.
So that is why, you know, it is best to have these things in a,
you know, as a logistic function or, or as a binary function.
Right.
So all in all, we have seen how we can train a language model using the sliding window concept.
That is skip gram.
We have also seen how CBOW is used to find contextual words, given a particular set of words.
We have seen how we can use the lookup embedding table.
Two for for each individual words in the skipgram sliding window to get the most
most probable or the one with the highest probability as the next predicted word.
So this is the concept of next sentence prediction on NSP.
When we are trying to predict the most problem, what, by analyzing them in
windows, sliding windows , looking up the embedding table, getting the maximum
probability value and then training a neural network model in a supervised manner.
Right.
Where we have error vectors, prediction, vectors, try to rectify the
prediction vectors too, so that they could correspond with the actual vectors.
And so on.
Now, there is another important concept in the paper
of two  that negative sampling now, negative sampling.
As I mentioned in my previous case that there are three different, you know steps.
The first is the lookup embeddings.
The second is the calculating the prediction.
And the third step is outputting the.
Prediction project to output vocabulary.
Now this is very computationally intensive, and this is because, you know when we
are outputting the vocabulary to a large set there is a computation time during,
inference, as well as, you know, converting those probabilities to individual words.
Now there are two different ways to split it, right?
The first is generating high-quality word embeddings.
This is very, very good, good feature to have.
And then the next step is to use this high-quality embeddings, right.
And then train language model again.
So instead of, you know, having to train you know, these, these kinds
of language models and put it projected with our vocabulary, we can
have already create, we can already create high-quality word embeddings.
And then we are going to see the importance of this binary function that is zero & one
function that is a sigmoid function and why it is very computationally faster.
So initially we had this concept where we are waiting to have an input vector.
That is an input word that is not, and we are having
a untrained model and we're predicting the next word.
That is the NSP or the neighboring ward that is thou.
No you feel if I were to just come, but this was slightly different value.
Right?
So in this case I will be getting a value of zero and one for a wrong answer and right.
On some perspective.
So let's say we switch it to a model, right.
That takes the input and the output word.
And we are trying to predict whether it is correct or not.
So let's say if I'm taking the input vector as not
as in the previous case, And I did the output vector.
As, as though now I pass it through the model and I want to say, and I want
to see that whether the not that is the input vector is thou or not, right.
That means after not the comes or not.
So this, this can be thought of first.
This is very confusing, for example.
So let me give you a different example.
So in place of not, let's say a Musk.
Okay.
So let's say what is the probability, right?
That the word dowel comes after the word mask.
So if it, if it is 0.9, that means that it is correct.
That means it is correct.
That we have taken or assumption of correct.
Now, if it is very low, that is, let's say 0.5 or 0.1.
That means it is we are wrong.
We are wrong in our assumption.
That thou does not come after the word mask.
Now this concept is very computationally faster because instead of,
you know, trying to generate probabilities between zero to one, we are,
we can clamp them as either you or one that is binary logic, right?
And this is very, very, you know this speeds up the model performance to a lot.
And this is where.
We convert this neural network models to a basic logistic regression model.
And thus it becomes a very fast and easier to calculate these, these different probabilistic values.
So in this example that you see in the example make, so make shall, make, not
make a, all of these are initially one because when you're, when you're training
it at the, for the first time, all of these words will be in the vocabulary
surrounding words, because we have taken a sliding window size of five.
And we have taken as soon as we collect in the example that we do make was in the middle.
And there were two words on either side of it.
So all of these words that is shall, not a machine, all
of them come in the context of neighboring words, right?
Also initially all of them would be one, but how can we make, how can we four,
if we include all these ones, it will be very redundant and a waste of time.
Because as, as Jay mentioned, it will be a smart model because it is officer
21, a and M because although if you take a sliding window, all of the
words in the sliding window for a particular word will be its neighbors.
So if we take, if we take, make us an example over
here, then shall, not a machine, all in its neighbors.
So all will be won by this logic.
So how can we improve this efficiency?
How can we improve this logic?
So the next tip came is negative sampling where we take.
Totally different words, totally different phrases, right?
Phrases, which are from different different windows, right?
So let's say the first window was in paragraph one and the second window was in paragraph two.
So these are far apart.
These windows are far apart and we are trying to, you know,
create a different data set from this, where there is noise.
That means if I place a word, which is in paragraph one with
respect to paragraph two, it is not there in its context.
So it is not that in its neighbors, neighboring words.
So in that case, it becomes a zero.
This is called us negative sampling.
Now by negative sampling, we need that we are taking different, different work
samples or different different phrases from different parts of the text, right?
In this example that we see that not here, we can take a different output
where we can either leave it at blank, or we can take a different output word.
Let's say aardvark.
So not an aardvark.
They are not related in by any chance in any Corpus, right.
So in that case, it becomes a negative sample.
So this negative sample is used to create zeros for our models, right?
For, for our datasets, for our target values.
And this becomes a classic logistic regression model.
So in this case, just to summarize, we have taken our input word.
That is not right.
Our task is to is to is to alleviate or remove this project to output for vocab
this computationally intensive tasks, but generating very high-quality embeddings.
For this, we convert our model stupid model where we have either a yes or no answer, whether
this one is, is the NSP or the neighboring one of that input, vector or not right to do this.
We first create the samples, right.
We first create the samples and in the initial stages, Just all
the words in a particular sliding window will be in its neighbors.
So all of them will have initials.
I do have one.
Now, if we move forward, what we can create negative samples
by taking extracts from different parts of the works.
Right now, this will allow us to have a very diverse and a zero one based
Corpus, which is suitable for logistic regression as smart facilities.
And that is what is exactly done in this example that you see not
an on, not at all in the same Corpus and not at all in the same.
Semantics or the dataset, right?
So that is why they have a target value of zero.
And this is a negative sample.
Again, not in taco.
These are also having these are also negative samples and that is why it is a target value of zero.
Now with this dataset, this, we can classify it in a binary manner.
And this is all this nice contrastive estimation.
Now this is a very important part of the paper, which which was proposed during what Dweck.
And it is highly recommended to take a look.
This is how, what we have talked about from using you
know, our sliding window concept for Skip gram, right?
And  then we are egenerating the lookup table and regenerating the embeddings.
Right.
And we are then converting them to high-quality embeddings
by passing, by making the model a logistic regression model.
Right.
Bye bye by making the model.
Yes.
Answer by making the model answer.
Yes, no questions, right based on probability values.
And then we are creating negative sampling just to add zeroes to our dataset,
to make the data set more diverse and properly suitable for logistic regression.
And this entire concept is called a skip gram with negative sampling, which is abbreviated as SGNS.
Right?
And this is a very important concept.
And this has improved the performance and efficiency of the Word2Vec model.
Now the training process of how, Word2vec trained is very, very simple.
We have two different metrics.
What is the embedding matrix?
And one is the context metrics and learning metrics has all the embeddings
of the words that you see and context metrics is another metrics, which,
which is almost similar to the embedding metrics, but has the condition.
For a particular word now, embedding embedding metrics, as it is mentioned,
300 is common size for embedding metrics, but we can create any size.
And also we can have a vocabulary size for reading this embeddings as we want.
Like in this case we have 10,000, but it can be anything.
And we are using this, the model that we that we, that we saw earlier,
that is the negative sampling skip gram model of logistical question.
Right.
And then the next thing is that process of how training is done on this logistic regression model.
logistic regression dataset.
So the embedding metrics and the PO and the, you know the context metrics, right.
We are used by the lookup embeddings, right?
And then they're used to generate whether or not has the word
Adam taco or adult, which is three words as the next one.
So as it is mentioned, so let's say we have four words, right?
The input, what is not, and the output contextual words are thou Adam and taco.
What are the negative samples that we saw?
No, we proceed to look up the embeddings.
That is the embedding table, right?
For the input input word.
We look at the embedding metrics for the contextuals.
We look into context metrics, right?
Even though water mattresses have an embedding for everyone in the
vocabulary, they are effectively the same, but what we are using, we are
using the embedding metrics and we are using the context metrics to look.
If they are, you know, matching.
Look, if they're matching, that means we are having a mapping
from one embedding metrics to the other embedding metrics.
This is all the Word2Vec was trained.
And we by this weekend identified that Aaron and taco and negative samples.
Now the next step is to take a dot product of this.
The dot product is with the input embedding vector that we have and the context embedded.
So if so in this example, we are taking a dot product of
done what not with the word thou and the target value is one.
So input output, because 0.2.
No, not with Aaron because target has a target value of zero because
Aaron is a negative sampling and it has a value of minus 0.1 0.11.
And not again with deco has a value of 0.0 0.74.
So in this way, we did the dot products to get there values.
Now very important  function is Sigmoid function.
Now we can translate this function into a binary or a logist or a logistic regression
function by converting these bypassing this sigmoid kernel sigmoid activation, right?
When we pass this input dot outputs, the Sigmoid function, we get these translated values.
So for one, we get 0.55.
That is for not and thou taken together dot product.
Right.
Not an ad dot productive gives a 0.25 hours or at score and not and taco gives you a 0.68.
Now why we do this?
Because we need to normalize the scores between zeros and ones.
That is why we need to sigmoid.
Right.
And we can, again, compute these errors, right?
As we mentioned in the training stage, right?
We have the predicted vectors and we have the actual vectors, right?
The predicted vectors that we see over here, that target vectors are actually one, zero and zero.
But the signal, this gives the predicted vectors.
So we compute the differences between the target vectors and the predictive vectors.
So it is one minus 0.55.
That is a 0.45.
Right?
So, so you do minus 0.2, five minutes minus 0.25.
So this is the error vector that gets generated now, which is effectively the error weights.
Because see overhead, that array is about to target minus point scores.
The next part is the learning part where we update this works, where we update this vector right.
And this is how in the pictorially, this, this model works are getting updated.
This concludes the model training step.
So we have seen how the model gets trained.
The main important concept is that.
We are converting the model into a yes-no logistic type of regression model, where we
are trying to generate a, we were trying to give a word and a corresponding output.
word, and we are trying to say with that, this word is the neighboring word of that.
So this becomes effective logistic regression model.
Then we use negative something to generate zeros our for our target target labels.
Right so that we can have a negative sampling.
And then we are within, we have seen how we can use the word2vec, how we can
train the word2vec by using this negative, negative sampling, skip gram technique.
Right.
And we have seen the entire process of how these do we take the dot products of these two vectors.
That is the one text vector, as well as the impeding vector.
Right?
And we take the, we have the target vector associated with them, right?
We take the dot, dot, dot product.
We convert them to a sigmoid function to scale between zero and one.
And we compute the error error vector.
That is the target vector minus the error vector.
So because we have a supervised learning thing that is going on.
So as, as, as discussed earlier, Right.
So this is all done model training on the Word2Vec model training and stuff.
So this entirety, this entirety, this model training is effectively
a skip gram, a sliding window based, you know embedding model, right.
Which is trained on logistic aggression.
Now there are certain hyper-parameters like window size
and the number of negative samples that you want to date.
Now the two key hyper-parameters in the training process or
doing the size in our case, we have taken a window size of five.
You know what example that is the all shall not example, which Jay
mentioned right now, different tasks have different window sizes.
One heuristic is that smaller window sizes that is ranged between.
Two and 15 lead two embeddings were high similarity scores between
two embeddings indicates that the words are interchangeable.
That means that antonyms are often interchangeable and we are and if you are looking
at the surrounding words, that is good and bad often appear in the similar context.
So, what does that mean?
This means that, you know if you're taking very high or very large you know,
if you're taking very small window sizes, that is between two to 15, these
embeddings are, have this similar, very high similarity scores, because
there are chances that these words can be used interchangeably in the corpus.
Now, if you have a larger window size that is greater than equal to
15 or more than 15 then the similarity is a more indicative ness.
Of the relation between the words that is how semantically related word is to one
another word right now for a particular as, as a notification the gensim window, window
sizes, five, two words before and towards, after thus the same as we mentioned in our.
You know the thou shall not example that we see over here.
So and there are negative sample associated with it too.
Right.
So if we see over here in the negative samples, how it is used in gensim, right?
We take an input word, make we compute an output word, Charlotte, right?
It has a target value of one because Charlotte is in the
neighborhood of make in the slightly the same window.
Right.
Make an add-on and is not in the same window as me.
So.
It is, it has a value of, you know, that that is, it is a negative sample, right?
So similarly, these, these samples are getting generic.
So that was entirely the, all of the different things that, that was taken
different concepts you know, from what, in Word2Vec embeddings, right.
So effectively, what we see to summarize the most important features are that if we just
move back, we have, we have taken some important key points that we'll cosine distances.
One of the most important features, right.
And to the represent, any feature or any, any particular entity
be it a word or read a sentence or be it any real word entity.
We need vectors, right?
We need properties.
So features.
And we have, we can calculate the similarity between features by taking assigning distances.
Similarly, when we come to word embeddings, we have seen how
different words and their embeddings are generated, right?
As, as a tupple or an array of decimals, how we can compute the
similarity between different words by using now by using the word, you
know, the additive vector algebra that we saw over here, King minus man.
That's right.
We also have an NSP that is next sentence prediction in language modeling, how we
initially created a black box model and we tried to predict the output word, right.
Given a particular word.
And we saw how to how this uses in places, a lookup table or another lookup table.
To find probability values for each individual words in the particular, in the
particular vocabulary and try and gets the maximum probabilistic value as output.
We also saw how to train the language model, right.
But first we analyzed it using the sliding window, right.
Which is a very important concept insight that we have taken an example of skip gram.
Right.
We have taken an example of skip gram.
First, we have created a data set.
We are taking in this case, you have taken a data set size of three, right.
A window size of three to create our dataset.
But the first two words are the input features.
And the third, what is output feature?
Right.
In the next example we have taken we have, we have what, what the feature
of skip gram when we try to predict a word, given a particular context as
in the example mentioned by Jay was hit by a blank and blank, plus comes in.
Right.
Then we have a continuous back and forth model where we have a
particular you know, a word, a particular as, you know phrases.
And we try to predict the context or the next phrases, common phrases
in the words, in, in the, in the vocabulary or in the sentence.
Right.
So Jay was hit and if you leave a blank by or bus gets
predicted, so this is a continuous bag of words model, right?
We also saw how to train the samples right.
How , this example, we have users from a window size of five.
We have taken the middle board as our input word and the rest of
the words in its neighboring words,  are the target words, right.
And I'm just, and we create a data set like this now for training,
for training process, we already mentioned training process.
We have the input words and the target word associated with it.
Now in most of the cases, there are two different mostly different important features is lookup
tables calculate the prediction and projected output for governor now projecting to the output.
Vocabulary is very computationally intensive, and the training process constitutes having an actual,
actual vector and the predicted vector and computing the differences or the error vector from them.
And updating those vectors, right?
This is what is mentioned in this blog.
And then we have the negative sampling because we want to increase
the efficiency by, by elevating this project vocabulary part.
So we are creating very high-quality embeddings and we
are changing our model to logistic regression model.
Right.
Where do we ask only yes or no questions.
That means whether or not come with a doubt comes after,
not if it is true, then it is in this case, it is 0.9.
That means it is true.
And if it is false, it is, it is close to zero.
Right?
So in this case, initially, we will be having all of the score is one all the target values as one.
Because needless to say all of this, all the words in a
particular vocabulary in the same window, I didn't see neighbors.
Now we have to create negative samples.
What our training did, us negative samples are created by taking different
phrases from different parts of the text, which are not at all related.
So if we go down and we see that not an  are not, so they have a negative sample of zero, right?
Negative target of zero, not in taco or also not really.
So it becomes a negative sample again.
So once we have seen up to now is a step around with negative sampling.
And when we try to train this model, we are using two different
tables that is embedding table and the context, right?
Do both these tables are used for looking up, right?
So if we see not an thou, right, they have a target value of one,
not an aron and they have a target . So that is a negative sample.
So first we create, we take a word from the input input word, right?
And we say, to say, that output is thou from the context  and we have a target value of one, right?
Because it is always there in the window, but we also compute the input output.
That is the dot product of this.
Right.
And after that, we translate them to the Sigmoid function
because we want to clamp it between zero and one.
And after that we complete the errors.
Just like I mentioned before, we compute the edits by subtracting from the
target of vectors, the Sigmoid vectors straight and then we update those vectors.
And this is how the supervise training model of the logistic regression takes place by Skip
Gram, you know, nice contrastive estimation on skip when skipping around with Ringo the string.
So in this, in this context, we have, we have learnt a lot about negative
sampling, how negative sampling has improved the accuracy by converting the model.
to an entirely new model by improving the efficiency of the model.
This negative sampling has only allowed us to train on large corpuses in a very moderate amount
of time because instead of training it and using property district values for all the different
words, we are trying to have an additional based on certain question where we can have an logistic
logistic model, kernal or logistic function to classify it for us using negative sampling.
Now, there are certain hyper-parameters like numbers and negative sampling.
How much negative sampling.
Then we want to use, what is the window size and numbers.
Some heuristics are mentioned in this, and these are all,
some of the important features that I wanted to mention.
And definitely this blog is, is very good for having, for reading different other articles on it.
That is all that I had to cover.
And I hope to see you guys in the, in the next video tutorial.
Thank you.