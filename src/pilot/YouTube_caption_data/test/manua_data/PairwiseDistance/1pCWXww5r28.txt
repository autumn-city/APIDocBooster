[Alexander Rakhlin] All right, so it's a pleasure to welcome Ery. He received his PhD in Statistics
from Stanford with David Donoho.  He held
a postdoc position at the Institute
for Pure and Applied Mathematics (IPAM) and the Mathematical Sciences Research Institute (MSRI),
then he joined the faculty in the
mathematics department at UCSD in 2005
and his research has been on high-dimensional statistics, machine learning
and applied probability and he is
especially well known for his work on
detection and minimax hypothesis testing,
unsupervised learning and denoising in
signal and image processing, and we're
very happy to have him today.
[Ery Arias-Castro] Thank you, Sasha.
So, let me just share my screen, then.
Okay, so can you see this thing?
Yes? All right, so I'm going to talk about -
sorry, the screen is a bit busy here.  All right, so I'm going to talk about some work I've
done over the last few years on - that
sort of involve - it's sort of tied together
on - but by having graph distances play a
crucial role, and it's in collaboration
with a number of talented people
including people who somehow are in the
audience (Thibaut Le Gouic). I just saw him - and so I'll - let me start and - it's just a
collection of projects again tied
together in that way. I'm going to start
with the problem of estimating position
from distances.  It has various names,
graph embedding is one of them, 
dimensional scaling is another one. Graph
realization, graph drawing, sensor network
localization.  It's an important problem and has been for
quite a long time, with a pretty long
history from the at least 1950's with work
by Torgerson - I'll come back to that a
bit later. So here is the the problem,
roughly speaking, and this is vaguely
stated. So, we're given a weighted graph -
(V is the note set, E is the edge set, delta is the weight function on the
edges, and we are also given a target
dimension called d, which is often two
for visualization purposes, and the goal
is to find the points in space -
y1 through yn - in Rd such that their
pairwise Euclidian distances are
approximately equal to the corresponding
edge weights when the points are
indexed accordingly.
So, you know I'm just here, not to - I assume you don't know much about this - so
I'm going to just go through some basics. So, for example, the first thing one
might want to try is to essentially take
the two quantities that we are after - so
the Euclidian distances and the weights, the corresponding weights, and simply,
essentially put them all together
in some loss function. I know that the
sum is over the existing edges, and in
fact this has been proposed before and
it's sometimes known as non-metric scaling in the statistics literature, but it's
a nasty problem to solve.
It's highly non-convex and such, so
it's definitely only a starting point,
not the end of the story at all.
So, like I said, the problem has multiple
names and there are important connections to
the nearest neighbor search and embedding of finite metrics spaces - although in that area, people
use more sophisticated tools from
mathematics, typically. Also
there's a type of connection with manifold
learning which I'll come back to a bit later.
Okay, so when the graph is complete,
meaning all edge weights are available,
there is an exact solution, if an exact
solution can be found, and it can be
found by solving - essentially - by matrix
decomposition, essentially in SVD, and the
method is known as Classical Scaling
and it's one that Torgerson came up
with in the 1950's. So, here goes - so this
is a particularly - this is the most
well-known algorithm for this problem.  It, again, requires that all pairwise distances be
available, so we have the distances and embedding dimension d and then again the goal
is to find a set of points that satisfy
the pairwise distance constraint.
So the idea is the following.  We
are given the distances. What we do is
recover the inner products by double
centering the distance matrix, which
essentially corresponds to the weight
matrix in the graph formulation
that I gave earlier and, you
know, this is just simple algebra that you
learn when you learn about Euclidian spaces, but this is in
matrix form here. That's all you do, you
go from Euclidian distances to inner
products. Once you are in inner products and you're faced with a Gram matrix which
you then therefore want to decompose
in a Cholesky fashion, and that's essentially
what you can do with an SVD if you want,
and so, in more detail, here - in fact, this, it's an
eigendecomposition, 
because the matrix is symmetric so you
look at the eigenvalues of this double
centered matrix - this should be a two here,
actually - and the corresponding eigenvectors and you just normalize the eigenvectors
properly and put them into columns and the n rows provide us with the embedding.
So that's the classical scaling algorithm. So, again, this was suggested in the 50's, and it is very popular,
likely the the most used, by far, just like
PCA, and even then its robustness to
noise was not super well-known. This is
something we came up with -
there was earlier work by Sibson in the
late 70's and a bit more recently by
De Silva and Joshua Tenenbaum in 2004, in a somehow unpublished technical report.
We actually obtained a true perturbation bound with Adel Javanmard and Bruno Pelletier.
What we do is - actually
let me walk through the settings - so we have
center points - the fact that it's centered is not essential but just for
convenience because the coordinates I
just described as classical scaling returns a
centered point set. So, you start with the point set y1 through yn, and so
we assume it has radius - this is half of
the diameter row, and half with omega and
delta ij are the pairwise seeking
distances, and then, okay.  So, the
deltas are the true pairwise distances,
the lambdas are going to be the noisy ones
and then this measures how much
noise there is, this eta does that,
and if eta is small enough compared to
the half-width of the point set then we
have this bound on why classical scaling
recovers. So, classical scaling cannot
recover the exact point set no matter
what because it's up to a rigid
transformation, we only have pairwise
distances after all, which are of course
invariant with respect to rigid
transformations. So therefore, we cannot
hope to recover the point set, we can
only hope to recover it up to a rigid
transformation. So, because the point set
is centered by rigid transformations,
meaning the orthogonal group here, and anyway, so this sort of result is hard to parse when
you're reading it but it's relatively
simple.  Intuitively, it is saying
that if the noise level is eta,
the overall noise level in the distances,
and although we have some noise in the distances, the quality of the recovery
degrades gracefully.  So
this is the quality of the recovery and
and then it's bound by a function of
eta
[Philippe Rigollet]: So, Eri, I have a couple questions,
the first one is -  what is the half width? And the second one is - so, this is for worst case
noises,  where you're just looking at - can we hope to get better
bounds if, for example, we assume that the
noise is like Gaussian or something like this?
[Ery Arias-Castro]: Yeah, so this other work by... his name escapes me now. You know, I think is his
last name is Sun (Lei Sun) and I think he's in Toronto, who has more recent work with -
can find a work that involves random matrix theory.  So when you assume that there is
randomness, you can hope to get much more precise results.
So this is just a perturbation bound ala,  you know, Davis-Kahan, for you know, like, eigenspaces.
The other question - what is the half-width? So the row is -
the half diameter, so the radius? And
the half-width is the the width of the
narrowest band that contains the point
set,
and a band is simply made of two
hyperplanes parallel to each other.
[Philippe Rigollet]: Okay, thanks
[Ery Arias-Castro]: Does that make sense?
[Philippe Rigollet]: Yes.
[Ery Arias-Castro]: So we - and in the same paper we
talked about how to apply this to
obtaining results for isomap.  One of
the most iconic methods for manifold
learning - I don't know if I talk about it
later in the talk here - but
that's the connection with graph
distances. I think I actually talk
about it later.
Okay, so this is all good when at least it is robust as in more like
stability to noise. I'm not talking
about outliers here, but this this method of
classical scaling requires knowledge/ availability of all distances even
though they are possibly noisy.  Now what happens when some of distances are missing - which
tends to be the case in practice. So,
it turns out there is actually a theory
on that - that actually asks this
kind of question. What we're tackling here is
a more algorithmic part and methodology
part to the question.  There is a more
fundamental question where you just look
at the adjacency matrix of the graph
without taking into account the weights,
and ask the question as to whether
generically, the graph can be embedded
uniquely, and generically here means that
essentially for almost all choices of
edge weights.  I won't say too much more
but there's a whole theory on this
called rigidity theory and it's closely
related to detecting the problem at a different level.
Here we're more interested in 
actual methods. The one we're
actually going to talk about is on
using graph distances to fill in the
dissimilarities in the weight matrix, 
meaning we're going to use graph
distances to estimate
missing pairwise distances,
and actually, this idea goes back to
Kruskal and Seery in 1980. It actually took me
a little while to discover this, even
though the same method was rediscovered
by Shang et al. in 2003 and then they
suggested a variant a bit later, and a
few others. In fact, I don't have it
here for some reason, but the isomap
algorithm for manifold learning fills in
the missing or so-called - the
distances using exactly this method. The
note that was also written since
rediscovery in 2000. So anyway, this is
the simplest method we can think of
so that's the one we're going to
study in more detail. There are other
methods that people have suggested. Again, this is for the case where we want to
- same problem - we want to embed points,  but some pairwise distances are
missing. So there's a somewhat greedy
approach that does the following, it
takes a clique - meaning all of a large enough size, something has
to be at least in dimension D, has to be
at least of size P plus one, I think, and
just because in a clique all
pairwise distances are available
it's just embeds that clique using classical
scaling and then at each iteration and
thereafter it compares any available
point by trilateration, and trilateration
is essentially like a triangulation, so it's positioning is based on
distance as opposed to the positioning
based on angles. So, a very intuitive
method - very greedy - and therefore
susceptible to noise, although that
hasn't been quantified as far as I know.
So there's another one where
instead of embedding just one clique, one
embeds a number of cliques,
enough that the cliques overlap the entire
graph - or cover the entire graph - and then
one tries to so-called 'synchronize the
cliques', meaning essentially align them
all together because they have to live - so
you saw by bending them as if in
parallel spaces, and then sort of stitch them together in the same space.
There's also going to be one less greedy and there are a
few other techniques that have to do
more with optimization.  So one is by
majorization and the other one is
by semidefinite programming relaxation.
Okay, so let's focus on graph distances from now on.
So here is an illustration, so we have points in space - of course those are
not provided to us - what we
have are the pairwise distances between
points that are joined by an edge here.
So, imagine just having that, and then
imagine we want to know the distance or
compute the distance between the two
end points here - so the square at the end -
and what this method does is, it just
computes the shortest path distance in
the graph and this is the truth.
So in red, this is our estimate, in purple
the truth, and we can see that it's going
to work and that proving that sort of thing is actually not very complicated.
Let me walk through it. So again, delta r provides the distances that are available at least,
The required distance is what most people here would know what they are - but
anyway if we take two points in the graph vertices in the
graph (i, j) and we look at all possible
paths going from i to j and then
some of the weights along the way and take the infimum over all those things,
meaning take the infimum of all the paths of their length. This is called the
length of the path. Okay, this is, of
course, very classical in graph theory
and so here we are interested in a
particular graphs that are known to talk
about consistency. We talked about actual points in the
background for which we have pairwise distances. Of course we don't have
the point, but we want to analyze the
algorithm based on assuming that we have
points so we can talk about notions of
consistency when our goal is to recover points.
Okay - so, um.  *Laughs*  I'm just laughing because there is a - Zoom hides a part of the slide here, but I will do what I can
So, just a reminder that E is the edge set and it provides an indication on when
weight or pairwise distance is available
and here is a special case where that
happens when the pairwise distance is less than
or equal to some r and this is typically
what people call sensor network
localization in engineering.  So, it's a
special case - a special variant of
dimensional scaling with missing entries,
when the entries are missing according
to their magnitude.  Meaning the distances
are missing according to whether they
are less than or greater than some
threshold, this being the simplest case.
And so we have this result that's what
refines existing results in the
literature.  There were other ones before
us, including in a technical report -
a somewhat famous technical report by the isomap authors, and that was there to
essentially, for them, justify their
algorithm.  It's a simple mathematical
model, so we refined them down and get
the square here as opposed to not
having a square which was a difference,
although did they do it in the curved
situation we do it in the flat situation.
Let me walk through the results. We
have a set of points X1 to xn and those
are the real ones. y the 1 through n
are the ones that are returned by a
method. That's how the
notation goes here. Epsilon is the
density of the points inside their own
convex hull and convexity here plays an
important role because we're using paths
in the graph to estimate Euclidian
distances so if there is somehow the
xi's are not dense in their compact set,
imagine if there's a hole. Then we
wouldn't be able to cover over all (?) distances accurately.
Okay, so that's it then (?) is a
connectivity radius of the graph meaning
your gives us what this is are available
and when epsilon is small enough
relative to R, then we have that, well the
first inequality here comes to (?)
space the straight lines or -
the line segments are shortest, but the
second part - this part right here is
non-trivial and just provides a
measure of performance on using
graph distances to estimate (?)
distances.
Okay, so I went through this
already there was another paper by
Montanari, asmall paper on
proving exactly the same thing in your
context but with a less precise
bound, like I said.
[Philippe Rigollet]: Ery, can I ask a quick question about it?
[Ery Arias-Castro]: Yes.
[Philippe Rigollet]: So the first thing is, so this
shortest path distance is always bias upward. Is there - do there exist
some techniques that people use to
try to correct the bias to make sure
it's more centered? So, that's my first question and my second
question is if I assume that my xi's
are iD in some compact sets, for
example - what is Epsilon in this
case?
[Ery Arias-Castro]: Okay, so if you're in dimension D
then a full dimensional right then if so
is going to be on the order of log n
over N to power 1 over D.  So there's the curse of dimensionality here.
I mean obviously the method is local, so
there's one surely yeah.
[Philippe Rigollet]: The second question was, do people try to correct for the bias? Are there any
heuristics to correct for this path?
[Ery Arias Castro]: Yeah, I think that's what this other paper I
mentioned by Sun that incorporated
that, and they used tools from random matrix theory
to understand - it's not exactly the same context as here,
but they are able in the context of
classical scaling to correct on bias
that they're able to show exists. I don't
know, I think it was even possibly
new to their research, but even though it was a bias that's again a very
particular setting, but - and then they go
after the bias and indeed try to
compensate for that. I wouldn't know how
to do it here, but it's possible you
might be able to do it assuming some
randomness on the exercise. In the
absolute, anything is possible
[Philippe Rigollet]: Thanks
[Ery Arias-Castro]: Okay, so moving on to something a bit different - well actually not -
the context is still a bit different in that we
are now on the surface and then soon
enough we won't consider estimating that
distance away with unconstrained so
going to be a variant and this basic
problem we've been talking about and
this is joint work with Thibaut Le Gouic, who is in the audience.
So in manifold learning, this is a
situation where points are in space but
nothing our low D instances are big G
code encode meaning high-
dimensional space in principle, and you
want to embed the points.  Again, the
problem is the same, but now you sort
assuming to curve the or to avoid the
cursor dimension right he assumed that
the points are in fact on the low-
dimensional surface somewhere in space.
the only other available just the the
points so.  It seems a bit different
because we do have access to a point
manifold learning but the
situation happened to be quite
similar in that we only trust the
shortest distances because those are the
ones that are close to who so many trust
is shortest you clean distances because
they're close to the geodesic distances
on the surface and those are the one we
are after. This would be a situation
where we want to estimate the distances
on the surface. There's been a lot of
work on that in particular isomap into
2000, there's an important paper in that area. There's also a tight connection with
motion flattening in robotics although
they tend to operate in more complex
spaces because it takes into account not
only position but also other
information available in the robot like
any constraints such as speed or
orientation acceleration and such.
Okay, so let's consider a subset which
again should be seen near the surface
really, but this point can be abstract in
some Euclidean space and
in principle high-dimensional space and
intrinsic distance sensory what people
would call it is existence in other
settings it's just simply what you think
it is, which is that you constrain
yourself to be to remain on s and
otherwise find the shortest path on s
joining two points. That's one way of
phrasing it, and what we have again is we
don't have the surface s itself we have
point sets on the surface and our goal
is to estimate those intrinsic distances.
So again, g denotes intrinsic distance on
s. So again, we're going to use graph distances and
in this case, because, well, we don't
really make any particular assumptions
on s except for later on, smoothness, so
we are sort of bound to operate locally
and the way to do that in this context
is to look at the graph distance between
an r ball neighborhood graph I built on
the point set so same setting as before
but now we are in a curved situation.
This parameter epsilon indicates the
density of the points on the surface and
so this was already proved by Bernstein et al., those are the isomap people.
Actually Bernstein was brought in to, I think, help with the proof and the
person wasn't part of their co-authors in
the famous isomap paper
published in Science magazine, but they
they worked on the proving that the
methods worked in this thing somehow
left unpublished technical report
which is actually quite a good paper. Okay, so they came up with
this r bound essentially by more - taking things from scratch and
what we did was, on this paper, we generalized on this paper, we generalized it and we also
reproved it using classical results by
Dubins in the 1950's on essentially the
early investigations on motion planning and we were able to prove
essentially the same result they have.
So, let me walk you through this result.
We have some very mild regulatory
conditions on s and we do assume
that the shortest paths on s have curvature
at most Kappa, which is essentially the
case when s is a submanifold with maximum
point wise coverage through Kappa, and in that case we have this bound right here.
Okay, so our remainder is the connectivity
radius and so yeah, sure -
so there's also so r cannot exceed the
reach, the reach is not only bounding the
the curvature locally, it is also bounding how much you have to jump to go from one
area of the surface to copy to different
area of the surface and this is
typically we don't have a picture. It's typically represented by a bottleneck. It would be
like the the width of the bottleneck, so
it would bound the width of the bottleneck
if there is one, and if r is sufficiently
small compared to all those things, then
we have this bound, so this was bounding
the shortest path distances on the r
only would graph by essentially one plus
something of the order of epsilon over r
times the intrinsic distances and now
this one gives you the other way around.
Okay, and we have the bound in r squared
on this side.
So, that's for knowing the estimation of
intrinsic distances using graph
distances. Everything sort of works the
way we would expect. What's a bit more
novel in the paper we have with Thibaut 
is on our estimation of shortest path
distances, but when the paths are
constrained to have curvature at most
some number Kappa and I got interested in
this personally because of - with previous
work with a student on using these sort of paths to try to unfold
a self-intersecting surface. It was like a
variant of manifold learning but when
the surface is allowed to self-intersect
and the goal was to - well, still the same,
to embed it - in a Euclidian space, so find
a parametrization in other words,
despite the fact that we can't self-intersect, and so we started looking at
curvature constrained shortest paths or
yeah that's, like - as the math does but
with a curvature constraint to - anyway I
won't go too much into detail back here
you can ask more information later.  It did turn out to be useful, although the problem
remains hard, and as far as I know we're
sort of the only one tackling it.
So, anyway, but in the abstract we can at
least consider this problem of, so now is
the same as before but not as a
subscript Kappa, so we're looking at
essentially shortest paths but with the
constraint among those paths
that curvature at most Kappa both ways,
and so we want to again use choice path
distances on graph, but now of course we
have to move the - actually is not
quite 'of course' - at the end we see
that's not necessarily the case, but in principle we
like to come up with a notion of
curvature for polygonal lines and
polygonal lines is what you get when you
consider shortest path distance on the graph.
The corresponding paths in Euclidean space are polygonal lines, and we
discover there are a number of
such quantities in the literature, and
the one we found most useful is this one,
is I think one of the most intuitive
as well. Where for a tripIet of points - and
the order here matters - I mean to the fact
that's y here being the center matters,
and what you do is - I think you have an
image here? Yeah - So, imagine that Y is
this point right here in the middle this
will be X and Z or the other way around,
doesn't matter but Y here is in the
middle and what we do is we - even though
this could be in any dimension, when we
have three points like this is a unique
circle and that goes through them, at
least as long as they're not aligned, and
in that case we take one over the
radius of that circle to be the curvature. So
that's true as long as the angle that
the segments make here is obtuse. If
it's a square or less then we just
define the curvature to be infinite. Okay,
so this one is particularly handy for us
because it has this simple but desirable
consistency property, where if
we fix a curve that's twice
differentiable for which therefore we
can define a curvature pointwise, then
if I fix a point somewhere so if it's
parameterized by A and B here, and I fix a
point somewhere between A and B and then I approach that point - so again, if you
see it as a curve, as a geometric
object, the point is gamma of s - and I
approach it on both sides of the curve
by points, then the curvature in the
middle, the way it's defined just in the
previous slide, converges to the
curvature of that curve at that location.
So that's obviously something where we were
wrong, and we were able to prove
something better. This one took a little
bit of effort, that we actually proved a
known - so this one is
as I approach and as the
point in the middle is approached on both
sides, and that this one is not to be the
case. You can actually bound - it's not an equality
though - but we can bound the curvature of
three points on a curve, as long as
you're not too far away from each other,
by the curvature on the curve,
[Question 1]: What goes wrong when you have the angle be acute or square?
[Ery Arias-Castro]: Yeah, so then it doesn't make as much sense to have y in the middle, in a sense.
So, if - that's where, perhaps, I could use my tablet.
Okay, let's see if I can make this work.
So, if you have - because if you have three
points like this, this would be Y, say - that's Y, X and Z.
So if you defined this way the circle here, it could be - circumscribing the circle
could be small, but later the way we use
this, we want to bound the curvature on
the polygonal line, right. So here if I don't, if I just say it's one over the -
circumscribe the raidus - the curvature at Y would not be too small - sorry, wouldn't be  too
large, right? But here, I want the curvature at Y to be large, because
the circumscribing circle - I mean I'm
gonna do a terrible job here - but it's
something like this, right? That will be
the circle that goes around right and
it's not particularly I mean the radius
can be quite large, right. So the
curvature could be quite small at Y
even though Y here should have a
pretty large curvature.
[Question 1]: I see.
[Ery Arias-Castro] I mean, the art is not
particularly precise here. Okay, 
I have to stop sharing.
[Question 1]: Thank you.
[Ery Arias-Castro]: You're welcome.
By the way, here it doesn't have to be at pi over two, it can be almost anything, because
we want, essentially, in our analysis, we're going to restrict ourselv - we're going to make sure
the paths with some trick, you're gonna
make sure that the paths have links that are -
that have a large angle - a very obtuse angle between them, but
this should be at least - there should be
a bound here, otherwise like I said you
run into trouble. When using - at least
when using the curvature in the way we
intend to use it. Anyway, so armed with this lemma in particular, we're
essentially able to do what we want to do but for technical reasons
we are actually not sure if it can be
avoided
but for technical reasons, we use
not a ball labeled graph, we use an
annulus labeled graph in which we
keep - so the edges that are of length
less than equal to r are kept - they're
available - and then the one that are less
than equal to r/4 - something like
that - are removed from the graph.
So it can be seen as rigorizing the graph. So we do that mostly for
technical reasons and now, like I said, we
had this notion now for the
curvatures, now we can consider
short distances in the graph. Among - so,
defined only using paths which as
polygonal lines in space have
curvature at most Kappa and then we
are able to compare the two.
So again, our goal is to estimate the
intrinsic distances with curvature
constraint, so I want to estimate g here -
g sub kappa. Now I am able to define the
discrete equivalent or at least analog,
and now I have an upper bound and lower
bound. So the upper bound is essentially
similar to what we had before,
I just need a little bit of leeway in
terms of the constraint I put on the the
polygonal lines that the graph adds,
so a little bit of leeway this is close
to kappa, so kappa prime is close to kappa as long as r and epsilon are
small, so I want r to be small and epsilon small, relative to r squared. We have no
idea if epsilon over r square being small is is
needed or not, perhaps epsilon over r small
is enough? We don't know. And we have
a bound that looks very much like the one
we had before in the flat - in the
unconstrained situation, and yeah,
in the other direction you're going to see
that the result looks a bit different. So,
this one does not require much at all.
It's possible that this actually is
infinite on this side and that's why
this just doesn't come with any
condition on the s really, but now we're
going to assume that s is smooth enough
in that the shortest paths on s have
curvature at most Kappa, and this is a
case - for example, if s is a compact
connected C2 submanifold with
with boundary that's either empty or
also C2, in that case we can take Kappa as
being the maximum of the curvature on s
and the curvature on its boundary, but
there are strange things that happen,
the theory of this is surprisingly complicated and has been
investigated in 'Spheres of Paper' by the
Alexander collaborators, for example.
Okay, so here's the result that, sort of, completes this one. So again, here we have a bound -
bounding the short-bound distances on the graph with the intrinsic distances from the-
on the surface.  I'm talking about the
constraint ones, and in the other
direction, in principle we would have to
have the other way around. We would
like to have this here bounded from
above by something that involves this,
okay? It turns out we can do a bit
better than that. It turns out that the
unconstrained shortest path distance in the graph already have curvature, at most, something
that's essentially Kappa plus something
on the order of Epsilon over r cubed, so
at this as long as Epsilon over r cubed
is small, which again we don't know if
it's optimal, but as long as Epsilon over r 
cubed is small then the unconstrained
shortest paths in the annulus graph -  that's
important - already satisfied the
curvature constraint, so therefore the
other side of the bound comes from the -
we can get the other, the reverse inequality essentially from what
we know about unconstrained paths.
Any questions so far? I'm going to go very
fast over this next one, which is anyway only tangentially related, but I
worked with a very talented undergrad
student who came for a visit - actually he
was a master student, I guess that's the
equivalent - and what we did was looking at
constructing random paths with smooth
realizations using this sort of a notion
of discrete curvature. And what we did
was - so this is a sort of realization that
you get out of this process. So it looks
pretty - I mean we were pretty happy with
the way it looks.  So, if you try to smooth
some - like say if you take a grand motion so he was
gonna be in dimension two. So if you're
if you take a grand (?) motion in dimension two
and smooth it out, you see that you
get some- not something that looks like -
something nice like this, which, you know,
has an amount of randomness and is smooth at
the same time. Another picture here, but
you can go to the paper, we have some
pictures where it doesn't agree with
what you hope a smooth, random path
would do. So what we do instead this we -
let me actually walk you through, since I
have you here but we'll go a bit fast. So I have a sequence of Theta which are going to play
the role of successive angles.  I start
with a direction space drawn uniformly
at random, for example, and then I update
the direction by essentially rotating
using the angle Theta at that stage, and
each time I take a step of
length one, and then I consider
essentially the resulting polygonal
line. Okay, so again I take a step of
length one in one direction, drawn at
random - uniform at random - and then I take another step of length one, making an
angle Theta - Theta J, I guess, with the
previous segment.
Okay, so that defines - if I take n steps
that defines the polygonal line which I
then parameterize as I do here. Okay,
so we actually, in the paper, we look at
what kind of conditions can we have
on the angles so that the resulting path
has what we want, which looks something
like this. We want finite curvature, and
it turns out that we came up with - this
is the simplest model that we came up with -
which is essentially a random walk on
the angles and so construction is
essentially that random walk on the
angles, and when we have that - importantly
the angles (the innovations),  they
are between minus alpha_n and alpha_n as well.
Actually, I say it right here.
Okay, so if that's the case in Alpha and then we had this limit - n only -
so n cube times Alpha squared
converges to a positive constant and we
have a consistent process limit
that has the store property that we want.
It is twice differentiable, in fact
consistently differentiable, it's likely a bit
smoother than that even, and then
it's already, as represented here, you
need speed parameterized and it's
curvature at time t is bounded and
actually bounded by - so you're given by
this quantity here - two-thirds Kappa times
the absolute value of B_t.  So let me just say two things: so X_t here is the
the polygonal line we talked about before. That's our discrete process, and then the
process converges to - has two random
concordants - so u, which is uniform at random
in the unit circle, and independently we
have a Brownian motion dimension one.
This i here is a complex i - i square is
equal to -1.
[Philippe Rigollet]: May I ask a question about this, or potential - okay, let's say I want
to build curves that have constant
curvature, right? So it seems like the
right thing to do would be to consider
second-order Markov processes, like
you need to actually take into account
three points if you wanted to do this.
So, I guess my question would be, do you know what would - do you
have any - okay, so I guess this particular
Markov process, this particular
continuous time process is not
particularly intuitive to parse, but
are these things studied in the
literature if I actually try to get
continuous processes or even curves with
continuous - with a constant
curvature?
[Ery Arias-Castro]: So, I mean, if the curve is in dimension
two, if he has constant curvature as a
piece of circle, right?
[Philippe Rigollet]: So is this what this would converge to?  No, right, you
would actually flip the direction every
once in a while.
[Ery Arias-Castro]: Yes, so it doesn't have
constant curvature, it has a finite
curvature.  Did I say constant?
[Philippe Rigollet]: No, no. You
said finite.
[Ery Arias-Castro]: Okay, it has finite
curvature, but it's - yeah, so he actually,
the maximum curvature is so - t runs
from zero to one, so the maximum
curvature is essentially proportional to
the maximum of the grand bridge, that's essentially what the maximum curvature is
going to be. Point wise we have an 
actual - the process is simple enough
that if you have an actual formula for
it. I mean, constant coverage in
dimension three possibly then, that's
what you're talking about? Like when you actually
have - I don't know if it's possible or not,
but yeah, so anyway, the extensions are
possible because we didn't look into
dimension three or more but that's
possible. You could in principle have, you
know, like, make it nice so that it
looks nice in dimension three. So that's
you know, like, in dimension three you have the
curvature, you also have the torsion to
take care of, and we haven't looked into
that. We thought about it, but we already
had enough in our hands to do some things.
We had these four, yeah.  Of course, there's much more sophisticated
theory, I mean very sophisticated theory
in random surfaces instead. That's no -
if you have any students that's a
whole different level obviously here, you
know, it's very simple but we didn't find
anything of that sort in the literature is
surprising to me. That it was nice as is.
Right,  like I say it was a bit tangential,
let me come back to a more classical
flavored problem, which is again estimating
distances, but based now on just the
adjacency information. So now, after just
ignoring this last project which I liked
very much but the other ones we had
weighted graphs, right, that we're dealing with.  And now let's just assume that we only
have the unweighted graph available to
us. Can we still do something? And I have
to go a bit fast because I'm running out
of time here.  So at least let me explain
the basic premise and jump to the the main
result we have. So, like I said we have
available adjacency information and
the graph will be undirected here. So the
mode we are assuming is a latent
position model in which - or sometimes
called latent graph model - in
which we have points in space that are
unknown to us. In fact
that's what we want to recover, those are
in fact the parameters. Somehow the
dimension is denoted by v in this
particular setting. Let's ignore
that, and our model is the following.
There's an edge between i and j given
the points, with probability Phi as a
function of the distance between the
corresponding positions in space. Okay,
and Phi is going to be long
increasing, of course with values in [0,1]
because it's got a number probability.
This is sometimes called 'link
function statistics'. For example, it is
similar to what a generalized linear model, how a generalized linear model is defined initially,
so the same vocabulary could be used and we assume that the otherwise the agency
entries are independent of each other.  Okay, and we can deal with the case where
the link function if i is known or
unknown, and the goal is to estimate the
pairwise distances. We know that once we
have the pairwise distances then we can
use classical scaling to embed the point. So our goal can be defined as being this.
There is some literature on the topic,
including perhaps the most relevant paper
somehow was this link prediction paper.
Which almost by coincidence we found it.
Actually, it is very closely related. But
there have been some studies on this
problem, including on so-called 'dot product graphs, random graphs,
which are based on linear products
as opposed to distances. Okay, and so
again we're going to use graph distances to
estimate the underlying Euclidian distances
but here, so like I said, we can
use - we can deal with
the situation where the link function is
unknown, and in that case the scale is sort
of lost forever. So, not only - we cannot
only recover the points up to a rigid
transformation, in fact up to a similitude, and that's otherwise, yeah,
we can deal with the case where
phi is unknown, and in any case we're
going to use graph distances to estimate the underlying distances between the
points. The simplest case is when phi is
the step function, so it's one if the distance
between two points is at most r and otherwise this is the random geometric graph setting.
In that case, so it works as intended, so
the d hats are r times the first path
distances based on the adjacency matrix,
and we have a performance bound on that
that looks very much like what we had
before, and we also have an information
sense lower bound that matches it. So
that's the simplest setting, this is a
proof of concept. We have - those are the
latent positions
and this is what we would cover with
various entries for r - values for r.
As before, convexity is crucial because
you know, if these are the latent
positions (so they're not dense in
their own convex set) then those are
the estimated positions (that's essentially
the best we can do with graph
distances). We are able to extend this
beyond step link function, that's - very
crucially phi has to become 0 at some
point, so it has to have compact support,
that's very crucial. I can say - so
actually, you can do things when that's
not the case, but you have to do other
things. And anyway, so we also have a
result on this that now depends on how
fast - so Alpha here dictates how fast Phi
approaches zero and that Alpha appears
right here in the rate.
Actually, that concludes my talk. Sorry, I was, I think I've been a bit over time but anyway
thank you for attending.
[Alexander Rakhlin]: Thank you very much. Any questions?
[Question 2]: I had a question to get back to
just the classical scaling algorithm. I
guess, in the setting where all the
distances are known, how do we know that the graph can be reconstructed? I'm assuming there's-
the distances are known?  How do we know when the graph can be reconstructed? I'm assuming there's-
[Ery Arias-Castro] Yes, yes.  So, if you are only given pairwise - sometimes they're even called dissimilarities
because it can be more or less anything -
let me go back to it - so you
can obviously implement this on a
computer, right, but if it happens that
some eigenvalue is negative then
the point set cannot be realized exactly
in a Euclidian space.
[Question 2]: I see, so if you have
like a slight perturbation of your
points that makes your eigenvalue
negative.
[Ery Arias-Castro]: Yes, so two things about this.
One is that you can use that to actually
deal with the case where you're not
provided with an embedding dimension.
But often - I mean this at least was developed
for visualization purposes, and often d
is equal to two in practice, two plus three.
So when that's the case, you just
cut at two and then we have this
portion, even if you have points that
are exactly embeddable in dimension ten
that you know if you cut a two is the same
as with PCA, in fact the two
methods are somewhat related, but one
works with dissimilarity and the other
one works with a set of points, and then
otherwise, if you have a little bit of noise
in this the eigenvectors may become negative.
[Philippe Rigollet] So maybe - it's Philippe again - one quick question, does your
last model with the link function,
do you know if it had
some connection with these
generalizations of Bradley-Terry loose
models that people have been studying
or learning from pairwise comparisons?
[Ery Arias-Castro]: Yeah, from rankings?
[Philippe Rigollet]:Yes, that's right so the name that comes to mind is, so, Shivani Agarwal has some work on
multivariate versions of Bradley-Terry
loose for example, a bunch of
generalizations. So I wondered if this was
connected. I know it's directed, so
maybe there's something different going
on there.
[Ery Arias-Castro]: Yes, ok, so I'm not aware of
higher-dimensional generalizations, but typically the
embedding there, in my understanding, at least in the more classical setting, would be
embedding dimension one, right? Because you presume a complete ranking
right and then so in that case...
... yes, I think that's a nice connection to
make. Yes. I think it depends what
model we consider ranking. The more parametric ones are exactly
of the four measures showed. They
have a link - I mean, you know this
better than I do, but they are the - you have a link function based on the
positions, and the positions there are
essentially real numbers that dictate
essentially the rank of an individual if we're looking
at individuals performing tasks, that
sort of thing, right, or competing
against each other. Yes, so there's a link but - my understanding is
that most of that work is in
embedding dimension one.
Otherwise, there's a relationship also with embedding, so all you know embedding
is also classical, at least dating back to the 60's by  - Chris Cole was
one of the early researchers there.
People were interested very early on in -
it's similar with rankings and that
they were interested in embedding points
based on comparing their distances, and this is obviously useful when
there's no - when the notion of scale is
subjective, right? So you ask some person
to compare musical artists. So you say 'is
artist a closer to artist b than
closer to artist c,' right, and then you
try to group all this information to
actually embed artists, as points in
dimension to again for visualization
purposes, and that also has a long
history. I have done some work on that
and it is related to this because it can - 
okay, it takes me to a bit - a bit far, but
let's see if I can make sense of that.
So the more complicated situation here
in the very last setting , the
crucial assumption we make in
our work is that phi is
compactly supported. If phi is not
compactly supported, then it becomes more
tricky because you have very long edges
now and then, right, you can have some kind of
background noise. So imagine that phi is
non-decreasing, right, so let's say that
at infinity converges to zero point one.
So it means that no matter the distance
between the points you have a priority
of at least 10 percent that they are
connected. So then therefore you
cannot use graph distances at that point.
What people have done before, and
that's the work on link prediction,
somehow, people had thought of that in link prediction, is that they sort of - you know
the graph in a sense by looking
at how many neighbors two points have in
common and that provides some sort
of measure of distance between points,
right, but then in particular when phi is
unknown, you have to - all you can
hope to get there is ordinal information.
Okay, so that's why - that's where
there's a link between this project with
this problem and all in all
embedding. So there is a relationship, but
we didn't push it in the paper at all.
[Philippe Rigollet]: Okay, thanks.
[Alexander Rakhlin]: Any other questions?
[Question 3]: So I had one more question, actually. So, in the case where some of
the distances are not given to you, I was
wondering if you did something very
naive like just randomly sampled from a
Gaussian and replaced the distances that
you don't know and then applied
classical scaling, or essentially chose
some way of replacing those missing
distances such that the eigenvalues of
this matrix are positive. Do
approaches like that work?
[Ery Arias-Castro] I'm not too aware of it,
but the second matrix seems - 
I don't know if it's computationally
feasible or not. That's an interesting
one. So, it has been worked on.
You can see this as a low-rank,
so the distance matrix is a
low-rank and assuming that it's 
the problem is exactly solvable in low
dimension, then when some entries are
missing, you can approach it as
a matrix inference when some entries are
missing, right, and people have done that
they've done, like, they input some values. Like I think zero is like a
favorite. Then put a value for the
missing entries and then literally just
do SVD or something like that, which is
essentially what you're recommending,
right? But the - so I think you
can get somewhere with that, but the - I
think it's gonna fail in situations
where you only have the closest
distance available.
[Question 3]: I see
[Ery Arias-Castro]: Well, if the
distances are missing at random, I think
that's essentially relates to the work
that people have done in lower-rank
matrix recovery when entries are missing.
Yeah, but there's a relationship there.
[Alexander Rakhlin]: Okay, any other questions? Perhaps we should wrap up
and thank Ery.  I don't know how to do it - we can unmute oursleves and clap.
*Applause*
[Alexander Rakhlin]: Thank you, Ery.