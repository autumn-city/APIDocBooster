so let's get started you know this
lesson is all about understanding our
data
um
so we are going to be working with this
mnist data set right which is our data
set of handwritten digits and we want
our neural network to be able to
identify these digits right
so the easiest way to download this data
set is using a library called tensorflow
which is kind of like an alternative to
pi torch
we're just going to be using it to
download the data
okay
and the way i do that is it's already
installed in google colab so i can just
press shift return
and you can see this is the main thing
that's keras.datasets.mnist.lowdata
and this downloads data from the
internet for you and it downloads both
what's called like training data and
test data we'll talk about this test
data um in the next lesson for right now
we can just focus in on the training
data
so what is that training great data
if you look at it if you look at i guess
the shape if i can tidy
it is
60 000 different observations
each that is a 28 by 28 grid
let's select out
the first observation and take a look at
it so we'll call this our first
image
and i can plot it
using matplotlib so this is you know
just a kind of a popular
plotting library
and we can do plt.mshow
i think and let's actually
directly put this in here so i'm
selecting the first image
right from my data or the first
observation let's plot it
there it is the number five you go to
the second image
there it is number zero and so on right
and this works um because remember x
train
this is like a list of images each of 28
by 28 so if you select out let's not use
mshow but let's just see
what this
thing is if we select an image
remember i told you that it we can kind
of represent an image by you know this
is a grayscale image simply by you know
the values so it looks like i would you
know
all the dark values these are zeros
and then when it lights up
and now we now we're getting into maybe
some lighter values
all right
so that's how we represent an image it's
just you know
this 28 by 28 matrix
of values
now
to pass this data
into our neural network we need to do a
couple of things so the first thing we
need to do is make sure our data is a
tensor
oh before i do that so this was
this was our our training data all right
x train
all right if we look at what's y train
remember if this is this is like the
list of features if you want to go back
to uh like the breast cancer data set
right this is like the mean concavity
and all that stuff what's this y data
let's look at the first five
uh y train sorry
well these are the answers these are the
labels so with their breast cancer data
set it was like
is it cancerous or not
and it gave us a one or a zero here
it's like oh the first number is five
these are all the features
and here's the correct answer
for the second value
this zero
and here's the correct answer so that's
what the y train is
is what we you know the target or the
labels
uh of the data set okay
so
we now we need to like coerce or
manipulate our data before we can feed
it into a neural network
um
so the issue is that if you look at this
data we call it type on it
we say like what type
is this data it is a numpy array and you
may remember that we cannot feed a numpy
array
into
a neural network you have to feed a
tensor so let's do import torch because
i'll use that before in this collab and
do torch.tensor
do that and let's call this my x
tensor
and now if i call
type of x tensor we see it is in fact a
tensor
great
um i say make sure it's the correct type
because if we look at this x
tensor we're still not done
if i look at the d type this is
an integer
and we want this to be of type float
so let's call this x tensor
float i guess
i think that's fine and remember how do
we convert it to a float just by doing
that
and now let's uh
take a look at the d type again
and we see that we are now working with
a float so that's great
the next things we need to do is worry
about i suppose the shape of the data
you know i know i'm going to confuse
myself i'm going to change this
to be x tensor and now if i look at x
tensor
right where that okay
so x tensor is
is now of type float okay let's keep
going
if you look at the shape of the data you
see you have 60 000 records
um each in this 28 by 28 grid
um but that's not the format we want
because remember what we'll do with the
neural network is we want to pass
through an entire vector and we want to
pass through a vector that's of length
28 times by 28 or 784 so we want you
know one
long list
of data and currently
we have
you know
that each each thing is a grid okay we
want to convert it into one long list
so
how do we do that oh and just to hammer
that point home right uh the matrix
algebra kind of tells us that right we
want to get
uh
um we want to
uh take this these inner dimensions
align
and then we'll get back uh
in this first layer 64 outputs uh but a
single row right a single row of 64
outputs
to then
pass to the next layer so per
observation we're going to get like a
single row
okay so we want to convert this into you
know
from this 20 by 28 grid into a vector so
that we can you know apply this vector
matrix multiplication that we know and
love
so how do we convert this and the idea
is
essentially to take this grid and like
unspoil it so it's like one you know
kind of connect this
second row here with this first row so
we just get one long enormous row
okay
so
to do something like that
one way if we start
here uh
i'll show you
we'll start we'll call this our first
image
do first image.shape
is 28x28
and one way we can change it
is we want this to be whatever 28 times
28 is i always forget sorry about that
it's
784
okay and we can do that
by
ah by calling the view function
and passing through 784 so watch this so
before it was 28 by 28 now we're saying
let's make this just one dimension
and if you call this view function
you see this
we've used the view function let's call
this reshaped
image
okay
we've used the view function
to change our 28 by 28
down to a single vector
if you look
this is just one long vector we can make
this as you know
even longer still just one long vector
so we've like kind of unspooled it right
into that one long vector which now we
can apply
uh that matrix
vector multiplication to that we know
and love
all right
we we should go a little bit more into
this view function
uh let's just explain this
see a little bit more about this view
function okay so the first point i want
to make about this view function let's
do it down here
is that pi torch does the math so what
does that mean
if you look at the shape this is kind of
the original
uh
you know uh image
before we turn it into a vector so it's
this 28 by 28
and
if you try to then convert it to say
a single vector of like length 783
pi torch will say hey
that
is missing something right we cannot
take two dimensions squares
square of that length you know 784
pixels and convert it down to 783 pixels
that doesn't make sense
783 is invalid for input size of 784
okay
so
this is kind of nice that it checks for
us what's even nicer is that if you say
want to
want to go from this
shape
of 20 by 28
and then just say hey let's go go down
from two dimensions
and we kind of have this grid
down to one dimension
one way to do that is to specify
one dimension of one vector of 784 or
you can just put a negative one here and
say hey why don't you figure out you
know how to take this two dimensional
data and convert it down to one
dimension it's not so hard right we
can't we did it we figured out it was
784 and it turns out pi torch
can do the same thing for us
[Music]
so by passing through this negative one
here
okay we'll call this my reshaped image
right by passing through the negative
one we're saying hey i want this to now
be just one dimension and you figure out
what it means to convert
this two-dimensional data down to one
dimension just just do the math
okay
what's kind of cool is that you can
actually take
all of the images so remember
uh we had this x tensor float
where i think it's just x tensor
that d type let's make sure that's a
type float okay
and if we look at the shape that's sixty
thousand by twenty eight by twenty eight
what we want it to be is sixty thousand
vectors
right sixty thousand
two you know sixty thousand observations
each is a single dimensional vector of
length 784
so what we can do is call view
6000 123
comma
negative 1.
so we're saying reduce this down to two
dimensions and you do the math
to figure this out and i'm going to call
this x tensor reshaped
and i do that
and now let's confirm let's see what
this is
so now if i do dot shape of this reshape
thing you see that it converted into 60
000 observations each one now is a
single vector we can further confirm
that by maybe looking at the first one
so we're getting our first image and now
this is just a vector
a one-dimensional vector of length 784.
see that
it's all just one vector so that's
pretty good i think that's
essentially what we need
is this one dimensional vector right
because now we have 60 000 observations
each one's a vector and we should be
able to pass
this list of vectors into our neural
network
all right so here's our neural network
right we're going to take
this thing
this is our single observation i guess
a vector of length 784 and pass that to
our linear layer which has a weight
vector
has a
64 neurons each of which has a weight
vector of length 784
and then they're all going to these 64
neurons are going to each apply the dot
product so we'll get 64 outputs
in that bias term but still 64 outputs
then those 64 outputs are passed to the
sigmoid function here we get 64 outputs
between zero and one
and then we pass those 64 outputs into
the next layer
then this is going to
have give us a weight vector you know 10
neurons each with a weight vector of
length 64.
we apply the dot product effectively 10
times
and that gives us 10 different outputs
for this soft map right then we apply
the soft max
to
give us that hey you know that
probability essentially right that
probability of
uh
an exaggerated prediction of which of
these digits from 0 to 9 do we think the
output is
okay so that's it right we've kind of
reshaped our data
and what's next is pass it through our
neural network so we have to find our
neural network
we have our reshape data let's just make
sure
this is in fact
reshaped
first image dot shape
and i guess i did not
x tensor reshaped is what i called that
there you go single vector let's pass
the simple vector into our neural
network let's see the output there it is
okay
so we have just
you know coerced our data
so that we could
uh pass it through our neural network
and we just did the first image but if
you wanted to like you can pass through
you know 50 baby at a time i just hope
this doesn't break and we get 50
predictions
okay so for each one of our images we're
now getting of these first 50 images
we're now getting a separate set of
predictions
now we have not yet trained our neural
network
so that will do in the next lesson
but what we have seen is you know the
steps to start with our data
download it from keras
we took a look at it
we then converted it into a tensor
we then converted that tensor to make
sure that it is a float and then we said
the dimensions are off
right because they're each of this grid
of 28 by 28 and instead what we want to
do is do this view
called this view function to reshape the
data so that's just each observation is
now
one dimension right one vector of length
784
and that way we can pass that vector and
do that vector matrix multiplication
right to get back
an output right past that data through
that first linear layer and we said the
way that we can do that is that we can
take our data
right if this is
60 000 by 28 by 28 we can do dot view
and say we still want 60 000
observations but now you pi towards
figure out how to scope it down and
that'll be my x
reshaped
and then if we look at the shape here
it's looking pretty good and that's
something that we can ultimately
pass through
the neural
network okay
cool
so the next thing is we will move on
towards
uh
you know going through this whole
process
of
you know taking our data and building a
neural network that trains on this data
and makes predictions uh in torch so
that is what's coming up next
see you guys in a little bit