hey guys and uh welcome to another
episode in my machine learning series
and
in this series um episode
i'm going to be demonstrating lstm
in my previous um episode i had talked
about the time series generators
and now we're going to use the time
series generators to produce data for
their stm
so before we start i wanted to put a
reference here from where i have
gained some understanding of how lstm
parameters work
so that link is up here and one of the
diagrams i like to refer often
is this diagram which kind of makes you
understand that what the hidden
structure of the stm would look like
it's a bit confusing in the beginning
but in lstm
cell has multiple gates for get gates
input gates and output gates and then
each gate kind of behaves like an
artificial neural network
and that you may already be familiar
with so this understanding that you have
the usual
weights and biases and that's what makes
up each of these gates
once you understand that then it kind of
breaks down the whole lstm
as just a bunch of a combination of a
and ns
and then you can understand that what
the hidden states are doing so
i would highly recommend you go here
before you
go look at my video please do visit this
blog to gain a clear understanding of
how the lstms really
implemented kind of like an a n inside
with that said um i want to also
talk briefly about the fact that the
lstm is a long short term memory
it's an evolution of the recurrent
neural network rnn
and the thing that's um interesting or
new in lstms is that it has this
um it has kind of solved the problem of
vanishing gradient
in which you can't remember too far back
so it has a little bit of a longer term
memory built into it as well as the
short term memory
now my purpose in this episode is to
show you that you can use this
method to kind of predict a time series
there's many other uses of this but i'm
going to be focused on time series and
how
it can help you predict time series and
time series prediction based on the
you know past data or history and
by using that this kind of structure you
can
um you can predict a time series and
also measure the error
and for me it's mostly the error
measurements that are important because
i can predict
when there are deviations from existing
behaviors
and that's one of the use cases i was
working with
so um the important thing here is that
we include a whole bunch of these
libraries for plotting and
and tensorflow obviously and numpy are
the big ones
pandas is another one that we use
all across um and then
once you're past the including all the
the stuff
then we basically pull in our csv files
let me just run through this real quick
with you so we pull in our csv file and
we're going to plot the csv file just to
see how the
input file looked and
so there you go that's what the input
data was
now we're going to try to split that
data into train data and test it and the
test percent is 0.4 so if i run this
you will see that this is the training
data in orange and the blue is the
test data we're going to run our test on
that okay
so with that now we are using our
familiar
uh generator and i think i'm using
tensorflow 2.5
so um we're going to use the tensor
sorry the time series generator which
i've described before now
i'm using the scale train and scale
train both and if you see my previous
video because i'm just using n
samples of the input data and predict
one sample of the output
i can do this i just use the scale train
skill train
my length is 30 so i'm going to input 30
um points and predict one point after
that
my batch size is one so i'm just putting
30 and
one output and that's that's what i'll
train on one at a time
and then the whole um
set would be used in such a way so the
generator will create
um this data and it creates 4
900 batches here
okay so over to um let me just run that
and so then we look at the model
so the model is created using
feature of one because even though i
have
30 time samples
the feature is only one because i have
this one stat right this one stat is my
only vector so
the way i create it is that i create a
sequential model
using keras and then in the sequential
model i
add the lstm layer and and i put 50
hidden
um units inside this lstm layer
written sequences true for this first
layer input shape
is length comma features my length is
time steps which is 30 time steps
and my features only one because i'm
only inputting one
now the next one i i've used cascaded
layers
and then i converge it to one so i use
another one
and this layer would basically just
output to one
and then i compile with the mean square
and atom optimizer um feedback
and that is my model now
um it took me a while to understand why
it takes so many parameters
but like i said earlier if you go into
this blog and see that
there are four of these and then each
one of these has
certain numbers the other thing is that
you can see that this is 30 times steps
but
the important thing to understand is
that this represents not 30 time steps
but just one time step
so if you follow that um you have 50
feeding back
plus one of this so using that if you do
the math here
then there's 50 coming back into this
instead of these three that just be one
so 51 here
and the internal states are 50. so if
you follow that
then 51 times 50 plus
you know there are 50 biases here so if
you
add that and then you multiply by four
sets here because there's forget gate
two input gates and an output gate
if you compute that that comes to 10 400
so that is the magic
of how the lstm is organized this is
this is in fact a perfect structure to
understand how it's internally
working so um with that
i guess let's just run that quickly and
so we can see how many parameters total
is quite a lot of parameters which is
why
the training is so slow so i'm going to
now
fit the generator two epochs and
i'm gonna run that and it's gonna take a
while it takes about five minutes
to go through each epoch so it'll take
about 10 minutes
as you can see i've started this and
at this point i think i can pause the
video
and we'll come back to it when we are
ready
so let's just pause the video here
okay so now we have finished our
fitting and so now we're gonna go ahead
and do the plot for the loss function
we only had two uh epochs
so we still see just a flat line
but the um error has decreased
considerably in the two epochs
and now we're going to go on to
predictions
and uh we first going to predict the
training part
and this might take a while as well so
let's see let's run the
predictions for the training set
and so i'm going to pause again till
this finishes
okay so on the training on the
train set um
we're predicting the training set so
that has finished
and now we're going to do the plots to
see how that looks
and so here you go because it's a
training set which is
exactly what we have trained on it looks
extremely good the predictions match
whatever we were seeing so now we're
gonna
plug in our test data and that's the
scaled test data
and we're gonna reshape it and plug it
into the model
and remember the thing that we're doing
is that we take 30 samples of the
the test data and try to predict the
next sample
and that's how we will proceed through
that
and i have yet another method here which
i have commented out
and in this case instead of taking the
input data
what we can use it is start with the
input data
but then start to feed in the product
predicted data
and so you can predict much further out
into the future
we're not going to do that in this
episode what we are going to do is just
always follow the input use 30 samples
and predict 1
and then take the next 30 of the input
predict the next one and so on
and then we can plot what we see um
on these so let's go ahead and run this
and i may pause again
because this may take a while so i'm
going to launch
this prediction for the test data now
okay so we have now run through the
prediction part of the
test sequence now i just want to spend a
moment here on how it's done
so obviously we have used a
scaled version of the test data from the
beginning
and you have to go through this
reshaping process
so the the input has to be in this
format the number of features in our
case is one
length is the number of time steps which
is 30 in our case
and so the format of this stuff that
goes into the model
is is one by a number of time steps by
number of features
so um that's
the form that's the reformatting of the
input data that we did here
um and then we feed it into this um
the model and the prediction comes out
we pick out the the output and we stuff
it into this
array uh or list and we keep doing that
over and over um and
that's how we will get this uh
predictions and now that we have these
predictions the next thing is to plot
these predictions so let's run that
and so there we have it
the um you can see that
we've gone ahead and predicted
and we've plotted the error and you can
see that the predictions
um look um pretty well matched here
and um only towards the end we are
diverging a little bit but otherwise the
predictions
seem to be right on so
um as you can see that
using the lstm models with just two
epochs of training
we have been able to um
quite nicely predict this time sequence
and if your objective is to do some
predictions
you can do those predictions if your
objective is to
monitor the error from your predictions
to see where
the past is not a good prediction of the
future you can do that as well
and measure the errors in your time
sequence which is
what i like to do and so i can see that
the error is high here
now i've shown a threshold here so if
you
um since the error is fairly small
what you can do is you can play with
this threshold
and i'm going to lower this threshold to
let's say 50
and run this again
and now you can see that
some of the data lies above that
threshold so you can see that
most of the data here is below this
threshold
and only the one in red here so it just
briefly goes into an excursion above
50 so the errors error is fairly small
so if i set my threshold to 50 only
these would be flagged here
deviations here but everything else is
well within tolerance limit of 50.
so that's really it for on this
episode and i really wanted to go back
and
recommend that you read this um this
article
especially the understanding of lstm
um long short term memory which helps
solve the time sequences the most
important thing
is you know how many how the parameters
are used and how
it kind of looks like an a n a bunch of
a n's
and then some gates four in four gates
that are basically forming the structure
of an lstm cell
when you put the hidden
units this is where the hidden units lie
so you can see that this has two hidden
units
so those two hidden units lie in every
single
of these gates the forget gate to input
gates and output gate
and so that's it that's that's um all it
takes so
um hopefully you guys liked this episode
and you can run
your own sequences i'll put this code
on bitbucket and you can run this code
you can apply your own time series and
see how your
series performs and how well this
is able to predict and obviously the
next thing would be if your
sequence does not get predicted well
try to play with this model here
and make sure you're applying the model
uh
with some tuning to get maybe more
elements or
the number of features you're using and
the number of time steps you're using
uh you can try to vary all those
parameters
and see if you are able to make good
predictions
of your sequence so that's it for this
episode guys
and uh if you like this machine learning
series and what you're seeing so far
then do hit like or leave me some
comments
and subscribe to my channel and
then i can bring you more in this
machine learning series thanks a lot
again and see you in
another episode until then bye