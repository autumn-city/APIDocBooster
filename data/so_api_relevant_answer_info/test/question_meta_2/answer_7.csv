Block Label,Cluster Label,sentences to be labeled,question title,question link,question description
,,Custom LSTMCells don't support GPU acceleration capabilities - this statement probably means GPU acceleration capabilities become limited if you use LSTMCells.,Change Tanh activation in LSTM to ReLU,www.stackoverflow.com/questions/49040180,"The default non-linear activation function in LSTM class is tanh. I wish to use ReLU for my project. Browsing through the documentation and other resources, I'm unable to find a way to do this in a simple manner. The only way I could find was to define my own custom LSTMCell, but [here (hyper-link)] the author says that custom LSTMCells don't support GPU acceleration capabilities(or has that changed since the article was published?). I need to use CUDA to speed up my training. Any help would be appreciated.
"
,,"And definitely, you can write your own implementation of LSTM but you need to sacrifice runtime.",Change Tanh activation in LSTM to ReLU,www.stackoverflow.com/questions/49040180,"The default non-linear activation function in LSTM class is tanh. I wish to use ReLU for my project. Browsing through the documentation and other resources, I'm unable to find a way to do this in a simple manner. The only way I could find was to define my own custom LSTMCell, but [here (hyper-link)] the author says that custom LSTMCells don't support GPU acceleration capabilities(or has that changed since the article was published?). I need to use CUDA to speed up my training. Any help would be appreciated.
"
Null,Null,Paragraph end
,,"For example, once I implemented an LSTM (based on linear layers) as follows which used to take 2~3 times more time than [LSTM (hyper-link)] (provided in PyTorch) when used as a part of a deep neural model.",Change Tanh activation in LSTM to ReLU,www.stackoverflow.com/questions/49040180,"The default non-linear activation function in LSTM class is tanh. I wish to use ReLU for my project. Browsing through the documentation and other resources, I'm unable to find a way to do this in a simple manner. The only way I could find was to define my own custom LSTMCell, but [here (hyper-link)] the author says that custom LSTMCells don't support GPU acceleration capabilities(or has that changed since the article was published?). I need to use CUDA to speed up my training. Any help would be appreciated.
"
Null,Null,[code snippet]
,,I would be happy to know if the runtime of custom implementation of LSTM can be improved!,Change Tanh activation in LSTM to ReLU,www.stackoverflow.com/questions/49040180,"The default non-linear activation function in LSTM class is tanh. I wish to use ReLU for my project. Browsing through the documentation and other resources, I'm unable to find a way to do this in a simple manner. The only way I could find was to define my own custom LSTMCell, but [here (hyper-link)] the author says that custom LSTMCells don't support GPU acceleration capabilities(or has that changed since the article was published?). I need to use CUDA to speed up my training. Any help would be appreciated.
"
Null,Null,Paragraph end
