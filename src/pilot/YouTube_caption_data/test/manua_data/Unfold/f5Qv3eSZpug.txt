Sanyam Bhutani: Hey, this is
Sanyam Bhutani and you're
listening to "Chai Time Data
Science" podcast for data
science enthusiasts, where I
interview practitioners and
researchers and Kagglers about
their journey, experience, and
talk all things about data
science.
Hello, and welcome to another
episode of the "Chai Time Data
Science" show. In this episode,
I interviewed three great
amazing contributors to Pytorch
and also the authors of an
upcoming book title. Deep
learning with Pytorch by Manning
publications, Eli Stevens, Luca
Antiga and Thomas. In this
interview, we'll talk about the
journey into machine learning
and the journey with Pytorch
towards the efforts into writing
the book, for the audience that
might be curious, what does
writing a book, as, as an
experience look like? We also
discuss about that. The authors
also share what can one expect
from the book? What are you
expected to have prepared before
learning from the book and what
can you take away from it. There
is also a lot of great advice
around project building, which
is, I believe, one of the
essence to getting your break
into the field. So please stay
tuned for that. Thanks to
Manning publications. I'm also
doing a giveaway of the book.
I've already given away one copy
for the AMA, four more copies
will be given away along with
this episode. To participate.
The details are there in the
description, but just share your
favourite code from any
interview on the series this one
or otherwise, and I'll get in
touch with the four selected
winners,Manning publications has
also been very trying to offer a
40% discount on all of their
products. Please use the coupon
code PodChai20 podchaI20 to use
this discount coupon, thanks to
Manning for doing the giveaway
and for the discounts. For now,
here's the interview. Please
enjoy the show.
Everyone, I'm really excited to
be talking to three amazing
people, Thomas, Luca and Elo on
the show. If you could please
introduce yourself, by your ways
because some of the audience
will be tuning in from the
audio.
Eli Stevens: Sure, I'll go ahead
and start uh, my name is Eli
Stevens. I'm currently in
Machine Learning infrastructure
engineer at zooks. And one of
the three authors on deep
learning with Pytorch.
Luca Antiga: And I'm Luca
Antiga. I am co founder of a
couple of companies. One is
Orobix it's based in Italy. T
e other one is called Tenser
erk, and it spun off from ae
obics last year. Orobix is doin
 AI engineering and serv
ce. And Tenswerk is doing too
 development. So this is what I
m into the most. Like I'm a bio
ngineer by background.
Thomas Viehmann: Yeah, so I'm
Tom, I, mathematician by
background and kind of started
PyTorch and machine learning 
onsulting company in 2018. And 
eah, when I set up with and I 
hink it Piotr, if you're active,
you probably know from Pytorch.
Yeah, he and I joked about an
imaginary book and then he like
connected me to a real book. So
that's why I'm here.
Sanyam Bhutani: Awesome. So I
want to having us really
excited. We're talking to all of
you. So before I talk about what
you're working in your day to
day I want to start about how
did you get interested in
machine learning deep learning,
broadly speaking? Eli maybe if
you want to share your story,
where did you finally find your
passion and you're passionate
enough to switch into career in
machine learning?
Eli Stevens: Yeah, so I'd always
been interested in artificial
intelligence ever since college,
I wanted to do AI for video
games. But I graduated just
after the first internet bubble
had burst. And so it was like
get a job, any job. And so all
of those, all of those things
kind of fell by the wayside. And
it wasn't until 2017, early
2017, there was a, a kaggle
competition, the data science
bowls having their lung cancer
detection challenge. And that
was finally the thing that says
like, Okay, I need to get back
into that million dollar prize
pool. I knew I wasn't gonna win
anything, but I was like this,
this is finally the time, okay.
And I don't know if this is a
coincidence or not, but Pytorch
0.1 had released at basically
the exact same time. And so the
two just dovetails really
nicely. I dove into pi torch.
And I was like, Okay, I need to
go ahead and set up a career
pivot into this. Which is part
of the reason why I accepted
when Manning approached me and
said, Hey, do you want wanna
write a book?
Sanyam Bhutani: That would have
been one of the first Pytorch
approaches on Kaggle, I believe,
if you;
Eli Stevens: possibly, um, like
I said, I did not do
particularly well, because I was
learning Pytorch and learning
deep learning all at the same
time. And that's not a recipe
for hitting the top of the
Kaggle leaderboards. But it's a
great learning experience. And I
think that the couple of bug
fixes and documentation updates
that I submitted back to the
Pytorch project super early is
what got me on Manning
publications radar. So,
Sanyam Bhutani: Okay, and Luca,
I believe you've been enrolled
in the medical space for over 20
years. When did you find your
interest for machine learning?
Luca Antiga: Yeah, so pretty
late. I would say I'm a junior
and I've been doing research in
the 2000 mostly on
cardiovascular mechanics and
medical image processing. And
back then we were doing a lot of
you know, that your background,
you knew a bunch of algorithms.
And you had to come up with a
great cocktail of stuff that
would make things happen. And
the way we got into machine
learning was with aerobics. So
we got to an important gig with
a pharma company. And we got
this 4000 scans, it was back in
2014, I think. And after eight
months, we couldn't, you know,
we couldn't see we we had solved
the problem. And, and we said,
okay, let's try, you know, we,
there were a few papers, not not
very many papers on deep
learning in on medical imaging,
and we say, well, yeah, let's
try with that. And we went to
the torch seven. And after a few
months, we we had we had a
solution to the problem. So we
kind of understood at that point
that the world was going to
change. And so we said, okay,
let's verticalized the company
on these technologies and open
up from medical to manufacturing
to gaming we last year, it was a
game that we contributed to Moto
GP 19. Reinforcement learning.
And so that's how I got into
that. And that's how I got into
torch because it had good 3d
convolutions for 3d like
volumes. And it was simple. So
you could read the high levels
down to the colonels, and I like
that fact. And, and then that's
what got me into pi torch and
then I got in touch with the
community and start
contributing. So it's been a
really great experience.
Sanyam Bhutani: Awesome. Are you
talking about torch or Pytorch
because some of the millennial
audience might not know the
difference.
Luca Antiga: but like back then,
you know in 2015, or so, if you
want to torch you cut torch
seven, which was based on Lua
and had C, back end with C and
CUDA. So the really nice thing
about torch about torch seven is
that you could really understand
everything you could start from
the top and go down to the
bottom and it was very readable.
It was, let's say particular in
the way it was built very, very,
very smart. And in fact, the
Pytorch 1.0 0.1 sorry, was a
Python layer on top of the same
back end. So that's why I got
excited. And then it was amazing
to me to see that pi torch had
basically the same API very
early on with a very few
breaking changes, very, very few
breaking changes, but everything
underneath, transformed over the
course of two or three years. It
was great to, you know, be
involved at that early stage.
Sanyam Bhutani: Awesome. Both of
you are earlier doctors of torch
and Pytorch. Thomas is one of
the core developers behind
Pytorch. Thomas, when did you
find your interest for machine
learning?
Thomas Viehmann: So, in a way,
I've been a data guy very early
on. I remember when I was like
six or seven, I wrote a little
programme to pluck bar charts
of, I think it was some juvenile
delinquency statistics that my
father was working on, or
working with. And so that was
the very first computer
programme I saved to a floppy
disk. I remember because I wrote
the programme, we went and
bought the floppy disk and then
I'd say So I have this
destination for detail, and
probably the science I even took
when I went to university, I
took a class called neural
networks and pattern
recognition. There was the 2000
style variant of that. But then
I kind of went into pencil and
paper mathematics mainly for
social reasons. Because kind of
I met a great crowd of
mathematicians. And then I went
to become an actuary, so I help
insurance companies with
statistics and mathematical
modelling. And then at some
point, and I've always been kind
of a computer person. I used to
be a Deb Debian developer back
then. And so at some point, I
went back to doing machine
learning things. At that time it
was tiano. And in TensorFlow
early TensorFlow on and then
when Pytorch, the first public
release of Pytorch came out, it
immediately destinated me there
was so easy to use and, and very
pythonic in a way, I've been
programming Python for, I don't
know, 25 years or so. So that
really made me feel at home. And
then I started contributing a
few patches. And then there was
a GTC in Munich, and Sumit and
Adam and a bunch of the other
Facebook people came to Munich
and I wrote to Sumit hey, want
to go for a beer. And so I
always say I came for the for
pytorch the library but I stayed
for the company. I found with
the other developers and really
I've been very good friends with
theatre and met him in real life
for a couple of times because it
was also based in Munich
originally. Also based in
Germany, it's not that far from
Munich. Yeah. And so this is
what got me into pi torch. And
so at some point chapter told
me, hey, you're showing up on
the person self interest page of
like, that's kind of the story
behind that story.
Eli Stevens: Yeah, there's a
connective story to this I'd
never realised when you met
Sumit and Adam, the day before
or two days before? GTC that
your I believe they either
hijacked their, their trip to
hold the I think the first
workshop public worship on
pytorch in Milan. So I, we were
connected some ball and they say
yeah, sure we're coming to GTC.
So I also met Adam for the first
time there. So in a short time
span then they make you. It's
interesting.
Sanyam Bhutani: Awesome.
Probably Did you find your math
background to be your secret
weapon many people assume you
need to do so with math when
you're jumping into machine
learning. Did you find that
super?
Unknown: Well, so it certainly
shapes how I think about these
things. And also kind of
sometimes makes me demand very
hard things from my co authors
in terms of giving numbers and
graphs and also how to present
things graphically. I find
myself having lots of opinions
there. I will would certainly
say that a mathematical
intuition of some sort helps.
You certainly don't need a PhD
in maths to be successful in
machine learning. But, so I
guess not being scared of maths
certainly helps and shapes my
view of things. Of course, I
have to learn a lot of the
engineering things as I go along
that come more natural. So you
guys are;
Sanyam Bhutani: Awesome. Now I
want to fast forward to today,
if you could share what you're
working on today, or the task
you're tackling, what have you
and are you using Pytorch in
production today?
Eli Stevens: I'll go ahead and
start on this one because I
think I've got the least
interesting answer. Like I said,
I'm working at zooks for which
is in the autonomous mobility
space, self driving cars. I'm
doing machine learning
infrastructure work there. But
I've been told by the legal
department, I can't talk about
what I do much beyond that. So
they're they're very concerned
with IP protection. And so I'm
trying to respect their wishes
there.
Sanyam Bhutani: But it was one
of the first I believe to get
permission in California to
support transport of I think
people love to public, but I'll
I'll have the website link in
the description for what we can
view publicly.
Unknown: Sounds good.
Sanyam Bhutani: Luca, do you
want to say your current day to
day work?
Luca Antiga: Yeah, so aerobics
arrows were about 30 people now.
And we do projects on different
in different fields, in
everything we do is basically
Pytorch. So Pytorch is very
pervasive to what we do. It's
actually the only deep learning
framework views and I would say
we in the data science floor. We
do primarily deep learning. So,
yeah, we're, we follow along
very much. And then with tensor
work on the other end, we do
infrastructure tools. And so
again, Pytorch comes up, you
know, very prominently there.
So, yeah, I'm happy to to,
unfortunately, to be able to
include that in my day to day
work.
Sanyam Bhutani: Awesome.
Thomas Viehmann: Yeah, so I'm
doing lots of fight for things
and also doing a few things that
are kind of below the Pytorch
layer if you want so we can
relatively close to the
hardware, developing kernels, or
things that can produce new
turnouts by certainly do a lot
of work with it. Towards two for
various clients. So I fought to
work with aerobics and have the
honour and pleasure to meet many
of Luca's colleagues there and
they're a great team to work
with. And and then I used to do
and used to like to do in person
workshops to using pi torch. So
for example, Pytorch on the
raspberry and things like that,
but unfortunately in person
training isn't that good a deal
right now. But yeah.
Sanyam Bhutani: I think many
people miss out on the fact that
code developers go through the
pain of writing the kernel so
that we don't need to know
what's happening under the hood.
And that's thanks to the amazing
books, the resources that we
have out there, coming to the
booth, but all three of you are
working on deep learning with
Pytorch by Manning Publications,
what made you decide to start
writing it? I think the
community is so simple, great in
pytorch already, why did you
decide to start writing it?
Eli Stevens: Because we can just
keep the same order on this. So
I think I was approached by
Manning, asking if I'd be
interested in writing the book,
and at the time, I was a little
bit hesitant. I was working at a
startup that we were in the
process of trying to sell. I had
two young kids at home, I was
like, I do not have time for
this. And I said, yeah, yeah, we
hear you. We hear you. How about
a co author, do you think that
would help? And so they
introduced me to Luca, and we
talked and the way I like to put
it is Luca and I put our heads
together and disgusted a little
bit decided that that you know,
two times zero was enough. Time
to write a book. So for me, the
big motivation was wanting to
facilitate a career pivot. My
career prior had had a lot of
Python experience, but nothing
on the deep learning side. And
like I said, I had wanted to, to
be into that field for a long
time. And I was like, okay, this
is my chance to go ahead and do
something that will get me into
those job interviews that, you
know, I probably would just be
passed over because like, why is
this guy applying for this job?
He doesn't have any experience.
So that was a big part of my
motivation is to go ahead and
use it as a way of communicating
my seriousness about wanting
that wanting that career pivot.
Yeah, I also told Manning I
wouldn't I didn't have time and
yeah, that was there was there
must be some game theory behind
this. At the time, I was, like,
I've been contributing to
Pytorch for a couple of years or
one year and a half. And then
life happened and in Pytorch two
really took off. So I decided to
slow that down. And, but at the
time, I would say I was very
involved. And so I really wanted
to understand how things worked.
And I think there's a it's a
good chance, you know, to have
having to explain things to
others. It's a it's a good
opportunity to to learn them
yourself. And so that's why I
kind of decided or let life take
its course on this. I didn't, I
wouldn't, I didn't realise it
would be an endeavour that
would, you know, accompany me
for quite some time and, but I
think it was a really good
decision.
Sanyam Bhutani: Thomas, do you
want to see your story?
Thomas Viehmann: Yeah. And so at
about the same time, I think
when the public excerpts of the
book were finalised, Luca and
Eli kind of wanted the secret
weapon by guests. And yeah. And
so when when Piotr and I jok
d on the on the Pytorch sel
ct channel about our ima
inary book, Eli said, well, whi
h one of you want to write or 
fforts out with the with a rea
 book instead of just an ima
inary one. And, yeah, I was the
first to show my hand and obv
ously with being part of my own
company, I don't have a leg
l department to keep me from doi
g it and so I mean, if you loo
 at the the public book, it'
 a really I really like the par
s that they already had. And so 
 decided to help with the, wit
 the final chapter. And, and bit
 of the bits of the first par
 will be covering new PI tor
h and things like that. And yea
, so I always wanted to wri
e a book and now this was the
chance that I can pass on and
at the same time, it was a goo
 opportunity to also go to con
act with the with Eli and loo
 at because it's also in Eur
pe in particular. And there was
great, a great thing and is a g
eat thing.
Eli Stevens: Yeah, and if I can
expand on that a little bit, I
think that that Thomas is kind
of underselling how much he
helped get the book over the
finish line. It did. It had
been, I think, when we contacted
him a little over two years that
Luca and I have been working on
the book, and we've made a lot
of great progress, but Thomas
just really brought like an
attention to detail and, and was
able to do, like, you know, just
a fresh set of eyes pass over
the book, while at the same time
actually helping rather than
just, well, you should make this
more clear, you should make this
more clear, he was actually able
to make it more clear, which was
really key in terms of actually,
all three of us being able to
wrap up the project. So it was
it was really good that he came
on when he did.
Sanyam Bhutani: Awesome. Thank
you for reading the book. I
would have mentioned this in the
intro that I record separately,
I'm doing a giveaway and the
book is already live, but for
the people, what can we expect
from the book who would like to
check it out? What kind of
audience are you expecting and
if you could expand on the
structure of the book When the
audience who reads a takeaway
from the book, we can follow the
same order or as easy.
Eli Stevens: Okay, yeah, I could
jump in on that, um, we, we
wanted to aim the book at a
software engineers who were
comfortable using Python. So,
you know, we expect you to know
classes and loops and all that
kind of stuff, we're not going
to cover any of that. We expect
that the readers will have an
interest in deep learning. So,
you know, hopefully, you know,
real, real simple exposure to
some of the basic concepts, we
explain all of that stuff. But
it always helps if you kind of
have an idea of what it is
you're about to learn before you
learn it. And we broke the book
into three parts. The first part
is talking about the API of
Pytorch, talking about how it's
basic data structures. are set
up how they interact, what you
can use them for, and going
through, basically building that
intuitive sense of what's going
on under the hood. When you do
deep learning, how does
backpropagation work? How do you
compute a gradient and then
building that up to the by the
end of part one, you've done
basic image recognition using
convolutional neural networks.
Then for, we kind of switch in
part two. Part two is we take a
single project and it's actually
the same Data Science Bowl lung
cancer detection project that we
mentioned earlier. We take that
project and and work through
that over the course of several
chapters. And it's a lot of
let's get the most basic thing
that could possibly work
implemented. See what it takes
in order to do that and then
investigate problems with it.
Why isn't it working? How can we
make it work better? And then
the next chapter, we'll go say,
okay, we found this problem.
last chapter, let's figure out
what we need to do to fix it.
How do we improve our metrics?
How do we improve our logging?
How do we, you know, improve the
actual results that we're
getting. And then we take that
through then to actually being
able to do consumers CTC scan
and to detect lung cancer on it.
Obviously, the results aren't
clinical. And we mentioned we
can talk about that in the book,
but you know, you're still able
to see a larger scope project
end to end. Then in part three,
we talked about the, the
production aspects of how do you
then take this and actually get
it into your users hands. Part
three is a single chapter. And
so it's a little bit shorter.
And unfortunately, it's one of
the parts that's, that's
changing the most rapidly. And
so, you know, there's been some
recent announcements of torch
serving and torch elastic that
unfortunately aren't going to
make it into the book. We had
one of the readers on the forums
asked about that today. I was
like, sorry, it's the text is
done. It's out of our hands.
Luca, Tom, do you guys want to
add anything to my ramble there?
Unknown: Yeah, the plan for the
book, if you remember, you like;
Oh, boy, oh boy.
So first message if you want to
write a book, you know, do a
very good first plan. And just
be aware that that is not gonna
happen. Yeah, I, part one is the
one I was definitely more
involved in like, and I thought,
I spent many hours in that. And
when you set like when you focus
on a, on a possible reader, and
then you say, I'll explain this
in that and then we'll work
through this. And then you start
really writing it you find
yourself in the middle of it,
and you know one of the scenes
in the hotel hall where the
corridors become longer, you
know, it just was it what is it
called Dolly zoom, something
like that. That's a feeling, you
know, when you start writing,
then the road ahead of you
becomes longer as you write, it
becomes even longer. Because
you've realised that really to,
to approach a subject like that.
And if you don't really want to,
you know, pretend you explain
something, but you actually want
to build some kind of intuition
behind it with a person that
doesn't have that background,
then you really have to take a
few steps and maybe renounce
some some some some formal
introduction and concepts and
maybe focus them a bit more and
building the intuition that will
serve you to then get those
concepts out. And so I think the
scope as being asked to, from,
from very early on, to write a
book that could take a developer
or an engineer, or an interested
person, in any case, even social
science with some programming
background. To the point they
could consume the online blogs,
the excellent documentation
online, you know, to that stage.
So there were really few
resources that could connect you
from from basics to that point.
And so we try to create a book
that addresses that.
Thomas Viehmann: Yeah, I think I
can echo what Eli and Luca said.
So the first part and that's
essentially most of it is
publicly available for free from
the Pytorch, website and the
focus there in really the the
thing we've tried very hard to
do is to kind of deliver an
index intuition and find
intuitions and like kind of,
can, can take you very far
without kind of slapping
formulas, people because I'm a
mathematician, I can be very, I
can read a lot of formulas, but
I also learned how if you go
light of formulas, you'll have a
much, much more agreeable day,
many days, right. And so, we, we
really try to to make this about
intuition. So that was the first
part. And at the same time, you
have, you have the Pytorch code,
and you have little bits of
Pytorch code throughout the
throughout the chapters to
follow along, you need, code
 right. Um, and then the secon
 part really is about the journe
 that a typical data scienc
 project would take, or a typica
 deep learning project will take
 And so and I think that's kin
 of probably looking like, we'l
 agree can have the techniques
 the Pytorch, neural networ
 architecture building blocks
 They are just a tiny part of an
 real deep learning project. An
 so, in part two, we emphaticall
 don't show. Yeah, here, you us
 this network on this data set
 and then you get whateve
 results. But we want to take yo
 on a more free where the dat
 is, like data in the real worl
 is and you have to at ever
 point, you have to ask yourself
 Well, what can I do with th
 data? Where do I have to watc
 out And what can I do? Th
 standard methods, kind of th
 tutorial methods that you migh
 apply, for example, fo
 classification when this doesn'
 work out, well, what are w
 going to do next? And so it'
 really kind of this realisti
 journey. That's our focus. An
 that's maybe also the differen
 two books that kind of show yo
 yeah, you can do this and thi
 and this with PI torch and ca
 listen to this in this network
 We really have convolutio
 neural networks and also unit
 course and then fine. And thos
 are very simple and wel
 seasoned architectures for now
 Really, to have an end to en
 project to fall lower. So mayb
 something that gives you som
 orientation when you're at th
 same some intuition when you'r
 the same place in your ow
 project. I think that's kind o
 the key. thing that we want t
 convey in that part two, an
 that's really, I think, also th
 test of the books for you is i
 you want to see something lik
 this end to end and learn b
 doing it, rather than learn b
 having it explained to you
Sanyam Bhutani: Awesome, I think
many people miss out on that
some books don't survive the
time, the test of time, so to
speak, and they specifically
when architectures that will
inevitably inevitably get
outdated in the near it's
machine learning we're talking
about, if we could talk about,
give us a behind the scenes view
of how does the writing process
to, like, many people are used
to writing blog posts, they
think, hey, maybe I can just
compile a series of blog posts,
think along those lines, and
that becomes a book is that the
case? Or if you could talk about
your process of writing the
book.
Unknown: So I've not been there
but I'd say writing a book
pretty much a top down process.
So you start out with a table of
contents and then you drill down
and now look Eli will tell me
that's not how it works.
Eli Stevens: Um so I think Luca
hinted at this early on when we
when we set out to write the
book, we actually had an idea of
taking interesting papers of the
day implementing them in Pytorch
explaining them and and, like
having a paper per chapter. And
then we realised like that, that
each chapter was going to be a
book all by itself. Or rather, I
think our publisher realised
that and told us no, no, don't
do that. But, uh, uh, we
definitely like Tom said, we had
our table of contents we
outlined what we want to do and,
you know, no, no good plan
survives contact with the enemy
and the enemy here is time. And
so the the nature of what it is
that we were trying to do, in
the grand sense was pretty
consistent. And like the the
individual chapters that we were
writing were pretty consistent,
but like that kind of middle,
like, what, what should part
three be? Is there going to be a
part four? Like, a lot of that
ended up changing as, as the
project developed. I had a very
strong feeling of wanting to
provide the book that I wish I
had had six months earlier. And
so that's why I was I was really
insistent that that we use the
the length of a book to be able
to do something more in depth.
And so I was I was really
pushing for that part to single
project. Let's like, actually do
it. And that's the kind of thing
that that I think that you might
be be able to pull off as a
series of blog posts. But one of
the things that I found is that
as I was writing, you know,
chapter 13, I was having to go
back to chapter 11. And make
sure that the chapter 11 set up
things in a way that you know,
the features we were adding in
chapter 13, slotted in cleanly.
And so like, when you look at
the, you know, the chapter 11
code, you're like, oh, yeah,
okay, this seems like reasonable
code. But there was, and I'm not
trying to, you know, toot my own
horn on this, but it felt like
there was a lot of a lot of
careful craft that had to go
into getting code that seemed
reasonable for chapter 11. And
also still seemed reasonable for
chapter 13. Because by the time
chapter 13 rolled around, it's
like, oh, my God, this is I'm
not gonna be able to fit this in
here. I have to refactor all of
this. And yet that refactor that
needs to get back ported several
chapters and still look okay
when you remove the reason for
the refactor and so at least for
for my experience, which I
realise is not not even the
whole of this book, much less
every book out there. I think
that the amount of
interconnectedness and planning
that that particular let's walk
through a large project requires
made it. It would be difficult
to do it as a series of blog
posts, I think, because you'd
have to write all the blog
posts, and then be done with
them, and then just dole them
out, you know, one a week, once
you're already finished with the
whole thing, which at that
point, just write a book and get
a publisher, you know, like.
Sanyam Bhutani: definitely.
Thomas Viehmann: Yeah, I think
we should also mention that the
team at Manning that's been
doing a really great job not
only in the planning aspects
that we already touched, but
also in telling us where we were
to turn so where we things not
ideal way and remind us that,
well, we have to tell the reader
and we have to show the code for
everything and explain, like how
everything works and not gloss
over the things. We thought were
maybe not that interesting to
us, but a reader probably would
want to have been told about.
And so at least for me,
personally, I exchanged the most
emails with Francis Bryan and
Tiffany and they really made it
a much better book than three of
us could write on our own. And
so having this professional team
in there certainly is an
important part not only for
finishing the book, actually,
but also for, for the quality of
what the reader gets.
Unknown: Yeah.
no, I was saying my experience.
As I said, I've worked mostly on
part one. And my, my experience
has been different because part
one is easier like from from the
structure standpoint, it's more
sequential. So it was less of a
challenge and it's organised in
chapters, they're almost self
contained. You just have you
just need to realise that what
you explained and not take any
anything for granted. What I
bring home from from this
experiences that I I really
cherish the moments that the
nights that I said there and
just let it out. So in total,
the writing process didn't take
that long, like it was very
concentrated in time. It was in
dining reading sprints with
individual chapters that would
go just blue and, and I didn't
really know how a chapter would
evolve. It just came that way.
And it was really nice almost
narrative to, to follow the
lead. And at the at the end of
the session, you had something
that didn't exist before. So it
made me discover this long form,
you know, after writing several
papers for, you know,
scientifically and so on. And
writing scientific papers is a
game of Tetris that it's really
hard to win, like super master
level you have, every sentence
has to be functional to
something and necessary and
sufficient to explain something,
but having the luxury of writing
a whole chapter in, you know,
and creating it in a way that
wouldn't be the same if you
wrote it the next day. It was
very fascinating. So there is
this aspect to me that that it's
important that I, I hope I'll do
it again if we get time, but,
but I hope to find myself in
that position. I didn't know the
technical books or whatever. But
it's a nice feeling.
Sanyam Bhutani: I hope so too. I
guess it's like you mentioned,
it's like the process of going
through the study and not having
to see the aim of you're going
through the process many people
miss out on the fact that blogs,
okay, casually written, I guess
I started off with a bad example
In comparison, but still, and
books take a lot of professional
for the same reason that someone
picks them up for their
professional journey. They trust
the author, they trust the book
to take away something for those
people will do. So this way to
go from the book once they've
made it to the book for someone
seeking a career in deep
learning or just like hoping to
get ahead in their path in deep
learning. Where do you see this
going from?
Luca Antiga: So I've never
written a blog post I actually
think yeah, I am very shy about
that. I don't know how, but I
can I can share the blame with
these things. To further yeah.
Eli Stevens: I think that my
take on on how someone would go
from from zero to career. I
think that I think that one of
the big challenges is, is going
to be just just the process of
interviewing. Because there's,
for better or worse, a lot of
times when a job is posted on
the internet, there will be a
flood of people applying to it,
especially at least the tech
industry specifically. You know,
I'm coming at this from the San
Francisco Bay Area perspective.
So I don't know about how the
rest of the things work
elsewhere. But you're gonna get
a flood of applications from a
bunch of people who haven't read
any of the requirements. don't
meet any of the requirements,
and just just click the button
and on Craigslist or LinkedIn or
wherever, and so one of the
challenges then, for someone
who's trying to break in is to
catch the eye of that hiring
manager of that recruiter and
say, hey, I'm someone who even
though I may not have, you know,
10 years of experience doing
deep learning, which, you know,
given the state of the world is
incredibly difficult to have
right now. Like, you need to be
able to catch their eye in some
way to say, okay, this isn't,
this is one of the ones I just
need to throw this resume in the
trash because they didn't read
anything. Um, you can't do that
with a cover letter, because
half the time the cover letters
just get thrown in the trash
too. And so my suggestion would
be to go ahead, work on a
project, you know, the book
hopefully equips readers to be
able to do that kind of thing.
So go ahead and do it. Pick
something that you think is
interesting. Maybe it's a
capital project, maybe it's
something that you know, that
you've wanted to try. You know,
it doesn't have to be this this
grand thing. Just something
something that you've done, try
and get some results with it,
get it up on GitHub. And and
have a section on your resume
that lists out, you know,
probably file it as open source
rather than hobbyist, but like
open source contributions, you
know, lead developer on this
project and talk, you know, have
a couple bullet points about the
results that you've achieved.
It's like that I would hope, at
least for some people, is going
to be enough to say, oh, well,
maybe I should take a deeper
look at this person and see what
they're about. That's my hope.
We'll see what happens when the
books actually out there. And we
have gotten some feedback from
from a couple early readers that
that indicated that those kinds
of things were possible. So I'm
hoping I'm not too far off base
there. I don't know. Tom,
anything. You got the;
Thomas Viehmann: Yeah, I mean, I
think I think writing a book
probably isn't the best path.
Eli Stevens: It's not but it's
really not.
Thomas Viehmann: And not the
fastest either, I should say, at
least for you guys. But so more
seriously, I think that if you
if you contribute to open source
projects on GitHub, and you in
your manage to find a project
where you can have meaningful
contributions, and it's a
reasonably popular project, then
you'll eventually get mailed
from recruit recruiters to so I
can track that because I use a
distinct email from GitHub. And
so quite a few larger companis
contacted me because of that.
When before people, I once
applied for it for a data
science position, before doing
my own thing, they rejected me
because I didn't have any
practical credentials. And so, I
mean, this is basically do
something either contribute to,
to an existing project that
people will look at, well, who's
doing that and who might be
available from that. And the
larger companies actually seem
to do that. And the other thing
is, I guess just do other fun
things like tutorials, blog
posts can be can be pretty good
too. And also projects. And if
you do a project, don't do the
in and this is something where
I'm hesitant to wholeheartedly
recommend Kaggle if you have
your own project idea, like a
creative application now of
classification, right?
Classification isn't all that
hard, or at least the core of it
isn't all that hard in deep
learning to do with deep
learning. But if you have a
really good idea what to what to
apply it to, I think that makes
a great project because you can
then really show and maybe you
can even if you if you have the
chance, you could even include a
picture in your application of
your, of the output of your
project. So I think that's
really creating the project that
you can work with as one way,
one great way the other would
be, kind of contribute
meaningfully to a relatively
high profile project. And the
area where you want to work.
Luca Antiga: Yeah, totally
agree. If I can add something to
that. I need point of, I think
one of the things that one of
the dangers is to, to do small
peeks of one thing and the other
thing which is fine for a
certain stretch of time. But
then you run the risk of not
building stuff so not putting
this tiny bits on top of each
other and actually see something
grow from that, you know, be
either contributions or your own
project. So what I can recommend
is persistence, so keep at it,
and you'll get better and
better. And so, if and if
possible, you should pick
something that you will live
with for for a while that you're
okay living with for a while.
Because I think reaching some
level of polish or some presence
or accruing experience, sticks
time. So even in this changing
world;
Sanyam Bhutani: It, I like to
call it the curse of infinite
learning we just keep going from
one resource to another because
this because the market did so
well and you keep feeling
intimidated Hey, I don't know
what this is. Let me just try
that another book try another
course maybe after that I don't
know how to get it out 10% of
cargo, but it's about being in
the right discomfort zone where
you can just keep learning and
building a skill. If you could
recommend for example, someone
who might want to contribute to
Pytorch for example, a way would
you recommend them to start
after reading the book, for
example.
Thomas Viehmann: probably one of
the ways finding some things I
where you think something is
wrong in Pytorch and then go
find a way to fix it. And
obviously if if you do you're
part of the research my
experience with the with the
Pytorch people is that they'll
be very happy to help you along
to kind of go from you've done
the research, you've developed a
rough idea how to fix things to
you actually submit that pull
request. Um, yeah. So I think
trying to fix something small or
at least kind of, well scope. So
for example, if you find, yeah,
here, for example, there's
something slow about, about some
particular kernel. The other day
someone, someone fixed the
unfold kernel backwards. And so
there, there was something funny
going on and the Pytorch
cleaner, and most of the Pytorch
kernels are very, very good. But
there's always things that you
can improve also, with specific
functions. And if you find
something like that, that's
probably a fairly good project
or a fairly good first thing to
work on. But, again, if you if
you feel like, like doing your
own project, I think that might
be even a superior approach. And
for that I would strictly
recommend to go from, you have a
problem you want to tackle, and
then you solve that. And the PI
torch is there to help you go
from problem to solution.
Pytorch is not an end in itself.
For most parts, I mean improving
pi torch in order to facilitate
more great projects is a good
goal too. But you always want to
have something that's kind of a
problem you want to solve and
then you want to work to it. And
the things you learn in between
should be kind of getting you
ahead with the problem, rather
than just trying to know
everything about everything or
something.
Luca Antiga: I guess maybe not
sorry, sorry.
Sanyam Bhutani: I guess that's
it. That's a great point. Like
you mentioned, someone might
feel I have no idea what a
kernel is what a backcourt is.
But if it's curiosity driven,
you might with a fresh set of
eyes, you'd say something funny
is going on. And then you end up
contacting the developers and
then you end up waking up here.
But again, if it's curiosity
driven, it's easier to make that
contribution instead of just
going out there and hunting
okay, let me put something on my
resume related to Pytorch, maybe
I should go to the GitHub. Maybe
that's not the best route.
Luca Antiga: Yeah, I think one
of the opportunities out there
right now in the Pytorch world
is the ecosystem. So while
Pytorch is by now it's become a
very elaborate project, even
though it's approachable. But
there's a lot of very good
engineers working on it, and so
on. There's a growing ecosystem,
that for sure has a few more low
hanging fruit. In maybe it's a
bit easier to approach for a new
colour because it's scope scope
to be more narrowly and so on.
So I would suggest also look
into that.
Eli Stevens: I was actually just
going to mention the ecosystem
as well. I think the the Pytorch
ecosystem, you know, in 2020 is
in a really interesting place,
you're starting to see a lot of
interesting, slightly higher
level training frameworks, a lot
of there's a project called
cornea that has a lot of image
manipulation routines that are
all support backpropagation So
that you can do things and still
train through those operations.
You know, it's pi torch
lightning and catalyst and the
work that fast AI is doing. Like
there's a bunch of interesting
things going on out there. And I
think that, like Lucas said,
that's a great place to go ahead
and get a toehold.
Sanyam Bhutani: For the
audience. If you're interested
to make a contribution to fast
AI, I pretty much live on the
forums, please find me contact
me anytime. I'd be most happy to
do that, too. But that, that
brings me to a question from the
AMA. This is by a Akash Nain.
And he talks about there are so
many high level API's are coming
up for Pytorch Catalyst, PyTorch
Lightning was lightning. Just 
o mention two more, don't you 
hink this would lead to 
ragmentation that happened with 
ensorflow earlier?
Eli Stevens: So I've actually
been been looking into this a
little bit and one of the things
that that I've noticed that I'm
really happy about is that a lot
of these are focusing on
removing the busywork while
making it so that your actual
model code remains agnostic. So
you have this, you have this
training loop that's going to do
distributed training for you,
it's going to do early stopping
for you, it's going to log
things to tensor board, but your
actual model isn't going to be
impacted by your choice of
training loop. And that I think,
is is really powerful because it
means that if you decide, oh, I
don't like the direction that
this project is going, or I need
this, this killer feature from
some other thing, you just pick
up your model. And I mean, I'm
not gonna say it's going to be
zero work, but you just go over
to the other one, you pop it in
and you get running over there.
Um, the other thing that I've
found is that of the, the ones
that I've looked into, and I'm
not gonna name names because I'm
not going to try and make
favourites out there, but due to
the nature of how Pythonic
Pytorch code is, getting into
these train frameworks and
reading their source code is
really approachable. And so one
of the things that that when I'm
adopting a tool like that I like
to do is I like to go and look
through the source code and say,
okay, if I need to, could I
maintain this myself? You know,
and I'm not saying that I could
maintain it to the same level of
innovation that the true
maintainer is on, but you know,
they all get hit by a bus, am I
going to be stuck with a pile of
code that I can't interpret? And
I've been really pleased with,
with how approachable these code
bases are. So I think that
there's, um, there's the
possibility of fragmentation.
But I think that that, and this
isn't globally true, but I think
that in a lot of cases, they're
they're less in the frameworks
are imposing less on their
customers than might have been
true previously.
Thomas Viehmann: Yeah. So to add
to Eli, I think one of the
criteria I have for picking
something there. I'm not using
many of the higher level
frameworks. I don't know them
that well, but one of the things
I look at is, does it? Is it
kind of helping me do some
things that don't want to code
up all the time, but can still
use Pytorch otherwise? Or is it
kind of getting between me and
Pytorch? And I'd probably try to
do something that doesn't
distance me from Pytorch or, or
the maps because then what Eli
said, moving if you need to, or
using something in Pytorch that
the framework isn't anticipated.
wouldn't be possible. But if you
have something that kind of
strictly simplify something that
lets you do your modelling in
pytorch properly, that's
probably probably a good thing.
And then the other thing is that
there's kind of a balance to
strike between having Canada a
simple landscape of tools and
having one tool for the job
only, and kind of having
something that can innovate
independently. And so can we see
we've seen this with with with
deep learning frameworks
themselves, were kind of pi
torch and tensor flow. Now, they
have converged quite a bit on
similar feature sets, at least
in the coming days, very, very,
very bird's eye view. And then
if you if you want, if you to
innovate, can you need someone
to drive it and you could either
drive it within projects or you
could have several projects. So
for example, frameworks like
JAX kind of contribute somet
ing because they're just, they'
e not just fragmenting but they'
e also contributing anoth
r source of innovations. I think
that's similar for the highe
 level frameworks.
Luca Antiga: Yeah, I think it's
a it's a bit different from the
10 fragmentation that we
observed in it more in the
TensorFlow one times, they were
the framework itself didn't help
you. You know, thinking high
level too much. And so people
started to to, to design
abstractions on top of that, but
the the problem with those is
that they are struck
obstructions were often leaky.
So, you know, after you did your
part over there, then you had to
go down and still, you know, for
for many tasks understand either
because you had to parse the
errors that came out and so on.
There was still this reality
down there coming up and hidden
you somehow. And things have
changed since in TensorFlow
land. So, but what I see in the
Python Pytorch ecosystem, like
lightning and optimism and so
on, is that the approach is
totally different don't try to
mask what what is fighters but
they're trying to make you do
less or more like say, Yeah, not
care about things that are
important that can be
implemented automatically and
you focus on modelling style.
And so unlike Eli and almost
said, said, I think it's a
pretty different place and he
said, testimony to the design of
Pytorch.
Sanyam Bhutani: I'd like to drop
a plug, I've interviewed the
creator of catalyst on the
series. So audience, if you'd
like to check that out, please
find the link in the description
of this podcast. Another
question by Akash is and I think
I'm not sure if this is covered
in the book. But speaking of on
the intuitive side, if someone
wants to switch to Pytorch,
basically, they are interested
in edge influences mlms devices.
What are your thoughts in that
area?
Thomas Viehmann: Inference? You
mean, like deployment on various
devices? Right?
Sanyam Bhutani: I believe so.
Yes. On this question is on edg
 devices I'm sure you cannot tra
n yet on these devices, 
Thomas Viehmann: Well, yes, so.
So I think I think the story is
kind of continually improving.
So it was a year and a quarter
or so it was in December. 20
2018 when I first tried and I
got to proof of concept of
running pi torch and Android and
back then Facebook didn't really
have any interest in that and so
there wasn't a huge investment.
But then in the last summer
there was Pytorch mobile and
then that came out. And so I
think there's for example for
mobile deployments Facebook has
invested quite a bit in this has
seen a lot of love and now we
have, unfortunately it was too
late for the book. Now we are
titled serving as the go to
thing for serving and while not
everything is quite as optimised
as we, as we would like to have
it eventually. I think it's,
it's getting there very quickly.
And you can have very decent
results also on all sorts of
devices. So I've worked quite a
bit with Pytorch on the
raspberry. And that works
reasonably well for inference.
And of course, with the small
GPU computers, you can also do
training if you want to, for
example, if you want to do
things like federated training
might make sense to do things on
the device more intensely. And
that works very well to the
beauty of it is that with
Pytorch, I think and I think
that's something we're pipeworks
really shines to. You have kind
of the end to end experience as
Really in PI torch. So you don't
really, you don't have to export
your model and convert it to
kind of special mobile format or
anything. But you can just let
your model go through a budget.
And then this Pytorch model, you
can either reload in PI torch
itself and run it there. Or you
can run the same thing on
mobile. And I think that's,
that's something that makes it
really easy because you can
switch back and forth and try
something on your main device
and then to go back to the, to
the edge device. And I think
that's, that's something that I
really liked about how pi torch
develops there. And I think
there's a lot of good things and
a lot of performance
improvements also. Coming over
the next half, so.
Sanyam Bhutani: So the next
question is sort of a trick
question. What are your thoughts
on deep learning with
TensorFlow, so to speak, that
sort of book for the audience I
am just talking about deep
learning using TensorFlow,
generally.
Thomas Viehmann: Yes, you can
download Pytorch. Give it a try.
Sanyam Bhutani: I would agree.
Thomas Viehmann: More seriously.
Unknown: Really, I'm sorry go
ahead.
Thomas Viehmann: More seriously.
I think TensorFlow has a lot
going for it. And obviously,
it's a it's a large, well
supported framework. And people
are doing great things with it.
And, and like it and that's a
great thing. So I think there's
nothing wrong about using using
tensorflow and we shouldn't. I
mean, we like Pytorch for things
that are good about Pytorch. But
at the same time there, there
are people that like tensor flow
for things that are good about
tensor flow, and that's
perfectly cool. Not everyone is
the same, not everyone wants the
same thing. So, I think it's a,
it's a good thing. Sometimes
it's annoying when you find
something that's written for the
other framework, but I'm sure
that goes both ways, right?
Sanyam Bhutani: Yeah.
Unknown: I think I always
suggest when they asked me which
framework or Wi Fi 13, I would
say, you know, you should choose
the framework that you use stick
to the framework that you know
best. And I think, you know, you
need to be intimate with the
framework, because you don't
have to see the framework, the
framework has to disappear at
some point right. So, I find
that pi torches as a as an
easier path towards that goal.
So these appears sooner you
know, you stop thinking about
fighters and stuff thinking
about what to do with it sooner,
it's easier to, to know, to have
a good sense of what he does.
And then of course, to know it,
there's a lot beneath, but it
never pops up. So, so much. So I
think that that's the that's
advantage of pi torch over other
frameworks, but not knowing, for
example, TensorFlow well, in the
TensorFlow two era, then I can't
really speak for Tesla.
Sanyam Bhutani: This has been an
insightful interview for me, I
want to ask you one final
question for people that are of
a non traditional background
that aren't math or CS.
background of your citizens to
such people who are looking to
get a break into machine.
Thomas Viehmann: I think start
with the project. That's from
your domain of expertise. And
then after you've read our book,
of course, you'll be all set and
be. And maybe that's the other
recommendation. That can is
something that I think is very
important is finding kind of a
community where you think you,
you you fit in and you feel
comfortable interacting with
people, because everyone will
get stuck at some point. I know
I certainly did, and I certainly
still do. And so I always
recommend the Pytorch forums to
get help. But obviously, other
frameworks and other toolkits
and other libraries also have
great communities. Sai has a
very active community as you
know. And so Tensorflow has a
lot of mindshare. Well, I don't
know that they have a forum
similar to Pytorch to the
Pytorch one, I think. And I
don't think they have chapter
yet he was the face of Pytorch
on the tightwad forums. But but
so find something where you feel
comfortable asking questions.
And you also feel comfortable
asking stupid questions because
you will get stuck and you will
have to ask. And after you ask,
you will say I could have known
that. But that happens to every
one of us.
Eli Stevens: I actually have a a
quote unquote dumb question that
I asked just just a week ago, I
was trying to figure out why I
couldn't get decent scaling on
multi GPU training. So asking on
the Pytorch forums and going on
to this. It turns out that the
the whole machine that I've got
with two GPUs, the airflow is
bad and I was having thermal
throttling when they were both
running. It's just like basic
physics. And yet I'm like, why?
Why can't I get good scaling?
It's so bad. Um, to go to go
back to the original question,
one of the things I also wanted
to add is that um, I do think
that for people coming in from
from, you know, outside of the
traditional background, and I do
not say this in order to
dissuade anyone at all, but do
recognise that, you know, CS
programmes and math backgrounds
and all of those kinds of things
are so common in this field for
reason. Someone coming in
without those those I'm gonna
call them advantages, you
recognise that you're going to
be lacking in in those
advantages. And as you are
working to towards proficiency.
Be very cognizant of where your
gaps are, and and proactively
work. To to fill those gaps in
ways so that they're not holding
you back. Um and if that means,
you know, shifting your focus in
certain ways to focus more on
your strengths, like Tom said,
pick a project that's already in
your area of expertise. So
you're not having to learn
domain knowledge at the same
time as having to learn the you
know, API knowledge and all of
that kind of stuff. And use that
time savings to then kind of
catch up on the other parts that
you're going to need. And so I
think that you know, being
honest with yourself in terms of
like, knowing what you need to
work on and knowing what you've
got down cold is really going to
be helpful as as as you make
those kinds of transitions.
Luca Antiga: Yeah, what I would
say is that coming from other
domains, is a great strength.
Having a domain question is
something that it's really hard
to have, if you have a purely
computer science background,
because then you have to learn
how to what questions you might
want to ask when you're getting
to something. And he takes a lot
of time and effort to to be able
to formulate the right questions
or interesting questions. So I
think there's a great
opportunity to fill the gaps, as
Eli was saying, are coming from
an excellent, different
background, and have all this
baggage of experience and
questions and curiosity, that
it's really you know, this soft
thing that it's hard to acquire,
if not, you know, being having
been immersed in a different in
a different setting before. So I
think there's a great
opportunity, great for
chemistry, you know, take this
and take that, put them together
and and see what happens. So
it's it's really precious.
Thomas Viehmann: Yeah. Maybe to
add to that kind of teaming up
with someone, maybe even someone
who helps you bridge the gaps
can be hugely beneficial. And
also teaming up with someone
probably is universally a good
idea. When you're under very new
projects in particular if it's
going to be longer ones. I mean,
if it is teaming up helps
writing a book that teaming up
helps with almost anything.
Luca Antiga: Yeah, teaming up
team up with someone. And when
you think you don't, you will
never make it. Look for a third
one.
Sanyam Bhutani: Kaggle is the
easiest platform to team up on.
Also people think of how shall I
get into machine learning. Maybe
the reason you weren't excited
about it in the first place is
because you'd like to apply it
to your field and this is one of
those those rare fields where
you don't have to start from a
junior role all and work your
way up again, bring machine
learning to your field already
leverage your knowledge, see and
go from there. Many people miss
out on the fact that it doesn't
have to always start from
scratch in that sense.
Unknown: It's an excellent
point. Yeah.
Sanyam Bhutani: Now, before we
end the call, I'll definitely
have your LinkedIn websites and
Twitter handles. Any other
platforms where we can follow
you and follow your work or any
platforms you'd want to mention?
Unknown: Yeah;
Thomas Viehmann: I haven't
actually. Sorry Eli;
Unknown: Yeah, go ahead. Go
ahead.
Thomas Viehmann: No, no, you go.
Eli Stevens: I was just gonna
say that I have an absolutely
dire social media presence. It
is. It is a barren wasteland.
So;
Sanyam Bhutani: Maybe our
audience can help change that if
you want.
Eli Stevens: It sounds good.
Thomas Viehmann: Yes, so I have
a blog. And yeah, recently I pos
ed too much about epidemiologic.
Because I needed to understand 
hat for my own well being. Bu
 usually I try to blog about coo
 kernels. So things you can
do with Pytorch and stuff like
that. 
Sanyam Bhutani: Awesome.
Luca Antiga: Yeah, I have a very
low volume Twitter feed. So I
like to keep from time to time
along TiVo is my Mo. And then
and then among a bit of GitHub,
I contribute to Radis.ai and 
] does a very few other thing
. So yeah, that those are m
 social things.
Sanyam Bhutani: Awesome, again,
audience please find that in the
description of this podcast.
Eli, Luca and Thomas thank you
so much for your time. And thank
you so much for;
Unknown: Yeah, it's been a
pleasure. Thank you for having
us on.
Yeah. Thank you. Thank you.
Sanyam Bhutani: Thank you so
much for listening to this
episode. If you enjoyed the
show, please be sure to give it
a review, or feel free to shoot
me a message. You can find all
of the social media links in the
description. If you like the
show, please subscribe and tune
in each week to "Chai Time Data
Science."