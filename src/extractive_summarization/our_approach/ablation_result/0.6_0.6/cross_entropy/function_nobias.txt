Cross entropy is one out of many possible loss functions (another popular one is SVM hinge loss).  These loss functions are typically written as J(theta) and can be used within gradient descent, which is an iterative algorithm to move the parameters (or coefficients) towards the optimum values. In the equation below, you would replace J(theta) with H(p, q). But note that you need to compute the derivative of H(p, q) with respect to the parameters first.
 So yeah, the  negative log likelihood and the binary cross entropy are  equivalent.  And in practice in deep learning, people just say  cross entropy, multi category cross entropy, which would be a  multi class version of the negative log likelihood, which  we will cover later in this lecture when we talk about the  softmax function.  So just to keep it brief, the negative log  likelihood that we just covered a few videos ago, is the same as  what people call the binary cross entropy, they were just  formulated in different contexts.  So negative log  likelihood comes more like from, I think it's like, it's probably  from a statistics context, I don't know the first paper, or  reference that mentioned that.  But this is something usually I  see in statistics papers, and the binary cross entropy thing  has originated from the field of information theory, or computer  science.  So we have actually seen that, or not, the cross  entropy, where we have seen the self entropy, or just entropy,  and statistics 451. 
 And  there's also a multi category version is the multi category  cross entropy, which is just a generalization of the binary  cross entropy to multiple classes.  So in order to make  that negative log likelihood or binary cross entropy work for  multiple classes, we assume a so called one hot encoding, where  the class labels are either zero or one for some reason, it was  cut off here. 
 We have   to use something called cross entropy loss,  and this is actually the loss function that   fastai picked for us before without us  knowing. 
 Basically it turns out that all of the loss  functions in pytorch have two versions – there's   a version which is a class, this is a class,  which you can instantiate passing in various   tweaks you might want, and there's also  a version which is just a function,   and so if you don't need any of these tweaks  you can just use the function. 
