Welcome. So, today we will be ah doing our
exercises with ah transfer learning. So, in
the last lecture ah we had discussed about
one of these aspects which is called as duman
adaptation . And duman adaptation as it goes
down is to an experiment I had ah through
one of the results from one of our earlier
publication papers and through a lot of other
aspects I will shown you that if you have
a classifier of some sort . So, not necessarily
only a deep neural network, but then you have
a classifier which has been trained on one
particular data set to do a particular task.
And then can you use all of this ah in order
to solve another related kind of a problem.
So, one of those examples was ah what we did
with retinal images in which it was (Refer
Time: 00:49) on one data set called as drive
which was more of from healthy people and
was from a different ah camera a different
hospital and everything. And then ah when
we wanted to use the same model which was
trained to do vessel segmentation on that
particular data set on a different data set,
and then you saw this problem that ah initially
it was not ah giving out good results if directly
deployed.
And ah then what we had an idea was can we
use some examples at least a minimum number
of examples over there in order to make this
come down to a convergent point and will that
be better then trying to train a network from
scratch, which is ah using only the data from
that other data set on diabetic retinopathy
which is called a stair ok.
Now, standing on top of that ah on those experiments
ah what I said is that for deep neural networks
. We had done our experiments still now using
ah some of our pre trained ah models, but
then we never use the weights over there.
We were just importing the model architecture
and then we trying to do it. So, we had done
it on ah ah LeNet we had done it on ah vgg
net on ah GoogLeNet on ah ResNet on densnet
and ah over there what we had done is just
take down the architecture, and ah trained
it now for our own data set, and for us most
of these experiments we were doing it on see
for dataset which is just a ten class classification
problem and the images are pretty small at
32 cross 32 pixels and colored images .
So, the only ah common ah denominator which
was binding all of them was that these were
natural images and GoogLeNet kind of networks
which were also done where for natural images,
but then ah um all of these networks were
for a thousand class classification problem
on image net. And here was just a ten class
classification problem, and we took this ten
class example over there ah from the aspect
that, it is easier to train when you have
lesser number of classes, you will be requiring
lesser number of data. So, it was just a good
toy example to get started with around.
Now, today what I am going to discuss is if
we take a network which was already trained
to solve the image net problem to a great
extent. So, say these GoogLeNets and ah vgg
net then dens net, ResNet which made up its
ah real ah soaring appearance in cvpr on virtue
of winning the image net challenge. So, can
we use all these pre trained models and would
that give us an leverage.
Now, definitely looking down at the aspect
of what we had learnt from the earlier class
ah if we have a trained model and then we
just try to refine it out it. It would be
much easier to do ; however, then there can
be multiple ways of refining . So, one of
the ways of refining maybe where I I update
all the weights over that for the whole network,
but that is a costly ah process .
Now, the other way you can refine it that
ah you just just update that final terminal
layer, anyways you will have to change the
architecture for the terminal layer . Since
you are now not doing a thousand class classification
problem, but just a ten class classification
problem . So, let us let us look into what
happens over there, and today's experiment
will have something where I do not ah use
a pre trained model. I just train everything
ah from the start versus ah if we use a pre
trained model and then either modified partly
or or modify end-to-end and then what comes
out ok. .
So, the over here ah ah as it goes through
ah if we look into the codes over here, then
ah you can see down that ah, the first part
is pretty simple and standard that is the
header which we have been using ah as of now
for all of our ah experiments. We do not make
any change over there. Next comes down our
data. So, for the data, we are using ah CIFAR10,
and now since ah this particular experiment
deals with GoogLeNet.
And what we had seen is that GoogLeNet ah
by it is ah virtue core virtue over there
; is something which was requiring 299 cross
299 pixel image is not two four ah cross two
two four sized images over there. And for
that ah reason we had to scale and transform
these images over there. And ah once you have
the scale version of images at 3 cross 2 2
ah 3 cross 299 cross 299 available then you
can work it out. And ah we also choose down
a smaller batch size. In fact, here we have
even a lesser batch size of 8 ah for the fact
that ah we would be training actually three
networks; and that is going to consume a lot
of memory on on our side as well ok .
So, let us ah get down to this one, so there
is the train loader and the test loader, the
train set and the test set which are completely
created out. And we are just going to you
make use of that . Now ah the, so since we
have the data and file already there it is
it is confirmed and it works it out.
So, the CIFAR data set which we have is of
50000 examples in the train set; and 10,000
examples on the test set . And then here we
start with defining ah um our network. Now,
if you look into the first network ah um,
this is the one which ah is ah similar to
the definition which we had created in the
earlier case example, which was just ah pull
it down from our taught fusion library of
models . And ah this is inception v3 which
we were using.
Now, I create the next network which is ah
um the same as over here, except for the fact
that ah I used this pre trained equal to true
argument over there. And what this does is
it does not just pull in the architecture
from the web archive source, but it also pulls
in the weights over there . So, although pre
trained weights ah we will now get important
and I am going to make use of this pre trained
model. So, this is something which is already
trained for solving the imaginary problem,
and now we are going to look down whether
it is able to solve this CIFAR10 ah classification
problem as well ok .
So, then we define another one which is called
as net 3 and next we also has the same kind
of a mechanism over there . The only change
comes down is that in net 2 ah. So, this is
what we have commented over here .
So, net 2 is the network which we are going
to do end to end fine tuning which is you
replace the last layer ah of 10,000 nodes
ah of of 1,000 nodes, ah and then replace
it by just 10 nodes over there . In an ah
when you are training you are going to do
the whole error back propagation over the
whole network .
In the other one, what we are going to do
is, we are just going to update the last layer
weights we are not going to update the complete
network over there . So, this is the change
which we will look down and we will look into,
so one aspect is that when you are just updating
the last layer ah weights over there. So,
your total compute complexity is really coming
down, because you do not have to update the
whole network at a go . Ah
On the other side, we need really need to
look at that ah at this reduced compute complexity
which ah we are offering, so that is a gain
for us definitely in terms of the compute
ah potential for the network . The last is
in terms of decrease in accuracy; so is accuracy
significantly decreasing ah because of this
reason or or can it really, keep on still
working out . So, these this is the actual
motivation of ah the whole work over there.
So, we just print down net 1 architecture,
because as such it does not print weights
if we are just printing down ah the other
network over there . So, net 2 and net 3 we
will have the same architecture which looks
out .
So, next the straightforward thing which we
had for ah inception and we just ah have that,
now ah what we do is ah . So, this one was
looking into the total number of parameters
over there, and then ah to count it out, so
this was another part which we had done earlier
this week which was look into the theory . Now
actually finding out the total number of ah
computational parameters .
Now, if you go down through this part, you
would be able to find out the total compute
parameters as we had done; in the earlier
experiment on GoogLeNet and that was something
around 27 million parameters over there. Now
once ah um that gets over .
The next part is to do this architecture modification.
So, as you remember, that in all of the cases
what we were doing is that the last layer
over there was getting replaced from ah 2048
to 1,000 neuron connection instead of that;
we are just going to replace it ah with 2048
to 10 neuron connection. As well as the same
kind of a thing happens for the auxiliary
layer as well . Because in each of these,
things you again have an auxiliary layer . So,
the auxi auxiliary layer also gets replaced
from ah ah 768 ah neuron to 1,000 neuron connection
instead of that, we are going to put down
768 to 10 neuron connections over there. . So,
these are the two modifications which we do
for all the networks.
And ah um ah again recalling back from the
GoogLeNet lecture over there. So, the particular
model which we are using in ah torch ah in
pi torch over here . Is the one it just has
only one auxiliary classification arm it does
not have two auxiliary classifications and
for that reason, we just take down to changing
only one of these arms over there; because
the there is not the other one in anyway ok
.
So, now what we do is we just copy down our
weights, the initial weights over there. Now
keep in mind one thing that ah for your net
1, this is which is randomly pre trained ah
this is this randomly initialized this is
not at all pre trained; net 2 and net 3 are
the pre trained model. So, net 2 and net 3
is initial state weights are the same in in
both the cases. They do not ah mix up in any
of them and net 1 weight is very different
from net 2 and net 3.
Now, we get into the pattern GPU availability,
now it is if it if it is available then there
is a typecast operator. And ah we are done
in the previous lectures on what happens during
typecasting, and how ah there is a dma transferred
and and the whole model as well as the weights
and ah the data itself gets transferred over
there. So, this part is also quite clear with
you guys on what we were doing with the typecasting
and what was the ah compute equivalent on
on the computer hardware side of it, what
would was it initiating on a hardware ah aspect
as well.
Now, from there ah coming down to the loss
criteria and they are pretty simple . So,
since we are solving the classification problem,
so it is the negative log likelihood loss
which we are taking down and then ah the optimizer
is Adam for all of them and ah the um ah ah
learning rate over there is also kept constant
ok.
Now, having said that there is one critical
aspect which we need to look into it . So,
for network 1 and network 2 is where we are
going to do end to end. So, network 1 is which
was randomly initialized and then you had
trained it ah from the final node to the initial
ah layers over there . Network 2 is which
is already pre initialized with the ah ah
the weights from solving the image net problem
and we are still going to update the total
ah weights of the all the layers from the
last layer till the first layer . Network
3 is where we are going to ah freeze in the
weights of the layers from the first till
the terminal one . So, only the last layer
which is for connecting down 2048 neurons
to 10 neurons that is the one which is getting
updated everything else does not get updated.
Now, as a result when I am not updating anything
else over there, so I do not even need the
auxiliary losses, because there is no back
propagation happening off for all of them.
So, they are they are the ones which are going
to remain preserved . Now, that is the change
which we have in the code. So, in the code,
if you look down into the parameters what
we do for network three is we just use this
fully connected parameters over there of the
fc layer. So, this was this ah fc layer ah
which we had changed over there for each of
them .
Now, we are going to just use the parameters
over here in order to optimize. And what that
would mean is that this is these are the only
set of parameters on which the update rules
are going to run on the other parameters the
update rules are not going to run, and you
will end up updating only the last layer ah
for this network ok. . So, now, ah once that
goes down, we ah um train this model just
for 5 iterations because ah some part of it
is already trained. So, it is it is easier
to show it to you. The other downside is that
it takes really long [laugher] say ah GoogLeNet
per epoch was taking down ah like what 8 to
9 ah minutes of it to train down .
And now since we have three networks to train
down in one single shot. So, whether it takes
about 24 minutes or roughly half an hour of
time to train it. So, ah going it down for
more epochs is going to take more amount of
time; so for us we have just done it for five
epochs. In fact, ah within five epochs, it
does come down to the saturation ah point
what we wanted to show on the performance
characteristics ok. Now, there goes down ah
the losses and an ah accuracy ah tensors which
we needed for each of the three models and
then we start ah for each of the model over
there.
Now, within ah an epoch, I am going to load
down one batch of data . Now, for that batch
of data I just ah see if it is ah if there
is a gpu available, then we just do a cuda
typecasting which is to get the dma transfer
of the batch of data, onto my gpu ram. So,
that it can work over there . And then ah
my first part is, do a feed forward over it,
collect down the outputs from the auxiliary
arm. As well as from the actual terminal output
over there. And then do a final ah ah um classification
accuracy compute over there . Now, once ah
this this ah um accuracy is computed over
there.
Now, what we need to do is ah you will come
down to your ah optimizers and then ah actually
create down, ah zero down all all the gradients
intermediate over that .
Then do a criterion based loss function compute
for the auxiliary ah node; as well as for
the main ah terminal node over there. And
then find out what is your total loss for
the model. And this is what you keep on storing,
as well as ah within your ah optimizer you
are going to make use of all of this . And
do an optimizer dot step which just once the
update rule.
Now, since in your first two networks network
and network one and network two, net 1 and
net 2. You had collected down all the parameters
to be used within atom . So, all the parameters
are getting updated in net 3, we had just
used the last fully connected layer . And,
so the updates which happened down are only
in the last fully connected layer over there
and nothing else gets updated or modified
in any way ok.
Now, once that is done. We have our losses
accuracies and everything taken down and averaged
out over it. And then within each epoch I
am also calculating out my validation scores.
So, this is on the test data set which is
left over there. So, at the end of update
of one of these epochs, how much is the ah
change which happens, during the test performance
for the module. So, this is what I compute
out and then it pretty much goes on in the
same way. The only difference over here is
that instead of using one network, we have
three networks which we are using . And then
ah I decide to plot all of them. So, we will
let us ah go down and look into ah the models
and how they are training and then come down
to it ok.
So, within each iteration there are three
models which are training down over there.
So, if you look down ah at the training loss;
So, the first model ah which was a randomly
initialized model it is starts with a much
higher loss. whereas, the second model where
ah you had done a ah you you had just modified
only the terminal node over there. That is
the one which has a lower loss, while the
other one is where you had modified the terminal
layer, but not updated all the weights you
had just modified only the terminal layer
weights . So, in net 2 is where you do an
end-to-end update. In net 3 is where you do
only the terminal layer update. . So, it trains
out definitely much faster, but then the loss
is ah not so low . So, the loss is relatively
high over there .
Now, if we keep on going ah um through the
network, you would what you would find out
is that at the end of ah five epochs over
there. So, for model 1 the loss is somewhere
around point 06; for model 2 on net 2 is,
where ah you had updated the whole network
end-to-end for CIFAR problem is is around
point ah 1. And for the ah last one, where
you are just updating the final layers is,
where the loss is a bit higher.
Now, if you look even at accuracies what you
see is that ah the first model where you had
started with a random initialization, a random
guess over the weights and then updated all
the weights together; that has a higher accuracy
on the test data set . As compared to the
other model where you were updating only the
last terminal nodes .
So, this definitely means that, ah one point
which I was ah emphasizing on the lectures
on, ah domain adaptation and transfer learning
was that you need to ah really update, all
the weights and features, and everything over
there. Not just the classifier layer over
there . . So, it may be that ah your features
over there, are not quite specific to the
actual problem which you are dealing with
and they might also need to get an update.
So, this is a clear example where you can
see this ah discrimination coming down.
Where in the first case, when you are updating
all the weights over there. So, you see an
accuracy come down to at 81 percent; whereas,
in the last case where ah you are updating
only the last layer weights over there. So,
the accuracy does not go up that high, it
just stays at 74 percent ok. So, we look into
this part over there. So, this is the auxiliary
loss . Now, look into one thing in model 3
is, where we were updating only the final
layer . So, we all not connecting the auxiliary
arm, parameters and updating that. Though
we were collecting the losses, and there was
this arm kept over that . So, this remains
fixed this does not change in any way .
And for the other two models, you would see
that the auxiliary loss also keeps on decreasing
substantial. Now, model 1 is where it was
a random guess. So, it starts with a much
higher one; and then keeps on going down.
Now, in the next case is where ah you see
for the first network. network one, where
I was training it from scratch and doing it.
So, this is how the my train and test, ah
losses are decreasing and this is how my accuracies
are going down.
So, you see ah crossover point at about the
fifth epoch itself. Is when ah my train accuracy
is increasing, but my test accuracy is is,
now either saturating or trying to decrease
out. So, this is the first crossover point
which comes down over there . Now, this is
for my second model where I was doing an end-to-end
ah update based on the training over there
. Now, definitely it comes down ah much faster,
towards the convergence as compared to the
earlier model.
Where it was ah not initialized ah based on
some ah lever guess, but it was a random initialized
one ah. You see that, the crossover also takes
place much earlier. So, this is already at
the at at this point . So, which is ah almost
like this is the first epoch, ah second epoch
, so this is my third epoch. So, in between
the second and the third epoch is where the
crossover is already taking place over there.
.
So, maybe just after 2 epochs you can just
stop it, or a maximum you can go out to 3
epochs; and then you see that the whole network
is updated. So, this is much faster . So,
if you look into the earlier case, ah you
do not get this much of an accuracy of ah
say, 94 percent in the in in single shot over
there .
So, if you if we look into the first case
which is this model . So, at the end of third
epoch which is somewhere over here, I have
an accuracy of 75 percent, but then if I start
down with my weights which are taken down
from the image net problem. And then at the
end of third epoch, I am already at 94 percent
which is really good and and that is the advantage
which you get down with the transfer learning
ah of ah using a model which was trained on
a similar or or a rel or a on a related kind
of a data; for the related kind of a problem
and then just modify it. So, you have a much
higher accuracy coming down with just, lesser
number of epochs of ah training .
And then if you look into the last one, where
ah I was looking ah into just modifying the
final nodes over there .
Now, that will not always be helpful, because
you still see that you are limited ah um at
the below 75 percent accuracy in in either
of these cases and the ah . So, the trained
accuracy is also lowered, though the test
accuracy or the generalizability is much better
, but then one that your test accuracy is
still limited below 75 percent.
So, this is another critical fact which you
need to keep in mind that always updating,
just the final terminal nodes might not always
be the best possible solution to go around
with .
So, once we have that ah the next idea was
to ah . So, this is where it says just the
same kind of ah plot, but ah taken down into
the same ah thing together. So, this is my
train loss, and my ah test losses, I also
have my ah test accuracy plotted down over
there for all three of them . .
So, if you look into it your model 2 which
was a pre initialized model, and then ah just
being updated over there . So, your test accuracy
is really keep on ah increasing, and it is
definitely a few folds higher than your ah
other models over there; model 1 and model
2 ah, model 1 and model 3. Model one which
was just randomly initialized and trained.
Model 3 is where were only the final nodes,
were getting updated . .
So, now, let us look into the weights over
there. So, this is for my model 1 where I
had my random initialization. So, this was
ah the weights for the first convolutional
layer. So, you have ah 3 cross 3 kernels over
there. And ah 1, 2, 3, 4, 5, 6, 7, 8 and 8
so that was ah um ah sorry 4 and ah 1, 2,
3, 4 1, 2, 3, 4 . So, there were 32 ah. So,
you have ah 32 channels over there and 3 cross
three convolutional ah kernels over there,
which you had for the first layer.
Now, what you see is these were the random
initializations, which had been done . This
was the update after one ah ah after at the
end of five epochs over there, and these were
the change of the weights which you see . Now,
for your ah second layer over there, this
is what you observe over there. Now, in the
second layer ah definitely, because the input
was ah 32 channels over there, so we just
display for one of the kernels all the channels
coming down over there.
Now, if you look into the second model where
you had taken a pre trained model. So, this
were the weights at the start of the training
process, and these were the weights at the
end of the training process over there, and
these were the difference of the weights which
were coming now. Now, if you look into the
ah second convolutional layer over there,
so do you see these were the initial final
states as well as the difference coming down
.
Now, here comes the interesting part which
was the model 3, in which what I was doing
is updating only the final nodes and I was
not updating anything intermediate. So, these
were my starting states . And since I was
not updating anything on the previous layers
over there, so this becomes my ending state;
then my start state and end state is the same.
And for that reason, you see that your ah
weight difference matrix over there is completely
0. So, there is there is no update which is
happening over there.
Similarly, is the case for the second convolution
layer for ah the network three as well. So,
this is what ah we have as a clever and clear
exercise, for ah doing your transfer learning
on ah any kind of a deep network. So, this
was one example just using ah GoogLeNet in
which you have, the aspect of arm auxiliary
arm as well . And then how do you ah do this
either a full end-to-end training , or you
are just concerned with the last final edge
of ah final classification terminal nodes
over there. .
So, now, ah one ah thing which is clear from
these graphs is that, ah definitely it is
always advantageous to take down a model which
is pre trained. And ah for you it is going
to consume lesser amount of your CPU time
yes it does take more amount of time to download
the model, because the model is not just the
architecture definition. It also has the weights
over there. So, that is the additional bytes
which will be downloading over there . On
the other side of it your CPU time which is
going to get consumed is lesser.
Now, the next factors ah will you be updating
only the terminal layer. Now, this is case
to case dependent, we cannot directly comment
over here. But typical practice is that, only
updating the terminal layer might not be a
very good idea, because ah your features might
not be optimum for your particular ah ah data
set which you are looking . So, if you look
into ah image data sets from image net, which
are two two four cross two two four sized
over there ah. The granularity and resolution
is much higher . Whereas, for CIFAR it is
ah smaller sized image, they were just thumbnails
of 32 cross 32.
So, when once once you scale it up, in fact
there is a lot of blurring which comes down
around over that . Now, the features associated
with trying to identify an object which is
blurred versus the features associated with
trying to identify an object, where the image
is very crisp and of a high resolution are
going to be very different. And for that reason
what you would find out, is that ah this particular
model, does not work out good if you are just
updating the final layer.
Whereas, if you go down for newer kind of
problems, where it is a activity net is a
new challenge which comes up. A lot of people
have a good way of ah just trying to take
a pre trained ah ah GoogLeNet ah were, this
GoogLeNet was trained on for the image net
problem. And then you update only the final
layers, you do not update everything, because
on activity net also your your videos are
almost of the same size of two two four cross
two two four . And then natural looking images,
they are of high resolution; there is this
this this mismatch of early level or or low
level feature descriptors is, not that .
. So, that is ah what we have for the transfer
learning. As of now we have, the next lecture
where we are going to cover down, ah um transfer
learning with receiver networks as well; so
what happens within residual connections,
and ah what is an advantage which you gain
and if if there is some disadvantages as well
. So, we will be doing the same thing and
and repeating it, ah once over again to check
down ah what goes on over there as well . So,
with that we come to an end to today's one.
And thank you.