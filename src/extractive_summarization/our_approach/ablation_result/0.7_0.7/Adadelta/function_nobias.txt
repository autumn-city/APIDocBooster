 We created our ADADELTA method to overcome the sensitivity to the hyperparameter selection as well as to avoid the continual decay of the learning rates.".  The second good thing about AdaDelta is you don't have to choose the learning rate, it automatically computes it. 
Adaptive gradient algorithms have learning rates for each parameter.  This is very helpful when you have models where some parameters might be more sparse (increase its learning rate) or not sparse (decrease its learning rate).
Adadelta optimizer has a way to adaptively change learning rate but still, it needs an initial value of learning rate. 
 This was made possible due to this rho hyperparameter.  It is also known as 'Decay Constant'.  Instead of writing it with this equation, I have written it separately because we will need delta theta in this equation.  So, this term over here is the learning rate calculated by AdaDelta. 
 We are dividing square root of delta x with the square root of alpha... again this epsilon is a very small number to avoid zero division error.  Then we can sum it with our theta term... normal gradient descent stuff!  At last, we will update our initialized delta x term by rho multiplied by delta x of the previous iteration plus one minus rho into update square. 
