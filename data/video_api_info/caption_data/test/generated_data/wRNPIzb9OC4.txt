hello guys welcome to this python
by dodge tutorial actually and in this
tutorial we will see how to train an lcm
classifier on the amazon reviews data
set so first let's create um
the conda enb
and we'll call it torch dev
for now
uh maybe yeah i'll add something else
we
we would like
3.9 please
and yep
we activate
this env
let's make a data folder and then we'll
download
they require data files
style transfer okay yeah there you go
we need this guy
so i'll simply
paste it
and if we change this to
so now we should have both the files
yeah this definitely seems like it
downloaded okay okay so this is the data
set should not be very unfamiliar to you
all right so let's get started so we'll
start with reading those files and
creating a data set uh for ourselves and
we'll create a train validation and test
split which is going to be
um an 80 10 10 split
not here
um
that's
so it receives a path so this function
will receive a path and it will return
us all the lines in that text file
so
[Music]
file
and it returns all the lines now
let's let's
implement something called prepare data
set and what it will
receive is
a data directory and then
pus path is going to be os dot
path dot join
data directory plus pos dot txt
and the same thing for negative five now
these to get rid of
these squiggly lines let's import os
okay so now we have the pos path now we
have the negative path and
um
[Music]
pos lines
would be read text file
pos path
and
we'll have something similar for the
negative counterpart
okay so we should have all the lines now
all right
all right
let's actually see if this works okay so
if
name equal to equal to main
you know standard python stuff
we'll create a main function
let's
and for now this prepare dataset calls
it from this
data directory
let's see if
this works or not oh seems like it did
you should print something probably
okay yeah that was too fast to free run
it so
um
cool
so you know it has 400 000 lines as it
should
so let's now proceed to creating
like tokenizing stuff and whatnot so in
our case um
in our case
i will not be using nltk uh i'll keep it
simple and we'll treat you know uh we'll
treat every token that's delimited by a
space to be a new world in our
vocabulary
so that's quite simple
to do i guess
for line in
pos lines
let's create a sample
so this sample is going to be
first of all the line itself and then
we'll also add the label to it so we
know this is positive class
so we added that
oh yeah just making sure it was recorded
um
[Music]
and we'll do something similar
for
the negative part
instead of voice it's going to be
negative
so now data set should have 800 000
lines
and let's do some
you know minimal
text processing here
oops something happened
let's lower case everything and then
strip them as well
um
lowercase i'll strip them
and let's now also quickly create a
vocabulary for our downstream task
and
so that vocabulary is just going to
receive this these lines
um
it's just going to receive all the
sentences in our data set and then it's
going to create a vocab for us so let's
call this function create vocabulary
and it receives lines
and
i guess i should have
yeah i can pass the data set here
and
create vocabulary lines or we call it a
corpus we can also call it a purpose so
looping through all the lines
for sample in lines we know that text is
going to be
sample dot split
at this
since we added this extra
you know
tab here
so now we have the text and then in this
text we are going to
uh we need to maintain something called
word to index
right
and let's also maintain
well let's see how it goes
um
for word in word to index
actually not word
for word in text dot split so this is my
tokenization
literally split is my tokenization part
so if i'll see if this word
is
not in word to index i'll
create
i'll add it basically
uh this of word and we need to start
with an id right so what we can do to
build it dynamically is just get the
length of the current
dictionary and then
use something like this
and what it returns this function in the
end is uh
sorry word to index
so the total number of words that we
have in this dictionary or the total
number of keys that we'll have in this
dictionary
that's going to be our vocabulary size
and let's actually check out what the
vocab size is
um
list has no attribute oh yeah
my bad
it should be
this
okay so this is our current vocab size
and
as you can see it's quite huge like
283
256
yeah not not not so good so when you're
if you try to
train something
with
this higher vocab number you're not
going to be able to train anything
probably
so
to fix that we
can to fix that what we can do is
introduce something called minimum
frequency so words that only uh that
occur at least the minimum frequency
number of times in our dataset uh in the
corpus we'll only be retaining those
words otherwise we will not and in order
to filter out such things we will um
yeah we'll also need to maintain
something called
word to count
you know later on to filter stuff oops
and it will be initially empty
and if a word was not originally
there we can simply
initialize it to be zero
or no sorry one
else if a word already exists then we
only need to increment the counter
here
now
once we have this we can do something
like
actually we don't need to
so we don't really need to create this
guy here
um
yeah we won't need to create this guy
here
we can do it
after we have built the counts
so
um for
word in word to count
actually let's try to be a bit more
efficient for words
counts in
this
if
c is greater than or equal to minimum
frequency
um
word to index of this word
is going to be
um length of that yeah length of wood
index
and i think that should be it
let's see what's the vocab size now
oh create vocab is missing obviously
so let's say the threshold is 10 so
anything that's that occurs less than 10
times in our data set that's going to be
removed and whoa it's like a 10 folds
decrease right so from 280 something
thousand we
went from there to like 29 000 something
so this still looks okay
but you know we can do something more
so let's try to
let's try to further you know remove
some extra stuff and
while we are doing that
let me do
yeah let's install this
paper install
so these are two different packages just
telling you
black is something just that i need for
my purposes you it's not required
and
and then
so this is what black's gonna do it's
gonna format things
for me automatically
so now that we have this um this
function should return the data set that
we have well there are a lot of issues
with our data set we'll fix them later
data set
is equal to this and
print
oh
my god
i should have printed just a few of them
cool
so these are the top 10 sentences in our
data set and as you can see so this is
our sample basically so this is the
sentence this is the label associated
to it
now
you know there will be some instances
that we can
optimize upon so for example if there
are you know
there's something like this or there are
some other occurrences like that we can
we can take care of these guys
by
writing something
to normalize text
and
it will receive a
sentence so x here is a sentence
and first of all i'll get rid of
all the
well i need to import this to
we'll need rejects for this and
unicode data
let's first actually
write this function
so you know when you are dealing with
raw text files it's always possible that
some weird symbols
sneak in
so all i'm doing here is
yeah normalizing some
normalizing those special
guys
you don't need to do this i guess you
were doing something more special you
were anyways asked to
remove those special characters in the
assignment right so
you know you can ignore this part
this is just for me this is like a hacky
way for me to
take care of some things
because i don't want to implement
a complicated tokenization
here
so what we will be doing in this one is
okay
i guess if
so we'll replace the occurrence of these
guys
with
with
whatever's inside
and
and
anything that's
not
oopsy
so we just replace them with a space
so this is going to get rid of
uh some
some weird characters that might sneak
in
so instead of this
what we can do is line is
normalize text
of line and then we can simply pass the
line here
and
we can do the same thing
for the negative part and
this tutorial might be a bit long so
feel free to skip to the part that you
actually care about
let's see if this makes any difference
i should add some print statements
so you so you see when he added this
normalization part it's taking longer
because it's running that thing so yeah
now you see that you know from 29 000 we
were able to go down to like 23 000 or
24 000 capsules
this is still a bit higher but for our
purposes it should be okay not it's not
too bad because we'll be implementing an
lstm classifier which will be not too
deep and yeah we will see
um preparing data set
and
um
processing
positive.txt
and you know so that we don't process
all those 80 000 something lines again
and again
at some point
i'll truncate the dataset
okay so now we have a way to
so now we did two things we removed
anything that's below this minimum
frequency in our data set in our corpus
um
so
while we did that
um
so we only did that so we only removed
those things from our vocabulary however
we did not like actually remove those
words from the data set so when you pass
these things
and something that's not in the vocab
appears as it will
we will need to handle that
and to handle such special cases
we will add some tokens to it some
special token we will introduce some
special tokens in our vocabulary i'll
get back to that later
um
or maybe i can just add them here
huh
let's let's just do it here
um word to index
yeah that's just
so
for any nlp task there are
four
there are there are usually like there
can be more than four but usually you'll
find these four special tokens it need
not be exactly it need not exactly have
like this value
but there's a token usually to represent
the start of the sentence the end of the
sentence
um
the padding token we'll see why we need
that and then
there's this unknown token
okay
so
we will see where these guys
actually come in handy
and we'll get rid of this
we'll get rid of this as well
so now what to count blah blah blah
cool
so we should be good so now we will be
able to handle things that are not in
the vocab but in the sentence what we'll
do with the with those guys we'll just
replace them with this unknown token
but we will get to that before that we
actually need to create a
train validation and test split
so
if
before that we also should shuffle the
data
set oh there's no numpy
and i'm surprised
there isn't
oh it's already there it's just not
detecting
interesting
does it work
well it does work for some reason
the env is not updated
um
yeah
so here what we'll do is uh we'll
shuffle the data set
and
we'll create the train validation and
test split
oops
this is the vocab size we can
print this
all right
create train valve and
test split and it's as i said it's going
to be an 8 10 10 split so
let's
uh
whatever the length of the data set is
that multiplied by 0.8
well in our case we know that it's going
to be
a whole number but just in case it's not
for data sets it's not
okay
lengths of data set
0.1 and
now
the train part is going let's actually
let's actually create a dictionary for
our data set and what this dictionary is
going to look like is
so the first
entrained samples will make up for our
training set
the
the next nval examples after that
will make up for the validation set
and
whatever remains
that will make up for our test set
okay
okay
and
we are returning this data set now
so now we should be printing let's see
what's the data set size
length of
data set
this is the size of the training data
set this is the size of the validation
data set
and hopefully they should match
the numbers that you might have seen
before
okay
time to
time to truncate this
okay so yeah eighty percent of eight
hundred thousand is this and ten percent
of it is eighty thousand so it makes
sense now it's time to
you know truncate these guys i don't
want
all the 80 000 lines to
be pre-processed
every time you want to test something so
this is gonna speed up so this is just
temporary i'll remove it later on
this is just so can just so we can you
know
train things faster now let's also
return the
word
or the vocab
here
right or let's be explicit
and call it what it is
it is word to index
and why okay so another reason why do we
actually need to map a word to an index
is that that's because you know our
models cannot process these
raw texts
so what they understand a are these
values
so
eventually what we'll do is what we is
we will convert something that looks
like you know let's say i
like
this oops
i like this room
uh
to something like this where these
numbers represent the index of every
word
uh in in this word to index or in our
vocabulary right
so
yeah that's what this
indicates
but we don't have that thing yet
now
uh let's actually
this should be
it for
the data set preparation part
so let's actually move all of this
to another file
let's call it utils
and
from utils
i'll import prepare datasets that's all
we need so this is how so this is what i
should have told you this earlier and i
did in the in in-person
tutorial but
anyways so this is our raw data
from here we want to pre-process it
or prepare
pre-process it and then we need to
convert these into
uh
indices uh those tokens
i showed you before then we feed them to
the model we'll get some predictions and
we'll optimize a loss
against uh you know some ground rule and
that's how that's what we'll do in the
end so what we have right now is
sort of this and we want to basically
implement these this training pipeline
so
there are
i mean there are multiple ways of doing
this uh what we can do is now that we
have a data set uh we can create
you know we can manually create batches
from
these objects and then we said we can
simply look through those patches so one
example would look something like for
batch in
uh data set
train
um
64.
yeah so this is basically gonna so if 64
is our batch size then uh it's going to
create a batch of 64. so this batch
batch length is going to be 64.
so this is one way of doing it however
when we are
writing code in pi torch or when we want
to write it in by docs then there are
some utilities that we can use uh to
ease in our lives uh to add some
efficiency
computational efficiency to it and
we can do that by using the data set and
data set loader classes that my dodge
provides
so
hmm so basically what we'll do is we'll
create uh
a dataset class for us for us
amazon reviews data set and this guy
needs to inherit from this guy
i think it's
storage.utils.dataset if i'm not
mistaken
let's actually double check
torch
okay
this is what i usually do when i have
any confusion
um
data set trade
yeah thoughts utils data dataset
should be good
so when we implement our own custom data
set then we actually need to implement
or rather overwrite
the
these three
methods so one is the init or the
constructor of this class
the other is get item
and the last one is
len
so
so i mean these three methods they exist
for every other
python object uh so whenever you do
something like you know whenever so
first of all whenever you create an
object this method is called it's the
constructor and whenever you do
something like let's say for a list uh
you do something like
a of zero you're trying to access these
zeroth elements then for this list this
get item method uh is triggered under
the hood
and this length is
triggered when you want to get the
length of that object so for example
when you do if a is a list and then you
do length of a then this length inside
the list class
will be triggered
so now we'll simply
so what we'll do is we'll wrap the the
different training parts that we have
here
um
to this and we will see yeah
so the constructor is going to receive
the data
and
yeah
it's gonna receive the data set and
basically this is what we need to
the length can be uh
you can simply return
itself
data
cool
so whenever we do something like you
know length of whatever training
datasets we have
this is going to be triggered and
yep
okay so let's uh so
this is the interesting bit i guess
at least the beginning of the
interesting bit
so you know
we can actually trigger this
train part
is equal to data set
train
and let's try to wrap this
thing around
the amazon reviews data set that we have
here
yeah
let's see
and yeah you can ignore this vocab size
this is only because you know we are
processing just
200 lines in total
and we will fix this later on
this is just for debugging purposes
okay
interesting
interesting
oh i see what's the problem
word to index
so remember we changed the
signature so now we are also returning
this guy
let's try to rerun this
what happened
well it executed
without an error
and oh
so
okay
so as you can see this thing was not
triggered and it's only triggered when
you try to access
something
so this get items triggered
okay so now this so now we are in
and
let's see
so this is what the sample looks like
that we have currently in our data set
right so
notice this extra tab here
so what we will be doing is we'll
convert this uh this thing um to a list
of indices
and
all that needs to be done inside this
get item so that when when you know we
loop through this data set and
we try to access the ith element what it
returns is
um
yeah the things that we need like the
label label
the
token ids and stuff like that
so in order to convert this into those
token ids we will need to
pass the word to index dictionary here
as well
and
word to index
word to index
sweet
so from this sample we can extract the
text and the label as follows so if we
remember we were splitting it at the top
right so when you split it you'll get
two elements the first element would be
the text the next element would be the
label uh so right now the table is going
to be a
string so let's convert it into an
integer
right and then for this text
so let's uh implement something called
text to
indices or something like that
and you know let's move this guy
up here
and we can add that method here
we'll call it text to indices
and what it receives is some text
and it's gonna return
yeah it's gonna run that word to index
dictionary on top of it and then it's
gonna return us an id form of that guy
so
for word in x dot split
so x here is a sentence remember right
it will be a sentence
for word in x dot split
what we'll do is
we'll simply
word to
self word to index of
word
let's try to do
input ids
and
input ids is self self.text of indices
of
text
cool
let's see if this works or not
and let's get rid of this guy this guy
this guy
so you see that we
ran into this key error so this
basically means that still
doesn't occur
enough number of times in our essa so we
ended up removing it you remember we had
this threshold we have this threshold of
10 so things have to occur at least 10
times in our data set for them to be
considered in our vocabulary
so how do we fix this so for this
precise reason we added you know these
special tokens
where where
where did it go yeah yeah so anything
that's out of vocabulary we'll replace
that with an unknown
token and that's exactly what we'll be
doing here
okay
so
unknown id
is going to be self worth to index
of
this unknown token
and now we will change this to
dot
get
w
unknown id so if something is out of
vocabulary so if it returns this key
error then it will assign unknown id
it will append unknown id to this input
ids
and now it should be fine
let's see
let's hope it works
if it doesn't we'll see why not
all right
input ids
oh it doesn't exist yet
because this hasn't been executed
so you see all these threes they are
basically
they're basically uh
you know all these threes they're
basically the unknown tokens because
that's worthy and and it will be more
apparent if i i guess we also build
um
text yeah let's also maintain some text
updated text
and we can actually initialize both
these variables here
text
now
to this text what we will be doing is
okay cool
so first of all we get the token id
so the token id is going to be
self worth to index of this word
actually
this thing
the exact same thing and
if this token id is equal to equal to
unknown id which means that it's an out
of vocabulary word
to input ids will append
token id
well in any case we'll be appending this
um
we'll append
an unknown token here
and in cases it's not
okay let's try to be explicit here
oops
and
next door bend
that word
all right
let's also return this text
um
let's call it so this since this is like
a clean version or an alternative
version of this
guy
let's call it underscore text for now
and
let's see what this looks like so as you
can see
um anything that was out of vocabulary
it has been replaced by an unknown token
and there are a lot of unknown tokens
here because you know
the vocab size itself is too low this is
going to be fixed when we run this on
the full version of the data set
okay so now we have some things that we
need and
we can now return this so we can we can
go with a tuple like we can i return
something like this but you know tuples
are always less readable and
they're also immutable
so we'll go with a dictionary here
um so we'll update the sorry we'll
return the actual text that will be seen
by the model in the end like whatever
the input might have been
this is the text that the model is going
to see
and these are the input ids
and next we'll have to label
cool
so
we can
we can
okay
cool so now that we have that
we can let's try running this
so now you see uh what we get is a
dictionary instead of just a sentence
right so we get actually we need to fix
something this should be a string
um
okay now it should be a string yeah
so
okay so now there are some things to
note actually let's let's
go a step ahead and
create the actual data loader so to
create a data loader what we
need to do is to wrap this data set
that we just had
uh
inside
i think it's the same correct api call
towards utils data data set
towards util state of data logger and
what it accepts is a batch size so let's
say the batch size is 64 in our case and
there's an additional argument to decide
whether we want to shuffle it or not uh
we will
we'll not go with that for now and
let's see what the and actually let's
try with a lower batch size first
so now since this is gonna be
a data loader object we cannot simply do
a print
on top of it
so we need to
so now we can
yeah so we ran into some issues here
right
so that's because uh unlike in c
computer version um
in text we have sentences of variable
length so the sequences can vary so when
it was trying to and what th what
happened right now here is it was trying
to give us a batch of 10 sentences but
in order for it to work uh all the
sentences must be of equal length at
least right so
and this is a general problem in nlp
like practical issue in nlp so you'll
probably not see this in the papers but
when whenever you implement something uh
that deals with text you have something
of a threshold so that threshold decides
like what's the maximum length of a
sentence in your data set so for example
like there could be a sentence that's
you know
100 tokens long in your data set
um but you don't really care about that
you know even if so you you just
truncate it to let's say 30 or 40
because that's that will cover let's say
you're maybe 1995 of the dslr and
it's not uh really a bad thing you're
because you'll not be losing out on much
and especially for this task like this
is just a binary classification task so
if for one data point for like just one
class out of 400 000 you happen to
truncate some
information uh it's not gonna it's not
gonna
uh you know
be too bad
so let's let's implement that
thresholding here so we'll decide let's
say the
max length of the sentence in our data
set it could be
20 25 let's go with 25
or maybe
we can play with this
as much as we want
so now that we have uh
this parameter you might know
you might guess that you know some
sentences will exceed this threshold
some will
be shorter than this threshold so what
do we do for these guys so
for sentences that are longer than this
length
we truncate them
for sentences that are shorter than this
we pad them
so that's where
this guy comes in
and you know whenever you're training a
nlp model
you
add these two tokens two special tokens
to tell the model uh like which part to
focus on so for example uh when you if
your model is uh trained well enough
then it's gonna ignore anything that
comes after the eos because otherwise
it's not gonna it's it's not gonna know
there's not uh there's no way of telling
the model to uh
focus there's no way to tell the model
like which part to focus on
um right so for example if your sentence
goes like i like blah blah blah blah and
then
you don't have the eos token and then
you have a bunch of padding tokens
padding tokens uh so how will the model
know that you know it doesn't have to
process these guys so for that we need a
marker and that is exactly what this eos
token is and similar logic applies for
this sos token so
anything that's between these two guys
the model learns to focus on only these
and ignore anything that comes after
that
this is i guess a bit peculiar to the
nlp field
um
so let's let's
implement this
so we need to add this guy
here so whenever we are
okay
so similar to the unknown id we need
sos
here it's going to be eos
here it's going to be the padding token
because now we'll be we'll be needing
all these guys and you'll see in a
moment why so we'll need sos because
that's what we initialize the input ids
with
and then we also initialize the sentence
with sos tokens
not oss
sos tokens and
when
okay and here's where we're gonna add
that check okay
so if length of
input ids
if length of this is
greater than
max length
sentence dot then what we will do is
we'll
input ids is equal to
input ids
max length
and we'll do the same thing for text
right
because until now both of them are just
lists
this text being a list of strings this
input id is being a list of integers
right
so now what to do with those
that are shorter
so in this case we'll
add
so here we are truncating here we are
patting so how much to add
uh sorry how much to pad how many
padding tokens to add um that's what
we'll decide
right now and it's quite easy right
whatever your threshold is minus
whatever the
length of your input ids
is so i mean basically you are missing
these many
tokens for your batch to be of the same
size
cool
now what we want to do
is
add this guy
so we want to add these many pairing
tokens
oopsie
these many pairing tokens
text and
okay and these many pairing tokens to
the input ids
all right all right
oh by the way we forgot to
i forgot to not be i forgot to add the
eos token
so once we are done with this we will
add
the eos token
and
instead of
this it's gonna be the eos id for input
ids
oopsie
okay
all right
um
and we need to do something similar but
here we need to do this before adding
the padding tokens
otherwise
what's the point
so if we also so if we do this after
padding then it's going to look
something like
abc and then some padding tokens
padding tokens you know what i'm trying
to write here
and then it's an eos token and we
ideally want to ignore these pairing
tokens so
we add the eos before and then we add
the padding tokens
hopefully this should be okay
okay
let's see if this works or not
init is missing uh of course it's
missing something
yeah
so here let's say the maximum sentence
length is 30 in our case
or
to actually show you the effect let's go
with something lower
each element in the list should be of
equal size
that's true
i feel you there
but what's the
issue
let me take a moment
okay so i figured out the issue and the
figure issue was quite minor uh i needed
to add this guy here
instead of
doing it there because you know if
otherwise it's it's going to mess up
with this calculation right because we
are adding a token we don't want to
consider
so
now it should work
and
i add
this to the bug
you don't need it anymore
so this is how it's going to look like
now i know it looks ugly sort of but
what i want you to uh
focus on is this part so
as you might
have seen
this method was implemented for just one
sample but since
we wrapped this data set inside this
data loader
it's gonna
it's gonna apply that operation on this
batch
of size you know 10 so that's why you
see this and
right now it's a list of tensors
but let's let's make it a tensor not a
list of tensor
because that's what our model
understands we'll understand
um
so now it's a tensor
right
cool
this is by default the tensor
okay
okay okay
is there
anything
else i'm missing
nope oh by the way one thing about this
random shuffling is
try not to use random dot shuffle uh
i've encountered some issues with it in
the past like its behavior is not very
trustworthy so just stick to numpy's
random dot shuffle so i think here we
are done with the data loading part
and let's move again
all this clutter inside
a different file
oops
yeah let's move
this
somewhere
else
and
that's somewhere else let's call that
somewhere else
data logo dot pi
okay
and let's also add our function here
let's also add a function here which
says make
data loader what it will accept is data
set
it will accept a data set it will accept
the word index it will accept the max
sentence length i mean everything that
our data set requires like everything
that this constructor requires
um
[Music]
in addition to that it will receive a
batch size
and what it's going to do is it's going
to make
first of all
an instance of this
special
custom data set class
you know there's a better way of doing
this but i'll not discuss that here it
might be too magical for now
and
it's not data
and what it will do is it will return
start
util start data data
loader
of
this data set
batch size this
now
here what we can do now is
we so this is our training data loader
oh i see we can let's also import
from
data loader
import
make data loader
make data loader of
data set
of train
of train
and what's the word to index
max sentence length is let's set it to
30 right now and batch size is 64.
let's go with that and we'll do this for
we'll do this for
the
validation as well as the
and yeah mind you like this is
this can all again be written in a much
more nicer way but we're
going with this for now
and let's see if we are able to see the
first
you know training batch
oh let's let's let me also take a moment
it works let me also take a moment to
show you
why you know this guy is needed so the
so when you include this
whatever you have written here it's only
gonna be executed when this script is
called like this module is called as a
script
however if you don't have this line
let's say you don't have this line then
it's going to be called every time this
function is going to be executed every
time a
different file a different sub module
imports from main
let me show you how that works
let's say we have this data loaded right
and let's just make
let's just call this data loader here
uh
so let's
make let's make a dummy function my bad
i don't want to create
print
called
and
yeah
so
if we execute this
this thing was called
so it basically means that this function
was executed however we don't want it to
be executed if we are importing this uh
if importing this data order
so to avoid that
we add
this thing
and now it should not and actually let's
also not print this
so now you see that cold was not printed
even though we are calling that thing
here so this is the difference between
including those lines versus not
including those lines
i hope that answered questions
for some of you
okay so now we can you know actually
proceed to writing a training loop
because we have so we are here right now
so we have the raw data we have the
you know
pre-process the data converted them into
indices
now we want to just pass them to a model
and let's start our model implementation
so whenever you are implementing a model
class
let's call it lstm classifier
and you have to import sorry not import
in inherit from this guy torch.nbc
okay
and
can i magically initialize this
yeah it looks like it
okay
sorry
lstm classifier itself
so this is
the
initializer and
forward method so we will see what
how what what these guys do
how to use these guys
um
okay
so now that we have a data like a batch
so this is our batch right
and our model is
this guy
so when you do whenever you do model of
batch
this forward function is called and this
is equivalent to doing a forward
propagation forward pass on your data
right so that's how pi torch is
implemented so this line is exactly the
same as this line so i mean except you
don't really need to explicitly write
that forward whenever you call a batch
on
something that inherits from
torch.module
it automatically executes the forward
thingy
so now in this model of ours what we
will be having first of all is
an embedding layer
you know um
torch dot embedding
so
okay as you can see
the documentation says that the in first
argument is the number of embeddings so
in our case it's going to be vocab size
right
and
the second dimension is the embedding
dimension so
how many what's the embedding dimension
for our use case
and
that's
it for now
i guess
that should be enough and
let's also in yeah let's also feed these
guys
and before i move forward let me explain
what why we need these embedding
matrices or this embedding matrix
so think of this embedding matrix as a
lookup table so for every word in
your vocabulary it's gonna
so okay so here's what our
input ids will look like right it's
going to be an input
something like this except it's going to
be a batched version of it i'm just
including showing one sample for for
ease so when you pass these uh things to
the like these input ids as is to the
model
um
it's essentially equivalent of uh
using one hot encodings except you're
not exactly you know using uh one hot
encodings here but it's sort of similar
to doing that and the model is basically
gonna have a hard time relating between
these two random numbers right uh like
what do like how does this word
correlate this token correlate with this
uh token
so
what we do is we project each of these
tokens into a higher dimensional space
and that higher dimensional space in our
case is the embedding space
so instead of so every
uh element in this input id is now
converted into a three-dimensional
embedding for one
it's a 300 dimensional
and bearing for 34 and so on so
initially if your bats
initially if your input ids
was
you know path size by whatever the max
sentence length you had
let's call it t
then
after running it through the embedding
layer
it should it will be
uh whatever your embedding dimension
oh i wrote 300 because in the back of my
mind i have 300 as my
embedding dimension
so if your embedding dimension is 300
um
you go with that
and that's exactly what we are gonna do
here
let's pass
so we'll be passing this patch through
it and we know that this patch is a
dictionary
so
embedded is equal to
self.embedding layer of
x of
input ids
and you know let's let's
intercept this call and
look if it works or not
of course it's missing two things one is
the vocab size the vocab size is is
going to be length of
word to index in our case and embedding
dimension since i had 300
let's go with that
or actually let's start with 100 it's
not going to hurt
we want things to move fast
okay
so now if you look at the embeddings
shape it's going to be
batch size by your max sequence length
by embedding dimension
and what it looks like is something like
this you cannot really make much sense
of it but out of it but essentially your
input
ids
oopsie
x input
so you know you had these uh
let's just consider
so
for each of these guys so for 0 it
pulled out a 300 dimensional vector for
zero it pulled out a 100 dimensional
vector for four and so on and
that's why you have that extra dimension
here
right so this extra dimension is
because of that
so now that we have our embedding layer
we actually want to be want to want to
pass it through an rnn
right so in our case this is going to be
a simple lstm
oopsie
door start end of lstm
and although it doesn't say here let's
look at
lstm
yeah so if you read through the
documentation you'll see the first one
is input size the second one is hidden
and the third one is num
that's what we will need for now but you
can see that you know there are other
things like if you want to make this a
bi-directional lstm you can go with this
if you want to add dropout to your
network you can go with that
as you will be required i
guess to experiment with but yeah
so in our case the
in our case the
input size for
the lstm is going to be
embedding dimensions so basically when
you embed it uh when you fetch the
embeddings now you'll pass this thing
through the network and get the feature
vectors like these feature vectors for
the sentences
uh and the output dimension is going to
be hidden dimension
hidden dimension
uh like how many hidden units do we want
to have and then
oops
number of layers we are not currently
accepting
any of those guys
let's fix that
let's see
you know
what
we'll go with
300 and then let's say
it's a two layered network
right
now
okay
okay
so right now what we have is embedded
so it's a bad size by d by i'm bearing
dimension and we want to pass this thing
we want to pass this thing through our
rnn
oopsie
voila so now we have the outputs from
the rnl so what does an lstm output
let's look
so
this is the input this is these are the
outputs so it's gonna output
uh so it's gonna return us three things
as it should
so lstm returns you three things one is
the output h and the cn
so let's talk about them for a moment
so hn and cn these are hidden states and
cell states respectively and output as
you can read here contains the output
features from the last layer of the lstm
for every time step right
hn is basically the final hidden state
for every element in the sequence
and as you might see this is the only
thing that you know
depends on
depends on the number of layers
right
so
for classification tasks what we use is
the outputs
because that's what we care about right
so we we care about the output of the
last layer it's similar to i guess even
uh vision like even if you have some
kind of a cnn you pass your model
through your cnn you get your
features from the last layer and then
you use those features for like
classification right in the end you get
the logics and then you make
classifications here it's a similar idea
you use outputs for classification but
when do you actually use these etchants
and cs these are for
encoded equator type tasks so when you
want to actually get
the latent vector for a given sentence
you actually use etchants and cell
states are
yeah mostly they they're they are
very less likely to be used by anyone
but it's still useful in some cases like
when i personally
test it
to see what cn
is helps helps with uh it does helps
help in some cases but you know
etching is anyways more helpful so
for all practical purposes you can
ignore this cell state
and focus on h and output and remember
that we use output for
classification tasks and
hn for more sequence to sequence like
tasks
with that being said
so that's what our output is going to be
and we can ignore whatever is the rest
okay
so as you can see we went from the
embedding space
to the model's hidden space right so
so the bearing space was 300 dimensional
the model space is 500 dimensional as we
specified here
um
so now what we actually need you know is
something like this so this is what we
have right now we have something bad
size d and hidden dimension
but what we actually seek is
something like
a feature vector
on top that we can use to
you know compute logics so you see
logics are
batch size by number of classes
dimension so they are basically they
have as many elements as you have in the
as as number of classes you have in your
real set in our case it's two so in our
case the logits are gonna be like
of length two for every input sample
um
so we basically need
so basically we need a way to go from
here to
here two because we have only two
classes so we need to do two things
first of all we need to get rid of this
extra dimension
and
then we'll need to
project this from you know 512
dimensions to just two dimensions
now how do we actually get rid of this
guy
so
what we do essentially is we only look
at the um we only we only consider the
output representation of the last
token uh in a given sentence and why do
we do that because that it is when we
train rnns since they have a feedback
loop we hope that the representation for
the last token in a given sentence it
can also contains information about its
past that is and all the tokens from the
beginning and the
that essentially can be used as a
sentence representation so it's
essentially going from a token
representation to a sentence
representation
right and
let's
ignore that
so how do we extract the embedding for
the last
token from every element
well it's quite simple
we just need to do this
and this is what it looks like
so what we essentially did here
so what we essentially did here
is tensor slicing
so you know how we
have a list let's say we have a list
one two three
and
if you do something like this it's not
gonna slice it right so this is like
doing tensor slicing but on every
dimension so we know that output is
initially three-dimensional
so
doing
nothing on the first dimension we need
everything from there
but from the t dimension we only need
the last one that's shown by minus one
and from the third dimension also we
need everything but since this is
occurring after this we can
after this guy
we can ignore this so these two guys are
like equivalent you can play around with
this to understand more
but yeah so we went from bst hidden
dimension to
bs hidden dimension now we want to
actually have a projection
let's call it the output layer
so this is going to be a linear layer in
our case
the input being hidden dimension the
output being uh
number of classes right
so we don't have the number of classes
as input yet
and classes
okay
two layers two classes and essentially
this is going to be my output
so
when you pass a batch
through this model
this is what it's going to return
this is what it's gonna return
so this thing will be
this thing will be
batch size by two dimensional
and let's look at this guy
yay
shape of this is bad side by two
okay so now we have the forward method
implemented and so let's actually move
on to write a training loop here
um
all right so we have these three data
loaders so first things first here's our
we have already have our model
now we want a loss function
so loss function we will be using is
cross entropy loss
then we'll need an optimizer
optimizer
uh it's gonna be tors dot opt-in dot
let's go with adam
so what it's going to accept is the
model's parameters so that's what will
tell
uh
which parameters to optimize later on
and
learning rate so you can see the
documentation here params learning rate
yeah params learning rate so by default
it's one e minus three but let's be ex
extra explicit here and
so later so
what we can do later on is you know move
all these hyper parameters to a common
file called config or whatever and then
we can read these convert like we can
read these hyper parameters from there
instead of you know hard coding it here
but this is a tutorial so i'll not do
that here
so we have the model we have the loss
function we have the optimizer
now what to do now we can actually move
forward to writing a training loop for
for batch in
training data loader
so here's our logits logits are going to
be model model of
batch
and
from these logits we calculate the loss
first so the loss is going to be
calculated
okay
against these logics and whatever the
ground truth tables are so let's touch
the ground troop labels too
um
what did we call them label i guess
yeah we call it label
and
we have the loss
and once we have the loss
we essentially now
can do stuff like
backward
but there is something else that we also
need to keep in mind here so when we run
this loss dot backward
uh
so here's what happens
get rid of this guy
so this is your input
right and then
from input you you feel your input
to the model and then your model outputs
some logics you optimize those projects
against uh you know some some
ground rules classes and then you get
the actual loss but how do you make your
model
uh how do you make your model
learn like how do you pass that feedback
along to the model
well when you call that lost or backward
function it essentially passes this
feedback to first the logics
and
essentially so let's break this model
too
so this model is essentially made up of
this rna
so model is made up of this embedding
layer then we have this rna layer
then we have this
output layer
right
in order so
the output of this layer is the logics
and first the losses loss is transferred
here the feedback transfer here then the
to the rnn then to the embedding layer
now notice that you know i'm just
initializing uh when i run this it
basically initializes a vocab size by
embedding dimension matrix
so and and that matrix is by default
trainable
so
yeah that matrix is d by default
trainable which means that you know
initial initially all the 300
dimensional embeddings that it pulls for
any token they are randomly initialized
and over time as our training progresses
it learns a meaningful representation
now like those values change to
something more meaningful
all right
so
with that being said
we cannot just yeah with that being said
i want to add one more thing
uh
well i like logically you can do like a
lost or backboard directly but what we
actually need to do before this is
zero crowd because sometimes what can
happen is you know there are some
gradient issues so there are some
lingering gradients from the previous
iteration and what not so just get to
get rid of to flush out those extra
junk
be zero out those guys and then we apply
this last thought backward and once we
applied so this is going to take care of
the
this is going to take care of the feed
like
this is going to provide the feedback
and then
the optimizer is also going to be
updated likewise
so this is i mean this is pretty much it
for the training loop
i guess
and what we actually need to be able to
see is i guess this
let's see if this works or not first and
then we can do extra stuff
we didn't see any errors so it worked i
guess
okay
okay so actually to
track how much
training has been proceeded
just finished we can do
so this dq dm if you have anything
around like any iterate iterator
around tqdm it's gonna show you the
progress bar
and it's not currently in my env
so i need to install it
and that was surprisingly fast again
so now at least i should see yeah so we
see this progress bar so we know that
the training then did actually happen on
whatever tiny data set we had you know
we have we still haven't removed those
100 line truncation thingy
so let's actually print the loss values
losses and
we'll also print the test sorry train
accuracy here
right
so we have the losses and let's actually
look at the loss
let's actually look at the loss
that will help explain
some things
specific to
by torch
so if you just do print off loss
it's going to wrap everything around
this tensor and
we don't want this
for the purpose of for tracking purposes
we only need
this floating point value so we do lost
our item to give us that
and that's exactly what we are going to
append to
this guy and what about accuracy so in
our case it's going to be very easy
so we already have the logits so how do
we get the predictions from the logics
oops yeah so how do we get it
we apply the arc max
hardmax over the logics
and this just tells it to run it on the
batch dimension and not let's just
actually look at here
so this is our
these are our logics and we want to
and this is what they look like
let's look at one logic vector right so
it says right now
uh well it's a bit tricky to explain
because this is not exactly soft max so
you usually well i'll get to that
what i want to show you is what happens
when you apply an arc max operation on
top of your logics
so okay so either this can happen
is not what we want
we want this right so
we want to apply the arc max on the
first dimension
i mean yeah not the 0th dimension
uh so when you apply this rmax operation
on every
uh element in that
batch size by n classes logic vector
you essentially get the classes right
you essentially get the predictions when
you wrote
these don't need to make sense right now
i'm just trying to show you
what will happen when i apply it
now
one thing to notice here is you know
don't apply these this arc max before
calculating this loss i mean
here i'm still calculating the loss on
the logics and not these predictions
that's because arc max is a is a
non-differentiable operator so you know
uh
as you see saw before
i can go from here to zero because if i
get a max like if i
track like at what index i have the
maximum value i can get zero
but i cannot go from go back from here
to here
right so
i can get the index of the maximum
element
in this vector
but i cannot get the
vector if i just know the index like the
value of this vector if i just know the
index right
so that's why
um
we apply the arc marks we don't uh so we
apply the loss on the logits
so these are the predictions and
okay
now accurate accuracies
accuracy is just going to be in our case
this
or maybe
let me see
if i apply arc max over my
logics
um
okay so i do need to change it to end
all right and if i do i mean
floating point number okay so instead of
m let's go with float okay so i have the
mean that's interesting
and
i don't need to convert this into an
accurace or append
accuracy start append
whatever i did here
cool
and
to fetch the element and not the tensor
object
cool cool
so we have the accuracies and let's
actually
so after this
loop
let's print the
loss and the accuracy
i see i guess i'll need it anyways
so we want to print the mean right so
let's
import numpy after all
because i don't want to do like some and
then blah blah blah
i don't think this no this is not gonna
work or maybe it does
you know let's try
only one way to find out
okay let's get rid of
can't call an empire tensor that
requires crowd use tensor dot attach
this is on line
55
of course
951
um i see what's the issue here
so instead of this i guess what we need
to do is
this
and because these guys are tensors so
and every dancer has a numpy
uh
well
we
we will
we will we will
it should be uh
let's let's look at what's happening
here
this could be
interesting
because this might be okay
oh i see
i see i see
i updated some things in my
head but
not in the code
losses
and
lo says is
what it
should be
and let's see
so
so if i i cannot do a mean on top of
this so what we'll do so we'll need to
switch it back
and a
okay so i guess now we can switch back
to what we had earlier
this np array version
okay except we need
to do
this
let's see if this actually will work or
not let's test it
it does work so
we can remove this
close it
and run it again
okay so after first quote-unquote epoch
our test training loss was this and then
the testing loss was
this
so training accuracy was this 66
but
well
we need to actually wrap this
inside so this was just one epoch
for epoch
in range whatever number of epochs you
decide let's say it's 10.
you
print this
and you print
print
okay
hip hop
[Music]
and
what this is is
loss
training loss
and
this is just gonna restrict it to the
first four floating point
numbers otherwise it could be you know
this long
and train
accuracy
this guy
this guy and this guy
except
not that guy
so now if you run this for epoch
whoa
whoa see the model literally overfitted
because we only have like 200 examples
and let's now
remove that constraint
this ridiculous constraint
because we said oh i have the training
group but i don't have the validation
loop yet
so let's add that for every epoch
i'm gonna have to do some validation
so with storage dot no grad
so when you do this
there will be no creations required
and
you need to do two things basically
whenever you want to run your
uh neural network in evaluation mode
first
disable gradient calculation and switch
it to eval mode
so this is going to take care of stuff
like drop out and things like that
um
okay so in your valid your validation
loop is going to look exactly like the
training loop except you will not
compute the losses and i mean you can
compute losses and that's how
you will be able to um i guess
see like uh
if your model is let's see if your model
is overfitted or under fair or things
like that but what you don't need to do
is you don't need to do this backward
propagation you can compute the loss but
you don't really need to compute the
loop
you don't need to perform a backward
propagation
this is going to be
validation loss
and this is going to be my evaluation
accuracy
and the
testing loop is going to be exactly the
same as the validation loop except
instead of the validation loader you
have the
test loader and yeah the model obviously
overfits
um
so actually you know let's let's
let's let's write functions for these
two guys
instead of having them like this
let's write a function for train
definition train
does it fix it for me
no it doesn't it's not that smart
so
what this train is gonna need is a
data loader
it's gonna need the model it's gonna
need the lost criterion and it's gonna
need the optimizer
and
it should be good
so now
call that
train
on train data loader
and model
criterion
optimizer
all right
so i mean you can actually use the same
function for training validation and
testing you'll need
an extra flag to first of all i guess
determine
whether it's training or not but for
our purposes we will
just write a different function
it's more readable
and we'll literally copy paste
stuff
so this is the most mundane part of
actually not this one
yet
and instead of this i'll just add a
decorator
so this is uh
equivalent to
writing that with source.nugrad env
thingy but here now we have saved
essentially an indentation so yeah that
looks nicer the code looks nicer
so we'll use the same function for
validation
and testing
but since we don't have a way to tell
whether it's validation or not we'll
just call it loss and accuracy and print
that out
outside when we call them
okay
so that should do it
validate on
pal data ruler model
crystal
validating
and
testing
cool
so now we have
our stuff implemented
a whole pipeline implemented
or not
so this error happens inside validate
and line 79
sorry what
line 79
module object is
oh my bad my bad it's not cold
i'm sorry
yay
so we have the okay okay okay so we have
something like this so for ever for a
given uh epoch it's first gonna train
the model trend the training loss
accuracy from the
validation
loss and accuracy then the testing loss
and accuracy
okay
so we are almost done with the tutorial
except what we are missing right now is
you know we don't have a way to save the
files
uh so saving files
is helpful
so in your assignments i i asked you to
always implement uh
always implement uh
inference script so
uh saving files will also allow you to
do things like you know transfer
learning so for example if you save a
checkpoint and then
uh your training if you uh your training
breaks uh in the middle for some reason
and then
you know you're doing you didn't you
were not saving any checkpoints then
you'll essentially need to start your
training from scratch so that's why you
need to save your checkpoint so that if
something breaks or if you want to
resume your training later on
you can do that
now
let's see
[Music]
let's create a folder called
checkpoints
checkpoints and
well saving a model is pretty easy and
i'll reduce this to just
one epoch even
um
saving is quite easy in pytorch
you literally just have to call this
torch.save method on any object that you
would like to save now what do we want
to save here
the model's state dictionary so this
dodge dot save is i mean it's like a
pickling operation but it's better than
that and this is the object that we want
to save
uh we can also save the optimizers state
dipped and we let's let's actually see
what this looks like
to give you an idea
okay
oh
so you see
the state dictionary of the model
contains all the different uh things in
our model and their
saved values like trained values until
that point so if you want to be able to
save these guys you can do the same
thing with the optimizer and you know
save the optimizers check points as well
if you want to
save them but
for for for this tutorial we'll just
save the
model itself
checkpoints.ls
not lstm model.pt
and
that should be oh well any save function
is incomplete
without its corresponding i guess
load method or load function we can
write it here let's just write it here
checkpoint it receives a model and then
it does this
then we also need to write a load model
actually we'll do that in the inference
script let's let's write the
inference script now
let's call it
train
our main was also fine i don't know why
i changed it
inference dot pi now here is where we
load the model
load the model and then run
predictions
so i'm just going to show you how to
load the model like actually running the
prediction it's
it's the same thing as it's very similar
to
like testing and validation so oh and we
can get rid of this now
definition
load checkpoint so the name
cp path so similar to torch
uh so similar to
torch.save we have torch.load so when we
so when we run this torch.load on
whatever's in this cp path
it's going to load the object so right
now it contains the state dictionary of
the
model
right so we'll actually need to
we'll actually need to
apply these
models to whatever like we'll actually
need to create a model
uh is what i'm trying to say here so to
do that
we need to first of all create
an instance of the model
and let's also do one thing
yeah let's move this classifier
from here to
another thing you can actually do to
make your code more modular is you know
add this train and
test as like a
method
inside this lstm classifier class so you
know have something like
oops
have something like train or whatever
and then it does the training and
similar
for validation and then you can also
have something of something like a
predict method so that will make your
code look nicer and you know it will be
easier for you to manage
so now what we can do is here we can
import our lstm
from models import lstm class
and
lstm classifier it's gonna
be vocab size
so okay so here's what here's where the
catch lies
we are not saving the vocabulary oh okay
interesting
so let's also save the vocabulary that's
a good idea
with open
data
word
you can load them
word to index in this
to this file
we need to import
oh
we need to import pickle and now we
should also and we will basically load
that
um
oh yeah for sure
we have this though um
from modules import lstm classifier
okay cool so now we have this and we
should at least be able to get the gap
size
word index is equal to
um
this is by the way not a good way of
doing this whole thing
there's probably a better way to
to not repeat these operations
as much
but for now
um
i'm just gonna go with
this word to
index.pickle
and i'm gonna read it
and the vocab size now is gonna be word
to index
and embedding is i know it's 300
512
10 layers
two classes
now this is the checkpoint now what you
want to do is
you want to load this state depth so
this is
torch.load is going to return you the
static and then
when you call it on top of the
model you will be able to actually
so that's your load checkpoint and i'm
not gonna finish this whole script but
yeah
once you have loaded the model it's
essentially doing the same thing as here
you get the
you make
a data sample
so
so here you'll need to make a data
sample which will have like you know
those input ids
input ids
uh something and then
i guess that's all you will need to have
or
okay actually my bad so you should you
should here you can just start to start
with a sentence let's say i like
this restaurant
and then on top of this x you need to
apply whatever preprocessing you did in
your data loader and then get to get
that batch that dictionary with input id
is text and blah blah blah and just pass
it through the model like so
uh
as you are doing that here in this test
function just
method function sorry
um
except you will not be using a data
loader you just need the model and
yeah
you will be able to get the predictions
in real time for your deploy system
so you know let's finally finally remove
this
because i want to show you one last
thing before we
call it a day
so this is the training
loop
so now it's taking time because
it's processing all the 800 000
sentences
and okay as you can see
it's showing me like about half half an
hour for one training epoch and that's
quite slow and the reason for that is
because everything is happening on a cpu
right now
so we need to move things to the gpu
let's do that let's do that um
that's quite straightforward
so device
cooler
if
closed or cooler is obvious so if the
cooler driver is detected it's going to
be cooler
else it's gonna run on cpu so this code
is uh gonna be executable from you know
both your even if your mac i guess or
your windows local
local windows machine is just gonna take
a bit longer as you just saw now once we
have this device we will actually need
to put our tensors
there
so
these guys
need to be on the tensor on the gpu
or to keep things simple
or actually let's add it here it's gonna
save us some time
overall
um
labels can stay on the
device device
and we probably need to add this device
amazon review data set here
cool
device
alrighty
now all we need to do is
this
okay
okay
so we put our input tenses
now we also need to put our model
on the gpu
it should hopefully work
okay so somewhere it found
a discrepancy
right here in the logics and by oh so
the logics are on the cpu but the these
guys are only
so let's also
where is it
so until now it was a tensor sorry it
was an integer but
since we want to put this
since we want to call this
two method
we need to convert it
to a tensor
and
okay
voila so
we were able to reduce that training
time projected training time at least
from half an hour to a few minutes
now you know
this can further be reduced let me show
you how
um
so there are two ways
so this is the
naive way of
implementing an sdm classifier
now
if you if you remember we added a lot of
padding tokens to it and then there
exists something uh in pytorch that we
can utilize to you know
implement a more logical or a more
smarter way of doing this forward pass
so that we ignore those pairing tokens
um
all together
so
that thing is called
pack padded sequence and it's counter
equivalent
so what it does essentially is it backs
packs uh
quote-unquote packs an input tensor
that's already padded and it also
accepts the lengths of the inputs right
so this is how it decides that anything
that's that comes after
that that
that comes after this id in a given
sentence it's not going to be useful
right
so
we almost have everything to implement
this except we don't have this length so
let's have let's compute that as well
and that's quite easy to do we can do it
right here inside this text to indices
so when we are doing blah blah blah
i guess
huh
yeah i guess what we can do is
he adds us and
text length is equal to length of input
ides
and for the other case it's gonna be
text
length is equal to length of
input ids
again
and we will return this text length
and
and
text length is going to be
what we are getting
text length
and
we should now have this attribute there
where's the model
so you know for
let's reintroduce
these guys
to make sure it works first and then
okay so once we have that what we
basically need to do is we have this
embedding matrix i'm wearing vectors for
all the guys
so what we need to do now is we need to
um we need to pack them
we will so what's the caller
torch and then utils
torch and utils
rnn
back pattern sequence so the input
tensor here is going to be embedded
the lengths are going to be
text actually let's just call it length
okay okay
okay
um
length and
end force sorted is false
so by default this parameter is true and
if we keep it true to true
it will accept expect that this embedded
or this input ids this x of input ids is
sorted by like decreasing value of their
lens so the first element in the patch
should be the longest and the last one
should be the shortest but if we but
later on in pytorch they introduced this
thing
so it's not required now
um
[Music]
so this is the fact
so we kind of got packed it
now we will pass instead of the
embedding embedded thingy we will pass
this guy
and
so let's
see
what this
so if you look at lstm's documentation
that then if you input it a packed pilot
sequence then it's going to return a
back padded sequence as well a packed
sequence as well
so
we can
actually
look at it right here
intercept it
oh
my bad
okay let's look at
this thing
self.rnn
oh i see
so i'll need to
since there's a script already running
on
that gpu
we saw that error
okay
oh i see
let's just run it on uh
oops
let's try running it on a cpu this is
for debugging purposes anyways
maybe vs code doesn't support
so there was there's a bug
so the bug is indices element is out of
data bounds highly
actually let's also
okay
just to be
cool so now we have
this thing so instead of you know if you
remember we had a tensor before you used
to have a tensor before now we have a
backed sequence and this part sequence
is a funny object so if you look at the
spark sequence it contains a lot of
things and
you'd expect that this data is what we
would need however
this is weirdly shaped and that's
because it's padding all the different
sorry it's packing all the different
sentences of different lengths so that's
why
we need to basically unpack
uh unpack this
so how do we unpack it so again this is
the packed output remember
and in this case
um you know let's let's do something
i'm gonna not completely remove this
so
okay
and
so that we have this version as well so
now in so this is forward
packed
simple
so now
i was yeah i was gonna unpack this guy
so
how do we unpack this guy we use
the
opposite method of backpack sequence
that's
bad
bad back sequence
yeah so although it says pad in the name
all it's doing is it's kind of un
it's kind of
unpacking stuff and it accepts the
pairing value
and
okay
and okay let's see what it outputs by
the way
it returns a tuple of tensor containing
the pirate sequence that ends up
containing a list of lengths so we only
need the first object i guess so we will
not we will ignore the other guy again
um
okay
okay
sweet
torch.nbc
sequence
we pass it the output
um
and it also accepts oh yeah of course
patch burst will be true but it also
accepts the pairing value now i can go
with a con if we go with the config file
we won't need to hard code this value
like right now but we have we don't have
such a luxury
so i'll just hard code that padding
value to what it is in our data set
that's two
and that's first
so
when i do batch first it's gonna
it's gonna
put the batch dimension as the first
dimension
it makes
things easier to
process
okay so
let's see what we get after running
this guy
our output is not defined
output
now
okay
you know what i'll just
rerun this
some weird issue
let's see
all right
so this is the
unpacked output
right
and if you look at the shape of it
it's going to be 64 by 27 by
512 and we got this by
essentially
doing that length thingy we passed those
lengths
right
so how do we actually make sense of this
thing
so when it returns because we want to
again go from bs to d to
300 to
bs to
2 essentially so we need to construct
the feature vector here and constructing
the featured vector in this case it's
not really straightforward
i mean it
is if you kind of understand what's
happening under the hood
so what so we'll again be doing tensor
slicing but this guy won't do now
because since we you know tried to
optimize stuff we cannot know beforehand
what's the
what's the um
what's the i guess
length what's the value of the second
index
so
what we want to be doing here is
for every element in the patch for that
it element we'll be basically
uh we did have the length right
so we can use this length
because we know that for that particular
element this is the maximum value like
the index of that maximum element
um
that's that we are going to be needing
and this is going to be our feature
instead now
and if we apply this oopsie
and if we apply this operation here
hola we got this and
um
let's also look at the values so yeah
so actually let's see what happens if
let's try
let's try to do what we were doing
before
so as you can see here it's literally
just getting those padding uh
padded pattern uh ids pirate sequences
like those padding tokens
and it's the shape
how uh by the way it's this but you know
it's all so that's why it's important to
not only match the shapes but to
understand
what's uh
what's the value of the object
understand what's going on so here what
we are doing is for every element we are
basically fetching
this id element
otherwise it was just gonna
fetch the padding token for us
now it should be fine
and hopefully we see some speed up in
the training process let's see
bye
so
actually let's also
increase this guy
oh it's sorry
it's everything's happening on a gpu
again sorry cpu again
waiting waiting waiting
and voila
what do we have here
so if you remember we went from half an
hour to something like eight minutes to
something like three minutes
so i mean this is this task is pretty
easy so it will be difficult for
uh i mean it's just a few minutes per
epoch but if you scale it to larger
models it's gonna save you maybe even
months or
days of training
and
i guess one other thing also that would
happen is your model is going to learn
faster it's not going to be very evident
in this case because you know like it's
a binary classification task and
well the model is too expressive right
now like it has
um so it's not going to be very visible
the difference in performance but
this guy is going to learn faster than
the other guy and it runs faster than
the other guy so why not
so that concludes this lecture and i
hope you found not lecture sorry this
tutorial and i hope you will find this
video useful for your assignments
and yeah feel free to play around with
these hyper parameters and i'll be
uploading this code somewhere i'll
add the link too
thank you
i'm sorry this turned out to be a bit
long