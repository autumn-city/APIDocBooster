what is going on guys hope you're doing
awesome in this
quick video i will show you how to use
float 16
when training networks um and so
if you're using float16 it will reduce
your vram by approximately 50
and also on newer gpus it will just
train a lot faster
so it's definitely something you want to
use let's
let's take a look at exactly how we how
we use it
[Music]
all right so to make this video a little
bit quicker we're going to go to
my github page and we're just going to
take one of the code from
my previous tutorial so let's pick this
convolutional
network and we're just going to copy all
the code
for that one all right and let's paste
that in
and then what i want to do is uh make
the network just a little bit larger
so maybe we'll change the our channels
to i don't know
420 and let's do out channels
a thousand something like that all right
so it's just a little bit bigger
and let's now change match is maybe 128
and let's see if we can run this all
right so the limit seems to be
at a batch size of 590. so if we change
the batch size to 600
uh there's a crash but if we set it to
590 it's okay
so what we'll try is if we can set up
the float16 training and see
how much of a larger batch size we can
now use
so what we'll do is we'll do scalar is
torch dot
cuda dot amp dot grad scaler
and this is something that you set up
before training so outside of the
training loop
then what you'll do in the forward part
of the of the comp
of the network you'll do with torch
torch.cuda.amp.autocast
and you'll just perform
the computations the forward and the
loss inside of
inside of that autocast then for the
backwards step
it's going to be pretty it's going to be
the same for the optimizer
zero grad but for the loss.backward
we'll first do
scalar.scale of loss
and then we'll do instead of optimize
that step we'll do scalar dot step
up of optimizer and the last thing is
we'll do scalar dot update
all right so it's pretty close right to
how it normally looks like
but so now let's see first of all if you
can run with a batch size of 590
and then let's try to see sort of how
much larger we can have
so maybe we can try 800
so when we used flow 32 we
we could use so float fp32
we could use a batch size of 590. for
float16
we can use a batch size of 1000 so
that's about
a 70 larger batch size and
i haven't checked we can also perhaps
make this a little bit larger
but anyways as you can see there's a big
difference
and there should be and you should use
float16
pretty much as default when training
networks alright so this video was
useful thank you so much for watching
and i hope to see you in the next one
you