 After this, we will update the alpha term unlike AdaGrad there is one more term in this equation known as 'rho' this is used to avoid infinitely increasing alpha value.  As we saw in AdaGrad, the alpha value was increasing with each iteration but in the case of AdaDelta, the alpha first increased till 50 iterations and then continuously decreased so now there is no problem of learning rate decay. 
 Now, let's visit our Adam again... Adam's main motive was to add the concept of momentum into the previous optimizer like AdaDelta. 
 So,   you find that this RMSProp which takes  expose exponentially decaying average,   the AdaDelta takes a moving window average of  the squared gradients.  So, that is the only   difference between RMSProp and AdaDelta. 
Adadelta has a very slow start. 
I think Adadelta performs better with bigger networks than yours, and after some iterations it should equal the performance of RMSProp or Adam. 
