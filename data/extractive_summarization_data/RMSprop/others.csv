"Vanilla adaptive gradients (RMSProp, Adagrad, Adam, etc) do not match well with L2 regularization. "
"Usually batch sizes around 40 gives better results, as for my experience training with 40 batch size for 3 epocs using default RMsprop gives around 89% accuracy. "
"Have you tried adjusting the learning rate of RMsprop optimizer ?  try with a very small value first ( default is 0.001, in keras implementation) and try to increment it with factors of 10 or 100."
Adam is a recently proposed update that looks a bit like RMSProp with momentum.
"Just a question for you, why aren't you using Adam Optimizer which seem to be the best optimizer in a lot of cases ?  (It is even partially inspired from RMSProp that you use)"
"In   particular, when combined with adaptive gradients, L2   regularization leads to weights with large gradients   being regularized less than they would be when using   weight decay."
"Although the expression ""Adam is RMSProp with momentum"" is widely used indeed, it is just a very rough shorthand description, and it should not be taken at face value; already in the original [Adam paper (hyper-link)], it was explicitly clarified (p. 6): "
"There are a few important differences between RMSProp with momentum and Adam: RMSProp with momentum generates its parameter updates using a momentum on the rescaled gradient, whereas Adam updates are directly estimated using a running average of first and second moment of the gradient. "
"The best algorithm is the one that can traverse the loss for that problem pretty well."